//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32345990
// Cuda compilation tools, release 12.1, V12.1.55
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	assemble_matrix_cuda_kernel_forward
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9all_hostsE = 1;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_35_bitE = 2;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_37_bitE = 4;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_50_bitE = 8;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_52_bitE = 16;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_53_bitE = 32;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_60_bitE = 64;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_61_bitE = 128;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_62_bitE = 256;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_70_bitE = 512;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_72_bitE = 1024;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_75_bitE = 2048;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_80_bitE = 4096;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_86_bitE = 8192;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_87_bitE = 16384;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_89_bitE = 32768;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail9sm_90_bitE = 65536;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target6detail11all_devicesE = 131070;
.global .align 8 .b8 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target7is_hostE[8] = {1, 0, 0, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target9is_deviceE[8] = {254, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target10any_targetE[8] = {255, 255, 1, 0, 0, 0, 0, 0};
.global .align 8 .b8 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target9no_targetE[8];
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_35E = 35;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_37E = 37;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_50E = 50;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_52E = 52;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_53E = 53;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_60E = 60;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_61E = 61;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_62E = 62;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_70E = 70;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_72E = 72;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_75E = 75;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_80E = 80;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_86E = 86;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_87E = 87;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_89E = 89;
.global .align 8 .u64 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2nv6target5sm_90E = 90;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;
.global .align 1 .b8 $str[54] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10, 0};
.global .align 1 .b8 $str$1[92] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 105, 58, 32, 37, 100, 44, 32, 106, 58, 32, 37, 100, 44, 32, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 110, 95, 114, 111, 119, 115, 58, 32, 37, 100, 44, 32, 110, 95, 99, 111, 108, 115, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10, 0};
.global .align 1 .b8 $str$3[10] = {112, 116, 32, 110, 97, 110, 33, 33, 33, 0};
.global .align 1 .b8 $str$4[88] = {101, 101, 32, 110, 97, 110, 33, 33, 33, 44, 32, 101, 100, 103, 101, 95, 97, 114, 101, 97, 58, 32, 40, 37, 108, 102, 44, 32, 37, 108, 102, 41, 44, 32, 98, 97, 114, 114, 105, 101, 114, 32, 118, 97, 108, 58, 32, 37, 108, 102, 44, 32, 100, 115, 113, 58, 32, 37, 108, 102, 44, 32, 97, 99, 116, 105, 118, 101, 71, 97, 112, 50, 58, 32, 37, 108, 102, 44, 32, 120, 105, 58, 32, 37, 108, 102, 10, 0};
.global .align 1 .b8 $str$5[4] = {37, 115, 10, 0};
.global .align 1 .b8 $str$6[9] = {97, 100, 106, 58, 32, 37, 115, 10, 0};

.visible .entry assemble_matrix_cuda_kernel_forward(
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_1[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_2[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_5[56],
	.param .u32 assemble_matrix_cuda_kernel_forward_param_6,
	.param .u32 assemble_matrix_cuda_kernel_forward_param_7
)
{
	.local .align 8 .b8 	__local_depot0[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<301>;
	.reg .b16 	%rs<123>;
	.reg .b32 	%r<822>;
	.reg .f64 	%fd<1729>;
	.reg .b64 	%rd<1218>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r404, %r405}, [assemble_matrix_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r406, %r407}, [assemble_matrix_cuda_kernel_forward_param_0+8];
	mov.b64 	%rd159, assemble_matrix_cuda_kernel_forward_param_2;
	ld.param.v2.u32 	{%r412, %r413}, [assemble_matrix_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r420, %r421}, [assemble_matrix_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r428, %r429}, [assemble_matrix_cuda_kernel_forward_param_5+32];
	ld.param.u32 	%r403, [assemble_matrix_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd164, [assemble_matrix_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd162, [assemble_matrix_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd160, [assemble_matrix_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd157, [assemble_matrix_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r375, [assemble_matrix_cuda_kernel_forward_param_0+16];
	mov.u32 	%r432, %ntid.x;
	mov.u32 	%r433, %ctaid.x;
	mul.wide.u32 	%rd166, %r432, %r433;
	mov.u32 	%r434, %tid.x;
	cvt.u64.u32 	%rd167, %r434;
	add.s64 	%rd1214, %rd166, %rd167;
	setp.ge.u64 	%p1, %rd1214, %rd157;
	@%p1 bra 	$L__BB0_209;

	cvta.to.global.u64 	%rd3, %rd164;
	cvta.to.global.u64 	%rd4, %rd162;
	cvta.to.global.u64 	%rd5, %rd160;
	cvt.s64.s32 	%rd6, %r407;
	cvt.s64.s32 	%rd7, %r406;
	cvt.s64.s32 	%rd8, %r405;
	cvt.s64.s32 	%rd9, %r428;
	cvt.s64.s32 	%rd10, %r412;
	cvt.s64.s32 	%rd11, %r420;
	mov.u64 	%rd58, %rd159;

$L__BB0_2:
	setp.lt.s32 	%p2, %r375, 4;
	mov.u64 	%rd1215, %rd1214;
	@%p2 bra 	$L__BB0_6;

	or.b64  	%rd168, %rd1214, %rd6;
	and.b64  	%rd169, %rd168, -4294967296;
	setp.eq.s64 	%p3, %rd169, 0;
	@%p3 bra 	$L__BB0_5;

	div.u64 	%rd1215, %rd1214, %rd6;
	bra.uni 	$L__BB0_6;

$L__BB0_5:
	cvt.u32.u64 	%r435, %rd6;
	cvt.u32.u64 	%r436, %rd1214;
	div.u32 	%r437, %r436, %r435;
	cvt.u64.u32 	%rd1215, %r437;

$L__BB0_6:
	setp.lt.s32 	%p4, %r375, 3;
	@%p4 bra 	$L__BB0_10;

	or.b64  	%rd170, %rd1215, %rd7;
	and.b64  	%rd171, %rd170, -4294967296;
	setp.eq.s64 	%p5, %rd171, 0;
	@%p5 bra 	$L__BB0_9;

	div.u64 	%rd1215, %rd1215, %rd7;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r438, %rd7;
	cvt.u32.u64 	%r439, %rd1215;
	div.u32 	%r440, %r439, %r438;
	cvt.u64.u32 	%rd1215, %r440;

$L__BB0_10:
	setp.lt.s32 	%p6, %r375, 2;
	@%p6 bra 	$L__BB0_14;

	or.b64  	%rd172, %rd1215, %rd8;
	and.b64  	%rd173, %rd172, -4294967296;
	setp.eq.s64 	%p7, %rd173, 0;
	@%p7 bra 	$L__BB0_13;

	div.u64 	%rd1215, %rd1215, %rd8;
	bra.uni 	$L__BB0_14;

$L__BB0_13:
	cvt.u32.u64 	%r441, %rd8;
	cvt.u32.u64 	%r442, %rd1215;
	div.u32 	%r443, %r442, %r441;
	cvt.u64.u32 	%rd1215, %r443;

$L__BB0_14:
	cvt.u32.u64 	%r444, %rd1215;
	setp.gt.s32 	%p8, %r375, 0;
	selp.b32 	%r2, %r444, 0, %p8;
	setp.ge.s32 	%p9, %r2, %r403;
	@%p9 bra 	$L__BB0_208;

	setp.gt.s32 	%p300, %r375, 0;
	cvt.u32.u64 	%r820, %rd1215;
	selp.b32 	%r819, %r820, 0, %p300;
	mov.b64 	%rd1211, assemble_matrix_cuda_kernel_forward_param_1;
	mov.u64 	%rd1210, %rd1211;
	cvt.s64.s32 	%rd174, %r819;
	mul.lo.s64 	%rd175, %rd174, %rd9;
	add.s64 	%rd176, %rd3, %rd175;
	ld.global.f64 	%fd1, [%rd176+4600];
	ld.global.f64 	%fd2, [%rd176+4592];
	ld.global.f64 	%fd3, [%rd176+4584];
	ld.global.f64 	%fd4, [%rd176+4576];
	ld.global.f64 	%fd5, [%rd176+4568];
	ld.global.f64 	%fd6, [%rd176+4560];
	ld.global.f64 	%fd7, [%rd176+4552];
	ld.global.f64 	%fd8, [%rd176+4544];
	ld.global.f64 	%fd9, [%rd176+4536];
	ld.global.f64 	%fd10, [%rd176+4528];
	ld.global.f64 	%fd11, [%rd176+4520];
	ld.global.f64 	%fd12, [%rd176+4512];
	ld.global.f64 	%fd13, [%rd176+4504];
	ld.global.f64 	%fd14, [%rd176+4496];
	ld.global.f64 	%fd15, [%rd176+4488];
	ld.global.f64 	%fd16, [%rd176+4480];
	ld.global.f64 	%fd17, [%rd176+4472];
	ld.global.f64 	%fd18, [%rd176+4464];
	ld.global.f64 	%fd19, [%rd176+4456];
	ld.global.f64 	%fd20, [%rd176+4448];
	ld.global.f64 	%fd21, [%rd176+4440];
	ld.global.f64 	%fd22, [%rd176+4432];
	ld.global.f64 	%fd23, [%rd176+4424];
	ld.global.f64 	%fd24, [%rd176+4416];
	ld.global.f64 	%fd25, [%rd176+4408];
	ld.global.f64 	%fd26, [%rd176+4400];
	ld.global.f64 	%fd27, [%rd176+4392];
	ld.global.f64 	%fd28, [%rd176+4384];
	ld.global.f64 	%fd29, [%rd176+4376];
	ld.global.f64 	%fd30, [%rd176+4368];
	ld.global.f64 	%fd31, [%rd176+4360];
	ld.global.f64 	%fd32, [%rd176+4352];
	ld.global.f64 	%fd33, [%rd176+4344];
	ld.global.f64 	%fd34, [%rd176+4336];
	ld.global.f64 	%fd35, [%rd176+4328];
	ld.global.f64 	%fd36, [%rd176+4320];
	ld.global.f64 	%fd37, [%rd176+4312];
	ld.global.f64 	%fd38, [%rd176+4304];
	ld.global.f64 	%fd39, [%rd176+4296];
	ld.global.f64 	%fd40, [%rd176+4288];
	ld.global.f64 	%fd41, [%rd176+4280];
	ld.global.f64 	%fd42, [%rd176+4272];
	ld.global.f64 	%fd43, [%rd176+4264];
	ld.global.f64 	%fd44, [%rd176+4256];
	ld.global.f64 	%fd45, [%rd176+4248];
	ld.global.f64 	%fd46, [%rd176+4240];
	ld.global.f64 	%fd47, [%rd176+4232];
	ld.global.f64 	%fd48, [%rd176+4224];
	ld.global.f64 	%fd49, [%rd176+4216];
	ld.global.f64 	%fd50, [%rd176+4208];
	ld.global.f64 	%fd51, [%rd176+4200];
	ld.global.f64 	%fd52, [%rd176+4192];
	ld.global.f64 	%fd53, [%rd176+4184];
	ld.global.f64 	%fd54, [%rd176+4176];
	ld.global.f64 	%fd55, [%rd176+4168];
	ld.global.f64 	%fd56, [%rd176+4160];
	ld.global.f64 	%fd57, [%rd176+4152];
	ld.global.f64 	%fd58, [%rd176+4144];
	ld.global.f64 	%fd59, [%rd176+4136];
	ld.global.f64 	%fd60, [%rd176+4128];
	ld.global.f64 	%fd61, [%rd176+4120];
	ld.global.f64 	%fd62, [%rd176+4112];
	ld.global.f64 	%fd63, [%rd176+4104];
	ld.global.f64 	%fd64, [%rd176+4096];
	ld.global.f64 	%fd65, [%rd176+4088];
	ld.global.f64 	%fd66, [%rd176+4080];
	ld.global.f64 	%fd67, [%rd176+4072];
	ld.global.f64 	%fd68, [%rd176+4064];
	ld.global.f64 	%fd69, [%rd176+4056];
	ld.global.f64 	%fd70, [%rd176+4048];
	ld.global.f64 	%fd71, [%rd176+4040];
	ld.global.f64 	%fd72, [%rd176+4032];
	ld.global.f64 	%fd73, [%rd176+4024];
	ld.global.f64 	%fd74, [%rd176+4016];
	ld.global.f64 	%fd75, [%rd176+4008];
	ld.global.f64 	%fd76, [%rd176+4000];
	ld.global.f64 	%fd77, [%rd176+3992];
	ld.global.f64 	%fd78, [%rd176+3984];
	ld.global.f64 	%fd79, [%rd176+3976];
	ld.global.f64 	%fd80, [%rd176+3968];
	ld.global.f64 	%fd81, [%rd176+3960];
	ld.global.f64 	%fd82, [%rd176+3952];
	ld.global.f64 	%fd83, [%rd176+3944];
	ld.global.f64 	%fd84, [%rd176+3936];
	ld.global.f64 	%fd85, [%rd176+3928];
	ld.global.f64 	%fd86, [%rd176+3920];
	ld.global.f64 	%fd87, [%rd176+3912];
	ld.global.f64 	%fd88, [%rd176+3904];
	ld.global.f64 	%fd89, [%rd176+3896];
	ld.global.f64 	%fd90, [%rd176+3888];
	ld.global.f64 	%fd91, [%rd176+3880];
	ld.global.f64 	%fd92, [%rd176+3872];
	ld.global.f64 	%fd93, [%rd176+3864];
	ld.global.f64 	%fd94, [%rd176+3856];
	ld.global.f64 	%fd95, [%rd176+3848];
	ld.global.f64 	%fd96, [%rd176+3840];
	ld.global.f64 	%fd97, [%rd176+3832];
	ld.global.f64 	%fd98, [%rd176+3824];
	ld.global.f64 	%fd99, [%rd176+3816];
	ld.global.f64 	%fd100, [%rd176+3808];
	ld.global.f64 	%fd101, [%rd176+3800];
	ld.global.f64 	%fd102, [%rd176+3792];
	ld.global.f64 	%fd103, [%rd176+3784];
	ld.global.f64 	%fd104, [%rd176+3776];
	ld.global.f64 	%fd105, [%rd176+3768];
	ld.global.f64 	%fd106, [%rd176+3760];
	ld.global.f64 	%fd107, [%rd176+3752];
	ld.global.f64 	%fd108, [%rd176+3744];
	ld.global.f64 	%fd109, [%rd176+3736];
	ld.global.f64 	%fd110, [%rd176+3728];
	ld.global.f64 	%fd111, [%rd176+3720];
	ld.global.f64 	%fd112, [%rd176+3712];
	ld.global.f64 	%fd113, [%rd176+3704];
	ld.global.f64 	%fd114, [%rd176+3696];
	ld.global.f64 	%fd115, [%rd176+3688];
	ld.global.f64 	%fd116, [%rd176+3680];
	ld.global.f64 	%fd117, [%rd176+3672];
	ld.global.f64 	%fd118, [%rd176+3664];
	ld.global.f64 	%fd119, [%rd176+3656];
	ld.global.f64 	%fd120, [%rd176+3648];
	ld.global.f64 	%fd121, [%rd176+3640];
	ld.global.f64 	%fd122, [%rd176+3632];
	ld.global.f64 	%fd123, [%rd176+3624];
	ld.global.f64 	%fd124, [%rd176+3616];
	ld.global.f64 	%fd125, [%rd176+3608];
	ld.global.f64 	%fd126, [%rd176+3600];
	ld.global.f64 	%fd127, [%rd176+3592];
	ld.global.f64 	%fd128, [%rd176+3584];
	ld.global.f64 	%fd129, [%rd176+3576];
	ld.global.f64 	%fd130, [%rd176+3568];
	ld.global.f64 	%fd131, [%rd176+3560];
	ld.global.f64 	%fd132, [%rd176+3552];
	ld.global.f64 	%fd133, [%rd176+3544];
	ld.global.f64 	%fd134, [%rd176+3536];
	ld.global.f64 	%fd135, [%rd176+3528];
	ld.global.f64 	%fd136, [%rd176+3520];
	ld.global.f64 	%fd137, [%rd176+3512];
	ld.global.f64 	%fd138, [%rd176+3504];
	ld.global.f64 	%fd139, [%rd176+3496];
	ld.global.f64 	%fd140, [%rd176+3488];
	ld.global.f64 	%fd141, [%rd176+3480];
	ld.global.f64 	%fd142, [%rd176+3472];
	ld.global.f64 	%fd143, [%rd176+3464];
	ld.global.f64 	%fd144, [%rd176+3456];
	ld.global.f64 	%fd145, [%rd176+3448];
	ld.global.f64 	%fd146, [%rd176+3440];
	ld.global.f64 	%fd147, [%rd176+3432];
	ld.global.f64 	%fd148, [%rd176+3424];
	ld.global.f64 	%fd149, [%rd176+3416];
	ld.global.f64 	%fd150, [%rd176+3408];
	ld.global.f64 	%fd151, [%rd176+3400];
	ld.global.f64 	%fd152, [%rd176+3392];
	ld.global.f64 	%fd153, [%rd176+3384];
	ld.global.f64 	%fd154, [%rd176+3376];
	ld.global.f64 	%fd155, [%rd176+3368];
	ld.global.f64 	%fd156, [%rd176+3360];
	ld.global.f64 	%fd157, [%rd176+3352];
	ld.global.f64 	%fd158, [%rd176+3344];
	ld.global.f64 	%fd159, [%rd176+3336];
	ld.global.f64 	%fd160, [%rd176+3328];
	ld.global.f64 	%fd161, [%rd176+3320];
	ld.global.f64 	%fd162, [%rd176+3312];
	ld.global.f64 	%fd163, [%rd176+3304];
	ld.global.f64 	%fd164, [%rd176+3296];
	ld.global.f64 	%fd165, [%rd176+3288];
	ld.global.f64 	%fd166, [%rd176+3280];
	ld.global.f64 	%fd167, [%rd176+3272];
	ld.global.f64 	%fd168, [%rd176+3264];
	ld.global.f64 	%fd169, [%rd176+3256];
	ld.global.f64 	%fd170, [%rd176+3248];
	ld.global.f64 	%fd171, [%rd176+3240];
	ld.global.f64 	%fd172, [%rd176+3232];
	ld.global.f64 	%fd173, [%rd176+3224];
	ld.global.f64 	%fd174, [%rd176+3216];
	ld.global.f64 	%fd175, [%rd176+3208];
	ld.global.f64 	%fd176, [%rd176+3200];
	ld.global.f64 	%fd177, [%rd176+3192];
	ld.global.f64 	%fd178, [%rd176+3184];
	ld.global.f64 	%fd179, [%rd176+3176];
	ld.global.f64 	%fd180, [%rd176+3168];
	ld.global.f64 	%fd181, [%rd176+3160];
	ld.global.f64 	%fd182, [%rd176+3152];
	ld.global.f64 	%fd183, [%rd176+3144];
	ld.global.f64 	%fd184, [%rd176+3136];
	ld.global.f64 	%fd185, [%rd176+3128];
	ld.global.f64 	%fd186, [%rd176+3120];
	ld.global.f64 	%fd187, [%rd176+3112];
	ld.global.f64 	%fd188, [%rd176+3104];
	ld.global.f64 	%fd189, [%rd176+3096];
	ld.global.f64 	%fd190, [%rd176+3088];
	ld.global.f64 	%fd191, [%rd176+3080];
	ld.global.f64 	%fd192, [%rd176+3072];
	ld.global.f64 	%fd193, [%rd176+3064];
	ld.global.f64 	%fd194, [%rd176+3056];
	ld.global.f64 	%fd195, [%rd176+3048];
	ld.global.f64 	%fd196, [%rd176+3040];
	ld.global.f64 	%fd197, [%rd176+3032];
	ld.global.f64 	%fd198, [%rd176+3024];
	ld.global.f64 	%fd199, [%rd176+3016];
	ld.global.f64 	%fd200, [%rd176+3008];
	ld.global.f64 	%fd201, [%rd176+3000];
	ld.global.f64 	%fd202, [%rd176+2992];
	ld.global.f64 	%fd203, [%rd176+2984];
	ld.global.f64 	%fd204, [%rd176+2976];
	ld.global.f64 	%fd205, [%rd176+2968];
	ld.global.f64 	%fd206, [%rd176+2960];
	ld.global.f64 	%fd207, [%rd176+2952];
	ld.global.f64 	%fd208, [%rd176+2944];
	ld.global.f64 	%fd209, [%rd176+2936];
	ld.global.f64 	%fd210, [%rd176+2928];
	ld.global.f64 	%fd211, [%rd176+2920];
	ld.global.f64 	%fd212, [%rd176+2912];
	ld.global.f64 	%fd213, [%rd176+2904];
	ld.global.f64 	%fd214, [%rd176+2896];
	ld.global.f64 	%fd215, [%rd176+2888];
	ld.global.f64 	%fd216, [%rd176+2880];
	ld.global.f64 	%fd217, [%rd176+2872];
	ld.global.f64 	%fd218, [%rd176+2864];
	ld.global.f64 	%fd219, [%rd176+2856];
	ld.global.f64 	%fd220, [%rd176+2848];
	ld.global.f64 	%fd221, [%rd176+2840];
	ld.global.f64 	%fd222, [%rd176+2832];
	ld.global.f64 	%fd223, [%rd176+2824];
	ld.global.f64 	%fd224, [%rd176+2816];
	ld.global.f64 	%fd225, [%rd176+2808];
	ld.global.f64 	%fd226, [%rd176+2800];
	ld.global.f64 	%fd227, [%rd176+2792];
	ld.global.f64 	%fd228, [%rd176+2784];
	ld.global.f64 	%fd229, [%rd176+2776];
	ld.global.f64 	%fd230, [%rd176+2768];
	ld.global.f64 	%fd231, [%rd176+2760];
	ld.global.f64 	%fd232, [%rd176+2752];
	ld.global.f64 	%fd233, [%rd176+2744];
	ld.global.f64 	%fd234, [%rd176+2736];
	ld.global.f64 	%fd235, [%rd176+2728];
	ld.global.f64 	%fd236, [%rd176+2720];
	ld.global.f64 	%fd237, [%rd176+2712];
	ld.global.f64 	%fd238, [%rd176+2704];
	ld.global.f64 	%fd239, [%rd176+2696];
	ld.global.f64 	%fd240, [%rd176+2688];
	ld.global.f64 	%fd241, [%rd176+2680];
	ld.global.f64 	%fd242, [%rd176+2672];
	ld.global.f64 	%fd243, [%rd176+2664];
	ld.global.f64 	%fd244, [%rd176+2656];
	ld.global.f64 	%fd245, [%rd176+2648];
	ld.global.f64 	%fd246, [%rd176+2640];
	ld.global.f64 	%fd247, [%rd176+2632];
	ld.global.f64 	%fd248, [%rd176+2624];
	ld.global.f64 	%fd249, [%rd176+2616];
	ld.global.f64 	%fd250, [%rd176+2608];
	ld.global.f64 	%fd251, [%rd176+2600];
	ld.global.f64 	%fd252, [%rd176+2592];
	ld.global.f64 	%fd253, [%rd176+2584];
	ld.global.f64 	%fd254, [%rd176+2576];
	ld.global.f64 	%fd255, [%rd176+2568];
	ld.global.f64 	%fd256, [%rd176+2560];
	ld.global.f64 	%fd257, [%rd176+2552];
	ld.global.f64 	%fd258, [%rd176+2544];
	ld.global.f64 	%fd259, [%rd176+2536];
	ld.global.f64 	%fd260, [%rd176+2528];
	ld.global.f64 	%fd261, [%rd176+2520];
	ld.global.f64 	%fd262, [%rd176+2512];
	ld.global.f64 	%fd263, [%rd176+2504];
	ld.global.f64 	%fd264, [%rd176+2496];
	ld.global.f64 	%fd265, [%rd176+2488];
	ld.global.f64 	%fd266, [%rd176+2480];
	ld.global.f64 	%fd267, [%rd176+2472];
	ld.global.f64 	%fd268, [%rd176+2464];
	ld.global.f64 	%fd269, [%rd176+2456];
	ld.global.f64 	%fd270, [%rd176+2448];
	ld.global.f64 	%fd271, [%rd176+2440];
	ld.global.f64 	%fd272, [%rd176+2432];
	ld.global.f64 	%fd273, [%rd176+2424];
	ld.global.f64 	%fd274, [%rd176+2416];
	ld.global.f64 	%fd275, [%rd176+2408];
	ld.global.f64 	%fd276, [%rd176+2400];
	ld.global.f64 	%fd277, [%rd176+2392];
	ld.global.f64 	%fd278, [%rd176+2384];
	ld.global.f64 	%fd279, [%rd176+2376];
	ld.global.f64 	%fd280, [%rd176+2368];
	ld.global.f64 	%fd281, [%rd176+2360];
	ld.global.f64 	%fd282, [%rd176+2352];
	ld.global.f64 	%fd283, [%rd176+2344];
	ld.global.f64 	%fd284, [%rd176+2336];
	ld.global.f64 	%fd285, [%rd176+2328];
	ld.global.f64 	%fd286, [%rd176+2320];
	ld.global.f64 	%fd287, [%rd176+2312];
	ld.global.f64 	%fd288, [%rd176+2304];
	ld.global.f64 	%fd289, [%rd176+2296];
	ld.global.f64 	%fd290, [%rd176+2288];
	ld.global.f64 	%fd291, [%rd176+2280];
	ld.global.f64 	%fd292, [%rd176+2272];
	ld.global.f64 	%fd293, [%rd176+2264];
	ld.global.f64 	%fd294, [%rd176+2256];
	ld.global.f64 	%fd295, [%rd176+2248];
	ld.global.f64 	%fd296, [%rd176+2240];
	ld.global.f64 	%fd297, [%rd176+2232];
	ld.global.f64 	%fd298, [%rd176+2224];
	ld.global.f64 	%fd299, [%rd176+2216];
	ld.global.f64 	%fd300, [%rd176+2208];
	ld.global.f64 	%fd301, [%rd176+2200];
	ld.global.f64 	%fd302, [%rd176+2192];
	ld.global.f64 	%fd303, [%rd176+2184];
	ld.global.f64 	%fd304, [%rd176+2176];
	ld.global.f64 	%fd305, [%rd176+2168];
	ld.global.f64 	%fd306, [%rd176+2160];
	ld.global.f64 	%fd307, [%rd176+2152];
	ld.global.f64 	%fd308, [%rd176+2144];
	ld.global.f64 	%fd309, [%rd176+2136];
	ld.global.f64 	%fd310, [%rd176+2128];
	ld.global.f64 	%fd311, [%rd176+2120];
	ld.global.f64 	%fd312, [%rd176+2112];
	ld.global.f64 	%fd313, [%rd176+2104];
	ld.global.f64 	%fd314, [%rd176+2096];
	ld.global.f64 	%fd315, [%rd176+2088];
	ld.global.f64 	%fd316, [%rd176+2080];
	ld.global.f64 	%fd317, [%rd176+2072];
	ld.global.f64 	%fd318, [%rd176+2064];
	ld.global.f64 	%fd319, [%rd176+2056];
	ld.global.f64 	%fd320, [%rd176+2048];
	ld.global.f64 	%fd321, [%rd176+2040];
	ld.global.f64 	%fd322, [%rd176+2032];
	ld.global.f64 	%fd323, [%rd176+2024];
	ld.global.f64 	%fd324, [%rd176+2016];
	ld.global.f64 	%fd325, [%rd176+2008];
	ld.global.f64 	%fd326, [%rd176+2000];
	ld.global.f64 	%fd327, [%rd176+1992];
	ld.global.f64 	%fd328, [%rd176+1984];
	ld.global.f64 	%fd329, [%rd176+1976];
	ld.global.f64 	%fd330, [%rd176+1968];
	ld.global.f64 	%fd331, [%rd176+1960];
	ld.global.f64 	%fd332, [%rd176+1952];
	ld.global.f64 	%fd333, [%rd176+1944];
	ld.global.f64 	%fd334, [%rd176+1936];
	ld.global.f64 	%fd335, [%rd176+1928];
	ld.global.f64 	%fd336, [%rd176+1920];
	ld.global.f64 	%fd337, [%rd176+1912];
	ld.global.f64 	%fd338, [%rd176+1904];
	ld.global.f64 	%fd339, [%rd176+1896];
	ld.global.f64 	%fd340, [%rd176+1888];
	ld.global.f64 	%fd341, [%rd176+1880];
	ld.global.f64 	%fd342, [%rd176+1872];
	ld.global.f64 	%fd343, [%rd176+1864];
	ld.global.f64 	%fd344, [%rd176+1856];
	ld.global.f64 	%fd345, [%rd176+1848];
	ld.global.f64 	%fd346, [%rd176+1840];
	ld.global.f64 	%fd347, [%rd176+1832];
	ld.global.f64 	%fd348, [%rd176+1824];
	ld.global.f64 	%fd349, [%rd176+1816];
	ld.global.f64 	%fd350, [%rd176+1808];
	ld.global.f64 	%fd351, [%rd176+1800];
	ld.global.f64 	%fd352, [%rd176+1792];
	ld.global.f64 	%fd353, [%rd176+1784];
	ld.global.f64 	%fd354, [%rd176+1776];
	ld.global.f64 	%fd355, [%rd176+1768];
	ld.global.f64 	%fd356, [%rd176+1760];
	ld.global.f64 	%fd357, [%rd176+1752];
	ld.global.f64 	%fd358, [%rd176+1744];
	ld.global.f64 	%fd359, [%rd176+1736];
	ld.global.f64 	%fd360, [%rd176+1728];
	ld.global.f64 	%fd361, [%rd176+1720];
	ld.global.f64 	%fd362, [%rd176+1712];
	ld.global.f64 	%fd363, [%rd176+1704];
	ld.global.f64 	%fd364, [%rd176+1696];
	ld.global.f64 	%fd365, [%rd176+1688];
	ld.global.f64 	%fd366, [%rd176+1680];
	ld.global.f64 	%fd367, [%rd176+1672];
	ld.global.f64 	%fd368, [%rd176+1664];
	ld.global.f64 	%fd369, [%rd176+1656];
	ld.global.f64 	%fd370, [%rd176+1648];
	ld.global.f64 	%fd371, [%rd176+1640];
	ld.global.f64 	%fd372, [%rd176+1632];
	ld.global.f64 	%fd373, [%rd176+1624];
	ld.global.f64 	%fd374, [%rd176+1616];
	ld.global.f64 	%fd375, [%rd176+1608];
	ld.global.f64 	%fd376, [%rd176+1600];
	ld.global.f64 	%fd377, [%rd176+1592];
	ld.global.f64 	%fd378, [%rd176+1584];
	ld.global.f64 	%fd379, [%rd176+1576];
	ld.global.f64 	%fd380, [%rd176+1568];
	ld.global.f64 	%fd381, [%rd176+1560];
	ld.global.f64 	%fd382, [%rd176+1552];
	ld.global.f64 	%fd383, [%rd176+1544];
	ld.global.f64 	%fd384, [%rd176+1536];
	ld.global.f64 	%fd385, [%rd176+1528];
	ld.global.f64 	%fd386, [%rd176+1520];
	ld.global.f64 	%fd387, [%rd176+1512];
	ld.global.f64 	%fd388, [%rd176+1504];
	ld.global.f64 	%fd389, [%rd176+1496];
	ld.global.f64 	%fd390, [%rd176+1488];
	ld.global.f64 	%fd391, [%rd176+1480];
	ld.global.f64 	%fd392, [%rd176+1472];
	ld.global.f64 	%fd393, [%rd176+1464];
	ld.global.f64 	%fd394, [%rd176+1456];
	ld.global.f64 	%fd395, [%rd176+1448];
	ld.global.f64 	%fd396, [%rd176+1440];
	ld.global.f64 	%fd397, [%rd176+1432];
	ld.global.f64 	%fd398, [%rd176+1424];
	ld.global.f64 	%fd399, [%rd176+1416];
	ld.global.f64 	%fd400, [%rd176+1408];
	ld.global.f64 	%fd401, [%rd176+1400];
	ld.global.f64 	%fd402, [%rd176+1392];
	ld.global.f64 	%fd403, [%rd176+1384];
	ld.global.f64 	%fd404, [%rd176+1376];
	ld.global.f64 	%fd405, [%rd176+1368];
	ld.global.f64 	%fd406, [%rd176+1360];
	ld.global.f64 	%fd407, [%rd176+1352];
	ld.global.f64 	%fd408, [%rd176+1344];
	ld.global.f64 	%fd409, [%rd176+1336];
	ld.global.f64 	%fd410, [%rd176+1328];
	ld.global.f64 	%fd411, [%rd176+1320];
	ld.global.f64 	%fd412, [%rd176+1312];
	ld.global.f64 	%fd413, [%rd176+1304];
	ld.global.f64 	%fd414, [%rd176+1296];
	ld.global.f64 	%fd415, [%rd176+1288];
	ld.global.f64 	%fd416, [%rd176+1280];
	ld.global.f64 	%fd417, [%rd176+1272];
	ld.global.f64 	%fd418, [%rd176+1264];
	ld.global.f64 	%fd419, [%rd176+1256];
	ld.global.f64 	%fd420, [%rd176+1248];
	ld.global.f64 	%fd421, [%rd176+1240];
	ld.global.f64 	%fd422, [%rd176+1232];
	ld.global.f64 	%fd423, [%rd176+1224];
	ld.global.f64 	%fd424, [%rd176+1216];
	ld.global.f64 	%fd425, [%rd176+1208];
	ld.global.f64 	%fd426, [%rd176+1200];
	ld.global.f64 	%fd427, [%rd176+1192];
	ld.global.f64 	%fd428, [%rd176+1184];
	ld.global.f64 	%fd429, [%rd176+1176];
	ld.global.f64 	%fd430, [%rd176+1168];
	ld.global.f64 	%fd431, [%rd176+1160];
	ld.global.f64 	%fd432, [%rd176+1152];
	ld.global.f64 	%fd433, [%rd176+1144];
	ld.global.f64 	%fd434, [%rd176+1136];
	ld.global.f64 	%fd435, [%rd176+1128];
	ld.global.f64 	%fd436, [%rd176+1120];
	ld.global.f64 	%fd437, [%rd176+1112];
	ld.global.f64 	%fd438, [%rd176+1104];
	ld.global.f64 	%fd439, [%rd176+1096];
	ld.global.f64 	%fd440, [%rd176+1088];
	ld.global.f64 	%fd441, [%rd176+1080];
	ld.global.f64 	%fd442, [%rd176+1072];
	ld.global.f64 	%fd443, [%rd176+1064];
	ld.global.f64 	%fd444, [%rd176+1056];
	ld.global.f64 	%fd445, [%rd176+1048];
	ld.global.f64 	%fd446, [%rd176+1040];
	ld.global.f64 	%fd447, [%rd176+1032];
	ld.global.f64 	%fd448, [%rd176+1024];
	ld.global.f64 	%fd449, [%rd176+1016];
	ld.global.f64 	%fd450, [%rd176+1008];
	ld.global.f64 	%fd451, [%rd176+1000];
	ld.global.f64 	%fd452, [%rd176+992];
	ld.global.f64 	%fd453, [%rd176+984];
	ld.global.f64 	%fd454, [%rd176+976];
	ld.global.f64 	%fd455, [%rd176+968];
	ld.global.f64 	%fd456, [%rd176+960];
	ld.global.f64 	%fd457, [%rd176+952];
	ld.global.f64 	%fd458, [%rd176+944];
	ld.global.f64 	%fd459, [%rd176+936];
	ld.global.f64 	%fd460, [%rd176+928];
	ld.global.f64 	%fd461, [%rd176+920];
	ld.global.f64 	%fd462, [%rd176+912];
	ld.global.f64 	%fd463, [%rd176+904];
	ld.global.f64 	%fd464, [%rd176+896];
	ld.global.f64 	%fd465, [%rd176+888];
	ld.global.f64 	%fd466, [%rd176+880];
	ld.global.f64 	%fd467, [%rd176+872];
	ld.global.f64 	%fd468, [%rd176+864];
	ld.global.f64 	%fd469, [%rd176+856];
	ld.global.f64 	%fd470, [%rd176+848];
	ld.global.f64 	%fd471, [%rd176+840];
	ld.global.f64 	%fd472, [%rd176+832];
	ld.global.f64 	%fd473, [%rd176+824];
	ld.global.f64 	%fd474, [%rd176+816];
	ld.global.f64 	%fd475, [%rd176+808];
	ld.global.f64 	%fd476, [%rd176+800];
	ld.global.f64 	%fd477, [%rd176+792];
	ld.global.f64 	%fd478, [%rd176+784];
	ld.global.f64 	%fd479, [%rd176+776];
	ld.global.f64 	%fd480, [%rd176+768];
	ld.global.f64 	%fd481, [%rd176+760];
	ld.global.f64 	%fd482, [%rd176+752];
	ld.global.f64 	%fd483, [%rd176+744];
	ld.global.f64 	%fd484, [%rd176+736];
	ld.global.f64 	%fd485, [%rd176+728];
	ld.global.f64 	%fd486, [%rd176+720];
	ld.global.f64 	%fd487, [%rd176+712];
	ld.global.f64 	%fd488, [%rd176+704];
	ld.global.f64 	%fd489, [%rd176+696];
	ld.global.f64 	%fd490, [%rd176+688];
	ld.global.f64 	%fd491, [%rd176+680];
	ld.global.f64 	%fd492, [%rd176+672];
	ld.global.f64 	%fd493, [%rd176+664];
	ld.global.f64 	%fd494, [%rd176+656];
	ld.global.f64 	%fd495, [%rd176+648];
	ld.global.f64 	%fd496, [%rd176+640];
	ld.global.f64 	%fd497, [%rd176+632];
	ld.global.f64 	%fd498, [%rd176+624];
	ld.global.f64 	%fd499, [%rd176+616];
	ld.global.f64 	%fd500, [%rd176+608];
	ld.global.f64 	%fd501, [%rd176+600];
	ld.global.f64 	%fd502, [%rd176+592];
	ld.global.f64 	%fd503, [%rd176+584];
	ld.global.f64 	%fd504, [%rd176+576];
	ld.global.f64 	%fd505, [%rd176+568];
	ld.global.f64 	%fd506, [%rd176+560];
	ld.global.f64 	%fd507, [%rd176+552];
	ld.global.f64 	%fd508, [%rd176+544];
	ld.global.f64 	%fd509, [%rd176+536];
	ld.global.f64 	%fd510, [%rd176+528];
	ld.global.f64 	%fd511, [%rd176+520];
	ld.global.f64 	%fd512, [%rd176+512];
	ld.global.f64 	%fd513, [%rd176+504];
	ld.global.f64 	%fd514, [%rd176+496];
	ld.global.f64 	%fd515, [%rd176+488];
	ld.global.f64 	%fd516, [%rd176+480];
	ld.global.f64 	%fd517, [%rd176+472];
	ld.global.f64 	%fd518, [%rd176+464];
	ld.global.f64 	%fd519, [%rd176+456];
	ld.global.f64 	%fd520, [%rd176+448];
	ld.global.f64 	%fd521, [%rd176+440];
	ld.global.f64 	%fd522, [%rd176+432];
	ld.global.f64 	%fd523, [%rd176+424];
	ld.global.f64 	%fd524, [%rd176+416];
	ld.global.f64 	%fd525, [%rd176+408];
	ld.global.f64 	%fd526, [%rd176+400];
	ld.global.f64 	%fd527, [%rd176+392];
	ld.global.f64 	%fd528, [%rd176+384];
	ld.global.f64 	%fd529, [%rd176+376];
	ld.global.f64 	%fd530, [%rd176+368];
	ld.global.f64 	%fd531, [%rd176+360];
	ld.global.f64 	%fd532, [%rd176+352];
	ld.global.f64 	%fd533, [%rd176+344];
	ld.global.f64 	%fd534, [%rd176+336];
	ld.global.f64 	%fd535, [%rd176+328];
	ld.global.f64 	%fd536, [%rd176+320];
	ld.global.f64 	%fd537, [%rd176+312];
	ld.global.f64 	%fd538, [%rd176+304];
	ld.global.f64 	%fd539, [%rd176+296];
	ld.global.f64 	%fd540, [%rd176+288];
	ld.global.f64 	%fd541, [%rd176+280];
	ld.global.f64 	%fd542, [%rd176+272];
	ld.global.f64 	%fd543, [%rd176+264];
	ld.global.f64 	%fd544, [%rd176+256];
	ld.global.f64 	%fd545, [%rd176+248];
	ld.global.f64 	%fd546, [%rd176+240];
	ld.global.f64 	%fd547, [%rd176+232];
	ld.global.f64 	%fd548, [%rd176+224];
	ld.global.f64 	%fd549, [%rd176+216];
	ld.global.f64 	%fd550, [%rd176+208];
	ld.global.f64 	%fd551, [%rd176+200];
	ld.global.f64 	%fd552, [%rd176+192];
	ld.global.f64 	%fd553, [%rd176+184];
	ld.global.f64 	%fd554, [%rd176+176];
	ld.global.f64 	%fd555, [%rd176+168];
	ld.global.f64 	%fd556, [%rd176+160];
	ld.global.f64 	%fd557, [%rd176+152];
	ld.global.f64 	%fd558, [%rd176+144];
	ld.global.f64 	%fd559, [%rd176+136];
	ld.global.f64 	%fd560, [%rd176+128];
	ld.global.f64 	%fd561, [%rd176+120];
	ld.global.f64 	%fd562, [%rd176+112];
	ld.global.f64 	%fd563, [%rd176+104];
	ld.global.f64 	%fd564, [%rd176+96];
	ld.global.f64 	%fd565, [%rd176+88];
	ld.global.f64 	%fd566, [%rd176+80];
	ld.global.f64 	%fd567, [%rd176+72];
	ld.global.f64 	%fd568, [%rd176+64];
	ld.global.f64 	%fd569, [%rd176+56];
	ld.global.f64 	%fd570, [%rd176+48];
	ld.global.f64 	%fd571, [%rd176+40];
	ld.global.f64 	%fd572, [%rd176+32];
	ld.global.f64 	%fd573, [%rd176+24];
	ld.global.f64 	%fd574, [%rd176+16];
	ld.global.f64 	%fd575, [%rd176+8];
	ld.global.f64 	%fd576, [%rd176];
	mul.lo.s64 	%rd177, %rd174, %rd10;
	add.s64 	%rd22, %rd5, %rd177;
	ld.global.u32 	%r445, [%rd22];
	shl.b32 	%r3, %r445, 4;
	add.s64 	%rd23, %rd1210, 112;
	ld.param.u32 	%r5, [%rd1210+172];
	setp.le.s32 	%p10, %r5, %r3;
	selp.u16 	%rs25, 1, 0, %p10;
	shr.u32 	%r446, %r445, 27;
	cvt.u16.u32 	%rs26, %r446;
	and.b16  	%rs27, %rs26, 1;
	or.b16  	%rs28, %rs27, %rs25;
	setp.eq.s16 	%p11, %rs28, 0;
	add.u64 	%rd179, %SP, 0;
	add.u64 	%rd25, %SPL, 0;
	@%p11 bra 	$L__BB0_17;

	st.local.v2.u32 	[%rd25], {%r3, %r5};
	mov.u64 	%rd180, $str;
	cvta.global.u64 	%rd181, %rd180;
	{ // callseq 252, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd181;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r447, [retval0+0];
	} // callseq 252
	bra.uni 	$L__BB0_18;

$L__BB0_17:
	ld.param.u64 	%rd1213, [%rd1210+112];
	mov.b64 	%rd1212, assemble_matrix_cuda_kernel_forward_param_1;
	ld.param.u32 	%r821, [%rd1210+144];
	mul.wide.s32 	%rd192, %r821, %r3;
	add.s64 	%rd183, %rd1213, %rd192;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd183],%fd576; }

	// end inline asm
	add.s64 	%rd184, %rd183, 8;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd184],%fd575; }

	// end inline asm
	add.s64 	%rd185, %rd183, 16;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd185],%fd574; }

	// end inline asm
	add.s64 	%rd186, %rd183, 24;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd186],%fd552; }

	// end inline asm
	add.s64 	%rd187, %rd183, 32;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd187],%fd551; }

	// end inline asm
	add.s64 	%rd188, %rd183, 40;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd188],%fd550; }

	// end inline asm
	add.s64 	%rd189, %rd183, 48;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd189],%fd528; }

	// end inline asm
	add.s64 	%rd190, %rd183, 56;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd190],%fd527; }

	// end inline asm
	add.s64 	%rd191, %rd183, 64;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd191],%fd526; }

	// end inline asm

$L__BB0_18:
	ld.param.u64 	%rd26, [%rd23];
	ld.param.u32 	%r6, [%rd23+32];
	ld.param.u32 	%r7, [%rd23+60];
	add.s32 	%r8, %r3, 1;
	setp.le.s32 	%p12, %r7, %r8;
	selp.u16 	%rs29, 1, 0, %p12;
	shr.u32 	%r448, %r8, 31;
	cvt.u16.u32 	%rs30, %r448;
	or.b16  	%rs31, %rs29, %rs30;
	setp.eq.s16 	%p13, %rs31, 0;
	@%p13 bra 	$L__BB0_20;

	add.s32 	%r678, %r3, 1;
	st.local.v2.u32 	[%rd25], {%r678, %r7};
	mov.u64 	%rd193, $str;
	cvta.global.u64 	%rd194, %rd193;
	{ // callseq 253, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd194;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r449, [retval0+0];
	} // callseq 253
	bra.uni 	$L__BB0_21;

$L__BB0_20:
	mul.wide.s32 	%rd205, %r6, %r8;
	add.s64 	%rd196, %rd26, %rd205;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd196],%fd573; }

	// end inline asm
	add.s64 	%rd197, %rd196, 8;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd197],%fd572; }

	// end inline asm
	add.s64 	%rd198, %rd196, 16;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd198],%fd571; }

	// end inline asm
	add.s64 	%rd199, %rd196, 24;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd199],%fd549; }

	// end inline asm
	add.s64 	%rd200, %rd196, 32;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd200],%fd548; }

	// end inline asm
	add.s64 	%rd201, %rd196, 40;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd201],%fd547; }

	// end inline asm
	add.s64 	%rd202, %rd196, 48;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd202],%fd525; }

	// end inline asm
	add.s64 	%rd203, %rd196, 56;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd203],%fd524; }

	// end inline asm
	add.s64 	%rd204, %rd196, 64;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd204],%fd523; }

	// end inline asm

$L__BB0_21:
	ld.param.u64 	%rd27, [%rd23];
	ld.param.u32 	%r9, [%rd23+32];
	ld.param.u32 	%r10, [%rd23+60];
	add.s32 	%r11, %r3, 2;
	setp.le.s32 	%p14, %r10, %r11;
	selp.u16 	%rs32, 1, 0, %p14;
	shr.u32 	%r450, %r11, 31;
	cvt.u16.u32 	%rs33, %r450;
	or.b16  	%rs34, %rs32, %rs33;
	setp.eq.s16 	%p15, %rs34, 0;
	@%p15 bra 	$L__BB0_23;

	add.s32 	%r679, %r3, 2;
	st.local.v2.u32 	[%rd25], {%r679, %r10};
	mov.u64 	%rd206, $str;
	cvta.global.u64 	%rd207, %rd206;
	{ // callseq 254, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd207;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r451, [retval0+0];
	} // callseq 254
	bra.uni 	$L__BB0_24;

$L__BB0_23:
	mul.wide.s32 	%rd218, %r9, %r11;
	add.s64 	%rd209, %rd27, %rd218;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd209],%fd570; }

	// end inline asm
	add.s64 	%rd210, %rd209, 8;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd210],%fd569; }

	// end inline asm
	add.s64 	%rd211, %rd209, 16;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd211],%fd568; }

	// end inline asm
	add.s64 	%rd212, %rd209, 24;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd212],%fd546; }

	// end inline asm
	add.s64 	%rd213, %rd209, 32;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd213],%fd545; }

	// end inline asm
	add.s64 	%rd214, %rd209, 40;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd214],%fd544; }

	// end inline asm
	add.s64 	%rd215, %rd209, 48;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd215],%fd522; }

	// end inline asm
	add.s64 	%rd216, %rd209, 56;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd216],%fd521; }

	// end inline asm
	add.s64 	%rd217, %rd209, 64;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd217],%fd520; }

	// end inline asm

$L__BB0_24:
	ld.param.u64 	%rd28, [%rd23];
	ld.param.u32 	%r12, [%rd23+32];
	ld.param.u32 	%r13, [%rd23+60];
	add.s32 	%r14, %r3, 3;
	setp.le.s32 	%p16, %r13, %r14;
	selp.u16 	%rs35, 1, 0, %p16;
	shr.u32 	%r452, %r14, 31;
	cvt.u16.u32 	%rs36, %r452;
	or.b16  	%rs37, %rs35, %rs36;
	setp.eq.s16 	%p17, %rs37, 0;
	@%p17 bra 	$L__BB0_26;

	add.s32 	%r680, %r3, 3;
	st.local.v2.u32 	[%rd25], {%r680, %r13};
	mov.u64 	%rd219, $str;
	cvta.global.u64 	%rd220, %rd219;
	{ // callseq 255, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd220;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r453, [retval0+0];
	} // callseq 255
	bra.uni 	$L__BB0_27;

$L__BB0_26:
	mul.wide.s32 	%rd231, %r12, %r14;
	add.s64 	%rd222, %rd28, %rd231;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd222],%fd567; }

	// end inline asm
	add.s64 	%rd223, %rd222, 8;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd223],%fd566; }

	// end inline asm
	add.s64 	%rd224, %rd222, 16;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd224],%fd565; }

	// end inline asm
	add.s64 	%rd225, %rd222, 24;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd225],%fd543; }

	// end inline asm
	add.s64 	%rd226, %rd222, 32;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd226],%fd542; }

	// end inline asm
	add.s64 	%rd227, %rd222, 40;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd227],%fd541; }

	// end inline asm
	add.s64 	%rd228, %rd222, 48;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd228],%fd519; }

	// end inline asm
	add.s64 	%rd229, %rd222, 56;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd229],%fd518; }

	// end inline asm
	add.s64 	%rd230, %rd222, 64;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd230],%fd517; }

	// end inline asm

$L__BB0_27:
	ld.param.u64 	%rd29, [%rd23];
	ld.param.u32 	%r15, [%rd23+32];
	ld.param.u32 	%r16, [%rd23+60];
	add.s32 	%r17, %r3, 4;
	setp.le.s32 	%p18, %r16, %r17;
	selp.u16 	%rs38, 1, 0, %p18;
	shr.u32 	%r454, %r17, 31;
	cvt.u16.u32 	%rs39, %r454;
	or.b16  	%rs40, %rs38, %rs39;
	setp.eq.s16 	%p19, %rs40, 0;
	@%p19 bra 	$L__BB0_29;

	add.s32 	%r681, %r3, 4;
	st.local.v2.u32 	[%rd25], {%r681, %r16};
	mov.u64 	%rd232, $str;
	cvta.global.u64 	%rd233, %rd232;
	{ // callseq 256, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd233;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r455, [retval0+0];
	} // callseq 256
	bra.uni 	$L__BB0_30;

$L__BB0_29:
	mul.wide.s32 	%rd244, %r15, %r17;
	add.s64 	%rd235, %rd29, %rd244;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd235],%fd504; }

	// end inline asm
	add.s64 	%rd236, %rd235, 8;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd236],%fd503; }

	// end inline asm
	add.s64 	%rd237, %rd235, 16;
	// begin inline asm
	{ atom.add.f64 %fd653,[%rd237],%fd502; }

	// end inline asm
	add.s64 	%rd238, %rd235, 24;
	// begin inline asm
	{ atom.add.f64 %fd655,[%rd238],%fd480; }

	// end inline asm
	add.s64 	%rd239, %rd235, 32;
	// begin inline asm
	{ atom.add.f64 %fd657,[%rd239],%fd479; }

	// end inline asm
	add.s64 	%rd240, %rd235, 40;
	// begin inline asm
	{ atom.add.f64 %fd659,[%rd240],%fd478; }

	// end inline asm
	add.s64 	%rd241, %rd235, 48;
	// begin inline asm
	{ atom.add.f64 %fd661,[%rd241],%fd456; }

	// end inline asm
	add.s64 	%rd242, %rd235, 56;
	// begin inline asm
	{ atom.add.f64 %fd663,[%rd242],%fd455; }

	// end inline asm
	add.s64 	%rd243, %rd235, 64;
	// begin inline asm
	{ atom.add.f64 %fd665,[%rd243],%fd454; }

	// end inline asm

$L__BB0_30:
	ld.param.u64 	%rd30, [%rd23];
	ld.param.u32 	%r18, [%rd23+32];
	ld.param.u32 	%r19, [%rd23+60];
	add.s32 	%r20, %r3, 5;
	setp.le.s32 	%p20, %r19, %r20;
	selp.u16 	%rs41, 1, 0, %p20;
	shr.u32 	%r456, %r20, 31;
	cvt.u16.u32 	%rs42, %r456;
	or.b16  	%rs43, %rs41, %rs42;
	setp.eq.s16 	%p21, %rs43, 0;
	@%p21 bra 	$L__BB0_32;

	add.s32 	%r682, %r3, 5;
	st.local.v2.u32 	[%rd25], {%r682, %r19};
	mov.u64 	%rd245, $str;
	cvta.global.u64 	%rd246, %rd245;
	{ // callseq 257, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd246;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r457, [retval0+0];
	} // callseq 257
	bra.uni 	$L__BB0_33;

$L__BB0_32:
	mul.wide.s32 	%rd257, %r18, %r20;
	add.s64 	%rd248, %rd30, %rd257;
	// begin inline asm
	{ atom.add.f64 %fd667,[%rd248],%fd501; }

	// end inline asm
	add.s64 	%rd249, %rd248, 8;
	// begin inline asm
	{ atom.add.f64 %fd669,[%rd249],%fd500; }

	// end inline asm
	add.s64 	%rd250, %rd248, 16;
	// begin inline asm
	{ atom.add.f64 %fd671,[%rd250],%fd499; }

	// end inline asm
	add.s64 	%rd251, %rd248, 24;
	// begin inline asm
	{ atom.add.f64 %fd673,[%rd251],%fd477; }

	// end inline asm
	add.s64 	%rd252, %rd248, 32;
	// begin inline asm
	{ atom.add.f64 %fd675,[%rd252],%fd476; }

	// end inline asm
	add.s64 	%rd253, %rd248, 40;
	// begin inline asm
	{ atom.add.f64 %fd677,[%rd253],%fd475; }

	// end inline asm
	add.s64 	%rd254, %rd248, 48;
	// begin inline asm
	{ atom.add.f64 %fd679,[%rd254],%fd453; }

	// end inline asm
	add.s64 	%rd255, %rd248, 56;
	// begin inline asm
	{ atom.add.f64 %fd681,[%rd255],%fd452; }

	// end inline asm
	add.s64 	%rd256, %rd248, 64;
	// begin inline asm
	{ atom.add.f64 %fd683,[%rd256],%fd451; }

	// end inline asm

$L__BB0_33:
	ld.param.u64 	%rd31, [%rd23];
	ld.param.u32 	%r21, [%rd23+32];
	ld.param.u32 	%r22, [%rd23+60];
	add.s32 	%r23, %r3, 6;
	setp.le.s32 	%p22, %r22, %r23;
	selp.u16 	%rs44, 1, 0, %p22;
	shr.u32 	%r458, %r23, 31;
	cvt.u16.u32 	%rs45, %r458;
	or.b16  	%rs46, %rs44, %rs45;
	setp.eq.s16 	%p23, %rs46, 0;
	@%p23 bra 	$L__BB0_35;

	add.s32 	%r683, %r3, 6;
	st.local.v2.u32 	[%rd25], {%r683, %r22};
	mov.u64 	%rd258, $str;
	cvta.global.u64 	%rd259, %rd258;
	{ // callseq 258, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd259;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r459, [retval0+0];
	} // callseq 258
	bra.uni 	$L__BB0_36;

$L__BB0_35:
	mul.wide.s32 	%rd270, %r21, %r23;
	add.s64 	%rd261, %rd31, %rd270;
	// begin inline asm
	{ atom.add.f64 %fd685,[%rd261],%fd498; }

	// end inline asm
	add.s64 	%rd262, %rd261, 8;
	// begin inline asm
	{ atom.add.f64 %fd687,[%rd262],%fd497; }

	// end inline asm
	add.s64 	%rd263, %rd261, 16;
	// begin inline asm
	{ atom.add.f64 %fd689,[%rd263],%fd496; }

	// end inline asm
	add.s64 	%rd264, %rd261, 24;
	// begin inline asm
	{ atom.add.f64 %fd691,[%rd264],%fd474; }

	// end inline asm
	add.s64 	%rd265, %rd261, 32;
	// begin inline asm
	{ atom.add.f64 %fd693,[%rd265],%fd473; }

	// end inline asm
	add.s64 	%rd266, %rd261, 40;
	// begin inline asm
	{ atom.add.f64 %fd695,[%rd266],%fd472; }

	// end inline asm
	add.s64 	%rd267, %rd261, 48;
	// begin inline asm
	{ atom.add.f64 %fd697,[%rd267],%fd450; }

	// end inline asm
	add.s64 	%rd268, %rd261, 56;
	// begin inline asm
	{ atom.add.f64 %fd699,[%rd268],%fd449; }

	// end inline asm
	add.s64 	%rd269, %rd261, 64;
	// begin inline asm
	{ atom.add.f64 %fd701,[%rd269],%fd448; }

	// end inline asm

$L__BB0_36:
	ld.param.u64 	%rd32, [%rd23];
	ld.param.u32 	%r24, [%rd23+32];
	ld.param.u32 	%r25, [%rd23+60];
	add.s32 	%r26, %r3, 7;
	setp.le.s32 	%p24, %r25, %r26;
	selp.u16 	%rs47, 1, 0, %p24;
	shr.u32 	%r460, %r26, 31;
	cvt.u16.u32 	%rs48, %r460;
	or.b16  	%rs49, %rs47, %rs48;
	setp.eq.s16 	%p25, %rs49, 0;
	@%p25 bra 	$L__BB0_38;

	add.s32 	%r684, %r3, 7;
	st.local.v2.u32 	[%rd25], {%r684, %r25};
	mov.u64 	%rd271, $str;
	cvta.global.u64 	%rd272, %rd271;
	{ // callseq 259, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd272;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r461, [retval0+0];
	} // callseq 259
	bra.uni 	$L__BB0_39;

$L__BB0_38:
	mul.wide.s32 	%rd283, %r24, %r26;
	add.s64 	%rd274, %rd32, %rd283;
	// begin inline asm
	{ atom.add.f64 %fd703,[%rd274],%fd495; }

	// end inline asm
	add.s64 	%rd275, %rd274, 8;
	// begin inline asm
	{ atom.add.f64 %fd705,[%rd275],%fd494; }

	// end inline asm
	add.s64 	%rd276, %rd274, 16;
	// begin inline asm
	{ atom.add.f64 %fd707,[%rd276],%fd493; }

	// end inline asm
	add.s64 	%rd277, %rd274, 24;
	// begin inline asm
	{ atom.add.f64 %fd709,[%rd277],%fd471; }

	// end inline asm
	add.s64 	%rd278, %rd274, 32;
	// begin inline asm
	{ atom.add.f64 %fd711,[%rd278],%fd470; }

	// end inline asm
	add.s64 	%rd279, %rd274, 40;
	// begin inline asm
	{ atom.add.f64 %fd713,[%rd279],%fd469; }

	// end inline asm
	add.s64 	%rd280, %rd274, 48;
	// begin inline asm
	{ atom.add.f64 %fd715,[%rd280],%fd447; }

	// end inline asm
	add.s64 	%rd281, %rd274, 56;
	// begin inline asm
	{ atom.add.f64 %fd717,[%rd281],%fd446; }

	// end inline asm
	add.s64 	%rd282, %rd274, 64;
	// begin inline asm
	{ atom.add.f64 %fd719,[%rd282],%fd445; }

	// end inline asm

$L__BB0_39:
	ld.param.u64 	%rd33, [%rd23];
	ld.param.u32 	%r27, [%rd23+32];
	ld.param.u32 	%r28, [%rd23+60];
	add.s32 	%r29, %r3, 8;
	setp.le.s32 	%p26, %r28, %r29;
	selp.u16 	%rs50, 1, 0, %p26;
	shr.u32 	%r462, %r29, 31;
	cvt.u16.u32 	%rs51, %r462;
	or.b16  	%rs52, %rs50, %rs51;
	setp.eq.s16 	%p27, %rs52, 0;
	@%p27 bra 	$L__BB0_41;

	add.s32 	%r685, %r3, 8;
	st.local.v2.u32 	[%rd25], {%r685, %r28};
	mov.u64 	%rd284, $str;
	cvta.global.u64 	%rd285, %rd284;
	{ // callseq 260, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd285;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r463, [retval0+0];
	} // callseq 260
	bra.uni 	$L__BB0_42;

$L__BB0_41:
	mul.wide.s32 	%rd296, %r27, %r29;
	add.s64 	%rd287, %rd33, %rd296;
	// begin inline asm
	{ atom.add.f64 %fd721,[%rd287],%fd432; }

	// end inline asm
	add.s64 	%rd288, %rd287, 8;
	// begin inline asm
	{ atom.add.f64 %fd723,[%rd288],%fd431; }

	// end inline asm
	add.s64 	%rd289, %rd287, 16;
	// begin inline asm
	{ atom.add.f64 %fd725,[%rd289],%fd430; }

	// end inline asm
	add.s64 	%rd290, %rd287, 24;
	// begin inline asm
	{ atom.add.f64 %fd727,[%rd290],%fd408; }

	// end inline asm
	add.s64 	%rd291, %rd287, 32;
	// begin inline asm
	{ atom.add.f64 %fd729,[%rd291],%fd407; }

	// end inline asm
	add.s64 	%rd292, %rd287, 40;
	// begin inline asm
	{ atom.add.f64 %fd731,[%rd292],%fd406; }

	// end inline asm
	add.s64 	%rd293, %rd287, 48;
	// begin inline asm
	{ atom.add.f64 %fd733,[%rd293],%fd384; }

	// end inline asm
	add.s64 	%rd294, %rd287, 56;
	// begin inline asm
	{ atom.add.f64 %fd735,[%rd294],%fd383; }

	// end inline asm
	add.s64 	%rd295, %rd287, 64;
	// begin inline asm
	{ atom.add.f64 %fd737,[%rd295],%fd382; }

	// end inline asm

$L__BB0_42:
	ld.param.u64 	%rd34, [%rd23];
	ld.param.u32 	%r30, [%rd23+32];
	ld.param.u32 	%r31, [%rd23+60];
	add.s32 	%r32, %r3, 9;
	setp.le.s32 	%p28, %r31, %r32;
	selp.u16 	%rs53, 1, 0, %p28;
	shr.u32 	%r464, %r32, 31;
	cvt.u16.u32 	%rs54, %r464;
	or.b16  	%rs55, %rs53, %rs54;
	setp.eq.s16 	%p29, %rs55, 0;
	@%p29 bra 	$L__BB0_44;

	add.s32 	%r686, %r3, 9;
	st.local.v2.u32 	[%rd25], {%r686, %r31};
	mov.u64 	%rd297, $str;
	cvta.global.u64 	%rd298, %rd297;
	{ // callseq 261, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd298;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r465, [retval0+0];
	} // callseq 261
	bra.uni 	$L__BB0_45;

$L__BB0_44:
	mul.wide.s32 	%rd309, %r30, %r32;
	add.s64 	%rd300, %rd34, %rd309;
	// begin inline asm
	{ atom.add.f64 %fd739,[%rd300],%fd429; }

	// end inline asm
	add.s64 	%rd301, %rd300, 8;
	// begin inline asm
	{ atom.add.f64 %fd741,[%rd301],%fd428; }

	// end inline asm
	add.s64 	%rd302, %rd300, 16;
	// begin inline asm
	{ atom.add.f64 %fd743,[%rd302],%fd427; }

	// end inline asm
	add.s64 	%rd303, %rd300, 24;
	// begin inline asm
	{ atom.add.f64 %fd745,[%rd303],%fd405; }

	// end inline asm
	add.s64 	%rd304, %rd300, 32;
	// begin inline asm
	{ atom.add.f64 %fd747,[%rd304],%fd404; }

	// end inline asm
	add.s64 	%rd305, %rd300, 40;
	// begin inline asm
	{ atom.add.f64 %fd749,[%rd305],%fd403; }

	// end inline asm
	add.s64 	%rd306, %rd300, 48;
	// begin inline asm
	{ atom.add.f64 %fd751,[%rd306],%fd381; }

	// end inline asm
	add.s64 	%rd307, %rd300, 56;
	// begin inline asm
	{ atom.add.f64 %fd753,[%rd307],%fd380; }

	// end inline asm
	add.s64 	%rd308, %rd300, 64;
	// begin inline asm
	{ atom.add.f64 %fd755,[%rd308],%fd379; }

	// end inline asm

$L__BB0_45:
	ld.param.u64 	%rd35, [%rd23];
	ld.param.u32 	%r33, [%rd23+32];
	ld.param.u32 	%r34, [%rd23+60];
	add.s32 	%r35, %r3, 10;
	setp.le.s32 	%p30, %r34, %r35;
	selp.u16 	%rs56, 1, 0, %p30;
	shr.u32 	%r466, %r35, 31;
	cvt.u16.u32 	%rs57, %r466;
	or.b16  	%rs58, %rs56, %rs57;
	setp.eq.s16 	%p31, %rs58, 0;
	@%p31 bra 	$L__BB0_47;

	add.s32 	%r687, %r3, 10;
	st.local.v2.u32 	[%rd25], {%r687, %r34};
	mov.u64 	%rd310, $str;
	cvta.global.u64 	%rd311, %rd310;
	{ // callseq 262, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd311;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r467, [retval0+0];
	} // callseq 262
	bra.uni 	$L__BB0_48;

$L__BB0_47:
	mul.wide.s32 	%rd322, %r33, %r35;
	add.s64 	%rd313, %rd35, %rd322;
	// begin inline asm
	{ atom.add.f64 %fd757,[%rd313],%fd426; }

	// end inline asm
	add.s64 	%rd314, %rd313, 8;
	// begin inline asm
	{ atom.add.f64 %fd759,[%rd314],%fd425; }

	// end inline asm
	add.s64 	%rd315, %rd313, 16;
	// begin inline asm
	{ atom.add.f64 %fd761,[%rd315],%fd424; }

	// end inline asm
	add.s64 	%rd316, %rd313, 24;
	// begin inline asm
	{ atom.add.f64 %fd763,[%rd316],%fd402; }

	// end inline asm
	add.s64 	%rd317, %rd313, 32;
	// begin inline asm
	{ atom.add.f64 %fd765,[%rd317],%fd401; }

	// end inline asm
	add.s64 	%rd318, %rd313, 40;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd318],%fd400; }

	// end inline asm
	add.s64 	%rd319, %rd313, 48;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd319],%fd378; }

	// end inline asm
	add.s64 	%rd320, %rd313, 56;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd320],%fd377; }

	// end inline asm
	add.s64 	%rd321, %rd313, 64;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd321],%fd376; }

	// end inline asm

$L__BB0_48:
	ld.param.u64 	%rd36, [%rd23];
	ld.param.u32 	%r36, [%rd23+32];
	ld.param.u32 	%r37, [%rd23+60];
	add.s32 	%r38, %r3, 11;
	setp.le.s32 	%p32, %r37, %r38;
	selp.u16 	%rs59, 1, 0, %p32;
	shr.u32 	%r468, %r38, 31;
	cvt.u16.u32 	%rs60, %r468;
	or.b16  	%rs61, %rs59, %rs60;
	setp.eq.s16 	%p33, %rs61, 0;
	@%p33 bra 	$L__BB0_50;

	add.s32 	%r688, %r3, 11;
	st.local.v2.u32 	[%rd25], {%r688, %r37};
	mov.u64 	%rd323, $str;
	cvta.global.u64 	%rd324, %rd323;
	{ // callseq 263, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd324;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r469, [retval0+0];
	} // callseq 263
	bra.uni 	$L__BB0_51;

$L__BB0_50:
	mul.wide.s32 	%rd335, %r36, %r38;
	add.s64 	%rd326, %rd36, %rd335;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd326],%fd423; }

	// end inline asm
	add.s64 	%rd327, %rd326, 8;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd327],%fd422; }

	// end inline asm
	add.s64 	%rd328, %rd326, 16;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd328],%fd421; }

	// end inline asm
	add.s64 	%rd329, %rd326, 24;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd329],%fd399; }

	// end inline asm
	add.s64 	%rd330, %rd326, 32;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd330],%fd398; }

	// end inline asm
	add.s64 	%rd331, %rd326, 40;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd331],%fd397; }

	// end inline asm
	add.s64 	%rd332, %rd326, 48;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd332],%fd375; }

	// end inline asm
	add.s64 	%rd333, %rd326, 56;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd333],%fd374; }

	// end inline asm
	add.s64 	%rd334, %rd326, 64;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd334],%fd373; }

	// end inline asm

$L__BB0_51:
	ld.param.u64 	%rd37, [%rd23];
	ld.param.u32 	%r39, [%rd23+32];
	ld.param.u32 	%r40, [%rd23+60];
	add.s32 	%r41, %r3, 12;
	setp.le.s32 	%p34, %r40, %r41;
	selp.u16 	%rs62, 1, 0, %p34;
	shr.u32 	%r470, %r41, 31;
	cvt.u16.u32 	%rs63, %r470;
	or.b16  	%rs64, %rs62, %rs63;
	setp.eq.s16 	%p35, %rs64, 0;
	@%p35 bra 	$L__BB0_53;

	add.s32 	%r689, %r3, 12;
	st.local.v2.u32 	[%rd25], {%r689, %r40};
	mov.u64 	%rd336, $str;
	cvta.global.u64 	%rd337, %rd336;
	{ // callseq 264, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd337;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r471, [retval0+0];
	} // callseq 264
	bra.uni 	$L__BB0_54;

$L__BB0_53:
	mul.wide.s32 	%rd348, %r39, %r41;
	add.s64 	%rd339, %rd37, %rd348;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd339],%fd360; }

	// end inline asm
	add.s64 	%rd340, %rd339, 8;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd340],%fd359; }

	// end inline asm
	add.s64 	%rd341, %rd339, 16;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd341],%fd358; }

	// end inline asm
	add.s64 	%rd342, %rd339, 24;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd342],%fd336; }

	// end inline asm
	add.s64 	%rd343, %rd339, 32;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd343],%fd335; }

	// end inline asm
	add.s64 	%rd344, %rd339, 40;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd344],%fd334; }

	// end inline asm
	add.s64 	%rd345, %rd339, 48;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd345],%fd312; }

	// end inline asm
	add.s64 	%rd346, %rd339, 56;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd346],%fd311; }

	// end inline asm
	add.s64 	%rd347, %rd339, 64;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd347],%fd310; }

	// end inline asm

$L__BB0_54:
	ld.param.u64 	%rd38, [%rd23];
	ld.param.u32 	%r42, [%rd23+32];
	ld.param.u32 	%r43, [%rd23+60];
	add.s32 	%r44, %r3, 13;
	setp.le.s32 	%p36, %r43, %r44;
	selp.u16 	%rs65, 1, 0, %p36;
	shr.u32 	%r472, %r44, 31;
	cvt.u16.u32 	%rs66, %r472;
	or.b16  	%rs67, %rs65, %rs66;
	setp.eq.s16 	%p37, %rs67, 0;
	@%p37 bra 	$L__BB0_56;

	add.s32 	%r690, %r3, 13;
	st.local.v2.u32 	[%rd25], {%r690, %r43};
	mov.u64 	%rd349, $str;
	cvta.global.u64 	%rd350, %rd349;
	{ // callseq 265, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd350;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r473, [retval0+0];
	} // callseq 265
	bra.uni 	$L__BB0_57;

$L__BB0_56:
	mul.wide.s32 	%rd361, %r42, %r44;
	add.s64 	%rd352, %rd38, %rd361;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd352],%fd357; }

	// end inline asm
	add.s64 	%rd353, %rd352, 8;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd353],%fd356; }

	// end inline asm
	add.s64 	%rd354, %rd352, 16;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd354],%fd355; }

	// end inline asm
	add.s64 	%rd355, %rd352, 24;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd355],%fd333; }

	// end inline asm
	add.s64 	%rd356, %rd352, 32;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd356],%fd332; }

	// end inline asm
	add.s64 	%rd357, %rd352, 40;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd357],%fd331; }

	// end inline asm
	add.s64 	%rd358, %rd352, 48;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd358],%fd309; }

	// end inline asm
	add.s64 	%rd359, %rd352, 56;
	// begin inline asm
	{ atom.add.f64 %fd825,[%rd359],%fd308; }

	// end inline asm
	add.s64 	%rd360, %rd352, 64;
	// begin inline asm
	{ atom.add.f64 %fd827,[%rd360],%fd307; }

	// end inline asm

$L__BB0_57:
	ld.param.u64 	%rd39, [%rd23];
	ld.param.u32 	%r45, [%rd23+32];
	ld.param.u32 	%r46, [%rd23+60];
	add.s32 	%r47, %r3, 14;
	setp.le.s32 	%p38, %r46, %r47;
	selp.u16 	%rs68, 1, 0, %p38;
	shr.u32 	%r474, %r47, 31;
	cvt.u16.u32 	%rs69, %r474;
	or.b16  	%rs70, %rs68, %rs69;
	setp.eq.s16 	%p39, %rs70, 0;
	@%p39 bra 	$L__BB0_59;

	add.s32 	%r691, %r3, 14;
	st.local.v2.u32 	[%rd25], {%r691, %r46};
	mov.u64 	%rd362, $str;
	cvta.global.u64 	%rd363, %rd362;
	{ // callseq 266, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd363;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r475, [retval0+0];
	} // callseq 266
	bra.uni 	$L__BB0_60;

$L__BB0_59:
	mul.wide.s32 	%rd374, %r45, %r47;
	add.s64 	%rd365, %rd39, %rd374;
	// begin inline asm
	{ atom.add.f64 %fd829,[%rd365],%fd354; }

	// end inline asm
	add.s64 	%rd366, %rd365, 8;
	// begin inline asm
	{ atom.add.f64 %fd831,[%rd366],%fd353; }

	// end inline asm
	add.s64 	%rd367, %rd365, 16;
	// begin inline asm
	{ atom.add.f64 %fd833,[%rd367],%fd352; }

	// end inline asm
	add.s64 	%rd368, %rd365, 24;
	// begin inline asm
	{ atom.add.f64 %fd835,[%rd368],%fd330; }

	// end inline asm
	add.s64 	%rd369, %rd365, 32;
	// begin inline asm
	{ atom.add.f64 %fd837,[%rd369],%fd329; }

	// end inline asm
	add.s64 	%rd370, %rd365, 40;
	// begin inline asm
	{ atom.add.f64 %fd839,[%rd370],%fd328; }

	// end inline asm
	add.s64 	%rd371, %rd365, 48;
	// begin inline asm
	{ atom.add.f64 %fd841,[%rd371],%fd306; }

	// end inline asm
	add.s64 	%rd372, %rd365, 56;
	// begin inline asm
	{ atom.add.f64 %fd843,[%rd372],%fd305; }

	// end inline asm
	add.s64 	%rd373, %rd365, 64;
	// begin inline asm
	{ atom.add.f64 %fd845,[%rd373],%fd304; }

	// end inline asm

$L__BB0_60:
	ld.param.u64 	%rd40, [%rd23];
	ld.param.u32 	%r48, [%rd23+32];
	ld.param.u32 	%r49, [%rd23+60];
	add.s32 	%r50, %r3, 15;
	setp.le.s32 	%p40, %r49, %r50;
	selp.u16 	%rs71, 1, 0, %p40;
	shr.u32 	%r476, %r50, 31;
	cvt.u16.u32 	%rs72, %r476;
	or.b16  	%rs73, %rs71, %rs72;
	setp.eq.s16 	%p41, %rs73, 0;
	@%p41 bra 	$L__BB0_62;

	add.s32 	%r692, %r3, 15;
	st.local.v2.u32 	[%rd25], {%r692, %r49};
	mov.u64 	%rd375, $str;
	cvta.global.u64 	%rd376, %rd375;
	{ // callseq 267, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd376;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r477, [retval0+0];
	} // callseq 267
	bra.uni 	$L__BB0_63;

$L__BB0_62:
	mul.wide.s32 	%rd387, %r48, %r50;
	add.s64 	%rd378, %rd40, %rd387;
	// begin inline asm
	{ atom.add.f64 %fd847,[%rd378],%fd351; }

	// end inline asm
	add.s64 	%rd379, %rd378, 8;
	// begin inline asm
	{ atom.add.f64 %fd849,[%rd379],%fd350; }

	// end inline asm
	add.s64 	%rd380, %rd378, 16;
	// begin inline asm
	{ atom.add.f64 %fd851,[%rd380],%fd349; }

	// end inline asm
	add.s64 	%rd381, %rd378, 24;
	// begin inline asm
	{ atom.add.f64 %fd853,[%rd381],%fd327; }

	// end inline asm
	add.s64 	%rd382, %rd378, 32;
	// begin inline asm
	{ atom.add.f64 %fd855,[%rd382],%fd326; }

	// end inline asm
	add.s64 	%rd383, %rd378, 40;
	// begin inline asm
	{ atom.add.f64 %fd857,[%rd383],%fd325; }

	// end inline asm
	add.s64 	%rd384, %rd378, 48;
	// begin inline asm
	{ atom.add.f64 %fd859,[%rd384],%fd303; }

	// end inline asm
	add.s64 	%rd385, %rd378, 56;
	// begin inline asm
	{ atom.add.f64 %fd861,[%rd385],%fd302; }

	// end inline asm
	add.s64 	%rd386, %rd378, 64;
	// begin inline asm
	{ atom.add.f64 %fd863,[%rd386],%fd301; }

	// end inline asm

$L__BB0_63:
	setp.gt.s32 	%p299, %r375, 0;
	cvt.u32.u64 	%r817, %rd1215;
	selp.b32 	%r816, %r817, 0, %p299;
	cvt.s64.s32 	%rd1208, %r816;
	mul.lo.s64 	%rd389, %rd1208, %rd11;
	add.s64 	%rd41, %rd4, %rd389;
	ld.global.u32 	%r478, [%rd41];
	shl.b32 	%r51, %r478, 4;
	ld.param.u64 	%rd42, [%rd23];
	ld.param.u32 	%r52, [%rd23+32];
	ld.param.u32 	%r53, [%rd23+60];
	setp.le.s32 	%p42, %r53, %r51;
	selp.u16 	%rs74, 1, 0, %p42;
	shr.u32 	%r479, %r478, 27;
	cvt.u16.u32 	%rs75, %r479;
	and.b16  	%rs76, %rs75, 1;
	or.b16  	%rs77, %rs76, %rs74;
	setp.eq.s16 	%p43, %rs77, 0;
	@%p43 bra 	$L__BB0_65;

	st.local.v2.u32 	[%rd25], {%r51, %r53};
	mov.u64 	%rd390, $str;
	cvta.global.u64 	%rd391, %rd390;
	{ // callseq 268, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd391;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r480, [retval0+0];
	} // callseq 268
	bra.uni 	$L__BB0_66;

$L__BB0_65:
	mul.wide.s32 	%rd402, %r52, %r51;
	add.s64 	%rd393, %rd42, %rd402;
	// begin inline asm
	{ atom.add.f64 %fd865,[%rd393],%fd276; }

	// end inline asm
	add.s64 	%rd394, %rd393, 8;
	// begin inline asm
	{ atom.add.f64 %fd867,[%rd394],%fd275; }

	// end inline asm
	add.s64 	%rd395, %rd393, 16;
	// begin inline asm
	{ atom.add.f64 %fd869,[%rd395],%fd274; }

	// end inline asm
	add.s64 	%rd396, %rd393, 24;
	// begin inline asm
	{ atom.add.f64 %fd871,[%rd396],%fd252; }

	// end inline asm
	add.s64 	%rd397, %rd393, 32;
	// begin inline asm
	{ atom.add.f64 %fd873,[%rd397],%fd251; }

	// end inline asm
	add.s64 	%rd398, %rd393, 40;
	// begin inline asm
	{ atom.add.f64 %fd875,[%rd398],%fd250; }

	// end inline asm
	add.s64 	%rd399, %rd393, 48;
	// begin inline asm
	{ atom.add.f64 %fd877,[%rd399],%fd228; }

	// end inline asm
	add.s64 	%rd400, %rd393, 56;
	// begin inline asm
	{ atom.add.f64 %fd879,[%rd400],%fd227; }

	// end inline asm
	add.s64 	%rd401, %rd393, 64;
	// begin inline asm
	{ atom.add.f64 %fd881,[%rd401],%fd226; }

	// end inline asm

$L__BB0_66:
	ld.param.u64 	%rd43, [%rd23];
	ld.param.u32 	%r54, [%rd23+32];
	ld.param.u32 	%r55, [%rd23+60];
	add.s32 	%r56, %r51, 1;
	setp.le.s32 	%p44, %r55, %r56;
	selp.u16 	%rs78, 1, 0, %p44;
	shr.u32 	%r481, %r56, 31;
	cvt.u16.u32 	%rs79, %r481;
	or.b16  	%rs80, %rs78, %rs79;
	setp.eq.s16 	%p45, %rs80, 0;
	@%p45 bra 	$L__BB0_68;

	add.s32 	%r693, %r51, 1;
	st.local.v2.u32 	[%rd25], {%r693, %r55};
	mov.u64 	%rd403, $str;
	cvta.global.u64 	%rd404, %rd403;
	{ // callseq 269, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd404;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r482, [retval0+0];
	} // callseq 269
	bra.uni 	$L__BB0_69;

$L__BB0_68:
	mul.wide.s32 	%rd415, %r54, %r56;
	add.s64 	%rd406, %rd43, %rd415;
	// begin inline asm
	{ atom.add.f64 %fd883,[%rd406],%fd273; }

	// end inline asm
	add.s64 	%rd407, %rd406, 8;
	// begin inline asm
	{ atom.add.f64 %fd885,[%rd407],%fd272; }

	// end inline asm
	add.s64 	%rd408, %rd406, 16;
	// begin inline asm
	{ atom.add.f64 %fd887,[%rd408],%fd271; }

	// end inline asm
	add.s64 	%rd409, %rd406, 24;
	// begin inline asm
	{ atom.add.f64 %fd889,[%rd409],%fd249; }

	// end inline asm
	add.s64 	%rd410, %rd406, 32;
	// begin inline asm
	{ atom.add.f64 %fd891,[%rd410],%fd248; }

	// end inline asm
	add.s64 	%rd411, %rd406, 40;
	// begin inline asm
	{ atom.add.f64 %fd893,[%rd411],%fd247; }

	// end inline asm
	add.s64 	%rd412, %rd406, 48;
	// begin inline asm
	{ atom.add.f64 %fd895,[%rd412],%fd225; }

	// end inline asm
	add.s64 	%rd413, %rd406, 56;
	// begin inline asm
	{ atom.add.f64 %fd897,[%rd413],%fd224; }

	// end inline asm
	add.s64 	%rd414, %rd406, 64;
	// begin inline asm
	{ atom.add.f64 %fd899,[%rd414],%fd223; }

	// end inline asm

$L__BB0_69:
	ld.param.u64 	%rd44, [%rd23];
	ld.param.u32 	%r57, [%rd23+32];
	ld.param.u32 	%r58, [%rd23+60];
	add.s32 	%r59, %r51, 2;
	setp.le.s32 	%p46, %r58, %r59;
	selp.u16 	%rs81, 1, 0, %p46;
	shr.u32 	%r483, %r59, 31;
	cvt.u16.u32 	%rs82, %r483;
	or.b16  	%rs83, %rs81, %rs82;
	setp.eq.s16 	%p47, %rs83, 0;
	@%p47 bra 	$L__BB0_71;

	add.s32 	%r694, %r51, 2;
	st.local.v2.u32 	[%rd25], {%r694, %r58};
	mov.u64 	%rd416, $str;
	cvta.global.u64 	%rd417, %rd416;
	{ // callseq 270, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd417;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r484, [retval0+0];
	} // callseq 270
	bra.uni 	$L__BB0_72;

$L__BB0_71:
	mul.wide.s32 	%rd428, %r57, %r59;
	add.s64 	%rd419, %rd44, %rd428;
	// begin inline asm
	{ atom.add.f64 %fd901,[%rd419],%fd270; }

	// end inline asm
	add.s64 	%rd420, %rd419, 8;
	// begin inline asm
	{ atom.add.f64 %fd903,[%rd420],%fd269; }

	// end inline asm
	add.s64 	%rd421, %rd419, 16;
	// begin inline asm
	{ atom.add.f64 %fd905,[%rd421],%fd268; }

	// end inline asm
	add.s64 	%rd422, %rd419, 24;
	// begin inline asm
	{ atom.add.f64 %fd907,[%rd422],%fd246; }

	// end inline asm
	add.s64 	%rd423, %rd419, 32;
	// begin inline asm
	{ atom.add.f64 %fd909,[%rd423],%fd245; }

	// end inline asm
	add.s64 	%rd424, %rd419, 40;
	// begin inline asm
	{ atom.add.f64 %fd911,[%rd424],%fd244; }

	// end inline asm
	add.s64 	%rd425, %rd419, 48;
	// begin inline asm
	{ atom.add.f64 %fd913,[%rd425],%fd222; }

	// end inline asm
	add.s64 	%rd426, %rd419, 56;
	// begin inline asm
	{ atom.add.f64 %fd915,[%rd426],%fd221; }

	// end inline asm
	add.s64 	%rd427, %rd419, 64;
	// begin inline asm
	{ atom.add.f64 %fd917,[%rd427],%fd220; }

	// end inline asm

$L__BB0_72:
	ld.param.u64 	%rd45, [%rd23];
	ld.param.u32 	%r60, [%rd23+32];
	ld.param.u32 	%r61, [%rd23+60];
	add.s32 	%r62, %r51, 3;
	setp.le.s32 	%p48, %r61, %r62;
	selp.u16 	%rs84, 1, 0, %p48;
	shr.u32 	%r485, %r62, 31;
	cvt.u16.u32 	%rs85, %r485;
	or.b16  	%rs86, %rs84, %rs85;
	setp.eq.s16 	%p49, %rs86, 0;
	@%p49 bra 	$L__BB0_74;

	add.s32 	%r695, %r51, 3;
	st.local.v2.u32 	[%rd25], {%r695, %r61};
	mov.u64 	%rd429, $str;
	cvta.global.u64 	%rd430, %rd429;
	{ // callseq 271, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd430;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r486, [retval0+0];
	} // callseq 271
	bra.uni 	$L__BB0_75;

$L__BB0_74:
	mul.wide.s32 	%rd441, %r60, %r62;
	add.s64 	%rd432, %rd45, %rd441;
	// begin inline asm
	{ atom.add.f64 %fd919,[%rd432],%fd267; }

	// end inline asm
	add.s64 	%rd433, %rd432, 8;
	// begin inline asm
	{ atom.add.f64 %fd921,[%rd433],%fd266; }

	// end inline asm
	add.s64 	%rd434, %rd432, 16;
	// begin inline asm
	{ atom.add.f64 %fd923,[%rd434],%fd265; }

	// end inline asm
	add.s64 	%rd435, %rd432, 24;
	// begin inline asm
	{ atom.add.f64 %fd925,[%rd435],%fd243; }

	// end inline asm
	add.s64 	%rd436, %rd432, 32;
	// begin inline asm
	{ atom.add.f64 %fd927,[%rd436],%fd242; }

	// end inline asm
	add.s64 	%rd437, %rd432, 40;
	// begin inline asm
	{ atom.add.f64 %fd929,[%rd437],%fd241; }

	// end inline asm
	add.s64 	%rd438, %rd432, 48;
	// begin inline asm
	{ atom.add.f64 %fd931,[%rd438],%fd219; }

	// end inline asm
	add.s64 	%rd439, %rd432, 56;
	// begin inline asm
	{ atom.add.f64 %fd933,[%rd439],%fd218; }

	// end inline asm
	add.s64 	%rd440, %rd432, 64;
	// begin inline asm
	{ atom.add.f64 %fd935,[%rd440],%fd217; }

	// end inline asm

$L__BB0_75:
	ld.param.u64 	%rd46, [%rd23];
	ld.param.u32 	%r63, [%rd23+32];
	ld.param.u32 	%r64, [%rd23+60];
	add.s32 	%r65, %r51, 4;
	setp.le.s32 	%p50, %r64, %r65;
	selp.u16 	%rs87, 1, 0, %p50;
	shr.u32 	%r487, %r65, 31;
	cvt.u16.u32 	%rs88, %r487;
	or.b16  	%rs89, %rs87, %rs88;
	setp.eq.s16 	%p51, %rs89, 0;
	@%p51 bra 	$L__BB0_77;

	add.s32 	%r696, %r51, 4;
	st.local.v2.u32 	[%rd25], {%r696, %r64};
	mov.u64 	%rd442, $str;
	cvta.global.u64 	%rd443, %rd442;
	{ // callseq 272, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd443;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r488, [retval0+0];
	} // callseq 272
	bra.uni 	$L__BB0_78;

$L__BB0_77:
	mul.wide.s32 	%rd454, %r63, %r65;
	add.s64 	%rd445, %rd46, %rd454;
	// begin inline asm
	{ atom.add.f64 %fd937,[%rd445],%fd204; }

	// end inline asm
	add.s64 	%rd446, %rd445, 8;
	// begin inline asm
	{ atom.add.f64 %fd939,[%rd446],%fd203; }

	// end inline asm
	add.s64 	%rd447, %rd445, 16;
	// begin inline asm
	{ atom.add.f64 %fd941,[%rd447],%fd202; }

	// end inline asm
	add.s64 	%rd448, %rd445, 24;
	// begin inline asm
	{ atom.add.f64 %fd943,[%rd448],%fd180; }

	// end inline asm
	add.s64 	%rd449, %rd445, 32;
	// begin inline asm
	{ atom.add.f64 %fd945,[%rd449],%fd179; }

	// end inline asm
	add.s64 	%rd450, %rd445, 40;
	// begin inline asm
	{ atom.add.f64 %fd947,[%rd450],%fd178; }

	// end inline asm
	add.s64 	%rd451, %rd445, 48;
	// begin inline asm
	{ atom.add.f64 %fd949,[%rd451],%fd156; }

	// end inline asm
	add.s64 	%rd452, %rd445, 56;
	// begin inline asm
	{ atom.add.f64 %fd951,[%rd452],%fd155; }

	// end inline asm
	add.s64 	%rd453, %rd445, 64;
	// begin inline asm
	{ atom.add.f64 %fd953,[%rd453],%fd154; }

	// end inline asm

$L__BB0_78:
	ld.param.u64 	%rd47, [%rd23];
	ld.param.u32 	%r66, [%rd23+32];
	ld.param.u32 	%r67, [%rd23+60];
	add.s32 	%r68, %r51, 5;
	setp.le.s32 	%p52, %r67, %r68;
	selp.u16 	%rs90, 1, 0, %p52;
	shr.u32 	%r489, %r68, 31;
	cvt.u16.u32 	%rs91, %r489;
	or.b16  	%rs92, %rs90, %rs91;
	setp.eq.s16 	%p53, %rs92, 0;
	@%p53 bra 	$L__BB0_80;

	add.s32 	%r697, %r51, 5;
	st.local.v2.u32 	[%rd25], {%r697, %r67};
	mov.u64 	%rd455, $str;
	cvta.global.u64 	%rd456, %rd455;
	{ // callseq 273, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd456;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r490, [retval0+0];
	} // callseq 273
	bra.uni 	$L__BB0_81;

$L__BB0_80:
	mul.wide.s32 	%rd467, %r66, %r68;
	add.s64 	%rd458, %rd47, %rd467;
	// begin inline asm
	{ atom.add.f64 %fd955,[%rd458],%fd201; }

	// end inline asm
	add.s64 	%rd459, %rd458, 8;
	// begin inline asm
	{ atom.add.f64 %fd957,[%rd459],%fd200; }

	// end inline asm
	add.s64 	%rd460, %rd458, 16;
	// begin inline asm
	{ atom.add.f64 %fd959,[%rd460],%fd199; }

	// end inline asm
	add.s64 	%rd461, %rd458, 24;
	// begin inline asm
	{ atom.add.f64 %fd961,[%rd461],%fd177; }

	// end inline asm
	add.s64 	%rd462, %rd458, 32;
	// begin inline asm
	{ atom.add.f64 %fd963,[%rd462],%fd176; }

	// end inline asm
	add.s64 	%rd463, %rd458, 40;
	// begin inline asm
	{ atom.add.f64 %fd965,[%rd463],%fd175; }

	// end inline asm
	add.s64 	%rd464, %rd458, 48;
	// begin inline asm
	{ atom.add.f64 %fd967,[%rd464],%fd153; }

	// end inline asm
	add.s64 	%rd465, %rd458, 56;
	// begin inline asm
	{ atom.add.f64 %fd969,[%rd465],%fd152; }

	// end inline asm
	add.s64 	%rd466, %rd458, 64;
	// begin inline asm
	{ atom.add.f64 %fd971,[%rd466],%fd151; }

	// end inline asm

$L__BB0_81:
	ld.param.u64 	%rd48, [%rd23];
	ld.param.u32 	%r69, [%rd23+32];
	ld.param.u32 	%r70, [%rd23+60];
	add.s32 	%r71, %r51, 6;
	setp.le.s32 	%p54, %r70, %r71;
	selp.u16 	%rs93, 1, 0, %p54;
	shr.u32 	%r491, %r71, 31;
	cvt.u16.u32 	%rs94, %r491;
	or.b16  	%rs95, %rs93, %rs94;
	setp.eq.s16 	%p55, %rs95, 0;
	@%p55 bra 	$L__BB0_83;

	add.s32 	%r698, %r51, 6;
	st.local.v2.u32 	[%rd25], {%r698, %r70};
	mov.u64 	%rd468, $str;
	cvta.global.u64 	%rd469, %rd468;
	{ // callseq 274, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd469;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r492, [retval0+0];
	} // callseq 274
	bra.uni 	$L__BB0_84;

$L__BB0_83:
	mul.wide.s32 	%rd480, %r69, %r71;
	add.s64 	%rd471, %rd48, %rd480;
	// begin inline asm
	{ atom.add.f64 %fd973,[%rd471],%fd198; }

	// end inline asm
	add.s64 	%rd472, %rd471, 8;
	// begin inline asm
	{ atom.add.f64 %fd975,[%rd472],%fd197; }

	// end inline asm
	add.s64 	%rd473, %rd471, 16;
	// begin inline asm
	{ atom.add.f64 %fd977,[%rd473],%fd196; }

	// end inline asm
	add.s64 	%rd474, %rd471, 24;
	// begin inline asm
	{ atom.add.f64 %fd979,[%rd474],%fd174; }

	// end inline asm
	add.s64 	%rd475, %rd471, 32;
	// begin inline asm
	{ atom.add.f64 %fd981,[%rd475],%fd173; }

	// end inline asm
	add.s64 	%rd476, %rd471, 40;
	// begin inline asm
	{ atom.add.f64 %fd983,[%rd476],%fd172; }

	// end inline asm
	add.s64 	%rd477, %rd471, 48;
	// begin inline asm
	{ atom.add.f64 %fd985,[%rd477],%fd150; }

	// end inline asm
	add.s64 	%rd478, %rd471, 56;
	// begin inline asm
	{ atom.add.f64 %fd987,[%rd478],%fd149; }

	// end inline asm
	add.s64 	%rd479, %rd471, 64;
	// begin inline asm
	{ atom.add.f64 %fd989,[%rd479],%fd148; }

	// end inline asm

$L__BB0_84:
	ld.param.u64 	%rd49, [%rd23];
	ld.param.u32 	%r72, [%rd23+32];
	ld.param.u32 	%r73, [%rd23+60];
	add.s32 	%r74, %r51, 7;
	setp.le.s32 	%p56, %r73, %r74;
	selp.u16 	%rs96, 1, 0, %p56;
	shr.u32 	%r493, %r74, 31;
	cvt.u16.u32 	%rs97, %r493;
	or.b16  	%rs98, %rs96, %rs97;
	setp.eq.s16 	%p57, %rs98, 0;
	@%p57 bra 	$L__BB0_86;

	add.s32 	%r699, %r51, 7;
	st.local.v2.u32 	[%rd25], {%r699, %r73};
	mov.u64 	%rd481, $str;
	cvta.global.u64 	%rd482, %rd481;
	{ // callseq 275, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd482;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r494, [retval0+0];
	} // callseq 275
	bra.uni 	$L__BB0_87;

$L__BB0_86:
	mul.wide.s32 	%rd493, %r72, %r74;
	add.s64 	%rd484, %rd49, %rd493;
	// begin inline asm
	{ atom.add.f64 %fd991,[%rd484],%fd195; }

	// end inline asm
	add.s64 	%rd485, %rd484, 8;
	// begin inline asm
	{ atom.add.f64 %fd993,[%rd485],%fd194; }

	// end inline asm
	add.s64 	%rd486, %rd484, 16;
	// begin inline asm
	{ atom.add.f64 %fd995,[%rd486],%fd193; }

	// end inline asm
	add.s64 	%rd487, %rd484, 24;
	// begin inline asm
	{ atom.add.f64 %fd997,[%rd487],%fd171; }

	// end inline asm
	add.s64 	%rd488, %rd484, 32;
	// begin inline asm
	{ atom.add.f64 %fd999,[%rd488],%fd170; }

	// end inline asm
	add.s64 	%rd489, %rd484, 40;
	// begin inline asm
	{ atom.add.f64 %fd1001,[%rd489],%fd169; }

	// end inline asm
	add.s64 	%rd490, %rd484, 48;
	// begin inline asm
	{ atom.add.f64 %fd1003,[%rd490],%fd147; }

	// end inline asm
	add.s64 	%rd491, %rd484, 56;
	// begin inline asm
	{ atom.add.f64 %fd1005,[%rd491],%fd146; }

	// end inline asm
	add.s64 	%rd492, %rd484, 64;
	// begin inline asm
	{ atom.add.f64 %fd1007,[%rd492],%fd145; }

	// end inline asm

$L__BB0_87:
	ld.param.u64 	%rd50, [%rd23];
	ld.param.u32 	%r75, [%rd23+32];
	ld.param.u32 	%r76, [%rd23+60];
	add.s32 	%r77, %r51, 8;
	setp.le.s32 	%p58, %r76, %r77;
	selp.u16 	%rs99, 1, 0, %p58;
	shr.u32 	%r495, %r77, 31;
	cvt.u16.u32 	%rs100, %r495;
	or.b16  	%rs101, %rs99, %rs100;
	setp.eq.s16 	%p59, %rs101, 0;
	@%p59 bra 	$L__BB0_89;

	add.s32 	%r700, %r51, 8;
	st.local.v2.u32 	[%rd25], {%r700, %r76};
	mov.u64 	%rd494, $str;
	cvta.global.u64 	%rd495, %rd494;
	{ // callseq 276, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd495;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r496, [retval0+0];
	} // callseq 276
	bra.uni 	$L__BB0_90;

$L__BB0_89:
	mul.wide.s32 	%rd506, %r75, %r77;
	add.s64 	%rd497, %rd50, %rd506;
	// begin inline asm
	{ atom.add.f64 %fd1009,[%rd497],%fd132; }

	// end inline asm
	add.s64 	%rd498, %rd497, 8;
	// begin inline asm
	{ atom.add.f64 %fd1011,[%rd498],%fd131; }

	// end inline asm
	add.s64 	%rd499, %rd497, 16;
	// begin inline asm
	{ atom.add.f64 %fd1013,[%rd499],%fd130; }

	// end inline asm
	add.s64 	%rd500, %rd497, 24;
	// begin inline asm
	{ atom.add.f64 %fd1015,[%rd500],%fd108; }

	// end inline asm
	add.s64 	%rd501, %rd497, 32;
	// begin inline asm
	{ atom.add.f64 %fd1017,[%rd501],%fd107; }

	// end inline asm
	add.s64 	%rd502, %rd497, 40;
	// begin inline asm
	{ atom.add.f64 %fd1019,[%rd502],%fd106; }

	// end inline asm
	add.s64 	%rd503, %rd497, 48;
	// begin inline asm
	{ atom.add.f64 %fd1021,[%rd503],%fd84; }

	// end inline asm
	add.s64 	%rd504, %rd497, 56;
	// begin inline asm
	{ atom.add.f64 %fd1023,[%rd504],%fd83; }

	// end inline asm
	add.s64 	%rd505, %rd497, 64;
	// begin inline asm
	{ atom.add.f64 %fd1025,[%rd505],%fd82; }

	// end inline asm

$L__BB0_90:
	ld.param.u64 	%rd51, [%rd23];
	ld.param.u32 	%r78, [%rd23+32];
	ld.param.u32 	%r79, [%rd23+60];
	add.s32 	%r80, %r51, 9;
	setp.le.s32 	%p60, %r79, %r80;
	selp.u16 	%rs102, 1, 0, %p60;
	shr.u32 	%r497, %r80, 31;
	cvt.u16.u32 	%rs103, %r497;
	or.b16  	%rs104, %rs102, %rs103;
	setp.eq.s16 	%p61, %rs104, 0;
	@%p61 bra 	$L__BB0_92;

	add.s32 	%r701, %r51, 9;
	st.local.v2.u32 	[%rd25], {%r701, %r79};
	mov.u64 	%rd507, $str;
	cvta.global.u64 	%rd508, %rd507;
	{ // callseq 277, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd508;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r498, [retval0+0];
	} // callseq 277
	bra.uni 	$L__BB0_93;

$L__BB0_92:
	mul.wide.s32 	%rd519, %r78, %r80;
	add.s64 	%rd510, %rd51, %rd519;
	// begin inline asm
	{ atom.add.f64 %fd1027,[%rd510],%fd129; }

	// end inline asm
	add.s64 	%rd511, %rd510, 8;
	// begin inline asm
	{ atom.add.f64 %fd1029,[%rd511],%fd128; }

	// end inline asm
	add.s64 	%rd512, %rd510, 16;
	// begin inline asm
	{ atom.add.f64 %fd1031,[%rd512],%fd127; }

	// end inline asm
	add.s64 	%rd513, %rd510, 24;
	// begin inline asm
	{ atom.add.f64 %fd1033,[%rd513],%fd105; }

	// end inline asm
	add.s64 	%rd514, %rd510, 32;
	// begin inline asm
	{ atom.add.f64 %fd1035,[%rd514],%fd104; }

	// end inline asm
	add.s64 	%rd515, %rd510, 40;
	// begin inline asm
	{ atom.add.f64 %fd1037,[%rd515],%fd103; }

	// end inline asm
	add.s64 	%rd516, %rd510, 48;
	// begin inline asm
	{ atom.add.f64 %fd1039,[%rd516],%fd81; }

	// end inline asm
	add.s64 	%rd517, %rd510, 56;
	// begin inline asm
	{ atom.add.f64 %fd1041,[%rd517],%fd80; }

	// end inline asm
	add.s64 	%rd518, %rd510, 64;
	// begin inline asm
	{ atom.add.f64 %fd1043,[%rd518],%fd79; }

	// end inline asm

$L__BB0_93:
	ld.param.u64 	%rd52, [%rd23];
	ld.param.u32 	%r81, [%rd23+32];
	ld.param.u32 	%r82, [%rd23+60];
	add.s32 	%r83, %r51, 10;
	setp.le.s32 	%p62, %r82, %r83;
	selp.u16 	%rs105, 1, 0, %p62;
	shr.u32 	%r499, %r83, 31;
	cvt.u16.u32 	%rs106, %r499;
	or.b16  	%rs107, %rs105, %rs106;
	setp.eq.s16 	%p63, %rs107, 0;
	@%p63 bra 	$L__BB0_95;

	add.s32 	%r702, %r51, 10;
	st.local.v2.u32 	[%rd25], {%r702, %r82};
	mov.u64 	%rd520, $str;
	cvta.global.u64 	%rd521, %rd520;
	{ // callseq 278, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd521;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r500, [retval0+0];
	} // callseq 278
	bra.uni 	$L__BB0_96;

$L__BB0_95:
	mul.wide.s32 	%rd532, %r81, %r83;
	add.s64 	%rd523, %rd52, %rd532;
	// begin inline asm
	{ atom.add.f64 %fd1045,[%rd523],%fd126; }

	// end inline asm
	add.s64 	%rd524, %rd523, 8;
	// begin inline asm
	{ atom.add.f64 %fd1047,[%rd524],%fd125; }

	// end inline asm
	add.s64 	%rd525, %rd523, 16;
	// begin inline asm
	{ atom.add.f64 %fd1049,[%rd525],%fd124; }

	// end inline asm
	add.s64 	%rd526, %rd523, 24;
	// begin inline asm
	{ atom.add.f64 %fd1051,[%rd526],%fd102; }

	// end inline asm
	add.s64 	%rd527, %rd523, 32;
	// begin inline asm
	{ atom.add.f64 %fd1053,[%rd527],%fd101; }

	// end inline asm
	add.s64 	%rd528, %rd523, 40;
	// begin inline asm
	{ atom.add.f64 %fd1055,[%rd528],%fd100; }

	// end inline asm
	add.s64 	%rd529, %rd523, 48;
	// begin inline asm
	{ atom.add.f64 %fd1057,[%rd529],%fd78; }

	// end inline asm
	add.s64 	%rd530, %rd523, 56;
	// begin inline asm
	{ atom.add.f64 %fd1059,[%rd530],%fd77; }

	// end inline asm
	add.s64 	%rd531, %rd523, 64;
	// begin inline asm
	{ atom.add.f64 %fd1061,[%rd531],%fd76; }

	// end inline asm

$L__BB0_96:
	ld.param.u64 	%rd53, [%rd23];
	ld.param.u32 	%r84, [%rd23+32];
	ld.param.u32 	%r85, [%rd23+60];
	add.s32 	%r86, %r51, 11;
	setp.le.s32 	%p64, %r85, %r86;
	selp.u16 	%rs108, 1, 0, %p64;
	shr.u32 	%r501, %r86, 31;
	cvt.u16.u32 	%rs109, %r501;
	or.b16  	%rs110, %rs108, %rs109;
	setp.eq.s16 	%p65, %rs110, 0;
	@%p65 bra 	$L__BB0_98;

	add.s32 	%r703, %r51, 11;
	st.local.v2.u32 	[%rd25], {%r703, %r85};
	mov.u64 	%rd533, $str;
	cvta.global.u64 	%rd534, %rd533;
	{ // callseq 279, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd534;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r502, [retval0+0];
	} // callseq 279
	bra.uni 	$L__BB0_99;

$L__BB0_98:
	mul.wide.s32 	%rd545, %r84, %r86;
	add.s64 	%rd536, %rd53, %rd545;
	// begin inline asm
	{ atom.add.f64 %fd1063,[%rd536],%fd123; }

	// end inline asm
	add.s64 	%rd537, %rd536, 8;
	// begin inline asm
	{ atom.add.f64 %fd1065,[%rd537],%fd122; }

	// end inline asm
	add.s64 	%rd538, %rd536, 16;
	// begin inline asm
	{ atom.add.f64 %fd1067,[%rd538],%fd121; }

	// end inline asm
	add.s64 	%rd539, %rd536, 24;
	// begin inline asm
	{ atom.add.f64 %fd1069,[%rd539],%fd99; }

	// end inline asm
	add.s64 	%rd540, %rd536, 32;
	// begin inline asm
	{ atom.add.f64 %fd1071,[%rd540],%fd98; }

	// end inline asm
	add.s64 	%rd541, %rd536, 40;
	// begin inline asm
	{ atom.add.f64 %fd1073,[%rd541],%fd97; }

	// end inline asm
	add.s64 	%rd542, %rd536, 48;
	// begin inline asm
	{ atom.add.f64 %fd1075,[%rd542],%fd75; }

	// end inline asm
	add.s64 	%rd543, %rd536, 56;
	// begin inline asm
	{ atom.add.f64 %fd1077,[%rd543],%fd74; }

	// end inline asm
	add.s64 	%rd544, %rd536, 64;
	// begin inline asm
	{ atom.add.f64 %fd1079,[%rd544],%fd73; }

	// end inline asm

$L__BB0_99:
	ld.param.u64 	%rd54, [%rd23];
	ld.param.u32 	%r87, [%rd23+32];
	ld.param.u32 	%r88, [%rd23+60];
	add.s32 	%r89, %r51, 12;
	setp.le.s32 	%p66, %r88, %r89;
	selp.u16 	%rs111, 1, 0, %p66;
	shr.u32 	%r503, %r89, 31;
	cvt.u16.u32 	%rs112, %r503;
	or.b16  	%rs113, %rs111, %rs112;
	setp.eq.s16 	%p67, %rs113, 0;
	@%p67 bra 	$L__BB0_101;

	add.s32 	%r704, %r51, 12;
	st.local.v2.u32 	[%rd25], {%r704, %r88};
	mov.u64 	%rd546, $str;
	cvta.global.u64 	%rd547, %rd546;
	{ // callseq 280, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd547;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r504, [retval0+0];
	} // callseq 280
	bra.uni 	$L__BB0_102;

$L__BB0_101:
	mul.wide.s32 	%rd558, %r87, %r89;
	add.s64 	%rd549, %rd54, %rd558;
	// begin inline asm
	{ atom.add.f64 %fd1081,[%rd549],%fd60; }

	// end inline asm
	add.s64 	%rd550, %rd549, 8;
	// begin inline asm
	{ atom.add.f64 %fd1083,[%rd550],%fd59; }

	// end inline asm
	add.s64 	%rd551, %rd549, 16;
	// begin inline asm
	{ atom.add.f64 %fd1085,[%rd551],%fd58; }

	// end inline asm
	add.s64 	%rd552, %rd549, 24;
	// begin inline asm
	{ atom.add.f64 %fd1087,[%rd552],%fd36; }

	// end inline asm
	add.s64 	%rd553, %rd549, 32;
	// begin inline asm
	{ atom.add.f64 %fd1089,[%rd553],%fd35; }

	// end inline asm
	add.s64 	%rd554, %rd549, 40;
	// begin inline asm
	{ atom.add.f64 %fd1091,[%rd554],%fd34; }

	// end inline asm
	add.s64 	%rd555, %rd549, 48;
	// begin inline asm
	{ atom.add.f64 %fd1093,[%rd555],%fd12; }

	// end inline asm
	add.s64 	%rd556, %rd549, 56;
	// begin inline asm
	{ atom.add.f64 %fd1095,[%rd556],%fd11; }

	// end inline asm
	add.s64 	%rd557, %rd549, 64;
	// begin inline asm
	{ atom.add.f64 %fd1097,[%rd557],%fd10; }

	// end inline asm

$L__BB0_102:
	ld.param.u64 	%rd55, [%rd23];
	ld.param.u32 	%r90, [%rd23+32];
	ld.param.u32 	%r91, [%rd23+60];
	add.s32 	%r92, %r51, 13;
	setp.le.s32 	%p68, %r91, %r92;
	selp.u16 	%rs114, 1, 0, %p68;
	shr.u32 	%r505, %r92, 31;
	cvt.u16.u32 	%rs115, %r505;
	or.b16  	%rs116, %rs114, %rs115;
	setp.eq.s16 	%p69, %rs116, 0;
	@%p69 bra 	$L__BB0_104;

	add.s32 	%r705, %r51, 13;
	st.local.v2.u32 	[%rd25], {%r705, %r91};
	mov.u64 	%rd559, $str;
	cvta.global.u64 	%rd560, %rd559;
	{ // callseq 281, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd560;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r506, [retval0+0];
	} // callseq 281
	bra.uni 	$L__BB0_105;

$L__BB0_104:
	mul.wide.s32 	%rd571, %r90, %r92;
	add.s64 	%rd562, %rd55, %rd571;
	// begin inline asm
	{ atom.add.f64 %fd1099,[%rd562],%fd57; }

	// end inline asm
	add.s64 	%rd563, %rd562, 8;
	// begin inline asm
	{ atom.add.f64 %fd1101,[%rd563],%fd56; }

	// end inline asm
	add.s64 	%rd564, %rd562, 16;
	// begin inline asm
	{ atom.add.f64 %fd1103,[%rd564],%fd55; }

	// end inline asm
	add.s64 	%rd565, %rd562, 24;
	// begin inline asm
	{ atom.add.f64 %fd1105,[%rd565],%fd33; }

	// end inline asm
	add.s64 	%rd566, %rd562, 32;
	// begin inline asm
	{ atom.add.f64 %fd1107,[%rd566],%fd32; }

	// end inline asm
	add.s64 	%rd567, %rd562, 40;
	// begin inline asm
	{ atom.add.f64 %fd1109,[%rd567],%fd31; }

	// end inline asm
	add.s64 	%rd568, %rd562, 48;
	// begin inline asm
	{ atom.add.f64 %fd1111,[%rd568],%fd9; }

	// end inline asm
	add.s64 	%rd569, %rd562, 56;
	// begin inline asm
	{ atom.add.f64 %fd1113,[%rd569],%fd8; }

	// end inline asm
	add.s64 	%rd570, %rd562, 64;
	// begin inline asm
	{ atom.add.f64 %fd1115,[%rd570],%fd7; }

	// end inline asm

$L__BB0_105:
	ld.param.u64 	%rd56, [%rd23];
	ld.param.u32 	%r93, [%rd23+32];
	ld.param.u32 	%r94, [%rd23+60];
	add.s32 	%r95, %r51, 14;
	setp.le.s32 	%p70, %r94, %r95;
	selp.u16 	%rs117, 1, 0, %p70;
	shr.u32 	%r507, %r95, 31;
	cvt.u16.u32 	%rs118, %r507;
	or.b16  	%rs119, %rs117, %rs118;
	setp.eq.s16 	%p71, %rs119, 0;
	@%p71 bra 	$L__BB0_107;

	add.s32 	%r706, %r51, 14;
	st.local.v2.u32 	[%rd25], {%r706, %r94};
	mov.u64 	%rd572, $str;
	cvta.global.u64 	%rd573, %rd572;
	{ // callseq 282, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd573;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r508, [retval0+0];
	} // callseq 282
	bra.uni 	$L__BB0_108;

$L__BB0_107:
	mul.wide.s32 	%rd584, %r93, %r95;
	add.s64 	%rd575, %rd56, %rd584;
	// begin inline asm
	{ atom.add.f64 %fd1117,[%rd575],%fd54; }

	// end inline asm
	add.s64 	%rd576, %rd575, 8;
	// begin inline asm
	{ atom.add.f64 %fd1119,[%rd576],%fd53; }

	// end inline asm
	add.s64 	%rd577, %rd575, 16;
	// begin inline asm
	{ atom.add.f64 %fd1121,[%rd577],%fd52; }

	// end inline asm
	add.s64 	%rd578, %rd575, 24;
	// begin inline asm
	{ atom.add.f64 %fd1123,[%rd578],%fd30; }

	// end inline asm
	add.s64 	%rd579, %rd575, 32;
	// begin inline asm
	{ atom.add.f64 %fd1125,[%rd579],%fd29; }

	// end inline asm
	add.s64 	%rd580, %rd575, 40;
	// begin inline asm
	{ atom.add.f64 %fd1127,[%rd580],%fd28; }

	// end inline asm
	add.s64 	%rd581, %rd575, 48;
	// begin inline asm
	{ atom.add.f64 %fd1129,[%rd581],%fd6; }

	// end inline asm
	add.s64 	%rd582, %rd575, 56;
	// begin inline asm
	{ atom.add.f64 %fd1131,[%rd582],%fd5; }

	// end inline asm
	add.s64 	%rd583, %rd575, 64;
	// begin inline asm
	{ atom.add.f64 %fd1133,[%rd583],%fd4; }

	// end inline asm

$L__BB0_108:
	ld.param.u64 	%rd57, [%rd23];
	ld.param.u32 	%r96, [%rd23+32];
	ld.param.u32 	%r97, [%rd23+60];
	add.s32 	%r98, %r51, 15;
	setp.le.s32 	%p72, %r97, %r98;
	selp.u16 	%rs120, 1, 0, %p72;
	shr.u32 	%r509, %r98, 31;
	cvt.u16.u32 	%rs121, %r509;
	or.b16  	%rs122, %rs120, %rs121;
	setp.eq.s16 	%p73, %rs122, 0;
	@%p73 bra 	$L__BB0_110;

	add.s32 	%r707, %r51, 15;
	st.local.v2.u32 	[%rd25], {%r707, %r97};
	mov.u64 	%rd585, $str;
	cvta.global.u64 	%rd586, %rd585;
	{ // callseq 283, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd586;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r510, [retval0+0];
	} // callseq 283
	bra.uni 	$L__BB0_111;

$L__BB0_110:
	mul.wide.s32 	%rd597, %r96, %r98;
	add.s64 	%rd588, %rd57, %rd597;
	// begin inline asm
	{ atom.add.f64 %fd1135,[%rd588],%fd51; }

	// end inline asm
	add.s64 	%rd589, %rd588, 8;
	// begin inline asm
	{ atom.add.f64 %fd1137,[%rd589],%fd50; }

	// end inline asm
	add.s64 	%rd590, %rd588, 16;
	// begin inline asm
	{ atom.add.f64 %fd1139,[%rd590],%fd49; }

	// end inline asm
	add.s64 	%rd591, %rd588, 24;
	// begin inline asm
	{ atom.add.f64 %fd1141,[%rd591],%fd27; }

	// end inline asm
	add.s64 	%rd592, %rd588, 32;
	// begin inline asm
	{ atom.add.f64 %fd1143,[%rd592],%fd26; }

	// end inline asm
	add.s64 	%rd593, %rd588, 40;
	// begin inline asm
	{ atom.add.f64 %fd1145,[%rd593],%fd25; }

	// end inline asm
	add.s64 	%rd594, %rd588, 48;
	// begin inline asm
	{ atom.add.f64 %fd1147,[%rd594],%fd3; }

	// end inline asm
	add.s64 	%rd595, %rd588, 56;
	// begin inline asm
	{ atom.add.f64 %fd1149,[%rd595],%fd2; }

	// end inline asm
	add.s64 	%rd596, %rd588, 64;
	// begin inline asm
	{ atom.add.f64 %fd1151,[%rd596],%fd1; }

	// end inline asm

$L__BB0_111:
	ld.global.u32 	%r511, [%rd22];
	shl.b32 	%r99, %r511, 2;
	ld.global.u32 	%r512, [%rd41];
	shl.b32 	%r100, %r512, 2;
	ld.param.u64 	%rd59, [%rd58];
	ld.param.u32 	%r101, [%rd58+32];
	ld.param.u64 	%rd60, [%rd58+56];
	ld.param.u32 	%r102, [%rd58+88];
	ld.param.u64 	%rd61, [%rd58+112];
	ld.param.u32 	%r103, [%rd58+144];
	ld.param.u32 	%r104, [%rd58+172];
	ld.param.v2.u32 	{%r513, %r514}, [%rd58+176];
	setp.le.s32 	%p74, %r513, %r99;
	setp.le.s32 	%p75, %r514, %r100;
	shl.b32 	%r108, %r2, 4;
	setp.le.s32 	%p76, %r104, %r108;
	or.pred  	%p77, %p74, %p75;
	or.b32  	%r515, %r99, %r108;
	or.b32  	%r516, %r515, %r100;
	setp.lt.s32 	%p78, %r516, 0;
	or.pred  	%p79, %p78, %p77;
	or.pred  	%p80, %p76, %p79;
	@%p80 bra 	$L__BB0_113;
	bra.uni 	$L__BB0_112;

$L__BB0_113:
	st.local.v2.u32 	[%rd25], {%r99, %r100};
	st.local.v2.u32 	[%rd25+8], {%r108, %r513};
	st.local.v2.u32 	[%rd25+16], {%r514, %r104};
	mov.u64 	%rd615, $str$1;
	cvta.global.u64 	%rd616, %rd615;
	{ // callseq 284, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd616;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r517, [retval0+0];
	} // callseq 284
	bra.uni 	$L__BB0_114;

$L__BB0_112:
	cvta.to.global.u64 	%rd608, %rd60;
	mul.wide.s32 	%rd609, %r103, %r108;
	add.s64 	%rd599, %rd61, %rd609;
	// begin inline asm
	{ atom.add.f64 %fd1153,[%rd599],%fd564; }

	// end inline asm
	add.s64 	%rd600, %rd599, 8;
	// begin inline asm
	{ atom.add.f64 %fd1155,[%rd600],%fd563; }

	// end inline asm
	add.s64 	%rd601, %rd599, 16;
	// begin inline asm
	{ atom.add.f64 %fd1157,[%rd601],%fd562; }

	// end inline asm
	add.s64 	%rd602, %rd599, 24;
	// begin inline asm
	{ atom.add.f64 %fd1159,[%rd602],%fd540; }

	// end inline asm
	add.s64 	%rd603, %rd599, 32;
	// begin inline asm
	{ atom.add.f64 %fd1161,[%rd603],%fd539; }

	// end inline asm
	add.s64 	%rd604, %rd599, 40;
	// begin inline asm
	{ atom.add.f64 %fd1163,[%rd604],%fd538; }

	// end inline asm
	add.s64 	%rd605, %rd599, 48;
	// begin inline asm
	{ atom.add.f64 %fd1165,[%rd605],%fd516; }

	// end inline asm
	add.s64 	%rd606, %rd599, 56;
	// begin inline asm
	{ atom.add.f64 %fd1167,[%rd606],%fd515; }

	// end inline asm
	add.s64 	%rd607, %rd599, 64;
	// begin inline asm
	{ atom.add.f64 %fd1169,[%rd607],%fd514; }

	// end inline asm
	mul.wide.s32 	%rd610, %r101, %r108;
	cvta.to.global.u64 	%rd611, %rd59;
	add.s64 	%rd612, %rd611, %rd610;
	mul.wide.s32 	%rd613, %r102, %r108;
	add.s64 	%rd614, %rd608, %rd613;
	st.global.u32 	[%rd612], %r99;
	st.global.u32 	[%rd614], %r100;

$L__BB0_114:
	ld.param.u64 	%rd63, [%rd58];
	ld.param.u32 	%r109, [%rd58+32];
	ld.param.u64 	%rd64, [%rd58+56];
	ld.param.u32 	%r110, [%rd58+88];
	ld.param.u64 	%rd65, [%rd58+112];
	ld.param.u32 	%r111, [%rd58+144];
	ld.param.u32 	%r112, [%rd58+172];
	ld.param.v2.u32 	{%r518, %r519}, [%rd58+176];
	setp.le.s32 	%p81, %r518, %r99;
	add.s32 	%r116, %r100, 1;
	setp.le.s32 	%p82, %r519, %r116;
	add.s32 	%r117, %r108, 1;
	setp.le.s32 	%p83, %r112, %r117;
	or.pred  	%p84, %p81, %p82;
	or.b32  	%r520, %r99, %r117;
	or.b32  	%r521, %r520, %r116;
	setp.lt.s32 	%p85, %r521, 0;
	or.pred  	%p86, %p85, %p84;
	or.pred  	%p87, %p83, %p86;
	@%p87 bra 	$L__BB0_116;
	bra.uni 	$L__BB0_115;

$L__BB0_116:
	add.s32 	%r709, %r100, 1;
	st.local.v2.u32 	[%rd25], {%r99, %r709};
	add.s32 	%r710, %r108, 1;
	st.local.v2.u32 	[%rd25+8], {%r710, %r518};
	st.local.v2.u32 	[%rd25+16], {%r519, %r112};
	mov.u64 	%rd634, $str$1;
	cvta.global.u64 	%rd635, %rd634;
	{ // callseq 285, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd635;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r522, [retval0+0];
	} // callseq 285
	bra.uni 	$L__BB0_117;

$L__BB0_115:
	cvta.to.global.u64 	%rd627, %rd64;
	mul.wide.s32 	%rd628, %r111, %r117;
	add.s64 	%rd618, %rd65, %rd628;
	// begin inline asm
	{ atom.add.f64 %fd1171,[%rd618],%fd561; }

	// end inline asm
	add.s64 	%rd619, %rd618, 8;
	// begin inline asm
	{ atom.add.f64 %fd1173,[%rd619],%fd560; }

	// end inline asm
	add.s64 	%rd620, %rd618, 16;
	// begin inline asm
	{ atom.add.f64 %fd1175,[%rd620],%fd559; }

	// end inline asm
	add.s64 	%rd621, %rd618, 24;
	// begin inline asm
	{ atom.add.f64 %fd1177,[%rd621],%fd537; }

	// end inline asm
	add.s64 	%rd622, %rd618, 32;
	// begin inline asm
	{ atom.add.f64 %fd1179,[%rd622],%fd536; }

	// end inline asm
	add.s64 	%rd623, %rd618, 40;
	// begin inline asm
	{ atom.add.f64 %fd1181,[%rd623],%fd535; }

	// end inline asm
	add.s64 	%rd624, %rd618, 48;
	// begin inline asm
	{ atom.add.f64 %fd1183,[%rd624],%fd513; }

	// end inline asm
	add.s64 	%rd625, %rd618, 56;
	// begin inline asm
	{ atom.add.f64 %fd1185,[%rd625],%fd512; }

	// end inline asm
	add.s64 	%rd626, %rd618, 64;
	// begin inline asm
	{ atom.add.f64 %fd1187,[%rd626],%fd511; }

	// end inline asm
	mul.wide.s32 	%rd629, %r109, %r117;
	cvta.to.global.u64 	%rd630, %rd63;
	add.s64 	%rd631, %rd630, %rd629;
	mul.wide.s32 	%rd632, %r110, %r117;
	add.s64 	%rd633, %rd627, %rd632;
	st.global.u32 	[%rd631], %r99;
	add.s32 	%r708, %r100, 1;
	st.global.u32 	[%rd633], %r708;

$L__BB0_117:
	ld.param.u64 	%rd66, [%rd58];
	ld.param.u32 	%r118, [%rd58+32];
	ld.param.u64 	%rd67, [%rd58+56];
	ld.param.u32 	%r119, [%rd58+88];
	ld.param.u64 	%rd68, [%rd58+112];
	ld.param.u32 	%r120, [%rd58+144];
	ld.param.u32 	%r121, [%rd58+172];
	ld.param.v2.u32 	{%r523, %r524}, [%rd58+176];
	setp.le.s32 	%p88, %r523, %r99;
	add.s32 	%r125, %r100, 2;
	setp.le.s32 	%p89, %r524, %r125;
	add.s32 	%r126, %r108, 2;
	setp.le.s32 	%p90, %r121, %r126;
	or.pred  	%p91, %p88, %p89;
	or.b32  	%r525, %r99, %r126;
	or.b32  	%r526, %r525, %r125;
	setp.lt.s32 	%p92, %r526, 0;
	or.pred  	%p93, %p92, %p91;
	or.pred  	%p94, %p90, %p93;
	@%p94 bra 	$L__BB0_119;
	bra.uni 	$L__BB0_118;

$L__BB0_119:
	add.s32 	%r712, %r100, 2;
	st.local.v2.u32 	[%rd25], {%r99, %r712};
	add.s32 	%r713, %r108, 2;
	st.local.v2.u32 	[%rd25+8], {%r713, %r523};
	st.local.v2.u32 	[%rd25+16], {%r524, %r121};
	mov.u64 	%rd653, $str$1;
	cvta.global.u64 	%rd654, %rd653;
	{ // callseq 286, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd654;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r527, [retval0+0];
	} // callseq 286
	bra.uni 	$L__BB0_120;

$L__BB0_118:
	cvta.to.global.u64 	%rd646, %rd67;
	mul.wide.s32 	%rd647, %r120, %r126;
	add.s64 	%rd637, %rd68, %rd647;
	// begin inline asm
	{ atom.add.f64 %fd1189,[%rd637],%fd558; }

	// end inline asm
	add.s64 	%rd638, %rd637, 8;
	// begin inline asm
	{ atom.add.f64 %fd1191,[%rd638],%fd557; }

	// end inline asm
	add.s64 	%rd639, %rd637, 16;
	// begin inline asm
	{ atom.add.f64 %fd1193,[%rd639],%fd556; }

	// end inline asm
	add.s64 	%rd640, %rd637, 24;
	// begin inline asm
	{ atom.add.f64 %fd1195,[%rd640],%fd534; }

	// end inline asm
	add.s64 	%rd641, %rd637, 32;
	// begin inline asm
	{ atom.add.f64 %fd1197,[%rd641],%fd533; }

	// end inline asm
	add.s64 	%rd642, %rd637, 40;
	// begin inline asm
	{ atom.add.f64 %fd1199,[%rd642],%fd532; }

	// end inline asm
	add.s64 	%rd643, %rd637, 48;
	// begin inline asm
	{ atom.add.f64 %fd1201,[%rd643],%fd510; }

	// end inline asm
	add.s64 	%rd644, %rd637, 56;
	// begin inline asm
	{ atom.add.f64 %fd1203,[%rd644],%fd509; }

	// end inline asm
	add.s64 	%rd645, %rd637, 64;
	// begin inline asm
	{ atom.add.f64 %fd1205,[%rd645],%fd508; }

	// end inline asm
	mul.wide.s32 	%rd648, %r118, %r126;
	cvta.to.global.u64 	%rd649, %rd66;
	add.s64 	%rd650, %rd649, %rd648;
	mul.wide.s32 	%rd651, %r119, %r126;
	add.s64 	%rd652, %rd646, %rd651;
	st.global.u32 	[%rd650], %r99;
	add.s32 	%r711, %r100, 2;
	st.global.u32 	[%rd652], %r711;

$L__BB0_120:
	ld.param.u64 	%rd69, [%rd58];
	ld.param.u32 	%r127, [%rd58+32];
	ld.param.u64 	%rd70, [%rd58+56];
	ld.param.u32 	%r128, [%rd58+88];
	ld.param.u64 	%rd71, [%rd58+112];
	ld.param.u32 	%r129, [%rd58+144];
	ld.param.u32 	%r130, [%rd58+172];
	ld.param.v2.u32 	{%r528, %r529}, [%rd58+176];
	setp.le.s32 	%p95, %r528, %r99;
	add.s32 	%r134, %r100, 3;
	setp.le.s32 	%p96, %r529, %r134;
	add.s32 	%r135, %r108, 3;
	setp.le.s32 	%p97, %r130, %r135;
	or.pred  	%p98, %p95, %p96;
	or.b32  	%r530, %r99, %r135;
	or.b32  	%r531, %r530, %r134;
	setp.lt.s32 	%p99, %r531, 0;
	or.pred  	%p100, %p99, %p98;
	or.pred  	%p101, %p97, %p100;
	@%p101 bra 	$L__BB0_122;
	bra.uni 	$L__BB0_121;

$L__BB0_122:
	add.s32 	%r715, %r100, 3;
	st.local.v2.u32 	[%rd25], {%r99, %r715};
	add.s32 	%r716, %r108, 3;
	st.local.v2.u32 	[%rd25+8], {%r716, %r528};
	st.local.v2.u32 	[%rd25+16], {%r529, %r130};
	mov.u64 	%rd672, $str$1;
	cvta.global.u64 	%rd673, %rd672;
	{ // callseq 287, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd673;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r532, [retval0+0];
	} // callseq 287
	bra.uni 	$L__BB0_123;

$L__BB0_121:
	cvta.to.global.u64 	%rd665, %rd70;
	mul.wide.s32 	%rd666, %r129, %r135;
	add.s64 	%rd656, %rd71, %rd666;
	// begin inline asm
	{ atom.add.f64 %fd1207,[%rd656],%fd555; }

	// end inline asm
	add.s64 	%rd657, %rd656, 8;
	// begin inline asm
	{ atom.add.f64 %fd1209,[%rd657],%fd554; }

	// end inline asm
	add.s64 	%rd658, %rd656, 16;
	// begin inline asm
	{ atom.add.f64 %fd1211,[%rd658],%fd553; }

	// end inline asm
	add.s64 	%rd659, %rd656, 24;
	// begin inline asm
	{ atom.add.f64 %fd1213,[%rd659],%fd531; }

	// end inline asm
	add.s64 	%rd660, %rd656, 32;
	// begin inline asm
	{ atom.add.f64 %fd1215,[%rd660],%fd530; }

	// end inline asm
	add.s64 	%rd661, %rd656, 40;
	// begin inline asm
	{ atom.add.f64 %fd1217,[%rd661],%fd529; }

	// end inline asm
	add.s64 	%rd662, %rd656, 48;
	// begin inline asm
	{ atom.add.f64 %fd1219,[%rd662],%fd507; }

	// end inline asm
	add.s64 	%rd663, %rd656, 56;
	// begin inline asm
	{ atom.add.f64 %fd1221,[%rd663],%fd506; }

	// end inline asm
	add.s64 	%rd664, %rd656, 64;
	// begin inline asm
	{ atom.add.f64 %fd1223,[%rd664],%fd505; }

	// end inline asm
	mul.wide.s32 	%rd667, %r127, %r135;
	cvta.to.global.u64 	%rd668, %rd69;
	add.s64 	%rd669, %rd668, %rd667;
	mul.wide.s32 	%rd670, %r128, %r135;
	add.s64 	%rd671, %rd665, %rd670;
	st.global.u32 	[%rd669], %r99;
	add.s32 	%r714, %r100, 3;
	st.global.u32 	[%rd671], %r714;

$L__BB0_123:
	ld.param.u64 	%rd72, [%rd58];
	ld.param.u32 	%r136, [%rd58+32];
	ld.param.u64 	%rd73, [%rd58+56];
	ld.param.u32 	%r137, [%rd58+88];
	ld.param.u64 	%rd74, [%rd58+112];
	ld.param.u32 	%r138, [%rd58+144];
	ld.param.u32 	%r139, [%rd58+172];
	ld.param.v2.u32 	{%r533, %r534}, [%rd58+176];
	add.s32 	%r143, %r99, 1;
	setp.le.s32 	%p102, %r533, %r143;
	setp.le.s32 	%p103, %r534, %r100;
	add.s32 	%r144, %r108, 4;
	setp.le.s32 	%p104, %r139, %r144;
	or.pred  	%p105, %p102, %p103;
	or.b32  	%r535, %r100, %r144;
	or.b32  	%r536, %r535, %r143;
	setp.lt.s32 	%p106, %r536, 0;
	or.pred  	%p107, %p106, %p105;
	or.pred  	%p108, %p104, %p107;
	@%p108 bra 	$L__BB0_125;
	bra.uni 	$L__BB0_124;

$L__BB0_125:
	add.s32 	%r718, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r718, %r100};
	add.s32 	%r719, %r108, 4;
	st.local.v2.u32 	[%rd25+8], {%r719, %r533};
	st.local.v2.u32 	[%rd25+16], {%r534, %r139};
	mov.u64 	%rd691, $str$1;
	cvta.global.u64 	%rd692, %rd691;
	{ // callseq 288, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd692;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r537, [retval0+0];
	} // callseq 288
	bra.uni 	$L__BB0_126;

$L__BB0_124:
	cvta.to.global.u64 	%rd684, %rd73;
	mul.wide.s32 	%rd685, %r138, %r144;
	add.s64 	%rd675, %rd74, %rd685;
	// begin inline asm
	{ atom.add.f64 %fd1225,[%rd675],%fd492; }

	// end inline asm
	add.s64 	%rd676, %rd675, 8;
	// begin inline asm
	{ atom.add.f64 %fd1227,[%rd676],%fd491; }

	// end inline asm
	add.s64 	%rd677, %rd675, 16;
	// begin inline asm
	{ atom.add.f64 %fd1229,[%rd677],%fd490; }

	// end inline asm
	add.s64 	%rd678, %rd675, 24;
	// begin inline asm
	{ atom.add.f64 %fd1231,[%rd678],%fd468; }

	// end inline asm
	add.s64 	%rd679, %rd675, 32;
	// begin inline asm
	{ atom.add.f64 %fd1233,[%rd679],%fd467; }

	// end inline asm
	add.s64 	%rd680, %rd675, 40;
	// begin inline asm
	{ atom.add.f64 %fd1235,[%rd680],%fd466; }

	// end inline asm
	add.s64 	%rd681, %rd675, 48;
	// begin inline asm
	{ atom.add.f64 %fd1237,[%rd681],%fd444; }

	// end inline asm
	add.s64 	%rd682, %rd675, 56;
	// begin inline asm
	{ atom.add.f64 %fd1239,[%rd682],%fd443; }

	// end inline asm
	add.s64 	%rd683, %rd675, 64;
	// begin inline asm
	{ atom.add.f64 %fd1241,[%rd683],%fd442; }

	// end inline asm
	mul.wide.s32 	%rd686, %r136, %r144;
	cvta.to.global.u64 	%rd687, %rd72;
	add.s64 	%rd688, %rd687, %rd686;
	mul.wide.s32 	%rd689, %r137, %r144;
	add.s64 	%rd690, %rd684, %rd689;
	add.s32 	%r717, %r99, 1;
	st.global.u32 	[%rd688], %r717;
	st.global.u32 	[%rd690], %r100;

$L__BB0_126:
	ld.param.u64 	%rd75, [%rd58];
	ld.param.u32 	%r145, [%rd58+32];
	ld.param.u64 	%rd76, [%rd58+56];
	ld.param.u32 	%r146, [%rd58+88];
	ld.param.u64 	%rd77, [%rd58+112];
	ld.param.u32 	%r147, [%rd58+144];
	ld.param.u32 	%r148, [%rd58+172];
	ld.param.v2.u32 	{%r538, %r539}, [%rd58+176];
	setp.le.s32 	%p109, %r538, %r143;
	setp.le.s32 	%p110, %r539, %r116;
	add.s32 	%r152, %r108, 5;
	setp.le.s32 	%p111, %r148, %r152;
	or.pred  	%p112, %p109, %p110;
	or.b32  	%r540, %r143, %r152;
	or.b32  	%r541, %r540, %r116;
	setp.lt.s32 	%p113, %r541, 0;
	or.pred  	%p114, %p113, %p112;
	or.pred  	%p115, %p111, %p114;
	@%p115 bra 	$L__BB0_128;
	bra.uni 	$L__BB0_127;

$L__BB0_128:
	add.s32 	%r722, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r722, %r116};
	add.s32 	%r723, %r108, 5;
	st.local.v2.u32 	[%rd25+8], {%r723, %r538};
	st.local.v2.u32 	[%rd25+16], {%r539, %r148};
	mov.u64 	%rd710, $str$1;
	cvta.global.u64 	%rd711, %rd710;
	{ // callseq 289, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd711;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r542, [retval0+0];
	} // callseq 289
	bra.uni 	$L__BB0_129;

$L__BB0_127:
	cvta.to.global.u64 	%rd703, %rd76;
	mul.wide.s32 	%rd704, %r147, %r152;
	add.s64 	%rd694, %rd77, %rd704;
	// begin inline asm
	{ atom.add.f64 %fd1243,[%rd694],%fd489; }

	// end inline asm
	add.s64 	%rd695, %rd694, 8;
	// begin inline asm
	{ atom.add.f64 %fd1245,[%rd695],%fd488; }

	// end inline asm
	add.s64 	%rd696, %rd694, 16;
	// begin inline asm
	{ atom.add.f64 %fd1247,[%rd696],%fd487; }

	// end inline asm
	add.s64 	%rd697, %rd694, 24;
	// begin inline asm
	{ atom.add.f64 %fd1249,[%rd697],%fd465; }

	// end inline asm
	add.s64 	%rd698, %rd694, 32;
	// begin inline asm
	{ atom.add.f64 %fd1251,[%rd698],%fd464; }

	// end inline asm
	add.s64 	%rd699, %rd694, 40;
	// begin inline asm
	{ atom.add.f64 %fd1253,[%rd699],%fd463; }

	// end inline asm
	add.s64 	%rd700, %rd694, 48;
	// begin inline asm
	{ atom.add.f64 %fd1255,[%rd700],%fd441; }

	// end inline asm
	add.s64 	%rd701, %rd694, 56;
	// begin inline asm
	{ atom.add.f64 %fd1257,[%rd701],%fd440; }

	// end inline asm
	add.s64 	%rd702, %rd694, 64;
	// begin inline asm
	{ atom.add.f64 %fd1259,[%rd702],%fd439; }

	// end inline asm
	mul.wide.s32 	%rd705, %r145, %r152;
	cvta.to.global.u64 	%rd706, %rd75;
	add.s64 	%rd707, %rd706, %rd705;
	mul.wide.s32 	%rd708, %r146, %r152;
	add.s64 	%rd709, %rd703, %rd708;
	add.s32 	%r720, %r99, 1;
	st.global.u32 	[%rd707], %r720;
	add.s32 	%r721, %r100, 1;
	st.global.u32 	[%rd709], %r721;

$L__BB0_129:
	ld.param.u64 	%rd78, [%rd58];
	ld.param.u32 	%r153, [%rd58+32];
	ld.param.u64 	%rd79, [%rd58+56];
	ld.param.u32 	%r154, [%rd58+88];
	ld.param.u64 	%rd80, [%rd58+112];
	ld.param.u32 	%r155, [%rd58+144];
	ld.param.u32 	%r156, [%rd58+172];
	ld.param.v2.u32 	{%r543, %r544}, [%rd58+176];
	setp.le.s32 	%p116, %r543, %r143;
	setp.le.s32 	%p117, %r544, %r125;
	add.s32 	%r160, %r108, 6;
	setp.le.s32 	%p118, %r156, %r160;
	or.pred  	%p119, %p116, %p117;
	or.b32  	%r545, %r143, %r160;
	or.b32  	%r546, %r545, %r125;
	setp.lt.s32 	%p120, %r546, 0;
	or.pred  	%p121, %p120, %p119;
	or.pred  	%p122, %p118, %p121;
	@%p122 bra 	$L__BB0_131;
	bra.uni 	$L__BB0_130;

$L__BB0_131:
	add.s32 	%r726, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r726, %r125};
	add.s32 	%r727, %r108, 6;
	st.local.v2.u32 	[%rd25+8], {%r727, %r543};
	st.local.v2.u32 	[%rd25+16], {%r544, %r156};
	mov.u64 	%rd729, $str$1;
	cvta.global.u64 	%rd730, %rd729;
	{ // callseq 290, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd730;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r547, [retval0+0];
	} // callseq 290
	bra.uni 	$L__BB0_132;

$L__BB0_130:
	cvta.to.global.u64 	%rd722, %rd79;
	mul.wide.s32 	%rd723, %r155, %r160;
	add.s64 	%rd713, %rd80, %rd723;
	// begin inline asm
	{ atom.add.f64 %fd1261,[%rd713],%fd486; }

	// end inline asm
	add.s64 	%rd714, %rd713, 8;
	// begin inline asm
	{ atom.add.f64 %fd1263,[%rd714],%fd485; }

	// end inline asm
	add.s64 	%rd715, %rd713, 16;
	// begin inline asm
	{ atom.add.f64 %fd1265,[%rd715],%fd484; }

	// end inline asm
	add.s64 	%rd716, %rd713, 24;
	// begin inline asm
	{ atom.add.f64 %fd1267,[%rd716],%fd462; }

	// end inline asm
	add.s64 	%rd717, %rd713, 32;
	// begin inline asm
	{ atom.add.f64 %fd1269,[%rd717],%fd461; }

	// end inline asm
	add.s64 	%rd718, %rd713, 40;
	// begin inline asm
	{ atom.add.f64 %fd1271,[%rd718],%fd460; }

	// end inline asm
	add.s64 	%rd719, %rd713, 48;
	// begin inline asm
	{ atom.add.f64 %fd1273,[%rd719],%fd438; }

	// end inline asm
	add.s64 	%rd720, %rd713, 56;
	// begin inline asm
	{ atom.add.f64 %fd1275,[%rd720],%fd437; }

	// end inline asm
	add.s64 	%rd721, %rd713, 64;
	// begin inline asm
	{ atom.add.f64 %fd1277,[%rd721],%fd436; }

	// end inline asm
	mul.wide.s32 	%rd724, %r153, %r160;
	cvta.to.global.u64 	%rd725, %rd78;
	add.s64 	%rd726, %rd725, %rd724;
	mul.wide.s32 	%rd727, %r154, %r160;
	add.s64 	%rd728, %rd722, %rd727;
	add.s32 	%r724, %r99, 1;
	st.global.u32 	[%rd726], %r724;
	add.s32 	%r725, %r100, 2;
	st.global.u32 	[%rd728], %r725;

$L__BB0_132:
	ld.param.u64 	%rd81, [%rd58];
	ld.param.u32 	%r161, [%rd58+32];
	ld.param.u64 	%rd82, [%rd58+56];
	ld.param.u32 	%r162, [%rd58+88];
	ld.param.u64 	%rd83, [%rd58+112];
	ld.param.u32 	%r163, [%rd58+144];
	ld.param.u32 	%r164, [%rd58+172];
	ld.param.v2.u32 	{%r548, %r549}, [%rd58+176];
	setp.le.s32 	%p123, %r548, %r143;
	setp.le.s32 	%p124, %r549, %r134;
	add.s32 	%r168, %r108, 7;
	setp.le.s32 	%p125, %r164, %r168;
	or.pred  	%p126, %p123, %p124;
	or.b32  	%r550, %r143, %r168;
	or.b32  	%r551, %r550, %r134;
	setp.lt.s32 	%p127, %r551, 0;
	or.pred  	%p128, %p127, %p126;
	or.pred  	%p129, %p125, %p128;
	@%p129 bra 	$L__BB0_134;
	bra.uni 	$L__BB0_133;

$L__BB0_134:
	add.s32 	%r730, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r730, %r134};
	add.s32 	%r731, %r108, 7;
	st.local.v2.u32 	[%rd25+8], {%r731, %r548};
	st.local.v2.u32 	[%rd25+16], {%r549, %r164};
	mov.u64 	%rd748, $str$1;
	cvta.global.u64 	%rd749, %rd748;
	{ // callseq 291, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd749;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r552, [retval0+0];
	} // callseq 291
	bra.uni 	$L__BB0_135;

$L__BB0_133:
	cvta.to.global.u64 	%rd741, %rd82;
	mul.wide.s32 	%rd742, %r163, %r168;
	add.s64 	%rd732, %rd83, %rd742;
	// begin inline asm
	{ atom.add.f64 %fd1279,[%rd732],%fd483; }

	// end inline asm
	add.s64 	%rd733, %rd732, 8;
	// begin inline asm
	{ atom.add.f64 %fd1281,[%rd733],%fd482; }

	// end inline asm
	add.s64 	%rd734, %rd732, 16;
	// begin inline asm
	{ atom.add.f64 %fd1283,[%rd734],%fd481; }

	// end inline asm
	add.s64 	%rd735, %rd732, 24;
	// begin inline asm
	{ atom.add.f64 %fd1285,[%rd735],%fd459; }

	// end inline asm
	add.s64 	%rd736, %rd732, 32;
	// begin inline asm
	{ atom.add.f64 %fd1287,[%rd736],%fd458; }

	// end inline asm
	add.s64 	%rd737, %rd732, 40;
	// begin inline asm
	{ atom.add.f64 %fd1289,[%rd737],%fd457; }

	// end inline asm
	add.s64 	%rd738, %rd732, 48;
	// begin inline asm
	{ atom.add.f64 %fd1291,[%rd738],%fd435; }

	// end inline asm
	add.s64 	%rd739, %rd732, 56;
	// begin inline asm
	{ atom.add.f64 %fd1293,[%rd739],%fd434; }

	// end inline asm
	add.s64 	%rd740, %rd732, 64;
	// begin inline asm
	{ atom.add.f64 %fd1295,[%rd740],%fd433; }

	// end inline asm
	mul.wide.s32 	%rd743, %r161, %r168;
	cvta.to.global.u64 	%rd744, %rd81;
	add.s64 	%rd745, %rd744, %rd743;
	mul.wide.s32 	%rd746, %r162, %r168;
	add.s64 	%rd747, %rd741, %rd746;
	add.s32 	%r728, %r99, 1;
	st.global.u32 	[%rd745], %r728;
	add.s32 	%r729, %r100, 3;
	st.global.u32 	[%rd747], %r729;

$L__BB0_135:
	ld.param.u64 	%rd84, [%rd58];
	ld.param.u32 	%r169, [%rd58+32];
	ld.param.u64 	%rd85, [%rd58+56];
	ld.param.u32 	%r170, [%rd58+88];
	ld.param.u64 	%rd86, [%rd58+112];
	ld.param.u32 	%r171, [%rd58+144];
	ld.param.u32 	%r172, [%rd58+172];
	ld.param.v2.u32 	{%r553, %r554}, [%rd58+176];
	add.s32 	%r176, %r99, 2;
	setp.le.s32 	%p130, %r553, %r176;
	setp.le.s32 	%p131, %r554, %r100;
	add.s32 	%r177, %r108, 8;
	setp.le.s32 	%p132, %r172, %r177;
	or.pred  	%p133, %p130, %p131;
	or.b32  	%r555, %r100, %r177;
	or.b32  	%r556, %r555, %r176;
	setp.lt.s32 	%p134, %r556, 0;
	or.pred  	%p135, %p134, %p133;
	or.pred  	%p136, %p132, %p135;
	@%p136 bra 	$L__BB0_137;
	bra.uni 	$L__BB0_136;

$L__BB0_137:
	add.s32 	%r733, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r733, %r100};
	add.s32 	%r734, %r108, 8;
	st.local.v2.u32 	[%rd25+8], {%r734, %r553};
	st.local.v2.u32 	[%rd25+16], {%r554, %r172};
	mov.u64 	%rd767, $str$1;
	cvta.global.u64 	%rd768, %rd767;
	{ // callseq 292, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd768;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r557, [retval0+0];
	} // callseq 292
	bra.uni 	$L__BB0_138;

$L__BB0_136:
	cvta.to.global.u64 	%rd760, %rd85;
	mul.wide.s32 	%rd761, %r171, %r177;
	add.s64 	%rd751, %rd86, %rd761;
	// begin inline asm
	{ atom.add.f64 %fd1297,[%rd751],%fd420; }

	// end inline asm
	add.s64 	%rd752, %rd751, 8;
	// begin inline asm
	{ atom.add.f64 %fd1299,[%rd752],%fd419; }

	// end inline asm
	add.s64 	%rd753, %rd751, 16;
	// begin inline asm
	{ atom.add.f64 %fd1301,[%rd753],%fd418; }

	// end inline asm
	add.s64 	%rd754, %rd751, 24;
	// begin inline asm
	{ atom.add.f64 %fd1303,[%rd754],%fd396; }

	// end inline asm
	add.s64 	%rd755, %rd751, 32;
	// begin inline asm
	{ atom.add.f64 %fd1305,[%rd755],%fd395; }

	// end inline asm
	add.s64 	%rd756, %rd751, 40;
	// begin inline asm
	{ atom.add.f64 %fd1307,[%rd756],%fd394; }

	// end inline asm
	add.s64 	%rd757, %rd751, 48;
	// begin inline asm
	{ atom.add.f64 %fd1309,[%rd757],%fd372; }

	// end inline asm
	add.s64 	%rd758, %rd751, 56;
	// begin inline asm
	{ atom.add.f64 %fd1311,[%rd758],%fd371; }

	// end inline asm
	add.s64 	%rd759, %rd751, 64;
	// begin inline asm
	{ atom.add.f64 %fd1313,[%rd759],%fd370; }

	// end inline asm
	mul.wide.s32 	%rd762, %r169, %r177;
	cvta.to.global.u64 	%rd763, %rd84;
	add.s64 	%rd764, %rd763, %rd762;
	mul.wide.s32 	%rd765, %r170, %r177;
	add.s64 	%rd766, %rd760, %rd765;
	add.s32 	%r732, %r99, 2;
	st.global.u32 	[%rd764], %r732;
	st.global.u32 	[%rd766], %r100;

$L__BB0_138:
	ld.param.u64 	%rd87, [%rd58];
	ld.param.u32 	%r178, [%rd58+32];
	ld.param.u64 	%rd88, [%rd58+56];
	ld.param.u32 	%r179, [%rd58+88];
	ld.param.u64 	%rd89, [%rd58+112];
	ld.param.u32 	%r180, [%rd58+144];
	ld.param.u32 	%r181, [%rd58+172];
	ld.param.v2.u32 	{%r558, %r559}, [%rd58+176];
	setp.le.s32 	%p137, %r558, %r176;
	setp.le.s32 	%p138, %r559, %r116;
	add.s32 	%r185, %r108, 9;
	setp.le.s32 	%p139, %r181, %r185;
	or.pred  	%p140, %p137, %p138;
	or.b32  	%r560, %r176, %r185;
	or.b32  	%r561, %r560, %r116;
	setp.lt.s32 	%p141, %r561, 0;
	or.pred  	%p142, %p141, %p140;
	or.pred  	%p143, %p139, %p142;
	@%p143 bra 	$L__BB0_140;
	bra.uni 	$L__BB0_139;

$L__BB0_140:
	add.s32 	%r737, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r737, %r116};
	add.s32 	%r738, %r108, 9;
	st.local.v2.u32 	[%rd25+8], {%r738, %r558};
	st.local.v2.u32 	[%rd25+16], {%r559, %r181};
	mov.u64 	%rd786, $str$1;
	cvta.global.u64 	%rd787, %rd786;
	{ // callseq 293, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd787;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r562, [retval0+0];
	} // callseq 293
	bra.uni 	$L__BB0_141;

$L__BB0_139:
	cvta.to.global.u64 	%rd779, %rd88;
	mul.wide.s32 	%rd780, %r180, %r185;
	add.s64 	%rd770, %rd89, %rd780;
	// begin inline asm
	{ atom.add.f64 %fd1315,[%rd770],%fd417; }

	// end inline asm
	add.s64 	%rd771, %rd770, 8;
	// begin inline asm
	{ atom.add.f64 %fd1317,[%rd771],%fd416; }

	// end inline asm
	add.s64 	%rd772, %rd770, 16;
	// begin inline asm
	{ atom.add.f64 %fd1319,[%rd772],%fd415; }

	// end inline asm
	add.s64 	%rd773, %rd770, 24;
	// begin inline asm
	{ atom.add.f64 %fd1321,[%rd773],%fd393; }

	// end inline asm
	add.s64 	%rd774, %rd770, 32;
	// begin inline asm
	{ atom.add.f64 %fd1323,[%rd774],%fd392; }

	// end inline asm
	add.s64 	%rd775, %rd770, 40;
	// begin inline asm
	{ atom.add.f64 %fd1325,[%rd775],%fd391; }

	// end inline asm
	add.s64 	%rd776, %rd770, 48;
	// begin inline asm
	{ atom.add.f64 %fd1327,[%rd776],%fd369; }

	// end inline asm
	add.s64 	%rd777, %rd770, 56;
	// begin inline asm
	{ atom.add.f64 %fd1329,[%rd777],%fd368; }

	// end inline asm
	add.s64 	%rd778, %rd770, 64;
	// begin inline asm
	{ atom.add.f64 %fd1331,[%rd778],%fd367; }

	// end inline asm
	mul.wide.s32 	%rd781, %r178, %r185;
	cvta.to.global.u64 	%rd782, %rd87;
	add.s64 	%rd783, %rd782, %rd781;
	mul.wide.s32 	%rd784, %r179, %r185;
	add.s64 	%rd785, %rd779, %rd784;
	add.s32 	%r735, %r99, 2;
	st.global.u32 	[%rd783], %r735;
	add.s32 	%r736, %r100, 1;
	st.global.u32 	[%rd785], %r736;

$L__BB0_141:
	ld.param.u64 	%rd90, [%rd58];
	ld.param.u32 	%r186, [%rd58+32];
	ld.param.u64 	%rd91, [%rd58+56];
	ld.param.u32 	%r187, [%rd58+88];
	ld.param.u64 	%rd92, [%rd58+112];
	ld.param.u32 	%r188, [%rd58+144];
	ld.param.u32 	%r189, [%rd58+172];
	ld.param.v2.u32 	{%r563, %r564}, [%rd58+176];
	setp.le.s32 	%p144, %r563, %r176;
	setp.le.s32 	%p145, %r564, %r125;
	add.s32 	%r193, %r108, 10;
	setp.le.s32 	%p146, %r189, %r193;
	or.pred  	%p147, %p144, %p145;
	or.b32  	%r565, %r176, %r193;
	or.b32  	%r566, %r565, %r125;
	setp.lt.s32 	%p148, %r566, 0;
	or.pred  	%p149, %p148, %p147;
	or.pred  	%p150, %p146, %p149;
	@%p150 bra 	$L__BB0_143;
	bra.uni 	$L__BB0_142;

$L__BB0_143:
	add.s32 	%r741, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r741, %r125};
	add.s32 	%r742, %r108, 10;
	st.local.v2.u32 	[%rd25+8], {%r742, %r563};
	st.local.v2.u32 	[%rd25+16], {%r564, %r189};
	mov.u64 	%rd805, $str$1;
	cvta.global.u64 	%rd806, %rd805;
	{ // callseq 294, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd806;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r567, [retval0+0];
	} // callseq 294
	bra.uni 	$L__BB0_144;

$L__BB0_142:
	cvta.to.global.u64 	%rd798, %rd91;
	mul.wide.s32 	%rd799, %r188, %r193;
	add.s64 	%rd789, %rd92, %rd799;
	// begin inline asm
	{ atom.add.f64 %fd1333,[%rd789],%fd414; }

	// end inline asm
	add.s64 	%rd790, %rd789, 8;
	// begin inline asm
	{ atom.add.f64 %fd1335,[%rd790],%fd413; }

	// end inline asm
	add.s64 	%rd791, %rd789, 16;
	// begin inline asm
	{ atom.add.f64 %fd1337,[%rd791],%fd412; }

	// end inline asm
	add.s64 	%rd792, %rd789, 24;
	// begin inline asm
	{ atom.add.f64 %fd1339,[%rd792],%fd390; }

	// end inline asm
	add.s64 	%rd793, %rd789, 32;
	// begin inline asm
	{ atom.add.f64 %fd1341,[%rd793],%fd389; }

	// end inline asm
	add.s64 	%rd794, %rd789, 40;
	// begin inline asm
	{ atom.add.f64 %fd1343,[%rd794],%fd388; }

	// end inline asm
	add.s64 	%rd795, %rd789, 48;
	// begin inline asm
	{ atom.add.f64 %fd1345,[%rd795],%fd366; }

	// end inline asm
	add.s64 	%rd796, %rd789, 56;
	// begin inline asm
	{ atom.add.f64 %fd1347,[%rd796],%fd365; }

	// end inline asm
	add.s64 	%rd797, %rd789, 64;
	// begin inline asm
	{ atom.add.f64 %fd1349,[%rd797],%fd364; }

	// end inline asm
	mul.wide.s32 	%rd800, %r186, %r193;
	cvta.to.global.u64 	%rd801, %rd90;
	add.s64 	%rd802, %rd801, %rd800;
	mul.wide.s32 	%rd803, %r187, %r193;
	add.s64 	%rd804, %rd798, %rd803;
	add.s32 	%r739, %r99, 2;
	st.global.u32 	[%rd802], %r739;
	add.s32 	%r740, %r100, 2;
	st.global.u32 	[%rd804], %r740;

$L__BB0_144:
	ld.param.u64 	%rd93, [%rd58];
	ld.param.u32 	%r194, [%rd58+32];
	ld.param.u64 	%rd94, [%rd58+56];
	ld.param.u32 	%r195, [%rd58+88];
	ld.param.u64 	%rd95, [%rd58+112];
	ld.param.u32 	%r196, [%rd58+144];
	ld.param.u32 	%r197, [%rd58+172];
	ld.param.v2.u32 	{%r568, %r569}, [%rd58+176];
	setp.le.s32 	%p151, %r568, %r176;
	setp.le.s32 	%p152, %r569, %r134;
	add.s32 	%r201, %r108, 11;
	setp.le.s32 	%p153, %r197, %r201;
	or.pred  	%p154, %p151, %p152;
	or.b32  	%r570, %r176, %r201;
	or.b32  	%r571, %r570, %r134;
	setp.lt.s32 	%p155, %r571, 0;
	or.pred  	%p156, %p155, %p154;
	or.pred  	%p157, %p153, %p156;
	@%p157 bra 	$L__BB0_146;
	bra.uni 	$L__BB0_145;

$L__BB0_146:
	add.s32 	%r745, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r745, %r134};
	add.s32 	%r746, %r108, 11;
	st.local.v2.u32 	[%rd25+8], {%r746, %r568};
	st.local.v2.u32 	[%rd25+16], {%r569, %r197};
	mov.u64 	%rd824, $str$1;
	cvta.global.u64 	%rd825, %rd824;
	{ // callseq 295, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd825;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r572, [retval0+0];
	} // callseq 295
	bra.uni 	$L__BB0_147;

$L__BB0_145:
	cvta.to.global.u64 	%rd817, %rd94;
	mul.wide.s32 	%rd818, %r196, %r201;
	add.s64 	%rd808, %rd95, %rd818;
	// begin inline asm
	{ atom.add.f64 %fd1351,[%rd808],%fd411; }

	// end inline asm
	add.s64 	%rd809, %rd808, 8;
	// begin inline asm
	{ atom.add.f64 %fd1353,[%rd809],%fd410; }

	// end inline asm
	add.s64 	%rd810, %rd808, 16;
	// begin inline asm
	{ atom.add.f64 %fd1355,[%rd810],%fd409; }

	// end inline asm
	add.s64 	%rd811, %rd808, 24;
	// begin inline asm
	{ atom.add.f64 %fd1357,[%rd811],%fd387; }

	// end inline asm
	add.s64 	%rd812, %rd808, 32;
	// begin inline asm
	{ atom.add.f64 %fd1359,[%rd812],%fd386; }

	// end inline asm
	add.s64 	%rd813, %rd808, 40;
	// begin inline asm
	{ atom.add.f64 %fd1361,[%rd813],%fd385; }

	// end inline asm
	add.s64 	%rd814, %rd808, 48;
	// begin inline asm
	{ atom.add.f64 %fd1363,[%rd814],%fd363; }

	// end inline asm
	add.s64 	%rd815, %rd808, 56;
	// begin inline asm
	{ atom.add.f64 %fd1365,[%rd815],%fd362; }

	// end inline asm
	add.s64 	%rd816, %rd808, 64;
	// begin inline asm
	{ atom.add.f64 %fd1367,[%rd816],%fd361; }

	// end inline asm
	mul.wide.s32 	%rd819, %r194, %r201;
	cvta.to.global.u64 	%rd820, %rd93;
	add.s64 	%rd821, %rd820, %rd819;
	mul.wide.s32 	%rd822, %r195, %r201;
	add.s64 	%rd823, %rd817, %rd822;
	add.s32 	%r743, %r99, 2;
	st.global.u32 	[%rd821], %r743;
	add.s32 	%r744, %r100, 3;
	st.global.u32 	[%rd823], %r744;

$L__BB0_147:
	ld.param.u64 	%rd96, [%rd58];
	ld.param.u32 	%r202, [%rd58+32];
	ld.param.u64 	%rd97, [%rd58+56];
	ld.param.u32 	%r203, [%rd58+88];
	ld.param.u64 	%rd98, [%rd58+112];
	ld.param.u32 	%r204, [%rd58+144];
	ld.param.u32 	%r205, [%rd58+172];
	ld.param.v2.u32 	{%r573, %r574}, [%rd58+176];
	add.s32 	%r209, %r99, 3;
	setp.le.s32 	%p158, %r573, %r209;
	setp.le.s32 	%p159, %r574, %r100;
	add.s32 	%r210, %r108, 12;
	setp.le.s32 	%p160, %r205, %r210;
	or.pred  	%p161, %p158, %p159;
	or.b32  	%r575, %r100, %r210;
	or.b32  	%r576, %r575, %r209;
	setp.lt.s32 	%p162, %r576, 0;
	or.pred  	%p163, %p162, %p161;
	or.pred  	%p164, %p160, %p163;
	@%p164 bra 	$L__BB0_149;
	bra.uni 	$L__BB0_148;

$L__BB0_149:
	add.s32 	%r748, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r748, %r100};
	add.s32 	%r749, %r108, 12;
	st.local.v2.u32 	[%rd25+8], {%r749, %r573};
	st.local.v2.u32 	[%rd25+16], {%r574, %r205};
	mov.u64 	%rd843, $str$1;
	cvta.global.u64 	%rd844, %rd843;
	{ // callseq 296, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd844;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r577, [retval0+0];
	} // callseq 296
	bra.uni 	$L__BB0_150;

$L__BB0_148:
	cvta.to.global.u64 	%rd836, %rd97;
	mul.wide.s32 	%rd837, %r204, %r210;
	add.s64 	%rd827, %rd98, %rd837;
	// begin inline asm
	{ atom.add.f64 %fd1369,[%rd827],%fd348; }

	// end inline asm
	add.s64 	%rd828, %rd827, 8;
	// begin inline asm
	{ atom.add.f64 %fd1371,[%rd828],%fd347; }

	// end inline asm
	add.s64 	%rd829, %rd827, 16;
	// begin inline asm
	{ atom.add.f64 %fd1373,[%rd829],%fd346; }

	// end inline asm
	add.s64 	%rd830, %rd827, 24;
	// begin inline asm
	{ atom.add.f64 %fd1375,[%rd830],%fd324; }

	// end inline asm
	add.s64 	%rd831, %rd827, 32;
	// begin inline asm
	{ atom.add.f64 %fd1377,[%rd831],%fd323; }

	// end inline asm
	add.s64 	%rd832, %rd827, 40;
	// begin inline asm
	{ atom.add.f64 %fd1379,[%rd832],%fd322; }

	// end inline asm
	add.s64 	%rd833, %rd827, 48;
	// begin inline asm
	{ atom.add.f64 %fd1381,[%rd833],%fd300; }

	// end inline asm
	add.s64 	%rd834, %rd827, 56;
	// begin inline asm
	{ atom.add.f64 %fd1383,[%rd834],%fd299; }

	// end inline asm
	add.s64 	%rd835, %rd827, 64;
	// begin inline asm
	{ atom.add.f64 %fd1385,[%rd835],%fd298; }

	// end inline asm
	mul.wide.s32 	%rd838, %r202, %r210;
	cvta.to.global.u64 	%rd839, %rd96;
	add.s64 	%rd840, %rd839, %rd838;
	mul.wide.s32 	%rd841, %r203, %r210;
	add.s64 	%rd842, %rd836, %rd841;
	add.s32 	%r747, %r99, 3;
	st.global.u32 	[%rd840], %r747;
	st.global.u32 	[%rd842], %r100;

$L__BB0_150:
	ld.param.u64 	%rd99, [%rd58];
	ld.param.u32 	%r211, [%rd58+32];
	ld.param.u64 	%rd100, [%rd58+56];
	ld.param.u32 	%r212, [%rd58+88];
	ld.param.u64 	%rd101, [%rd58+112];
	ld.param.u32 	%r213, [%rd58+144];
	ld.param.u32 	%r214, [%rd58+172];
	ld.param.v2.u32 	{%r578, %r579}, [%rd58+176];
	setp.le.s32 	%p165, %r578, %r209;
	setp.le.s32 	%p166, %r579, %r116;
	add.s32 	%r218, %r108, 13;
	setp.le.s32 	%p167, %r214, %r218;
	or.pred  	%p168, %p165, %p166;
	or.b32  	%r580, %r209, %r218;
	or.b32  	%r581, %r580, %r116;
	setp.lt.s32 	%p169, %r581, 0;
	or.pred  	%p170, %p169, %p168;
	or.pred  	%p171, %p167, %p170;
	@%p171 bra 	$L__BB0_152;
	bra.uni 	$L__BB0_151;

$L__BB0_152:
	add.s32 	%r752, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r752, %r116};
	add.s32 	%r753, %r108, 13;
	st.local.v2.u32 	[%rd25+8], {%r753, %r578};
	st.local.v2.u32 	[%rd25+16], {%r579, %r214};
	mov.u64 	%rd862, $str$1;
	cvta.global.u64 	%rd863, %rd862;
	{ // callseq 297, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd863;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r582, [retval0+0];
	} // callseq 297
	bra.uni 	$L__BB0_153;

$L__BB0_151:
	cvta.to.global.u64 	%rd855, %rd100;
	mul.wide.s32 	%rd856, %r213, %r218;
	add.s64 	%rd846, %rd101, %rd856;
	// begin inline asm
	{ atom.add.f64 %fd1387,[%rd846],%fd345; }

	// end inline asm
	add.s64 	%rd847, %rd846, 8;
	// begin inline asm
	{ atom.add.f64 %fd1389,[%rd847],%fd344; }

	// end inline asm
	add.s64 	%rd848, %rd846, 16;
	// begin inline asm
	{ atom.add.f64 %fd1391,[%rd848],%fd343; }

	// end inline asm
	add.s64 	%rd849, %rd846, 24;
	// begin inline asm
	{ atom.add.f64 %fd1393,[%rd849],%fd321; }

	// end inline asm
	add.s64 	%rd850, %rd846, 32;
	// begin inline asm
	{ atom.add.f64 %fd1395,[%rd850],%fd320; }

	// end inline asm
	add.s64 	%rd851, %rd846, 40;
	// begin inline asm
	{ atom.add.f64 %fd1397,[%rd851],%fd319; }

	// end inline asm
	add.s64 	%rd852, %rd846, 48;
	// begin inline asm
	{ atom.add.f64 %fd1399,[%rd852],%fd297; }

	// end inline asm
	add.s64 	%rd853, %rd846, 56;
	// begin inline asm
	{ atom.add.f64 %fd1401,[%rd853],%fd296; }

	// end inline asm
	add.s64 	%rd854, %rd846, 64;
	// begin inline asm
	{ atom.add.f64 %fd1403,[%rd854],%fd295; }

	// end inline asm
	mul.wide.s32 	%rd857, %r211, %r218;
	cvta.to.global.u64 	%rd858, %rd99;
	add.s64 	%rd859, %rd858, %rd857;
	mul.wide.s32 	%rd860, %r212, %r218;
	add.s64 	%rd861, %rd855, %rd860;
	add.s32 	%r750, %r99, 3;
	st.global.u32 	[%rd859], %r750;
	add.s32 	%r751, %r100, 1;
	st.global.u32 	[%rd861], %r751;

$L__BB0_153:
	ld.param.u64 	%rd102, [%rd58];
	ld.param.u32 	%r219, [%rd58+32];
	ld.param.u64 	%rd103, [%rd58+56];
	ld.param.u32 	%r220, [%rd58+88];
	ld.param.u64 	%rd104, [%rd58+112];
	ld.param.u32 	%r221, [%rd58+144];
	ld.param.u32 	%r222, [%rd58+172];
	ld.param.v2.u32 	{%r583, %r584}, [%rd58+176];
	setp.le.s32 	%p172, %r583, %r209;
	setp.le.s32 	%p173, %r584, %r125;
	add.s32 	%r226, %r108, 14;
	setp.le.s32 	%p174, %r222, %r226;
	or.pred  	%p175, %p172, %p173;
	or.b32  	%r585, %r209, %r226;
	or.b32  	%r586, %r585, %r125;
	setp.lt.s32 	%p176, %r586, 0;
	or.pred  	%p177, %p176, %p175;
	or.pred  	%p178, %p174, %p177;
	@%p178 bra 	$L__BB0_155;
	bra.uni 	$L__BB0_154;

$L__BB0_155:
	add.s32 	%r756, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r756, %r125};
	add.s32 	%r757, %r108, 14;
	st.local.v2.u32 	[%rd25+8], {%r757, %r583};
	st.local.v2.u32 	[%rd25+16], {%r584, %r222};
	mov.u64 	%rd881, $str$1;
	cvta.global.u64 	%rd882, %rd881;
	{ // callseq 298, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd882;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r587, [retval0+0];
	} // callseq 298
	bra.uni 	$L__BB0_156;

$L__BB0_154:
	cvta.to.global.u64 	%rd874, %rd103;
	mul.wide.s32 	%rd875, %r221, %r226;
	add.s64 	%rd865, %rd104, %rd875;
	// begin inline asm
	{ atom.add.f64 %fd1405,[%rd865],%fd342; }

	// end inline asm
	add.s64 	%rd866, %rd865, 8;
	// begin inline asm
	{ atom.add.f64 %fd1407,[%rd866],%fd341; }

	// end inline asm
	add.s64 	%rd867, %rd865, 16;
	// begin inline asm
	{ atom.add.f64 %fd1409,[%rd867],%fd340; }

	// end inline asm
	add.s64 	%rd868, %rd865, 24;
	// begin inline asm
	{ atom.add.f64 %fd1411,[%rd868],%fd318; }

	// end inline asm
	add.s64 	%rd869, %rd865, 32;
	// begin inline asm
	{ atom.add.f64 %fd1413,[%rd869],%fd317; }

	// end inline asm
	add.s64 	%rd870, %rd865, 40;
	// begin inline asm
	{ atom.add.f64 %fd1415,[%rd870],%fd316; }

	// end inline asm
	add.s64 	%rd871, %rd865, 48;
	// begin inline asm
	{ atom.add.f64 %fd1417,[%rd871],%fd294; }

	// end inline asm
	add.s64 	%rd872, %rd865, 56;
	// begin inline asm
	{ atom.add.f64 %fd1419,[%rd872],%fd293; }

	// end inline asm
	add.s64 	%rd873, %rd865, 64;
	// begin inline asm
	{ atom.add.f64 %fd1421,[%rd873],%fd292; }

	// end inline asm
	mul.wide.s32 	%rd876, %r219, %r226;
	cvta.to.global.u64 	%rd877, %rd102;
	add.s64 	%rd878, %rd877, %rd876;
	mul.wide.s32 	%rd879, %r220, %r226;
	add.s64 	%rd880, %rd874, %rd879;
	add.s32 	%r754, %r99, 3;
	st.global.u32 	[%rd878], %r754;
	add.s32 	%r755, %r100, 2;
	st.global.u32 	[%rd880], %r755;

$L__BB0_156:
	ld.param.u64 	%rd105, [%rd58];
	ld.param.u32 	%r227, [%rd58+32];
	ld.param.u64 	%rd106, [%rd58+56];
	ld.param.u32 	%r228, [%rd58+88];
	ld.param.u64 	%rd107, [%rd58+112];
	ld.param.u32 	%r229, [%rd58+144];
	ld.param.u32 	%r230, [%rd58+172];
	ld.param.v2.u32 	{%r588, %r589}, [%rd58+176];
	setp.le.s32 	%p179, %r588, %r209;
	setp.le.s32 	%p180, %r589, %r134;
	add.s32 	%r234, %r108, 15;
	setp.le.s32 	%p181, %r230, %r234;
	or.pred  	%p182, %p179, %p180;
	or.b32  	%r590, %r209, %r234;
	or.b32  	%r591, %r590, %r134;
	setp.lt.s32 	%p183, %r591, 0;
	or.pred  	%p184, %p183, %p182;
	or.pred  	%p185, %p181, %p184;
	@%p185 bra 	$L__BB0_158;
	bra.uni 	$L__BB0_157;

$L__BB0_158:
	add.s32 	%r760, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r760, %r134};
	add.s32 	%r761, %r108, 15;
	st.local.v2.u32 	[%rd25+8], {%r761, %r588};
	st.local.v2.u32 	[%rd25+16], {%r589, %r230};
	mov.u64 	%rd900, $str$1;
	cvta.global.u64 	%rd901, %rd900;
	{ // callseq 299, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd901;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r592, [retval0+0];
	} // callseq 299
	bra.uni 	$L__BB0_159;

$L__BB0_157:
	cvta.to.global.u64 	%rd893, %rd106;
	mul.wide.s32 	%rd894, %r229, %r234;
	add.s64 	%rd884, %rd107, %rd894;
	// begin inline asm
	{ atom.add.f64 %fd1423,[%rd884],%fd339; }

	// end inline asm
	add.s64 	%rd885, %rd884, 8;
	// begin inline asm
	{ atom.add.f64 %fd1425,[%rd885],%fd338; }

	// end inline asm
	add.s64 	%rd886, %rd884, 16;
	// begin inline asm
	{ atom.add.f64 %fd1427,[%rd886],%fd337; }

	// end inline asm
	add.s64 	%rd887, %rd884, 24;
	// begin inline asm
	{ atom.add.f64 %fd1429,[%rd887],%fd315; }

	// end inline asm
	add.s64 	%rd888, %rd884, 32;
	// begin inline asm
	{ atom.add.f64 %fd1431,[%rd888],%fd314; }

	// end inline asm
	add.s64 	%rd889, %rd884, 40;
	// begin inline asm
	{ atom.add.f64 %fd1433,[%rd889],%fd313; }

	// end inline asm
	add.s64 	%rd890, %rd884, 48;
	// begin inline asm
	{ atom.add.f64 %fd1435,[%rd890],%fd291; }

	// end inline asm
	add.s64 	%rd891, %rd884, 56;
	// begin inline asm
	{ atom.add.f64 %fd1437,[%rd891],%fd290; }

	// end inline asm
	add.s64 	%rd892, %rd884, 64;
	// begin inline asm
	{ atom.add.f64 %fd1439,[%rd892],%fd289; }

	// end inline asm
	mul.wide.s32 	%rd895, %r227, %r234;
	cvta.to.global.u64 	%rd896, %rd105;
	add.s64 	%rd897, %rd896, %rd895;
	mul.wide.s32 	%rd898, %r228, %r234;
	add.s64 	%rd899, %rd893, %rd898;
	add.s32 	%r758, %r99, 3;
	st.global.u32 	[%rd897], %r758;
	add.s32 	%r759, %r100, 3;
	st.global.u32 	[%rd899], %r759;

$L__BB0_159:
	add.s32 	%r593, %r2, %r403;
	shl.b32 	%r235, %r593, 4;
	ld.global.u32 	%r594, [%rd41];
	shl.b32 	%r236, %r594, 2;
	ld.global.u32 	%r595, [%rd22];
	shl.b32 	%r237, %r595, 2;
	ld.param.u64 	%rd108, [%rd58];
	ld.param.u32 	%r238, [%rd58+32];
	ld.param.u64 	%rd109, [%rd58+56];
	ld.param.u32 	%r239, [%rd58+88];
	ld.param.u64 	%rd110, [%rd58+112];
	ld.param.u32 	%r240, [%rd58+144];
	ld.param.u32 	%r241, [%rd58+172];
	ld.param.v2.u32 	{%r596, %r597}, [%rd58+176];
	setp.le.s32 	%p186, %r596, %r236;
	setp.le.s32 	%p187, %r597, %r237;
	setp.le.s32 	%p188, %r241, %r235;
	or.pred  	%p189, %p186, %p187;
	or.b32  	%r598, %r236, %r235;
	or.b32  	%r599, %r598, %r237;
	setp.lt.s32 	%p190, %r599, 0;
	or.pred  	%p191, %p190, %p189;
	or.pred  	%p192, %p188, %p191;
	@%p192 bra 	$L__BB0_161;
	bra.uni 	$L__BB0_160;

$L__BB0_161:
	st.local.v2.u32 	[%rd25], {%r236, %r237};
	st.local.v2.u32 	[%rd25+8], {%r235, %r596};
	st.local.v2.u32 	[%rd25+16], {%r597, %r241};
	mov.u64 	%rd919, $str$1;
	cvta.global.u64 	%rd920, %rd919;
	{ // callseq 300, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd920;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r600, [retval0+0];
	} // callseq 300
	bra.uni 	$L__BB0_162;

$L__BB0_160:
	cvta.to.global.u64 	%rd912, %rd109;
	mul.wide.s32 	%rd913, %r240, %r235;
	add.s64 	%rd903, %rd110, %rd913;
	// begin inline asm
	{ atom.add.f64 %fd1441,[%rd903],%fd288; }

	// end inline asm
	add.s64 	%rd904, %rd903, 8;
	// begin inline asm
	{ atom.add.f64 %fd1443,[%rd904],%fd287; }

	// end inline asm
	add.s64 	%rd905, %rd903, 16;
	// begin inline asm
	{ atom.add.f64 %fd1445,[%rd905],%fd286; }

	// end inline asm
	add.s64 	%rd906, %rd903, 24;
	// begin inline asm
	{ atom.add.f64 %fd1447,[%rd906],%fd264; }

	// end inline asm
	add.s64 	%rd907, %rd903, 32;
	// begin inline asm
	{ atom.add.f64 %fd1449,[%rd907],%fd263; }

	// end inline asm
	add.s64 	%rd908, %rd903, 40;
	// begin inline asm
	{ atom.add.f64 %fd1451,[%rd908],%fd262; }

	// end inline asm
	add.s64 	%rd909, %rd903, 48;
	// begin inline asm
	{ atom.add.f64 %fd1453,[%rd909],%fd240; }

	// end inline asm
	add.s64 	%rd910, %rd903, 56;
	// begin inline asm
	{ atom.add.f64 %fd1455,[%rd910],%fd239; }

	// end inline asm
	add.s64 	%rd911, %rd903, 64;
	// begin inline asm
	{ atom.add.f64 %fd1457,[%rd911],%fd238; }

	// end inline asm
	mul.wide.s32 	%rd914, %r238, %r235;
	cvta.to.global.u64 	%rd915, %rd108;
	add.s64 	%rd916, %rd915, %rd914;
	mul.wide.s32 	%rd917, %r239, %r235;
	add.s64 	%rd918, %rd912, %rd917;
	st.global.u32 	[%rd916], %r236;
	st.global.u32 	[%rd918], %r237;

$L__BB0_162:
	ld.param.u64 	%rd111, [%rd58];
	ld.param.u32 	%r245, [%rd58+32];
	ld.param.u64 	%rd112, [%rd58+56];
	ld.param.u32 	%r246, [%rd58+88];
	ld.param.u64 	%rd113, [%rd58+112];
	ld.param.u32 	%r247, [%rd58+144];
	ld.param.u32 	%r248, [%rd58+172];
	ld.param.v2.u32 	{%r601, %r602}, [%rd58+176];
	setp.le.s32 	%p193, %r601, %r236;
	add.s32 	%r252, %r237, 1;
	setp.le.s32 	%p194, %r602, %r252;
	add.s32 	%r253, %r235, 1;
	setp.le.s32 	%p195, %r248, %r253;
	or.pred  	%p196, %p193, %p194;
	or.b32  	%r603, %r236, %r253;
	or.b32  	%r604, %r603, %r252;
	setp.lt.s32 	%p197, %r604, 0;
	or.pred  	%p198, %p197, %p196;
	or.pred  	%p199, %p195, %p198;
	@%p199 bra 	$L__BB0_164;
	bra.uni 	$L__BB0_163;

$L__BB0_164:
	add.s32 	%r763, %r237, 1;
	st.local.v2.u32 	[%rd25], {%r236, %r763};
	add.s32 	%r764, %r235, 1;
	st.local.v2.u32 	[%rd25+8], {%r764, %r601};
	st.local.v2.u32 	[%rd25+16], {%r602, %r248};
	mov.u64 	%rd938, $str$1;
	cvta.global.u64 	%rd939, %rd938;
	{ // callseq 301, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd939;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r605, [retval0+0];
	} // callseq 301
	bra.uni 	$L__BB0_165;

$L__BB0_163:
	cvta.to.global.u64 	%rd931, %rd112;
	mul.wide.s32 	%rd932, %r247, %r253;
	add.s64 	%rd922, %rd113, %rd932;
	// begin inline asm
	{ atom.add.f64 %fd1459,[%rd922],%fd285; }

	// end inline asm
	add.s64 	%rd923, %rd922, 8;
	// begin inline asm
	{ atom.add.f64 %fd1461,[%rd923],%fd284; }

	// end inline asm
	add.s64 	%rd924, %rd922, 16;
	// begin inline asm
	{ atom.add.f64 %fd1463,[%rd924],%fd283; }

	// end inline asm
	add.s64 	%rd925, %rd922, 24;
	// begin inline asm
	{ atom.add.f64 %fd1465,[%rd925],%fd261; }

	// end inline asm
	add.s64 	%rd926, %rd922, 32;
	// begin inline asm
	{ atom.add.f64 %fd1467,[%rd926],%fd260; }

	// end inline asm
	add.s64 	%rd927, %rd922, 40;
	// begin inline asm
	{ atom.add.f64 %fd1469,[%rd927],%fd259; }

	// end inline asm
	add.s64 	%rd928, %rd922, 48;
	// begin inline asm
	{ atom.add.f64 %fd1471,[%rd928],%fd237; }

	// end inline asm
	add.s64 	%rd929, %rd922, 56;
	// begin inline asm
	{ atom.add.f64 %fd1473,[%rd929],%fd236; }

	// end inline asm
	add.s64 	%rd930, %rd922, 64;
	// begin inline asm
	{ atom.add.f64 %fd1475,[%rd930],%fd235; }

	// end inline asm
	mul.wide.s32 	%rd933, %r245, %r253;
	cvta.to.global.u64 	%rd934, %rd111;
	add.s64 	%rd935, %rd934, %rd933;
	mul.wide.s32 	%rd936, %r246, %r253;
	add.s64 	%rd937, %rd931, %rd936;
	st.global.u32 	[%rd935], %r236;
	add.s32 	%r762, %r237, 1;
	st.global.u32 	[%rd937], %r762;

$L__BB0_165:
	ld.param.u64 	%rd114, [%rd58];
	ld.param.u32 	%r254, [%rd58+32];
	ld.param.u64 	%rd115, [%rd58+56];
	ld.param.u32 	%r255, [%rd58+88];
	ld.param.u64 	%rd116, [%rd58+112];
	ld.param.u32 	%r256, [%rd58+144];
	ld.param.u32 	%r257, [%rd58+172];
	ld.param.v2.u32 	{%r606, %r607}, [%rd58+176];
	setp.le.s32 	%p200, %r606, %r236;
	add.s32 	%r261, %r237, 2;
	setp.le.s32 	%p201, %r607, %r261;
	add.s32 	%r262, %r235, 2;
	setp.le.s32 	%p202, %r257, %r262;
	or.pred  	%p203, %p200, %p201;
	or.b32  	%r608, %r236, %r262;
	or.b32  	%r609, %r608, %r261;
	setp.lt.s32 	%p204, %r609, 0;
	or.pred  	%p205, %p204, %p203;
	or.pred  	%p206, %p202, %p205;
	@%p206 bra 	$L__BB0_167;
	bra.uni 	$L__BB0_166;

$L__BB0_167:
	add.s32 	%r766, %r237, 2;
	st.local.v2.u32 	[%rd25], {%r236, %r766};
	add.s32 	%r767, %r235, 2;
	st.local.v2.u32 	[%rd25+8], {%r767, %r606};
	st.local.v2.u32 	[%rd25+16], {%r607, %r257};
	mov.u64 	%rd957, $str$1;
	cvta.global.u64 	%rd958, %rd957;
	{ // callseq 302, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd958;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r610, [retval0+0];
	} // callseq 302
	bra.uni 	$L__BB0_168;

$L__BB0_166:
	cvta.to.global.u64 	%rd950, %rd115;
	mul.wide.s32 	%rd951, %r256, %r262;
	add.s64 	%rd941, %rd116, %rd951;
	// begin inline asm
	{ atom.add.f64 %fd1477,[%rd941],%fd282; }

	// end inline asm
	add.s64 	%rd942, %rd941, 8;
	// begin inline asm
	{ atom.add.f64 %fd1479,[%rd942],%fd281; }

	// end inline asm
	add.s64 	%rd943, %rd941, 16;
	// begin inline asm
	{ atom.add.f64 %fd1481,[%rd943],%fd280; }

	// end inline asm
	add.s64 	%rd944, %rd941, 24;
	// begin inline asm
	{ atom.add.f64 %fd1483,[%rd944],%fd258; }

	// end inline asm
	add.s64 	%rd945, %rd941, 32;
	// begin inline asm
	{ atom.add.f64 %fd1485,[%rd945],%fd257; }

	// end inline asm
	add.s64 	%rd946, %rd941, 40;
	// begin inline asm
	{ atom.add.f64 %fd1487,[%rd946],%fd256; }

	// end inline asm
	add.s64 	%rd947, %rd941, 48;
	// begin inline asm
	{ atom.add.f64 %fd1489,[%rd947],%fd234; }

	// end inline asm
	add.s64 	%rd948, %rd941, 56;
	// begin inline asm
	{ atom.add.f64 %fd1491,[%rd948],%fd233; }

	// end inline asm
	add.s64 	%rd949, %rd941, 64;
	// begin inline asm
	{ atom.add.f64 %fd1493,[%rd949],%fd232; }

	// end inline asm
	mul.wide.s32 	%rd952, %r254, %r262;
	cvta.to.global.u64 	%rd953, %rd114;
	add.s64 	%rd954, %rd953, %rd952;
	mul.wide.s32 	%rd955, %r255, %r262;
	add.s64 	%rd956, %rd950, %rd955;
	st.global.u32 	[%rd954], %r236;
	add.s32 	%r765, %r237, 2;
	st.global.u32 	[%rd956], %r765;

$L__BB0_168:
	ld.param.u64 	%rd117, [%rd58];
	ld.param.u32 	%r263, [%rd58+32];
	ld.param.u64 	%rd118, [%rd58+56];
	ld.param.u32 	%r264, [%rd58+88];
	ld.param.u64 	%rd119, [%rd58+112];
	ld.param.u32 	%r265, [%rd58+144];
	ld.param.u32 	%r266, [%rd58+172];
	ld.param.v2.u32 	{%r611, %r612}, [%rd58+176];
	setp.le.s32 	%p207, %r611, %r236;
	add.s32 	%r270, %r237, 3;
	setp.le.s32 	%p208, %r612, %r270;
	add.s32 	%r271, %r235, 3;
	setp.le.s32 	%p209, %r266, %r271;
	or.pred  	%p210, %p207, %p208;
	or.b32  	%r613, %r236, %r271;
	or.b32  	%r614, %r613, %r270;
	setp.lt.s32 	%p211, %r614, 0;
	or.pred  	%p212, %p211, %p210;
	or.pred  	%p213, %p209, %p212;
	@%p213 bra 	$L__BB0_170;
	bra.uni 	$L__BB0_169;

$L__BB0_170:
	add.s32 	%r769, %r237, 3;
	st.local.v2.u32 	[%rd25], {%r236, %r769};
	add.s32 	%r770, %r235, 3;
	st.local.v2.u32 	[%rd25+8], {%r770, %r611};
	st.local.v2.u32 	[%rd25+16], {%r612, %r266};
	mov.u64 	%rd976, $str$1;
	cvta.global.u64 	%rd977, %rd976;
	{ // callseq 303, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd977;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r615, [retval0+0];
	} // callseq 303
	bra.uni 	$L__BB0_171;

$L__BB0_169:
	cvta.to.global.u64 	%rd969, %rd118;
	mul.wide.s32 	%rd970, %r265, %r271;
	add.s64 	%rd960, %rd119, %rd970;
	// begin inline asm
	{ atom.add.f64 %fd1495,[%rd960],%fd279; }

	// end inline asm
	add.s64 	%rd961, %rd960, 8;
	// begin inline asm
	{ atom.add.f64 %fd1497,[%rd961],%fd278; }

	// end inline asm
	add.s64 	%rd962, %rd960, 16;
	// begin inline asm
	{ atom.add.f64 %fd1499,[%rd962],%fd277; }

	// end inline asm
	add.s64 	%rd963, %rd960, 24;
	// begin inline asm
	{ atom.add.f64 %fd1501,[%rd963],%fd255; }

	// end inline asm
	add.s64 	%rd964, %rd960, 32;
	// begin inline asm
	{ atom.add.f64 %fd1503,[%rd964],%fd254; }

	// end inline asm
	add.s64 	%rd965, %rd960, 40;
	// begin inline asm
	{ atom.add.f64 %fd1505,[%rd965],%fd253; }

	// end inline asm
	add.s64 	%rd966, %rd960, 48;
	// begin inline asm
	{ atom.add.f64 %fd1507,[%rd966],%fd231; }

	// end inline asm
	add.s64 	%rd967, %rd960, 56;
	// begin inline asm
	{ atom.add.f64 %fd1509,[%rd967],%fd230; }

	// end inline asm
	add.s64 	%rd968, %rd960, 64;
	// begin inline asm
	{ atom.add.f64 %fd1511,[%rd968],%fd229; }

	// end inline asm
	mul.wide.s32 	%rd971, %r263, %r271;
	cvta.to.global.u64 	%rd972, %rd117;
	add.s64 	%rd973, %rd972, %rd971;
	mul.wide.s32 	%rd974, %r264, %r271;
	add.s64 	%rd975, %rd969, %rd974;
	st.global.u32 	[%rd973], %r236;
	add.s32 	%r768, %r237, 3;
	st.global.u32 	[%rd975], %r768;

$L__BB0_171:
	ld.param.u64 	%rd120, [%rd58];
	ld.param.u32 	%r272, [%rd58+32];
	ld.param.u64 	%rd121, [%rd58+56];
	ld.param.u32 	%r273, [%rd58+88];
	ld.param.u64 	%rd122, [%rd58+112];
	ld.param.u32 	%r274, [%rd58+144];
	ld.param.u32 	%r275, [%rd58+172];
	ld.param.v2.u32 	{%r616, %r617}, [%rd58+176];
	add.s32 	%r279, %r236, 1;
	setp.le.s32 	%p214, %r616, %r279;
	setp.le.s32 	%p215, %r617, %r237;
	add.s32 	%r280, %r235, 4;
	setp.le.s32 	%p216, %r275, %r280;
	or.pred  	%p217, %p214, %p215;
	or.b32  	%r618, %r237, %r280;
	or.b32  	%r619, %r618, %r279;
	setp.lt.s32 	%p218, %r619, 0;
	or.pred  	%p219, %p218, %p217;
	or.pred  	%p220, %p216, %p219;
	@%p220 bra 	$L__BB0_173;
	bra.uni 	$L__BB0_172;

$L__BB0_173:
	add.s32 	%r772, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r772, %r237};
	add.s32 	%r773, %r235, 4;
	st.local.v2.u32 	[%rd25+8], {%r773, %r616};
	st.local.v2.u32 	[%rd25+16], {%r617, %r275};
	mov.u64 	%rd995, $str$1;
	cvta.global.u64 	%rd996, %rd995;
	{ // callseq 304, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd996;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r620, [retval0+0];
	} // callseq 304
	bra.uni 	$L__BB0_174;

$L__BB0_172:
	cvta.to.global.u64 	%rd988, %rd121;
	mul.wide.s32 	%rd989, %r274, %r280;
	add.s64 	%rd979, %rd122, %rd989;
	// begin inline asm
	{ atom.add.f64 %fd1513,[%rd979],%fd216; }

	// end inline asm
	add.s64 	%rd980, %rd979, 8;
	// begin inline asm
	{ atom.add.f64 %fd1515,[%rd980],%fd215; }

	// end inline asm
	add.s64 	%rd981, %rd979, 16;
	// begin inline asm
	{ atom.add.f64 %fd1517,[%rd981],%fd214; }

	// end inline asm
	add.s64 	%rd982, %rd979, 24;
	// begin inline asm
	{ atom.add.f64 %fd1519,[%rd982],%fd192; }

	// end inline asm
	add.s64 	%rd983, %rd979, 32;
	// begin inline asm
	{ atom.add.f64 %fd1521,[%rd983],%fd191; }

	// end inline asm
	add.s64 	%rd984, %rd979, 40;
	// begin inline asm
	{ atom.add.f64 %fd1523,[%rd984],%fd190; }

	// end inline asm
	add.s64 	%rd985, %rd979, 48;
	// begin inline asm
	{ atom.add.f64 %fd1525,[%rd985],%fd168; }

	// end inline asm
	add.s64 	%rd986, %rd979, 56;
	// begin inline asm
	{ atom.add.f64 %fd1527,[%rd986],%fd167; }

	// end inline asm
	add.s64 	%rd987, %rd979, 64;
	// begin inline asm
	{ atom.add.f64 %fd1529,[%rd987],%fd166; }

	// end inline asm
	mul.wide.s32 	%rd990, %r272, %r280;
	cvta.to.global.u64 	%rd991, %rd120;
	add.s64 	%rd992, %rd991, %rd990;
	mul.wide.s32 	%rd993, %r273, %r280;
	add.s64 	%rd994, %rd988, %rd993;
	add.s32 	%r771, %r236, 1;
	st.global.u32 	[%rd992], %r771;
	st.global.u32 	[%rd994], %r237;

$L__BB0_174:
	ld.param.u64 	%rd123, [%rd58];
	ld.param.u32 	%r281, [%rd58+32];
	ld.param.u64 	%rd124, [%rd58+56];
	ld.param.u32 	%r282, [%rd58+88];
	ld.param.u64 	%rd125, [%rd58+112];
	ld.param.u32 	%r283, [%rd58+144];
	ld.param.u32 	%r284, [%rd58+172];
	ld.param.v2.u32 	{%r621, %r622}, [%rd58+176];
	setp.le.s32 	%p221, %r621, %r279;
	setp.le.s32 	%p222, %r622, %r252;
	add.s32 	%r288, %r235, 5;
	setp.le.s32 	%p223, %r284, %r288;
	or.pred  	%p224, %p221, %p222;
	or.b32  	%r623, %r279, %r288;
	or.b32  	%r624, %r623, %r252;
	setp.lt.s32 	%p225, %r624, 0;
	or.pred  	%p226, %p225, %p224;
	or.pred  	%p227, %p223, %p226;
	@%p227 bra 	$L__BB0_176;
	bra.uni 	$L__BB0_175;

$L__BB0_176:
	add.s32 	%r776, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r776, %r252};
	add.s32 	%r777, %r235, 5;
	st.local.v2.u32 	[%rd25+8], {%r777, %r621};
	st.local.v2.u32 	[%rd25+16], {%r622, %r284};
	mov.u64 	%rd1014, $str$1;
	cvta.global.u64 	%rd1015, %rd1014;
	{ // callseq 305, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1015;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r625, [retval0+0];
	} // callseq 305
	bra.uni 	$L__BB0_177;

$L__BB0_175:
	cvta.to.global.u64 	%rd1007, %rd124;
	mul.wide.s32 	%rd1008, %r283, %r288;
	add.s64 	%rd998, %rd125, %rd1008;
	// begin inline asm
	{ atom.add.f64 %fd1531,[%rd998],%fd213; }

	// end inline asm
	add.s64 	%rd999, %rd998, 8;
	// begin inline asm
	{ atom.add.f64 %fd1533,[%rd999],%fd212; }

	// end inline asm
	add.s64 	%rd1000, %rd998, 16;
	// begin inline asm
	{ atom.add.f64 %fd1535,[%rd1000],%fd211; }

	// end inline asm
	add.s64 	%rd1001, %rd998, 24;
	// begin inline asm
	{ atom.add.f64 %fd1537,[%rd1001],%fd189; }

	// end inline asm
	add.s64 	%rd1002, %rd998, 32;
	// begin inline asm
	{ atom.add.f64 %fd1539,[%rd1002],%fd188; }

	// end inline asm
	add.s64 	%rd1003, %rd998, 40;
	// begin inline asm
	{ atom.add.f64 %fd1541,[%rd1003],%fd187; }

	// end inline asm
	add.s64 	%rd1004, %rd998, 48;
	// begin inline asm
	{ atom.add.f64 %fd1543,[%rd1004],%fd165; }

	// end inline asm
	add.s64 	%rd1005, %rd998, 56;
	// begin inline asm
	{ atom.add.f64 %fd1545,[%rd1005],%fd164; }

	// end inline asm
	add.s64 	%rd1006, %rd998, 64;
	// begin inline asm
	{ atom.add.f64 %fd1547,[%rd1006],%fd163; }

	// end inline asm
	mul.wide.s32 	%rd1009, %r281, %r288;
	cvta.to.global.u64 	%rd1010, %rd123;
	add.s64 	%rd1011, %rd1010, %rd1009;
	mul.wide.s32 	%rd1012, %r282, %r288;
	add.s64 	%rd1013, %rd1007, %rd1012;
	add.s32 	%r774, %r236, 1;
	st.global.u32 	[%rd1011], %r774;
	add.s32 	%r775, %r237, 1;
	st.global.u32 	[%rd1013], %r775;

$L__BB0_177:
	ld.param.u64 	%rd126, [%rd58];
	ld.param.u32 	%r289, [%rd58+32];
	ld.param.u64 	%rd127, [%rd58+56];
	ld.param.u32 	%r290, [%rd58+88];
	ld.param.u64 	%rd128, [%rd58+112];
	ld.param.u32 	%r291, [%rd58+144];
	ld.param.u32 	%r292, [%rd58+172];
	ld.param.v2.u32 	{%r626, %r627}, [%rd58+176];
	setp.le.s32 	%p228, %r626, %r279;
	setp.le.s32 	%p229, %r627, %r261;
	add.s32 	%r296, %r235, 6;
	setp.le.s32 	%p230, %r292, %r296;
	or.pred  	%p231, %p228, %p229;
	or.b32  	%r628, %r279, %r296;
	or.b32  	%r629, %r628, %r261;
	setp.lt.s32 	%p232, %r629, 0;
	or.pred  	%p233, %p232, %p231;
	or.pred  	%p234, %p230, %p233;
	@%p234 bra 	$L__BB0_179;
	bra.uni 	$L__BB0_178;

$L__BB0_179:
	add.s32 	%r780, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r780, %r261};
	add.s32 	%r781, %r235, 6;
	st.local.v2.u32 	[%rd25+8], {%r781, %r626};
	st.local.v2.u32 	[%rd25+16], {%r627, %r292};
	mov.u64 	%rd1033, $str$1;
	cvta.global.u64 	%rd1034, %rd1033;
	{ // callseq 306, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1034;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r630, [retval0+0];
	} // callseq 306
	bra.uni 	$L__BB0_180;

$L__BB0_178:
	cvta.to.global.u64 	%rd1026, %rd127;
	mul.wide.s32 	%rd1027, %r291, %r296;
	add.s64 	%rd1017, %rd128, %rd1027;
	// begin inline asm
	{ atom.add.f64 %fd1549,[%rd1017],%fd210; }

	// end inline asm
	add.s64 	%rd1018, %rd1017, 8;
	// begin inline asm
	{ atom.add.f64 %fd1551,[%rd1018],%fd209; }

	// end inline asm
	add.s64 	%rd1019, %rd1017, 16;
	// begin inline asm
	{ atom.add.f64 %fd1553,[%rd1019],%fd208; }

	// end inline asm
	add.s64 	%rd1020, %rd1017, 24;
	// begin inline asm
	{ atom.add.f64 %fd1555,[%rd1020],%fd186; }

	// end inline asm
	add.s64 	%rd1021, %rd1017, 32;
	// begin inline asm
	{ atom.add.f64 %fd1557,[%rd1021],%fd185; }

	// end inline asm
	add.s64 	%rd1022, %rd1017, 40;
	// begin inline asm
	{ atom.add.f64 %fd1559,[%rd1022],%fd184; }

	// end inline asm
	add.s64 	%rd1023, %rd1017, 48;
	// begin inline asm
	{ atom.add.f64 %fd1561,[%rd1023],%fd162; }

	// end inline asm
	add.s64 	%rd1024, %rd1017, 56;
	// begin inline asm
	{ atom.add.f64 %fd1563,[%rd1024],%fd161; }

	// end inline asm
	add.s64 	%rd1025, %rd1017, 64;
	// begin inline asm
	{ atom.add.f64 %fd1565,[%rd1025],%fd160; }

	// end inline asm
	mul.wide.s32 	%rd1028, %r289, %r296;
	cvta.to.global.u64 	%rd1029, %rd126;
	add.s64 	%rd1030, %rd1029, %rd1028;
	mul.wide.s32 	%rd1031, %r290, %r296;
	add.s64 	%rd1032, %rd1026, %rd1031;
	add.s32 	%r778, %r236, 1;
	st.global.u32 	[%rd1030], %r778;
	add.s32 	%r779, %r237, 2;
	st.global.u32 	[%rd1032], %r779;

$L__BB0_180:
	ld.param.u64 	%rd129, [%rd58];
	ld.param.u32 	%r297, [%rd58+32];
	ld.param.u64 	%rd130, [%rd58+56];
	ld.param.u32 	%r298, [%rd58+88];
	ld.param.u64 	%rd131, [%rd58+112];
	ld.param.u32 	%r299, [%rd58+144];
	ld.param.u32 	%r300, [%rd58+172];
	ld.param.v2.u32 	{%r631, %r632}, [%rd58+176];
	setp.le.s32 	%p235, %r631, %r279;
	setp.le.s32 	%p236, %r632, %r270;
	add.s32 	%r304, %r235, 7;
	setp.le.s32 	%p237, %r300, %r304;
	or.pred  	%p238, %p235, %p236;
	or.b32  	%r633, %r279, %r304;
	or.b32  	%r634, %r633, %r270;
	setp.lt.s32 	%p239, %r634, 0;
	or.pred  	%p240, %p239, %p238;
	or.pred  	%p241, %p237, %p240;
	@%p241 bra 	$L__BB0_182;
	bra.uni 	$L__BB0_181;

$L__BB0_182:
	add.s32 	%r784, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r784, %r270};
	add.s32 	%r785, %r235, 7;
	st.local.v2.u32 	[%rd25+8], {%r785, %r631};
	st.local.v2.u32 	[%rd25+16], {%r632, %r300};
	mov.u64 	%rd1052, $str$1;
	cvta.global.u64 	%rd1053, %rd1052;
	{ // callseq 307, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1053;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r635, [retval0+0];
	} // callseq 307
	bra.uni 	$L__BB0_183;

$L__BB0_181:
	cvta.to.global.u64 	%rd1045, %rd130;
	mul.wide.s32 	%rd1046, %r299, %r304;
	add.s64 	%rd1036, %rd131, %rd1046;
	// begin inline asm
	{ atom.add.f64 %fd1567,[%rd1036],%fd207; }

	// end inline asm
	add.s64 	%rd1037, %rd1036, 8;
	// begin inline asm
	{ atom.add.f64 %fd1569,[%rd1037],%fd206; }

	// end inline asm
	add.s64 	%rd1038, %rd1036, 16;
	// begin inline asm
	{ atom.add.f64 %fd1571,[%rd1038],%fd205; }

	// end inline asm
	add.s64 	%rd1039, %rd1036, 24;
	// begin inline asm
	{ atom.add.f64 %fd1573,[%rd1039],%fd183; }

	// end inline asm
	add.s64 	%rd1040, %rd1036, 32;
	// begin inline asm
	{ atom.add.f64 %fd1575,[%rd1040],%fd182; }

	// end inline asm
	add.s64 	%rd1041, %rd1036, 40;
	// begin inline asm
	{ atom.add.f64 %fd1577,[%rd1041],%fd181; }

	// end inline asm
	add.s64 	%rd1042, %rd1036, 48;
	// begin inline asm
	{ atom.add.f64 %fd1579,[%rd1042],%fd159; }

	// end inline asm
	add.s64 	%rd1043, %rd1036, 56;
	// begin inline asm
	{ atom.add.f64 %fd1581,[%rd1043],%fd158; }

	// end inline asm
	add.s64 	%rd1044, %rd1036, 64;
	// begin inline asm
	{ atom.add.f64 %fd1583,[%rd1044],%fd157; }

	// end inline asm
	mul.wide.s32 	%rd1047, %r297, %r304;
	cvta.to.global.u64 	%rd1048, %rd129;
	add.s64 	%rd1049, %rd1048, %rd1047;
	mul.wide.s32 	%rd1050, %r298, %r304;
	add.s64 	%rd1051, %rd1045, %rd1050;
	add.s32 	%r782, %r236, 1;
	st.global.u32 	[%rd1049], %r782;
	add.s32 	%r783, %r237, 3;
	st.global.u32 	[%rd1051], %r783;

$L__BB0_183:
	ld.param.u64 	%rd132, [%rd58];
	ld.param.u32 	%r305, [%rd58+32];
	ld.param.u64 	%rd133, [%rd58+56];
	ld.param.u32 	%r306, [%rd58+88];
	ld.param.u64 	%rd134, [%rd58+112];
	ld.param.u32 	%r307, [%rd58+144];
	ld.param.u32 	%r308, [%rd58+172];
	ld.param.v2.u32 	{%r636, %r637}, [%rd58+176];
	add.s32 	%r312, %r236, 2;
	setp.le.s32 	%p242, %r636, %r312;
	setp.le.s32 	%p243, %r637, %r237;
	add.s32 	%r313, %r235, 8;
	setp.le.s32 	%p244, %r308, %r313;
	or.pred  	%p245, %p242, %p243;
	or.b32  	%r638, %r237, %r313;
	or.b32  	%r639, %r638, %r312;
	setp.lt.s32 	%p246, %r639, 0;
	or.pred  	%p247, %p246, %p245;
	or.pred  	%p248, %p244, %p247;
	@%p248 bra 	$L__BB0_185;
	bra.uni 	$L__BB0_184;

$L__BB0_185:
	add.s32 	%r787, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r787, %r237};
	add.s32 	%r788, %r235, 8;
	st.local.v2.u32 	[%rd25+8], {%r788, %r636};
	st.local.v2.u32 	[%rd25+16], {%r637, %r308};
	mov.u64 	%rd1071, $str$1;
	cvta.global.u64 	%rd1072, %rd1071;
	{ // callseq 308, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1072;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r640, [retval0+0];
	} // callseq 308
	bra.uni 	$L__BB0_186;

$L__BB0_184:
	cvta.to.global.u64 	%rd1064, %rd133;
	mul.wide.s32 	%rd1065, %r307, %r313;
	add.s64 	%rd1055, %rd134, %rd1065;
	// begin inline asm
	{ atom.add.f64 %fd1585,[%rd1055],%fd144; }

	// end inline asm
	add.s64 	%rd1056, %rd1055, 8;
	// begin inline asm
	{ atom.add.f64 %fd1587,[%rd1056],%fd143; }

	// end inline asm
	add.s64 	%rd1057, %rd1055, 16;
	// begin inline asm
	{ atom.add.f64 %fd1589,[%rd1057],%fd142; }

	// end inline asm
	add.s64 	%rd1058, %rd1055, 24;
	// begin inline asm
	{ atom.add.f64 %fd1591,[%rd1058],%fd120; }

	// end inline asm
	add.s64 	%rd1059, %rd1055, 32;
	// begin inline asm
	{ atom.add.f64 %fd1593,[%rd1059],%fd119; }

	// end inline asm
	add.s64 	%rd1060, %rd1055, 40;
	// begin inline asm
	{ atom.add.f64 %fd1595,[%rd1060],%fd118; }

	// end inline asm
	add.s64 	%rd1061, %rd1055, 48;
	// begin inline asm
	{ atom.add.f64 %fd1597,[%rd1061],%fd96; }

	// end inline asm
	add.s64 	%rd1062, %rd1055, 56;
	// begin inline asm
	{ atom.add.f64 %fd1599,[%rd1062],%fd95; }

	// end inline asm
	add.s64 	%rd1063, %rd1055, 64;
	// begin inline asm
	{ atom.add.f64 %fd1601,[%rd1063],%fd94; }

	// end inline asm
	mul.wide.s32 	%rd1066, %r305, %r313;
	cvta.to.global.u64 	%rd1067, %rd132;
	add.s64 	%rd1068, %rd1067, %rd1066;
	mul.wide.s32 	%rd1069, %r306, %r313;
	add.s64 	%rd1070, %rd1064, %rd1069;
	add.s32 	%r786, %r236, 2;
	st.global.u32 	[%rd1068], %r786;
	st.global.u32 	[%rd1070], %r237;

$L__BB0_186:
	ld.param.u64 	%rd135, [%rd58];
	ld.param.u32 	%r314, [%rd58+32];
	ld.param.u64 	%rd136, [%rd58+56];
	ld.param.u32 	%r315, [%rd58+88];
	ld.param.u64 	%rd137, [%rd58+112];
	ld.param.u32 	%r316, [%rd58+144];
	ld.param.u32 	%r317, [%rd58+172];
	ld.param.v2.u32 	{%r641, %r642}, [%rd58+176];
	setp.le.s32 	%p249, %r641, %r312;
	setp.le.s32 	%p250, %r642, %r252;
	add.s32 	%r321, %r235, 9;
	setp.le.s32 	%p251, %r317, %r321;
	or.pred  	%p252, %p249, %p250;
	or.b32  	%r643, %r312, %r321;
	or.b32  	%r644, %r643, %r252;
	setp.lt.s32 	%p253, %r644, 0;
	or.pred  	%p254, %p253, %p252;
	or.pred  	%p255, %p251, %p254;
	@%p255 bra 	$L__BB0_188;
	bra.uni 	$L__BB0_187;

$L__BB0_188:
	add.s32 	%r791, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r791, %r252};
	add.s32 	%r792, %r235, 9;
	st.local.v2.u32 	[%rd25+8], {%r792, %r641};
	st.local.v2.u32 	[%rd25+16], {%r642, %r317};
	mov.u64 	%rd1090, $str$1;
	cvta.global.u64 	%rd1091, %rd1090;
	{ // callseq 309, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1091;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r645, [retval0+0];
	} // callseq 309
	bra.uni 	$L__BB0_189;

$L__BB0_187:
	cvta.to.global.u64 	%rd1083, %rd136;
	mul.wide.s32 	%rd1084, %r316, %r321;
	add.s64 	%rd1074, %rd137, %rd1084;
	// begin inline asm
	{ atom.add.f64 %fd1603,[%rd1074],%fd141; }

	// end inline asm
	add.s64 	%rd1075, %rd1074, 8;
	// begin inline asm
	{ atom.add.f64 %fd1605,[%rd1075],%fd140; }

	// end inline asm
	add.s64 	%rd1076, %rd1074, 16;
	// begin inline asm
	{ atom.add.f64 %fd1607,[%rd1076],%fd139; }

	// end inline asm
	add.s64 	%rd1077, %rd1074, 24;
	// begin inline asm
	{ atom.add.f64 %fd1609,[%rd1077],%fd117; }

	// end inline asm
	add.s64 	%rd1078, %rd1074, 32;
	// begin inline asm
	{ atom.add.f64 %fd1611,[%rd1078],%fd116; }

	// end inline asm
	add.s64 	%rd1079, %rd1074, 40;
	// begin inline asm
	{ atom.add.f64 %fd1613,[%rd1079],%fd115; }

	// end inline asm
	add.s64 	%rd1080, %rd1074, 48;
	// begin inline asm
	{ atom.add.f64 %fd1615,[%rd1080],%fd93; }

	// end inline asm
	add.s64 	%rd1081, %rd1074, 56;
	// begin inline asm
	{ atom.add.f64 %fd1617,[%rd1081],%fd92; }

	// end inline asm
	add.s64 	%rd1082, %rd1074, 64;
	// begin inline asm
	{ atom.add.f64 %fd1619,[%rd1082],%fd91; }

	// end inline asm
	mul.wide.s32 	%rd1085, %r314, %r321;
	cvta.to.global.u64 	%rd1086, %rd135;
	add.s64 	%rd1087, %rd1086, %rd1085;
	mul.wide.s32 	%rd1088, %r315, %r321;
	add.s64 	%rd1089, %rd1083, %rd1088;
	add.s32 	%r789, %r236, 2;
	st.global.u32 	[%rd1087], %r789;
	add.s32 	%r790, %r237, 1;
	st.global.u32 	[%rd1089], %r790;

$L__BB0_189:
	ld.param.u64 	%rd138, [%rd58];
	ld.param.u32 	%r322, [%rd58+32];
	ld.param.u64 	%rd139, [%rd58+56];
	ld.param.u32 	%r323, [%rd58+88];
	ld.param.u64 	%rd140, [%rd58+112];
	ld.param.u32 	%r324, [%rd58+144];
	ld.param.u32 	%r325, [%rd58+172];
	ld.param.v2.u32 	{%r646, %r647}, [%rd58+176];
	setp.le.s32 	%p256, %r646, %r312;
	setp.le.s32 	%p257, %r647, %r261;
	add.s32 	%r329, %r235, 10;
	setp.le.s32 	%p258, %r325, %r329;
	or.pred  	%p259, %p256, %p257;
	or.b32  	%r648, %r312, %r329;
	or.b32  	%r649, %r648, %r261;
	setp.lt.s32 	%p260, %r649, 0;
	or.pred  	%p261, %p260, %p259;
	or.pred  	%p262, %p258, %p261;
	@%p262 bra 	$L__BB0_191;
	bra.uni 	$L__BB0_190;

$L__BB0_191:
	add.s32 	%r795, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r795, %r261};
	add.s32 	%r796, %r235, 10;
	st.local.v2.u32 	[%rd25+8], {%r796, %r646};
	st.local.v2.u32 	[%rd25+16], {%r647, %r325};
	mov.u64 	%rd1109, $str$1;
	cvta.global.u64 	%rd1110, %rd1109;
	{ // callseq 310, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r650, [retval0+0];
	} // callseq 310
	bra.uni 	$L__BB0_192;

$L__BB0_190:
	cvta.to.global.u64 	%rd1102, %rd139;
	mul.wide.s32 	%rd1103, %r324, %r329;
	add.s64 	%rd1093, %rd140, %rd1103;
	// begin inline asm
	{ atom.add.f64 %fd1621,[%rd1093],%fd138; }

	// end inline asm
	add.s64 	%rd1094, %rd1093, 8;
	// begin inline asm
	{ atom.add.f64 %fd1623,[%rd1094],%fd137; }

	// end inline asm
	add.s64 	%rd1095, %rd1093, 16;
	// begin inline asm
	{ atom.add.f64 %fd1625,[%rd1095],%fd136; }

	// end inline asm
	add.s64 	%rd1096, %rd1093, 24;
	// begin inline asm
	{ atom.add.f64 %fd1627,[%rd1096],%fd114; }

	// end inline asm
	add.s64 	%rd1097, %rd1093, 32;
	// begin inline asm
	{ atom.add.f64 %fd1629,[%rd1097],%fd113; }

	// end inline asm
	add.s64 	%rd1098, %rd1093, 40;
	// begin inline asm
	{ atom.add.f64 %fd1631,[%rd1098],%fd112; }

	// end inline asm
	add.s64 	%rd1099, %rd1093, 48;
	// begin inline asm
	{ atom.add.f64 %fd1633,[%rd1099],%fd90; }

	// end inline asm
	add.s64 	%rd1100, %rd1093, 56;
	// begin inline asm
	{ atom.add.f64 %fd1635,[%rd1100],%fd89; }

	// end inline asm
	add.s64 	%rd1101, %rd1093, 64;
	// begin inline asm
	{ atom.add.f64 %fd1637,[%rd1101],%fd88; }

	// end inline asm
	mul.wide.s32 	%rd1104, %r322, %r329;
	cvta.to.global.u64 	%rd1105, %rd138;
	add.s64 	%rd1106, %rd1105, %rd1104;
	mul.wide.s32 	%rd1107, %r323, %r329;
	add.s64 	%rd1108, %rd1102, %rd1107;
	add.s32 	%r793, %r236, 2;
	st.global.u32 	[%rd1106], %r793;
	add.s32 	%r794, %r237, 2;
	st.global.u32 	[%rd1108], %r794;

$L__BB0_192:
	ld.param.u64 	%rd141, [%rd58];
	ld.param.u32 	%r330, [%rd58+32];
	ld.param.u64 	%rd142, [%rd58+56];
	ld.param.u32 	%r331, [%rd58+88];
	ld.param.u64 	%rd143, [%rd58+112];
	ld.param.u32 	%r332, [%rd58+144];
	ld.param.u32 	%r333, [%rd58+172];
	ld.param.v2.u32 	{%r651, %r652}, [%rd58+176];
	setp.le.s32 	%p263, %r651, %r312;
	setp.le.s32 	%p264, %r652, %r270;
	add.s32 	%r337, %r235, 11;
	setp.le.s32 	%p265, %r333, %r337;
	or.pred  	%p266, %p263, %p264;
	or.b32  	%r653, %r312, %r337;
	or.b32  	%r654, %r653, %r270;
	setp.lt.s32 	%p267, %r654, 0;
	or.pred  	%p268, %p267, %p266;
	or.pred  	%p269, %p265, %p268;
	@%p269 bra 	$L__BB0_194;
	bra.uni 	$L__BB0_193;

$L__BB0_194:
	add.s32 	%r799, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r799, %r270};
	add.s32 	%r800, %r235, 11;
	st.local.v2.u32 	[%rd25+8], {%r800, %r651};
	st.local.v2.u32 	[%rd25+16], {%r652, %r333};
	mov.u64 	%rd1128, $str$1;
	cvta.global.u64 	%rd1129, %rd1128;
	{ // callseq 311, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1129;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r655, [retval0+0];
	} // callseq 311
	bra.uni 	$L__BB0_195;

$L__BB0_193:
	cvta.to.global.u64 	%rd1121, %rd142;
	mul.wide.s32 	%rd1122, %r332, %r337;
	add.s64 	%rd1112, %rd143, %rd1122;
	// begin inline asm
	{ atom.add.f64 %fd1639,[%rd1112],%fd135; }

	// end inline asm
	add.s64 	%rd1113, %rd1112, 8;
	// begin inline asm
	{ atom.add.f64 %fd1641,[%rd1113],%fd134; }

	// end inline asm
	add.s64 	%rd1114, %rd1112, 16;
	// begin inline asm
	{ atom.add.f64 %fd1643,[%rd1114],%fd133; }

	// end inline asm
	add.s64 	%rd1115, %rd1112, 24;
	// begin inline asm
	{ atom.add.f64 %fd1645,[%rd1115],%fd111; }

	// end inline asm
	add.s64 	%rd1116, %rd1112, 32;
	// begin inline asm
	{ atom.add.f64 %fd1647,[%rd1116],%fd110; }

	// end inline asm
	add.s64 	%rd1117, %rd1112, 40;
	// begin inline asm
	{ atom.add.f64 %fd1649,[%rd1117],%fd109; }

	// end inline asm
	add.s64 	%rd1118, %rd1112, 48;
	// begin inline asm
	{ atom.add.f64 %fd1651,[%rd1118],%fd87; }

	// end inline asm
	add.s64 	%rd1119, %rd1112, 56;
	// begin inline asm
	{ atom.add.f64 %fd1653,[%rd1119],%fd86; }

	// end inline asm
	add.s64 	%rd1120, %rd1112, 64;
	// begin inline asm
	{ atom.add.f64 %fd1655,[%rd1120],%fd85; }

	// end inline asm
	mul.wide.s32 	%rd1123, %r330, %r337;
	cvta.to.global.u64 	%rd1124, %rd141;
	add.s64 	%rd1125, %rd1124, %rd1123;
	mul.wide.s32 	%rd1126, %r331, %r337;
	add.s64 	%rd1127, %rd1121, %rd1126;
	add.s32 	%r797, %r236, 2;
	st.global.u32 	[%rd1125], %r797;
	add.s32 	%r798, %r237, 3;
	st.global.u32 	[%rd1127], %r798;

$L__BB0_195:
	ld.param.u64 	%rd144, [%rd58];
	ld.param.u32 	%r338, [%rd58+32];
	ld.param.u64 	%rd145, [%rd58+56];
	ld.param.u32 	%r339, [%rd58+88];
	ld.param.u64 	%rd146, [%rd58+112];
	ld.param.u32 	%r340, [%rd58+144];
	ld.param.u32 	%r341, [%rd58+172];
	ld.param.v2.u32 	{%r656, %r657}, [%rd58+176];
	add.s32 	%r345, %r236, 3;
	setp.le.s32 	%p270, %r656, %r345;
	setp.le.s32 	%p271, %r657, %r237;
	add.s32 	%r346, %r235, 12;
	setp.le.s32 	%p272, %r341, %r346;
	or.pred  	%p273, %p270, %p271;
	or.b32  	%r658, %r237, %r346;
	or.b32  	%r659, %r658, %r345;
	setp.lt.s32 	%p274, %r659, 0;
	or.pred  	%p275, %p274, %p273;
	or.pred  	%p276, %p272, %p275;
	@%p276 bra 	$L__BB0_197;
	bra.uni 	$L__BB0_196;

$L__BB0_197:
	add.s32 	%r802, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r802, %r237};
	add.s32 	%r803, %r235, 12;
	st.local.v2.u32 	[%rd25+8], {%r803, %r656};
	st.local.v2.u32 	[%rd25+16], {%r657, %r341};
	mov.u64 	%rd1147, $str$1;
	cvta.global.u64 	%rd1148, %rd1147;
	{ // callseq 312, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1148;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r660, [retval0+0];
	} // callseq 312
	bra.uni 	$L__BB0_198;

$L__BB0_196:
	cvta.to.global.u64 	%rd1140, %rd145;
	mul.wide.s32 	%rd1141, %r340, %r346;
	add.s64 	%rd1131, %rd146, %rd1141;
	// begin inline asm
	{ atom.add.f64 %fd1657,[%rd1131],%fd72; }

	// end inline asm
	add.s64 	%rd1132, %rd1131, 8;
	// begin inline asm
	{ atom.add.f64 %fd1659,[%rd1132],%fd71; }

	// end inline asm
	add.s64 	%rd1133, %rd1131, 16;
	// begin inline asm
	{ atom.add.f64 %fd1661,[%rd1133],%fd70; }

	// end inline asm
	add.s64 	%rd1134, %rd1131, 24;
	// begin inline asm
	{ atom.add.f64 %fd1663,[%rd1134],%fd48; }

	// end inline asm
	add.s64 	%rd1135, %rd1131, 32;
	// begin inline asm
	{ atom.add.f64 %fd1665,[%rd1135],%fd47; }

	// end inline asm
	add.s64 	%rd1136, %rd1131, 40;
	// begin inline asm
	{ atom.add.f64 %fd1667,[%rd1136],%fd46; }

	// end inline asm
	add.s64 	%rd1137, %rd1131, 48;
	// begin inline asm
	{ atom.add.f64 %fd1669,[%rd1137],%fd24; }

	// end inline asm
	add.s64 	%rd1138, %rd1131, 56;
	// begin inline asm
	{ atom.add.f64 %fd1671,[%rd1138],%fd23; }

	// end inline asm
	add.s64 	%rd1139, %rd1131, 64;
	// begin inline asm
	{ atom.add.f64 %fd1673,[%rd1139],%fd22; }

	// end inline asm
	mul.wide.s32 	%rd1142, %r338, %r346;
	cvta.to.global.u64 	%rd1143, %rd144;
	add.s64 	%rd1144, %rd1143, %rd1142;
	mul.wide.s32 	%rd1145, %r339, %r346;
	add.s64 	%rd1146, %rd1140, %rd1145;
	add.s32 	%r801, %r236, 3;
	st.global.u32 	[%rd1144], %r801;
	st.global.u32 	[%rd1146], %r237;

$L__BB0_198:
	ld.param.u64 	%rd147, [%rd58];
	ld.param.u32 	%r347, [%rd58+32];
	ld.param.u64 	%rd148, [%rd58+56];
	ld.param.u32 	%r348, [%rd58+88];
	ld.param.u64 	%rd149, [%rd58+112];
	ld.param.u32 	%r349, [%rd58+144];
	ld.param.u32 	%r350, [%rd58+172];
	ld.param.v2.u32 	{%r661, %r662}, [%rd58+176];
	setp.le.s32 	%p277, %r661, %r345;
	setp.le.s32 	%p278, %r662, %r252;
	add.s32 	%r354, %r235, 13;
	setp.le.s32 	%p279, %r350, %r354;
	or.pred  	%p280, %p277, %p278;
	or.b32  	%r663, %r345, %r354;
	or.b32  	%r664, %r663, %r252;
	setp.lt.s32 	%p281, %r664, 0;
	or.pred  	%p282, %p281, %p280;
	or.pred  	%p283, %p279, %p282;
	@%p283 bra 	$L__BB0_200;
	bra.uni 	$L__BB0_199;

$L__BB0_200:
	add.s32 	%r806, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r806, %r252};
	add.s32 	%r807, %r235, 13;
	st.local.v2.u32 	[%rd25+8], {%r807, %r661};
	st.local.v2.u32 	[%rd25+16], {%r662, %r350};
	mov.u64 	%rd1166, $str$1;
	cvta.global.u64 	%rd1167, %rd1166;
	{ // callseq 313, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1167;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r665, [retval0+0];
	} // callseq 313
	bra.uni 	$L__BB0_201;

$L__BB0_199:
	cvta.to.global.u64 	%rd1159, %rd148;
	mul.wide.s32 	%rd1160, %r349, %r354;
	add.s64 	%rd1150, %rd149, %rd1160;
	// begin inline asm
	{ atom.add.f64 %fd1675,[%rd1150],%fd69; }

	// end inline asm
	add.s64 	%rd1151, %rd1150, 8;
	// begin inline asm
	{ atom.add.f64 %fd1677,[%rd1151],%fd68; }

	// end inline asm
	add.s64 	%rd1152, %rd1150, 16;
	// begin inline asm
	{ atom.add.f64 %fd1679,[%rd1152],%fd67; }

	// end inline asm
	add.s64 	%rd1153, %rd1150, 24;
	// begin inline asm
	{ atom.add.f64 %fd1681,[%rd1153],%fd45; }

	// end inline asm
	add.s64 	%rd1154, %rd1150, 32;
	// begin inline asm
	{ atom.add.f64 %fd1683,[%rd1154],%fd44; }

	// end inline asm
	add.s64 	%rd1155, %rd1150, 40;
	// begin inline asm
	{ atom.add.f64 %fd1685,[%rd1155],%fd43; }

	// end inline asm
	add.s64 	%rd1156, %rd1150, 48;
	// begin inline asm
	{ atom.add.f64 %fd1687,[%rd1156],%fd21; }

	// end inline asm
	add.s64 	%rd1157, %rd1150, 56;
	// begin inline asm
	{ atom.add.f64 %fd1689,[%rd1157],%fd20; }

	// end inline asm
	add.s64 	%rd1158, %rd1150, 64;
	// begin inline asm
	{ atom.add.f64 %fd1691,[%rd1158],%fd19; }

	// end inline asm
	mul.wide.s32 	%rd1161, %r347, %r354;
	cvta.to.global.u64 	%rd1162, %rd147;
	add.s64 	%rd1163, %rd1162, %rd1161;
	mul.wide.s32 	%rd1164, %r348, %r354;
	add.s64 	%rd1165, %rd1159, %rd1164;
	add.s32 	%r804, %r236, 3;
	st.global.u32 	[%rd1163], %r804;
	add.s32 	%r805, %r237, 1;
	st.global.u32 	[%rd1165], %r805;

$L__BB0_201:
	ld.param.u64 	%rd150, [%rd58];
	ld.param.u32 	%r355, [%rd58+32];
	ld.param.u64 	%rd151, [%rd58+56];
	ld.param.u32 	%r356, [%rd58+88];
	ld.param.u64 	%rd152, [%rd58+112];
	ld.param.u32 	%r357, [%rd58+144];
	ld.param.u32 	%r358, [%rd58+172];
	ld.param.v2.u32 	{%r666, %r667}, [%rd58+176];
	setp.le.s32 	%p284, %r666, %r345;
	setp.le.s32 	%p285, %r667, %r261;
	add.s32 	%r362, %r235, 14;
	setp.le.s32 	%p286, %r358, %r362;
	or.pred  	%p287, %p284, %p285;
	or.b32  	%r668, %r345, %r362;
	or.b32  	%r669, %r668, %r261;
	setp.lt.s32 	%p288, %r669, 0;
	or.pred  	%p289, %p288, %p287;
	or.pred  	%p290, %p286, %p289;
	@%p290 bra 	$L__BB0_203;
	bra.uni 	$L__BB0_202;

$L__BB0_203:
	add.s32 	%r810, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r810, %r261};
	add.s32 	%r811, %r235, 14;
	st.local.v2.u32 	[%rd25+8], {%r811, %r666};
	st.local.v2.u32 	[%rd25+16], {%r667, %r358};
	mov.u64 	%rd1185, $str$1;
	cvta.global.u64 	%rd1186, %rd1185;
	{ // callseq 314, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1186;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r670, [retval0+0];
	} // callseq 314
	bra.uni 	$L__BB0_204;

$L__BB0_202:
	cvta.to.global.u64 	%rd1178, %rd151;
	mul.wide.s32 	%rd1179, %r357, %r362;
	add.s64 	%rd1169, %rd152, %rd1179;
	// begin inline asm
	{ atom.add.f64 %fd1693,[%rd1169],%fd66; }

	// end inline asm
	add.s64 	%rd1170, %rd1169, 8;
	// begin inline asm
	{ atom.add.f64 %fd1695,[%rd1170],%fd65; }

	// end inline asm
	add.s64 	%rd1171, %rd1169, 16;
	// begin inline asm
	{ atom.add.f64 %fd1697,[%rd1171],%fd64; }

	// end inline asm
	add.s64 	%rd1172, %rd1169, 24;
	// begin inline asm
	{ atom.add.f64 %fd1699,[%rd1172],%fd42; }

	// end inline asm
	add.s64 	%rd1173, %rd1169, 32;
	// begin inline asm
	{ atom.add.f64 %fd1701,[%rd1173],%fd41; }

	// end inline asm
	add.s64 	%rd1174, %rd1169, 40;
	// begin inline asm
	{ atom.add.f64 %fd1703,[%rd1174],%fd40; }

	// end inline asm
	add.s64 	%rd1175, %rd1169, 48;
	// begin inline asm
	{ atom.add.f64 %fd1705,[%rd1175],%fd18; }

	// end inline asm
	add.s64 	%rd1176, %rd1169, 56;
	// begin inline asm
	{ atom.add.f64 %fd1707,[%rd1176],%fd17; }

	// end inline asm
	add.s64 	%rd1177, %rd1169, 64;
	// begin inline asm
	{ atom.add.f64 %fd1709,[%rd1177],%fd16; }

	// end inline asm
	mul.wide.s32 	%rd1180, %r355, %r362;
	cvta.to.global.u64 	%rd1181, %rd150;
	add.s64 	%rd1182, %rd1181, %rd1180;
	mul.wide.s32 	%rd1183, %r356, %r362;
	add.s64 	%rd1184, %rd1178, %rd1183;
	add.s32 	%r808, %r236, 3;
	st.global.u32 	[%rd1182], %r808;
	add.s32 	%r809, %r237, 2;
	st.global.u32 	[%rd1184], %r809;

$L__BB0_204:
	ld.param.u64 	%rd153, [%rd58];
	ld.param.u32 	%r363, [%rd58+32];
	ld.param.u64 	%rd154, [%rd58+56];
	ld.param.u32 	%r364, [%rd58+88];
	ld.param.u64 	%rd155, [%rd58+112];
	ld.param.u32 	%r365, [%rd58+144];
	ld.param.u32 	%r366, [%rd58+172];
	ld.param.v2.u32 	{%r671, %r672}, [%rd58+176];
	setp.le.s32 	%p291, %r671, %r345;
	setp.le.s32 	%p292, %r672, %r270;
	add.s32 	%r370, %r235, 15;
	setp.le.s32 	%p293, %r366, %r370;
	or.pred  	%p294, %p291, %p292;
	or.b32  	%r673, %r345, %r370;
	or.b32  	%r674, %r673, %r270;
	setp.lt.s32 	%p295, %r674, 0;
	or.pred  	%p296, %p295, %p294;
	or.pred  	%p297, %p293, %p296;
	@%p297 bra 	$L__BB0_206;
	bra.uni 	$L__BB0_205;

$L__BB0_206:
	add.s32 	%r814, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r814, %r270};
	add.s32 	%r815, %r235, 15;
	st.local.v2.u32 	[%rd25+8], {%r815, %r671};
	st.local.v2.u32 	[%rd25+16], {%r672, %r366};
	mov.u64 	%rd1204, $str$1;
	cvta.global.u64 	%rd1205, %rd1204;
	{ // callseq 315, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1205;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r675, [retval0+0];
	} // callseq 315
	bra.uni 	$L__BB0_208;

$L__BB0_205:
	cvta.to.global.u64 	%rd1197, %rd154;
	mul.wide.s32 	%rd1198, %r365, %r370;
	add.s64 	%rd1188, %rd155, %rd1198;
	// begin inline asm
	{ atom.add.f64 %fd1711,[%rd1188],%fd63; }

	// end inline asm
	add.s64 	%rd1189, %rd1188, 8;
	// begin inline asm
	{ atom.add.f64 %fd1713,[%rd1189],%fd62; }

	// end inline asm
	add.s64 	%rd1190, %rd1188, 16;
	// begin inline asm
	{ atom.add.f64 %fd1715,[%rd1190],%fd61; }

	// end inline asm
	add.s64 	%rd1191, %rd1188, 24;
	// begin inline asm
	{ atom.add.f64 %fd1717,[%rd1191],%fd39; }

	// end inline asm
	add.s64 	%rd1192, %rd1188, 32;
	// begin inline asm
	{ atom.add.f64 %fd1719,[%rd1192],%fd38; }

	// end inline asm
	add.s64 	%rd1193, %rd1188, 40;
	// begin inline asm
	{ atom.add.f64 %fd1721,[%rd1193],%fd37; }

	// end inline asm
	add.s64 	%rd1194, %rd1188, 48;
	// begin inline asm
	{ atom.add.f64 %fd1723,[%rd1194],%fd15; }

	// end inline asm
	add.s64 	%rd1195, %rd1188, 56;
	// begin inline asm
	{ atom.add.f64 %fd1725,[%rd1195],%fd14; }

	// end inline asm
	add.s64 	%rd1196, %rd1188, 64;
	// begin inline asm
	{ atom.add.f64 %fd1727,[%rd1196],%fd13; }

	// end inline asm
	mul.wide.s32 	%rd1199, %r363, %r370;
	cvta.to.global.u64 	%rd1200, %rd153;
	add.s64 	%rd1201, %rd1200, %rd1199;
	mul.wide.s32 	%rd1202, %r364, %r370;
	add.s64 	%rd1203, %rd1197, %rd1202;
	add.s32 	%r812, %r236, 3;
	st.global.u32 	[%rd1201], %r812;
	add.s32 	%r813, %r237, 3;
	st.global.u32 	[%rd1203], %r813;

$L__BB0_208:
	ld.param.u64 	%rd1209, [assemble_matrix_cuda_kernel_forward_param_0+24];
	mov.u32 	%r818, %ntid.x;
	mov.u32 	%r677, %nctaid.x;
	mul.wide.u32 	%rd1207, %r818, %r677;
	add.s64 	%rd1214, %rd1214, %rd1207;
	setp.lt.u64 	%p298, %rd1214, %rd1209;
	@%p298 bra 	$L__BB0_2;

$L__BB0_209:
	ret;

}
	// .globl	assemble_matrix_cuda_kernel_backward
.visible .entry assemble_matrix_cuda_kernel_backward(
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_1[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_2[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_5[56],
	.param .u32 assemble_matrix_cuda_kernel_backward_param_6,
	.param .u32 assemble_matrix_cuda_kernel_backward_param_7,
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_8[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_9[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_12[56],
	.param .u32 assemble_matrix_cuda_kernel_backward_param_13,
	.param .u32 assemble_matrix_cuda_kernel_backward_param_14
)
{
	.local .align 8 .b8 	__local_depot1[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<948>;
	.reg .b16 	%rs<297>;
	.reg .b32 	%r<2261>;
	.reg .f64 	%fd<11521>;
	.reg .b64 	%rd<4125>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r1060, %r1061}, [assemble_matrix_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r1062, %r1063}, [assemble_matrix_cuda_kernel_backward_param_0+8];
	mov.b64 	%rd355, assemble_matrix_cuda_kernel_backward_param_1;
	mov.b64 	%rd356, assemble_matrix_cuda_kernel_backward_param_2;
	ld.param.v2.u32 	{%r1068, %r1069}, [assemble_matrix_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r1076, %r1077}, [assemble_matrix_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r1084, %r1085}, [assemble_matrix_cuda_kernel_backward_param_5+32];
	ld.param.u32 	%r1050, [assemble_matrix_cuda_kernel_backward_param_7];
	ld.param.v2.u32 	{%r1092, %r1093}, [assemble_matrix_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd363, [assemble_matrix_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd362, [assemble_matrix_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd361, [assemble_matrix_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd359, [assemble_matrix_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd357, [assemble_matrix_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd354, [assemble_matrix_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r1022, [assemble_matrix_cuda_kernel_backward_param_0+16];
	mov.u32 	%r1096, %ntid.x;
	mov.u32 	%r1097, %ctaid.x;
	mul.wide.u32 	%rd365, %r1096, %r1097;
	mov.u32 	%r1098, %tid.x;
	cvt.u64.u32 	%rd366, %r1098;
	add.s64 	%rd4121, %rd365, %rd366;
	setp.ge.u64 	%p1, %rd4121, %rd354;
	@%p1 bra 	$L__BB1_662;

	cvta.to.global.u64 	%rd5, %rd361;
	cvta.to.global.u64 	%rd6, %rd359;
	cvta.to.global.u64 	%rd7, %rd357;
	cvt.s64.s32 	%rd8, %r1063;
	cvt.s64.s32 	%rd9, %r1062;
	cvt.s64.s32 	%rd10, %r1061;
	cvt.s64.s32 	%rd11, %r1084;
	cvt.s64.s32 	%rd12, %r1068;
	cvt.s64.s32 	%rd13, %r1076;
	cvt.s64.s32 	%rd14, %r1092;
	mov.u64 	%rd373, %rd355;
	add.s64 	%rd25, %rd373, 112;
	mov.u64 	%rd27, %rd356;

$L__BB1_2:
	setp.lt.s32 	%p2, %r1022, 4;
	mov.u64 	%rd4122, %rd4121;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd367, %rd4121, %rd8;
	and.b64  	%rd368, %rd367, -4294967296;
	setp.eq.s64 	%p3, %rd368, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd4122, %rd4121, %rd8;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r1100, %rd8;
	cvt.u32.u64 	%r1101, %rd4121;
	div.u32 	%r1102, %r1101, %r1100;
	cvt.u64.u32 	%rd4122, %r1102;

$L__BB1_6:
	setp.lt.s32 	%p4, %r1022, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd369, %rd4122, %rd9;
	and.b64  	%rd370, %rd369, -4294967296;
	setp.eq.s64 	%p5, %rd370, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd4122, %rd4122, %rd9;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r1103, %rd9;
	cvt.u32.u64 	%r1104, %rd4122;
	div.u32 	%r1105, %r1104, %r1103;
	cvt.u64.u32 	%rd4122, %r1105;

$L__BB1_10:
	setp.lt.s32 	%p6, %r1022, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd371, %rd4122, %rd10;
	and.b64  	%rd372, %rd371, -4294967296;
	setp.eq.s64 	%p7, %rd372, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd4122, %rd4122, %rd10;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r1106, %rd10;
	cvt.u32.u64 	%r1107, %rd4122;
	div.u32 	%r1108, %r1107, %r1106;
	cvt.u64.u32 	%rd4122, %r1108;

$L__BB1_14:
	cvt.u32.u64 	%r1109, %rd4122;
	setp.gt.s32 	%p8, %r1022, 0;
	selp.b32 	%r10, %r1109, 0, %p8;
	setp.ge.s32 	%p9, %r10, %r1050;
	add.u64 	%rd374, %SP, 0;
	add.u64 	%rd26, %SPL, 0;
	mov.f64 	%fd10369, 0d0000000000000000;
	mov.f64 	%fd10370, %fd10369;
	mov.f64 	%fd10371, %fd10369;
	mov.f64 	%fd10372, %fd10369;
	mov.f64 	%fd10373, %fd10369;
	mov.f64 	%fd10374, %fd10369;
	mov.f64 	%fd10375, %fd10369;
	mov.f64 	%fd10376, %fd10369;
	mov.f64 	%fd10377, %fd10369;
	mov.f64 	%fd10378, %fd10369;
	mov.f64 	%fd10379, %fd10369;
	mov.f64 	%fd10380, %fd10369;
	mov.f64 	%fd10381, %fd10369;
	mov.f64 	%fd10382, %fd10369;
	mov.f64 	%fd10383, %fd10369;
	mov.f64 	%fd10384, %fd10369;
	mov.f64 	%fd10385, %fd10369;
	mov.f64 	%fd10386, %fd10369;
	mov.f64 	%fd10387, %fd10369;
	mov.f64 	%fd10388, %fd10369;
	mov.f64 	%fd10389, %fd10369;
	mov.f64 	%fd10390, %fd10369;
	mov.f64 	%fd10391, %fd10369;
	mov.f64 	%fd10392, %fd10369;
	mov.f64 	%fd10393, %fd10369;
	mov.f64 	%fd10394, %fd10369;
	mov.f64 	%fd10395, %fd10369;
	mov.f64 	%fd10396, %fd10369;
	mov.f64 	%fd10397, %fd10369;
	mov.f64 	%fd10398, %fd10369;
	mov.f64 	%fd10399, %fd10369;
	mov.f64 	%fd10400, %fd10369;
	mov.f64 	%fd10401, %fd10369;
	mov.f64 	%fd10402, %fd10369;
	mov.f64 	%fd10403, %fd10369;
	mov.f64 	%fd10404, %fd10369;
	mov.f64 	%fd10405, %fd10369;
	mov.f64 	%fd10406, %fd10369;
	mov.f64 	%fd10407, %fd10369;
	mov.f64 	%fd10408, %fd10369;
	mov.f64 	%fd10409, %fd10369;
	mov.f64 	%fd10410, %fd10369;
	mov.f64 	%fd10411, %fd10369;
	mov.f64 	%fd10412, %fd10369;
	mov.f64 	%fd10413, %fd10369;
	mov.f64 	%fd10414, %fd10369;
	mov.f64 	%fd10415, %fd10369;
	mov.f64 	%fd10416, %fd10369;
	mov.f64 	%fd10417, %fd10369;
	mov.f64 	%fd10418, %fd10369;
	mov.f64 	%fd10419, %fd10369;
	mov.f64 	%fd10420, %fd10369;
	mov.f64 	%fd10421, %fd10369;
	mov.f64 	%fd10422, %fd10369;
	mov.f64 	%fd10423, %fd10369;
	mov.f64 	%fd10424, %fd10369;
	mov.f64 	%fd10425, %fd10369;
	mov.f64 	%fd10426, %fd10369;
	mov.f64 	%fd10427, %fd10369;
	mov.f64 	%fd10428, %fd10369;
	mov.f64 	%fd10429, %fd10369;
	mov.f64 	%fd10430, %fd10369;
	mov.f64 	%fd10431, %fd10369;
	mov.f64 	%fd10432, %fd10369;
	mov.f64 	%fd10433, %fd10369;
	mov.f64 	%fd10434, %fd10369;
	mov.f64 	%fd10435, %fd10369;
	mov.f64 	%fd10436, %fd10369;
	mov.f64 	%fd10437, %fd10369;
	mov.f64 	%fd10438, %fd10369;
	mov.f64 	%fd10439, %fd10369;
	mov.f64 	%fd10440, %fd10369;
	mov.f64 	%fd10441, %fd10369;
	mov.f64 	%fd10442, %fd10369;
	mov.f64 	%fd10443, %fd10369;
	mov.f64 	%fd10444, %fd10369;
	mov.f64 	%fd10445, %fd10369;
	mov.f64 	%fd10446, %fd10369;
	mov.f64 	%fd10447, %fd10369;
	mov.f64 	%fd10448, %fd10369;
	mov.f64 	%fd10449, %fd10369;
	mov.f64 	%fd10450, %fd10369;
	mov.f64 	%fd10451, %fd10369;
	mov.f64 	%fd10452, %fd10369;
	mov.f64 	%fd10453, %fd10369;
	mov.f64 	%fd10454, %fd10369;
	mov.f64 	%fd10455, %fd10369;
	mov.f64 	%fd10456, %fd10369;
	mov.f64 	%fd10457, %fd10369;
	mov.f64 	%fd10458, %fd10369;
	mov.f64 	%fd10459, %fd10369;
	mov.f64 	%fd10460, %fd10369;
	mov.f64 	%fd10461, %fd10369;
	mov.f64 	%fd10462, %fd10369;
	mov.f64 	%fd10463, %fd10369;
	mov.f64 	%fd10464, %fd10369;
	mov.f64 	%fd10465, %fd10369;
	mov.f64 	%fd10466, %fd10369;
	mov.f64 	%fd10467, %fd10369;
	mov.f64 	%fd10468, %fd10369;
	mov.f64 	%fd10469, %fd10369;
	mov.f64 	%fd10470, %fd10369;
	mov.f64 	%fd10471, %fd10369;
	mov.f64 	%fd10472, %fd10369;
	mov.f64 	%fd10473, %fd10369;
	mov.f64 	%fd10474, %fd10369;
	mov.f64 	%fd10475, %fd10369;
	mov.f64 	%fd10476, %fd10369;
	mov.f64 	%fd10477, %fd10369;
	mov.f64 	%fd10478, %fd10369;
	mov.f64 	%fd10479, %fd10369;
	mov.f64 	%fd10480, %fd10369;
	mov.f64 	%fd10481, %fd10369;
	mov.f64 	%fd10482, %fd10369;
	mov.f64 	%fd10483, %fd10369;
	mov.f64 	%fd10484, %fd10369;
	mov.f64 	%fd10485, %fd10369;
	mov.f64 	%fd10486, %fd10369;
	mov.f64 	%fd10487, %fd10369;
	mov.f64 	%fd10488, %fd10369;
	mov.f64 	%fd10489, %fd10369;
	mov.f64 	%fd10490, %fd10369;
	mov.f64 	%fd10491, %fd10369;
	mov.f64 	%fd10492, %fd10369;
	mov.f64 	%fd10493, %fd10369;
	mov.f64 	%fd10494, %fd10369;
	mov.f64 	%fd10495, %fd10369;
	mov.f64 	%fd10496, %fd10369;
	mov.f64 	%fd10497, %fd10369;
	mov.f64 	%fd10498, %fd10369;
	mov.f64 	%fd10499, %fd10369;
	mov.f64 	%fd10500, %fd10369;
	mov.f64 	%fd10501, %fd10369;
	mov.f64 	%fd10502, %fd10369;
	mov.f64 	%fd10503, %fd10369;
	mov.f64 	%fd10504, %fd10369;
	mov.f64 	%fd10505, %fd10369;
	mov.f64 	%fd10506, %fd10369;
	mov.f64 	%fd10507, %fd10369;
	mov.f64 	%fd10508, %fd10369;
	mov.f64 	%fd10509, %fd10369;
	mov.f64 	%fd10510, %fd10369;
	mov.f64 	%fd10511, %fd10369;
	mov.f64 	%fd10512, %fd10369;
	mov.f64 	%fd10513, %fd10369;
	mov.f64 	%fd10514, %fd10369;
	mov.f64 	%fd10515, %fd10369;
	mov.f64 	%fd10516, %fd10369;
	mov.f64 	%fd10517, %fd10369;
	mov.f64 	%fd10518, %fd10369;
	mov.f64 	%fd10519, %fd10369;
	mov.f64 	%fd10520, %fd10369;
	mov.f64 	%fd10521, %fd10369;
	mov.f64 	%fd10522, %fd10369;
	mov.f64 	%fd10523, %fd10369;
	mov.f64 	%fd10524, %fd10369;
	mov.f64 	%fd10525, %fd10369;
	mov.f64 	%fd10526, %fd10369;
	mov.f64 	%fd10527, %fd10369;
	mov.f64 	%fd10528, %fd10369;
	mov.f64 	%fd10529, %fd10369;
	mov.f64 	%fd10530, %fd10369;
	mov.f64 	%fd10531, %fd10369;
	mov.f64 	%fd10532, %fd10369;
	mov.f64 	%fd10533, %fd10369;
	mov.f64 	%fd10534, %fd10369;
	mov.f64 	%fd10535, %fd10369;
	mov.f64 	%fd10536, %fd10369;
	mov.f64 	%fd10537, %fd10369;
	mov.f64 	%fd10538, %fd10369;
	mov.f64 	%fd10539, %fd10369;
	mov.f64 	%fd10540, %fd10369;
	mov.f64 	%fd10541, %fd10369;
	mov.f64 	%fd10542, %fd10369;
	mov.f64 	%fd10543, %fd10369;
	mov.f64 	%fd10544, %fd10369;
	mov.f64 	%fd10545, %fd10369;
	mov.f64 	%fd10546, %fd10369;
	mov.f64 	%fd10547, %fd10369;
	mov.f64 	%fd10548, %fd10369;
	mov.f64 	%fd10549, %fd10369;
	mov.f64 	%fd10550, %fd10369;
	mov.f64 	%fd10551, %fd10369;
	mov.f64 	%fd10552, %fd10369;
	mov.f64 	%fd10553, %fd10369;
	mov.f64 	%fd10554, %fd10369;
	mov.f64 	%fd10555, %fd10369;
	mov.f64 	%fd10556, %fd10369;
	mov.f64 	%fd10557, %fd10369;
	mov.f64 	%fd10558, %fd10369;
	mov.f64 	%fd10559, %fd10369;
	mov.f64 	%fd10560, %fd10369;
	mov.f64 	%fd10561, %fd10369;
	mov.f64 	%fd10562, %fd10369;
	mov.f64 	%fd10563, %fd10369;
	mov.f64 	%fd10564, %fd10369;
	mov.f64 	%fd10565, %fd10369;
	mov.f64 	%fd10566, %fd10369;
	mov.f64 	%fd10567, %fd10369;
	mov.f64 	%fd10568, %fd10369;
	mov.f64 	%fd10569, %fd10369;
	mov.f64 	%fd10570, %fd10369;
	mov.f64 	%fd10571, %fd10369;
	mov.f64 	%fd10572, %fd10369;
	mov.f64 	%fd10573, %fd10369;
	mov.f64 	%fd10574, %fd10369;
	mov.f64 	%fd10575, %fd10369;
	mov.f64 	%fd10576, %fd10369;
	mov.f64 	%fd10577, %fd10369;
	mov.f64 	%fd10578, %fd10369;
	mov.f64 	%fd10579, %fd10369;
	mov.f64 	%fd10580, %fd10369;
	mov.f64 	%fd10581, %fd10369;
	mov.f64 	%fd10582, %fd10369;
	mov.f64 	%fd10583, %fd10369;
	mov.f64 	%fd10584, %fd10369;
	mov.f64 	%fd10585, %fd10369;
	mov.f64 	%fd10586, %fd10369;
	mov.f64 	%fd10587, %fd10369;
	mov.f64 	%fd10588, %fd10369;
	mov.f64 	%fd10589, %fd10369;
	mov.f64 	%fd10590, %fd10369;
	mov.f64 	%fd10591, %fd10369;
	mov.f64 	%fd10592, %fd10369;
	mov.f64 	%fd10593, %fd10369;
	mov.f64 	%fd10594, %fd10369;
	mov.f64 	%fd10595, %fd10369;
	mov.f64 	%fd10596, %fd10369;
	mov.f64 	%fd10597, %fd10369;
	mov.f64 	%fd10598, %fd10369;
	mov.f64 	%fd10599, %fd10369;
	mov.f64 	%fd10600, %fd10369;
	mov.f64 	%fd10601, %fd10369;
	mov.f64 	%fd10602, %fd10369;
	mov.f64 	%fd10603, %fd10369;
	mov.f64 	%fd10604, %fd10369;
	mov.f64 	%fd10605, %fd10369;
	mov.f64 	%fd10606, %fd10369;
	mov.f64 	%fd10607, %fd10369;
	mov.f64 	%fd10608, %fd10369;
	mov.f64 	%fd10609, %fd10369;
	mov.f64 	%fd10610, %fd10369;
	mov.f64 	%fd10611, %fd10369;
	mov.f64 	%fd10612, %fd10369;
	mov.f64 	%fd10613, %fd10369;
	mov.f64 	%fd10614, %fd10369;
	mov.f64 	%fd10615, %fd10369;
	mov.f64 	%fd10616, %fd10369;
	mov.f64 	%fd10617, %fd10369;
	mov.f64 	%fd10618, %fd10369;
	mov.f64 	%fd10619, %fd10369;
	mov.f64 	%fd10620, %fd10369;
	mov.f64 	%fd10621, %fd10369;
	mov.f64 	%fd10622, %fd10369;
	mov.f64 	%fd10623, %fd10369;
	mov.f64 	%fd10624, %fd10369;
	mov.f64 	%fd10625, %fd10369;
	mov.f64 	%fd10626, %fd10369;
	mov.f64 	%fd10627, %fd10369;
	mov.f64 	%fd10628, %fd10369;
	mov.f64 	%fd10629, %fd10369;
	mov.f64 	%fd10630, %fd10369;
	mov.f64 	%fd10631, %fd10369;
	mov.f64 	%fd10632, %fd10369;
	mov.f64 	%fd10633, %fd10369;
	mov.f64 	%fd10634, %fd10369;
	mov.f64 	%fd10635, %fd10369;
	mov.f64 	%fd10636, %fd10369;
	mov.f64 	%fd10637, %fd10369;
	mov.f64 	%fd10638, %fd10369;
	mov.f64 	%fd10639, %fd10369;
	mov.f64 	%fd10640, %fd10369;
	mov.f64 	%fd10641, %fd10369;
	mov.f64 	%fd10642, %fd10369;
	mov.f64 	%fd10643, %fd10369;
	mov.f64 	%fd10644, %fd10369;
	mov.f64 	%fd10645, %fd10369;
	mov.f64 	%fd10646, %fd10369;
	mov.f64 	%fd10647, %fd10369;
	mov.f64 	%fd10648, %fd10369;
	mov.f64 	%fd10649, %fd10369;
	mov.f64 	%fd10650, %fd10369;
	mov.f64 	%fd10651, %fd10369;
	mov.f64 	%fd10652, %fd10369;
	mov.f64 	%fd10653, %fd10369;
	mov.f64 	%fd10654, %fd10369;
	mov.f64 	%fd10655, %fd10369;
	mov.f64 	%fd10656, %fd10369;
	mov.f64 	%fd10657, %fd10369;
	mov.f64 	%fd10658, %fd10369;
	mov.f64 	%fd10659, %fd10369;
	mov.f64 	%fd10660, %fd10369;
	mov.f64 	%fd10661, %fd10369;
	mov.f64 	%fd10662, %fd10369;
	mov.f64 	%fd10663, %fd10369;
	mov.f64 	%fd10664, %fd10369;
	mov.f64 	%fd10665, %fd10369;
	mov.f64 	%fd10666, %fd10369;
	mov.f64 	%fd10667, %fd10369;
	mov.f64 	%fd10668, %fd10369;
	mov.f64 	%fd10669, %fd10369;
	mov.f64 	%fd10670, %fd10369;
	mov.f64 	%fd10671, %fd10369;
	mov.f64 	%fd10672, %fd10369;
	mov.f64 	%fd10673, %fd10369;
	mov.f64 	%fd10674, %fd10369;
	mov.f64 	%fd10675, %fd10369;
	mov.f64 	%fd10676, %fd10369;
	mov.f64 	%fd10677, %fd10369;
	mov.f64 	%fd10678, %fd10369;
	mov.f64 	%fd10679, %fd10369;
	mov.f64 	%fd10680, %fd10369;
	mov.f64 	%fd10681, %fd10369;
	mov.f64 	%fd10682, %fd10369;
	mov.f64 	%fd10683, %fd10369;
	mov.f64 	%fd10684, %fd10369;
	mov.f64 	%fd10685, %fd10369;
	mov.f64 	%fd10686, %fd10369;
	mov.f64 	%fd10687, %fd10369;
	mov.f64 	%fd10688, %fd10369;
	mov.f64 	%fd10689, %fd10369;
	mov.f64 	%fd10690, %fd10369;
	mov.f64 	%fd10691, %fd10369;
	mov.f64 	%fd10692, %fd10369;
	mov.f64 	%fd10693, %fd10369;
	mov.f64 	%fd10694, %fd10369;
	mov.f64 	%fd10695, %fd10369;
	mov.f64 	%fd10696, %fd10369;
	mov.f64 	%fd10697, %fd10369;
	mov.f64 	%fd10698, %fd10369;
	mov.f64 	%fd10699, %fd10369;
	mov.f64 	%fd10700, %fd10369;
	mov.f64 	%fd10701, %fd10369;
	mov.f64 	%fd10702, %fd10369;
	mov.f64 	%fd10703, %fd10369;
	mov.f64 	%fd10704, %fd10369;
	mov.f64 	%fd10705, %fd10369;
	mov.f64 	%fd10706, %fd10369;
	mov.f64 	%fd10707, %fd10369;
	mov.f64 	%fd10708, %fd10369;
	mov.f64 	%fd10709, %fd10369;
	mov.f64 	%fd10710, %fd10369;
	mov.f64 	%fd10711, %fd10369;
	mov.f64 	%fd10712, %fd10369;
	mov.f64 	%fd10713, %fd10369;
	mov.f64 	%fd10714, %fd10369;
	mov.f64 	%fd10715, %fd10369;
	mov.f64 	%fd10716, %fd10369;
	mov.f64 	%fd10717, %fd10369;
	mov.f64 	%fd10718, %fd10369;
	mov.f64 	%fd10719, %fd10369;
	mov.f64 	%fd10720, %fd10369;
	mov.f64 	%fd10721, %fd10369;
	mov.f64 	%fd10722, %fd10369;
	mov.f64 	%fd10723, %fd10369;
	mov.f64 	%fd10724, %fd10369;
	mov.f64 	%fd10725, %fd10369;
	mov.f64 	%fd10726, %fd10369;
	mov.f64 	%fd10727, %fd10369;
	mov.f64 	%fd10728, %fd10369;
	mov.f64 	%fd10729, %fd10369;
	mov.f64 	%fd10730, %fd10369;
	mov.f64 	%fd10731, %fd10369;
	mov.f64 	%fd10732, %fd10369;
	mov.f64 	%fd10733, %fd10369;
	mov.f64 	%fd10734, %fd10369;
	mov.f64 	%fd10735, %fd10369;
	mov.f64 	%fd10736, %fd10369;
	mov.f64 	%fd10737, %fd10369;
	mov.f64 	%fd10738, %fd10369;
	mov.f64 	%fd10739, %fd10369;
	mov.f64 	%fd10740, %fd10369;
	mov.f64 	%fd10741, %fd10369;
	mov.f64 	%fd10742, %fd10369;
	mov.f64 	%fd10743, %fd10369;
	mov.f64 	%fd10744, %fd10369;
	mov.f64 	%fd10745, %fd10369;
	mov.f64 	%fd10746, %fd10369;
	mov.f64 	%fd10747, %fd10369;
	mov.f64 	%fd10748, %fd10369;
	mov.f64 	%fd10749, %fd10369;
	mov.f64 	%fd10750, %fd10369;
	mov.f64 	%fd10751, %fd10369;
	mov.f64 	%fd10752, %fd10369;
	mov.f64 	%fd10753, %fd10369;
	mov.f64 	%fd10754, %fd10369;
	mov.f64 	%fd10755, %fd10369;
	mov.f64 	%fd10756, %fd10369;
	mov.f64 	%fd10757, %fd10369;
	mov.f64 	%fd10758, %fd10369;
	mov.f64 	%fd10759, %fd10369;
	mov.f64 	%fd10760, %fd10369;
	mov.f64 	%fd10761, %fd10369;
	mov.f64 	%fd10762, %fd10369;
	mov.f64 	%fd10763, %fd10369;
	mov.f64 	%fd10764, %fd10369;
	mov.f64 	%fd10765, %fd10369;
	mov.f64 	%fd10766, %fd10369;
	mov.f64 	%fd10767, %fd10369;
	mov.f64 	%fd10768, %fd10369;
	mov.f64 	%fd10769, %fd10369;
	mov.f64 	%fd10770, %fd10369;
	mov.f64 	%fd10771, %fd10369;
	mov.f64 	%fd10772, %fd10369;
	mov.f64 	%fd10773, %fd10369;
	mov.f64 	%fd10774, %fd10369;
	mov.f64 	%fd10775, %fd10369;
	mov.f64 	%fd10776, %fd10369;
	mov.f64 	%fd10777, %fd10369;
	mov.f64 	%fd10778, %fd10369;
	mov.f64 	%fd10779, %fd10369;
	mov.f64 	%fd10780, %fd10369;
	mov.f64 	%fd10781, %fd10369;
	mov.f64 	%fd10782, %fd10369;
	mov.f64 	%fd10783, %fd10369;
	mov.f64 	%fd10784, %fd10369;
	mov.f64 	%fd10785, %fd10369;
	mov.f64 	%fd10786, %fd10369;
	mov.f64 	%fd10787, %fd10369;
	mov.f64 	%fd10788, %fd10369;
	mov.f64 	%fd10789, %fd10369;
	mov.f64 	%fd10790, %fd10369;
	mov.f64 	%fd10791, %fd10369;
	mov.f64 	%fd10792, %fd10369;
	mov.f64 	%fd10793, %fd10369;
	mov.f64 	%fd10794, %fd10369;
	mov.f64 	%fd10795, %fd10369;
	mov.f64 	%fd10796, %fd10369;
	mov.f64 	%fd10797, %fd10369;
	mov.f64 	%fd10798, %fd10369;
	mov.f64 	%fd10799, %fd10369;
	mov.f64 	%fd10800, %fd10369;
	mov.f64 	%fd10801, %fd10369;
	mov.f64 	%fd10802, %fd10369;
	mov.f64 	%fd10803, %fd10369;
	mov.f64 	%fd10804, %fd10369;
	mov.f64 	%fd10805, %fd10369;
	mov.f64 	%fd10806, %fd10369;
	mov.f64 	%fd10807, %fd10369;
	mov.f64 	%fd10808, %fd10369;
	mov.f64 	%fd10809, %fd10369;
	mov.f64 	%fd10810, %fd10369;
	mov.f64 	%fd10811, %fd10369;
	mov.f64 	%fd10812, %fd10369;
	mov.f64 	%fd10813, %fd10369;
	mov.f64 	%fd10814, %fd10369;
	mov.f64 	%fd10815, %fd10369;
	mov.f64 	%fd10816, %fd10369;
	mov.f64 	%fd10817, %fd10369;
	mov.f64 	%fd10818, %fd10369;
	mov.f64 	%fd10819, %fd10369;
	mov.f64 	%fd10820, %fd10369;
	mov.f64 	%fd10821, %fd10369;
	mov.f64 	%fd10822, %fd10369;
	mov.f64 	%fd10823, %fd10369;
	mov.f64 	%fd10824, %fd10369;
	mov.f64 	%fd10825, %fd10369;
	mov.f64 	%fd10826, %fd10369;
	mov.f64 	%fd10827, %fd10369;
	mov.f64 	%fd10828, %fd10369;
	mov.f64 	%fd10829, %fd10369;
	mov.f64 	%fd10830, %fd10369;
	mov.f64 	%fd10831, %fd10369;
	mov.f64 	%fd10832, %fd10369;
	mov.f64 	%fd10833, %fd10369;
	mov.f64 	%fd10834, %fd10369;
	mov.f64 	%fd10835, %fd10369;
	mov.f64 	%fd10836, %fd10369;
	mov.f64 	%fd10837, %fd10369;
	mov.f64 	%fd10838, %fd10369;
	mov.f64 	%fd10839, %fd10369;
	mov.f64 	%fd10840, %fd10369;
	mov.f64 	%fd10841, %fd10369;
	mov.f64 	%fd10842, %fd10369;
	mov.f64 	%fd10843, %fd10369;
	mov.f64 	%fd10844, %fd10369;
	mov.f64 	%fd10845, %fd10369;
	mov.f64 	%fd10846, %fd10369;
	mov.f64 	%fd10847, %fd10369;
	mov.f64 	%fd10848, %fd10369;
	mov.f64 	%fd10849, %fd10369;
	mov.f64 	%fd10850, %fd10369;
	mov.f64 	%fd10851, %fd10369;
	mov.f64 	%fd10852, %fd10369;
	mov.f64 	%fd10853, %fd10369;
	mov.f64 	%fd10854, %fd10369;
	mov.f64 	%fd10855, %fd10369;
	mov.f64 	%fd10856, %fd10369;
	mov.f64 	%fd10857, %fd10369;
	mov.f64 	%fd10858, %fd10369;
	mov.f64 	%fd10859, %fd10369;
	mov.f64 	%fd10860, %fd10369;
	mov.f64 	%fd10861, %fd10369;
	mov.f64 	%fd10862, %fd10369;
	mov.f64 	%fd10863, %fd10369;
	mov.f64 	%fd10864, %fd10369;
	mov.f64 	%fd10865, %fd10369;
	mov.f64 	%fd10866, %fd10369;
	mov.f64 	%fd10867, %fd10369;
	mov.f64 	%fd10868, %fd10369;
	mov.f64 	%fd10869, %fd10369;
	mov.f64 	%fd10870, %fd10369;
	mov.f64 	%fd10871, %fd10369;
	mov.f64 	%fd10872, %fd10369;
	mov.f64 	%fd10873, %fd10369;
	mov.f64 	%fd10874, %fd10369;
	mov.f64 	%fd10875, %fd10369;
	mov.f64 	%fd10876, %fd10369;
	mov.f64 	%fd10877, %fd10369;
	mov.f64 	%fd10878, %fd10369;
	mov.f64 	%fd10879, %fd10369;
	mov.f64 	%fd10880, %fd10369;
	mov.f64 	%fd10881, %fd10369;
	mov.f64 	%fd10882, %fd10369;
	mov.f64 	%fd10883, %fd10369;
	mov.f64 	%fd10884, %fd10369;
	mov.f64 	%fd10885, %fd10369;
	mov.f64 	%fd10886, %fd10369;
	mov.f64 	%fd10887, %fd10369;
	mov.f64 	%fd10888, %fd10369;
	mov.f64 	%fd10889, %fd10369;
	mov.f64 	%fd10890, %fd10369;
	mov.f64 	%fd10891, %fd10369;
	mov.f64 	%fd10892, %fd10369;
	mov.f64 	%fd10893, %fd10369;
	mov.f64 	%fd10894, %fd10369;
	mov.f64 	%fd10895, %fd10369;
	mov.f64 	%fd10896, %fd10369;
	mov.f64 	%fd10897, %fd10369;
	mov.f64 	%fd10898, %fd10369;
	mov.f64 	%fd10899, %fd10369;
	mov.f64 	%fd10900, %fd10369;
	mov.f64 	%fd10901, %fd10369;
	mov.f64 	%fd10902, %fd10369;
	mov.f64 	%fd10903, %fd10369;
	mov.f64 	%fd10904, %fd10369;
	mov.f64 	%fd10905, %fd10369;
	mov.f64 	%fd10906, %fd10369;
	mov.f64 	%fd10907, %fd10369;
	mov.f64 	%fd10908, %fd10369;
	mov.f64 	%fd10909, %fd10369;
	mov.f64 	%fd10910, %fd10369;
	mov.f64 	%fd10911, %fd10369;
	mov.f64 	%fd10912, %fd10369;
	mov.f64 	%fd10913, %fd10369;
	mov.f64 	%fd10914, %fd10369;
	mov.f64 	%fd10915, %fd10369;
	mov.f64 	%fd10916, %fd10369;
	mov.f64 	%fd10917, %fd10369;
	mov.f64 	%fd10918, %fd10369;
	mov.f64 	%fd10919, %fd10369;
	mov.f64 	%fd10920, %fd10369;
	mov.f64 	%fd10921, %fd10369;
	mov.f64 	%fd10922, %fd10369;
	mov.f64 	%fd10923, %fd10369;
	mov.f64 	%fd10924, %fd10369;
	mov.f64 	%fd10925, %fd10369;
	mov.f64 	%fd10926, %fd10369;
	mov.f64 	%fd10927, %fd10369;
	mov.f64 	%fd10928, %fd10369;
	mov.f64 	%fd10929, %fd10369;
	mov.f64 	%fd10930, %fd10369;
	mov.f64 	%fd10931, %fd10369;
	mov.f64 	%fd10932, %fd10369;
	mov.f64 	%fd10933, %fd10369;
	mov.f64 	%fd10934, %fd10369;
	mov.f64 	%fd10935, %fd10369;
	mov.f64 	%fd10936, %fd10369;
	mov.f64 	%fd10937, %fd10369;
	mov.f64 	%fd10938, %fd10369;
	mov.f64 	%fd10939, %fd10369;
	mov.f64 	%fd10940, %fd10369;
	mov.f64 	%fd10941, %fd10369;
	mov.f64 	%fd10942, %fd10369;
	mov.f64 	%fd10943, %fd10369;
	mov.f64 	%fd10944, %fd10369;
	@%p9 bra 	$L__BB1_208;

	cvt.s64.s32 	%rd29, %r10;
	mul.lo.s64 	%rd376, %rd29, %rd11;
	add.s64 	%rd377, %rd5, %rd376;
	ld.global.f64 	%fd10513, [%rd377+4600];
	ld.global.f64 	%fd10514, [%rd377+4592];
	ld.global.f64 	%fd10515, [%rd377+4584];
	ld.global.f64 	%fd10516, [%rd377+4576];
	ld.global.f64 	%fd10517, [%rd377+4568];
	ld.global.f64 	%fd10518, [%rd377+4560];
	ld.global.f64 	%fd10519, [%rd377+4552];
	ld.global.f64 	%fd10520, [%rd377+4544];
	ld.global.f64 	%fd10521, [%rd377+4536];
	ld.global.f64 	%fd10522, [%rd377+4528];
	ld.global.f64 	%fd10523, [%rd377+4520];
	ld.global.f64 	%fd10524, [%rd377+4512];
	ld.global.f64 	%fd10801, [%rd377+4504];
	ld.global.f64 	%fd10802, [%rd377+4496];
	ld.global.f64 	%fd10803, [%rd377+4488];
	ld.global.f64 	%fd10804, [%rd377+4480];
	ld.global.f64 	%fd10805, [%rd377+4472];
	ld.global.f64 	%fd10806, [%rd377+4464];
	ld.global.f64 	%fd10807, [%rd377+4456];
	ld.global.f64 	%fd10808, [%rd377+4448];
	ld.global.f64 	%fd10809, [%rd377+4440];
	ld.global.f64 	%fd10810, [%rd377+4432];
	ld.global.f64 	%fd10811, [%rd377+4424];
	ld.global.f64 	%fd10812, [%rd377+4416];
	ld.global.f64 	%fd10525, [%rd377+4408];
	ld.global.f64 	%fd10526, [%rd377+4400];
	ld.global.f64 	%fd10527, [%rd377+4392];
	ld.global.f64 	%fd10528, [%rd377+4384];
	ld.global.f64 	%fd10529, [%rd377+4376];
	ld.global.f64 	%fd10530, [%rd377+4368];
	ld.global.f64 	%fd10531, [%rd377+4360];
	ld.global.f64 	%fd10532, [%rd377+4352];
	ld.global.f64 	%fd10533, [%rd377+4344];
	ld.global.f64 	%fd10534, [%rd377+4336];
	ld.global.f64 	%fd10535, [%rd377+4328];
	ld.global.f64 	%fd10536, [%rd377+4320];
	ld.global.f64 	%fd10813, [%rd377+4312];
	ld.global.f64 	%fd10814, [%rd377+4304];
	ld.global.f64 	%fd10815, [%rd377+4296];
	ld.global.f64 	%fd10816, [%rd377+4288];
	ld.global.f64 	%fd10817, [%rd377+4280];
	ld.global.f64 	%fd10818, [%rd377+4272];
	ld.global.f64 	%fd10819, [%rd377+4264];
	ld.global.f64 	%fd10820, [%rd377+4256];
	ld.global.f64 	%fd10821, [%rd377+4248];
	ld.global.f64 	%fd10822, [%rd377+4240];
	ld.global.f64 	%fd10823, [%rd377+4232];
	ld.global.f64 	%fd10824, [%rd377+4224];
	ld.global.f64 	%fd10537, [%rd377+4216];
	ld.global.f64 	%fd10538, [%rd377+4208];
	ld.global.f64 	%fd10539, [%rd377+4200];
	ld.global.f64 	%fd10540, [%rd377+4192];
	ld.global.f64 	%fd10541, [%rd377+4184];
	ld.global.f64 	%fd10542, [%rd377+4176];
	ld.global.f64 	%fd10543, [%rd377+4168];
	ld.global.f64 	%fd10544, [%rd377+4160];
	ld.global.f64 	%fd10545, [%rd377+4152];
	ld.global.f64 	%fd10546, [%rd377+4144];
	ld.global.f64 	%fd10547, [%rd377+4136];
	ld.global.f64 	%fd10548, [%rd377+4128];
	ld.global.f64 	%fd10825, [%rd377+4120];
	ld.global.f64 	%fd10826, [%rd377+4112];
	ld.global.f64 	%fd10827, [%rd377+4104];
	ld.global.f64 	%fd10828, [%rd377+4096];
	ld.global.f64 	%fd10829, [%rd377+4088];
	ld.global.f64 	%fd10830, [%rd377+4080];
	ld.global.f64 	%fd10831, [%rd377+4072];
	ld.global.f64 	%fd10832, [%rd377+4064];
	ld.global.f64 	%fd10833, [%rd377+4056];
	ld.global.f64 	%fd10834, [%rd377+4048];
	ld.global.f64 	%fd10835, [%rd377+4040];
	ld.global.f64 	%fd10836, [%rd377+4032];
	ld.global.f64 	%fd10549, [%rd377+4024];
	ld.global.f64 	%fd10550, [%rd377+4016];
	ld.global.f64 	%fd10551, [%rd377+4008];
	ld.global.f64 	%fd10552, [%rd377+4000];
	ld.global.f64 	%fd10553, [%rd377+3992];
	ld.global.f64 	%fd10554, [%rd377+3984];
	ld.global.f64 	%fd10555, [%rd377+3976];
	ld.global.f64 	%fd10556, [%rd377+3968];
	ld.global.f64 	%fd10557, [%rd377+3960];
	ld.global.f64 	%fd10558, [%rd377+3952];
	ld.global.f64 	%fd10559, [%rd377+3944];
	ld.global.f64 	%fd10560, [%rd377+3936];
	ld.global.f64 	%fd10837, [%rd377+3928];
	ld.global.f64 	%fd10838, [%rd377+3920];
	ld.global.f64 	%fd10839, [%rd377+3912];
	ld.global.f64 	%fd10840, [%rd377+3904];
	ld.global.f64 	%fd10841, [%rd377+3896];
	ld.global.f64 	%fd10842, [%rd377+3888];
	ld.global.f64 	%fd10843, [%rd377+3880];
	ld.global.f64 	%fd10844, [%rd377+3872];
	ld.global.f64 	%fd10845, [%rd377+3864];
	ld.global.f64 	%fd10846, [%rd377+3856];
	ld.global.f64 	%fd10847, [%rd377+3848];
	ld.global.f64 	%fd10848, [%rd377+3840];
	ld.global.f64 	%fd10561, [%rd377+3832];
	ld.global.f64 	%fd10562, [%rd377+3824];
	ld.global.f64 	%fd10563, [%rd377+3816];
	ld.global.f64 	%fd10564, [%rd377+3808];
	ld.global.f64 	%fd10565, [%rd377+3800];
	ld.global.f64 	%fd10566, [%rd377+3792];
	ld.global.f64 	%fd10567, [%rd377+3784];
	ld.global.f64 	%fd10568, [%rd377+3776];
	ld.global.f64 	%fd10569, [%rd377+3768];
	ld.global.f64 	%fd10570, [%rd377+3760];
	ld.global.f64 	%fd10571, [%rd377+3752];
	ld.global.f64 	%fd10572, [%rd377+3744];
	ld.global.f64 	%fd10849, [%rd377+3736];
	ld.global.f64 	%fd10850, [%rd377+3728];
	ld.global.f64 	%fd10851, [%rd377+3720];
	ld.global.f64 	%fd10852, [%rd377+3712];
	ld.global.f64 	%fd10853, [%rd377+3704];
	ld.global.f64 	%fd10854, [%rd377+3696];
	ld.global.f64 	%fd10855, [%rd377+3688];
	ld.global.f64 	%fd10856, [%rd377+3680];
	ld.global.f64 	%fd10857, [%rd377+3672];
	ld.global.f64 	%fd10858, [%rd377+3664];
	ld.global.f64 	%fd10859, [%rd377+3656];
	ld.global.f64 	%fd10860, [%rd377+3648];
	ld.global.f64 	%fd10573, [%rd377+3640];
	ld.global.f64 	%fd10574, [%rd377+3632];
	ld.global.f64 	%fd10575, [%rd377+3624];
	ld.global.f64 	%fd10576, [%rd377+3616];
	ld.global.f64 	%fd10577, [%rd377+3608];
	ld.global.f64 	%fd10578, [%rd377+3600];
	ld.global.f64 	%fd10579, [%rd377+3592];
	ld.global.f64 	%fd10580, [%rd377+3584];
	ld.global.f64 	%fd10581, [%rd377+3576];
	ld.global.f64 	%fd10582, [%rd377+3568];
	ld.global.f64 	%fd10583, [%rd377+3560];
	ld.global.f64 	%fd10584, [%rd377+3552];
	ld.global.f64 	%fd10861, [%rd377+3544];
	ld.global.f64 	%fd10862, [%rd377+3536];
	ld.global.f64 	%fd10863, [%rd377+3528];
	ld.global.f64 	%fd10864, [%rd377+3520];
	ld.global.f64 	%fd10865, [%rd377+3512];
	ld.global.f64 	%fd10866, [%rd377+3504];
	ld.global.f64 	%fd10867, [%rd377+3496];
	ld.global.f64 	%fd10868, [%rd377+3488];
	ld.global.f64 	%fd10869, [%rd377+3480];
	ld.global.f64 	%fd10870, [%rd377+3472];
	ld.global.f64 	%fd10871, [%rd377+3464];
	ld.global.f64 	%fd10872, [%rd377+3456];
	ld.global.f64 	%fd10585, [%rd377+3448];
	ld.global.f64 	%fd10586, [%rd377+3440];
	ld.global.f64 	%fd10587, [%rd377+3432];
	ld.global.f64 	%fd10588, [%rd377+3424];
	ld.global.f64 	%fd10589, [%rd377+3416];
	ld.global.f64 	%fd10590, [%rd377+3408];
	ld.global.f64 	%fd10591, [%rd377+3400];
	ld.global.f64 	%fd10592, [%rd377+3392];
	ld.global.f64 	%fd10593, [%rd377+3384];
	ld.global.f64 	%fd10594, [%rd377+3376];
	ld.global.f64 	%fd10595, [%rd377+3368];
	ld.global.f64 	%fd10596, [%rd377+3360];
	ld.global.f64 	%fd10873, [%rd377+3352];
	ld.global.f64 	%fd10874, [%rd377+3344];
	ld.global.f64 	%fd10875, [%rd377+3336];
	ld.global.f64 	%fd10876, [%rd377+3328];
	ld.global.f64 	%fd10877, [%rd377+3320];
	ld.global.f64 	%fd10878, [%rd377+3312];
	ld.global.f64 	%fd10879, [%rd377+3304];
	ld.global.f64 	%fd10880, [%rd377+3296];
	ld.global.f64 	%fd10881, [%rd377+3288];
	ld.global.f64 	%fd10882, [%rd377+3280];
	ld.global.f64 	%fd10883, [%rd377+3272];
	ld.global.f64 	%fd10884, [%rd377+3264];
	ld.global.f64 	%fd10597, [%rd377+3256];
	ld.global.f64 	%fd10598, [%rd377+3248];
	ld.global.f64 	%fd10599, [%rd377+3240];
	ld.global.f64 	%fd10600, [%rd377+3232];
	ld.global.f64 	%fd10601, [%rd377+3224];
	ld.global.f64 	%fd10602, [%rd377+3216];
	ld.global.f64 	%fd10603, [%rd377+3208];
	ld.global.f64 	%fd10604, [%rd377+3200];
	ld.global.f64 	%fd10605, [%rd377+3192];
	ld.global.f64 	%fd10606, [%rd377+3184];
	ld.global.f64 	%fd10607, [%rd377+3176];
	ld.global.f64 	%fd10608, [%rd377+3168];
	ld.global.f64 	%fd10885, [%rd377+3160];
	ld.global.f64 	%fd10886, [%rd377+3152];
	ld.global.f64 	%fd10887, [%rd377+3144];
	ld.global.f64 	%fd10888, [%rd377+3136];
	ld.global.f64 	%fd10889, [%rd377+3128];
	ld.global.f64 	%fd10890, [%rd377+3120];
	ld.global.f64 	%fd10891, [%rd377+3112];
	ld.global.f64 	%fd10892, [%rd377+3104];
	ld.global.f64 	%fd10893, [%rd377+3096];
	ld.global.f64 	%fd10894, [%rd377+3088];
	ld.global.f64 	%fd10895, [%rd377+3080];
	ld.global.f64 	%fd10896, [%rd377+3072];
	ld.global.f64 	%fd10609, [%rd377+3064];
	ld.global.f64 	%fd10610, [%rd377+3056];
	ld.global.f64 	%fd10611, [%rd377+3048];
	ld.global.f64 	%fd10612, [%rd377+3040];
	ld.global.f64 	%fd10613, [%rd377+3032];
	ld.global.f64 	%fd10614, [%rd377+3024];
	ld.global.f64 	%fd10615, [%rd377+3016];
	ld.global.f64 	%fd10616, [%rd377+3008];
	ld.global.f64 	%fd10617, [%rd377+3000];
	ld.global.f64 	%fd10618, [%rd377+2992];
	ld.global.f64 	%fd10619, [%rd377+2984];
	ld.global.f64 	%fd10620, [%rd377+2976];
	ld.global.f64 	%fd10897, [%rd377+2968];
	ld.global.f64 	%fd10898, [%rd377+2960];
	ld.global.f64 	%fd10899, [%rd377+2952];
	ld.global.f64 	%fd10900, [%rd377+2944];
	ld.global.f64 	%fd10901, [%rd377+2936];
	ld.global.f64 	%fd10902, [%rd377+2928];
	ld.global.f64 	%fd10903, [%rd377+2920];
	ld.global.f64 	%fd10904, [%rd377+2912];
	ld.global.f64 	%fd10905, [%rd377+2904];
	ld.global.f64 	%fd10906, [%rd377+2896];
	ld.global.f64 	%fd10907, [%rd377+2888];
	ld.global.f64 	%fd10908, [%rd377+2880];
	ld.global.f64 	%fd10621, [%rd377+2872];
	ld.global.f64 	%fd10622, [%rd377+2864];
	ld.global.f64 	%fd10623, [%rd377+2856];
	ld.global.f64 	%fd10624, [%rd377+2848];
	ld.global.f64 	%fd10625, [%rd377+2840];
	ld.global.f64 	%fd10626, [%rd377+2832];
	ld.global.f64 	%fd10627, [%rd377+2824];
	ld.global.f64 	%fd10628, [%rd377+2816];
	ld.global.f64 	%fd10629, [%rd377+2808];
	ld.global.f64 	%fd10630, [%rd377+2800];
	ld.global.f64 	%fd10631, [%rd377+2792];
	ld.global.f64 	%fd10632, [%rd377+2784];
	ld.global.f64 	%fd10909, [%rd377+2776];
	ld.global.f64 	%fd10910, [%rd377+2768];
	ld.global.f64 	%fd10911, [%rd377+2760];
	ld.global.f64 	%fd10912, [%rd377+2752];
	ld.global.f64 	%fd10913, [%rd377+2744];
	ld.global.f64 	%fd10914, [%rd377+2736];
	ld.global.f64 	%fd10915, [%rd377+2728];
	ld.global.f64 	%fd10916, [%rd377+2720];
	ld.global.f64 	%fd10917, [%rd377+2712];
	ld.global.f64 	%fd10918, [%rd377+2704];
	ld.global.f64 	%fd10919, [%rd377+2696];
	ld.global.f64 	%fd10920, [%rd377+2688];
	ld.global.f64 	%fd10633, [%rd377+2680];
	ld.global.f64 	%fd10634, [%rd377+2672];
	ld.global.f64 	%fd10635, [%rd377+2664];
	ld.global.f64 	%fd10636, [%rd377+2656];
	ld.global.f64 	%fd10637, [%rd377+2648];
	ld.global.f64 	%fd10638, [%rd377+2640];
	ld.global.f64 	%fd10639, [%rd377+2632];
	ld.global.f64 	%fd10640, [%rd377+2624];
	ld.global.f64 	%fd10641, [%rd377+2616];
	ld.global.f64 	%fd10642, [%rd377+2608];
	ld.global.f64 	%fd10643, [%rd377+2600];
	ld.global.f64 	%fd10644, [%rd377+2592];
	ld.global.f64 	%fd10921, [%rd377+2584];
	ld.global.f64 	%fd10922, [%rd377+2576];
	ld.global.f64 	%fd10923, [%rd377+2568];
	ld.global.f64 	%fd10924, [%rd377+2560];
	ld.global.f64 	%fd10925, [%rd377+2552];
	ld.global.f64 	%fd10926, [%rd377+2544];
	ld.global.f64 	%fd10927, [%rd377+2536];
	ld.global.f64 	%fd10928, [%rd377+2528];
	ld.global.f64 	%fd10929, [%rd377+2520];
	ld.global.f64 	%fd10930, [%rd377+2512];
	ld.global.f64 	%fd10931, [%rd377+2504];
	ld.global.f64 	%fd10932, [%rd377+2496];
	ld.global.f64 	%fd10645, [%rd377+2488];
	ld.global.f64 	%fd10646, [%rd377+2480];
	ld.global.f64 	%fd10647, [%rd377+2472];
	ld.global.f64 	%fd10648, [%rd377+2464];
	ld.global.f64 	%fd10649, [%rd377+2456];
	ld.global.f64 	%fd10650, [%rd377+2448];
	ld.global.f64 	%fd10651, [%rd377+2440];
	ld.global.f64 	%fd10652, [%rd377+2432];
	ld.global.f64 	%fd10653, [%rd377+2424];
	ld.global.f64 	%fd10654, [%rd377+2416];
	ld.global.f64 	%fd10655, [%rd377+2408];
	ld.global.f64 	%fd10656, [%rd377+2400];
	ld.global.f64 	%fd10933, [%rd377+2392];
	ld.global.f64 	%fd10934, [%rd377+2384];
	ld.global.f64 	%fd10935, [%rd377+2376];
	ld.global.f64 	%fd10936, [%rd377+2368];
	ld.global.f64 	%fd10937, [%rd377+2360];
	ld.global.f64 	%fd10938, [%rd377+2352];
	ld.global.f64 	%fd10939, [%rd377+2344];
	ld.global.f64 	%fd10940, [%rd377+2336];
	ld.global.f64 	%fd10941, [%rd377+2328];
	ld.global.f64 	%fd10942, [%rd377+2320];
	ld.global.f64 	%fd10943, [%rd377+2312];
	ld.global.f64 	%fd10944, [%rd377+2304];
	ld.global.f64 	%fd10657, [%rd377+2296];
	ld.global.f64 	%fd10658, [%rd377+2288];
	ld.global.f64 	%fd10659, [%rd377+2280];
	ld.global.f64 	%fd10660, [%rd377+2272];
	ld.global.f64 	%fd10661, [%rd377+2264];
	ld.global.f64 	%fd10662, [%rd377+2256];
	ld.global.f64 	%fd10663, [%rd377+2248];
	ld.global.f64 	%fd10664, [%rd377+2240];
	ld.global.f64 	%fd10665, [%rd377+2232];
	ld.global.f64 	%fd10666, [%rd377+2224];
	ld.global.f64 	%fd10667, [%rd377+2216];
	ld.global.f64 	%fd10668, [%rd377+2208];
	ld.global.f64 	%fd10369, [%rd377+2200];
	ld.global.f64 	%fd10370, [%rd377+2192];
	ld.global.f64 	%fd10371, [%rd377+2184];
	ld.global.f64 	%fd10372, [%rd377+2176];
	ld.global.f64 	%fd10373, [%rd377+2168];
	ld.global.f64 	%fd10374, [%rd377+2160];
	ld.global.f64 	%fd10375, [%rd377+2152];
	ld.global.f64 	%fd10376, [%rd377+2144];
	ld.global.f64 	%fd10377, [%rd377+2136];
	ld.global.f64 	%fd10378, [%rd377+2128];
	ld.global.f64 	%fd10379, [%rd377+2120];
	ld.global.f64 	%fd10380, [%rd377+2112];
	ld.global.f64 	%fd10669, [%rd377+2104];
	ld.global.f64 	%fd10670, [%rd377+2096];
	ld.global.f64 	%fd10671, [%rd377+2088];
	ld.global.f64 	%fd10672, [%rd377+2080];
	ld.global.f64 	%fd10673, [%rd377+2072];
	ld.global.f64 	%fd10674, [%rd377+2064];
	ld.global.f64 	%fd10675, [%rd377+2056];
	ld.global.f64 	%fd10676, [%rd377+2048];
	ld.global.f64 	%fd10677, [%rd377+2040];
	ld.global.f64 	%fd10678, [%rd377+2032];
	ld.global.f64 	%fd10679, [%rd377+2024];
	ld.global.f64 	%fd10680, [%rd377+2016];
	ld.global.f64 	%fd10381, [%rd377+2008];
	ld.global.f64 	%fd10382, [%rd377+2000];
	ld.global.f64 	%fd10383, [%rd377+1992];
	ld.global.f64 	%fd10384, [%rd377+1984];
	ld.global.f64 	%fd10385, [%rd377+1976];
	ld.global.f64 	%fd10386, [%rd377+1968];
	ld.global.f64 	%fd10387, [%rd377+1960];
	ld.global.f64 	%fd10388, [%rd377+1952];
	ld.global.f64 	%fd10389, [%rd377+1944];
	ld.global.f64 	%fd10390, [%rd377+1936];
	ld.global.f64 	%fd10391, [%rd377+1928];
	ld.global.f64 	%fd10392, [%rd377+1920];
	ld.global.f64 	%fd10681, [%rd377+1912];
	ld.global.f64 	%fd10682, [%rd377+1904];
	ld.global.f64 	%fd10683, [%rd377+1896];
	ld.global.f64 	%fd10684, [%rd377+1888];
	ld.global.f64 	%fd10685, [%rd377+1880];
	ld.global.f64 	%fd10686, [%rd377+1872];
	ld.global.f64 	%fd10687, [%rd377+1864];
	ld.global.f64 	%fd10688, [%rd377+1856];
	ld.global.f64 	%fd10689, [%rd377+1848];
	ld.global.f64 	%fd10690, [%rd377+1840];
	ld.global.f64 	%fd10691, [%rd377+1832];
	ld.global.f64 	%fd10692, [%rd377+1824];
	ld.global.f64 	%fd10393, [%rd377+1816];
	ld.global.f64 	%fd10394, [%rd377+1808];
	ld.global.f64 	%fd10395, [%rd377+1800];
	ld.global.f64 	%fd10396, [%rd377+1792];
	ld.global.f64 	%fd10397, [%rd377+1784];
	ld.global.f64 	%fd10398, [%rd377+1776];
	ld.global.f64 	%fd10399, [%rd377+1768];
	ld.global.f64 	%fd10400, [%rd377+1760];
	ld.global.f64 	%fd10401, [%rd377+1752];
	ld.global.f64 	%fd10402, [%rd377+1744];
	ld.global.f64 	%fd10403, [%rd377+1736];
	ld.global.f64 	%fd10404, [%rd377+1728];
	ld.global.f64 	%fd10693, [%rd377+1720];
	ld.global.f64 	%fd10694, [%rd377+1712];
	ld.global.f64 	%fd10695, [%rd377+1704];
	ld.global.f64 	%fd10696, [%rd377+1696];
	ld.global.f64 	%fd10697, [%rd377+1688];
	ld.global.f64 	%fd10698, [%rd377+1680];
	ld.global.f64 	%fd10699, [%rd377+1672];
	ld.global.f64 	%fd10700, [%rd377+1664];
	ld.global.f64 	%fd10701, [%rd377+1656];
	ld.global.f64 	%fd10702, [%rd377+1648];
	ld.global.f64 	%fd10703, [%rd377+1640];
	ld.global.f64 	%fd10704, [%rd377+1632];
	ld.global.f64 	%fd10405, [%rd377+1624];
	ld.global.f64 	%fd10406, [%rd377+1616];
	ld.global.f64 	%fd10407, [%rd377+1608];
	ld.global.f64 	%fd10408, [%rd377+1600];
	ld.global.f64 	%fd10409, [%rd377+1592];
	ld.global.f64 	%fd10410, [%rd377+1584];
	ld.global.f64 	%fd10411, [%rd377+1576];
	ld.global.f64 	%fd10412, [%rd377+1568];
	ld.global.f64 	%fd10413, [%rd377+1560];
	ld.global.f64 	%fd10414, [%rd377+1552];
	ld.global.f64 	%fd10415, [%rd377+1544];
	ld.global.f64 	%fd10416, [%rd377+1536];
	ld.global.f64 	%fd10705, [%rd377+1528];
	ld.global.f64 	%fd10706, [%rd377+1520];
	ld.global.f64 	%fd10707, [%rd377+1512];
	ld.global.f64 	%fd10708, [%rd377+1504];
	ld.global.f64 	%fd10709, [%rd377+1496];
	ld.global.f64 	%fd10710, [%rd377+1488];
	ld.global.f64 	%fd10711, [%rd377+1480];
	ld.global.f64 	%fd10712, [%rd377+1472];
	ld.global.f64 	%fd10713, [%rd377+1464];
	ld.global.f64 	%fd10714, [%rd377+1456];
	ld.global.f64 	%fd10715, [%rd377+1448];
	ld.global.f64 	%fd10716, [%rd377+1440];
	ld.global.f64 	%fd10417, [%rd377+1432];
	ld.global.f64 	%fd10418, [%rd377+1424];
	ld.global.f64 	%fd10419, [%rd377+1416];
	ld.global.f64 	%fd10420, [%rd377+1408];
	ld.global.f64 	%fd10421, [%rd377+1400];
	ld.global.f64 	%fd10422, [%rd377+1392];
	ld.global.f64 	%fd10423, [%rd377+1384];
	ld.global.f64 	%fd10424, [%rd377+1376];
	ld.global.f64 	%fd10425, [%rd377+1368];
	ld.global.f64 	%fd10426, [%rd377+1360];
	ld.global.f64 	%fd10427, [%rd377+1352];
	ld.global.f64 	%fd10428, [%rd377+1344];
	ld.global.f64 	%fd10717, [%rd377+1336];
	ld.global.f64 	%fd10718, [%rd377+1328];
	ld.global.f64 	%fd10719, [%rd377+1320];
	ld.global.f64 	%fd10720, [%rd377+1312];
	ld.global.f64 	%fd10721, [%rd377+1304];
	ld.global.f64 	%fd10722, [%rd377+1296];
	ld.global.f64 	%fd10723, [%rd377+1288];
	ld.global.f64 	%fd10724, [%rd377+1280];
	ld.global.f64 	%fd10725, [%rd377+1272];
	ld.global.f64 	%fd10726, [%rd377+1264];
	ld.global.f64 	%fd10727, [%rd377+1256];
	ld.global.f64 	%fd10728, [%rd377+1248];
	ld.global.f64 	%fd10429, [%rd377+1240];
	ld.global.f64 	%fd10430, [%rd377+1232];
	ld.global.f64 	%fd10431, [%rd377+1224];
	ld.global.f64 	%fd10432, [%rd377+1216];
	ld.global.f64 	%fd10433, [%rd377+1208];
	ld.global.f64 	%fd10434, [%rd377+1200];
	ld.global.f64 	%fd10435, [%rd377+1192];
	ld.global.f64 	%fd10436, [%rd377+1184];
	ld.global.f64 	%fd10437, [%rd377+1176];
	ld.global.f64 	%fd10438, [%rd377+1168];
	ld.global.f64 	%fd10439, [%rd377+1160];
	ld.global.f64 	%fd10440, [%rd377+1152];
	ld.global.f64 	%fd10729, [%rd377+1144];
	ld.global.f64 	%fd10730, [%rd377+1136];
	ld.global.f64 	%fd10731, [%rd377+1128];
	ld.global.f64 	%fd10732, [%rd377+1120];
	ld.global.f64 	%fd10733, [%rd377+1112];
	ld.global.f64 	%fd10734, [%rd377+1104];
	ld.global.f64 	%fd10735, [%rd377+1096];
	ld.global.f64 	%fd10736, [%rd377+1088];
	ld.global.f64 	%fd10737, [%rd377+1080];
	ld.global.f64 	%fd10738, [%rd377+1072];
	ld.global.f64 	%fd10739, [%rd377+1064];
	ld.global.f64 	%fd10740, [%rd377+1056];
	ld.global.f64 	%fd10441, [%rd377+1048];
	ld.global.f64 	%fd10442, [%rd377+1040];
	ld.global.f64 	%fd10443, [%rd377+1032];
	ld.global.f64 	%fd10444, [%rd377+1024];
	ld.global.f64 	%fd10445, [%rd377+1016];
	ld.global.f64 	%fd10446, [%rd377+1008];
	ld.global.f64 	%fd10447, [%rd377+1000];
	ld.global.f64 	%fd10448, [%rd377+992];
	ld.global.f64 	%fd10449, [%rd377+984];
	ld.global.f64 	%fd10450, [%rd377+976];
	ld.global.f64 	%fd10451, [%rd377+968];
	ld.global.f64 	%fd10452, [%rd377+960];
	ld.global.f64 	%fd10741, [%rd377+952];
	ld.global.f64 	%fd10742, [%rd377+944];
	ld.global.f64 	%fd10743, [%rd377+936];
	ld.global.f64 	%fd10744, [%rd377+928];
	ld.global.f64 	%fd10745, [%rd377+920];
	ld.global.f64 	%fd10746, [%rd377+912];
	ld.global.f64 	%fd10747, [%rd377+904];
	ld.global.f64 	%fd10748, [%rd377+896];
	ld.global.f64 	%fd10749, [%rd377+888];
	ld.global.f64 	%fd10750, [%rd377+880];
	ld.global.f64 	%fd10751, [%rd377+872];
	ld.global.f64 	%fd10752, [%rd377+864];
	ld.global.f64 	%fd10453, [%rd377+856];
	ld.global.f64 	%fd10454, [%rd377+848];
	ld.global.f64 	%fd10455, [%rd377+840];
	ld.global.f64 	%fd10456, [%rd377+832];
	ld.global.f64 	%fd10457, [%rd377+824];
	ld.global.f64 	%fd10458, [%rd377+816];
	ld.global.f64 	%fd10459, [%rd377+808];
	ld.global.f64 	%fd10460, [%rd377+800];
	ld.global.f64 	%fd10461, [%rd377+792];
	ld.global.f64 	%fd10462, [%rd377+784];
	ld.global.f64 	%fd10463, [%rd377+776];
	ld.global.f64 	%fd10464, [%rd377+768];
	ld.global.f64 	%fd10753, [%rd377+760];
	ld.global.f64 	%fd10754, [%rd377+752];
	ld.global.f64 	%fd10755, [%rd377+744];
	ld.global.f64 	%fd10756, [%rd377+736];
	ld.global.f64 	%fd10757, [%rd377+728];
	ld.global.f64 	%fd10758, [%rd377+720];
	ld.global.f64 	%fd10759, [%rd377+712];
	ld.global.f64 	%fd10760, [%rd377+704];
	ld.global.f64 	%fd10761, [%rd377+696];
	ld.global.f64 	%fd10762, [%rd377+688];
	ld.global.f64 	%fd10763, [%rd377+680];
	ld.global.f64 	%fd10764, [%rd377+672];
	ld.global.f64 	%fd10465, [%rd377+664];
	ld.global.f64 	%fd10466, [%rd377+656];
	ld.global.f64 	%fd10467, [%rd377+648];
	ld.global.f64 	%fd10468, [%rd377+640];
	ld.global.f64 	%fd10469, [%rd377+632];
	ld.global.f64 	%fd10470, [%rd377+624];
	ld.global.f64 	%fd10471, [%rd377+616];
	ld.global.f64 	%fd10472, [%rd377+608];
	ld.global.f64 	%fd10473, [%rd377+600];
	ld.global.f64 	%fd10474, [%rd377+592];
	ld.global.f64 	%fd10475, [%rd377+584];
	ld.global.f64 	%fd10476, [%rd377+576];
	ld.global.f64 	%fd10765, [%rd377+568];
	ld.global.f64 	%fd10766, [%rd377+560];
	ld.global.f64 	%fd10767, [%rd377+552];
	ld.global.f64 	%fd10768, [%rd377+544];
	ld.global.f64 	%fd10769, [%rd377+536];
	ld.global.f64 	%fd10770, [%rd377+528];
	ld.global.f64 	%fd10771, [%rd377+520];
	ld.global.f64 	%fd10772, [%rd377+512];
	ld.global.f64 	%fd10773, [%rd377+504];
	ld.global.f64 	%fd10774, [%rd377+496];
	ld.global.f64 	%fd10775, [%rd377+488];
	ld.global.f64 	%fd10776, [%rd377+480];
	ld.global.f64 	%fd10477, [%rd377+472];
	ld.global.f64 	%fd10478, [%rd377+464];
	ld.global.f64 	%fd10479, [%rd377+456];
	ld.global.f64 	%fd10480, [%rd377+448];
	ld.global.f64 	%fd10481, [%rd377+440];
	ld.global.f64 	%fd10482, [%rd377+432];
	ld.global.f64 	%fd10483, [%rd377+424];
	ld.global.f64 	%fd10484, [%rd377+416];
	ld.global.f64 	%fd10485, [%rd377+408];
	ld.global.f64 	%fd10486, [%rd377+400];
	ld.global.f64 	%fd10487, [%rd377+392];
	ld.global.f64 	%fd10488, [%rd377+384];
	ld.global.f64 	%fd10777, [%rd377+376];
	ld.global.f64 	%fd10778, [%rd377+368];
	ld.global.f64 	%fd10779, [%rd377+360];
	ld.global.f64 	%fd10780, [%rd377+352];
	ld.global.f64 	%fd10781, [%rd377+344];
	ld.global.f64 	%fd10782, [%rd377+336];
	ld.global.f64 	%fd10783, [%rd377+328];
	ld.global.f64 	%fd10784, [%rd377+320];
	ld.global.f64 	%fd10785, [%rd377+312];
	ld.global.f64 	%fd10786, [%rd377+304];
	ld.global.f64 	%fd10787, [%rd377+296];
	ld.global.f64 	%fd10788, [%rd377+288];
	ld.global.f64 	%fd10489, [%rd377+280];
	ld.global.f64 	%fd10490, [%rd377+272];
	ld.global.f64 	%fd10491, [%rd377+264];
	ld.global.f64 	%fd10492, [%rd377+256];
	ld.global.f64 	%fd10493, [%rd377+248];
	ld.global.f64 	%fd10494, [%rd377+240];
	ld.global.f64 	%fd10495, [%rd377+232];
	ld.global.f64 	%fd10496, [%rd377+224];
	ld.global.f64 	%fd10497, [%rd377+216];
	ld.global.f64 	%fd10498, [%rd377+208];
	ld.global.f64 	%fd10499, [%rd377+200];
	ld.global.f64 	%fd10500, [%rd377+192];
	ld.global.f64 	%fd10789, [%rd377+184];
	ld.global.f64 	%fd10790, [%rd377+176];
	ld.global.f64 	%fd10791, [%rd377+168];
	ld.global.f64 	%fd10792, [%rd377+160];
	ld.global.f64 	%fd10793, [%rd377+152];
	ld.global.f64 	%fd10794, [%rd377+144];
	ld.global.f64 	%fd10795, [%rd377+136];
	ld.global.f64 	%fd10796, [%rd377+128];
	ld.global.f64 	%fd10797, [%rd377+120];
	ld.global.f64 	%fd10798, [%rd377+112];
	ld.global.f64 	%fd10799, [%rd377+104];
	ld.global.f64 	%fd10800, [%rd377+96];
	ld.global.f64 	%fd10501, [%rd377+88];
	ld.global.f64 	%fd10502, [%rd377+80];
	ld.global.f64 	%fd10503, [%rd377+72];
	ld.global.f64 	%fd10504, [%rd377+64];
	ld.global.f64 	%fd10505, [%rd377+56];
	ld.global.f64 	%fd10506, [%rd377+48];
	ld.global.f64 	%fd10507, [%rd377+40];
	ld.global.f64 	%fd10508, [%rd377+32];
	ld.global.f64 	%fd10509, [%rd377+24];
	ld.global.f64 	%fd10510, [%rd377+16];
	ld.global.f64 	%fd10511, [%rd377+8];
	ld.global.f64 	%fd10512, [%rd377];
	mul.lo.s64 	%rd378, %rd29, %rd12;
	add.s64 	%rd30, %rd7, %rd378;
	ld.global.u32 	%r2253, [%rd30];
	shl.b32 	%r12, %r2253, 4;
	ld.param.u32 	%r14, [%rd25+60];
	setp.le.s32 	%p10, %r14, %r12;
	selp.u16 	%rs65, 1, 0, %p10;
	shr.u32 	%r1110, %r2253, 27;
	cvt.u16.u32 	%rs66, %r1110;
	and.b16  	%rs67, %rs66, 1;
	or.b16  	%rs68, %rs67, %rs65;
	setp.eq.s16 	%p11, %rs68, 0;
	@%p11 bra 	$L__BB1_17;

	st.local.v2.u32 	[%rd26], {%r12, %r14};
	mov.u64 	%rd379, $str;
	cvta.global.u64 	%rd380, %rd379;
	{ // callseq 316, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd380;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1111, [retval0+0];
	} // callseq 316
	bra.uni 	$L__BB1_18;

$L__BB1_17:
	ld.param.u64 	%rd3995, [%rd25];
	ld.param.u32 	%r2077, [%rd25+32];
	mul.wide.s32 	%rd391, %r2077, %r12;
	add.s64 	%rd382, %rd3995, %rd391;
	// begin inline asm
	{ atom.add.f64 %fd3457,[%rd382],%fd10512; }

	// end inline asm
	add.s64 	%rd383, %rd382, 8;
	// begin inline asm
	{ atom.add.f64 %fd3459,[%rd383],%fd10511; }

	// end inline asm
	add.s64 	%rd384, %rd382, 16;
	// begin inline asm
	{ atom.add.f64 %fd3461,[%rd384],%fd10510; }

	// end inline asm
	add.s64 	%rd385, %rd382, 24;
	// begin inline asm
	{ atom.add.f64 %fd3463,[%rd385],%fd10500; }

	// end inline asm
	add.s64 	%rd386, %rd382, 32;
	// begin inline asm
	{ atom.add.f64 %fd3465,[%rd386],%fd10499; }

	// end inline asm
	add.s64 	%rd387, %rd382, 40;
	// begin inline asm
	{ atom.add.f64 %fd3467,[%rd387],%fd10498; }

	// end inline asm
	add.s64 	%rd388, %rd382, 48;
	// begin inline asm
	{ atom.add.f64 %fd3469,[%rd388],%fd10488; }

	// end inline asm
	add.s64 	%rd389, %rd382, 56;
	// begin inline asm
	{ atom.add.f64 %fd3471,[%rd389],%fd10487; }

	// end inline asm
	add.s64 	%rd390, %rd382, 64;
	// begin inline asm
	{ atom.add.f64 %fd3473,[%rd390],%fd10486; }

	// end inline asm

$L__BB1_18:
	ld.param.u32 	%r16, [%rd25+60];
	add.s32 	%r17, %r12, 1;
	setp.le.s32 	%p12, %r16, %r17;
	selp.u16 	%rs69, 1, 0, %p12;
	shr.u32 	%r1112, %r17, 31;
	cvt.u16.u32 	%rs70, %r1112;
	or.b16  	%rs71, %rs69, %rs70;
	setp.eq.s16 	%p13, %rs71, 0;
	@%p13 bra 	$L__BB1_20;

	add.s32 	%r1657, %r12, 1;
	st.local.v2.u32 	[%rd26], {%r1657, %r16};
	mov.u64 	%rd392, $str;
	cvta.global.u64 	%rd393, %rd392;
	{ // callseq 317, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd393;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1113, [retval0+0];
	} // callseq 317
	bra.uni 	$L__BB1_21;

$L__BB1_20:
	ld.param.u64 	%rd4079, [%rd25];
	ld.param.u32 	%r2193, [%rd25+32];
	mul.wide.s32 	%rd404, %r2193, %r17;
	add.s64 	%rd395, %rd4079, %rd404;
	// begin inline asm
	{ atom.add.f64 %fd3475,[%rd395],%fd10509; }

	// end inline asm
	add.s64 	%rd396, %rd395, 8;
	// begin inline asm
	{ atom.add.f64 %fd3477,[%rd396],%fd10508; }

	// end inline asm
	add.s64 	%rd397, %rd395, 16;
	// begin inline asm
	{ atom.add.f64 %fd3479,[%rd397],%fd10507; }

	// end inline asm
	add.s64 	%rd398, %rd395, 24;
	// begin inline asm
	{ atom.add.f64 %fd3481,[%rd398],%fd10497; }

	// end inline asm
	add.s64 	%rd399, %rd395, 32;
	// begin inline asm
	{ atom.add.f64 %fd3483,[%rd399],%fd10496; }

	// end inline asm
	add.s64 	%rd400, %rd395, 40;
	// begin inline asm
	{ atom.add.f64 %fd3485,[%rd400],%fd10495; }

	// end inline asm
	add.s64 	%rd401, %rd395, 48;
	// begin inline asm
	{ atom.add.f64 %fd3487,[%rd401],%fd10485; }

	// end inline asm
	add.s64 	%rd402, %rd395, 56;
	// begin inline asm
	{ atom.add.f64 %fd3489,[%rd402],%fd10484; }

	// end inline asm
	add.s64 	%rd403, %rd395, 64;
	// begin inline asm
	{ atom.add.f64 %fd3491,[%rd403],%fd10483; }

	// end inline asm

$L__BB1_21:
	ld.param.u32 	%r19, [%rd25+60];
	add.s32 	%r20, %r12, 2;
	setp.le.s32 	%p14, %r19, %r20;
	selp.u16 	%rs72, 1, 0, %p14;
	shr.u32 	%r1114, %r20, 31;
	cvt.u16.u32 	%rs73, %r1114;
	or.b16  	%rs74, %rs72, %rs73;
	setp.eq.s16 	%p15, %rs74, 0;
	@%p15 bra 	$L__BB1_23;

	add.s32 	%r1658, %r12, 2;
	st.local.v2.u32 	[%rd26], {%r1658, %r19};
	mov.u64 	%rd405, $str;
	cvta.global.u64 	%rd406, %rd405;
	{ // callseq 318, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd406;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1115, [retval0+0];
	} // callseq 318
	bra.uni 	$L__BB1_24;

$L__BB1_23:
	ld.param.u64 	%rd4080, [%rd25];
	ld.param.u32 	%r2194, [%rd25+32];
	mul.wide.s32 	%rd417, %r2194, %r20;
	add.s64 	%rd408, %rd4080, %rd417;
	// begin inline asm
	{ atom.add.f64 %fd3493,[%rd408],%fd10506; }

	// end inline asm
	add.s64 	%rd409, %rd408, 8;
	// begin inline asm
	{ atom.add.f64 %fd3495,[%rd409],%fd10505; }

	// end inline asm
	add.s64 	%rd410, %rd408, 16;
	// begin inline asm
	{ atom.add.f64 %fd3497,[%rd410],%fd10504; }

	// end inline asm
	add.s64 	%rd411, %rd408, 24;
	// begin inline asm
	{ atom.add.f64 %fd3499,[%rd411],%fd10494; }

	// end inline asm
	add.s64 	%rd412, %rd408, 32;
	// begin inline asm
	{ atom.add.f64 %fd3501,[%rd412],%fd10493; }

	// end inline asm
	add.s64 	%rd413, %rd408, 40;
	// begin inline asm
	{ atom.add.f64 %fd3503,[%rd413],%fd10492; }

	// end inline asm
	add.s64 	%rd414, %rd408, 48;
	// begin inline asm
	{ atom.add.f64 %fd3505,[%rd414],%fd10482; }

	// end inline asm
	add.s64 	%rd415, %rd408, 56;
	// begin inline asm
	{ atom.add.f64 %fd3507,[%rd415],%fd10481; }

	// end inline asm
	add.s64 	%rd416, %rd408, 64;
	// begin inline asm
	{ atom.add.f64 %fd3509,[%rd416],%fd10480; }

	// end inline asm

$L__BB1_24:
	ld.param.u32 	%r22, [%rd25+60];
	add.s32 	%r23, %r12, 3;
	setp.le.s32 	%p16, %r22, %r23;
	selp.u16 	%rs75, 1, 0, %p16;
	shr.u32 	%r1116, %r23, 31;
	cvt.u16.u32 	%rs76, %r1116;
	or.b16  	%rs77, %rs75, %rs76;
	setp.eq.s16 	%p17, %rs77, 0;
	@%p17 bra 	$L__BB1_26;

	add.s32 	%r1659, %r12, 3;
	st.local.v2.u32 	[%rd26], {%r1659, %r22};
	mov.u64 	%rd418, $str;
	cvta.global.u64 	%rd419, %rd418;
	{ // callseq 319, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd419;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1117, [retval0+0];
	} // callseq 319
	bra.uni 	$L__BB1_27;

$L__BB1_26:
	ld.param.u64 	%rd4081, [%rd25];
	ld.param.u32 	%r2195, [%rd25+32];
	mul.wide.s32 	%rd430, %r2195, %r23;
	add.s64 	%rd421, %rd4081, %rd430;
	// begin inline asm
	{ atom.add.f64 %fd3511,[%rd421],%fd10503; }

	// end inline asm
	add.s64 	%rd422, %rd421, 8;
	// begin inline asm
	{ atom.add.f64 %fd3513,[%rd422],%fd10502; }

	// end inline asm
	add.s64 	%rd423, %rd421, 16;
	// begin inline asm
	{ atom.add.f64 %fd3515,[%rd423],%fd10501; }

	// end inline asm
	add.s64 	%rd424, %rd421, 24;
	// begin inline asm
	{ atom.add.f64 %fd3517,[%rd424],%fd10491; }

	// end inline asm
	add.s64 	%rd425, %rd421, 32;
	// begin inline asm
	{ atom.add.f64 %fd3519,[%rd425],%fd10490; }

	// end inline asm
	add.s64 	%rd426, %rd421, 40;
	// begin inline asm
	{ atom.add.f64 %fd3521,[%rd426],%fd10489; }

	// end inline asm
	add.s64 	%rd427, %rd421, 48;
	// begin inline asm
	{ atom.add.f64 %fd3523,[%rd427],%fd10479; }

	// end inline asm
	add.s64 	%rd428, %rd421, 56;
	// begin inline asm
	{ atom.add.f64 %fd3525,[%rd428],%fd10478; }

	// end inline asm
	add.s64 	%rd429, %rd421, 64;
	// begin inline asm
	{ atom.add.f64 %fd3527,[%rd429],%fd10477; }

	// end inline asm

$L__BB1_27:
	ld.param.u32 	%r25, [%rd25+60];
	add.s32 	%r26, %r12, 4;
	setp.le.s32 	%p18, %r25, %r26;
	selp.u16 	%rs78, 1, 0, %p18;
	shr.u32 	%r1118, %r26, 31;
	cvt.u16.u32 	%rs79, %r1118;
	or.b16  	%rs80, %rs78, %rs79;
	setp.eq.s16 	%p19, %rs80, 0;
	@%p19 bra 	$L__BB1_29;

	add.s32 	%r1660, %r12, 4;
	st.local.v2.u32 	[%rd26], {%r1660, %r25};
	mov.u64 	%rd431, $str;
	cvta.global.u64 	%rd432, %rd431;
	{ // callseq 320, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd432;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1119, [retval0+0];
	} // callseq 320
	bra.uni 	$L__BB1_30;

$L__BB1_29:
	ld.param.u64 	%rd4082, [%rd25];
	ld.param.u32 	%r2196, [%rd25+32];
	mul.wide.s32 	%rd443, %r2196, %r26;
	add.s64 	%rd434, %rd4082, %rd443;
	// begin inline asm
	{ atom.add.f64 %fd3529,[%rd434],%fd10476; }

	// end inline asm
	add.s64 	%rd435, %rd434, 8;
	// begin inline asm
	{ atom.add.f64 %fd3531,[%rd435],%fd10475; }

	// end inline asm
	add.s64 	%rd436, %rd434, 16;
	// begin inline asm
	{ atom.add.f64 %fd3533,[%rd436],%fd10474; }

	// end inline asm
	add.s64 	%rd437, %rd434, 24;
	// begin inline asm
	{ atom.add.f64 %fd3535,[%rd437],%fd10464; }

	// end inline asm
	add.s64 	%rd438, %rd434, 32;
	// begin inline asm
	{ atom.add.f64 %fd3537,[%rd438],%fd10463; }

	// end inline asm
	add.s64 	%rd439, %rd434, 40;
	// begin inline asm
	{ atom.add.f64 %fd3539,[%rd439],%fd10462; }

	// end inline asm
	add.s64 	%rd440, %rd434, 48;
	// begin inline asm
	{ atom.add.f64 %fd3541,[%rd440],%fd10452; }

	// end inline asm
	add.s64 	%rd441, %rd434, 56;
	// begin inline asm
	{ atom.add.f64 %fd3543,[%rd441],%fd10451; }

	// end inline asm
	add.s64 	%rd442, %rd434, 64;
	// begin inline asm
	{ atom.add.f64 %fd3545,[%rd442],%fd10450; }

	// end inline asm

$L__BB1_30:
	ld.param.u32 	%r28, [%rd25+60];
	add.s32 	%r29, %r12, 5;
	setp.le.s32 	%p20, %r28, %r29;
	selp.u16 	%rs81, 1, 0, %p20;
	shr.u32 	%r1120, %r29, 31;
	cvt.u16.u32 	%rs82, %r1120;
	or.b16  	%rs83, %rs81, %rs82;
	setp.eq.s16 	%p21, %rs83, 0;
	@%p21 bra 	$L__BB1_32;

	add.s32 	%r1661, %r12, 5;
	st.local.v2.u32 	[%rd26], {%r1661, %r28};
	mov.u64 	%rd444, $str;
	cvta.global.u64 	%rd445, %rd444;
	{ // callseq 321, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd445;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1121, [retval0+0];
	} // callseq 321
	bra.uni 	$L__BB1_33;

$L__BB1_32:
	ld.param.u64 	%rd4083, [%rd25];
	ld.param.u32 	%r2197, [%rd25+32];
	mul.wide.s32 	%rd456, %r2197, %r29;
	add.s64 	%rd447, %rd4083, %rd456;
	// begin inline asm
	{ atom.add.f64 %fd3547,[%rd447],%fd10473; }

	// end inline asm
	add.s64 	%rd448, %rd447, 8;
	// begin inline asm
	{ atom.add.f64 %fd3549,[%rd448],%fd10472; }

	// end inline asm
	add.s64 	%rd449, %rd447, 16;
	// begin inline asm
	{ atom.add.f64 %fd3551,[%rd449],%fd10471; }

	// end inline asm
	add.s64 	%rd450, %rd447, 24;
	// begin inline asm
	{ atom.add.f64 %fd3553,[%rd450],%fd10461; }

	// end inline asm
	add.s64 	%rd451, %rd447, 32;
	// begin inline asm
	{ atom.add.f64 %fd3555,[%rd451],%fd10460; }

	// end inline asm
	add.s64 	%rd452, %rd447, 40;
	// begin inline asm
	{ atom.add.f64 %fd3557,[%rd452],%fd10459; }

	// end inline asm
	add.s64 	%rd453, %rd447, 48;
	// begin inline asm
	{ atom.add.f64 %fd3559,[%rd453],%fd10449; }

	// end inline asm
	add.s64 	%rd454, %rd447, 56;
	// begin inline asm
	{ atom.add.f64 %fd3561,[%rd454],%fd10448; }

	// end inline asm
	add.s64 	%rd455, %rd447, 64;
	// begin inline asm
	{ atom.add.f64 %fd3563,[%rd455],%fd10447; }

	// end inline asm

$L__BB1_33:
	ld.param.u32 	%r31, [%rd25+60];
	add.s32 	%r32, %r12, 6;
	setp.le.s32 	%p22, %r31, %r32;
	selp.u16 	%rs84, 1, 0, %p22;
	shr.u32 	%r1122, %r32, 31;
	cvt.u16.u32 	%rs85, %r1122;
	or.b16  	%rs86, %rs84, %rs85;
	setp.eq.s16 	%p23, %rs86, 0;
	@%p23 bra 	$L__BB1_35;

	add.s32 	%r1662, %r12, 6;
	st.local.v2.u32 	[%rd26], {%r1662, %r31};
	mov.u64 	%rd457, $str;
	cvta.global.u64 	%rd458, %rd457;
	{ // callseq 322, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd458;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1123, [retval0+0];
	} // callseq 322
	bra.uni 	$L__BB1_36;

$L__BB1_35:
	ld.param.u64 	%rd4084, [%rd25];
	ld.param.u32 	%r2198, [%rd25+32];
	mul.wide.s32 	%rd469, %r2198, %r32;
	add.s64 	%rd460, %rd4084, %rd469;
	// begin inline asm
	{ atom.add.f64 %fd3565,[%rd460],%fd10470; }

	// end inline asm
	add.s64 	%rd461, %rd460, 8;
	// begin inline asm
	{ atom.add.f64 %fd3567,[%rd461],%fd10469; }

	// end inline asm
	add.s64 	%rd462, %rd460, 16;
	// begin inline asm
	{ atom.add.f64 %fd3569,[%rd462],%fd10468; }

	// end inline asm
	add.s64 	%rd463, %rd460, 24;
	// begin inline asm
	{ atom.add.f64 %fd3571,[%rd463],%fd10458; }

	// end inline asm
	add.s64 	%rd464, %rd460, 32;
	// begin inline asm
	{ atom.add.f64 %fd3573,[%rd464],%fd10457; }

	// end inline asm
	add.s64 	%rd465, %rd460, 40;
	// begin inline asm
	{ atom.add.f64 %fd3575,[%rd465],%fd10456; }

	// end inline asm
	add.s64 	%rd466, %rd460, 48;
	// begin inline asm
	{ atom.add.f64 %fd3577,[%rd466],%fd10446; }

	// end inline asm
	add.s64 	%rd467, %rd460, 56;
	// begin inline asm
	{ atom.add.f64 %fd3579,[%rd467],%fd10445; }

	// end inline asm
	add.s64 	%rd468, %rd460, 64;
	// begin inline asm
	{ atom.add.f64 %fd3581,[%rd468],%fd10444; }

	// end inline asm

$L__BB1_36:
	ld.param.u32 	%r34, [%rd25+60];
	add.s32 	%r35, %r12, 7;
	setp.le.s32 	%p24, %r34, %r35;
	selp.u16 	%rs87, 1, 0, %p24;
	shr.u32 	%r1124, %r35, 31;
	cvt.u16.u32 	%rs88, %r1124;
	or.b16  	%rs89, %rs87, %rs88;
	setp.eq.s16 	%p25, %rs89, 0;
	@%p25 bra 	$L__BB1_38;

	add.s32 	%r1663, %r12, 7;
	st.local.v2.u32 	[%rd26], {%r1663, %r34};
	mov.u64 	%rd470, $str;
	cvta.global.u64 	%rd471, %rd470;
	{ // callseq 323, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd471;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1125, [retval0+0];
	} // callseq 323
	bra.uni 	$L__BB1_39;

$L__BB1_38:
	ld.param.u64 	%rd4085, [%rd25];
	ld.param.u32 	%r2199, [%rd25+32];
	mul.wide.s32 	%rd482, %r2199, %r35;
	add.s64 	%rd473, %rd4085, %rd482;
	// begin inline asm
	{ atom.add.f64 %fd3583,[%rd473],%fd10467; }

	// end inline asm
	add.s64 	%rd474, %rd473, 8;
	// begin inline asm
	{ atom.add.f64 %fd3585,[%rd474],%fd10466; }

	// end inline asm
	add.s64 	%rd475, %rd473, 16;
	// begin inline asm
	{ atom.add.f64 %fd3587,[%rd475],%fd10465; }

	// end inline asm
	add.s64 	%rd476, %rd473, 24;
	// begin inline asm
	{ atom.add.f64 %fd3589,[%rd476],%fd10455; }

	// end inline asm
	add.s64 	%rd477, %rd473, 32;
	// begin inline asm
	{ atom.add.f64 %fd3591,[%rd477],%fd10454; }

	// end inline asm
	add.s64 	%rd478, %rd473, 40;
	// begin inline asm
	{ atom.add.f64 %fd3593,[%rd478],%fd10453; }

	// end inline asm
	add.s64 	%rd479, %rd473, 48;
	// begin inline asm
	{ atom.add.f64 %fd3595,[%rd479],%fd10443; }

	// end inline asm
	add.s64 	%rd480, %rd473, 56;
	// begin inline asm
	{ atom.add.f64 %fd3597,[%rd480],%fd10442; }

	// end inline asm
	add.s64 	%rd481, %rd473, 64;
	// begin inline asm
	{ atom.add.f64 %fd3599,[%rd481],%fd10441; }

	// end inline asm

$L__BB1_39:
	ld.param.u32 	%r37, [%rd25+60];
	add.s32 	%r38, %r12, 8;
	setp.le.s32 	%p26, %r37, %r38;
	selp.u16 	%rs90, 1, 0, %p26;
	shr.u32 	%r1126, %r38, 31;
	cvt.u16.u32 	%rs91, %r1126;
	or.b16  	%rs92, %rs90, %rs91;
	setp.eq.s16 	%p27, %rs92, 0;
	@%p27 bra 	$L__BB1_41;

	add.s32 	%r1664, %r12, 8;
	st.local.v2.u32 	[%rd26], {%r1664, %r37};
	mov.u64 	%rd483, $str;
	cvta.global.u64 	%rd484, %rd483;
	{ // callseq 324, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd484;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1127, [retval0+0];
	} // callseq 324
	bra.uni 	$L__BB1_42;

$L__BB1_41:
	ld.param.u64 	%rd4086, [%rd25];
	ld.param.u32 	%r2200, [%rd25+32];
	mul.wide.s32 	%rd495, %r2200, %r38;
	add.s64 	%rd486, %rd4086, %rd495;
	// begin inline asm
	{ atom.add.f64 %fd3601,[%rd486],%fd10440; }

	// end inline asm
	add.s64 	%rd487, %rd486, 8;
	// begin inline asm
	{ atom.add.f64 %fd3603,[%rd487],%fd10439; }

	// end inline asm
	add.s64 	%rd488, %rd486, 16;
	// begin inline asm
	{ atom.add.f64 %fd3605,[%rd488],%fd10438; }

	// end inline asm
	add.s64 	%rd489, %rd486, 24;
	// begin inline asm
	{ atom.add.f64 %fd3607,[%rd489],%fd10428; }

	// end inline asm
	add.s64 	%rd490, %rd486, 32;
	// begin inline asm
	{ atom.add.f64 %fd3609,[%rd490],%fd10427; }

	// end inline asm
	add.s64 	%rd491, %rd486, 40;
	// begin inline asm
	{ atom.add.f64 %fd3611,[%rd491],%fd10426; }

	// end inline asm
	add.s64 	%rd492, %rd486, 48;
	// begin inline asm
	{ atom.add.f64 %fd3613,[%rd492],%fd10416; }

	// end inline asm
	add.s64 	%rd493, %rd486, 56;
	// begin inline asm
	{ atom.add.f64 %fd3615,[%rd493],%fd10415; }

	// end inline asm
	add.s64 	%rd494, %rd486, 64;
	// begin inline asm
	{ atom.add.f64 %fd3617,[%rd494],%fd10414; }

	// end inline asm

$L__BB1_42:
	ld.param.u32 	%r40, [%rd25+60];
	add.s32 	%r41, %r12, 9;
	setp.le.s32 	%p28, %r40, %r41;
	selp.u16 	%rs93, 1, 0, %p28;
	shr.u32 	%r1128, %r41, 31;
	cvt.u16.u32 	%rs94, %r1128;
	or.b16  	%rs95, %rs93, %rs94;
	setp.eq.s16 	%p29, %rs95, 0;
	@%p29 bra 	$L__BB1_44;

	add.s32 	%r1665, %r12, 9;
	st.local.v2.u32 	[%rd26], {%r1665, %r40};
	mov.u64 	%rd496, $str;
	cvta.global.u64 	%rd497, %rd496;
	{ // callseq 325, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd497;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1129, [retval0+0];
	} // callseq 325
	bra.uni 	$L__BB1_45;

$L__BB1_44:
	ld.param.u64 	%rd4087, [%rd25];
	ld.param.u32 	%r2201, [%rd25+32];
	mul.wide.s32 	%rd508, %r2201, %r41;
	add.s64 	%rd499, %rd4087, %rd508;
	// begin inline asm
	{ atom.add.f64 %fd3619,[%rd499],%fd10437; }

	// end inline asm
	add.s64 	%rd500, %rd499, 8;
	// begin inline asm
	{ atom.add.f64 %fd3621,[%rd500],%fd10436; }

	// end inline asm
	add.s64 	%rd501, %rd499, 16;
	// begin inline asm
	{ atom.add.f64 %fd3623,[%rd501],%fd10435; }

	// end inline asm
	add.s64 	%rd502, %rd499, 24;
	// begin inline asm
	{ atom.add.f64 %fd3625,[%rd502],%fd10425; }

	// end inline asm
	add.s64 	%rd503, %rd499, 32;
	// begin inline asm
	{ atom.add.f64 %fd3627,[%rd503],%fd10424; }

	// end inline asm
	add.s64 	%rd504, %rd499, 40;
	// begin inline asm
	{ atom.add.f64 %fd3629,[%rd504],%fd10423; }

	// end inline asm
	add.s64 	%rd505, %rd499, 48;
	// begin inline asm
	{ atom.add.f64 %fd3631,[%rd505],%fd10413; }

	// end inline asm
	add.s64 	%rd506, %rd499, 56;
	// begin inline asm
	{ atom.add.f64 %fd3633,[%rd506],%fd10412; }

	// end inline asm
	add.s64 	%rd507, %rd499, 64;
	// begin inline asm
	{ atom.add.f64 %fd3635,[%rd507],%fd10411; }

	// end inline asm

$L__BB1_45:
	ld.param.u32 	%r43, [%rd25+60];
	add.s32 	%r44, %r12, 10;
	setp.le.s32 	%p30, %r43, %r44;
	selp.u16 	%rs96, 1, 0, %p30;
	shr.u32 	%r1130, %r44, 31;
	cvt.u16.u32 	%rs97, %r1130;
	or.b16  	%rs98, %rs96, %rs97;
	setp.eq.s16 	%p31, %rs98, 0;
	@%p31 bra 	$L__BB1_47;

	add.s32 	%r1666, %r12, 10;
	st.local.v2.u32 	[%rd26], {%r1666, %r43};
	mov.u64 	%rd509, $str;
	cvta.global.u64 	%rd510, %rd509;
	{ // callseq 326, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd510;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1131, [retval0+0];
	} // callseq 326
	bra.uni 	$L__BB1_48;

$L__BB1_47:
	ld.param.u64 	%rd4088, [%rd25];
	ld.param.u32 	%r2202, [%rd25+32];
	mul.wide.s32 	%rd521, %r2202, %r44;
	add.s64 	%rd512, %rd4088, %rd521;
	// begin inline asm
	{ atom.add.f64 %fd3637,[%rd512],%fd10434; }

	// end inline asm
	add.s64 	%rd513, %rd512, 8;
	// begin inline asm
	{ atom.add.f64 %fd3639,[%rd513],%fd10433; }

	// end inline asm
	add.s64 	%rd514, %rd512, 16;
	// begin inline asm
	{ atom.add.f64 %fd3641,[%rd514],%fd10432; }

	// end inline asm
	add.s64 	%rd515, %rd512, 24;
	// begin inline asm
	{ atom.add.f64 %fd3643,[%rd515],%fd10422; }

	// end inline asm
	add.s64 	%rd516, %rd512, 32;
	// begin inline asm
	{ atom.add.f64 %fd3645,[%rd516],%fd10421; }

	// end inline asm
	add.s64 	%rd517, %rd512, 40;
	// begin inline asm
	{ atom.add.f64 %fd3647,[%rd517],%fd10420; }

	// end inline asm
	add.s64 	%rd518, %rd512, 48;
	// begin inline asm
	{ atom.add.f64 %fd3649,[%rd518],%fd10410; }

	// end inline asm
	add.s64 	%rd519, %rd512, 56;
	// begin inline asm
	{ atom.add.f64 %fd3651,[%rd519],%fd10409; }

	// end inline asm
	add.s64 	%rd520, %rd512, 64;
	// begin inline asm
	{ atom.add.f64 %fd3653,[%rd520],%fd10408; }

	// end inline asm

$L__BB1_48:
	ld.param.u32 	%r46, [%rd25+60];
	add.s32 	%r47, %r12, 11;
	setp.le.s32 	%p32, %r46, %r47;
	selp.u16 	%rs99, 1, 0, %p32;
	shr.u32 	%r1132, %r47, 31;
	cvt.u16.u32 	%rs100, %r1132;
	or.b16  	%rs101, %rs99, %rs100;
	setp.eq.s16 	%p33, %rs101, 0;
	@%p33 bra 	$L__BB1_50;

	add.s32 	%r1667, %r12, 11;
	st.local.v2.u32 	[%rd26], {%r1667, %r46};
	mov.u64 	%rd522, $str;
	cvta.global.u64 	%rd523, %rd522;
	{ // callseq 327, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd523;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1133, [retval0+0];
	} // callseq 327
	bra.uni 	$L__BB1_51;

$L__BB1_50:
	ld.param.u64 	%rd4089, [%rd25];
	ld.param.u32 	%r2203, [%rd25+32];
	mul.wide.s32 	%rd534, %r2203, %r47;
	add.s64 	%rd525, %rd4089, %rd534;
	// begin inline asm
	{ atom.add.f64 %fd3655,[%rd525],%fd10431; }

	// end inline asm
	add.s64 	%rd526, %rd525, 8;
	// begin inline asm
	{ atom.add.f64 %fd3657,[%rd526],%fd10430; }

	// end inline asm
	add.s64 	%rd527, %rd525, 16;
	// begin inline asm
	{ atom.add.f64 %fd3659,[%rd527],%fd10429; }

	// end inline asm
	add.s64 	%rd528, %rd525, 24;
	// begin inline asm
	{ atom.add.f64 %fd3661,[%rd528],%fd10419; }

	// end inline asm
	add.s64 	%rd529, %rd525, 32;
	// begin inline asm
	{ atom.add.f64 %fd3663,[%rd529],%fd10418; }

	// end inline asm
	add.s64 	%rd530, %rd525, 40;
	// begin inline asm
	{ atom.add.f64 %fd3665,[%rd530],%fd10417; }

	// end inline asm
	add.s64 	%rd531, %rd525, 48;
	// begin inline asm
	{ atom.add.f64 %fd3667,[%rd531],%fd10407; }

	// end inline asm
	add.s64 	%rd532, %rd525, 56;
	// begin inline asm
	{ atom.add.f64 %fd3669,[%rd532],%fd10406; }

	// end inline asm
	add.s64 	%rd533, %rd525, 64;
	// begin inline asm
	{ atom.add.f64 %fd3671,[%rd533],%fd10405; }

	// end inline asm

$L__BB1_51:
	ld.param.u32 	%r49, [%rd25+60];
	add.s32 	%r50, %r12, 12;
	setp.le.s32 	%p34, %r49, %r50;
	selp.u16 	%rs102, 1, 0, %p34;
	shr.u32 	%r1134, %r50, 31;
	cvt.u16.u32 	%rs103, %r1134;
	or.b16  	%rs104, %rs102, %rs103;
	setp.eq.s16 	%p35, %rs104, 0;
	@%p35 bra 	$L__BB1_53;

	add.s32 	%r1668, %r12, 12;
	st.local.v2.u32 	[%rd26], {%r1668, %r49};
	mov.u64 	%rd535, $str;
	cvta.global.u64 	%rd536, %rd535;
	{ // callseq 328, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd536;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1135, [retval0+0];
	} // callseq 328
	bra.uni 	$L__BB1_54;

$L__BB1_53:
	ld.param.u64 	%rd4090, [%rd25];
	ld.param.u32 	%r2204, [%rd25+32];
	mul.wide.s32 	%rd547, %r2204, %r50;
	add.s64 	%rd538, %rd4090, %rd547;
	// begin inline asm
	{ atom.add.f64 %fd3673,[%rd538],%fd10404; }

	// end inline asm
	add.s64 	%rd539, %rd538, 8;
	// begin inline asm
	{ atom.add.f64 %fd3675,[%rd539],%fd10403; }

	// end inline asm
	add.s64 	%rd540, %rd538, 16;
	// begin inline asm
	{ atom.add.f64 %fd3677,[%rd540],%fd10402; }

	// end inline asm
	add.s64 	%rd541, %rd538, 24;
	// begin inline asm
	{ atom.add.f64 %fd3679,[%rd541],%fd10392; }

	// end inline asm
	add.s64 	%rd542, %rd538, 32;
	// begin inline asm
	{ atom.add.f64 %fd3681,[%rd542],%fd10391; }

	// end inline asm
	add.s64 	%rd543, %rd538, 40;
	// begin inline asm
	{ atom.add.f64 %fd3683,[%rd543],%fd10390; }

	// end inline asm
	add.s64 	%rd544, %rd538, 48;
	// begin inline asm
	{ atom.add.f64 %fd3685,[%rd544],%fd10380; }

	// end inline asm
	add.s64 	%rd545, %rd538, 56;
	// begin inline asm
	{ atom.add.f64 %fd3687,[%rd545],%fd10379; }

	// end inline asm
	add.s64 	%rd546, %rd538, 64;
	// begin inline asm
	{ atom.add.f64 %fd3689,[%rd546],%fd10378; }

	// end inline asm

$L__BB1_54:
	ld.param.u32 	%r52, [%rd25+60];
	add.s32 	%r53, %r12, 13;
	setp.le.s32 	%p36, %r52, %r53;
	selp.u16 	%rs105, 1, 0, %p36;
	shr.u32 	%r1136, %r53, 31;
	cvt.u16.u32 	%rs106, %r1136;
	or.b16  	%rs107, %rs105, %rs106;
	setp.eq.s16 	%p37, %rs107, 0;
	@%p37 bra 	$L__BB1_56;

	add.s32 	%r1669, %r12, 13;
	st.local.v2.u32 	[%rd26], {%r1669, %r52};
	mov.u64 	%rd548, $str;
	cvta.global.u64 	%rd549, %rd548;
	{ // callseq 329, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd549;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1137, [retval0+0];
	} // callseq 329
	bra.uni 	$L__BB1_57;

$L__BB1_56:
	ld.param.u64 	%rd4091, [%rd25];
	ld.param.u32 	%r2205, [%rd25+32];
	mul.wide.s32 	%rd560, %r2205, %r53;
	add.s64 	%rd551, %rd4091, %rd560;
	// begin inline asm
	{ atom.add.f64 %fd3691,[%rd551],%fd10401; }

	// end inline asm
	add.s64 	%rd552, %rd551, 8;
	// begin inline asm
	{ atom.add.f64 %fd3693,[%rd552],%fd10400; }

	// end inline asm
	add.s64 	%rd553, %rd551, 16;
	// begin inline asm
	{ atom.add.f64 %fd3695,[%rd553],%fd10399; }

	// end inline asm
	add.s64 	%rd554, %rd551, 24;
	// begin inline asm
	{ atom.add.f64 %fd3697,[%rd554],%fd10389; }

	// end inline asm
	add.s64 	%rd555, %rd551, 32;
	// begin inline asm
	{ atom.add.f64 %fd3699,[%rd555],%fd10388; }

	// end inline asm
	add.s64 	%rd556, %rd551, 40;
	// begin inline asm
	{ atom.add.f64 %fd3701,[%rd556],%fd10387; }

	// end inline asm
	add.s64 	%rd557, %rd551, 48;
	// begin inline asm
	{ atom.add.f64 %fd3703,[%rd557],%fd10377; }

	// end inline asm
	add.s64 	%rd558, %rd551, 56;
	// begin inline asm
	{ atom.add.f64 %fd3705,[%rd558],%fd10376; }

	// end inline asm
	add.s64 	%rd559, %rd551, 64;
	// begin inline asm
	{ atom.add.f64 %fd3707,[%rd559],%fd10375; }

	// end inline asm

$L__BB1_57:
	ld.param.u32 	%r55, [%rd25+60];
	add.s32 	%r56, %r12, 14;
	setp.le.s32 	%p38, %r55, %r56;
	selp.u16 	%rs108, 1, 0, %p38;
	shr.u32 	%r1138, %r56, 31;
	cvt.u16.u32 	%rs109, %r1138;
	or.b16  	%rs110, %rs108, %rs109;
	setp.eq.s16 	%p39, %rs110, 0;
	@%p39 bra 	$L__BB1_59;

	add.s32 	%r1670, %r12, 14;
	st.local.v2.u32 	[%rd26], {%r1670, %r55};
	mov.u64 	%rd561, $str;
	cvta.global.u64 	%rd562, %rd561;
	{ // callseq 330, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd562;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1139, [retval0+0];
	} // callseq 330
	bra.uni 	$L__BB1_60;

$L__BB1_59:
	ld.param.u64 	%rd4092, [%rd25];
	ld.param.u32 	%r2206, [%rd25+32];
	mul.wide.s32 	%rd573, %r2206, %r56;
	add.s64 	%rd564, %rd4092, %rd573;
	// begin inline asm
	{ atom.add.f64 %fd3709,[%rd564],%fd10398; }

	// end inline asm
	add.s64 	%rd565, %rd564, 8;
	// begin inline asm
	{ atom.add.f64 %fd3711,[%rd565],%fd10397; }

	// end inline asm
	add.s64 	%rd566, %rd564, 16;
	// begin inline asm
	{ atom.add.f64 %fd3713,[%rd566],%fd10396; }

	// end inline asm
	add.s64 	%rd567, %rd564, 24;
	// begin inline asm
	{ atom.add.f64 %fd3715,[%rd567],%fd10386; }

	// end inline asm
	add.s64 	%rd568, %rd564, 32;
	// begin inline asm
	{ atom.add.f64 %fd3717,[%rd568],%fd10385; }

	// end inline asm
	add.s64 	%rd569, %rd564, 40;
	// begin inline asm
	{ atom.add.f64 %fd3719,[%rd569],%fd10384; }

	// end inline asm
	add.s64 	%rd570, %rd564, 48;
	// begin inline asm
	{ atom.add.f64 %fd3721,[%rd570],%fd10374; }

	// end inline asm
	add.s64 	%rd571, %rd564, 56;
	// begin inline asm
	{ atom.add.f64 %fd3723,[%rd571],%fd10373; }

	// end inline asm
	add.s64 	%rd572, %rd564, 64;
	// begin inline asm
	{ atom.add.f64 %fd3725,[%rd572],%fd10372; }

	// end inline asm

$L__BB1_60:
	ld.param.u32 	%r58, [%rd25+60];
	add.s32 	%r59, %r12, 15;
	setp.le.s32 	%p40, %r58, %r59;
	selp.u16 	%rs111, 1, 0, %p40;
	shr.u32 	%r1140, %r59, 31;
	cvt.u16.u32 	%rs112, %r1140;
	or.b16  	%rs113, %rs111, %rs112;
	setp.eq.s16 	%p41, %rs113, 0;
	@%p41 bra 	$L__BB1_62;

	add.s32 	%r1671, %r12, 15;
	st.local.v2.u32 	[%rd26], {%r1671, %r58};
	mov.u64 	%rd574, $str;
	cvta.global.u64 	%rd575, %rd574;
	{ // callseq 331, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd575;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1141, [retval0+0];
	} // callseq 331
	bra.uni 	$L__BB1_63;

$L__BB1_62:
	ld.param.u64 	%rd4093, [%rd25];
	ld.param.u32 	%r2207, [%rd25+32];
	mul.wide.s32 	%rd586, %r2207, %r59;
	add.s64 	%rd577, %rd4093, %rd586;
	// begin inline asm
	{ atom.add.f64 %fd3727,[%rd577],%fd10395; }

	// end inline asm
	add.s64 	%rd578, %rd577, 8;
	// begin inline asm
	{ atom.add.f64 %fd3729,[%rd578],%fd10394; }

	// end inline asm
	add.s64 	%rd579, %rd577, 16;
	// begin inline asm
	{ atom.add.f64 %fd3731,[%rd579],%fd10393; }

	// end inline asm
	add.s64 	%rd580, %rd577, 24;
	// begin inline asm
	{ atom.add.f64 %fd3733,[%rd580],%fd10383; }

	// end inline asm
	add.s64 	%rd581, %rd577, 32;
	// begin inline asm
	{ atom.add.f64 %fd3735,[%rd581],%fd10382; }

	// end inline asm
	add.s64 	%rd582, %rd577, 40;
	// begin inline asm
	{ atom.add.f64 %fd3737,[%rd582],%fd10381; }

	// end inline asm
	add.s64 	%rd583, %rd577, 48;
	// begin inline asm
	{ atom.add.f64 %fd3739,[%rd583],%fd10371; }

	// end inline asm
	add.s64 	%rd584, %rd577, 56;
	// begin inline asm
	{ atom.add.f64 %fd3741,[%rd584],%fd10370; }

	// end inline asm
	add.s64 	%rd585, %rd577, 64;
	// begin inline asm
	{ atom.add.f64 %fd3743,[%rd585],%fd10369; }

	// end inline asm

$L__BB1_63:
	setp.gt.s32 	%p942, %r1022, 0;
	cvt.u32.u64 	%r2076, %rd4122;
	selp.b32 	%r2075, %r2076, 0, %p942;
	cvt.s64.s32 	%rd3994, %r2075;
	mul.lo.s64 	%rd587, %rd3994, %rd13;
	add.s64 	%rd47, %rd6, %rd587;
	ld.global.u32 	%r2254, [%rd47];
	shl.b32 	%r61, %r2254, 4;
	ld.param.u32 	%r63, [%rd25+60];
	setp.le.s32 	%p42, %r63, %r61;
	selp.u16 	%rs114, 1, 0, %p42;
	shr.u32 	%r1142, %r2254, 27;
	cvt.u16.u32 	%rs115, %r1142;
	and.b16  	%rs116, %rs115, 1;
	or.b16  	%rs117, %rs116, %rs114;
	setp.eq.s16 	%p43, %rs117, 0;
	@%p43 bra 	$L__BB1_65;

	st.local.v2.u32 	[%rd26], {%r61, %r63};
	mov.u64 	%rd588, $str;
	cvta.global.u64 	%rd589, %rd588;
	{ // callseq 332, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd589;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1143, [retval0+0];
	} // callseq 332
	bra.uni 	$L__BB1_66;

$L__BB1_65:
	ld.param.u64 	%rd3996, [%rd25];
	ld.param.u32 	%r2078, [%rd25+32];
	mul.wide.s32 	%rd600, %r2078, %r61;
	add.s64 	%rd591, %rd3996, %rd600;
	// begin inline asm
	{ atom.add.f64 %fd3745,[%rd591],%fd10656; }

	// end inline asm
	add.s64 	%rd592, %rd591, 8;
	// begin inline asm
	{ atom.add.f64 %fd3747,[%rd592],%fd10655; }

	// end inline asm
	add.s64 	%rd593, %rd591, 16;
	// begin inline asm
	{ atom.add.f64 %fd3749,[%rd593],%fd10654; }

	// end inline asm
	add.s64 	%rd594, %rd591, 24;
	// begin inline asm
	{ atom.add.f64 %fd3751,[%rd594],%fd10644; }

	// end inline asm
	add.s64 	%rd595, %rd591, 32;
	// begin inline asm
	{ atom.add.f64 %fd3753,[%rd595],%fd10643; }

	// end inline asm
	add.s64 	%rd596, %rd591, 40;
	// begin inline asm
	{ atom.add.f64 %fd3755,[%rd596],%fd10642; }

	// end inline asm
	add.s64 	%rd597, %rd591, 48;
	// begin inline asm
	{ atom.add.f64 %fd3757,[%rd597],%fd10632; }

	// end inline asm
	add.s64 	%rd598, %rd591, 56;
	// begin inline asm
	{ atom.add.f64 %fd3759,[%rd598],%fd10631; }

	// end inline asm
	add.s64 	%rd599, %rd591, 64;
	// begin inline asm
	{ atom.add.f64 %fd3761,[%rd599],%fd10630; }

	// end inline asm

$L__BB1_66:
	ld.param.u32 	%r65, [%rd25+60];
	add.s32 	%r66, %r61, 1;
	setp.le.s32 	%p44, %r65, %r66;
	selp.u16 	%rs118, 1, 0, %p44;
	shr.u32 	%r1144, %r66, 31;
	cvt.u16.u32 	%rs119, %r1144;
	or.b16  	%rs120, %rs118, %rs119;
	setp.eq.s16 	%p45, %rs120, 0;
	@%p45 bra 	$L__BB1_68;

	add.s32 	%r1672, %r61, 1;
	st.local.v2.u32 	[%rd26], {%r1672, %r65};
	mov.u64 	%rd601, $str;
	cvta.global.u64 	%rd602, %rd601;
	{ // callseq 333, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd602;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1145, [retval0+0];
	} // callseq 333
	bra.uni 	$L__BB1_69;

$L__BB1_68:
	ld.param.u64 	%rd4094, [%rd25];
	ld.param.u32 	%r2208, [%rd25+32];
	mul.wide.s32 	%rd613, %r2208, %r66;
	add.s64 	%rd604, %rd4094, %rd613;
	// begin inline asm
	{ atom.add.f64 %fd3763,[%rd604],%fd10653; }

	// end inline asm
	add.s64 	%rd605, %rd604, 8;
	// begin inline asm
	{ atom.add.f64 %fd3765,[%rd605],%fd10652; }

	// end inline asm
	add.s64 	%rd606, %rd604, 16;
	// begin inline asm
	{ atom.add.f64 %fd3767,[%rd606],%fd10651; }

	// end inline asm
	add.s64 	%rd607, %rd604, 24;
	// begin inline asm
	{ atom.add.f64 %fd3769,[%rd607],%fd10641; }

	// end inline asm
	add.s64 	%rd608, %rd604, 32;
	// begin inline asm
	{ atom.add.f64 %fd3771,[%rd608],%fd10640; }

	// end inline asm
	add.s64 	%rd609, %rd604, 40;
	// begin inline asm
	{ atom.add.f64 %fd3773,[%rd609],%fd10639; }

	// end inline asm
	add.s64 	%rd610, %rd604, 48;
	// begin inline asm
	{ atom.add.f64 %fd3775,[%rd610],%fd10629; }

	// end inline asm
	add.s64 	%rd611, %rd604, 56;
	// begin inline asm
	{ atom.add.f64 %fd3777,[%rd611],%fd10628; }

	// end inline asm
	add.s64 	%rd612, %rd604, 64;
	// begin inline asm
	{ atom.add.f64 %fd3779,[%rd612],%fd10627; }

	// end inline asm

$L__BB1_69:
	ld.param.u32 	%r68, [%rd25+60];
	add.s32 	%r69, %r61, 2;
	setp.le.s32 	%p46, %r68, %r69;
	selp.u16 	%rs121, 1, 0, %p46;
	shr.u32 	%r1146, %r69, 31;
	cvt.u16.u32 	%rs122, %r1146;
	or.b16  	%rs123, %rs121, %rs122;
	setp.eq.s16 	%p47, %rs123, 0;
	@%p47 bra 	$L__BB1_71;

	add.s32 	%r1673, %r61, 2;
	st.local.v2.u32 	[%rd26], {%r1673, %r68};
	mov.u64 	%rd614, $str;
	cvta.global.u64 	%rd615, %rd614;
	{ // callseq 334, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd615;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1147, [retval0+0];
	} // callseq 334
	bra.uni 	$L__BB1_72;

$L__BB1_71:
	ld.param.u64 	%rd4095, [%rd25];
	ld.param.u32 	%r2209, [%rd25+32];
	mul.wide.s32 	%rd626, %r2209, %r69;
	add.s64 	%rd617, %rd4095, %rd626;
	// begin inline asm
	{ atom.add.f64 %fd3781,[%rd617],%fd10650; }

	// end inline asm
	add.s64 	%rd618, %rd617, 8;
	// begin inline asm
	{ atom.add.f64 %fd3783,[%rd618],%fd10649; }

	// end inline asm
	add.s64 	%rd619, %rd617, 16;
	// begin inline asm
	{ atom.add.f64 %fd3785,[%rd619],%fd10648; }

	// end inline asm
	add.s64 	%rd620, %rd617, 24;
	// begin inline asm
	{ atom.add.f64 %fd3787,[%rd620],%fd10638; }

	// end inline asm
	add.s64 	%rd621, %rd617, 32;
	// begin inline asm
	{ atom.add.f64 %fd3789,[%rd621],%fd10637; }

	// end inline asm
	add.s64 	%rd622, %rd617, 40;
	// begin inline asm
	{ atom.add.f64 %fd3791,[%rd622],%fd10636; }

	// end inline asm
	add.s64 	%rd623, %rd617, 48;
	// begin inline asm
	{ atom.add.f64 %fd3793,[%rd623],%fd10626; }

	// end inline asm
	add.s64 	%rd624, %rd617, 56;
	// begin inline asm
	{ atom.add.f64 %fd3795,[%rd624],%fd10625; }

	// end inline asm
	add.s64 	%rd625, %rd617, 64;
	// begin inline asm
	{ atom.add.f64 %fd3797,[%rd625],%fd10624; }

	// end inline asm

$L__BB1_72:
	ld.param.u32 	%r71, [%rd25+60];
	add.s32 	%r72, %r61, 3;
	setp.le.s32 	%p48, %r71, %r72;
	selp.u16 	%rs124, 1, 0, %p48;
	shr.u32 	%r1148, %r72, 31;
	cvt.u16.u32 	%rs125, %r1148;
	or.b16  	%rs126, %rs124, %rs125;
	setp.eq.s16 	%p49, %rs126, 0;
	@%p49 bra 	$L__BB1_74;

	add.s32 	%r1674, %r61, 3;
	st.local.v2.u32 	[%rd26], {%r1674, %r71};
	mov.u64 	%rd627, $str;
	cvta.global.u64 	%rd628, %rd627;
	{ // callseq 335, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd628;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1149, [retval0+0];
	} // callseq 335
	bra.uni 	$L__BB1_75;

$L__BB1_74:
	ld.param.u64 	%rd4096, [%rd25];
	ld.param.u32 	%r2210, [%rd25+32];
	mul.wide.s32 	%rd639, %r2210, %r72;
	add.s64 	%rd630, %rd4096, %rd639;
	// begin inline asm
	{ atom.add.f64 %fd3799,[%rd630],%fd10647; }

	// end inline asm
	add.s64 	%rd631, %rd630, 8;
	// begin inline asm
	{ atom.add.f64 %fd3801,[%rd631],%fd10646; }

	// end inline asm
	add.s64 	%rd632, %rd630, 16;
	// begin inline asm
	{ atom.add.f64 %fd3803,[%rd632],%fd10645; }

	// end inline asm
	add.s64 	%rd633, %rd630, 24;
	// begin inline asm
	{ atom.add.f64 %fd3805,[%rd633],%fd10635; }

	// end inline asm
	add.s64 	%rd634, %rd630, 32;
	// begin inline asm
	{ atom.add.f64 %fd3807,[%rd634],%fd10634; }

	// end inline asm
	add.s64 	%rd635, %rd630, 40;
	// begin inline asm
	{ atom.add.f64 %fd3809,[%rd635],%fd10633; }

	// end inline asm
	add.s64 	%rd636, %rd630, 48;
	// begin inline asm
	{ atom.add.f64 %fd3811,[%rd636],%fd10623; }

	// end inline asm
	add.s64 	%rd637, %rd630, 56;
	// begin inline asm
	{ atom.add.f64 %fd3813,[%rd637],%fd10622; }

	// end inline asm
	add.s64 	%rd638, %rd630, 64;
	// begin inline asm
	{ atom.add.f64 %fd3815,[%rd638],%fd10621; }

	// end inline asm

$L__BB1_75:
	ld.param.u32 	%r74, [%rd25+60];
	add.s32 	%r75, %r61, 4;
	setp.le.s32 	%p50, %r74, %r75;
	selp.u16 	%rs127, 1, 0, %p50;
	shr.u32 	%r1150, %r75, 31;
	cvt.u16.u32 	%rs128, %r1150;
	or.b16  	%rs129, %rs127, %rs128;
	setp.eq.s16 	%p51, %rs129, 0;
	@%p51 bra 	$L__BB1_77;

	add.s32 	%r1675, %r61, 4;
	st.local.v2.u32 	[%rd26], {%r1675, %r74};
	mov.u64 	%rd640, $str;
	cvta.global.u64 	%rd641, %rd640;
	{ // callseq 336, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd641;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1151, [retval0+0];
	} // callseq 336
	bra.uni 	$L__BB1_78;

$L__BB1_77:
	ld.param.u64 	%rd4097, [%rd25];
	ld.param.u32 	%r2211, [%rd25+32];
	mul.wide.s32 	%rd652, %r2211, %r75;
	add.s64 	%rd643, %rd4097, %rd652;
	// begin inline asm
	{ atom.add.f64 %fd3817,[%rd643],%fd10620; }

	// end inline asm
	add.s64 	%rd644, %rd643, 8;
	// begin inline asm
	{ atom.add.f64 %fd3819,[%rd644],%fd10619; }

	// end inline asm
	add.s64 	%rd645, %rd643, 16;
	// begin inline asm
	{ atom.add.f64 %fd3821,[%rd645],%fd10618; }

	// end inline asm
	add.s64 	%rd646, %rd643, 24;
	// begin inline asm
	{ atom.add.f64 %fd3823,[%rd646],%fd10608; }

	// end inline asm
	add.s64 	%rd647, %rd643, 32;
	// begin inline asm
	{ atom.add.f64 %fd3825,[%rd647],%fd10607; }

	// end inline asm
	add.s64 	%rd648, %rd643, 40;
	// begin inline asm
	{ atom.add.f64 %fd3827,[%rd648],%fd10606; }

	// end inline asm
	add.s64 	%rd649, %rd643, 48;
	// begin inline asm
	{ atom.add.f64 %fd3829,[%rd649],%fd10596; }

	// end inline asm
	add.s64 	%rd650, %rd643, 56;
	// begin inline asm
	{ atom.add.f64 %fd3831,[%rd650],%fd10595; }

	// end inline asm
	add.s64 	%rd651, %rd643, 64;
	// begin inline asm
	{ atom.add.f64 %fd3833,[%rd651],%fd10594; }

	// end inline asm

$L__BB1_78:
	ld.param.u32 	%r77, [%rd25+60];
	add.s32 	%r78, %r61, 5;
	setp.le.s32 	%p52, %r77, %r78;
	selp.u16 	%rs130, 1, 0, %p52;
	shr.u32 	%r1152, %r78, 31;
	cvt.u16.u32 	%rs131, %r1152;
	or.b16  	%rs132, %rs130, %rs131;
	setp.eq.s16 	%p53, %rs132, 0;
	@%p53 bra 	$L__BB1_80;

	add.s32 	%r1676, %r61, 5;
	st.local.v2.u32 	[%rd26], {%r1676, %r77};
	mov.u64 	%rd653, $str;
	cvta.global.u64 	%rd654, %rd653;
	{ // callseq 337, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd654;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1153, [retval0+0];
	} // callseq 337
	bra.uni 	$L__BB1_81;

$L__BB1_80:
	ld.param.u64 	%rd4098, [%rd25];
	ld.param.u32 	%r2212, [%rd25+32];
	mul.wide.s32 	%rd665, %r2212, %r78;
	add.s64 	%rd656, %rd4098, %rd665;
	// begin inline asm
	{ atom.add.f64 %fd3835,[%rd656],%fd10617; }

	// end inline asm
	add.s64 	%rd657, %rd656, 8;
	// begin inline asm
	{ atom.add.f64 %fd3837,[%rd657],%fd10616; }

	// end inline asm
	add.s64 	%rd658, %rd656, 16;
	// begin inline asm
	{ atom.add.f64 %fd3839,[%rd658],%fd10615; }

	// end inline asm
	add.s64 	%rd659, %rd656, 24;
	// begin inline asm
	{ atom.add.f64 %fd3841,[%rd659],%fd10605; }

	// end inline asm
	add.s64 	%rd660, %rd656, 32;
	// begin inline asm
	{ atom.add.f64 %fd3843,[%rd660],%fd10604; }

	// end inline asm
	add.s64 	%rd661, %rd656, 40;
	// begin inline asm
	{ atom.add.f64 %fd3845,[%rd661],%fd10603; }

	// end inline asm
	add.s64 	%rd662, %rd656, 48;
	// begin inline asm
	{ atom.add.f64 %fd3847,[%rd662],%fd10593; }

	// end inline asm
	add.s64 	%rd663, %rd656, 56;
	// begin inline asm
	{ atom.add.f64 %fd3849,[%rd663],%fd10592; }

	// end inline asm
	add.s64 	%rd664, %rd656, 64;
	// begin inline asm
	{ atom.add.f64 %fd3851,[%rd664],%fd10591; }

	// end inline asm

$L__BB1_81:
	ld.param.u32 	%r80, [%rd25+60];
	add.s32 	%r81, %r61, 6;
	setp.le.s32 	%p54, %r80, %r81;
	selp.u16 	%rs133, 1, 0, %p54;
	shr.u32 	%r1154, %r81, 31;
	cvt.u16.u32 	%rs134, %r1154;
	or.b16  	%rs135, %rs133, %rs134;
	setp.eq.s16 	%p55, %rs135, 0;
	@%p55 bra 	$L__BB1_83;

	add.s32 	%r1677, %r61, 6;
	st.local.v2.u32 	[%rd26], {%r1677, %r80};
	mov.u64 	%rd666, $str;
	cvta.global.u64 	%rd667, %rd666;
	{ // callseq 338, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd667;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1155, [retval0+0];
	} // callseq 338
	bra.uni 	$L__BB1_84;

$L__BB1_83:
	ld.param.u64 	%rd4099, [%rd25];
	ld.param.u32 	%r2213, [%rd25+32];
	mul.wide.s32 	%rd678, %r2213, %r81;
	add.s64 	%rd669, %rd4099, %rd678;
	// begin inline asm
	{ atom.add.f64 %fd3853,[%rd669],%fd10614; }

	// end inline asm
	add.s64 	%rd670, %rd669, 8;
	// begin inline asm
	{ atom.add.f64 %fd3855,[%rd670],%fd10613; }

	// end inline asm
	add.s64 	%rd671, %rd669, 16;
	// begin inline asm
	{ atom.add.f64 %fd3857,[%rd671],%fd10612; }

	// end inline asm
	add.s64 	%rd672, %rd669, 24;
	// begin inline asm
	{ atom.add.f64 %fd3859,[%rd672],%fd10602; }

	// end inline asm
	add.s64 	%rd673, %rd669, 32;
	// begin inline asm
	{ atom.add.f64 %fd3861,[%rd673],%fd10601; }

	// end inline asm
	add.s64 	%rd674, %rd669, 40;
	// begin inline asm
	{ atom.add.f64 %fd3863,[%rd674],%fd10600; }

	// end inline asm
	add.s64 	%rd675, %rd669, 48;
	// begin inline asm
	{ atom.add.f64 %fd3865,[%rd675],%fd10590; }

	// end inline asm
	add.s64 	%rd676, %rd669, 56;
	// begin inline asm
	{ atom.add.f64 %fd3867,[%rd676],%fd10589; }

	// end inline asm
	add.s64 	%rd677, %rd669, 64;
	// begin inline asm
	{ atom.add.f64 %fd3869,[%rd677],%fd10588; }

	// end inline asm

$L__BB1_84:
	ld.param.u32 	%r83, [%rd25+60];
	add.s32 	%r84, %r61, 7;
	setp.le.s32 	%p56, %r83, %r84;
	selp.u16 	%rs136, 1, 0, %p56;
	shr.u32 	%r1156, %r84, 31;
	cvt.u16.u32 	%rs137, %r1156;
	or.b16  	%rs138, %rs136, %rs137;
	setp.eq.s16 	%p57, %rs138, 0;
	@%p57 bra 	$L__BB1_86;

	add.s32 	%r1678, %r61, 7;
	st.local.v2.u32 	[%rd26], {%r1678, %r83};
	mov.u64 	%rd679, $str;
	cvta.global.u64 	%rd680, %rd679;
	{ // callseq 339, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd680;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1157, [retval0+0];
	} // callseq 339
	bra.uni 	$L__BB1_87;

$L__BB1_86:
	ld.param.u64 	%rd4100, [%rd25];
	ld.param.u32 	%r2214, [%rd25+32];
	mul.wide.s32 	%rd691, %r2214, %r84;
	add.s64 	%rd682, %rd4100, %rd691;
	// begin inline asm
	{ atom.add.f64 %fd3871,[%rd682],%fd10611; }

	// end inline asm
	add.s64 	%rd683, %rd682, 8;
	// begin inline asm
	{ atom.add.f64 %fd3873,[%rd683],%fd10610; }

	// end inline asm
	add.s64 	%rd684, %rd682, 16;
	// begin inline asm
	{ atom.add.f64 %fd3875,[%rd684],%fd10609; }

	// end inline asm
	add.s64 	%rd685, %rd682, 24;
	// begin inline asm
	{ atom.add.f64 %fd3877,[%rd685],%fd10599; }

	// end inline asm
	add.s64 	%rd686, %rd682, 32;
	// begin inline asm
	{ atom.add.f64 %fd3879,[%rd686],%fd10598; }

	// end inline asm
	add.s64 	%rd687, %rd682, 40;
	// begin inline asm
	{ atom.add.f64 %fd3881,[%rd687],%fd10597; }

	// end inline asm
	add.s64 	%rd688, %rd682, 48;
	// begin inline asm
	{ atom.add.f64 %fd3883,[%rd688],%fd10587; }

	// end inline asm
	add.s64 	%rd689, %rd682, 56;
	// begin inline asm
	{ atom.add.f64 %fd3885,[%rd689],%fd10586; }

	// end inline asm
	add.s64 	%rd690, %rd682, 64;
	// begin inline asm
	{ atom.add.f64 %fd3887,[%rd690],%fd10585; }

	// end inline asm

$L__BB1_87:
	ld.param.u32 	%r86, [%rd25+60];
	add.s32 	%r87, %r61, 8;
	setp.le.s32 	%p58, %r86, %r87;
	selp.u16 	%rs139, 1, 0, %p58;
	shr.u32 	%r1158, %r87, 31;
	cvt.u16.u32 	%rs140, %r1158;
	or.b16  	%rs141, %rs139, %rs140;
	setp.eq.s16 	%p59, %rs141, 0;
	@%p59 bra 	$L__BB1_89;

	add.s32 	%r1679, %r61, 8;
	st.local.v2.u32 	[%rd26], {%r1679, %r86};
	mov.u64 	%rd692, $str;
	cvta.global.u64 	%rd693, %rd692;
	{ // callseq 340, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd693;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1159, [retval0+0];
	} // callseq 340
	bra.uni 	$L__BB1_90;

$L__BB1_89:
	ld.param.u64 	%rd4101, [%rd25];
	ld.param.u32 	%r2215, [%rd25+32];
	mul.wide.s32 	%rd704, %r2215, %r87;
	add.s64 	%rd695, %rd4101, %rd704;
	// begin inline asm
	{ atom.add.f64 %fd3889,[%rd695],%fd10584; }

	// end inline asm
	add.s64 	%rd696, %rd695, 8;
	// begin inline asm
	{ atom.add.f64 %fd3891,[%rd696],%fd10583; }

	// end inline asm
	add.s64 	%rd697, %rd695, 16;
	// begin inline asm
	{ atom.add.f64 %fd3893,[%rd697],%fd10582; }

	// end inline asm
	add.s64 	%rd698, %rd695, 24;
	// begin inline asm
	{ atom.add.f64 %fd3895,[%rd698],%fd10572; }

	// end inline asm
	add.s64 	%rd699, %rd695, 32;
	// begin inline asm
	{ atom.add.f64 %fd3897,[%rd699],%fd10571; }

	// end inline asm
	add.s64 	%rd700, %rd695, 40;
	// begin inline asm
	{ atom.add.f64 %fd3899,[%rd700],%fd10570; }

	// end inline asm
	add.s64 	%rd701, %rd695, 48;
	// begin inline asm
	{ atom.add.f64 %fd3901,[%rd701],%fd10560; }

	// end inline asm
	add.s64 	%rd702, %rd695, 56;
	// begin inline asm
	{ atom.add.f64 %fd3903,[%rd702],%fd10559; }

	// end inline asm
	add.s64 	%rd703, %rd695, 64;
	// begin inline asm
	{ atom.add.f64 %fd3905,[%rd703],%fd10558; }

	// end inline asm

$L__BB1_90:
	ld.param.u32 	%r89, [%rd25+60];
	add.s32 	%r90, %r61, 9;
	setp.le.s32 	%p60, %r89, %r90;
	selp.u16 	%rs142, 1, 0, %p60;
	shr.u32 	%r1160, %r90, 31;
	cvt.u16.u32 	%rs143, %r1160;
	or.b16  	%rs144, %rs142, %rs143;
	setp.eq.s16 	%p61, %rs144, 0;
	@%p61 bra 	$L__BB1_92;

	add.s32 	%r1680, %r61, 9;
	st.local.v2.u32 	[%rd26], {%r1680, %r89};
	mov.u64 	%rd705, $str;
	cvta.global.u64 	%rd706, %rd705;
	{ // callseq 341, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd706;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1161, [retval0+0];
	} // callseq 341
	bra.uni 	$L__BB1_93;

$L__BB1_92:
	ld.param.u64 	%rd4102, [%rd25];
	ld.param.u32 	%r2216, [%rd25+32];
	mul.wide.s32 	%rd717, %r2216, %r90;
	add.s64 	%rd708, %rd4102, %rd717;
	// begin inline asm
	{ atom.add.f64 %fd3907,[%rd708],%fd10581; }

	// end inline asm
	add.s64 	%rd709, %rd708, 8;
	// begin inline asm
	{ atom.add.f64 %fd3909,[%rd709],%fd10580; }

	// end inline asm
	add.s64 	%rd710, %rd708, 16;
	// begin inline asm
	{ atom.add.f64 %fd3911,[%rd710],%fd10579; }

	// end inline asm
	add.s64 	%rd711, %rd708, 24;
	// begin inline asm
	{ atom.add.f64 %fd3913,[%rd711],%fd10569; }

	// end inline asm
	add.s64 	%rd712, %rd708, 32;
	// begin inline asm
	{ atom.add.f64 %fd3915,[%rd712],%fd10568; }

	// end inline asm
	add.s64 	%rd713, %rd708, 40;
	// begin inline asm
	{ atom.add.f64 %fd3917,[%rd713],%fd10567; }

	// end inline asm
	add.s64 	%rd714, %rd708, 48;
	// begin inline asm
	{ atom.add.f64 %fd3919,[%rd714],%fd10557; }

	// end inline asm
	add.s64 	%rd715, %rd708, 56;
	// begin inline asm
	{ atom.add.f64 %fd3921,[%rd715],%fd10556; }

	// end inline asm
	add.s64 	%rd716, %rd708, 64;
	// begin inline asm
	{ atom.add.f64 %fd3923,[%rd716],%fd10555; }

	// end inline asm

$L__BB1_93:
	ld.param.u32 	%r92, [%rd25+60];
	add.s32 	%r93, %r61, 10;
	setp.le.s32 	%p62, %r92, %r93;
	selp.u16 	%rs145, 1, 0, %p62;
	shr.u32 	%r1162, %r93, 31;
	cvt.u16.u32 	%rs146, %r1162;
	or.b16  	%rs147, %rs145, %rs146;
	setp.eq.s16 	%p63, %rs147, 0;
	@%p63 bra 	$L__BB1_95;

	add.s32 	%r1681, %r61, 10;
	st.local.v2.u32 	[%rd26], {%r1681, %r92};
	mov.u64 	%rd718, $str;
	cvta.global.u64 	%rd719, %rd718;
	{ // callseq 342, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd719;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1163, [retval0+0];
	} // callseq 342
	bra.uni 	$L__BB1_96;

$L__BB1_95:
	ld.param.u64 	%rd4103, [%rd25];
	ld.param.u32 	%r2217, [%rd25+32];
	mul.wide.s32 	%rd730, %r2217, %r93;
	add.s64 	%rd721, %rd4103, %rd730;
	// begin inline asm
	{ atom.add.f64 %fd3925,[%rd721],%fd10578; }

	// end inline asm
	add.s64 	%rd722, %rd721, 8;
	// begin inline asm
	{ atom.add.f64 %fd3927,[%rd722],%fd10577; }

	// end inline asm
	add.s64 	%rd723, %rd721, 16;
	// begin inline asm
	{ atom.add.f64 %fd3929,[%rd723],%fd10576; }

	// end inline asm
	add.s64 	%rd724, %rd721, 24;
	// begin inline asm
	{ atom.add.f64 %fd3931,[%rd724],%fd10566; }

	// end inline asm
	add.s64 	%rd725, %rd721, 32;
	// begin inline asm
	{ atom.add.f64 %fd3933,[%rd725],%fd10565; }

	// end inline asm
	add.s64 	%rd726, %rd721, 40;
	// begin inline asm
	{ atom.add.f64 %fd3935,[%rd726],%fd10564; }

	// end inline asm
	add.s64 	%rd727, %rd721, 48;
	// begin inline asm
	{ atom.add.f64 %fd3937,[%rd727],%fd10554; }

	// end inline asm
	add.s64 	%rd728, %rd721, 56;
	// begin inline asm
	{ atom.add.f64 %fd3939,[%rd728],%fd10553; }

	// end inline asm
	add.s64 	%rd729, %rd721, 64;
	// begin inline asm
	{ atom.add.f64 %fd3941,[%rd729],%fd10552; }

	// end inline asm

$L__BB1_96:
	ld.param.u32 	%r95, [%rd25+60];
	add.s32 	%r96, %r61, 11;
	setp.le.s32 	%p64, %r95, %r96;
	selp.u16 	%rs148, 1, 0, %p64;
	shr.u32 	%r1164, %r96, 31;
	cvt.u16.u32 	%rs149, %r1164;
	or.b16  	%rs150, %rs148, %rs149;
	setp.eq.s16 	%p65, %rs150, 0;
	@%p65 bra 	$L__BB1_98;

	add.s32 	%r1682, %r61, 11;
	st.local.v2.u32 	[%rd26], {%r1682, %r95};
	mov.u64 	%rd731, $str;
	cvta.global.u64 	%rd732, %rd731;
	{ // callseq 343, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd732;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1165, [retval0+0];
	} // callseq 343
	bra.uni 	$L__BB1_99;

$L__BB1_98:
	ld.param.u64 	%rd4104, [%rd25];
	ld.param.u32 	%r2218, [%rd25+32];
	mul.wide.s32 	%rd743, %r2218, %r96;
	add.s64 	%rd734, %rd4104, %rd743;
	// begin inline asm
	{ atom.add.f64 %fd3943,[%rd734],%fd10575; }

	// end inline asm
	add.s64 	%rd735, %rd734, 8;
	// begin inline asm
	{ atom.add.f64 %fd3945,[%rd735],%fd10574; }

	// end inline asm
	add.s64 	%rd736, %rd734, 16;
	// begin inline asm
	{ atom.add.f64 %fd3947,[%rd736],%fd10573; }

	// end inline asm
	add.s64 	%rd737, %rd734, 24;
	// begin inline asm
	{ atom.add.f64 %fd3949,[%rd737],%fd10563; }

	// end inline asm
	add.s64 	%rd738, %rd734, 32;
	// begin inline asm
	{ atom.add.f64 %fd3951,[%rd738],%fd10562; }

	// end inline asm
	add.s64 	%rd739, %rd734, 40;
	// begin inline asm
	{ atom.add.f64 %fd3953,[%rd739],%fd10561; }

	// end inline asm
	add.s64 	%rd740, %rd734, 48;
	// begin inline asm
	{ atom.add.f64 %fd3955,[%rd740],%fd10551; }

	// end inline asm
	add.s64 	%rd741, %rd734, 56;
	// begin inline asm
	{ atom.add.f64 %fd3957,[%rd741],%fd10550; }

	// end inline asm
	add.s64 	%rd742, %rd734, 64;
	// begin inline asm
	{ atom.add.f64 %fd3959,[%rd742],%fd10549; }

	// end inline asm

$L__BB1_99:
	ld.param.u32 	%r98, [%rd25+60];
	add.s32 	%r99, %r61, 12;
	setp.le.s32 	%p66, %r98, %r99;
	selp.u16 	%rs151, 1, 0, %p66;
	shr.u32 	%r1166, %r99, 31;
	cvt.u16.u32 	%rs152, %r1166;
	or.b16  	%rs153, %rs151, %rs152;
	setp.eq.s16 	%p67, %rs153, 0;
	@%p67 bra 	$L__BB1_101;

	add.s32 	%r1683, %r61, 12;
	st.local.v2.u32 	[%rd26], {%r1683, %r98};
	mov.u64 	%rd744, $str;
	cvta.global.u64 	%rd745, %rd744;
	{ // callseq 344, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd745;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1167, [retval0+0];
	} // callseq 344
	bra.uni 	$L__BB1_102;

$L__BB1_101:
	ld.param.u64 	%rd4105, [%rd25];
	ld.param.u32 	%r2219, [%rd25+32];
	mul.wide.s32 	%rd756, %r2219, %r99;
	add.s64 	%rd747, %rd4105, %rd756;
	// begin inline asm
	{ atom.add.f64 %fd3961,[%rd747],%fd10548; }

	// end inline asm
	add.s64 	%rd748, %rd747, 8;
	// begin inline asm
	{ atom.add.f64 %fd3963,[%rd748],%fd10547; }

	// end inline asm
	add.s64 	%rd749, %rd747, 16;
	// begin inline asm
	{ atom.add.f64 %fd3965,[%rd749],%fd10546; }

	// end inline asm
	add.s64 	%rd750, %rd747, 24;
	// begin inline asm
	{ atom.add.f64 %fd3967,[%rd750],%fd10536; }

	// end inline asm
	add.s64 	%rd751, %rd747, 32;
	// begin inline asm
	{ atom.add.f64 %fd3969,[%rd751],%fd10535; }

	// end inline asm
	add.s64 	%rd752, %rd747, 40;
	// begin inline asm
	{ atom.add.f64 %fd3971,[%rd752],%fd10534; }

	// end inline asm
	add.s64 	%rd753, %rd747, 48;
	// begin inline asm
	{ atom.add.f64 %fd3973,[%rd753],%fd10524; }

	// end inline asm
	add.s64 	%rd754, %rd747, 56;
	// begin inline asm
	{ atom.add.f64 %fd3975,[%rd754],%fd10523; }

	// end inline asm
	add.s64 	%rd755, %rd747, 64;
	// begin inline asm
	{ atom.add.f64 %fd3977,[%rd755],%fd10522; }

	// end inline asm

$L__BB1_102:
	ld.param.u32 	%r101, [%rd25+60];
	add.s32 	%r102, %r61, 13;
	setp.le.s32 	%p68, %r101, %r102;
	selp.u16 	%rs154, 1, 0, %p68;
	shr.u32 	%r1168, %r102, 31;
	cvt.u16.u32 	%rs155, %r1168;
	or.b16  	%rs156, %rs154, %rs155;
	setp.eq.s16 	%p69, %rs156, 0;
	@%p69 bra 	$L__BB1_104;

	add.s32 	%r1684, %r61, 13;
	st.local.v2.u32 	[%rd26], {%r1684, %r101};
	mov.u64 	%rd757, $str;
	cvta.global.u64 	%rd758, %rd757;
	{ // callseq 345, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd758;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1169, [retval0+0];
	} // callseq 345
	bra.uni 	$L__BB1_105;

$L__BB1_104:
	ld.param.u64 	%rd4106, [%rd25];
	ld.param.u32 	%r2220, [%rd25+32];
	mul.wide.s32 	%rd769, %r2220, %r102;
	add.s64 	%rd760, %rd4106, %rd769;
	// begin inline asm
	{ atom.add.f64 %fd3979,[%rd760],%fd10545; }

	// end inline asm
	add.s64 	%rd761, %rd760, 8;
	// begin inline asm
	{ atom.add.f64 %fd3981,[%rd761],%fd10544; }

	// end inline asm
	add.s64 	%rd762, %rd760, 16;
	// begin inline asm
	{ atom.add.f64 %fd3983,[%rd762],%fd10543; }

	// end inline asm
	add.s64 	%rd763, %rd760, 24;
	// begin inline asm
	{ atom.add.f64 %fd3985,[%rd763],%fd10533; }

	// end inline asm
	add.s64 	%rd764, %rd760, 32;
	// begin inline asm
	{ atom.add.f64 %fd3987,[%rd764],%fd10532; }

	// end inline asm
	add.s64 	%rd765, %rd760, 40;
	// begin inline asm
	{ atom.add.f64 %fd3989,[%rd765],%fd10531; }

	// end inline asm
	add.s64 	%rd766, %rd760, 48;
	// begin inline asm
	{ atom.add.f64 %fd3991,[%rd766],%fd10521; }

	// end inline asm
	add.s64 	%rd767, %rd760, 56;
	// begin inline asm
	{ atom.add.f64 %fd3993,[%rd767],%fd10520; }

	// end inline asm
	add.s64 	%rd768, %rd760, 64;
	// begin inline asm
	{ atom.add.f64 %fd3995,[%rd768],%fd10519; }

	// end inline asm

$L__BB1_105:
	ld.param.u32 	%r104, [%rd25+60];
	add.s32 	%r105, %r61, 14;
	setp.le.s32 	%p70, %r104, %r105;
	selp.u16 	%rs157, 1, 0, %p70;
	shr.u32 	%r1170, %r105, 31;
	cvt.u16.u32 	%rs158, %r1170;
	or.b16  	%rs159, %rs157, %rs158;
	setp.eq.s16 	%p71, %rs159, 0;
	@%p71 bra 	$L__BB1_107;

	add.s32 	%r1685, %r61, 14;
	st.local.v2.u32 	[%rd26], {%r1685, %r104};
	mov.u64 	%rd770, $str;
	cvta.global.u64 	%rd771, %rd770;
	{ // callseq 346, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd771;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1171, [retval0+0];
	} // callseq 346
	bra.uni 	$L__BB1_108;

$L__BB1_107:
	ld.param.u64 	%rd4107, [%rd25];
	ld.param.u32 	%r2221, [%rd25+32];
	mul.wide.s32 	%rd782, %r2221, %r105;
	add.s64 	%rd773, %rd4107, %rd782;
	// begin inline asm
	{ atom.add.f64 %fd3997,[%rd773],%fd10542; }

	// end inline asm
	add.s64 	%rd774, %rd773, 8;
	// begin inline asm
	{ atom.add.f64 %fd3999,[%rd774],%fd10541; }

	// end inline asm
	add.s64 	%rd775, %rd773, 16;
	// begin inline asm
	{ atom.add.f64 %fd4001,[%rd775],%fd10540; }

	// end inline asm
	add.s64 	%rd776, %rd773, 24;
	// begin inline asm
	{ atom.add.f64 %fd4003,[%rd776],%fd10530; }

	// end inline asm
	add.s64 	%rd777, %rd773, 32;
	// begin inline asm
	{ atom.add.f64 %fd4005,[%rd777],%fd10529; }

	// end inline asm
	add.s64 	%rd778, %rd773, 40;
	// begin inline asm
	{ atom.add.f64 %fd4007,[%rd778],%fd10528; }

	// end inline asm
	add.s64 	%rd779, %rd773, 48;
	// begin inline asm
	{ atom.add.f64 %fd4009,[%rd779],%fd10518; }

	// end inline asm
	add.s64 	%rd780, %rd773, 56;
	// begin inline asm
	{ atom.add.f64 %fd4011,[%rd780],%fd10517; }

	// end inline asm
	add.s64 	%rd781, %rd773, 64;
	// begin inline asm
	{ atom.add.f64 %fd4013,[%rd781],%fd10516; }

	// end inline asm

$L__BB1_108:
	ld.param.u32 	%r107, [%rd25+60];
	add.s32 	%r108, %r61, 15;
	setp.le.s32 	%p72, %r107, %r108;
	selp.u16 	%rs160, 1, 0, %p72;
	shr.u32 	%r1172, %r108, 31;
	cvt.u16.u32 	%rs161, %r1172;
	or.b16  	%rs162, %rs160, %rs161;
	setp.eq.s16 	%p73, %rs162, 0;
	@%p73 bra 	$L__BB1_110;

	add.s32 	%r1686, %r61, 15;
	st.local.v2.u32 	[%rd26], {%r1686, %r107};
	mov.u64 	%rd783, $str;
	cvta.global.u64 	%rd784, %rd783;
	{ // callseq 347, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd784;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1173, [retval0+0];
	} // callseq 347
	bra.uni 	$L__BB1_111;

$L__BB1_110:
	ld.param.u64 	%rd4108, [%rd25];
	ld.param.u32 	%r2222, [%rd25+32];
	mul.wide.s32 	%rd795, %r2222, %r108;
	add.s64 	%rd786, %rd4108, %rd795;
	// begin inline asm
	{ atom.add.f64 %fd4015,[%rd786],%fd10539; }

	// end inline asm
	add.s64 	%rd787, %rd786, 8;
	// begin inline asm
	{ atom.add.f64 %fd4017,[%rd787],%fd10538; }

	// end inline asm
	add.s64 	%rd788, %rd786, 16;
	// begin inline asm
	{ atom.add.f64 %fd4019,[%rd788],%fd10537; }

	// end inline asm
	add.s64 	%rd789, %rd786, 24;
	// begin inline asm
	{ atom.add.f64 %fd4021,[%rd789],%fd10527; }

	// end inline asm
	add.s64 	%rd790, %rd786, 32;
	// begin inline asm
	{ atom.add.f64 %fd4023,[%rd790],%fd10526; }

	// end inline asm
	add.s64 	%rd791, %rd786, 40;
	// begin inline asm
	{ atom.add.f64 %fd4025,[%rd791],%fd10525; }

	// end inline asm
	add.s64 	%rd792, %rd786, 48;
	// begin inline asm
	{ atom.add.f64 %fd4027,[%rd792],%fd10515; }

	// end inline asm
	add.s64 	%rd793, %rd786, 56;
	// begin inline asm
	{ atom.add.f64 %fd4029,[%rd793],%fd10514; }

	// end inline asm
	add.s64 	%rd794, %rd786, 64;
	// begin inline asm
	{ atom.add.f64 %fd4031,[%rd794],%fd10513; }

	// end inline asm

$L__BB1_111:
	ld.global.u32 	%r2256, [%rd30];
	shl.b32 	%r110, %r2256, 2;
	ld.global.u32 	%r2257, [%rd47];
	shl.b32 	%r112, %r2257, 2;
	ld.param.u32 	%r116, [%rd27+172];
	ld.param.v2.u32 	{%r1174, %r1175}, [%rd27+176];
	setp.le.s32 	%p74, %r1174, %r110;
	setp.le.s32 	%p75, %r1175, %r112;
	shl.b32 	%r120, %r10, 4;
	setp.le.s32 	%p76, %r116, %r120;
	or.pred  	%p77, %p74, %p75;
	or.b32  	%r1176, %r110, %r120;
	or.b32  	%r1177, %r1176, %r112;
	setp.lt.s32 	%p78, %r1177, 0;
	or.pred  	%p79, %p78, %p77;
	or.pred  	%p80, %p76, %p79;
	@%p80 bra 	$L__BB1_113;
	bra.uni 	$L__BB1_112;

$L__BB1_113:
	st.local.v2.u32 	[%rd26], {%r110, %r112};
	st.local.v2.u32 	[%rd26+8], {%r120, %r1174};
	st.local.v2.u32 	[%rd26+16], {%r1175, %r116};
	mov.u64 	%rd812, $str$1;
	cvta.global.u64 	%rd813, %rd812;
	{ // callseq 348, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd813;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1178, [retval0+0];
	} // callseq 348
	bra.uni 	$L__BB1_114;

$L__BB1_112:
	ld.param.u32 	%r2081, [%rd27+88];
	ld.param.u64 	%rd3999, [%rd27];
	ld.param.u32 	%r2080, [%rd27+32];
	ld.param.u64 	%rd3998, [%rd27+112];
	ld.param.u32 	%r2079, [%rd27+144];
	ld.param.u64 	%rd3997, [%rd27+56];
	cvta.to.global.u64 	%rd805, %rd3997;
	mul.wide.s32 	%rd806, %r2079, %r120;
	add.s64 	%rd796, %rd3998, %rd806;
	// begin inline asm
	{ atom.add.f64 %fd4033,[%rd796],%fd10800; }

	// end inline asm
	add.s64 	%rd797, %rd796, 8;
	// begin inline asm
	{ atom.add.f64 %fd4035,[%rd797],%fd10799; }

	// end inline asm
	add.s64 	%rd798, %rd796, 16;
	// begin inline asm
	{ atom.add.f64 %fd4037,[%rd798],%fd10798; }

	// end inline asm
	add.s64 	%rd799, %rd796, 24;
	// begin inline asm
	{ atom.add.f64 %fd4039,[%rd799],%fd10788; }

	// end inline asm
	add.s64 	%rd800, %rd796, 32;
	// begin inline asm
	{ atom.add.f64 %fd4041,[%rd800],%fd10787; }

	// end inline asm
	add.s64 	%rd801, %rd796, 40;
	// begin inline asm
	{ atom.add.f64 %fd4043,[%rd801],%fd10786; }

	// end inline asm
	add.s64 	%rd802, %rd796, 48;
	// begin inline asm
	{ atom.add.f64 %fd4045,[%rd802],%fd10776; }

	// end inline asm
	add.s64 	%rd803, %rd796, 56;
	// begin inline asm
	{ atom.add.f64 %fd4047,[%rd803],%fd10775; }

	// end inline asm
	add.s64 	%rd804, %rd796, 64;
	// begin inline asm
	{ atom.add.f64 %fd4049,[%rd804],%fd10774; }

	// end inline asm
	mul.wide.s32 	%rd807, %r2080, %r120;
	cvta.to.global.u64 	%rd808, %rd3999;
	add.s64 	%rd809, %rd808, %rd807;
	mul.wide.s32 	%rd810, %r2081, %r120;
	add.s64 	%rd811, %rd805, %rd810;
	st.global.u32 	[%rd809], %r110;
	st.global.u32 	[%rd811], %r112;

$L__BB1_114:
	ld.param.u32 	%r124, [%rd27+172];
	ld.param.v2.u32 	{%r1179, %r1180}, [%rd27+176];
	setp.le.s32 	%p81, %r1179, %r110;
	add.s32 	%r128, %r112, 1;
	setp.le.s32 	%p82, %r1180, %r128;
	add.s32 	%r129, %r120, 1;
	setp.le.s32 	%p83, %r124, %r129;
	or.pred  	%p84, %p81, %p82;
	or.b32  	%r1181, %r110, %r129;
	or.b32  	%r1182, %r1181, %r128;
	setp.lt.s32 	%p85, %r1182, 0;
	or.pred  	%p86, %p85, %p84;
	or.pred  	%p87, %p83, %p86;
	@%p87 bra 	$L__BB1_116;
	bra.uni 	$L__BB1_115;

$L__BB1_116:
	add.s32 	%r1688, %r112, 1;
	st.local.v2.u32 	[%rd26], {%r110, %r1688};
	add.s32 	%r1689, %r120, 1;
	st.local.v2.u32 	[%rd26+8], {%r1689, %r1179};
	st.local.v2.u32 	[%rd26+16], {%r1180, %r124};
	mov.u64 	%rd831, $str$1;
	cvta.global.u64 	%rd832, %rd831;
	{ // callseq 349, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd832;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1183, [retval0+0];
	} // callseq 349
	bra.uni 	$L__BB1_117;

$L__BB1_115:
	ld.param.u32 	%r2084, [%rd27+88];
	ld.param.u64 	%rd4002, [%rd27];
	ld.param.u32 	%r2083, [%rd27+32];
	ld.param.u64 	%rd4001, [%rd27+112];
	ld.param.u32 	%r2082, [%rd27+144];
	ld.param.u64 	%rd4000, [%rd27+56];
	cvta.to.global.u64 	%rd824, %rd4000;
	mul.wide.s32 	%rd825, %r2082, %r129;
	add.s64 	%rd815, %rd4001, %rd825;
	// begin inline asm
	{ atom.add.f64 %fd4051,[%rd815],%fd10797; }

	// end inline asm
	add.s64 	%rd816, %rd815, 8;
	// begin inline asm
	{ atom.add.f64 %fd4053,[%rd816],%fd10796; }

	// end inline asm
	add.s64 	%rd817, %rd815, 16;
	// begin inline asm
	{ atom.add.f64 %fd4055,[%rd817],%fd10795; }

	// end inline asm
	add.s64 	%rd818, %rd815, 24;
	// begin inline asm
	{ atom.add.f64 %fd4057,[%rd818],%fd10785; }

	// end inline asm
	add.s64 	%rd819, %rd815, 32;
	// begin inline asm
	{ atom.add.f64 %fd4059,[%rd819],%fd10784; }

	// end inline asm
	add.s64 	%rd820, %rd815, 40;
	// begin inline asm
	{ atom.add.f64 %fd4061,[%rd820],%fd10783; }

	// end inline asm
	add.s64 	%rd821, %rd815, 48;
	// begin inline asm
	{ atom.add.f64 %fd4063,[%rd821],%fd10773; }

	// end inline asm
	add.s64 	%rd822, %rd815, 56;
	// begin inline asm
	{ atom.add.f64 %fd4065,[%rd822],%fd10772; }

	// end inline asm
	add.s64 	%rd823, %rd815, 64;
	// begin inline asm
	{ atom.add.f64 %fd4067,[%rd823],%fd10771; }

	// end inline asm
	mul.wide.s32 	%rd826, %r2083, %r129;
	cvta.to.global.u64 	%rd827, %rd4002;
	add.s64 	%rd828, %rd827, %rd826;
	mul.wide.s32 	%rd829, %r2084, %r129;
	add.s64 	%rd830, %rd824, %rd829;
	st.global.u32 	[%rd828], %r110;
	add.s32 	%r1687, %r112, 1;
	st.global.u32 	[%rd830], %r1687;

$L__BB1_117:
	ld.param.u32 	%r133, [%rd27+172];
	ld.param.v2.u32 	{%r1184, %r1185}, [%rd27+176];
	setp.le.s32 	%p88, %r1184, %r110;
	add.s32 	%r137, %r112, 2;
	setp.le.s32 	%p89, %r1185, %r137;
	add.s32 	%r138, %r120, 2;
	setp.le.s32 	%p90, %r133, %r138;
	or.pred  	%p91, %p88, %p89;
	or.b32  	%r1186, %r110, %r138;
	or.b32  	%r1187, %r1186, %r137;
	setp.lt.s32 	%p92, %r1187, 0;
	or.pred  	%p93, %p92, %p91;
	or.pred  	%p94, %p90, %p93;
	@%p94 bra 	$L__BB1_119;
	bra.uni 	$L__BB1_118;

$L__BB1_119:
	add.s32 	%r1691, %r112, 2;
	st.local.v2.u32 	[%rd26], {%r110, %r1691};
	add.s32 	%r1692, %r120, 2;
	st.local.v2.u32 	[%rd26+8], {%r1692, %r1184};
	st.local.v2.u32 	[%rd26+16], {%r1185, %r133};
	mov.u64 	%rd850, $str$1;
	cvta.global.u64 	%rd851, %rd850;
	{ // callseq 350, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd851;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1188, [retval0+0];
	} // callseq 350
	bra.uni 	$L__BB1_120;

$L__BB1_118:
	ld.param.u32 	%r2087, [%rd27+88];
	ld.param.u64 	%rd4005, [%rd27];
	ld.param.u32 	%r2086, [%rd27+32];
	ld.param.u64 	%rd4004, [%rd27+112];
	ld.param.u32 	%r2085, [%rd27+144];
	ld.param.u64 	%rd4003, [%rd27+56];
	cvta.to.global.u64 	%rd843, %rd4003;
	mul.wide.s32 	%rd844, %r2085, %r138;
	add.s64 	%rd834, %rd4004, %rd844;
	// begin inline asm
	{ atom.add.f64 %fd4069,[%rd834],%fd10794; }

	// end inline asm
	add.s64 	%rd835, %rd834, 8;
	// begin inline asm
	{ atom.add.f64 %fd4071,[%rd835],%fd10793; }

	// end inline asm
	add.s64 	%rd836, %rd834, 16;
	// begin inline asm
	{ atom.add.f64 %fd4073,[%rd836],%fd10792; }

	// end inline asm
	add.s64 	%rd837, %rd834, 24;
	// begin inline asm
	{ atom.add.f64 %fd4075,[%rd837],%fd10782; }

	// end inline asm
	add.s64 	%rd838, %rd834, 32;
	// begin inline asm
	{ atom.add.f64 %fd4077,[%rd838],%fd10781; }

	// end inline asm
	add.s64 	%rd839, %rd834, 40;
	// begin inline asm
	{ atom.add.f64 %fd4079,[%rd839],%fd10780; }

	// end inline asm
	add.s64 	%rd840, %rd834, 48;
	// begin inline asm
	{ atom.add.f64 %fd4081,[%rd840],%fd10770; }

	// end inline asm
	add.s64 	%rd841, %rd834, 56;
	// begin inline asm
	{ atom.add.f64 %fd4083,[%rd841],%fd10769; }

	// end inline asm
	add.s64 	%rd842, %rd834, 64;
	// begin inline asm
	{ atom.add.f64 %fd4085,[%rd842],%fd10768; }

	// end inline asm
	mul.wide.s32 	%rd845, %r2086, %r138;
	cvta.to.global.u64 	%rd846, %rd4005;
	add.s64 	%rd847, %rd846, %rd845;
	mul.wide.s32 	%rd848, %r2087, %r138;
	add.s64 	%rd849, %rd843, %rd848;
	st.global.u32 	[%rd847], %r110;
	add.s32 	%r1690, %r112, 2;
	st.global.u32 	[%rd849], %r1690;

$L__BB1_120:
	ld.param.u32 	%r142, [%rd27+172];
	ld.param.v2.u32 	{%r1189, %r1190}, [%rd27+176];
	setp.le.s32 	%p95, %r1189, %r110;
	add.s32 	%r146, %r112, 3;
	setp.le.s32 	%p96, %r1190, %r146;
	add.s32 	%r147, %r120, 3;
	setp.le.s32 	%p97, %r142, %r147;
	or.pred  	%p98, %p95, %p96;
	or.b32  	%r1191, %r110, %r147;
	or.b32  	%r1192, %r1191, %r146;
	setp.lt.s32 	%p99, %r1192, 0;
	or.pred  	%p100, %p99, %p98;
	or.pred  	%p101, %p97, %p100;
	@%p101 bra 	$L__BB1_122;
	bra.uni 	$L__BB1_121;

$L__BB1_122:
	add.s32 	%r1694, %r112, 3;
	st.local.v2.u32 	[%rd26], {%r110, %r1694};
	add.s32 	%r1695, %r120, 3;
	st.local.v2.u32 	[%rd26+8], {%r1695, %r1189};
	st.local.v2.u32 	[%rd26+16], {%r1190, %r142};
	mov.u64 	%rd869, $str$1;
	cvta.global.u64 	%rd870, %rd869;
	{ // callseq 351, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd870;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1193, [retval0+0];
	} // callseq 351
	bra.uni 	$L__BB1_123;

$L__BB1_121:
	ld.param.u32 	%r2090, [%rd27+88];
	ld.param.u64 	%rd4008, [%rd27];
	ld.param.u32 	%r2089, [%rd27+32];
	ld.param.u64 	%rd4007, [%rd27+112];
	ld.param.u32 	%r2088, [%rd27+144];
	ld.param.u64 	%rd4006, [%rd27+56];
	cvta.to.global.u64 	%rd862, %rd4006;
	mul.wide.s32 	%rd863, %r2088, %r147;
	add.s64 	%rd853, %rd4007, %rd863;
	// begin inline asm
	{ atom.add.f64 %fd4087,[%rd853],%fd10791; }

	// end inline asm
	add.s64 	%rd854, %rd853, 8;
	// begin inline asm
	{ atom.add.f64 %fd4089,[%rd854],%fd10790; }

	// end inline asm
	add.s64 	%rd855, %rd853, 16;
	// begin inline asm
	{ atom.add.f64 %fd4091,[%rd855],%fd10789; }

	// end inline asm
	add.s64 	%rd856, %rd853, 24;
	// begin inline asm
	{ atom.add.f64 %fd4093,[%rd856],%fd10779; }

	// end inline asm
	add.s64 	%rd857, %rd853, 32;
	// begin inline asm
	{ atom.add.f64 %fd4095,[%rd857],%fd10778; }

	// end inline asm
	add.s64 	%rd858, %rd853, 40;
	// begin inline asm
	{ atom.add.f64 %fd4097,[%rd858],%fd10777; }

	// end inline asm
	add.s64 	%rd859, %rd853, 48;
	// begin inline asm
	{ atom.add.f64 %fd4099,[%rd859],%fd10767; }

	// end inline asm
	add.s64 	%rd860, %rd853, 56;
	// begin inline asm
	{ atom.add.f64 %fd4101,[%rd860],%fd10766; }

	// end inline asm
	add.s64 	%rd861, %rd853, 64;
	// begin inline asm
	{ atom.add.f64 %fd4103,[%rd861],%fd10765; }

	// end inline asm
	mul.wide.s32 	%rd864, %r2089, %r147;
	cvta.to.global.u64 	%rd865, %rd4008;
	add.s64 	%rd866, %rd865, %rd864;
	mul.wide.s32 	%rd867, %r2090, %r147;
	add.s64 	%rd868, %rd862, %rd867;
	st.global.u32 	[%rd866], %r110;
	add.s32 	%r1693, %r112, 3;
	st.global.u32 	[%rd868], %r1693;

$L__BB1_123:
	ld.param.u32 	%r151, [%rd27+172];
	ld.param.v2.u32 	{%r1194, %r1195}, [%rd27+176];
	add.s32 	%r155, %r110, 1;
	setp.le.s32 	%p102, %r1194, %r155;
	setp.le.s32 	%p103, %r1195, %r112;
	add.s32 	%r156, %r120, 4;
	setp.le.s32 	%p104, %r151, %r156;
	or.pred  	%p105, %p102, %p103;
	or.b32  	%r1196, %r112, %r156;
	or.b32  	%r1197, %r1196, %r155;
	setp.lt.s32 	%p106, %r1197, 0;
	or.pred  	%p107, %p106, %p105;
	or.pred  	%p108, %p104, %p107;
	@%p108 bra 	$L__BB1_125;
	bra.uni 	$L__BB1_124;

$L__BB1_125:
	add.s32 	%r1697, %r110, 1;
	st.local.v2.u32 	[%rd26], {%r1697, %r112};
	add.s32 	%r1698, %r120, 4;
	st.local.v2.u32 	[%rd26+8], {%r1698, %r1194};
	st.local.v2.u32 	[%rd26+16], {%r1195, %r151};
	mov.u64 	%rd888, $str$1;
	cvta.global.u64 	%rd889, %rd888;
	{ // callseq 352, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd889;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1198, [retval0+0];
	} // callseq 352
	bra.uni 	$L__BB1_126;

$L__BB1_124:
	ld.param.u32 	%r2025, [%rd27+88];
	ld.param.u64 	%rd3971, [%rd27];
	ld.param.u32 	%r2024, [%rd27+32];
	ld.param.u64 	%rd3970, [%rd27+112];
	ld.param.u32 	%r2023, [%rd27+144];
	ld.param.u64 	%rd3969, [%rd27+56];
	cvta.to.global.u64 	%rd881, %rd3969;
	mul.wide.s32 	%rd882, %r2023, %r156;
	add.s64 	%rd872, %rd3970, %rd882;
	// begin inline asm
	{ atom.add.f64 %fd4105,[%rd872],%fd10764; }

	// end inline asm
	add.s64 	%rd873, %rd872, 8;
	// begin inline asm
	{ atom.add.f64 %fd4107,[%rd873],%fd10763; }

	// end inline asm
	add.s64 	%rd874, %rd872, 16;
	// begin inline asm
	{ atom.add.f64 %fd4109,[%rd874],%fd10762; }

	// end inline asm
	add.s64 	%rd875, %rd872, 24;
	// begin inline asm
	{ atom.add.f64 %fd4111,[%rd875],%fd10752; }

	// end inline asm
	add.s64 	%rd876, %rd872, 32;
	// begin inline asm
	{ atom.add.f64 %fd4113,[%rd876],%fd10751; }

	// end inline asm
	add.s64 	%rd877, %rd872, 40;
	// begin inline asm
	{ atom.add.f64 %fd4115,[%rd877],%fd10750; }

	// end inline asm
	add.s64 	%rd878, %rd872, 48;
	// begin inline asm
	{ atom.add.f64 %fd4117,[%rd878],%fd10740; }

	// end inline asm
	add.s64 	%rd879, %rd872, 56;
	// begin inline asm
	{ atom.add.f64 %fd4119,[%rd879],%fd10739; }

	// end inline asm
	add.s64 	%rd880, %rd872, 64;
	// begin inline asm
	{ atom.add.f64 %fd4121,[%rd880],%fd10738; }

	// end inline asm
	mul.wide.s32 	%rd883, %r2024, %r156;
	cvta.to.global.u64 	%rd884, %rd3971;
	add.s64 	%rd885, %rd884, %rd883;
	mul.wide.s32 	%rd886, %r2025, %r156;
	add.s64 	%rd887, %rd881, %rd886;
	add.s32 	%r1696, %r110, 1;
	st.global.u32 	[%rd885], %r1696;
	st.global.u32 	[%rd887], %r112;

$L__BB1_126:
	add.s32 	%r2027, %r112, 1;
	add.s32 	%r2026, %r110, 1;
	ld.param.u32 	%r160, [%rd27+172];
	ld.param.v2.u32 	{%r1199, %r1200}, [%rd27+176];
	setp.le.s32 	%p109, %r1199, %r2026;
	setp.le.s32 	%p110, %r1200, %r2027;
	add.s32 	%r164, %r120, 5;
	setp.le.s32 	%p111, %r160, %r164;
	or.pred  	%p112, %p109, %p110;
	or.b32  	%r1201, %r2026, %r164;
	or.b32  	%r1202, %r1201, %r2027;
	setp.lt.s32 	%p113, %r1202, 0;
	or.pred  	%p114, %p113, %p112;
	or.pred  	%p115, %p111, %p114;
	@%p115 bra 	$L__BB1_128;
	bra.uni 	$L__BB1_127;

$L__BB1_128:
	add.s32 	%r2047, %r112, 1;
	add.s32 	%r1701, %r110, 1;
	st.local.v2.u32 	[%rd26], {%r1701, %r2047};
	add.s32 	%r1702, %r120, 5;
	st.local.v2.u32 	[%rd26+8], {%r1702, %r1199};
	st.local.v2.u32 	[%rd26+16], {%r1200, %r160};
	mov.u64 	%rd907, $str$1;
	cvta.global.u64 	%rd908, %rd907;
	{ // callseq 353, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd908;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1203, [retval0+0];
	} // callseq 353
	bra.uni 	$L__BB1_129;

$L__BB1_127:
	ld.param.u32 	%r2050, [%rd27+88];
	ld.param.u64 	%rd3975, [%rd27];
	ld.param.u32 	%r2049, [%rd27+32];
	ld.param.u64 	%rd3974, [%rd27+112];
	ld.param.u32 	%r2048, [%rd27+144];
	ld.param.u64 	%rd3973, [%rd27+56];
	cvta.to.global.u64 	%rd900, %rd3973;
	mul.wide.s32 	%rd901, %r2048, %r164;
	add.s64 	%rd891, %rd3974, %rd901;
	// begin inline asm
	{ atom.add.f64 %fd4123,[%rd891],%fd10761; }

	// end inline asm
	add.s64 	%rd892, %rd891, 8;
	// begin inline asm
	{ atom.add.f64 %fd4125,[%rd892],%fd10760; }

	// end inline asm
	add.s64 	%rd893, %rd891, 16;
	// begin inline asm
	{ atom.add.f64 %fd4127,[%rd893],%fd10759; }

	// end inline asm
	add.s64 	%rd894, %rd891, 24;
	// begin inline asm
	{ atom.add.f64 %fd4129,[%rd894],%fd10749; }

	// end inline asm
	add.s64 	%rd895, %rd891, 32;
	// begin inline asm
	{ atom.add.f64 %fd4131,[%rd895],%fd10748; }

	// end inline asm
	add.s64 	%rd896, %rd891, 40;
	// begin inline asm
	{ atom.add.f64 %fd4133,[%rd896],%fd10747; }

	// end inline asm
	add.s64 	%rd897, %rd891, 48;
	// begin inline asm
	{ atom.add.f64 %fd4135,[%rd897],%fd10737; }

	// end inline asm
	add.s64 	%rd898, %rd891, 56;
	// begin inline asm
	{ atom.add.f64 %fd4137,[%rd898],%fd10736; }

	// end inline asm
	add.s64 	%rd899, %rd891, 64;
	// begin inline asm
	{ atom.add.f64 %fd4139,[%rd899],%fd10735; }

	// end inline asm
	mul.wide.s32 	%rd902, %r2049, %r164;
	cvta.to.global.u64 	%rd903, %rd3975;
	add.s64 	%rd904, %rd903, %rd902;
	mul.wide.s32 	%rd905, %r2050, %r164;
	add.s64 	%rd906, %rd900, %rd905;
	add.s32 	%r1699, %r110, 1;
	st.global.u32 	[%rd904], %r1699;
	add.s32 	%r1700, %r112, 1;
	st.global.u32 	[%rd906], %r1700;

$L__BB1_129:
	add.s32 	%r2029, %r112, 2;
	add.s32 	%r2028, %r110, 1;
	ld.param.u32 	%r168, [%rd27+172];
	ld.param.v2.u32 	{%r1204, %r1205}, [%rd27+176];
	setp.le.s32 	%p116, %r1204, %r2028;
	setp.le.s32 	%p117, %r1205, %r2029;
	add.s32 	%r172, %r120, 6;
	setp.le.s32 	%p118, %r168, %r172;
	or.pred  	%p119, %p116, %p117;
	or.b32  	%r1206, %r2028, %r172;
	or.b32  	%r1207, %r1206, %r2029;
	setp.lt.s32 	%p120, %r1207, 0;
	or.pred  	%p121, %p120, %p119;
	or.pred  	%p122, %p118, %p121;
	@%p122 bra 	$L__BB1_131;
	bra.uni 	$L__BB1_130;

$L__BB1_131:
	add.s32 	%r2046, %r112, 2;
	add.s32 	%r1705, %r110, 1;
	st.local.v2.u32 	[%rd26], {%r1705, %r2046};
	add.s32 	%r1706, %r120, 6;
	st.local.v2.u32 	[%rd26+8], {%r1706, %r1204};
	st.local.v2.u32 	[%rd26+16], {%r1205, %r168};
	mov.u64 	%rd926, $str$1;
	cvta.global.u64 	%rd927, %rd926;
	{ // callseq 354, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd927;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1208, [retval0+0];
	} // callseq 354
	bra.uni 	$L__BB1_132;

$L__BB1_130:
	ld.param.u32 	%r2053, [%rd27+88];
	ld.param.u64 	%rd3978, [%rd27];
	ld.param.u32 	%r2052, [%rd27+32];
	ld.param.u64 	%rd3977, [%rd27+112];
	ld.param.u32 	%r2051, [%rd27+144];
	ld.param.u64 	%rd3976, [%rd27+56];
	cvta.to.global.u64 	%rd919, %rd3976;
	mul.wide.s32 	%rd920, %r2051, %r172;
	add.s64 	%rd910, %rd3977, %rd920;
	// begin inline asm
	{ atom.add.f64 %fd4141,[%rd910],%fd10758; }

	// end inline asm
	add.s64 	%rd911, %rd910, 8;
	// begin inline asm
	{ atom.add.f64 %fd4143,[%rd911],%fd10757; }

	// end inline asm
	add.s64 	%rd912, %rd910, 16;
	// begin inline asm
	{ atom.add.f64 %fd4145,[%rd912],%fd10756; }

	// end inline asm
	add.s64 	%rd913, %rd910, 24;
	// begin inline asm
	{ atom.add.f64 %fd4147,[%rd913],%fd10746; }

	// end inline asm
	add.s64 	%rd914, %rd910, 32;
	// begin inline asm
	{ atom.add.f64 %fd4149,[%rd914],%fd10745; }

	// end inline asm
	add.s64 	%rd915, %rd910, 40;
	// begin inline asm
	{ atom.add.f64 %fd4151,[%rd915],%fd10744; }

	// end inline asm
	add.s64 	%rd916, %rd910, 48;
	// begin inline asm
	{ atom.add.f64 %fd4153,[%rd916],%fd10734; }

	// end inline asm
	add.s64 	%rd917, %rd910, 56;
	// begin inline asm
	{ atom.add.f64 %fd4155,[%rd917],%fd10733; }

	// end inline asm
	add.s64 	%rd918, %rd910, 64;
	// begin inline asm
	{ atom.add.f64 %fd4157,[%rd918],%fd10732; }

	// end inline asm
	mul.wide.s32 	%rd921, %r2052, %r172;
	cvta.to.global.u64 	%rd922, %rd3978;
	add.s64 	%rd923, %rd922, %rd921;
	mul.wide.s32 	%rd924, %r2053, %r172;
	add.s64 	%rd925, %rd919, %rd924;
	add.s32 	%r1703, %r110, 1;
	st.global.u32 	[%rd923], %r1703;
	add.s32 	%r1704, %r112, 2;
	st.global.u32 	[%rd925], %r1704;

$L__BB1_132:
	add.s32 	%r2031, %r112, 3;
	add.s32 	%r2030, %r110, 1;
	ld.param.u32 	%r176, [%rd27+172];
	ld.param.v2.u32 	{%r1209, %r1210}, [%rd27+176];
	setp.le.s32 	%p123, %r1209, %r2030;
	setp.le.s32 	%p124, %r1210, %r2031;
	add.s32 	%r180, %r120, 7;
	setp.le.s32 	%p125, %r176, %r180;
	or.pred  	%p126, %p123, %p124;
	or.b32  	%r1211, %r2030, %r180;
	or.b32  	%r1212, %r1211, %r2031;
	setp.lt.s32 	%p127, %r1212, 0;
	or.pred  	%p128, %p127, %p126;
	or.pred  	%p129, %p125, %p128;
	@%p129 bra 	$L__BB1_134;
	bra.uni 	$L__BB1_133;

$L__BB1_134:
	add.s32 	%r2045, %r112, 3;
	add.s32 	%r1709, %r110, 1;
	st.local.v2.u32 	[%rd26], {%r1709, %r2045};
	add.s32 	%r1710, %r120, 7;
	st.local.v2.u32 	[%rd26+8], {%r1710, %r1209};
	st.local.v2.u32 	[%rd26+16], {%r1210, %r176};
	mov.u64 	%rd945, $str$1;
	cvta.global.u64 	%rd946, %rd945;
	{ // callseq 355, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd946;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1213, [retval0+0];
	} // callseq 355
	bra.uni 	$L__BB1_135;

$L__BB1_133:
	ld.param.u32 	%r2093, [%rd27+88];
	ld.param.u64 	%rd4011, [%rd27];
	ld.param.u32 	%r2092, [%rd27+32];
	ld.param.u64 	%rd4010, [%rd27+112];
	ld.param.u32 	%r2091, [%rd27+144];
	ld.param.u64 	%rd4009, [%rd27+56];
	cvta.to.global.u64 	%rd938, %rd4009;
	mul.wide.s32 	%rd939, %r2091, %r180;
	add.s64 	%rd929, %rd4010, %rd939;
	// begin inline asm
	{ atom.add.f64 %fd4159,[%rd929],%fd10755; }

	// end inline asm
	add.s64 	%rd930, %rd929, 8;
	// begin inline asm
	{ atom.add.f64 %fd4161,[%rd930],%fd10754; }

	// end inline asm
	add.s64 	%rd931, %rd929, 16;
	// begin inline asm
	{ atom.add.f64 %fd4163,[%rd931],%fd10753; }

	// end inline asm
	add.s64 	%rd932, %rd929, 24;
	// begin inline asm
	{ atom.add.f64 %fd4165,[%rd932],%fd10743; }

	// end inline asm
	add.s64 	%rd933, %rd929, 32;
	// begin inline asm
	{ atom.add.f64 %fd4167,[%rd933],%fd10742; }

	// end inline asm
	add.s64 	%rd934, %rd929, 40;
	// begin inline asm
	{ atom.add.f64 %fd4169,[%rd934],%fd10741; }

	// end inline asm
	add.s64 	%rd935, %rd929, 48;
	// begin inline asm
	{ atom.add.f64 %fd4171,[%rd935],%fd10731; }

	// end inline asm
	add.s64 	%rd936, %rd929, 56;
	// begin inline asm
	{ atom.add.f64 %fd4173,[%rd936],%fd10730; }

	// end inline asm
	add.s64 	%rd937, %rd929, 64;
	// begin inline asm
	{ atom.add.f64 %fd4175,[%rd937],%fd10729; }

	// end inline asm
	mul.wide.s32 	%rd940, %r2092, %r180;
	cvta.to.global.u64 	%rd941, %rd4011;
	add.s64 	%rd942, %rd941, %rd940;
	mul.wide.s32 	%rd943, %r2093, %r180;
	add.s64 	%rd944, %rd938, %rd943;
	add.s32 	%r1707, %r110, 1;
	st.global.u32 	[%rd942], %r1707;
	add.s32 	%r1708, %r112, 3;
	st.global.u32 	[%rd944], %r1708;

$L__BB1_135:
	ld.param.u32 	%r184, [%rd27+172];
	ld.param.v2.u32 	{%r1214, %r1215}, [%rd27+176];
	add.s32 	%r188, %r110, 2;
	setp.le.s32 	%p130, %r1214, %r188;
	setp.le.s32 	%p131, %r1215, %r112;
	add.s32 	%r189, %r120, 8;
	setp.le.s32 	%p132, %r184, %r189;
	or.pred  	%p133, %p130, %p131;
	or.b32  	%r1216, %r112, %r189;
	or.b32  	%r1217, %r1216, %r188;
	setp.lt.s32 	%p134, %r1217, 0;
	or.pred  	%p135, %p134, %p133;
	or.pred  	%p136, %p132, %p135;
	@%p136 bra 	$L__BB1_137;
	bra.uni 	$L__BB1_136;

$L__BB1_137:
	add.s32 	%r1712, %r110, 2;
	st.local.v2.u32 	[%rd26], {%r1712, %r112};
	add.s32 	%r1713, %r120, 8;
	st.local.v2.u32 	[%rd26+8], {%r1713, %r1214};
	st.local.v2.u32 	[%rd26+16], {%r1215, %r184};
	mov.u64 	%rd964, $str$1;
	cvta.global.u64 	%rd965, %rd964;
	{ // callseq 356, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd965;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1218, [retval0+0];
	} // callseq 356
	bra.uni 	$L__BB1_138;

$L__BB1_136:
	ld.param.u32 	%r2056, [%rd27+88];
	ld.param.u64 	%rd3981, [%rd27];
	ld.param.u32 	%r2055, [%rd27+32];
	ld.param.u64 	%rd3980, [%rd27+112];
	ld.param.u32 	%r2054, [%rd27+144];
	ld.param.u64 	%rd3979, [%rd27+56];
	cvta.to.global.u64 	%rd957, %rd3979;
	mul.wide.s32 	%rd958, %r2054, %r189;
	add.s64 	%rd948, %rd3980, %rd958;
	// begin inline asm
	{ atom.add.f64 %fd4177,[%rd948],%fd10728; }

	// end inline asm
	add.s64 	%rd949, %rd948, 8;
	// begin inline asm
	{ atom.add.f64 %fd4179,[%rd949],%fd10727; }

	// end inline asm
	add.s64 	%rd950, %rd948, 16;
	// begin inline asm
	{ atom.add.f64 %fd4181,[%rd950],%fd10726; }

	// end inline asm
	add.s64 	%rd951, %rd948, 24;
	// begin inline asm
	{ atom.add.f64 %fd4183,[%rd951],%fd10716; }

	// end inline asm
	add.s64 	%rd952, %rd948, 32;
	// begin inline asm
	{ atom.add.f64 %fd4185,[%rd952],%fd10715; }

	// end inline asm
	add.s64 	%rd953, %rd948, 40;
	// begin inline asm
	{ atom.add.f64 %fd4187,[%rd953],%fd10714; }

	// end inline asm
	add.s64 	%rd954, %rd948, 48;
	// begin inline asm
	{ atom.add.f64 %fd4189,[%rd954],%fd10704; }

	// end inline asm
	add.s64 	%rd955, %rd948, 56;
	// begin inline asm
	{ atom.add.f64 %fd4191,[%rd955],%fd10703; }

	// end inline asm
	add.s64 	%rd956, %rd948, 64;
	// begin inline asm
	{ atom.add.f64 %fd4193,[%rd956],%fd10702; }

	// end inline asm
	mul.wide.s32 	%rd959, %r2055, %r189;
	cvta.to.global.u64 	%rd960, %rd3981;
	add.s64 	%rd961, %rd960, %rd959;
	mul.wide.s32 	%rd962, %r2056, %r189;
	add.s64 	%rd963, %rd957, %rd962;
	add.s32 	%r1711, %r110, 2;
	st.global.u32 	[%rd961], %r1711;
	st.global.u32 	[%rd963], %r112;

$L__BB1_138:
	add.s32 	%r2057, %r110, 2;
	add.s32 	%r2032, %r112, 1;
	ld.param.u32 	%r193, [%rd27+172];
	ld.param.v2.u32 	{%r1219, %r1220}, [%rd27+176];
	setp.le.s32 	%p137, %r1219, %r2057;
	setp.le.s32 	%p138, %r1220, %r2032;
	add.s32 	%r197, %r120, 9;
	setp.le.s32 	%p139, %r193, %r197;
	or.pred  	%p140, %p137, %p138;
	or.b32  	%r1221, %r2057, %r197;
	or.b32  	%r1222, %r1221, %r2032;
	setp.lt.s32 	%p141, %r1222, 0;
	or.pred  	%p142, %p141, %p140;
	or.pred  	%p143, %p139, %p142;
	@%p143 bra 	$L__BB1_140;
	bra.uni 	$L__BB1_139;

$L__BB1_140:
	add.s32 	%r2044, %r112, 1;
	add.s32 	%r1716, %r110, 2;
	st.local.v2.u32 	[%rd26], {%r1716, %r2044};
	add.s32 	%r1717, %r120, 9;
	st.local.v2.u32 	[%rd26+8], {%r1717, %r1219};
	st.local.v2.u32 	[%rd26+16], {%r1220, %r193};
	mov.u64 	%rd983, $str$1;
	cvta.global.u64 	%rd984, %rd983;
	{ // callseq 357, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd984;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1223, [retval0+0];
	} // callseq 357
	bra.uni 	$L__BB1_141;

$L__BB1_139:
	ld.param.u32 	%r2062, [%rd27+88];
	ld.param.u64 	%rd3984, [%rd27];
	ld.param.u32 	%r2061, [%rd27+32];
	ld.param.u64 	%rd3983, [%rd27+112];
	ld.param.u32 	%r2060, [%rd27+144];
	ld.param.u64 	%rd3982, [%rd27+56];
	cvta.to.global.u64 	%rd976, %rd3982;
	mul.wide.s32 	%rd977, %r2060, %r197;
	add.s64 	%rd967, %rd3983, %rd977;
	// begin inline asm
	{ atom.add.f64 %fd4195,[%rd967],%fd10725; }

	// end inline asm
	add.s64 	%rd968, %rd967, 8;
	// begin inline asm
	{ atom.add.f64 %fd4197,[%rd968],%fd10724; }

	// end inline asm
	add.s64 	%rd969, %rd967, 16;
	// begin inline asm
	{ atom.add.f64 %fd4199,[%rd969],%fd10723; }

	// end inline asm
	add.s64 	%rd970, %rd967, 24;
	// begin inline asm
	{ atom.add.f64 %fd4201,[%rd970],%fd10713; }

	// end inline asm
	add.s64 	%rd971, %rd967, 32;
	// begin inline asm
	{ atom.add.f64 %fd4203,[%rd971],%fd10712; }

	// end inline asm
	add.s64 	%rd972, %rd967, 40;
	// begin inline asm
	{ atom.add.f64 %fd4205,[%rd972],%fd10711; }

	// end inline asm
	add.s64 	%rd973, %rd967, 48;
	// begin inline asm
	{ atom.add.f64 %fd4207,[%rd973],%fd10701; }

	// end inline asm
	add.s64 	%rd974, %rd967, 56;
	// begin inline asm
	{ atom.add.f64 %fd4209,[%rd974],%fd10700; }

	// end inline asm
	add.s64 	%rd975, %rd967, 64;
	// begin inline asm
	{ atom.add.f64 %fd4211,[%rd975],%fd10699; }

	// end inline asm
	mul.wide.s32 	%rd978, %r2061, %r197;
	cvta.to.global.u64 	%rd979, %rd3984;
	add.s64 	%rd980, %rd979, %rd978;
	mul.wide.s32 	%rd981, %r2062, %r197;
	add.s64 	%rd982, %rd976, %rd981;
	add.s32 	%r1714, %r110, 2;
	st.global.u32 	[%rd980], %r1714;
	add.s32 	%r1715, %r112, 1;
	st.global.u32 	[%rd982], %r1715;

$L__BB1_141:
	add.s32 	%r2058, %r110, 2;
	add.s32 	%r2033, %r112, 2;
	ld.param.u32 	%r201, [%rd27+172];
	ld.param.v2.u32 	{%r1224, %r1225}, [%rd27+176];
	setp.le.s32 	%p144, %r1224, %r2058;
	setp.le.s32 	%p145, %r1225, %r2033;
	add.s32 	%r205, %r120, 10;
	setp.le.s32 	%p146, %r201, %r205;
	or.pred  	%p147, %p144, %p145;
	or.b32  	%r1226, %r2058, %r205;
	or.b32  	%r1227, %r1226, %r2033;
	setp.lt.s32 	%p148, %r1227, 0;
	or.pred  	%p149, %p148, %p147;
	or.pred  	%p150, %p146, %p149;
	@%p150 bra 	$L__BB1_143;
	bra.uni 	$L__BB1_142;

$L__BB1_143:
	add.s32 	%r2043, %r112, 2;
	add.s32 	%r1720, %r110, 2;
	st.local.v2.u32 	[%rd26], {%r1720, %r2043};
	add.s32 	%r1721, %r120, 10;
	st.local.v2.u32 	[%rd26+8], {%r1721, %r1224};
	st.local.v2.u32 	[%rd26+16], {%r1225, %r201};
	mov.u64 	%rd1002, $str$1;
	cvta.global.u64 	%rd1003, %rd1002;
	{ // callseq 358, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1003;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1228, [retval0+0];
	} // callseq 358
	bra.uni 	$L__BB1_144;

$L__BB1_142:
	ld.param.u32 	%r2065, [%rd27+88];
	ld.param.u64 	%rd3987, [%rd27];
	ld.param.u32 	%r2064, [%rd27+32];
	ld.param.u64 	%rd3986, [%rd27+112];
	ld.param.u32 	%r2063, [%rd27+144];
	ld.param.u64 	%rd3985, [%rd27+56];
	cvta.to.global.u64 	%rd995, %rd3985;
	mul.wide.s32 	%rd996, %r2063, %r205;
	add.s64 	%rd986, %rd3986, %rd996;
	// begin inline asm
	{ atom.add.f64 %fd4213,[%rd986],%fd10722; }

	// end inline asm
	add.s64 	%rd987, %rd986, 8;
	// begin inline asm
	{ atom.add.f64 %fd4215,[%rd987],%fd10721; }

	// end inline asm
	add.s64 	%rd988, %rd986, 16;
	// begin inline asm
	{ atom.add.f64 %fd4217,[%rd988],%fd10720; }

	// end inline asm
	add.s64 	%rd989, %rd986, 24;
	// begin inline asm
	{ atom.add.f64 %fd4219,[%rd989],%fd10710; }

	// end inline asm
	add.s64 	%rd990, %rd986, 32;
	// begin inline asm
	{ atom.add.f64 %fd4221,[%rd990],%fd10709; }

	// end inline asm
	add.s64 	%rd991, %rd986, 40;
	// begin inline asm
	{ atom.add.f64 %fd4223,[%rd991],%fd10708; }

	// end inline asm
	add.s64 	%rd992, %rd986, 48;
	// begin inline asm
	{ atom.add.f64 %fd4225,[%rd992],%fd10698; }

	// end inline asm
	add.s64 	%rd993, %rd986, 56;
	// begin inline asm
	{ atom.add.f64 %fd4227,[%rd993],%fd10697; }

	// end inline asm
	add.s64 	%rd994, %rd986, 64;
	// begin inline asm
	{ atom.add.f64 %fd4229,[%rd994],%fd10696; }

	// end inline asm
	mul.wide.s32 	%rd997, %r2064, %r205;
	cvta.to.global.u64 	%rd998, %rd3987;
	add.s64 	%rd999, %rd998, %rd997;
	mul.wide.s32 	%rd1000, %r2065, %r205;
	add.s64 	%rd1001, %rd995, %rd1000;
	add.s32 	%r1718, %r110, 2;
	st.global.u32 	[%rd999], %r1718;
	add.s32 	%r1719, %r112, 2;
	st.global.u32 	[%rd1001], %r1719;

$L__BB1_144:
	add.s32 	%r2059, %r110, 2;
	add.s32 	%r2034, %r112, 3;
	ld.param.u32 	%r209, [%rd27+172];
	ld.param.v2.u32 	{%r1229, %r1230}, [%rd27+176];
	setp.le.s32 	%p151, %r1229, %r2059;
	setp.le.s32 	%p152, %r1230, %r2034;
	add.s32 	%r213, %r120, 11;
	setp.le.s32 	%p153, %r209, %r213;
	or.pred  	%p154, %p151, %p152;
	or.b32  	%r1231, %r2059, %r213;
	or.b32  	%r1232, %r1231, %r2034;
	setp.lt.s32 	%p155, %r1232, 0;
	or.pred  	%p156, %p155, %p154;
	or.pred  	%p157, %p153, %p156;
	@%p157 bra 	$L__BB1_146;
	bra.uni 	$L__BB1_145;

$L__BB1_146:
	add.s32 	%r2042, %r112, 3;
	add.s32 	%r1724, %r110, 2;
	st.local.v2.u32 	[%rd26], {%r1724, %r2042};
	add.s32 	%r1725, %r120, 11;
	st.local.v2.u32 	[%rd26+8], {%r1725, %r1229};
	st.local.v2.u32 	[%rd26+16], {%r1230, %r209};
	mov.u64 	%rd1021, $str$1;
	cvta.global.u64 	%rd1022, %rd1021;
	{ // callseq 359, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1022;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1233, [retval0+0];
	} // callseq 359
	bra.uni 	$L__BB1_147;

$L__BB1_145:
	ld.param.u32 	%r2096, [%rd27+88];
	ld.param.u64 	%rd4014, [%rd27];
	ld.param.u32 	%r2095, [%rd27+32];
	ld.param.u64 	%rd4013, [%rd27+112];
	ld.param.u32 	%r2094, [%rd27+144];
	ld.param.u64 	%rd4012, [%rd27+56];
	cvta.to.global.u64 	%rd1014, %rd4012;
	mul.wide.s32 	%rd1015, %r2094, %r213;
	add.s64 	%rd1005, %rd4013, %rd1015;
	// begin inline asm
	{ atom.add.f64 %fd4231,[%rd1005],%fd10719; }

	// end inline asm
	add.s64 	%rd1006, %rd1005, 8;
	// begin inline asm
	{ atom.add.f64 %fd4233,[%rd1006],%fd10718; }

	// end inline asm
	add.s64 	%rd1007, %rd1005, 16;
	// begin inline asm
	{ atom.add.f64 %fd4235,[%rd1007],%fd10717; }

	// end inline asm
	add.s64 	%rd1008, %rd1005, 24;
	// begin inline asm
	{ atom.add.f64 %fd4237,[%rd1008],%fd10707; }

	// end inline asm
	add.s64 	%rd1009, %rd1005, 32;
	// begin inline asm
	{ atom.add.f64 %fd4239,[%rd1009],%fd10706; }

	// end inline asm
	add.s64 	%rd1010, %rd1005, 40;
	// begin inline asm
	{ atom.add.f64 %fd4241,[%rd1010],%fd10705; }

	// end inline asm
	add.s64 	%rd1011, %rd1005, 48;
	// begin inline asm
	{ atom.add.f64 %fd4243,[%rd1011],%fd10695; }

	// end inline asm
	add.s64 	%rd1012, %rd1005, 56;
	// begin inline asm
	{ atom.add.f64 %fd4245,[%rd1012],%fd10694; }

	// end inline asm
	add.s64 	%rd1013, %rd1005, 64;
	// begin inline asm
	{ atom.add.f64 %fd4247,[%rd1013],%fd10693; }

	// end inline asm
	mul.wide.s32 	%rd1016, %r2095, %r213;
	cvta.to.global.u64 	%rd1017, %rd4014;
	add.s64 	%rd1018, %rd1017, %rd1016;
	mul.wide.s32 	%rd1019, %r2096, %r213;
	add.s64 	%rd1020, %rd1014, %rd1019;
	add.s32 	%r1722, %r110, 2;
	st.global.u32 	[%rd1018], %r1722;
	add.s32 	%r1723, %r112, 3;
	st.global.u32 	[%rd1020], %r1723;

$L__BB1_147:
	ld.param.u32 	%r217, [%rd27+172];
	ld.param.v2.u32 	{%r1234, %r1235}, [%rd27+176];
	add.s32 	%r221, %r110, 3;
	setp.le.s32 	%p158, %r1234, %r221;
	setp.le.s32 	%p159, %r1235, %r112;
	add.s32 	%r222, %r120, 12;
	setp.le.s32 	%p160, %r217, %r222;
	or.pred  	%p161, %p158, %p159;
	or.b32  	%r1236, %r112, %r222;
	or.b32  	%r1237, %r1236, %r221;
	setp.lt.s32 	%p162, %r1237, 0;
	or.pred  	%p163, %p162, %p161;
	or.pred  	%p164, %p160, %p163;
	@%p164 bra 	$L__BB1_149;
	bra.uni 	$L__BB1_148;

$L__BB1_149:
	add.s32 	%r1727, %r110, 3;
	st.local.v2.u32 	[%rd26], {%r1727, %r112};
	add.s32 	%r1728, %r120, 12;
	st.local.v2.u32 	[%rd26+8], {%r1728, %r1234};
	st.local.v2.u32 	[%rd26+16], {%r1235, %r217};
	mov.u64 	%rd1040, $str$1;
	cvta.global.u64 	%rd1041, %rd1040;
	{ // callseq 360, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1041;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1238, [retval0+0];
	} // callseq 360
	bra.uni 	$L__BB1_150;

$L__BB1_148:
	ld.param.u32 	%r2068, [%rd27+88];
	ld.param.u64 	%rd3990, [%rd27];
	ld.param.u32 	%r2067, [%rd27+32];
	ld.param.u64 	%rd3989, [%rd27+112];
	ld.param.u32 	%r2066, [%rd27+144];
	ld.param.u64 	%rd3988, [%rd27+56];
	cvta.to.global.u64 	%rd1033, %rd3988;
	mul.wide.s32 	%rd1034, %r2066, %r222;
	add.s64 	%rd1024, %rd3989, %rd1034;
	// begin inline asm
	{ atom.add.f64 %fd4249,[%rd1024],%fd10692; }

	// end inline asm
	add.s64 	%rd1025, %rd1024, 8;
	// begin inline asm
	{ atom.add.f64 %fd4251,[%rd1025],%fd10691; }

	// end inline asm
	add.s64 	%rd1026, %rd1024, 16;
	// begin inline asm
	{ atom.add.f64 %fd4253,[%rd1026],%fd10690; }

	// end inline asm
	add.s64 	%rd1027, %rd1024, 24;
	// begin inline asm
	{ atom.add.f64 %fd4255,[%rd1027],%fd10680; }

	// end inline asm
	add.s64 	%rd1028, %rd1024, 32;
	// begin inline asm
	{ atom.add.f64 %fd4257,[%rd1028],%fd10679; }

	// end inline asm
	add.s64 	%rd1029, %rd1024, 40;
	// begin inline asm
	{ atom.add.f64 %fd4259,[%rd1029],%fd10678; }

	// end inline asm
	add.s64 	%rd1030, %rd1024, 48;
	// begin inline asm
	{ atom.add.f64 %fd4261,[%rd1030],%fd10668; }

	// end inline asm
	add.s64 	%rd1031, %rd1024, 56;
	// begin inline asm
	{ atom.add.f64 %fd4263,[%rd1031],%fd10667; }

	// end inline asm
	add.s64 	%rd1032, %rd1024, 64;
	// begin inline asm
	{ atom.add.f64 %fd4265,[%rd1032],%fd10666; }

	// end inline asm
	mul.wide.s32 	%rd1035, %r2067, %r222;
	cvta.to.global.u64 	%rd1036, %rd3990;
	add.s64 	%rd1037, %rd1036, %rd1035;
	mul.wide.s32 	%rd1038, %r2068, %r222;
	add.s64 	%rd1039, %rd1033, %rd1038;
	add.s32 	%r1726, %r110, 3;
	st.global.u32 	[%rd1037], %r1726;
	st.global.u32 	[%rd1039], %r112;

$L__BB1_150:
	add.s32 	%r2069, %r110, 3;
	add.s32 	%r2035, %r112, 1;
	ld.param.u32 	%r226, [%rd27+172];
	ld.param.v2.u32 	{%r1239, %r1240}, [%rd27+176];
	setp.le.s32 	%p165, %r1239, %r2069;
	setp.le.s32 	%p166, %r1240, %r2035;
	add.s32 	%r230, %r120, 13;
	setp.le.s32 	%p167, %r226, %r230;
	or.pred  	%p168, %p165, %p166;
	or.b32  	%r1241, %r2069, %r230;
	or.b32  	%r1242, %r1241, %r2035;
	setp.lt.s32 	%p169, %r1242, 0;
	or.pred  	%p170, %p169, %p168;
	or.pred  	%p171, %p167, %p170;
	@%p171 bra 	$L__BB1_152;
	bra.uni 	$L__BB1_151;

$L__BB1_152:
	add.s32 	%r2041, %r112, 1;
	add.s32 	%r1731, %r110, 3;
	st.local.v2.u32 	[%rd26], {%r1731, %r2041};
	add.s32 	%r1732, %r120, 13;
	st.local.v2.u32 	[%rd26+8], {%r1732, %r1239};
	st.local.v2.u32 	[%rd26+16], {%r1240, %r226};
	mov.u64 	%rd1059, $str$1;
	cvta.global.u64 	%rd1060, %rd1059;
	{ // callseq 361, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1060;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1243, [retval0+0];
	} // callseq 361
	bra.uni 	$L__BB1_153;

$L__BB1_151:
	ld.param.u32 	%r2074, [%rd27+88];
	ld.param.u64 	%rd3993, [%rd27];
	ld.param.u32 	%r2073, [%rd27+32];
	ld.param.u64 	%rd3992, [%rd27+112];
	ld.param.u32 	%r2072, [%rd27+144];
	ld.param.u64 	%rd3991, [%rd27+56];
	cvta.to.global.u64 	%rd1052, %rd3991;
	mul.wide.s32 	%rd1053, %r2072, %r230;
	add.s64 	%rd1043, %rd3992, %rd1053;
	// begin inline asm
	{ atom.add.f64 %fd4267,[%rd1043],%fd10689; }

	// end inline asm
	add.s64 	%rd1044, %rd1043, 8;
	// begin inline asm
	{ atom.add.f64 %fd4269,[%rd1044],%fd10688; }

	// end inline asm
	add.s64 	%rd1045, %rd1043, 16;
	// begin inline asm
	{ atom.add.f64 %fd4271,[%rd1045],%fd10687; }

	// end inline asm
	add.s64 	%rd1046, %rd1043, 24;
	// begin inline asm
	{ atom.add.f64 %fd4273,[%rd1046],%fd10677; }

	// end inline asm
	add.s64 	%rd1047, %rd1043, 32;
	// begin inline asm
	{ atom.add.f64 %fd4275,[%rd1047],%fd10676; }

	// end inline asm
	add.s64 	%rd1048, %rd1043, 40;
	// begin inline asm
	{ atom.add.f64 %fd4277,[%rd1048],%fd10675; }

	// end inline asm
	add.s64 	%rd1049, %rd1043, 48;
	// begin inline asm
	{ atom.add.f64 %fd4279,[%rd1049],%fd10665; }

	// end inline asm
	add.s64 	%rd1050, %rd1043, 56;
	// begin inline asm
	{ atom.add.f64 %fd4281,[%rd1050],%fd10664; }

	// end inline asm
	add.s64 	%rd1051, %rd1043, 64;
	// begin inline asm
	{ atom.add.f64 %fd4283,[%rd1051],%fd10663; }

	// end inline asm
	mul.wide.s32 	%rd1054, %r2073, %r230;
	cvta.to.global.u64 	%rd1055, %rd3993;
	add.s64 	%rd1056, %rd1055, %rd1054;
	mul.wide.s32 	%rd1057, %r2074, %r230;
	add.s64 	%rd1058, %rd1052, %rd1057;
	add.s32 	%r1729, %r110, 3;
	st.global.u32 	[%rd1056], %r1729;
	add.s32 	%r1730, %r112, 1;
	st.global.u32 	[%rd1058], %r1730;

$L__BB1_153:
	add.s32 	%r2070, %r110, 3;
	add.s32 	%r2036, %r112, 2;
	ld.param.u32 	%r234, [%rd27+172];
	ld.param.v2.u32 	{%r1244, %r1245}, [%rd27+176];
	setp.le.s32 	%p172, %r1244, %r2070;
	setp.le.s32 	%p173, %r1245, %r2036;
	add.s32 	%r238, %r120, 14;
	setp.le.s32 	%p174, %r234, %r238;
	or.pred  	%p175, %p172, %p173;
	or.b32  	%r1246, %r2070, %r238;
	or.b32  	%r1247, %r1246, %r2036;
	setp.lt.s32 	%p176, %r1247, 0;
	or.pred  	%p177, %p176, %p175;
	or.pred  	%p178, %p174, %p177;
	@%p178 bra 	$L__BB1_155;
	bra.uni 	$L__BB1_154;

$L__BB1_155:
	add.s32 	%r2040, %r112, 2;
	add.s32 	%r1735, %r110, 3;
	st.local.v2.u32 	[%rd26], {%r1735, %r2040};
	add.s32 	%r1736, %r120, 14;
	st.local.v2.u32 	[%rd26+8], {%r1736, %r1244};
	st.local.v2.u32 	[%rd26+16], {%r1245, %r234};
	mov.u64 	%rd1078, $str$1;
	cvta.global.u64 	%rd1079, %rd1078;
	{ // callseq 362, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1079;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1248, [retval0+0];
	} // callseq 362
	bra.uni 	$L__BB1_156;

$L__BB1_154:
	ld.param.u32 	%r2099, [%rd27+88];
	ld.param.u64 	%rd4017, [%rd27];
	ld.param.u32 	%r2098, [%rd27+32];
	ld.param.u64 	%rd4016, [%rd27+112];
	ld.param.u32 	%r2097, [%rd27+144];
	ld.param.u64 	%rd4015, [%rd27+56];
	cvta.to.global.u64 	%rd1071, %rd4015;
	mul.wide.s32 	%rd1072, %r2097, %r238;
	add.s64 	%rd1062, %rd4016, %rd1072;
	// begin inline asm
	{ atom.add.f64 %fd4285,[%rd1062],%fd10686; }

	// end inline asm
	add.s64 	%rd1063, %rd1062, 8;
	// begin inline asm
	{ atom.add.f64 %fd4287,[%rd1063],%fd10685; }

	// end inline asm
	add.s64 	%rd1064, %rd1062, 16;
	// begin inline asm
	{ atom.add.f64 %fd4289,[%rd1064],%fd10684; }

	// end inline asm
	add.s64 	%rd1065, %rd1062, 24;
	// begin inline asm
	{ atom.add.f64 %fd4291,[%rd1065],%fd10674; }

	// end inline asm
	add.s64 	%rd1066, %rd1062, 32;
	// begin inline asm
	{ atom.add.f64 %fd4293,[%rd1066],%fd10673; }

	// end inline asm
	add.s64 	%rd1067, %rd1062, 40;
	// begin inline asm
	{ atom.add.f64 %fd4295,[%rd1067],%fd10672; }

	// end inline asm
	add.s64 	%rd1068, %rd1062, 48;
	// begin inline asm
	{ atom.add.f64 %fd4297,[%rd1068],%fd10662; }

	// end inline asm
	add.s64 	%rd1069, %rd1062, 56;
	// begin inline asm
	{ atom.add.f64 %fd4299,[%rd1069],%fd10661; }

	// end inline asm
	add.s64 	%rd1070, %rd1062, 64;
	// begin inline asm
	{ atom.add.f64 %fd4301,[%rd1070],%fd10660; }

	// end inline asm
	mul.wide.s32 	%rd1073, %r2098, %r238;
	cvta.to.global.u64 	%rd1074, %rd4017;
	add.s64 	%rd1075, %rd1074, %rd1073;
	mul.wide.s32 	%rd1076, %r2099, %r238;
	add.s64 	%rd1077, %rd1071, %rd1076;
	add.s32 	%r1733, %r110, 3;
	st.global.u32 	[%rd1075], %r1733;
	add.s32 	%r1734, %r112, 2;
	st.global.u32 	[%rd1077], %r1734;

$L__BB1_156:
	add.s32 	%r2071, %r110, 3;
	add.s32 	%r2037, %r112, 3;
	ld.param.u32 	%r242, [%rd27+172];
	ld.param.v2.u32 	{%r1249, %r1250}, [%rd27+176];
	setp.le.s32 	%p179, %r1249, %r2071;
	setp.le.s32 	%p180, %r1250, %r2037;
	add.s32 	%r246, %r120, 15;
	setp.le.s32 	%p181, %r242, %r246;
	or.pred  	%p182, %p179, %p180;
	or.b32  	%r1251, %r2071, %r246;
	or.b32  	%r1252, %r1251, %r2037;
	setp.lt.s32 	%p183, %r1252, 0;
	or.pred  	%p184, %p183, %p182;
	or.pred  	%p185, %p181, %p184;
	@%p185 bra 	$L__BB1_158;
	bra.uni 	$L__BB1_157;

$L__BB1_158:
	add.s32 	%r2039, %r112, 3;
	add.s32 	%r1739, %r110, 3;
	st.local.v2.u32 	[%rd26], {%r1739, %r2039};
	add.s32 	%r1740, %r120, 15;
	st.local.v2.u32 	[%rd26+8], {%r1740, %r1249};
	st.local.v2.u32 	[%rd26+16], {%r1250, %r242};
	mov.u64 	%rd1097, $str$1;
	cvta.global.u64 	%rd1098, %rd1097;
	{ // callseq 363, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1098;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1253, [retval0+0];
	} // callseq 363
	bra.uni 	$L__BB1_159;

$L__BB1_157:
	ld.param.u32 	%r2102, [%rd27+88];
	ld.param.u64 	%rd4020, [%rd27];
	ld.param.u32 	%r2101, [%rd27+32];
	ld.param.u64 	%rd4019, [%rd27+112];
	ld.param.u32 	%r2100, [%rd27+144];
	ld.param.u64 	%rd4018, [%rd27+56];
	cvta.to.global.u64 	%rd1090, %rd4018;
	mul.wide.s32 	%rd1091, %r2100, %r246;
	add.s64 	%rd1081, %rd4019, %rd1091;
	// begin inline asm
	{ atom.add.f64 %fd4303,[%rd1081],%fd10683; }

	// end inline asm
	add.s64 	%rd1082, %rd1081, 8;
	// begin inline asm
	{ atom.add.f64 %fd4305,[%rd1082],%fd10682; }

	// end inline asm
	add.s64 	%rd1083, %rd1081, 16;
	// begin inline asm
	{ atom.add.f64 %fd4307,[%rd1083],%fd10681; }

	// end inline asm
	add.s64 	%rd1084, %rd1081, 24;
	// begin inline asm
	{ atom.add.f64 %fd4309,[%rd1084],%fd10671; }

	// end inline asm
	add.s64 	%rd1085, %rd1081, 32;
	// begin inline asm
	{ atom.add.f64 %fd4311,[%rd1085],%fd10670; }

	// end inline asm
	add.s64 	%rd1086, %rd1081, 40;
	// begin inline asm
	{ atom.add.f64 %fd4313,[%rd1086],%fd10669; }

	// end inline asm
	add.s64 	%rd1087, %rd1081, 48;
	// begin inline asm
	{ atom.add.f64 %fd4315,[%rd1087],%fd10659; }

	// end inline asm
	add.s64 	%rd1088, %rd1081, 56;
	// begin inline asm
	{ atom.add.f64 %fd4317,[%rd1088],%fd10658; }

	// end inline asm
	add.s64 	%rd1089, %rd1081, 64;
	// begin inline asm
	{ atom.add.f64 %fd4319,[%rd1089],%fd10657; }

	// end inline asm
	mul.wide.s32 	%rd1092, %r2101, %r246;
	cvta.to.global.u64 	%rd1093, %rd4020;
	add.s64 	%rd1094, %rd1093, %rd1092;
	mul.wide.s32 	%rd1095, %r2102, %r246;
	add.s64 	%rd1096, %rd1090, %rd1095;
	add.s32 	%r1737, %r110, 3;
	st.global.u32 	[%rd1094], %r1737;
	add.s32 	%r1738, %r112, 3;
	st.global.u32 	[%rd1096], %r1738;

$L__BB1_159:
	add.s32 	%r1254, %r10, %r1050;
	shl.b32 	%r247, %r1254, 4;
	ld.global.u32 	%r2259, [%rd47];
	shl.b32 	%r249, %r2259, 2;
	ld.global.u32 	%r2260, [%rd30];
	shl.b32 	%r251, %r2260, 2;
	ld.param.u32 	%r255, [%rd27+172];
	ld.param.v2.u32 	{%r1255, %r1256}, [%rd27+176];
	setp.le.s32 	%p186, %r1255, %r249;
	setp.le.s32 	%p187, %r1256, %r251;
	setp.le.s32 	%p188, %r255, %r247;
	or.pred  	%p189, %p186, %p187;
	or.b32  	%r1257, %r249, %r247;
	or.b32  	%r1258, %r1257, %r251;
	setp.lt.s32 	%p190, %r1258, 0;
	or.pred  	%p191, %p190, %p189;
	or.pred  	%p192, %p188, %p191;
	@%p192 bra 	$L__BB1_161;
	bra.uni 	$L__BB1_160;

$L__BB1_161:
	st.local.v2.u32 	[%rd26], {%r249, %r251};
	st.local.v2.u32 	[%rd26+8], {%r247, %r1255};
	st.local.v2.u32 	[%rd26+16], {%r1256, %r255};
	mov.u64 	%rd1116, $str$1;
	cvta.global.u64 	%rd1117, %rd1116;
	{ // callseq 364, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1117;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1259, [retval0+0];
	} // callseq 364
	bra.uni 	$L__BB1_162;

$L__BB1_160:
	ld.param.u32 	%r2105, [%rd27+88];
	ld.param.u64 	%rd4023, [%rd27];
	ld.param.u32 	%r2104, [%rd27+32];
	ld.param.u64 	%rd4022, [%rd27+112];
	ld.param.u32 	%r2103, [%rd27+144];
	ld.param.u64 	%rd4021, [%rd27+56];
	cvta.to.global.u64 	%rd1109, %rd4021;
	mul.wide.s32 	%rd1110, %r2103, %r247;
	add.s64 	%rd1100, %rd4022, %rd1110;
	// begin inline asm
	{ atom.add.f64 %fd4321,[%rd1100],%fd10944; }

	// end inline asm
	add.s64 	%rd1101, %rd1100, 8;
	// begin inline asm
	{ atom.add.f64 %fd4323,[%rd1101],%fd10943; }

	// end inline asm
	add.s64 	%rd1102, %rd1100, 16;
	// begin inline asm
	{ atom.add.f64 %fd4325,[%rd1102],%fd10942; }

	// end inline asm
	add.s64 	%rd1103, %rd1100, 24;
	// begin inline asm
	{ atom.add.f64 %fd4327,[%rd1103],%fd10932; }

	// end inline asm
	add.s64 	%rd1104, %rd1100, 32;
	// begin inline asm
	{ atom.add.f64 %fd4329,[%rd1104],%fd10931; }

	// end inline asm
	add.s64 	%rd1105, %rd1100, 40;
	// begin inline asm
	{ atom.add.f64 %fd4331,[%rd1105],%fd10930; }

	// end inline asm
	add.s64 	%rd1106, %rd1100, 48;
	// begin inline asm
	{ atom.add.f64 %fd4333,[%rd1106],%fd10920; }

	// end inline asm
	add.s64 	%rd1107, %rd1100, 56;
	// begin inline asm
	{ atom.add.f64 %fd4335,[%rd1107],%fd10919; }

	// end inline asm
	add.s64 	%rd1108, %rd1100, 64;
	// begin inline asm
	{ atom.add.f64 %fd4337,[%rd1108],%fd10918; }

	// end inline asm
	mul.wide.s32 	%rd1111, %r2104, %r247;
	cvta.to.global.u64 	%rd1112, %rd4023;
	add.s64 	%rd1113, %rd1112, %rd1111;
	mul.wide.s32 	%rd1114, %r2105, %r247;
	add.s64 	%rd1115, %rd1109, %rd1114;
	st.global.u32 	[%rd1113], %r249;
	st.global.u32 	[%rd1115], %r251;

$L__BB1_162:
	ld.param.u32 	%r262, [%rd27+172];
	ld.param.v2.u32 	{%r1260, %r1261}, [%rd27+176];
	setp.le.s32 	%p193, %r1260, %r249;
	add.s32 	%r266, %r251, 1;
	setp.le.s32 	%p194, %r1261, %r266;
	add.s32 	%r267, %r247, 1;
	setp.le.s32 	%p195, %r262, %r267;
	or.pred  	%p196, %p193, %p194;
	or.b32  	%r1262, %r249, %r267;
	or.b32  	%r1263, %r1262, %r266;
	setp.lt.s32 	%p197, %r1263, 0;
	or.pred  	%p198, %p197, %p196;
	or.pred  	%p199, %p195, %p198;
	@%p199 bra 	$L__BB1_164;
	bra.uni 	$L__BB1_163;

$L__BB1_164:
	add.s32 	%r1742, %r251, 1;
	st.local.v2.u32 	[%rd26], {%r249, %r1742};
	add.s32 	%r1743, %r247, 1;
	st.local.v2.u32 	[%rd26+8], {%r1743, %r1260};
	st.local.v2.u32 	[%rd26+16], {%r1261, %r262};
	mov.u64 	%rd1135, $str$1;
	cvta.global.u64 	%rd1136, %rd1135;
	{ // callseq 365, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1136;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1264, [retval0+0];
	} // callseq 365
	bra.uni 	$L__BB1_165;

$L__BB1_163:
	ld.param.u32 	%r2111, [%rd27+88];
	ld.param.u64 	%rd4026, [%rd27];
	ld.param.u32 	%r2110, [%rd27+32];
	ld.param.u64 	%rd4025, [%rd27+112];
	ld.param.u32 	%r2109, [%rd27+144];
	ld.param.u64 	%rd4024, [%rd27+56];
	cvta.to.global.u64 	%rd1128, %rd4024;
	mul.wide.s32 	%rd1129, %r2109, %r267;
	add.s64 	%rd1119, %rd4025, %rd1129;
	// begin inline asm
	{ atom.add.f64 %fd4339,[%rd1119],%fd10941; }

	// end inline asm
	add.s64 	%rd1120, %rd1119, 8;
	// begin inline asm
	{ atom.add.f64 %fd4341,[%rd1120],%fd10940; }

	// end inline asm
	add.s64 	%rd1121, %rd1119, 16;
	// begin inline asm
	{ atom.add.f64 %fd4343,[%rd1121],%fd10939; }

	// end inline asm
	add.s64 	%rd1122, %rd1119, 24;
	// begin inline asm
	{ atom.add.f64 %fd4345,[%rd1122],%fd10929; }

	// end inline asm
	add.s64 	%rd1123, %rd1119, 32;
	// begin inline asm
	{ atom.add.f64 %fd4347,[%rd1123],%fd10928; }

	// end inline asm
	add.s64 	%rd1124, %rd1119, 40;
	// begin inline asm
	{ atom.add.f64 %fd4349,[%rd1124],%fd10927; }

	// end inline asm
	add.s64 	%rd1125, %rd1119, 48;
	// begin inline asm
	{ atom.add.f64 %fd4351,[%rd1125],%fd10917; }

	// end inline asm
	add.s64 	%rd1126, %rd1119, 56;
	// begin inline asm
	{ atom.add.f64 %fd4353,[%rd1126],%fd10916; }

	// end inline asm
	add.s64 	%rd1127, %rd1119, 64;
	// begin inline asm
	{ atom.add.f64 %fd4355,[%rd1127],%fd10915; }

	// end inline asm
	mul.wide.s32 	%rd1130, %r2110, %r267;
	cvta.to.global.u64 	%rd1131, %rd4026;
	add.s64 	%rd1132, %rd1131, %rd1130;
	mul.wide.s32 	%rd1133, %r2111, %r267;
	add.s64 	%rd1134, %rd1128, %rd1133;
	st.global.u32 	[%rd1132], %r249;
	add.s32 	%r1741, %r251, 1;
	st.global.u32 	[%rd1134], %r1741;

$L__BB1_165:
	ld.param.u32 	%r271, [%rd27+172];
	ld.param.v2.u32 	{%r1265, %r1266}, [%rd27+176];
	setp.le.s32 	%p200, %r1265, %r249;
	add.s32 	%r275, %r251, 2;
	setp.le.s32 	%p201, %r1266, %r275;
	add.s32 	%r276, %r247, 2;
	setp.le.s32 	%p202, %r271, %r276;
	or.pred  	%p203, %p200, %p201;
	or.b32  	%r1267, %r249, %r276;
	or.b32  	%r1268, %r1267, %r275;
	setp.lt.s32 	%p204, %r1268, 0;
	or.pred  	%p205, %p204, %p203;
	or.pred  	%p206, %p202, %p205;
	@%p206 bra 	$L__BB1_167;
	bra.uni 	$L__BB1_166;

$L__BB1_167:
	add.s32 	%r1745, %r251, 2;
	st.local.v2.u32 	[%rd26], {%r249, %r1745};
	add.s32 	%r1746, %r247, 2;
	st.local.v2.u32 	[%rd26+8], {%r1746, %r1265};
	st.local.v2.u32 	[%rd26+16], {%r1266, %r271};
	mov.u64 	%rd1154, $str$1;
	cvta.global.u64 	%rd1155, %rd1154;
	{ // callseq 366, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1155;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1269, [retval0+0];
	} // callseq 366
	bra.uni 	$L__BB1_168;

$L__BB1_166:
	ld.param.u32 	%r2120, [%rd27+88];
	ld.param.u64 	%rd4029, [%rd27];
	ld.param.u32 	%r2119, [%rd27+32];
	ld.param.u64 	%rd4028, [%rd27+112];
	ld.param.u32 	%r2118, [%rd27+144];
	ld.param.u64 	%rd4027, [%rd27+56];
	cvta.to.global.u64 	%rd1147, %rd4027;
	mul.wide.s32 	%rd1148, %r2118, %r276;
	add.s64 	%rd1138, %rd4028, %rd1148;
	// begin inline asm
	{ atom.add.f64 %fd4357,[%rd1138],%fd10938; }

	// end inline asm
	add.s64 	%rd1139, %rd1138, 8;
	// begin inline asm
	{ atom.add.f64 %fd4359,[%rd1139],%fd10937; }

	// end inline asm
	add.s64 	%rd1140, %rd1138, 16;
	// begin inline asm
	{ atom.add.f64 %fd4361,[%rd1140],%fd10936; }

	// end inline asm
	add.s64 	%rd1141, %rd1138, 24;
	// begin inline asm
	{ atom.add.f64 %fd4363,[%rd1141],%fd10926; }

	// end inline asm
	add.s64 	%rd1142, %rd1138, 32;
	// begin inline asm
	{ atom.add.f64 %fd4365,[%rd1142],%fd10925; }

	// end inline asm
	add.s64 	%rd1143, %rd1138, 40;
	// begin inline asm
	{ atom.add.f64 %fd4367,[%rd1143],%fd10924; }

	// end inline asm
	add.s64 	%rd1144, %rd1138, 48;
	// begin inline asm
	{ atom.add.f64 %fd4369,[%rd1144],%fd10914; }

	// end inline asm
	add.s64 	%rd1145, %rd1138, 56;
	// begin inline asm
	{ atom.add.f64 %fd4371,[%rd1145],%fd10913; }

	// end inline asm
	add.s64 	%rd1146, %rd1138, 64;
	// begin inline asm
	{ atom.add.f64 %fd4373,[%rd1146],%fd10912; }

	// end inline asm
	mul.wide.s32 	%rd1149, %r2119, %r276;
	cvta.to.global.u64 	%rd1150, %rd4029;
	add.s64 	%rd1151, %rd1150, %rd1149;
	mul.wide.s32 	%rd1152, %r2120, %r276;
	add.s64 	%rd1153, %rd1147, %rd1152;
	st.global.u32 	[%rd1151], %r249;
	add.s32 	%r1744, %r251, 2;
	st.global.u32 	[%rd1153], %r1744;

$L__BB1_168:
	ld.param.u32 	%r280, [%rd27+172];
	ld.param.v2.u32 	{%r1270, %r1271}, [%rd27+176];
	setp.le.s32 	%p207, %r1270, %r249;
	add.s32 	%r284, %r251, 3;
	setp.le.s32 	%p208, %r1271, %r284;
	add.s32 	%r285, %r247, 3;
	setp.le.s32 	%p209, %r280, %r285;
	or.pred  	%p210, %p207, %p208;
	or.b32  	%r1272, %r249, %r285;
	or.b32  	%r1273, %r1272, %r284;
	setp.lt.s32 	%p211, %r1273, 0;
	or.pred  	%p212, %p211, %p210;
	or.pred  	%p213, %p209, %p212;
	@%p213 bra 	$L__BB1_170;
	bra.uni 	$L__BB1_169;

$L__BB1_170:
	add.s32 	%r1748, %r251, 3;
	st.local.v2.u32 	[%rd26], {%r249, %r1748};
	add.s32 	%r1749, %r247, 3;
	st.local.v2.u32 	[%rd26+8], {%r1749, %r1270};
	st.local.v2.u32 	[%rd26+16], {%r1271, %r280};
	mov.u64 	%rd1173, $str$1;
	cvta.global.u64 	%rd1174, %rd1173;
	{ // callseq 367, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1174;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1274, [retval0+0];
	} // callseq 367
	bra.uni 	$L__BB1_171;

$L__BB1_169:
	ld.param.u32 	%r2129, [%rd27+88];
	ld.param.u64 	%rd4032, [%rd27];
	ld.param.u32 	%r2128, [%rd27+32];
	ld.param.u64 	%rd4031, [%rd27+112];
	ld.param.u32 	%r2127, [%rd27+144];
	ld.param.u64 	%rd4030, [%rd27+56];
	cvta.to.global.u64 	%rd1166, %rd4030;
	mul.wide.s32 	%rd1167, %r2127, %r285;
	add.s64 	%rd1157, %rd4031, %rd1167;
	// begin inline asm
	{ atom.add.f64 %fd4375,[%rd1157],%fd10935; }

	// end inline asm
	add.s64 	%rd1158, %rd1157, 8;
	// begin inline asm
	{ atom.add.f64 %fd4377,[%rd1158],%fd10934; }

	// end inline asm
	add.s64 	%rd1159, %rd1157, 16;
	// begin inline asm
	{ atom.add.f64 %fd4379,[%rd1159],%fd10933; }

	// end inline asm
	add.s64 	%rd1160, %rd1157, 24;
	// begin inline asm
	{ atom.add.f64 %fd4381,[%rd1160],%fd10923; }

	// end inline asm
	add.s64 	%rd1161, %rd1157, 32;
	// begin inline asm
	{ atom.add.f64 %fd4383,[%rd1161],%fd10922; }

	// end inline asm
	add.s64 	%rd1162, %rd1157, 40;
	// begin inline asm
	{ atom.add.f64 %fd4385,[%rd1162],%fd10921; }

	// end inline asm
	add.s64 	%rd1163, %rd1157, 48;
	// begin inline asm
	{ atom.add.f64 %fd4387,[%rd1163],%fd10911; }

	// end inline asm
	add.s64 	%rd1164, %rd1157, 56;
	// begin inline asm
	{ atom.add.f64 %fd4389,[%rd1164],%fd10910; }

	// end inline asm
	add.s64 	%rd1165, %rd1157, 64;
	// begin inline asm
	{ atom.add.f64 %fd4391,[%rd1165],%fd10909; }

	// end inline asm
	mul.wide.s32 	%rd1168, %r2128, %r285;
	cvta.to.global.u64 	%rd1169, %rd4032;
	add.s64 	%rd1170, %rd1169, %rd1168;
	mul.wide.s32 	%rd1171, %r2129, %r285;
	add.s64 	%rd1172, %rd1166, %rd1171;
	st.global.u32 	[%rd1170], %r249;
	add.s32 	%r1747, %r251, 3;
	st.global.u32 	[%rd1172], %r1747;

$L__BB1_171:
	ld.param.u32 	%r289, [%rd27+172];
	ld.param.v2.u32 	{%r1275, %r1276}, [%rd27+176];
	add.s32 	%r293, %r249, 1;
	setp.le.s32 	%p214, %r1275, %r293;
	setp.le.s32 	%p215, %r1276, %r251;
	add.s32 	%r294, %r247, 4;
	setp.le.s32 	%p216, %r289, %r294;
	or.pred  	%p217, %p214, %p215;
	or.b32  	%r1277, %r251, %r294;
	or.b32  	%r1278, %r1277, %r293;
	setp.lt.s32 	%p218, %r1278, 0;
	or.pred  	%p219, %p218, %p217;
	or.pred  	%p220, %p216, %p219;
	@%p220 bra 	$L__BB1_173;
	bra.uni 	$L__BB1_172;

$L__BB1_173:
	add.s32 	%r1751, %r249, 1;
	st.local.v2.u32 	[%rd26], {%r1751, %r251};
	add.s32 	%r1752, %r247, 4;
	st.local.v2.u32 	[%rd26+8], {%r1752, %r1275};
	st.local.v2.u32 	[%rd26+16], {%r1276, %r289};
	mov.u64 	%rd1192, $str$1;
	cvta.global.u64 	%rd1193, %rd1192;
	{ // callseq 368, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1193;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1279, [retval0+0];
	} // callseq 368
	bra.uni 	$L__BB1_174;

$L__BB1_172:
	ld.param.u32 	%r2138, [%rd27+88];
	ld.param.u64 	%rd4035, [%rd27];
	ld.param.u32 	%r2137, [%rd27+32];
	ld.param.u64 	%rd4034, [%rd27+112];
	ld.param.u32 	%r2136, [%rd27+144];
	ld.param.u64 	%rd4033, [%rd27+56];
	cvta.to.global.u64 	%rd1185, %rd4033;
	mul.wide.s32 	%rd1186, %r2136, %r294;
	add.s64 	%rd1176, %rd4034, %rd1186;
	// begin inline asm
	{ atom.add.f64 %fd4393,[%rd1176],%fd10908; }

	// end inline asm
	add.s64 	%rd1177, %rd1176, 8;
	// begin inline asm
	{ atom.add.f64 %fd4395,[%rd1177],%fd10907; }

	// end inline asm
	add.s64 	%rd1178, %rd1176, 16;
	// begin inline asm
	{ atom.add.f64 %fd4397,[%rd1178],%fd10906; }

	// end inline asm
	add.s64 	%rd1179, %rd1176, 24;
	// begin inline asm
	{ atom.add.f64 %fd4399,[%rd1179],%fd10896; }

	// end inline asm
	add.s64 	%rd1180, %rd1176, 32;
	// begin inline asm
	{ atom.add.f64 %fd4401,[%rd1180],%fd10895; }

	// end inline asm
	add.s64 	%rd1181, %rd1176, 40;
	// begin inline asm
	{ atom.add.f64 %fd4403,[%rd1181],%fd10894; }

	// end inline asm
	add.s64 	%rd1182, %rd1176, 48;
	// begin inline asm
	{ atom.add.f64 %fd4405,[%rd1182],%fd10884; }

	// end inline asm
	add.s64 	%rd1183, %rd1176, 56;
	// begin inline asm
	{ atom.add.f64 %fd4407,[%rd1183],%fd10883; }

	// end inline asm
	add.s64 	%rd1184, %rd1176, 64;
	// begin inline asm
	{ atom.add.f64 %fd4409,[%rd1184],%fd10882; }

	// end inline asm
	mul.wide.s32 	%rd1187, %r2137, %r294;
	cvta.to.global.u64 	%rd1188, %rd4035;
	add.s64 	%rd1189, %rd1188, %rd1187;
	mul.wide.s32 	%rd1190, %r2138, %r294;
	add.s64 	%rd1191, %rd1185, %rd1190;
	add.s32 	%r1750, %r249, 1;
	st.global.u32 	[%rd1189], %r1750;
	st.global.u32 	[%rd1191], %r251;

$L__BB1_174:
	add.s32 	%r2139, %r249, 1;
	add.s32 	%r2112, %r251, 1;
	ld.param.u32 	%r298, [%rd27+172];
	ld.param.v2.u32 	{%r1280, %r1281}, [%rd27+176];
	setp.le.s32 	%p221, %r1280, %r2139;
	setp.le.s32 	%p222, %r1281, %r2112;
	add.s32 	%r302, %r247, 5;
	setp.le.s32 	%p223, %r298, %r302;
	or.pred  	%p224, %p221, %p222;
	or.b32  	%r1282, %r2139, %r302;
	or.b32  	%r1283, %r1282, %r2112;
	setp.lt.s32 	%p225, %r1283, 0;
	or.pred  	%p226, %p225, %p224;
	or.pred  	%p227, %p223, %p226;
	@%p227 bra 	$L__BB1_176;
	bra.uni 	$L__BB1_175;

$L__BB1_176:
	add.s32 	%r2117, %r251, 1;
	add.s32 	%r1755, %r249, 1;
	st.local.v2.u32 	[%rd26], {%r1755, %r2117};
	add.s32 	%r1756, %r247, 5;
	st.local.v2.u32 	[%rd26+8], {%r1756, %r1280};
	st.local.v2.u32 	[%rd26+16], {%r1281, %r298};
	mov.u64 	%rd1211, $str$1;
	cvta.global.u64 	%rd1212, %rd1211;
	{ // callseq 369, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1212;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1284, [retval0+0];
	} // callseq 369
	bra.uni 	$L__BB1_177;

$L__BB1_175:
	ld.param.u32 	%r2144, [%rd27+88];
	ld.param.u64 	%rd4038, [%rd27];
	ld.param.u32 	%r2143, [%rd27+32];
	ld.param.u64 	%rd4037, [%rd27+112];
	ld.param.u32 	%r2142, [%rd27+144];
	ld.param.u64 	%rd4036, [%rd27+56];
	cvta.to.global.u64 	%rd1204, %rd4036;
	mul.wide.s32 	%rd1205, %r2142, %r302;
	add.s64 	%rd1195, %rd4037, %rd1205;
	// begin inline asm
	{ atom.add.f64 %fd4411,[%rd1195],%fd10905; }

	// end inline asm
	add.s64 	%rd1196, %rd1195, 8;
	// begin inline asm
	{ atom.add.f64 %fd4413,[%rd1196],%fd10904; }

	// end inline asm
	add.s64 	%rd1197, %rd1195, 16;
	// begin inline asm
	{ atom.add.f64 %fd4415,[%rd1197],%fd10903; }

	// end inline asm
	add.s64 	%rd1198, %rd1195, 24;
	// begin inline asm
	{ atom.add.f64 %fd4417,[%rd1198],%fd10893; }

	// end inline asm
	add.s64 	%rd1199, %rd1195, 32;
	// begin inline asm
	{ atom.add.f64 %fd4419,[%rd1199],%fd10892; }

	// end inline asm
	add.s64 	%rd1200, %rd1195, 40;
	// begin inline asm
	{ atom.add.f64 %fd4421,[%rd1200],%fd10891; }

	// end inline asm
	add.s64 	%rd1201, %rd1195, 48;
	// begin inline asm
	{ atom.add.f64 %fd4423,[%rd1201],%fd10881; }

	// end inline asm
	add.s64 	%rd1202, %rd1195, 56;
	// begin inline asm
	{ atom.add.f64 %fd4425,[%rd1202],%fd10880; }

	// end inline asm
	add.s64 	%rd1203, %rd1195, 64;
	// begin inline asm
	{ atom.add.f64 %fd4427,[%rd1203],%fd10879; }

	// end inline asm
	mul.wide.s32 	%rd1206, %r2143, %r302;
	cvta.to.global.u64 	%rd1207, %rd4038;
	add.s64 	%rd1208, %rd1207, %rd1206;
	mul.wide.s32 	%rd1209, %r2144, %r302;
	add.s64 	%rd1210, %rd1204, %rd1209;
	add.s32 	%r1753, %r249, 1;
	st.global.u32 	[%rd1208], %r1753;
	add.s32 	%r1754, %r251, 1;
	st.global.u32 	[%rd1210], %r1754;

$L__BB1_177:
	add.s32 	%r2140, %r249, 1;
	add.s32 	%r2121, %r251, 2;
	ld.param.u32 	%r306, [%rd27+172];
	ld.param.v2.u32 	{%r1285, %r1286}, [%rd27+176];
	setp.le.s32 	%p228, %r1285, %r2140;
	setp.le.s32 	%p229, %r1286, %r2121;
	add.s32 	%r310, %r247, 6;
	setp.le.s32 	%p230, %r306, %r310;
	or.pred  	%p231, %p228, %p229;
	or.b32  	%r1287, %r2140, %r310;
	or.b32  	%r1288, %r1287, %r2121;
	setp.lt.s32 	%p232, %r1288, 0;
	or.pred  	%p233, %p232, %p231;
	or.pred  	%p234, %p230, %p233;
	@%p234 bra 	$L__BB1_179;
	bra.uni 	$L__BB1_178;

$L__BB1_179:
	add.s32 	%r2126, %r251, 2;
	add.s32 	%r1759, %r249, 1;
	st.local.v2.u32 	[%rd26], {%r1759, %r2126};
	add.s32 	%r1760, %r247, 6;
	st.local.v2.u32 	[%rd26+8], {%r1760, %r1285};
	st.local.v2.u32 	[%rd26+16], {%r1286, %r306};
	mov.u64 	%rd1230, $str$1;
	cvta.global.u64 	%rd1231, %rd1230;
	{ // callseq 370, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1231;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1289, [retval0+0];
	} // callseq 370
	bra.uni 	$L__BB1_180;

$L__BB1_178:
	ld.param.u32 	%r2147, [%rd27+88];
	ld.param.u64 	%rd4041, [%rd27];
	ld.param.u32 	%r2146, [%rd27+32];
	ld.param.u64 	%rd4040, [%rd27+112];
	ld.param.u32 	%r2145, [%rd27+144];
	ld.param.u64 	%rd4039, [%rd27+56];
	cvta.to.global.u64 	%rd1223, %rd4039;
	mul.wide.s32 	%rd1224, %r2145, %r310;
	add.s64 	%rd1214, %rd4040, %rd1224;
	// begin inline asm
	{ atom.add.f64 %fd4429,[%rd1214],%fd10902; }

	// end inline asm
	add.s64 	%rd1215, %rd1214, 8;
	// begin inline asm
	{ atom.add.f64 %fd4431,[%rd1215],%fd10901; }

	// end inline asm
	add.s64 	%rd1216, %rd1214, 16;
	// begin inline asm
	{ atom.add.f64 %fd4433,[%rd1216],%fd10900; }

	// end inline asm
	add.s64 	%rd1217, %rd1214, 24;
	// begin inline asm
	{ atom.add.f64 %fd4435,[%rd1217],%fd10890; }

	// end inline asm
	add.s64 	%rd1218, %rd1214, 32;
	// begin inline asm
	{ atom.add.f64 %fd4437,[%rd1218],%fd10889; }

	// end inline asm
	add.s64 	%rd1219, %rd1214, 40;
	// begin inline asm
	{ atom.add.f64 %fd4439,[%rd1219],%fd10888; }

	// end inline asm
	add.s64 	%rd1220, %rd1214, 48;
	// begin inline asm
	{ atom.add.f64 %fd4441,[%rd1220],%fd10878; }

	// end inline asm
	add.s64 	%rd1221, %rd1214, 56;
	// begin inline asm
	{ atom.add.f64 %fd4443,[%rd1221],%fd10877; }

	// end inline asm
	add.s64 	%rd1222, %rd1214, 64;
	// begin inline asm
	{ atom.add.f64 %fd4445,[%rd1222],%fd10876; }

	// end inline asm
	mul.wide.s32 	%rd1225, %r2146, %r310;
	cvta.to.global.u64 	%rd1226, %rd4041;
	add.s64 	%rd1227, %rd1226, %rd1225;
	mul.wide.s32 	%rd1228, %r2147, %r310;
	add.s64 	%rd1229, %rd1223, %rd1228;
	add.s32 	%r1757, %r249, 1;
	st.global.u32 	[%rd1227], %r1757;
	add.s32 	%r1758, %r251, 2;
	st.global.u32 	[%rd1229], %r1758;

$L__BB1_180:
	add.s32 	%r2141, %r249, 1;
	add.s32 	%r2130, %r251, 3;
	ld.param.u32 	%r314, [%rd27+172];
	ld.param.v2.u32 	{%r1290, %r1291}, [%rd27+176];
	setp.le.s32 	%p235, %r1290, %r2141;
	setp.le.s32 	%p236, %r1291, %r2130;
	add.s32 	%r318, %r247, 7;
	setp.le.s32 	%p237, %r314, %r318;
	or.pred  	%p238, %p235, %p236;
	or.b32  	%r1292, %r2141, %r318;
	or.b32  	%r1293, %r1292, %r2130;
	setp.lt.s32 	%p239, %r1293, 0;
	or.pred  	%p240, %p239, %p238;
	or.pred  	%p241, %p237, %p240;
	@%p241 bra 	$L__BB1_182;
	bra.uni 	$L__BB1_181;

$L__BB1_182:
	add.s32 	%r2135, %r251, 3;
	add.s32 	%r1763, %r249, 1;
	st.local.v2.u32 	[%rd26], {%r1763, %r2135};
	add.s32 	%r1764, %r247, 7;
	st.local.v2.u32 	[%rd26+8], {%r1764, %r1290};
	st.local.v2.u32 	[%rd26+16], {%r1291, %r314};
	mov.u64 	%rd1249, $str$1;
	cvta.global.u64 	%rd1250, %rd1249;
	{ // callseq 371, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1250;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1294, [retval0+0];
	} // callseq 371
	bra.uni 	$L__BB1_183;

$L__BB1_181:
	ld.param.u32 	%r2150, [%rd27+88];
	ld.param.u64 	%rd4044, [%rd27];
	ld.param.u32 	%r2149, [%rd27+32];
	ld.param.u64 	%rd4043, [%rd27+112];
	ld.param.u32 	%r2148, [%rd27+144];
	ld.param.u64 	%rd4042, [%rd27+56];
	cvta.to.global.u64 	%rd1242, %rd4042;
	mul.wide.s32 	%rd1243, %r2148, %r318;
	add.s64 	%rd1233, %rd4043, %rd1243;
	// begin inline asm
	{ atom.add.f64 %fd4447,[%rd1233],%fd10899; }

	// end inline asm
	add.s64 	%rd1234, %rd1233, 8;
	// begin inline asm
	{ atom.add.f64 %fd4449,[%rd1234],%fd10898; }

	// end inline asm
	add.s64 	%rd1235, %rd1233, 16;
	// begin inline asm
	{ atom.add.f64 %fd4451,[%rd1235],%fd10897; }

	// end inline asm
	add.s64 	%rd1236, %rd1233, 24;
	// begin inline asm
	{ atom.add.f64 %fd4453,[%rd1236],%fd10887; }

	// end inline asm
	add.s64 	%rd1237, %rd1233, 32;
	// begin inline asm
	{ atom.add.f64 %fd4455,[%rd1237],%fd10886; }

	// end inline asm
	add.s64 	%rd1238, %rd1233, 40;
	// begin inline asm
	{ atom.add.f64 %fd4457,[%rd1238],%fd10885; }

	// end inline asm
	add.s64 	%rd1239, %rd1233, 48;
	// begin inline asm
	{ atom.add.f64 %fd4459,[%rd1239],%fd10875; }

	// end inline asm
	add.s64 	%rd1240, %rd1233, 56;
	// begin inline asm
	{ atom.add.f64 %fd4461,[%rd1240],%fd10874; }

	// end inline asm
	add.s64 	%rd1241, %rd1233, 64;
	// begin inline asm
	{ atom.add.f64 %fd4463,[%rd1241],%fd10873; }

	// end inline asm
	mul.wide.s32 	%rd1244, %r2149, %r318;
	cvta.to.global.u64 	%rd1245, %rd4044;
	add.s64 	%rd1246, %rd1245, %rd1244;
	mul.wide.s32 	%rd1247, %r2150, %r318;
	add.s64 	%rd1248, %rd1242, %rd1247;
	add.s32 	%r1761, %r249, 1;
	st.global.u32 	[%rd1246], %r1761;
	add.s32 	%r1762, %r251, 3;
	st.global.u32 	[%rd1248], %r1762;

$L__BB1_183:
	ld.param.u32 	%r322, [%rd27+172];
	ld.param.v2.u32 	{%r1295, %r1296}, [%rd27+176];
	add.s32 	%r326, %r249, 2;
	setp.le.s32 	%p242, %r1295, %r326;
	setp.le.s32 	%p243, %r1296, %r251;
	add.s32 	%r327, %r247, 8;
	setp.le.s32 	%p244, %r322, %r327;
	or.pred  	%p245, %p242, %p243;
	or.b32  	%r1297, %r251, %r327;
	or.b32  	%r1298, %r1297, %r326;
	setp.lt.s32 	%p246, %r1298, 0;
	or.pred  	%p247, %p246, %p245;
	or.pred  	%p248, %p244, %p247;
	@%p248 bra 	$L__BB1_185;
	bra.uni 	$L__BB1_184;

$L__BB1_185:
	add.s32 	%r1766, %r249, 2;
	st.local.v2.u32 	[%rd26], {%r1766, %r251};
	add.s32 	%r1767, %r247, 8;
	st.local.v2.u32 	[%rd26+8], {%r1767, %r1295};
	st.local.v2.u32 	[%rd26+16], {%r1296, %r322};
	mov.u64 	%rd1268, $str$1;
	cvta.global.u64 	%rd1269, %rd1268;
	{ // callseq 372, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1269;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1299, [retval0+0];
	} // callseq 372
	bra.uni 	$L__BB1_186;

$L__BB1_184:
	ld.param.u32 	%r2153, [%rd27+88];
	ld.param.u64 	%rd4047, [%rd27];
	ld.param.u32 	%r2152, [%rd27+32];
	ld.param.u64 	%rd4046, [%rd27+112];
	ld.param.u32 	%r2151, [%rd27+144];
	ld.param.u64 	%rd4045, [%rd27+56];
	cvta.to.global.u64 	%rd1261, %rd4045;
	mul.wide.s32 	%rd1262, %r2151, %r327;
	add.s64 	%rd1252, %rd4046, %rd1262;
	// begin inline asm
	{ atom.add.f64 %fd4465,[%rd1252],%fd10872; }

	// end inline asm
	add.s64 	%rd1253, %rd1252, 8;
	// begin inline asm
	{ atom.add.f64 %fd4467,[%rd1253],%fd10871; }

	// end inline asm
	add.s64 	%rd1254, %rd1252, 16;
	// begin inline asm
	{ atom.add.f64 %fd4469,[%rd1254],%fd10870; }

	// end inline asm
	add.s64 	%rd1255, %rd1252, 24;
	// begin inline asm
	{ atom.add.f64 %fd4471,[%rd1255],%fd10860; }

	// end inline asm
	add.s64 	%rd1256, %rd1252, 32;
	// begin inline asm
	{ atom.add.f64 %fd4473,[%rd1256],%fd10859; }

	// end inline asm
	add.s64 	%rd1257, %rd1252, 40;
	// begin inline asm
	{ atom.add.f64 %fd4475,[%rd1257],%fd10858; }

	// end inline asm
	add.s64 	%rd1258, %rd1252, 48;
	// begin inline asm
	{ atom.add.f64 %fd4477,[%rd1258],%fd10848; }

	// end inline asm
	add.s64 	%rd1259, %rd1252, 56;
	// begin inline asm
	{ atom.add.f64 %fd4479,[%rd1259],%fd10847; }

	// end inline asm
	add.s64 	%rd1260, %rd1252, 64;
	// begin inline asm
	{ atom.add.f64 %fd4481,[%rd1260],%fd10846; }

	// end inline asm
	mul.wide.s32 	%rd1263, %r2152, %r327;
	cvta.to.global.u64 	%rd1264, %rd4047;
	add.s64 	%rd1265, %rd1264, %rd1263;
	mul.wide.s32 	%rd1266, %r2153, %r327;
	add.s64 	%rd1267, %rd1261, %rd1266;
	add.s32 	%r1765, %r249, 2;
	st.global.u32 	[%rd1265], %r1765;
	st.global.u32 	[%rd1267], %r251;

$L__BB1_186:
	add.s32 	%r2154, %r249, 2;
	add.s32 	%r2113, %r251, 1;
	ld.param.u32 	%r331, [%rd27+172];
	ld.param.v2.u32 	{%r1300, %r1301}, [%rd27+176];
	setp.le.s32 	%p249, %r1300, %r2154;
	setp.le.s32 	%p250, %r1301, %r2113;
	add.s32 	%r335, %r247, 9;
	setp.le.s32 	%p251, %r331, %r335;
	or.pred  	%p252, %p249, %p250;
	or.b32  	%r1302, %r2154, %r335;
	or.b32  	%r1303, %r1302, %r2113;
	setp.lt.s32 	%p253, %r1303, 0;
	or.pred  	%p254, %p253, %p252;
	or.pred  	%p255, %p251, %p254;
	@%p255 bra 	$L__BB1_188;
	bra.uni 	$L__BB1_187;

$L__BB1_188:
	add.s32 	%r2116, %r251, 1;
	add.s32 	%r1770, %r249, 2;
	st.local.v2.u32 	[%rd26], {%r1770, %r2116};
	add.s32 	%r1771, %r247, 9;
	st.local.v2.u32 	[%rd26+8], {%r1771, %r1300};
	st.local.v2.u32 	[%rd26+16], {%r1301, %r331};
	mov.u64 	%rd1287, $str$1;
	cvta.global.u64 	%rd1288, %rd1287;
	{ // callseq 373, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1288;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1304, [retval0+0];
	} // callseq 373
	bra.uni 	$L__BB1_189;

$L__BB1_187:
	ld.param.u32 	%r2159, [%rd27+88];
	ld.param.u64 	%rd4050, [%rd27];
	ld.param.u32 	%r2158, [%rd27+32];
	ld.param.u64 	%rd4049, [%rd27+112];
	ld.param.u32 	%r2157, [%rd27+144];
	ld.param.u64 	%rd4048, [%rd27+56];
	cvta.to.global.u64 	%rd1280, %rd4048;
	mul.wide.s32 	%rd1281, %r2157, %r335;
	add.s64 	%rd1271, %rd4049, %rd1281;
	// begin inline asm
	{ atom.add.f64 %fd4483,[%rd1271],%fd10869; }

	// end inline asm
	add.s64 	%rd1272, %rd1271, 8;
	// begin inline asm
	{ atom.add.f64 %fd4485,[%rd1272],%fd10868; }

	// end inline asm
	add.s64 	%rd1273, %rd1271, 16;
	// begin inline asm
	{ atom.add.f64 %fd4487,[%rd1273],%fd10867; }

	// end inline asm
	add.s64 	%rd1274, %rd1271, 24;
	// begin inline asm
	{ atom.add.f64 %fd4489,[%rd1274],%fd10857; }

	// end inline asm
	add.s64 	%rd1275, %rd1271, 32;
	// begin inline asm
	{ atom.add.f64 %fd4491,[%rd1275],%fd10856; }

	// end inline asm
	add.s64 	%rd1276, %rd1271, 40;
	// begin inline asm
	{ atom.add.f64 %fd4493,[%rd1276],%fd10855; }

	// end inline asm
	add.s64 	%rd1277, %rd1271, 48;
	// begin inline asm
	{ atom.add.f64 %fd4495,[%rd1277],%fd10845; }

	// end inline asm
	add.s64 	%rd1278, %rd1271, 56;
	// begin inline asm
	{ atom.add.f64 %fd4497,[%rd1278],%fd10844; }

	// end inline asm
	add.s64 	%rd1279, %rd1271, 64;
	// begin inline asm
	{ atom.add.f64 %fd4499,[%rd1279],%fd10843; }

	// end inline asm
	mul.wide.s32 	%rd1282, %r2158, %r335;
	cvta.to.global.u64 	%rd1283, %rd4050;
	add.s64 	%rd1284, %rd1283, %rd1282;
	mul.wide.s32 	%rd1285, %r2159, %r335;
	add.s64 	%rd1286, %rd1280, %rd1285;
	add.s32 	%r1768, %r249, 2;
	st.global.u32 	[%rd1284], %r1768;
	add.s32 	%r1769, %r251, 1;
	st.global.u32 	[%rd1286], %r1769;

$L__BB1_189:
	add.s32 	%r2155, %r249, 2;
	add.s32 	%r2122, %r251, 2;
	ld.param.u32 	%r339, [%rd27+172];
	ld.param.v2.u32 	{%r1305, %r1306}, [%rd27+176];
	setp.le.s32 	%p256, %r1305, %r2155;
	setp.le.s32 	%p257, %r1306, %r2122;
	add.s32 	%r343, %r247, 10;
	setp.le.s32 	%p258, %r339, %r343;
	or.pred  	%p259, %p256, %p257;
	or.b32  	%r1307, %r2155, %r343;
	or.b32  	%r1308, %r1307, %r2122;
	setp.lt.s32 	%p260, %r1308, 0;
	or.pred  	%p261, %p260, %p259;
	or.pred  	%p262, %p258, %p261;
	@%p262 bra 	$L__BB1_191;
	bra.uni 	$L__BB1_190;

$L__BB1_191:
	add.s32 	%r2125, %r251, 2;
	add.s32 	%r1774, %r249, 2;
	st.local.v2.u32 	[%rd26], {%r1774, %r2125};
	add.s32 	%r1775, %r247, 10;
	st.local.v2.u32 	[%rd26+8], {%r1775, %r1305};
	st.local.v2.u32 	[%rd26+16], {%r1306, %r339};
	mov.u64 	%rd1306, $str$1;
	cvta.global.u64 	%rd1307, %rd1306;
	{ // callseq 374, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1307;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1309, [retval0+0];
	} // callseq 374
	bra.uni 	$L__BB1_192;

$L__BB1_190:
	ld.param.u32 	%r2162, [%rd27+88];
	ld.param.u64 	%rd4053, [%rd27];
	ld.param.u32 	%r2161, [%rd27+32];
	ld.param.u64 	%rd4052, [%rd27+112];
	ld.param.u32 	%r2160, [%rd27+144];
	ld.param.u64 	%rd4051, [%rd27+56];
	cvta.to.global.u64 	%rd1299, %rd4051;
	mul.wide.s32 	%rd1300, %r2160, %r343;
	add.s64 	%rd1290, %rd4052, %rd1300;
	// begin inline asm
	{ atom.add.f64 %fd4501,[%rd1290],%fd10866; }

	// end inline asm
	add.s64 	%rd1291, %rd1290, 8;
	// begin inline asm
	{ atom.add.f64 %fd4503,[%rd1291],%fd10865; }

	// end inline asm
	add.s64 	%rd1292, %rd1290, 16;
	// begin inline asm
	{ atom.add.f64 %fd4505,[%rd1292],%fd10864; }

	// end inline asm
	add.s64 	%rd1293, %rd1290, 24;
	// begin inline asm
	{ atom.add.f64 %fd4507,[%rd1293],%fd10854; }

	// end inline asm
	add.s64 	%rd1294, %rd1290, 32;
	// begin inline asm
	{ atom.add.f64 %fd4509,[%rd1294],%fd10853; }

	// end inline asm
	add.s64 	%rd1295, %rd1290, 40;
	// begin inline asm
	{ atom.add.f64 %fd4511,[%rd1295],%fd10852; }

	// end inline asm
	add.s64 	%rd1296, %rd1290, 48;
	// begin inline asm
	{ atom.add.f64 %fd4513,[%rd1296],%fd10842; }

	// end inline asm
	add.s64 	%rd1297, %rd1290, 56;
	// begin inline asm
	{ atom.add.f64 %fd4515,[%rd1297],%fd10841; }

	// end inline asm
	add.s64 	%rd1298, %rd1290, 64;
	// begin inline asm
	{ atom.add.f64 %fd4517,[%rd1298],%fd10840; }

	// end inline asm
	mul.wide.s32 	%rd1301, %r2161, %r343;
	cvta.to.global.u64 	%rd1302, %rd4053;
	add.s64 	%rd1303, %rd1302, %rd1301;
	mul.wide.s32 	%rd1304, %r2162, %r343;
	add.s64 	%rd1305, %rd1299, %rd1304;
	add.s32 	%r1772, %r249, 2;
	st.global.u32 	[%rd1303], %r1772;
	add.s32 	%r1773, %r251, 2;
	st.global.u32 	[%rd1305], %r1773;

$L__BB1_192:
	add.s32 	%r2156, %r249, 2;
	add.s32 	%r2131, %r251, 3;
	ld.param.u32 	%r347, [%rd27+172];
	ld.param.v2.u32 	{%r1310, %r1311}, [%rd27+176];
	setp.le.s32 	%p263, %r1310, %r2156;
	setp.le.s32 	%p264, %r1311, %r2131;
	add.s32 	%r351, %r247, 11;
	setp.le.s32 	%p265, %r347, %r351;
	or.pred  	%p266, %p263, %p264;
	or.b32  	%r1312, %r2156, %r351;
	or.b32  	%r1313, %r1312, %r2131;
	setp.lt.s32 	%p267, %r1313, 0;
	or.pred  	%p268, %p267, %p266;
	or.pred  	%p269, %p265, %p268;
	@%p269 bra 	$L__BB1_194;
	bra.uni 	$L__BB1_193;

$L__BB1_194:
	add.s32 	%r2134, %r251, 3;
	add.s32 	%r1778, %r249, 2;
	st.local.v2.u32 	[%rd26], {%r1778, %r2134};
	add.s32 	%r1779, %r247, 11;
	st.local.v2.u32 	[%rd26+8], {%r1779, %r1310};
	st.local.v2.u32 	[%rd26+16], {%r1311, %r347};
	mov.u64 	%rd1325, $str$1;
	cvta.global.u64 	%rd1326, %rd1325;
	{ // callseq 375, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1326;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1314, [retval0+0];
	} // callseq 375
	bra.uni 	$L__BB1_195;

$L__BB1_193:
	ld.param.u32 	%r2165, [%rd27+88];
	ld.param.u64 	%rd4056, [%rd27];
	ld.param.u32 	%r2164, [%rd27+32];
	ld.param.u64 	%rd4055, [%rd27+112];
	ld.param.u32 	%r2163, [%rd27+144];
	ld.param.u64 	%rd4054, [%rd27+56];
	cvta.to.global.u64 	%rd1318, %rd4054;
	mul.wide.s32 	%rd1319, %r2163, %r351;
	add.s64 	%rd1309, %rd4055, %rd1319;
	// begin inline asm
	{ atom.add.f64 %fd4519,[%rd1309],%fd10863; }

	// end inline asm
	add.s64 	%rd1310, %rd1309, 8;
	// begin inline asm
	{ atom.add.f64 %fd4521,[%rd1310],%fd10862; }

	// end inline asm
	add.s64 	%rd1311, %rd1309, 16;
	// begin inline asm
	{ atom.add.f64 %fd4523,[%rd1311],%fd10861; }

	// end inline asm
	add.s64 	%rd1312, %rd1309, 24;
	// begin inline asm
	{ atom.add.f64 %fd4525,[%rd1312],%fd10851; }

	// end inline asm
	add.s64 	%rd1313, %rd1309, 32;
	// begin inline asm
	{ atom.add.f64 %fd4527,[%rd1313],%fd10850; }

	// end inline asm
	add.s64 	%rd1314, %rd1309, 40;
	// begin inline asm
	{ atom.add.f64 %fd4529,[%rd1314],%fd10849; }

	// end inline asm
	add.s64 	%rd1315, %rd1309, 48;
	// begin inline asm
	{ atom.add.f64 %fd4531,[%rd1315],%fd10839; }

	// end inline asm
	add.s64 	%rd1316, %rd1309, 56;
	// begin inline asm
	{ atom.add.f64 %fd4533,[%rd1316],%fd10838; }

	// end inline asm
	add.s64 	%rd1317, %rd1309, 64;
	// begin inline asm
	{ atom.add.f64 %fd4535,[%rd1317],%fd10837; }

	// end inline asm
	mul.wide.s32 	%rd1320, %r2164, %r351;
	cvta.to.global.u64 	%rd1321, %rd4056;
	add.s64 	%rd1322, %rd1321, %rd1320;
	mul.wide.s32 	%rd1323, %r2165, %r351;
	add.s64 	%rd1324, %rd1318, %rd1323;
	add.s32 	%r1776, %r249, 2;
	st.global.u32 	[%rd1322], %r1776;
	add.s32 	%r1777, %r251, 3;
	st.global.u32 	[%rd1324], %r1777;

$L__BB1_195:
	ld.param.u32 	%r355, [%rd27+172];
	ld.param.v2.u32 	{%r1315, %r1316}, [%rd27+176];
	add.s32 	%r359, %r249, 3;
	setp.le.s32 	%p270, %r1315, %r359;
	setp.le.s32 	%p271, %r1316, %r251;
	add.s32 	%r360, %r247, 12;
	setp.le.s32 	%p272, %r355, %r360;
	or.pred  	%p273, %p270, %p271;
	or.b32  	%r1317, %r251, %r360;
	or.b32  	%r1318, %r1317, %r359;
	setp.lt.s32 	%p274, %r1318, 0;
	or.pred  	%p275, %p274, %p273;
	or.pred  	%p276, %p272, %p275;
	@%p276 bra 	$L__BB1_197;
	bra.uni 	$L__BB1_196;

$L__BB1_197:
	add.s32 	%r1781, %r249, 3;
	st.local.v2.u32 	[%rd26], {%r1781, %r251};
	add.s32 	%r1782, %r247, 12;
	st.local.v2.u32 	[%rd26+8], {%r1782, %r1315};
	st.local.v2.u32 	[%rd26+16], {%r1316, %r355};
	mov.u64 	%rd1344, $str$1;
	cvta.global.u64 	%rd1345, %rd1344;
	{ // callseq 376, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1345;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1319, [retval0+0];
	} // callseq 376
	bra.uni 	$L__BB1_198;

$L__BB1_196:
	ld.param.u32 	%r2168, [%rd27+88];
	ld.param.u64 	%rd4059, [%rd27];
	ld.param.u32 	%r2167, [%rd27+32];
	ld.param.u64 	%rd4058, [%rd27+112];
	ld.param.u32 	%r2166, [%rd27+144];
	ld.param.u64 	%rd4057, [%rd27+56];
	cvta.to.global.u64 	%rd1337, %rd4057;
	mul.wide.s32 	%rd1338, %r2166, %r360;
	add.s64 	%rd1328, %rd4058, %rd1338;
	// begin inline asm
	{ atom.add.f64 %fd4537,[%rd1328],%fd10836; }

	// end inline asm
	add.s64 	%rd1329, %rd1328, 8;
	// begin inline asm
	{ atom.add.f64 %fd4539,[%rd1329],%fd10835; }

	// end inline asm
	add.s64 	%rd1330, %rd1328, 16;
	// begin inline asm
	{ atom.add.f64 %fd4541,[%rd1330],%fd10834; }

	// end inline asm
	add.s64 	%rd1331, %rd1328, 24;
	// begin inline asm
	{ atom.add.f64 %fd4543,[%rd1331],%fd10824; }

	// end inline asm
	add.s64 	%rd1332, %rd1328, 32;
	// begin inline asm
	{ atom.add.f64 %fd4545,[%rd1332],%fd10823; }

	// end inline asm
	add.s64 	%rd1333, %rd1328, 40;
	// begin inline asm
	{ atom.add.f64 %fd4547,[%rd1333],%fd10822; }

	// end inline asm
	add.s64 	%rd1334, %rd1328, 48;
	// begin inline asm
	{ atom.add.f64 %fd4549,[%rd1334],%fd10812; }

	// end inline asm
	add.s64 	%rd1335, %rd1328, 56;
	// begin inline asm
	{ atom.add.f64 %fd4551,[%rd1335],%fd10811; }

	// end inline asm
	add.s64 	%rd1336, %rd1328, 64;
	// begin inline asm
	{ atom.add.f64 %fd4553,[%rd1336],%fd10810; }

	// end inline asm
	mul.wide.s32 	%rd1339, %r2167, %r360;
	cvta.to.global.u64 	%rd1340, %rd4059;
	add.s64 	%rd1341, %rd1340, %rd1339;
	mul.wide.s32 	%rd1342, %r2168, %r360;
	add.s64 	%rd1343, %rd1337, %rd1342;
	add.s32 	%r1780, %r249, 3;
	st.global.u32 	[%rd1341], %r1780;
	st.global.u32 	[%rd1343], %r251;

$L__BB1_198:
	add.s32 	%r2169, %r249, 3;
	add.s32 	%r2114, %r251, 1;
	ld.param.u32 	%r364, [%rd27+172];
	ld.param.v2.u32 	{%r1320, %r1321}, [%rd27+176];
	setp.le.s32 	%p277, %r1320, %r2169;
	setp.le.s32 	%p278, %r1321, %r2114;
	add.s32 	%r368, %r247, 13;
	setp.le.s32 	%p279, %r364, %r368;
	or.pred  	%p280, %p277, %p278;
	or.b32  	%r1322, %r2169, %r368;
	or.b32  	%r1323, %r1322, %r2114;
	setp.lt.s32 	%p281, %r1323, 0;
	or.pred  	%p282, %p281, %p280;
	or.pred  	%p283, %p279, %p282;
	@%p283 bra 	$L__BB1_200;
	bra.uni 	$L__BB1_199;

$L__BB1_200:
	add.s32 	%r2115, %r251, 1;
	add.s32 	%r1785, %r249, 3;
	st.local.v2.u32 	[%rd26], {%r1785, %r2115};
	add.s32 	%r1786, %r247, 13;
	st.local.v2.u32 	[%rd26+8], {%r1786, %r1320};
	st.local.v2.u32 	[%rd26+16], {%r1321, %r364};
	mov.u64 	%rd1363, $str$1;
	cvta.global.u64 	%rd1364, %rd1363;
	{ // callseq 377, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1364;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1324, [retval0+0];
	} // callseq 377
	bra.uni 	$L__BB1_201;

$L__BB1_199:
	ld.param.u32 	%r2174, [%rd27+88];
	ld.param.u64 	%rd4062, [%rd27];
	ld.param.u32 	%r2173, [%rd27+32];
	ld.param.u64 	%rd4061, [%rd27+112];
	ld.param.u32 	%r2172, [%rd27+144];
	ld.param.u64 	%rd4060, [%rd27+56];
	cvta.to.global.u64 	%rd1356, %rd4060;
	mul.wide.s32 	%rd1357, %r2172, %r368;
	add.s64 	%rd1347, %rd4061, %rd1357;
	// begin inline asm
	{ atom.add.f64 %fd4555,[%rd1347],%fd10833; }

	// end inline asm
	add.s64 	%rd1348, %rd1347, 8;
	// begin inline asm
	{ atom.add.f64 %fd4557,[%rd1348],%fd10832; }

	// end inline asm
	add.s64 	%rd1349, %rd1347, 16;
	// begin inline asm
	{ atom.add.f64 %fd4559,[%rd1349],%fd10831; }

	// end inline asm
	add.s64 	%rd1350, %rd1347, 24;
	// begin inline asm
	{ atom.add.f64 %fd4561,[%rd1350],%fd10821; }

	// end inline asm
	add.s64 	%rd1351, %rd1347, 32;
	// begin inline asm
	{ atom.add.f64 %fd4563,[%rd1351],%fd10820; }

	// end inline asm
	add.s64 	%rd1352, %rd1347, 40;
	// begin inline asm
	{ atom.add.f64 %fd4565,[%rd1352],%fd10819; }

	// end inline asm
	add.s64 	%rd1353, %rd1347, 48;
	// begin inline asm
	{ atom.add.f64 %fd4567,[%rd1353],%fd10809; }

	// end inline asm
	add.s64 	%rd1354, %rd1347, 56;
	// begin inline asm
	{ atom.add.f64 %fd4569,[%rd1354],%fd10808; }

	// end inline asm
	add.s64 	%rd1355, %rd1347, 64;
	// begin inline asm
	{ atom.add.f64 %fd4571,[%rd1355],%fd10807; }

	// end inline asm
	mul.wide.s32 	%rd1358, %r2173, %r368;
	cvta.to.global.u64 	%rd1359, %rd4062;
	add.s64 	%rd1360, %rd1359, %rd1358;
	mul.wide.s32 	%rd1361, %r2174, %r368;
	add.s64 	%rd1362, %rd1356, %rd1361;
	add.s32 	%r1783, %r249, 3;
	st.global.u32 	[%rd1360], %r1783;
	add.s32 	%r1784, %r251, 1;
	st.global.u32 	[%rd1362], %r1784;

$L__BB1_201:
	add.s32 	%r2170, %r249, 3;
	add.s32 	%r2123, %r251, 2;
	ld.param.u32 	%r372, [%rd27+172];
	ld.param.v2.u32 	{%r1325, %r1326}, [%rd27+176];
	setp.le.s32 	%p284, %r1325, %r2170;
	setp.le.s32 	%p285, %r1326, %r2123;
	add.s32 	%r376, %r247, 14;
	setp.le.s32 	%p286, %r372, %r376;
	or.pred  	%p287, %p284, %p285;
	or.b32  	%r1327, %r2170, %r376;
	or.b32  	%r1328, %r1327, %r2123;
	setp.lt.s32 	%p288, %r1328, 0;
	or.pred  	%p289, %p288, %p287;
	or.pred  	%p290, %p286, %p289;
	@%p290 bra 	$L__BB1_203;
	bra.uni 	$L__BB1_202;

$L__BB1_203:
	add.s32 	%r2124, %r251, 2;
	add.s32 	%r1789, %r249, 3;
	st.local.v2.u32 	[%rd26], {%r1789, %r2124};
	add.s32 	%r1790, %r247, 14;
	st.local.v2.u32 	[%rd26+8], {%r1790, %r1325};
	st.local.v2.u32 	[%rd26+16], {%r1326, %r372};
	mov.u64 	%rd1382, $str$1;
	cvta.global.u64 	%rd1383, %rd1382;
	{ // callseq 378, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1383;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1329, [retval0+0];
	} // callseq 378
	bra.uni 	$L__BB1_204;

$L__BB1_202:
	ld.param.u32 	%r2177, [%rd27+88];
	ld.param.u64 	%rd4065, [%rd27];
	ld.param.u32 	%r2176, [%rd27+32];
	ld.param.u64 	%rd4064, [%rd27+112];
	ld.param.u32 	%r2175, [%rd27+144];
	ld.param.u64 	%rd4063, [%rd27+56];
	cvta.to.global.u64 	%rd1375, %rd4063;
	mul.wide.s32 	%rd1376, %r2175, %r376;
	add.s64 	%rd1366, %rd4064, %rd1376;
	// begin inline asm
	{ atom.add.f64 %fd4573,[%rd1366],%fd10830; }

	// end inline asm
	add.s64 	%rd1367, %rd1366, 8;
	// begin inline asm
	{ atom.add.f64 %fd4575,[%rd1367],%fd10829; }

	// end inline asm
	add.s64 	%rd1368, %rd1366, 16;
	// begin inline asm
	{ atom.add.f64 %fd4577,[%rd1368],%fd10828; }

	// end inline asm
	add.s64 	%rd1369, %rd1366, 24;
	// begin inline asm
	{ atom.add.f64 %fd4579,[%rd1369],%fd10818; }

	// end inline asm
	add.s64 	%rd1370, %rd1366, 32;
	// begin inline asm
	{ atom.add.f64 %fd4581,[%rd1370],%fd10817; }

	// end inline asm
	add.s64 	%rd1371, %rd1366, 40;
	// begin inline asm
	{ atom.add.f64 %fd4583,[%rd1371],%fd10816; }

	// end inline asm
	add.s64 	%rd1372, %rd1366, 48;
	// begin inline asm
	{ atom.add.f64 %fd4585,[%rd1372],%fd10806; }

	// end inline asm
	add.s64 	%rd1373, %rd1366, 56;
	// begin inline asm
	{ atom.add.f64 %fd4587,[%rd1373],%fd10805; }

	// end inline asm
	add.s64 	%rd1374, %rd1366, 64;
	// begin inline asm
	{ atom.add.f64 %fd4589,[%rd1374],%fd10804; }

	// end inline asm
	mul.wide.s32 	%rd1377, %r2176, %r376;
	cvta.to.global.u64 	%rd1378, %rd4065;
	add.s64 	%rd1379, %rd1378, %rd1377;
	mul.wide.s32 	%rd1380, %r2177, %r376;
	add.s64 	%rd1381, %rd1375, %rd1380;
	add.s32 	%r1787, %r249, 3;
	st.global.u32 	[%rd1379], %r1787;
	add.s32 	%r1788, %r251, 2;
	st.global.u32 	[%rd1381], %r1788;

$L__BB1_204:
	add.s32 	%r2171, %r249, 3;
	add.s32 	%r2132, %r251, 3;
	ld.param.u32 	%r380, [%rd27+172];
	ld.param.v2.u32 	{%r1330, %r1331}, [%rd27+176];
	setp.le.s32 	%p291, %r1330, %r2171;
	setp.le.s32 	%p292, %r1331, %r2132;
	add.s32 	%r384, %r247, 15;
	setp.le.s32 	%p293, %r380, %r384;
	or.pred  	%p294, %p291, %p292;
	or.b32  	%r1332, %r2171, %r384;
	or.b32  	%r1333, %r1332, %r2132;
	setp.lt.s32 	%p295, %r1333, 0;
	or.pred  	%p296, %p295, %p294;
	or.pred  	%p297, %p293, %p296;
	@%p297 bra 	$L__BB1_206;
	bra.uni 	$L__BB1_205;

$L__BB1_206:
	add.s32 	%r2133, %r251, 3;
	add.s32 	%r1793, %r249, 3;
	st.local.v2.u32 	[%rd26], {%r1793, %r2133};
	add.s32 	%r1794, %r247, 15;
	st.local.v2.u32 	[%rd26+8], {%r1794, %r1330};
	st.local.v2.u32 	[%rd26+16], {%r1331, %r380};
	mov.u64 	%rd1401, $str$1;
	cvta.global.u64 	%rd1402, %rd1401;
	{ // callseq 379, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1402;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1334, [retval0+0];
	} // callseq 379
	bra.uni 	$L__BB1_207;

$L__BB1_205:
	ld.param.u32 	%r2180, [%rd27+88];
	ld.param.u64 	%rd4068, [%rd27];
	ld.param.u32 	%r2179, [%rd27+32];
	ld.param.u64 	%rd4067, [%rd27+112];
	ld.param.u32 	%r2178, [%rd27+144];
	ld.param.u64 	%rd4066, [%rd27+56];
	cvta.to.global.u64 	%rd1394, %rd4066;
	mul.wide.s32 	%rd1395, %r2178, %r384;
	add.s64 	%rd1385, %rd4067, %rd1395;
	// begin inline asm
	{ atom.add.f64 %fd4591,[%rd1385],%fd10827; }

	// end inline asm
	add.s64 	%rd1386, %rd1385, 8;
	// begin inline asm
	{ atom.add.f64 %fd4593,[%rd1386],%fd10826; }

	// end inline asm
	add.s64 	%rd1387, %rd1385, 16;
	// begin inline asm
	{ atom.add.f64 %fd4595,[%rd1387],%fd10825; }

	// end inline asm
	add.s64 	%rd1388, %rd1385, 24;
	// begin inline asm
	{ atom.add.f64 %fd4597,[%rd1388],%fd10815; }

	// end inline asm
	add.s64 	%rd1389, %rd1385, 32;
	// begin inline asm
	{ atom.add.f64 %fd4599,[%rd1389],%fd10814; }

	// end inline asm
	add.s64 	%rd1390, %rd1385, 40;
	// begin inline asm
	{ atom.add.f64 %fd4601,[%rd1390],%fd10813; }

	// end inline asm
	add.s64 	%rd1391, %rd1385, 48;
	// begin inline asm
	{ atom.add.f64 %fd4603,[%rd1391],%fd10803; }

	// end inline asm
	add.s64 	%rd1392, %rd1385, 56;
	// begin inline asm
	{ atom.add.f64 %fd4605,[%rd1392],%fd10802; }

	// end inline asm
	add.s64 	%rd1393, %rd1385, 64;
	// begin inline asm
	{ atom.add.f64 %fd4607,[%rd1393],%fd10801; }

	// end inline asm
	mul.wide.s32 	%rd1396, %r2179, %r384;
	cvta.to.global.u64 	%rd1397, %rd4068;
	add.s64 	%rd1398, %rd1397, %rd1396;
	mul.wide.s32 	%rd1399, %r2180, %r384;
	add.s64 	%rd1400, %rd1394, %rd1399;
	add.s32 	%r1791, %r249, 3;
	st.global.u32 	[%rd1398], %r1791;
	add.s32 	%r1792, %r251, 3;
	st.global.u32 	[%rd1400], %r1792;

$L__BB1_207:
	setp.gt.s32 	%p943, %r1022, 0;
	cvt.u32.u64 	%r2108, %rd4122;
	selp.b32 	%r2255, %r2108, 0, %p943;
	add.s32 	%r2258, %r2255, %r1050;

$L__BB1_208:
	@%p9 bra 	$L__BB1_661;

	ld.param.u32 	%r397, [%rd27+172];
	ld.param.v2.u32 	{%r1335, %r1336}, [%rd27+176];
	shl.b32 	%r401, %r2259, 2;
	setp.le.s32 	%p299, %r1335, %r401;
	shl.b32 	%r402, %r2260, 2;
	setp.le.s32 	%p300, %r1336, %r402;
	shl.b32 	%r403, %r2258, 4;
	setp.le.s32 	%p301, %r397, %r403;
	or.pred  	%p302, %p299, %p300;
	or.b32  	%r1337, %r401, %r403;
	or.b32  	%r404, %r1337, %r402;
	setp.lt.s32 	%p303, %r404, 0;
	or.pred  	%p304, %p303, %p302;
	or.pred  	%p305, %p301, %p304;
	@%p305 bra 	$L__BB1_211;
	bra.uni 	$L__BB1_210;

$L__BB1_211:
	st.local.v2.u32 	[%rd26], {%r401, %r402};
	st.local.v2.u32 	[%rd26+8], {%r403, %r1335};
	st.local.v2.u32 	[%rd26+16], {%r1336, %r397};
	mov.u64 	%rd1420, $str$1;
	cvta.global.u64 	%rd1421, %rd1420;
	{ // callseq 380, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1421;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1338, [retval0+0];
	} // callseq 380
	bra.uni 	$L__BB1_212;

$L__BB1_210:
	ld.param.u32 	%r2183, [%rd27+88];
	ld.param.u64 	%rd4071, [%rd27];
	ld.param.u32 	%r2182, [%rd27+32];
	ld.param.u64 	%rd4070, [%rd27+112];
	ld.param.u32 	%r2181, [%rd27+144];
	ld.param.u64 	%rd4069, [%rd27+56];
	cvta.to.global.u64 	%rd1413, %rd4069;
	mul.wide.s32 	%rd1414, %r2181, %r403;
	add.s64 	%rd1404, %rd4070, %rd1414;
	// begin inline asm
	{ atom.add.f64 %fd4609,[%rd1404],%fd10944; }

	// end inline asm
	add.s64 	%rd1405, %rd1404, 8;
	// begin inline asm
	{ atom.add.f64 %fd4611,[%rd1405],%fd10943; }

	// end inline asm
	add.s64 	%rd1406, %rd1404, 16;
	// begin inline asm
	{ atom.add.f64 %fd4613,[%rd1406],%fd10942; }

	// end inline asm
	add.s64 	%rd1407, %rd1404, 24;
	// begin inline asm
	{ atom.add.f64 %fd4615,[%rd1407],%fd10932; }

	// end inline asm
	add.s64 	%rd1408, %rd1404, 32;
	// begin inline asm
	{ atom.add.f64 %fd4617,[%rd1408],%fd10931; }

	// end inline asm
	add.s64 	%rd1409, %rd1404, 40;
	// begin inline asm
	{ atom.add.f64 %fd4619,[%rd1409],%fd10930; }

	// end inline asm
	add.s64 	%rd1410, %rd1404, 48;
	// begin inline asm
	{ atom.add.f64 %fd4621,[%rd1410],%fd10920; }

	// end inline asm
	add.s64 	%rd1411, %rd1404, 56;
	// begin inline asm
	{ atom.add.f64 %fd4623,[%rd1411],%fd10919; }

	// end inline asm
	add.s64 	%rd1412, %rd1404, 64;
	// begin inline asm
	{ atom.add.f64 %fd4625,[%rd1412],%fd10918; }

	// end inline asm
	mul.wide.s32 	%rd1415, %r2182, %r403;
	cvta.to.global.u64 	%rd1416, %rd4071;
	add.s64 	%rd1417, %rd1416, %rd1415;
	mul.wide.s32 	%rd1418, %r2183, %r403;
	add.s64 	%rd1419, %rd1413, %rd1418;
	st.global.u32 	[%rd1417], %r401;
	st.global.u32 	[%rd1419], %r402;

$L__BB1_212:
	ld.param.u32 	%r408, [%rd27+172];
	ld.param.v2.u32 	{%r1339, %r1340}, [%rd27+176];
	setp.le.s32 	%p306, %r1339, %r401;
	add.s32 	%r412, %r402, 1;
	setp.le.s32 	%p307, %r1340, %r412;
	add.s32 	%r413, %r403, 1;
	setp.le.s32 	%p308, %r408, %r413;
	or.pred  	%p309, %p306, %p307;
	or.b32  	%r1341, %r413, %r401;
	or.b32  	%r414, %r1341, %r412;
	setp.lt.s32 	%p310, %r414, 0;
	or.pred  	%p311, %p310, %p309;
	or.pred  	%p312, %p308, %p311;
	@%p312 bra 	$L__BB1_214;
	bra.uni 	$L__BB1_213;

$L__BB1_214:
	add.s32 	%r1796, %r402, 1;
	st.local.v2.u32 	[%rd26], {%r401, %r1796};
	add.s32 	%r1797, %r403, 1;
	st.local.v2.u32 	[%rd26+8], {%r1797, %r1339};
	st.local.v2.u32 	[%rd26+16], {%r1340, %r408};
	mov.u64 	%rd1439, $str$1;
	cvta.global.u64 	%rd1440, %rd1439;
	{ // callseq 381, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1440;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1342, [retval0+0];
	} // callseq 381
	bra.uni 	$L__BB1_215;

$L__BB1_213:
	ld.param.u32 	%r2226, [%rd27+88];
	ld.param.u64 	%rd4111, [%rd27];
	ld.param.u32 	%r2225, [%rd27+32];
	ld.param.u64 	%rd4110, [%rd27+112];
	add.s32 	%r2224, %r403, 1;
	ld.param.u32 	%r2223, [%rd27+144];
	ld.param.u64 	%rd4109, [%rd27+56];
	cvta.to.global.u64 	%rd1432, %rd4109;
	mul.wide.s32 	%rd1433, %r2223, %r2224;
	add.s64 	%rd1423, %rd4110, %rd1433;
	// begin inline asm
	{ atom.add.f64 %fd4627,[%rd1423],%fd10941; }

	// end inline asm
	add.s64 	%rd1424, %rd1423, 8;
	// begin inline asm
	{ atom.add.f64 %fd4629,[%rd1424],%fd10940; }

	// end inline asm
	add.s64 	%rd1425, %rd1423, 16;
	// begin inline asm
	{ atom.add.f64 %fd4631,[%rd1425],%fd10939; }

	// end inline asm
	add.s64 	%rd1426, %rd1423, 24;
	// begin inline asm
	{ atom.add.f64 %fd4633,[%rd1426],%fd10929; }

	// end inline asm
	add.s64 	%rd1427, %rd1423, 32;
	// begin inline asm
	{ atom.add.f64 %fd4635,[%rd1427],%fd10928; }

	// end inline asm
	add.s64 	%rd1428, %rd1423, 40;
	// begin inline asm
	{ atom.add.f64 %fd4637,[%rd1428],%fd10927; }

	// end inline asm
	add.s64 	%rd1429, %rd1423, 48;
	// begin inline asm
	{ atom.add.f64 %fd4639,[%rd1429],%fd10917; }

	// end inline asm
	add.s64 	%rd1430, %rd1423, 56;
	// begin inline asm
	{ atom.add.f64 %fd4641,[%rd1430],%fd10916; }

	// end inline asm
	add.s64 	%rd1431, %rd1423, 64;
	// begin inline asm
	{ atom.add.f64 %fd4643,[%rd1431],%fd10915; }

	// end inline asm
	mul.wide.s32 	%rd1434, %r2225, %r2224;
	cvta.to.global.u64 	%rd1435, %rd4111;
	add.s64 	%rd1436, %rd1435, %rd1434;
	mul.wide.s32 	%rd1437, %r2226, %r2224;
	add.s64 	%rd1438, %rd1432, %rd1437;
	st.global.u32 	[%rd1436], %r401;
	add.s32 	%r1795, %r402, 1;
	st.global.u32 	[%rd1438], %r1795;

$L__BB1_215:
	ld.param.u64 	%rd166, [%rd27];
	ld.param.u32 	%r415, [%rd27+32];
	ld.param.u64 	%rd167, [%rd27+56];
	ld.param.u32 	%r416, [%rd27+88];
	ld.param.u64 	%rd168, [%rd27+112];
	ld.param.u32 	%r417, [%rd27+144];
	ld.param.u32 	%r418, [%rd27+172];
	ld.param.v2.u32 	{%r1343, %r1344}, [%rd27+176];
	setp.le.s32 	%p313, %r1343, %r401;
	add.s32 	%r422, %r402, 2;
	setp.le.s32 	%p314, %r1344, %r422;
	add.s32 	%r423, %r403, 2;
	setp.le.s32 	%p315, %r418, %r423;
	or.pred  	%p316, %p313, %p314;
	or.b32  	%r1345, %r423, %r401;
	or.b32  	%r424, %r1345, %r422;
	setp.lt.s32 	%p317, %r424, 0;
	or.pred  	%p318, %p317, %p316;
	or.pred  	%p319, %p315, %p318;
	@%p319 bra 	$L__BB1_217;
	bra.uni 	$L__BB1_216;

$L__BB1_217:
	add.s32 	%r1799, %r402, 2;
	st.local.v2.u32 	[%rd26], {%r401, %r1799};
	add.s32 	%r1800, %r403, 2;
	st.local.v2.u32 	[%rd26+8], {%r1800, %r1343};
	st.local.v2.u32 	[%rd26+16], {%r1344, %r418};
	mov.u64 	%rd1458, $str$1;
	cvta.global.u64 	%rd1459, %rd1458;
	{ // callseq 382, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1459;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1346, [retval0+0];
	} // callseq 382
	bra.uni 	$L__BB1_218;

$L__BB1_216:
	cvta.to.global.u64 	%rd1451, %rd167;
	mul.wide.s32 	%rd1452, %r417, %r423;
	add.s64 	%rd1442, %rd168, %rd1452;
	// begin inline asm
	{ atom.add.f64 %fd4645,[%rd1442],%fd10938; }

	// end inline asm
	add.s64 	%rd1443, %rd1442, 8;
	// begin inline asm
	{ atom.add.f64 %fd4647,[%rd1443],%fd10937; }

	// end inline asm
	add.s64 	%rd1444, %rd1442, 16;
	// begin inline asm
	{ atom.add.f64 %fd4649,[%rd1444],%fd10936; }

	// end inline asm
	add.s64 	%rd1445, %rd1442, 24;
	// begin inline asm
	{ atom.add.f64 %fd4651,[%rd1445],%fd10926; }

	// end inline asm
	add.s64 	%rd1446, %rd1442, 32;
	// begin inline asm
	{ atom.add.f64 %fd4653,[%rd1446],%fd10925; }

	// end inline asm
	add.s64 	%rd1447, %rd1442, 40;
	// begin inline asm
	{ atom.add.f64 %fd4655,[%rd1447],%fd10924; }

	// end inline asm
	add.s64 	%rd1448, %rd1442, 48;
	// begin inline asm
	{ atom.add.f64 %fd4657,[%rd1448],%fd10914; }

	// end inline asm
	add.s64 	%rd1449, %rd1442, 56;
	// begin inline asm
	{ atom.add.f64 %fd4659,[%rd1449],%fd10913; }

	// end inline asm
	add.s64 	%rd1450, %rd1442, 64;
	// begin inline asm
	{ atom.add.f64 %fd4661,[%rd1450],%fd10912; }

	// end inline asm
	mul.wide.s32 	%rd1453, %r415, %r423;
	cvta.to.global.u64 	%rd1454, %rd166;
	add.s64 	%rd1455, %rd1454, %rd1453;
	mul.wide.s32 	%rd1456, %r416, %r423;
	add.s64 	%rd1457, %rd1451, %rd1456;
	st.global.u32 	[%rd1455], %r401;
	add.s32 	%r1798, %r402, 2;
	st.global.u32 	[%rd1457], %r1798;

$L__BB1_218:
	ld.param.u64 	%rd169, [%rd27];
	ld.param.u32 	%r425, [%rd27+32];
	ld.param.u64 	%rd170, [%rd27+56];
	ld.param.u32 	%r426, [%rd27+88];
	ld.param.u64 	%rd171, [%rd27+112];
	ld.param.u32 	%r427, [%rd27+144];
	ld.param.u32 	%r428, [%rd27+172];
	ld.param.v2.u32 	{%r1347, %r1348}, [%rd27+176];
	setp.le.s32 	%p320, %r1347, %r401;
	add.s32 	%r432, %r402, 3;
	setp.le.s32 	%p321, %r1348, %r432;
	add.s32 	%r433, %r403, 3;
	setp.le.s32 	%p322, %r428, %r433;
	or.pred  	%p323, %p320, %p321;
	or.b32  	%r1349, %r433, %r401;
	or.b32  	%r434, %r1349, %r432;
	setp.lt.s32 	%p324, %r434, 0;
	or.pred  	%p325, %p324, %p323;
	or.pred  	%p326, %p322, %p325;
	@%p326 bra 	$L__BB1_220;
	bra.uni 	$L__BB1_219;

$L__BB1_220:
	add.s32 	%r1802, %r402, 3;
	st.local.v2.u32 	[%rd26], {%r401, %r1802};
	add.s32 	%r1803, %r403, 3;
	st.local.v2.u32 	[%rd26+8], {%r1803, %r1347};
	st.local.v2.u32 	[%rd26+16], {%r1348, %r428};
	mov.u64 	%rd1477, $str$1;
	cvta.global.u64 	%rd1478, %rd1477;
	{ // callseq 383, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1478;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1350, [retval0+0];
	} // callseq 383
	bra.uni 	$L__BB1_221;

$L__BB1_219:
	cvta.to.global.u64 	%rd1470, %rd170;
	mul.wide.s32 	%rd1471, %r427, %r433;
	add.s64 	%rd1461, %rd171, %rd1471;
	// begin inline asm
	{ atom.add.f64 %fd4663,[%rd1461],%fd10935; }

	// end inline asm
	add.s64 	%rd1462, %rd1461, 8;
	// begin inline asm
	{ atom.add.f64 %fd4665,[%rd1462],%fd10934; }

	// end inline asm
	add.s64 	%rd1463, %rd1461, 16;
	// begin inline asm
	{ atom.add.f64 %fd4667,[%rd1463],%fd10933; }

	// end inline asm
	add.s64 	%rd1464, %rd1461, 24;
	// begin inline asm
	{ atom.add.f64 %fd4669,[%rd1464],%fd10923; }

	// end inline asm
	add.s64 	%rd1465, %rd1461, 32;
	// begin inline asm
	{ atom.add.f64 %fd4671,[%rd1465],%fd10922; }

	// end inline asm
	add.s64 	%rd1466, %rd1461, 40;
	// begin inline asm
	{ atom.add.f64 %fd4673,[%rd1466],%fd10921; }

	// end inline asm
	add.s64 	%rd1467, %rd1461, 48;
	// begin inline asm
	{ atom.add.f64 %fd4675,[%rd1467],%fd10911; }

	// end inline asm
	add.s64 	%rd1468, %rd1461, 56;
	// begin inline asm
	{ atom.add.f64 %fd4677,[%rd1468],%fd10910; }

	// end inline asm
	add.s64 	%rd1469, %rd1461, 64;
	// begin inline asm
	{ atom.add.f64 %fd4679,[%rd1469],%fd10909; }

	// end inline asm
	mul.wide.s32 	%rd1472, %r425, %r433;
	cvta.to.global.u64 	%rd1473, %rd169;
	add.s64 	%rd1474, %rd1473, %rd1472;
	mul.wide.s32 	%rd1475, %r426, %r433;
	add.s64 	%rd1476, %rd1470, %rd1475;
	st.global.u32 	[%rd1474], %r401;
	add.s32 	%r1801, %r402, 3;
	st.global.u32 	[%rd1476], %r1801;

$L__BB1_221:
	ld.param.u64 	%rd172, [%rd27];
	ld.param.u32 	%r435, [%rd27+32];
	ld.param.u64 	%rd173, [%rd27+56];
	ld.param.u32 	%r436, [%rd27+88];
	ld.param.u64 	%rd174, [%rd27+112];
	ld.param.u32 	%r437, [%rd27+144];
	ld.param.u32 	%r438, [%rd27+172];
	ld.param.v2.u32 	{%r1351, %r1352}, [%rd27+176];
	add.s32 	%r442, %r401, 1;
	setp.le.s32 	%p327, %r1351, %r442;
	setp.le.s32 	%p328, %r1352, %r402;
	add.s32 	%r443, %r403, 4;
	setp.le.s32 	%p329, %r438, %r443;
	or.pred  	%p330, %p327, %p328;
	or.b32  	%r1353, %r402, %r443;
	or.b32  	%r444, %r1353, %r442;
	setp.lt.s32 	%p331, %r444, 0;
	or.pred  	%p332, %p331, %p330;
	or.pred  	%p333, %p329, %p332;
	@%p333 bra 	$L__BB1_223;
	bra.uni 	$L__BB1_222;

$L__BB1_223:
	add.s32 	%r1805, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1805, %r402};
	add.s32 	%r1806, %r403, 4;
	st.local.v2.u32 	[%rd26+8], {%r1806, %r1351};
	st.local.v2.u32 	[%rd26+16], {%r1352, %r438};
	mov.u64 	%rd1496, $str$1;
	cvta.global.u64 	%rd1497, %rd1496;
	{ // callseq 384, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1497;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1354, [retval0+0];
	} // callseq 384
	bra.uni 	$L__BB1_224;

$L__BB1_222:
	cvta.to.global.u64 	%rd1489, %rd173;
	mul.wide.s32 	%rd1490, %r437, %r443;
	add.s64 	%rd1480, %rd174, %rd1490;
	// begin inline asm
	{ atom.add.f64 %fd4681,[%rd1480],%fd10908; }

	// end inline asm
	add.s64 	%rd1481, %rd1480, 8;
	// begin inline asm
	{ atom.add.f64 %fd4683,[%rd1481],%fd10907; }

	// end inline asm
	add.s64 	%rd1482, %rd1480, 16;
	// begin inline asm
	{ atom.add.f64 %fd4685,[%rd1482],%fd10906; }

	// end inline asm
	add.s64 	%rd1483, %rd1480, 24;
	// begin inline asm
	{ atom.add.f64 %fd4687,[%rd1483],%fd10896; }

	// end inline asm
	add.s64 	%rd1484, %rd1480, 32;
	// begin inline asm
	{ atom.add.f64 %fd4689,[%rd1484],%fd10895; }

	// end inline asm
	add.s64 	%rd1485, %rd1480, 40;
	// begin inline asm
	{ atom.add.f64 %fd4691,[%rd1485],%fd10894; }

	// end inline asm
	add.s64 	%rd1486, %rd1480, 48;
	// begin inline asm
	{ atom.add.f64 %fd4693,[%rd1486],%fd10884; }

	// end inline asm
	add.s64 	%rd1487, %rd1480, 56;
	// begin inline asm
	{ atom.add.f64 %fd4695,[%rd1487],%fd10883; }

	// end inline asm
	add.s64 	%rd1488, %rd1480, 64;
	// begin inline asm
	{ atom.add.f64 %fd4697,[%rd1488],%fd10882; }

	// end inline asm
	mul.wide.s32 	%rd1491, %r435, %r443;
	cvta.to.global.u64 	%rd1492, %rd172;
	add.s64 	%rd1493, %rd1492, %rd1491;
	mul.wide.s32 	%rd1494, %r436, %r443;
	add.s64 	%rd1495, %rd1489, %rd1494;
	add.s32 	%r1804, %r401, 1;
	st.global.u32 	[%rd1493], %r1804;
	st.global.u32 	[%rd1495], %r402;

$L__BB1_224:
	ld.param.u64 	%rd175, [%rd27];
	ld.param.u32 	%r445, [%rd27+32];
	ld.param.u64 	%rd176, [%rd27+56];
	ld.param.u32 	%r446, [%rd27+88];
	ld.param.u64 	%rd177, [%rd27+112];
	ld.param.u32 	%r447, [%rd27+144];
	ld.param.u32 	%r448, [%rd27+172];
	ld.param.v2.u32 	{%r1355, %r1356}, [%rd27+176];
	setp.le.s32 	%p334, %r1355, %r442;
	setp.le.s32 	%p335, %r1356, %r412;
	add.s32 	%r452, %r403, 5;
	setp.le.s32 	%p336, %r448, %r452;
	or.pred  	%p337, %p334, %p335;
	or.b32  	%r1357, %r442, %r452;
	or.b32  	%r453, %r1357, %r412;
	setp.lt.s32 	%p338, %r453, 0;
	or.pred  	%p339, %p338, %p337;
	or.pred  	%p340, %p336, %p339;
	@%p340 bra 	$L__BB1_226;
	bra.uni 	$L__BB1_225;

$L__BB1_226:
	add.s32 	%r1809, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1809, %r412};
	add.s32 	%r1810, %r403, 5;
	st.local.v2.u32 	[%rd26+8], {%r1810, %r1355};
	st.local.v2.u32 	[%rd26+16], {%r1356, %r448};
	mov.u64 	%rd1515, $str$1;
	cvta.global.u64 	%rd1516, %rd1515;
	{ // callseq 385, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1516;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1358, [retval0+0];
	} // callseq 385
	bra.uni 	$L__BB1_227;

$L__BB1_225:
	cvta.to.global.u64 	%rd1508, %rd176;
	mul.wide.s32 	%rd1509, %r447, %r452;
	add.s64 	%rd1499, %rd177, %rd1509;
	// begin inline asm
	{ atom.add.f64 %fd4699,[%rd1499],%fd10905; }

	// end inline asm
	add.s64 	%rd1500, %rd1499, 8;
	// begin inline asm
	{ atom.add.f64 %fd4701,[%rd1500],%fd10904; }

	// end inline asm
	add.s64 	%rd1501, %rd1499, 16;
	// begin inline asm
	{ atom.add.f64 %fd4703,[%rd1501],%fd10903; }

	// end inline asm
	add.s64 	%rd1502, %rd1499, 24;
	// begin inline asm
	{ atom.add.f64 %fd4705,[%rd1502],%fd10893; }

	// end inline asm
	add.s64 	%rd1503, %rd1499, 32;
	// begin inline asm
	{ atom.add.f64 %fd4707,[%rd1503],%fd10892; }

	// end inline asm
	add.s64 	%rd1504, %rd1499, 40;
	// begin inline asm
	{ atom.add.f64 %fd4709,[%rd1504],%fd10891; }

	// end inline asm
	add.s64 	%rd1505, %rd1499, 48;
	// begin inline asm
	{ atom.add.f64 %fd4711,[%rd1505],%fd10881; }

	// end inline asm
	add.s64 	%rd1506, %rd1499, 56;
	// begin inline asm
	{ atom.add.f64 %fd4713,[%rd1506],%fd10880; }

	// end inline asm
	add.s64 	%rd1507, %rd1499, 64;
	// begin inline asm
	{ atom.add.f64 %fd4715,[%rd1507],%fd10879; }

	// end inline asm
	mul.wide.s32 	%rd1510, %r445, %r452;
	cvta.to.global.u64 	%rd1511, %rd175;
	add.s64 	%rd1512, %rd1511, %rd1510;
	mul.wide.s32 	%rd1513, %r446, %r452;
	add.s64 	%rd1514, %rd1508, %rd1513;
	add.s32 	%r1807, %r401, 1;
	st.global.u32 	[%rd1512], %r1807;
	add.s32 	%r1808, %r402, 1;
	st.global.u32 	[%rd1514], %r1808;

$L__BB1_227:
	ld.param.u64 	%rd178, [%rd27];
	ld.param.u32 	%r454, [%rd27+32];
	ld.param.u64 	%rd179, [%rd27+56];
	ld.param.u32 	%r455, [%rd27+88];
	ld.param.u64 	%rd180, [%rd27+112];
	ld.param.u32 	%r456, [%rd27+144];
	ld.param.u32 	%r457, [%rd27+172];
	ld.param.v2.u32 	{%r1359, %r1360}, [%rd27+176];
	setp.le.s32 	%p341, %r1359, %r442;
	setp.le.s32 	%p342, %r1360, %r422;
	add.s32 	%r461, %r403, 6;
	setp.le.s32 	%p343, %r457, %r461;
	or.pred  	%p344, %p341, %p342;
	or.b32  	%r1361, %r442, %r461;
	or.b32  	%r462, %r1361, %r422;
	setp.lt.s32 	%p345, %r462, 0;
	or.pred  	%p346, %p345, %p344;
	or.pred  	%p347, %p343, %p346;
	@%p347 bra 	$L__BB1_229;
	bra.uni 	$L__BB1_228;

$L__BB1_229:
	add.s32 	%r1813, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1813, %r422};
	add.s32 	%r1814, %r403, 6;
	st.local.v2.u32 	[%rd26+8], {%r1814, %r1359};
	st.local.v2.u32 	[%rd26+16], {%r1360, %r457};
	mov.u64 	%rd1534, $str$1;
	cvta.global.u64 	%rd1535, %rd1534;
	{ // callseq 386, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1535;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1362, [retval0+0];
	} // callseq 386
	bra.uni 	$L__BB1_230;

$L__BB1_228:
	cvta.to.global.u64 	%rd1527, %rd179;
	mul.wide.s32 	%rd1528, %r456, %r461;
	add.s64 	%rd1518, %rd180, %rd1528;
	// begin inline asm
	{ atom.add.f64 %fd4717,[%rd1518],%fd10902; }

	// end inline asm
	add.s64 	%rd1519, %rd1518, 8;
	// begin inline asm
	{ atom.add.f64 %fd4719,[%rd1519],%fd10901; }

	// end inline asm
	add.s64 	%rd1520, %rd1518, 16;
	// begin inline asm
	{ atom.add.f64 %fd4721,[%rd1520],%fd10900; }

	// end inline asm
	add.s64 	%rd1521, %rd1518, 24;
	// begin inline asm
	{ atom.add.f64 %fd4723,[%rd1521],%fd10890; }

	// end inline asm
	add.s64 	%rd1522, %rd1518, 32;
	// begin inline asm
	{ atom.add.f64 %fd4725,[%rd1522],%fd10889; }

	// end inline asm
	add.s64 	%rd1523, %rd1518, 40;
	// begin inline asm
	{ atom.add.f64 %fd4727,[%rd1523],%fd10888; }

	// end inline asm
	add.s64 	%rd1524, %rd1518, 48;
	// begin inline asm
	{ atom.add.f64 %fd4729,[%rd1524],%fd10878; }

	// end inline asm
	add.s64 	%rd1525, %rd1518, 56;
	// begin inline asm
	{ atom.add.f64 %fd4731,[%rd1525],%fd10877; }

	// end inline asm
	add.s64 	%rd1526, %rd1518, 64;
	// begin inline asm
	{ atom.add.f64 %fd4733,[%rd1526],%fd10876; }

	// end inline asm
	mul.wide.s32 	%rd1529, %r454, %r461;
	cvta.to.global.u64 	%rd1530, %rd178;
	add.s64 	%rd1531, %rd1530, %rd1529;
	mul.wide.s32 	%rd1532, %r455, %r461;
	add.s64 	%rd1533, %rd1527, %rd1532;
	add.s32 	%r1811, %r401, 1;
	st.global.u32 	[%rd1531], %r1811;
	add.s32 	%r1812, %r402, 2;
	st.global.u32 	[%rd1533], %r1812;

$L__BB1_230:
	ld.param.u64 	%rd181, [%rd27];
	ld.param.u32 	%r463, [%rd27+32];
	ld.param.u64 	%rd182, [%rd27+56];
	ld.param.u32 	%r464, [%rd27+88];
	ld.param.u64 	%rd183, [%rd27+112];
	ld.param.u32 	%r465, [%rd27+144];
	ld.param.u32 	%r466, [%rd27+172];
	ld.param.v2.u32 	{%r1363, %r1364}, [%rd27+176];
	setp.le.s32 	%p348, %r1363, %r442;
	setp.le.s32 	%p349, %r1364, %r432;
	add.s32 	%r470, %r403, 7;
	setp.le.s32 	%p350, %r466, %r470;
	or.pred  	%p351, %p348, %p349;
	or.b32  	%r1365, %r442, %r470;
	or.b32  	%r471, %r1365, %r432;
	setp.lt.s32 	%p352, %r471, 0;
	or.pred  	%p353, %p352, %p351;
	or.pred  	%p354, %p350, %p353;
	@%p354 bra 	$L__BB1_232;
	bra.uni 	$L__BB1_231;

$L__BB1_232:
	add.s32 	%r1817, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1817, %r432};
	add.s32 	%r1818, %r403, 7;
	st.local.v2.u32 	[%rd26+8], {%r1818, %r1363};
	st.local.v2.u32 	[%rd26+16], {%r1364, %r466};
	mov.u64 	%rd1553, $str$1;
	cvta.global.u64 	%rd1554, %rd1553;
	{ // callseq 387, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1554;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1366, [retval0+0];
	} // callseq 387
	bra.uni 	$L__BB1_233;

$L__BB1_231:
	cvta.to.global.u64 	%rd1546, %rd182;
	mul.wide.s32 	%rd1547, %r465, %r470;
	add.s64 	%rd1537, %rd183, %rd1547;
	// begin inline asm
	{ atom.add.f64 %fd4735,[%rd1537],%fd10899; }

	// end inline asm
	add.s64 	%rd1538, %rd1537, 8;
	// begin inline asm
	{ atom.add.f64 %fd4737,[%rd1538],%fd10898; }

	// end inline asm
	add.s64 	%rd1539, %rd1537, 16;
	// begin inline asm
	{ atom.add.f64 %fd4739,[%rd1539],%fd10897; }

	// end inline asm
	add.s64 	%rd1540, %rd1537, 24;
	// begin inline asm
	{ atom.add.f64 %fd4741,[%rd1540],%fd10887; }

	// end inline asm
	add.s64 	%rd1541, %rd1537, 32;
	// begin inline asm
	{ atom.add.f64 %fd4743,[%rd1541],%fd10886; }

	// end inline asm
	add.s64 	%rd1542, %rd1537, 40;
	// begin inline asm
	{ atom.add.f64 %fd4745,[%rd1542],%fd10885; }

	// end inline asm
	add.s64 	%rd1543, %rd1537, 48;
	// begin inline asm
	{ atom.add.f64 %fd4747,[%rd1543],%fd10875; }

	// end inline asm
	add.s64 	%rd1544, %rd1537, 56;
	// begin inline asm
	{ atom.add.f64 %fd4749,[%rd1544],%fd10874; }

	// end inline asm
	add.s64 	%rd1545, %rd1537, 64;
	// begin inline asm
	{ atom.add.f64 %fd4751,[%rd1545],%fd10873; }

	// end inline asm
	mul.wide.s32 	%rd1548, %r463, %r470;
	cvta.to.global.u64 	%rd1549, %rd181;
	add.s64 	%rd1550, %rd1549, %rd1548;
	mul.wide.s32 	%rd1551, %r464, %r470;
	add.s64 	%rd1552, %rd1546, %rd1551;
	add.s32 	%r1815, %r401, 1;
	st.global.u32 	[%rd1550], %r1815;
	add.s32 	%r1816, %r402, 3;
	st.global.u32 	[%rd1552], %r1816;

$L__BB1_233:
	ld.param.u64 	%rd184, [%rd27];
	ld.param.u32 	%r472, [%rd27+32];
	ld.param.u64 	%rd185, [%rd27+56];
	ld.param.u32 	%r473, [%rd27+88];
	ld.param.u64 	%rd186, [%rd27+112];
	ld.param.u32 	%r474, [%rd27+144];
	ld.param.u32 	%r475, [%rd27+172];
	ld.param.v2.u32 	{%r1367, %r1368}, [%rd27+176];
	add.s32 	%r479, %r401, 2;
	setp.le.s32 	%p355, %r1367, %r479;
	setp.le.s32 	%p356, %r1368, %r402;
	add.s32 	%r480, %r403, 8;
	setp.le.s32 	%p357, %r475, %r480;
	or.pred  	%p358, %p355, %p356;
	or.b32  	%r1369, %r402, %r480;
	or.b32  	%r481, %r1369, %r479;
	setp.lt.s32 	%p359, %r481, 0;
	or.pred  	%p360, %p359, %p358;
	or.pred  	%p361, %p357, %p360;
	@%p361 bra 	$L__BB1_235;
	bra.uni 	$L__BB1_234;

$L__BB1_235:
	add.s32 	%r1820, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1820, %r402};
	add.s32 	%r1821, %r403, 8;
	st.local.v2.u32 	[%rd26+8], {%r1821, %r1367};
	st.local.v2.u32 	[%rd26+16], {%r1368, %r475};
	mov.u64 	%rd1572, $str$1;
	cvta.global.u64 	%rd1573, %rd1572;
	{ // callseq 388, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1573;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1370, [retval0+0];
	} // callseq 388
	bra.uni 	$L__BB1_236;

$L__BB1_234:
	cvta.to.global.u64 	%rd1565, %rd185;
	mul.wide.s32 	%rd1566, %r474, %r480;
	add.s64 	%rd1556, %rd186, %rd1566;
	// begin inline asm
	{ atom.add.f64 %fd4753,[%rd1556],%fd10872; }

	// end inline asm
	add.s64 	%rd1557, %rd1556, 8;
	// begin inline asm
	{ atom.add.f64 %fd4755,[%rd1557],%fd10871; }

	// end inline asm
	add.s64 	%rd1558, %rd1556, 16;
	// begin inline asm
	{ atom.add.f64 %fd4757,[%rd1558],%fd10870; }

	// end inline asm
	add.s64 	%rd1559, %rd1556, 24;
	// begin inline asm
	{ atom.add.f64 %fd4759,[%rd1559],%fd10860; }

	// end inline asm
	add.s64 	%rd1560, %rd1556, 32;
	// begin inline asm
	{ atom.add.f64 %fd4761,[%rd1560],%fd10859; }

	// end inline asm
	add.s64 	%rd1561, %rd1556, 40;
	// begin inline asm
	{ atom.add.f64 %fd4763,[%rd1561],%fd10858; }

	// end inline asm
	add.s64 	%rd1562, %rd1556, 48;
	// begin inline asm
	{ atom.add.f64 %fd4765,[%rd1562],%fd10848; }

	// end inline asm
	add.s64 	%rd1563, %rd1556, 56;
	// begin inline asm
	{ atom.add.f64 %fd4767,[%rd1563],%fd10847; }

	// end inline asm
	add.s64 	%rd1564, %rd1556, 64;
	// begin inline asm
	{ atom.add.f64 %fd4769,[%rd1564],%fd10846; }

	// end inline asm
	mul.wide.s32 	%rd1567, %r472, %r480;
	cvta.to.global.u64 	%rd1568, %rd184;
	add.s64 	%rd1569, %rd1568, %rd1567;
	mul.wide.s32 	%rd1570, %r473, %r480;
	add.s64 	%rd1571, %rd1565, %rd1570;
	add.s32 	%r1819, %r401, 2;
	st.global.u32 	[%rd1569], %r1819;
	st.global.u32 	[%rd1571], %r402;

$L__BB1_236:
	ld.param.u64 	%rd187, [%rd27];
	ld.param.u32 	%r482, [%rd27+32];
	ld.param.u64 	%rd188, [%rd27+56];
	ld.param.u32 	%r483, [%rd27+88];
	ld.param.u64 	%rd189, [%rd27+112];
	ld.param.u32 	%r484, [%rd27+144];
	ld.param.u32 	%r485, [%rd27+172];
	ld.param.v2.u32 	{%r1371, %r1372}, [%rd27+176];
	setp.le.s32 	%p362, %r1371, %r479;
	setp.le.s32 	%p363, %r1372, %r412;
	add.s32 	%r489, %r403, 9;
	setp.le.s32 	%p364, %r485, %r489;
	or.pred  	%p365, %p362, %p363;
	or.b32  	%r1373, %r479, %r489;
	or.b32  	%r490, %r1373, %r412;
	setp.lt.s32 	%p366, %r490, 0;
	or.pred  	%p367, %p366, %p365;
	or.pred  	%p368, %p364, %p367;
	@%p368 bra 	$L__BB1_238;
	bra.uni 	$L__BB1_237;

$L__BB1_238:
	add.s32 	%r1824, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1824, %r412};
	add.s32 	%r1825, %r403, 9;
	st.local.v2.u32 	[%rd26+8], {%r1825, %r1371};
	st.local.v2.u32 	[%rd26+16], {%r1372, %r485};
	mov.u64 	%rd1591, $str$1;
	cvta.global.u64 	%rd1592, %rd1591;
	{ // callseq 389, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1592;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1374, [retval0+0];
	} // callseq 389
	bra.uni 	$L__BB1_239;

$L__BB1_237:
	cvta.to.global.u64 	%rd1584, %rd188;
	mul.wide.s32 	%rd1585, %r484, %r489;
	add.s64 	%rd1575, %rd189, %rd1585;
	// begin inline asm
	{ atom.add.f64 %fd4771,[%rd1575],%fd10869; }

	// end inline asm
	add.s64 	%rd1576, %rd1575, 8;
	// begin inline asm
	{ atom.add.f64 %fd4773,[%rd1576],%fd10868; }

	// end inline asm
	add.s64 	%rd1577, %rd1575, 16;
	// begin inline asm
	{ atom.add.f64 %fd4775,[%rd1577],%fd10867; }

	// end inline asm
	add.s64 	%rd1578, %rd1575, 24;
	// begin inline asm
	{ atom.add.f64 %fd4777,[%rd1578],%fd10857; }

	// end inline asm
	add.s64 	%rd1579, %rd1575, 32;
	// begin inline asm
	{ atom.add.f64 %fd4779,[%rd1579],%fd10856; }

	// end inline asm
	add.s64 	%rd1580, %rd1575, 40;
	// begin inline asm
	{ atom.add.f64 %fd4781,[%rd1580],%fd10855; }

	// end inline asm
	add.s64 	%rd1581, %rd1575, 48;
	// begin inline asm
	{ atom.add.f64 %fd4783,[%rd1581],%fd10845; }

	// end inline asm
	add.s64 	%rd1582, %rd1575, 56;
	// begin inline asm
	{ atom.add.f64 %fd4785,[%rd1582],%fd10844; }

	// end inline asm
	add.s64 	%rd1583, %rd1575, 64;
	// begin inline asm
	{ atom.add.f64 %fd4787,[%rd1583],%fd10843; }

	// end inline asm
	mul.wide.s32 	%rd1586, %r482, %r489;
	cvta.to.global.u64 	%rd1587, %rd187;
	add.s64 	%rd1588, %rd1587, %rd1586;
	mul.wide.s32 	%rd1589, %r483, %r489;
	add.s64 	%rd1590, %rd1584, %rd1589;
	add.s32 	%r1822, %r401, 2;
	st.global.u32 	[%rd1588], %r1822;
	add.s32 	%r1823, %r402, 1;
	st.global.u32 	[%rd1590], %r1823;

$L__BB1_239:
	ld.param.u64 	%rd190, [%rd27];
	ld.param.u32 	%r491, [%rd27+32];
	ld.param.u64 	%rd191, [%rd27+56];
	ld.param.u32 	%r492, [%rd27+88];
	ld.param.u64 	%rd192, [%rd27+112];
	ld.param.u32 	%r493, [%rd27+144];
	ld.param.u32 	%r494, [%rd27+172];
	ld.param.v2.u32 	{%r1375, %r1376}, [%rd27+176];
	setp.le.s32 	%p369, %r1375, %r479;
	setp.le.s32 	%p370, %r1376, %r422;
	add.s32 	%r498, %r403, 10;
	setp.le.s32 	%p371, %r494, %r498;
	or.pred  	%p372, %p369, %p370;
	or.b32  	%r1377, %r479, %r498;
	or.b32  	%r499, %r1377, %r422;
	setp.lt.s32 	%p373, %r499, 0;
	or.pred  	%p374, %p373, %p372;
	or.pred  	%p375, %p371, %p374;
	@%p375 bra 	$L__BB1_241;
	bra.uni 	$L__BB1_240;

$L__BB1_241:
	add.s32 	%r1828, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1828, %r422};
	add.s32 	%r1829, %r403, 10;
	st.local.v2.u32 	[%rd26+8], {%r1829, %r1375};
	st.local.v2.u32 	[%rd26+16], {%r1376, %r494};
	mov.u64 	%rd1610, $str$1;
	cvta.global.u64 	%rd1611, %rd1610;
	{ // callseq 390, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1611;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1378, [retval0+0];
	} // callseq 390
	bra.uni 	$L__BB1_242;

$L__BB1_240:
	cvta.to.global.u64 	%rd1603, %rd191;
	mul.wide.s32 	%rd1604, %r493, %r498;
	add.s64 	%rd1594, %rd192, %rd1604;
	// begin inline asm
	{ atom.add.f64 %fd4789,[%rd1594],%fd10866; }

	// end inline asm
	add.s64 	%rd1595, %rd1594, 8;
	// begin inline asm
	{ atom.add.f64 %fd4791,[%rd1595],%fd10865; }

	// end inline asm
	add.s64 	%rd1596, %rd1594, 16;
	// begin inline asm
	{ atom.add.f64 %fd4793,[%rd1596],%fd10864; }

	// end inline asm
	add.s64 	%rd1597, %rd1594, 24;
	// begin inline asm
	{ atom.add.f64 %fd4795,[%rd1597],%fd10854; }

	// end inline asm
	add.s64 	%rd1598, %rd1594, 32;
	// begin inline asm
	{ atom.add.f64 %fd4797,[%rd1598],%fd10853; }

	// end inline asm
	add.s64 	%rd1599, %rd1594, 40;
	// begin inline asm
	{ atom.add.f64 %fd4799,[%rd1599],%fd10852; }

	// end inline asm
	add.s64 	%rd1600, %rd1594, 48;
	// begin inline asm
	{ atom.add.f64 %fd4801,[%rd1600],%fd10842; }

	// end inline asm
	add.s64 	%rd1601, %rd1594, 56;
	// begin inline asm
	{ atom.add.f64 %fd4803,[%rd1601],%fd10841; }

	// end inline asm
	add.s64 	%rd1602, %rd1594, 64;
	// begin inline asm
	{ atom.add.f64 %fd4805,[%rd1602],%fd10840; }

	// end inline asm
	mul.wide.s32 	%rd1605, %r491, %r498;
	cvta.to.global.u64 	%rd1606, %rd190;
	add.s64 	%rd1607, %rd1606, %rd1605;
	mul.wide.s32 	%rd1608, %r492, %r498;
	add.s64 	%rd1609, %rd1603, %rd1608;
	add.s32 	%r1826, %r401, 2;
	st.global.u32 	[%rd1607], %r1826;
	add.s32 	%r1827, %r402, 2;
	st.global.u32 	[%rd1609], %r1827;

$L__BB1_242:
	ld.param.u64 	%rd193, [%rd27];
	ld.param.u32 	%r500, [%rd27+32];
	ld.param.u64 	%rd194, [%rd27+56];
	ld.param.u32 	%r501, [%rd27+88];
	ld.param.u64 	%rd195, [%rd27+112];
	ld.param.u32 	%r502, [%rd27+144];
	ld.param.u32 	%r503, [%rd27+172];
	ld.param.v2.u32 	{%r1379, %r1380}, [%rd27+176];
	setp.le.s32 	%p376, %r1379, %r479;
	setp.le.s32 	%p377, %r1380, %r432;
	add.s32 	%r507, %r403, 11;
	setp.le.s32 	%p378, %r503, %r507;
	or.pred  	%p379, %p376, %p377;
	or.b32  	%r1381, %r479, %r507;
	or.b32  	%r508, %r1381, %r432;
	setp.lt.s32 	%p380, %r508, 0;
	or.pred  	%p381, %p380, %p379;
	or.pred  	%p382, %p378, %p381;
	@%p382 bra 	$L__BB1_244;
	bra.uni 	$L__BB1_243;

$L__BB1_244:
	add.s32 	%r1832, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1832, %r432};
	add.s32 	%r1833, %r403, 11;
	st.local.v2.u32 	[%rd26+8], {%r1833, %r1379};
	st.local.v2.u32 	[%rd26+16], {%r1380, %r503};
	mov.u64 	%rd1629, $str$1;
	cvta.global.u64 	%rd1630, %rd1629;
	{ // callseq 391, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1630;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1382, [retval0+0];
	} // callseq 391
	bra.uni 	$L__BB1_245;

$L__BB1_243:
	cvta.to.global.u64 	%rd1622, %rd194;
	mul.wide.s32 	%rd1623, %r502, %r507;
	add.s64 	%rd1613, %rd195, %rd1623;
	// begin inline asm
	{ atom.add.f64 %fd4807,[%rd1613],%fd10863; }

	// end inline asm
	add.s64 	%rd1614, %rd1613, 8;
	// begin inline asm
	{ atom.add.f64 %fd4809,[%rd1614],%fd10862; }

	// end inline asm
	add.s64 	%rd1615, %rd1613, 16;
	// begin inline asm
	{ atom.add.f64 %fd4811,[%rd1615],%fd10861; }

	// end inline asm
	add.s64 	%rd1616, %rd1613, 24;
	// begin inline asm
	{ atom.add.f64 %fd4813,[%rd1616],%fd10851; }

	// end inline asm
	add.s64 	%rd1617, %rd1613, 32;
	// begin inline asm
	{ atom.add.f64 %fd4815,[%rd1617],%fd10850; }

	// end inline asm
	add.s64 	%rd1618, %rd1613, 40;
	// begin inline asm
	{ atom.add.f64 %fd4817,[%rd1618],%fd10849; }

	// end inline asm
	add.s64 	%rd1619, %rd1613, 48;
	// begin inline asm
	{ atom.add.f64 %fd4819,[%rd1619],%fd10839; }

	// end inline asm
	add.s64 	%rd1620, %rd1613, 56;
	// begin inline asm
	{ atom.add.f64 %fd4821,[%rd1620],%fd10838; }

	// end inline asm
	add.s64 	%rd1621, %rd1613, 64;
	// begin inline asm
	{ atom.add.f64 %fd4823,[%rd1621],%fd10837; }

	// end inline asm
	mul.wide.s32 	%rd1624, %r500, %r507;
	cvta.to.global.u64 	%rd1625, %rd193;
	add.s64 	%rd1626, %rd1625, %rd1624;
	mul.wide.s32 	%rd1627, %r501, %r507;
	add.s64 	%rd1628, %rd1622, %rd1627;
	add.s32 	%r1830, %r401, 2;
	st.global.u32 	[%rd1626], %r1830;
	add.s32 	%r1831, %r402, 3;
	st.global.u32 	[%rd1628], %r1831;

$L__BB1_245:
	ld.param.u64 	%rd196, [%rd27];
	ld.param.u32 	%r509, [%rd27+32];
	ld.param.u64 	%rd197, [%rd27+56];
	ld.param.u32 	%r510, [%rd27+88];
	ld.param.u64 	%rd198, [%rd27+112];
	ld.param.u32 	%r511, [%rd27+144];
	ld.param.u32 	%r512, [%rd27+172];
	ld.param.v2.u32 	{%r1383, %r1384}, [%rd27+176];
	add.s32 	%r516, %r401, 3;
	setp.le.s32 	%p383, %r1383, %r516;
	setp.le.s32 	%p384, %r1384, %r402;
	add.s32 	%r517, %r403, 12;
	setp.le.s32 	%p385, %r512, %r517;
	or.pred  	%p386, %p383, %p384;
	or.b32  	%r1385, %r402, %r517;
	or.b32  	%r518, %r1385, %r516;
	setp.lt.s32 	%p387, %r518, 0;
	or.pred  	%p388, %p387, %p386;
	or.pred  	%p389, %p385, %p388;
	@%p389 bra 	$L__BB1_247;
	bra.uni 	$L__BB1_246;

$L__BB1_247:
	add.s32 	%r1835, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1835, %r402};
	add.s32 	%r1836, %r403, 12;
	st.local.v2.u32 	[%rd26+8], {%r1836, %r1383};
	st.local.v2.u32 	[%rd26+16], {%r1384, %r512};
	mov.u64 	%rd1648, $str$1;
	cvta.global.u64 	%rd1649, %rd1648;
	{ // callseq 392, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1649;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1386, [retval0+0];
	} // callseq 392
	bra.uni 	$L__BB1_248;

$L__BB1_246:
	cvta.to.global.u64 	%rd1641, %rd197;
	mul.wide.s32 	%rd1642, %r511, %r517;
	add.s64 	%rd1632, %rd198, %rd1642;
	// begin inline asm
	{ atom.add.f64 %fd4825,[%rd1632],%fd10836; }

	// end inline asm
	add.s64 	%rd1633, %rd1632, 8;
	// begin inline asm
	{ atom.add.f64 %fd4827,[%rd1633],%fd10835; }

	// end inline asm
	add.s64 	%rd1634, %rd1632, 16;
	// begin inline asm
	{ atom.add.f64 %fd4829,[%rd1634],%fd10834; }

	// end inline asm
	add.s64 	%rd1635, %rd1632, 24;
	// begin inline asm
	{ atom.add.f64 %fd4831,[%rd1635],%fd10824; }

	// end inline asm
	add.s64 	%rd1636, %rd1632, 32;
	// begin inline asm
	{ atom.add.f64 %fd4833,[%rd1636],%fd10823; }

	// end inline asm
	add.s64 	%rd1637, %rd1632, 40;
	// begin inline asm
	{ atom.add.f64 %fd4835,[%rd1637],%fd10822; }

	// end inline asm
	add.s64 	%rd1638, %rd1632, 48;
	// begin inline asm
	{ atom.add.f64 %fd4837,[%rd1638],%fd10812; }

	// end inline asm
	add.s64 	%rd1639, %rd1632, 56;
	// begin inline asm
	{ atom.add.f64 %fd4839,[%rd1639],%fd10811; }

	// end inline asm
	add.s64 	%rd1640, %rd1632, 64;
	// begin inline asm
	{ atom.add.f64 %fd4841,[%rd1640],%fd10810; }

	// end inline asm
	mul.wide.s32 	%rd1643, %r509, %r517;
	cvta.to.global.u64 	%rd1644, %rd196;
	add.s64 	%rd1645, %rd1644, %rd1643;
	mul.wide.s32 	%rd1646, %r510, %r517;
	add.s64 	%rd1647, %rd1641, %rd1646;
	add.s32 	%r1834, %r401, 3;
	st.global.u32 	[%rd1645], %r1834;
	st.global.u32 	[%rd1647], %r402;

$L__BB1_248:
	ld.param.u64 	%rd199, [%rd27];
	ld.param.u32 	%r519, [%rd27+32];
	ld.param.u64 	%rd200, [%rd27+56];
	ld.param.u32 	%r520, [%rd27+88];
	ld.param.u64 	%rd201, [%rd27+112];
	ld.param.u32 	%r521, [%rd27+144];
	ld.param.u32 	%r522, [%rd27+172];
	ld.param.v2.u32 	{%r1387, %r1388}, [%rd27+176];
	setp.le.s32 	%p390, %r1387, %r516;
	setp.le.s32 	%p391, %r1388, %r412;
	add.s32 	%r526, %r403, 13;
	setp.le.s32 	%p392, %r522, %r526;
	or.pred  	%p393, %p390, %p391;
	or.b32  	%r1389, %r516, %r526;
	or.b32  	%r527, %r1389, %r412;
	setp.lt.s32 	%p394, %r527, 0;
	or.pred  	%p395, %p394, %p393;
	or.pred  	%p396, %p392, %p395;
	@%p396 bra 	$L__BB1_250;
	bra.uni 	$L__BB1_249;

$L__BB1_250:
	add.s32 	%r1839, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1839, %r412};
	add.s32 	%r1840, %r403, 13;
	st.local.v2.u32 	[%rd26+8], {%r1840, %r1387};
	st.local.v2.u32 	[%rd26+16], {%r1388, %r522};
	mov.u64 	%rd1667, $str$1;
	cvta.global.u64 	%rd1668, %rd1667;
	{ // callseq 393, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1668;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1390, [retval0+0];
	} // callseq 393
	bra.uni 	$L__BB1_251;

$L__BB1_249:
	cvta.to.global.u64 	%rd1660, %rd200;
	mul.wide.s32 	%rd1661, %r521, %r526;
	add.s64 	%rd1651, %rd201, %rd1661;
	// begin inline asm
	{ atom.add.f64 %fd4843,[%rd1651],%fd10833; }

	// end inline asm
	add.s64 	%rd1652, %rd1651, 8;
	// begin inline asm
	{ atom.add.f64 %fd4845,[%rd1652],%fd10832; }

	// end inline asm
	add.s64 	%rd1653, %rd1651, 16;
	// begin inline asm
	{ atom.add.f64 %fd4847,[%rd1653],%fd10831; }

	// end inline asm
	add.s64 	%rd1654, %rd1651, 24;
	// begin inline asm
	{ atom.add.f64 %fd4849,[%rd1654],%fd10821; }

	// end inline asm
	add.s64 	%rd1655, %rd1651, 32;
	// begin inline asm
	{ atom.add.f64 %fd4851,[%rd1655],%fd10820; }

	// end inline asm
	add.s64 	%rd1656, %rd1651, 40;
	// begin inline asm
	{ atom.add.f64 %fd4853,[%rd1656],%fd10819; }

	// end inline asm
	add.s64 	%rd1657, %rd1651, 48;
	// begin inline asm
	{ atom.add.f64 %fd4855,[%rd1657],%fd10809; }

	// end inline asm
	add.s64 	%rd1658, %rd1651, 56;
	// begin inline asm
	{ atom.add.f64 %fd4857,[%rd1658],%fd10808; }

	// end inline asm
	add.s64 	%rd1659, %rd1651, 64;
	// begin inline asm
	{ atom.add.f64 %fd4859,[%rd1659],%fd10807; }

	// end inline asm
	mul.wide.s32 	%rd1662, %r519, %r526;
	cvta.to.global.u64 	%rd1663, %rd199;
	add.s64 	%rd1664, %rd1663, %rd1662;
	mul.wide.s32 	%rd1665, %r520, %r526;
	add.s64 	%rd1666, %rd1660, %rd1665;
	add.s32 	%r1837, %r401, 3;
	st.global.u32 	[%rd1664], %r1837;
	add.s32 	%r1838, %r402, 1;
	st.global.u32 	[%rd1666], %r1838;

$L__BB1_251:
	ld.param.u64 	%rd202, [%rd27];
	ld.param.u32 	%r528, [%rd27+32];
	ld.param.u64 	%rd203, [%rd27+56];
	ld.param.u32 	%r529, [%rd27+88];
	ld.param.u64 	%rd204, [%rd27+112];
	ld.param.u32 	%r530, [%rd27+144];
	ld.param.u32 	%r531, [%rd27+172];
	ld.param.v2.u32 	{%r1391, %r1392}, [%rd27+176];
	setp.le.s32 	%p397, %r1391, %r516;
	setp.le.s32 	%p398, %r1392, %r422;
	add.s32 	%r535, %r403, 14;
	setp.le.s32 	%p399, %r531, %r535;
	or.pred  	%p400, %p397, %p398;
	or.b32  	%r1393, %r516, %r535;
	or.b32  	%r536, %r1393, %r422;
	setp.lt.s32 	%p401, %r536, 0;
	or.pred  	%p402, %p401, %p400;
	or.pred  	%p403, %p399, %p402;
	@%p403 bra 	$L__BB1_253;
	bra.uni 	$L__BB1_252;

$L__BB1_253:
	add.s32 	%r1843, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1843, %r422};
	add.s32 	%r1844, %r403, 14;
	st.local.v2.u32 	[%rd26+8], {%r1844, %r1391};
	st.local.v2.u32 	[%rd26+16], {%r1392, %r531};
	mov.u64 	%rd1686, $str$1;
	cvta.global.u64 	%rd1687, %rd1686;
	{ // callseq 394, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1687;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1394, [retval0+0];
	} // callseq 394
	bra.uni 	$L__BB1_254;

$L__BB1_252:
	cvta.to.global.u64 	%rd1679, %rd203;
	mul.wide.s32 	%rd1680, %r530, %r535;
	add.s64 	%rd1670, %rd204, %rd1680;
	// begin inline asm
	{ atom.add.f64 %fd4861,[%rd1670],%fd10830; }

	// end inline asm
	add.s64 	%rd1671, %rd1670, 8;
	// begin inline asm
	{ atom.add.f64 %fd4863,[%rd1671],%fd10829; }

	// end inline asm
	add.s64 	%rd1672, %rd1670, 16;
	// begin inline asm
	{ atom.add.f64 %fd4865,[%rd1672],%fd10828; }

	// end inline asm
	add.s64 	%rd1673, %rd1670, 24;
	// begin inline asm
	{ atom.add.f64 %fd4867,[%rd1673],%fd10818; }

	// end inline asm
	add.s64 	%rd1674, %rd1670, 32;
	// begin inline asm
	{ atom.add.f64 %fd4869,[%rd1674],%fd10817; }

	// end inline asm
	add.s64 	%rd1675, %rd1670, 40;
	// begin inline asm
	{ atom.add.f64 %fd4871,[%rd1675],%fd10816; }

	// end inline asm
	add.s64 	%rd1676, %rd1670, 48;
	// begin inline asm
	{ atom.add.f64 %fd4873,[%rd1676],%fd10806; }

	// end inline asm
	add.s64 	%rd1677, %rd1670, 56;
	// begin inline asm
	{ atom.add.f64 %fd4875,[%rd1677],%fd10805; }

	// end inline asm
	add.s64 	%rd1678, %rd1670, 64;
	// begin inline asm
	{ atom.add.f64 %fd4877,[%rd1678],%fd10804; }

	// end inline asm
	mul.wide.s32 	%rd1681, %r528, %r535;
	cvta.to.global.u64 	%rd1682, %rd202;
	add.s64 	%rd1683, %rd1682, %rd1681;
	mul.wide.s32 	%rd1684, %r529, %r535;
	add.s64 	%rd1685, %rd1679, %rd1684;
	add.s32 	%r1841, %r401, 3;
	st.global.u32 	[%rd1683], %r1841;
	add.s32 	%r1842, %r402, 2;
	st.global.u32 	[%rd1685], %r1842;

$L__BB1_254:
	ld.param.u64 	%rd205, [%rd27];
	ld.param.u32 	%r537, [%rd27+32];
	ld.param.u64 	%rd206, [%rd27+56];
	ld.param.u32 	%r538, [%rd27+88];
	ld.param.u64 	%rd207, [%rd27+112];
	ld.param.u32 	%r539, [%rd27+144];
	ld.param.u32 	%r540, [%rd27+172];
	ld.param.v2.u32 	{%r1395, %r1396}, [%rd27+176];
	setp.le.s32 	%p404, %r1395, %r516;
	setp.le.s32 	%p405, %r1396, %r432;
	add.s32 	%r544, %r403, 15;
	setp.le.s32 	%p406, %r540, %r544;
	or.pred  	%p407, %p404, %p405;
	or.b32  	%r1397, %r516, %r544;
	or.b32  	%r545, %r1397, %r432;
	setp.lt.s32 	%p408, %r545, 0;
	or.pred  	%p409, %p408, %p407;
	or.pred  	%p410, %p406, %p409;
	@%p410 bra 	$L__BB1_256;
	bra.uni 	$L__BB1_255;

$L__BB1_256:
	add.s32 	%r1847, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1847, %r432};
	add.s32 	%r1848, %r403, 15;
	st.local.v2.u32 	[%rd26+8], {%r1848, %r1395};
	st.local.v2.u32 	[%rd26+16], {%r1396, %r540};
	mov.u64 	%rd1705, $str$1;
	cvta.global.u64 	%rd1706, %rd1705;
	{ // callseq 395, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1706;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1398, [retval0+0];
	} // callseq 395
	bra.uni 	$L__BB1_257;

$L__BB1_255:
	cvta.to.global.u64 	%rd1698, %rd206;
	mul.wide.s32 	%rd1699, %r539, %r544;
	add.s64 	%rd1689, %rd207, %rd1699;
	// begin inline asm
	{ atom.add.f64 %fd4879,[%rd1689],%fd10827; }

	// end inline asm
	add.s64 	%rd1690, %rd1689, 8;
	// begin inline asm
	{ atom.add.f64 %fd4881,[%rd1690],%fd10826; }

	// end inline asm
	add.s64 	%rd1691, %rd1689, 16;
	// begin inline asm
	{ atom.add.f64 %fd4883,[%rd1691],%fd10825; }

	// end inline asm
	add.s64 	%rd1692, %rd1689, 24;
	// begin inline asm
	{ atom.add.f64 %fd4885,[%rd1692],%fd10815; }

	// end inline asm
	add.s64 	%rd1693, %rd1689, 32;
	// begin inline asm
	{ atom.add.f64 %fd4887,[%rd1693],%fd10814; }

	// end inline asm
	add.s64 	%rd1694, %rd1689, 40;
	// begin inline asm
	{ atom.add.f64 %fd4889,[%rd1694],%fd10813; }

	// end inline asm
	add.s64 	%rd1695, %rd1689, 48;
	// begin inline asm
	{ atom.add.f64 %fd4891,[%rd1695],%fd10803; }

	// end inline asm
	add.s64 	%rd1696, %rd1689, 56;
	// begin inline asm
	{ atom.add.f64 %fd4893,[%rd1696],%fd10802; }

	// end inline asm
	add.s64 	%rd1697, %rd1689, 64;
	// begin inline asm
	{ atom.add.f64 %fd4895,[%rd1697],%fd10801; }

	// end inline asm
	mul.wide.s32 	%rd1700, %r537, %r544;
	cvta.to.global.u64 	%rd1701, %rd205;
	add.s64 	%rd1702, %rd1701, %rd1700;
	mul.wide.s32 	%rd1703, %r538, %r544;
	add.s64 	%rd1704, %rd1698, %rd1703;
	add.s32 	%r1845, %r401, 3;
	st.global.u32 	[%rd1702], %r1845;
	add.s32 	%r1846, %r402, 3;
	st.global.u32 	[%rd1704], %r1846;

$L__BB1_257:
	ld.param.u64 	%rd209, [%rd27+120];
	ld.param.u32 	%r546, [%rd27+144];
	ld.param.u32 	%r547, [%rd27+172];
	ld.param.v2.u32 	{%r1399, %r1400}, [%rd27+176];
	setp.le.s32 	%p412, %r1399, %r516;
	setp.le.s32 	%p413, %r1400, %r432;
	setp.le.s32 	%p414, %r547, %r544;
	or.pred  	%p415, %p412, %p413;
	or.pred  	%p416, %p408, %p415;
	or.pred  	%p417, %p414, %p416;
	mov.f64 	%fd10945, 0d0000000000000000;
	mov.f64 	%fd10946, 0d0000000000000000;
	mov.f64 	%fd10947, 0d0000000000000000;
	mov.f64 	%fd10948, 0d0000000000000000;
	mov.f64 	%fd10949, 0d0000000000000000;
	mov.f64 	%fd10950, 0d0000000000000000;
	mov.f64 	%fd10951, 0d0000000000000000;
	mov.f64 	%fd10952, 0d0000000000000000;
	mov.f64 	%fd10953, 0d0000000000000000;
	@%p417 bra 	$L__BB1_260;
	bra.uni 	$L__BB1_258;

$L__BB1_260:
	add.s32 	%r1849, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1849, %r432};
	add.s32 	%r1850, %r403, 15;
	st.local.v2.u32 	[%rd26+8], {%r1850, %r1399};
	st.local.v2.u32 	[%rd26+16], {%r1400, %r547};
	mov.u64 	%rd1711, $str$1;
	cvta.global.u64 	%rd1712, %rd1711;
	{ // callseq 396, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1712;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1401, [retval0+0];
	} // callseq 396
	bra.uni 	$L__BB1_261;

$L__BB1_258:
	setp.eq.s64 	%p418, %rd209, 0;
	@%p418 bra 	$L__BB1_261;

	cvta.to.global.u64 	%rd1708, %rd209;
	mul.wide.s32 	%rd1709, %r546, %r544;
	add.s64 	%rd1710, %rd1708, %rd1709;
	ld.global.f64 	%fd4906, [%rd1710];
	add.f64 	%fd10945, %fd4906, 0d0000000000000000;
	ld.global.f64 	%fd4907, [%rd1710+8];
	add.f64 	%fd10946, %fd4907, 0d0000000000000000;
	ld.global.f64 	%fd4908, [%rd1710+16];
	add.f64 	%fd10947, %fd4908, 0d0000000000000000;
	ld.global.f64 	%fd4909, [%rd1710+24];
	add.f64 	%fd10948, %fd4909, 0d0000000000000000;
	ld.global.f64 	%fd4910, [%rd1710+32];
	add.f64 	%fd10949, %fd4910, 0d0000000000000000;
	ld.global.f64 	%fd4911, [%rd1710+40];
	add.f64 	%fd10950, %fd4911, 0d0000000000000000;
	ld.global.f64 	%fd4912, [%rd1710+48];
	add.f64 	%fd10951, %fd4912, 0d0000000000000000;
	ld.global.f64 	%fd4913, [%rd1710+56];
	add.f64 	%fd10952, %fd4913, 0d0000000000000000;
	ld.global.f64 	%fd4914, [%rd1710+64];
	add.f64 	%fd10953, %fd4914, 0d0000000000000000;

$L__BB1_261:
	add.f64 	%fd1171, %fd10953, 0d0000000000000000;
	add.f64 	%fd1172, %fd10952, 0d0000000000000000;
	add.f64 	%fd1173, %fd10951, 0d0000000000000000;
	add.f64 	%fd1174, %fd10950, 0d0000000000000000;
	add.f64 	%fd1175, %fd10949, 0d0000000000000000;
	add.f64 	%fd1176, %fd10948, 0d0000000000000000;
	add.f64 	%fd1177, %fd10947, 0d0000000000000000;
	add.f64 	%fd1178, %fd10946, 0d0000000000000000;
	add.f64 	%fd1179, %fd10945, 0d0000000000000000;
	ld.param.u64 	%rd210, [%rd27+120];
	ld.param.u32 	%r551, [%rd27+144];
	ld.param.u32 	%r552, [%rd27+172];
	ld.param.v2.u32 	{%r1402, %r1403}, [%rd27+176];
	setp.le.s32 	%p419, %r1402, %r516;
	setp.le.s32 	%p420, %r1403, %r422;
	setp.le.s32 	%p421, %r552, %r535;
	or.pred  	%p422, %p419, %p420;
	or.pred  	%p424, %p401, %p422;
	or.pred  	%p425, %p421, %p424;
	mov.f64 	%fd10954, 0d0000000000000000;
	mov.f64 	%fd10955, 0d0000000000000000;
	mov.f64 	%fd10956, 0d0000000000000000;
	mov.f64 	%fd10957, 0d0000000000000000;
	mov.f64 	%fd10958, 0d0000000000000000;
	mov.f64 	%fd10959, 0d0000000000000000;
	mov.f64 	%fd10960, 0d0000000000000000;
	mov.f64 	%fd10961, 0d0000000000000000;
	mov.f64 	%fd10962, 0d0000000000000000;
	@%p425 bra 	$L__BB1_264;
	bra.uni 	$L__BB1_262;

$L__BB1_264:
	add.s32 	%r1851, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1851, %r422};
	add.s32 	%r1852, %r403, 14;
	st.local.v2.u32 	[%rd26+8], {%r1852, %r1402};
	st.local.v2.u32 	[%rd26+16], {%r1403, %r552};
	mov.u64 	%rd1717, $str$1;
	cvta.global.u64 	%rd1718, %rd1717;
	{ // callseq 397, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1718;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1404, [retval0+0];
	} // callseq 397
	bra.uni 	$L__BB1_265;

$L__BB1_262:
	setp.eq.s64 	%p426, %rd210, 0;
	@%p426 bra 	$L__BB1_265;

	cvta.to.global.u64 	%rd1714, %rd210;
	mul.wide.s32 	%rd1715, %r551, %r535;
	add.s64 	%rd1716, %rd1714, %rd1715;
	ld.global.f64 	%fd4933, [%rd1716];
	add.f64 	%fd10954, %fd4933, 0d0000000000000000;
	ld.global.f64 	%fd4934, [%rd1716+8];
	add.f64 	%fd10955, %fd4934, 0d0000000000000000;
	ld.global.f64 	%fd4935, [%rd1716+16];
	add.f64 	%fd10956, %fd4935, 0d0000000000000000;
	ld.global.f64 	%fd4936, [%rd1716+24];
	add.f64 	%fd10957, %fd4936, 0d0000000000000000;
	ld.global.f64 	%fd4937, [%rd1716+32];
	add.f64 	%fd10958, %fd4937, 0d0000000000000000;
	ld.global.f64 	%fd4938, [%rd1716+40];
	add.f64 	%fd10959, %fd4938, 0d0000000000000000;
	ld.global.f64 	%fd4939, [%rd1716+48];
	add.f64 	%fd10960, %fd4939, 0d0000000000000000;
	ld.global.f64 	%fd4940, [%rd1716+56];
	add.f64 	%fd10961, %fd4940, 0d0000000000000000;
	ld.global.f64 	%fd4941, [%rd1716+64];
	add.f64 	%fd10962, %fd4941, 0d0000000000000000;

$L__BB1_265:
	add.f64 	%fd1198, %fd10962, 0d0000000000000000;
	add.f64 	%fd1199, %fd10961, 0d0000000000000000;
	add.f64 	%fd1200, %fd10960, 0d0000000000000000;
	add.f64 	%fd1201, %fd10959, 0d0000000000000000;
	add.f64 	%fd1202, %fd10958, 0d0000000000000000;
	add.f64 	%fd1203, %fd10957, 0d0000000000000000;
	add.f64 	%fd1204, %fd10956, 0d0000000000000000;
	add.f64 	%fd1205, %fd10955, 0d0000000000000000;
	add.f64 	%fd1206, %fd10954, 0d0000000000000000;
	ld.param.u64 	%rd211, [%rd27+120];
	ld.param.u32 	%r556, [%rd27+144];
	ld.param.u32 	%r557, [%rd27+172];
	ld.param.v2.u32 	{%r1405, %r1406}, [%rd27+176];
	setp.le.s32 	%p427, %r1405, %r516;
	setp.le.s32 	%p428, %r1406, %r412;
	setp.le.s32 	%p429, %r557, %r526;
	or.pred  	%p430, %p427, %p428;
	or.pred  	%p432, %p394, %p430;
	or.pred  	%p433, %p429, %p432;
	mov.f64 	%fd10963, 0d0000000000000000;
	mov.f64 	%fd10964, 0d0000000000000000;
	mov.f64 	%fd10965, 0d0000000000000000;
	mov.f64 	%fd10966, 0d0000000000000000;
	mov.f64 	%fd10967, 0d0000000000000000;
	mov.f64 	%fd10968, 0d0000000000000000;
	mov.f64 	%fd10969, 0d0000000000000000;
	mov.f64 	%fd10970, 0d0000000000000000;
	mov.f64 	%fd10971, 0d0000000000000000;
	@%p433 bra 	$L__BB1_268;
	bra.uni 	$L__BB1_266;

$L__BB1_268:
	add.s32 	%r1853, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1853, %r412};
	add.s32 	%r1854, %r403, 13;
	st.local.v2.u32 	[%rd26+8], {%r1854, %r1405};
	st.local.v2.u32 	[%rd26+16], {%r1406, %r557};
	mov.u64 	%rd1723, $str$1;
	cvta.global.u64 	%rd1724, %rd1723;
	{ // callseq 398, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1724;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1407, [retval0+0];
	} // callseq 398
	bra.uni 	$L__BB1_269;

$L__BB1_266:
	setp.eq.s64 	%p434, %rd211, 0;
	@%p434 bra 	$L__BB1_269;

	cvta.to.global.u64 	%rd1720, %rd211;
	mul.wide.s32 	%rd1721, %r556, %r526;
	add.s64 	%rd1722, %rd1720, %rd1721;
	ld.global.f64 	%fd4960, [%rd1722];
	add.f64 	%fd10963, %fd4960, 0d0000000000000000;
	ld.global.f64 	%fd4961, [%rd1722+8];
	add.f64 	%fd10964, %fd4961, 0d0000000000000000;
	ld.global.f64 	%fd4962, [%rd1722+16];
	add.f64 	%fd10965, %fd4962, 0d0000000000000000;
	ld.global.f64 	%fd4963, [%rd1722+24];
	add.f64 	%fd10966, %fd4963, 0d0000000000000000;
	ld.global.f64 	%fd4964, [%rd1722+32];
	add.f64 	%fd10967, %fd4964, 0d0000000000000000;
	ld.global.f64 	%fd4965, [%rd1722+40];
	add.f64 	%fd10968, %fd4965, 0d0000000000000000;
	ld.global.f64 	%fd4966, [%rd1722+48];
	add.f64 	%fd10969, %fd4966, 0d0000000000000000;
	ld.global.f64 	%fd4967, [%rd1722+56];
	add.f64 	%fd10970, %fd4967, 0d0000000000000000;
	ld.global.f64 	%fd4968, [%rd1722+64];
	add.f64 	%fd10971, %fd4968, 0d0000000000000000;

$L__BB1_269:
	add.f64 	%fd1225, %fd10971, 0d0000000000000000;
	add.f64 	%fd1226, %fd10970, 0d0000000000000000;
	add.f64 	%fd1227, %fd10969, 0d0000000000000000;
	add.f64 	%fd1228, %fd10968, 0d0000000000000000;
	add.f64 	%fd1229, %fd10967, 0d0000000000000000;
	add.f64 	%fd1230, %fd10966, 0d0000000000000000;
	add.f64 	%fd1231, %fd10965, 0d0000000000000000;
	add.f64 	%fd1232, %fd10964, 0d0000000000000000;
	add.f64 	%fd1233, %fd10963, 0d0000000000000000;
	ld.param.u64 	%rd212, [%rd27+120];
	ld.param.u32 	%r561, [%rd27+144];
	ld.param.u32 	%r562, [%rd27+172];
	ld.param.v2.u32 	{%r1408, %r1409}, [%rd27+176];
	setp.le.s32 	%p435, %r1408, %r516;
	setp.le.s32 	%p436, %r1409, %r402;
	setp.le.s32 	%p437, %r562, %r517;
	or.pred  	%p438, %p435, %p436;
	or.pred  	%p440, %p387, %p438;
	or.pred  	%p441, %p437, %p440;
	mov.f64 	%fd10972, 0d0000000000000000;
	mov.f64 	%fd10973, 0d0000000000000000;
	mov.f64 	%fd10974, 0d0000000000000000;
	mov.f64 	%fd10975, 0d0000000000000000;
	mov.f64 	%fd10976, 0d0000000000000000;
	mov.f64 	%fd10977, 0d0000000000000000;
	mov.f64 	%fd10978, 0d0000000000000000;
	mov.f64 	%fd10979, 0d0000000000000000;
	mov.f64 	%fd10980, 0d0000000000000000;
	@%p441 bra 	$L__BB1_272;
	bra.uni 	$L__BB1_270;

$L__BB1_272:
	add.s32 	%r1855, %r401, 3;
	st.local.v2.u32 	[%rd26], {%r1855, %r402};
	add.s32 	%r1856, %r403, 12;
	st.local.v2.u32 	[%rd26+8], {%r1856, %r1408};
	st.local.v2.u32 	[%rd26+16], {%r1409, %r562};
	mov.u64 	%rd1729, $str$1;
	cvta.global.u64 	%rd1730, %rd1729;
	{ // callseq 399, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1730;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1410, [retval0+0];
	} // callseq 399
	bra.uni 	$L__BB1_273;

$L__BB1_270:
	setp.eq.s64 	%p442, %rd212, 0;
	@%p442 bra 	$L__BB1_273;

	cvta.to.global.u64 	%rd1726, %rd212;
	mul.wide.s32 	%rd1727, %r561, %r517;
	add.s64 	%rd1728, %rd1726, %rd1727;
	ld.global.f64 	%fd4987, [%rd1728];
	add.f64 	%fd10972, %fd4987, 0d0000000000000000;
	ld.global.f64 	%fd4988, [%rd1728+8];
	add.f64 	%fd10973, %fd4988, 0d0000000000000000;
	ld.global.f64 	%fd4989, [%rd1728+16];
	add.f64 	%fd10974, %fd4989, 0d0000000000000000;
	ld.global.f64 	%fd4990, [%rd1728+24];
	add.f64 	%fd10975, %fd4990, 0d0000000000000000;
	ld.global.f64 	%fd4991, [%rd1728+32];
	add.f64 	%fd10976, %fd4991, 0d0000000000000000;
	ld.global.f64 	%fd4992, [%rd1728+40];
	add.f64 	%fd10977, %fd4992, 0d0000000000000000;
	ld.global.f64 	%fd4993, [%rd1728+48];
	add.f64 	%fd10978, %fd4993, 0d0000000000000000;
	ld.global.f64 	%fd4994, [%rd1728+56];
	add.f64 	%fd10979, %fd4994, 0d0000000000000000;
	ld.global.f64 	%fd4995, [%rd1728+64];
	add.f64 	%fd10980, %fd4995, 0d0000000000000000;

$L__BB1_273:
	add.f64 	%fd1252, %fd10980, 0d0000000000000000;
	add.f64 	%fd1253, %fd10979, 0d0000000000000000;
	add.f64 	%fd1254, %fd10978, 0d0000000000000000;
	add.f64 	%fd1255, %fd10977, 0d0000000000000000;
	add.f64 	%fd1256, %fd10976, 0d0000000000000000;
	add.f64 	%fd1257, %fd10975, 0d0000000000000000;
	add.f64 	%fd1258, %fd10974, 0d0000000000000000;
	add.f64 	%fd1259, %fd10973, 0d0000000000000000;
	add.f64 	%fd1260, %fd10972, 0d0000000000000000;
	ld.param.u64 	%rd213, [%rd27+120];
	ld.param.u32 	%r566, [%rd27+144];
	ld.param.u32 	%r567, [%rd27+172];
	ld.param.v2.u32 	{%r1411, %r1412}, [%rd27+176];
	setp.le.s32 	%p443, %r1411, %r479;
	setp.le.s32 	%p444, %r1412, %r432;
	setp.le.s32 	%p445, %r567, %r507;
	or.pred  	%p446, %p443, %p444;
	or.pred  	%p448, %p380, %p446;
	or.pred  	%p449, %p445, %p448;
	mov.f64 	%fd10981, 0d0000000000000000;
	mov.f64 	%fd10982, 0d0000000000000000;
	mov.f64 	%fd10983, 0d0000000000000000;
	mov.f64 	%fd10984, 0d0000000000000000;
	mov.f64 	%fd10985, 0d0000000000000000;
	mov.f64 	%fd10986, 0d0000000000000000;
	mov.f64 	%fd10987, 0d0000000000000000;
	mov.f64 	%fd10988, 0d0000000000000000;
	mov.f64 	%fd10989, 0d0000000000000000;
	@%p449 bra 	$L__BB1_276;
	bra.uni 	$L__BB1_274;

$L__BB1_276:
	add.s32 	%r1857, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1857, %r432};
	add.s32 	%r1858, %r403, 11;
	st.local.v2.u32 	[%rd26+8], {%r1858, %r1411};
	st.local.v2.u32 	[%rd26+16], {%r1412, %r567};
	mov.u64 	%rd1735, $str$1;
	cvta.global.u64 	%rd1736, %rd1735;
	{ // callseq 400, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1736;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1413, [retval0+0];
	} // callseq 400
	bra.uni 	$L__BB1_277;

$L__BB1_274:
	setp.eq.s64 	%p450, %rd213, 0;
	@%p450 bra 	$L__BB1_277;

	cvta.to.global.u64 	%rd1732, %rd213;
	mul.wide.s32 	%rd1733, %r566, %r507;
	add.s64 	%rd1734, %rd1732, %rd1733;
	ld.global.f64 	%fd5014, [%rd1734];
	add.f64 	%fd10981, %fd5014, 0d0000000000000000;
	ld.global.f64 	%fd5015, [%rd1734+8];
	add.f64 	%fd10982, %fd5015, 0d0000000000000000;
	ld.global.f64 	%fd5016, [%rd1734+16];
	add.f64 	%fd10983, %fd5016, 0d0000000000000000;
	ld.global.f64 	%fd5017, [%rd1734+24];
	add.f64 	%fd10984, %fd5017, 0d0000000000000000;
	ld.global.f64 	%fd5018, [%rd1734+32];
	add.f64 	%fd10985, %fd5018, 0d0000000000000000;
	ld.global.f64 	%fd5019, [%rd1734+40];
	add.f64 	%fd10986, %fd5019, 0d0000000000000000;
	ld.global.f64 	%fd5020, [%rd1734+48];
	add.f64 	%fd10987, %fd5020, 0d0000000000000000;
	ld.global.f64 	%fd5021, [%rd1734+56];
	add.f64 	%fd10988, %fd5021, 0d0000000000000000;
	ld.global.f64 	%fd5022, [%rd1734+64];
	add.f64 	%fd10989, %fd5022, 0d0000000000000000;

$L__BB1_277:
	add.f64 	%fd1279, %fd10989, 0d0000000000000000;
	add.f64 	%fd1280, %fd10988, 0d0000000000000000;
	add.f64 	%fd1281, %fd10987, 0d0000000000000000;
	add.f64 	%fd1282, %fd10986, 0d0000000000000000;
	add.f64 	%fd1283, %fd10985, 0d0000000000000000;
	add.f64 	%fd1284, %fd10984, 0d0000000000000000;
	add.f64 	%fd1285, %fd10983, 0d0000000000000000;
	add.f64 	%fd1286, %fd10982, 0d0000000000000000;
	add.f64 	%fd1287, %fd10981, 0d0000000000000000;
	ld.param.u64 	%rd214, [%rd27+120];
	ld.param.u32 	%r571, [%rd27+144];
	ld.param.u32 	%r572, [%rd27+172];
	ld.param.v2.u32 	{%r1414, %r1415}, [%rd27+176];
	setp.le.s32 	%p451, %r1414, %r479;
	setp.le.s32 	%p452, %r1415, %r422;
	setp.le.s32 	%p453, %r572, %r498;
	or.pred  	%p454, %p451, %p452;
	or.pred  	%p456, %p373, %p454;
	or.pred  	%p457, %p453, %p456;
	mov.f64 	%fd10990, 0d0000000000000000;
	mov.f64 	%fd10991, 0d0000000000000000;
	mov.f64 	%fd10992, 0d0000000000000000;
	mov.f64 	%fd10993, 0d0000000000000000;
	mov.f64 	%fd10994, 0d0000000000000000;
	mov.f64 	%fd10995, 0d0000000000000000;
	mov.f64 	%fd10996, 0d0000000000000000;
	mov.f64 	%fd10997, 0d0000000000000000;
	mov.f64 	%fd10998, 0d0000000000000000;
	@%p457 bra 	$L__BB1_280;
	bra.uni 	$L__BB1_278;

$L__BB1_280:
	add.s32 	%r1859, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1859, %r422};
	add.s32 	%r1860, %r403, 10;
	st.local.v2.u32 	[%rd26+8], {%r1860, %r1414};
	st.local.v2.u32 	[%rd26+16], {%r1415, %r572};
	mov.u64 	%rd1741, $str$1;
	cvta.global.u64 	%rd1742, %rd1741;
	{ // callseq 401, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1742;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1416, [retval0+0];
	} // callseq 401
	bra.uni 	$L__BB1_281;

$L__BB1_278:
	setp.eq.s64 	%p458, %rd214, 0;
	@%p458 bra 	$L__BB1_281;

	cvta.to.global.u64 	%rd1738, %rd214;
	mul.wide.s32 	%rd1739, %r571, %r498;
	add.s64 	%rd1740, %rd1738, %rd1739;
	ld.global.f64 	%fd5041, [%rd1740];
	add.f64 	%fd10990, %fd5041, 0d0000000000000000;
	ld.global.f64 	%fd5042, [%rd1740+8];
	add.f64 	%fd10991, %fd5042, 0d0000000000000000;
	ld.global.f64 	%fd5043, [%rd1740+16];
	add.f64 	%fd10992, %fd5043, 0d0000000000000000;
	ld.global.f64 	%fd5044, [%rd1740+24];
	add.f64 	%fd10993, %fd5044, 0d0000000000000000;
	ld.global.f64 	%fd5045, [%rd1740+32];
	add.f64 	%fd10994, %fd5045, 0d0000000000000000;
	ld.global.f64 	%fd5046, [%rd1740+40];
	add.f64 	%fd10995, %fd5046, 0d0000000000000000;
	ld.global.f64 	%fd5047, [%rd1740+48];
	add.f64 	%fd10996, %fd5047, 0d0000000000000000;
	ld.global.f64 	%fd5048, [%rd1740+56];
	add.f64 	%fd10997, %fd5048, 0d0000000000000000;
	ld.global.f64 	%fd5049, [%rd1740+64];
	add.f64 	%fd10998, %fd5049, 0d0000000000000000;

$L__BB1_281:
	add.f64 	%fd1306, %fd10998, 0d0000000000000000;
	add.f64 	%fd1307, %fd10997, 0d0000000000000000;
	add.f64 	%fd1308, %fd10996, 0d0000000000000000;
	add.f64 	%fd1309, %fd10995, 0d0000000000000000;
	add.f64 	%fd1310, %fd10994, 0d0000000000000000;
	add.f64 	%fd1311, %fd10993, 0d0000000000000000;
	add.f64 	%fd1312, %fd10992, 0d0000000000000000;
	add.f64 	%fd1313, %fd10991, 0d0000000000000000;
	add.f64 	%fd1314, %fd10990, 0d0000000000000000;
	ld.param.u64 	%rd215, [%rd27+120];
	ld.param.u32 	%r576, [%rd27+144];
	ld.param.u32 	%r577, [%rd27+172];
	ld.param.v2.u32 	{%r1417, %r1418}, [%rd27+176];
	setp.le.s32 	%p459, %r1417, %r479;
	setp.le.s32 	%p460, %r1418, %r412;
	setp.le.s32 	%p461, %r577, %r489;
	or.pred  	%p462, %p459, %p460;
	or.pred  	%p464, %p366, %p462;
	or.pred  	%p465, %p461, %p464;
	mov.f64 	%fd10999, 0d0000000000000000;
	mov.f64 	%fd11000, 0d0000000000000000;
	mov.f64 	%fd11001, 0d0000000000000000;
	mov.f64 	%fd11002, 0d0000000000000000;
	mov.f64 	%fd11003, 0d0000000000000000;
	mov.f64 	%fd11004, 0d0000000000000000;
	mov.f64 	%fd11005, 0d0000000000000000;
	mov.f64 	%fd11006, 0d0000000000000000;
	mov.f64 	%fd11007, 0d0000000000000000;
	@%p465 bra 	$L__BB1_284;
	bra.uni 	$L__BB1_282;

$L__BB1_284:
	add.s32 	%r1861, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1861, %r412};
	add.s32 	%r1862, %r403, 9;
	st.local.v2.u32 	[%rd26+8], {%r1862, %r1417};
	st.local.v2.u32 	[%rd26+16], {%r1418, %r577};
	mov.u64 	%rd1747, $str$1;
	cvta.global.u64 	%rd1748, %rd1747;
	{ // callseq 402, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1748;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1419, [retval0+0];
	} // callseq 402
	bra.uni 	$L__BB1_285;

$L__BB1_282:
	setp.eq.s64 	%p466, %rd215, 0;
	@%p466 bra 	$L__BB1_285;

	cvta.to.global.u64 	%rd1744, %rd215;
	mul.wide.s32 	%rd1745, %r576, %r489;
	add.s64 	%rd1746, %rd1744, %rd1745;
	ld.global.f64 	%fd5068, [%rd1746];
	add.f64 	%fd10999, %fd5068, 0d0000000000000000;
	ld.global.f64 	%fd5069, [%rd1746+8];
	add.f64 	%fd11000, %fd5069, 0d0000000000000000;
	ld.global.f64 	%fd5070, [%rd1746+16];
	add.f64 	%fd11001, %fd5070, 0d0000000000000000;
	ld.global.f64 	%fd5071, [%rd1746+24];
	add.f64 	%fd11002, %fd5071, 0d0000000000000000;
	ld.global.f64 	%fd5072, [%rd1746+32];
	add.f64 	%fd11003, %fd5072, 0d0000000000000000;
	ld.global.f64 	%fd5073, [%rd1746+40];
	add.f64 	%fd11004, %fd5073, 0d0000000000000000;
	ld.global.f64 	%fd5074, [%rd1746+48];
	add.f64 	%fd11005, %fd5074, 0d0000000000000000;
	ld.global.f64 	%fd5075, [%rd1746+56];
	add.f64 	%fd11006, %fd5075, 0d0000000000000000;
	ld.global.f64 	%fd5076, [%rd1746+64];
	add.f64 	%fd11007, %fd5076, 0d0000000000000000;

$L__BB1_285:
	add.f64 	%fd1333, %fd11007, 0d0000000000000000;
	add.f64 	%fd1334, %fd11006, 0d0000000000000000;
	add.f64 	%fd1335, %fd11005, 0d0000000000000000;
	add.f64 	%fd1336, %fd11004, 0d0000000000000000;
	add.f64 	%fd1337, %fd11003, 0d0000000000000000;
	add.f64 	%fd1338, %fd11002, 0d0000000000000000;
	add.f64 	%fd1339, %fd11001, 0d0000000000000000;
	add.f64 	%fd1340, %fd11000, 0d0000000000000000;
	add.f64 	%fd1341, %fd10999, 0d0000000000000000;
	ld.param.u64 	%rd216, [%rd27+120];
	ld.param.u32 	%r581, [%rd27+144];
	ld.param.u32 	%r582, [%rd27+172];
	ld.param.v2.u32 	{%r1420, %r1421}, [%rd27+176];
	setp.le.s32 	%p467, %r1420, %r479;
	setp.le.s32 	%p468, %r1421, %r402;
	setp.le.s32 	%p469, %r582, %r480;
	or.pred  	%p470, %p467, %p468;
	or.pred  	%p472, %p359, %p470;
	or.pred  	%p473, %p469, %p472;
	mov.f64 	%fd11008, 0d0000000000000000;
	mov.f64 	%fd11009, 0d0000000000000000;
	mov.f64 	%fd11010, 0d0000000000000000;
	mov.f64 	%fd11011, 0d0000000000000000;
	mov.f64 	%fd11012, 0d0000000000000000;
	mov.f64 	%fd11013, 0d0000000000000000;
	mov.f64 	%fd11014, 0d0000000000000000;
	mov.f64 	%fd11015, 0d0000000000000000;
	mov.f64 	%fd11016, 0d0000000000000000;
	@%p473 bra 	$L__BB1_288;
	bra.uni 	$L__BB1_286;

$L__BB1_288:
	add.s32 	%r1863, %r401, 2;
	st.local.v2.u32 	[%rd26], {%r1863, %r402};
	add.s32 	%r1864, %r403, 8;
	st.local.v2.u32 	[%rd26+8], {%r1864, %r1420};
	st.local.v2.u32 	[%rd26+16], {%r1421, %r582};
	mov.u64 	%rd1753, $str$1;
	cvta.global.u64 	%rd1754, %rd1753;
	{ // callseq 403, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1754;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1422, [retval0+0];
	} // callseq 403
	bra.uni 	$L__BB1_289;

$L__BB1_286:
	setp.eq.s64 	%p474, %rd216, 0;
	@%p474 bra 	$L__BB1_289;

	cvta.to.global.u64 	%rd1750, %rd216;
	mul.wide.s32 	%rd1751, %r581, %r480;
	add.s64 	%rd1752, %rd1750, %rd1751;
	ld.global.f64 	%fd5095, [%rd1752];
	add.f64 	%fd11008, %fd5095, 0d0000000000000000;
	ld.global.f64 	%fd5096, [%rd1752+8];
	add.f64 	%fd11009, %fd5096, 0d0000000000000000;
	ld.global.f64 	%fd5097, [%rd1752+16];
	add.f64 	%fd11010, %fd5097, 0d0000000000000000;
	ld.global.f64 	%fd5098, [%rd1752+24];
	add.f64 	%fd11011, %fd5098, 0d0000000000000000;
	ld.global.f64 	%fd5099, [%rd1752+32];
	add.f64 	%fd11012, %fd5099, 0d0000000000000000;
	ld.global.f64 	%fd5100, [%rd1752+40];
	add.f64 	%fd11013, %fd5100, 0d0000000000000000;
	ld.global.f64 	%fd5101, [%rd1752+48];
	add.f64 	%fd11014, %fd5101, 0d0000000000000000;
	ld.global.f64 	%fd5102, [%rd1752+56];
	add.f64 	%fd11015, %fd5102, 0d0000000000000000;
	ld.global.f64 	%fd5103, [%rd1752+64];
	add.f64 	%fd11016, %fd5103, 0d0000000000000000;

$L__BB1_289:
	add.f64 	%fd1360, %fd11016, 0d0000000000000000;
	add.f64 	%fd1361, %fd11015, 0d0000000000000000;
	add.f64 	%fd1362, %fd11014, 0d0000000000000000;
	add.f64 	%fd1363, %fd11013, 0d0000000000000000;
	add.f64 	%fd1364, %fd11012, 0d0000000000000000;
	add.f64 	%fd1365, %fd11011, 0d0000000000000000;
	add.f64 	%fd1366, %fd11010, 0d0000000000000000;
	add.f64 	%fd1367, %fd11009, 0d0000000000000000;
	add.f64 	%fd1368, %fd11008, 0d0000000000000000;
	ld.param.u64 	%rd217, [%rd27+120];
	ld.param.u32 	%r586, [%rd27+144];
	ld.param.u32 	%r587, [%rd27+172];
	ld.param.v2.u32 	{%r1423, %r1424}, [%rd27+176];
	setp.le.s32 	%p475, %r1423, %r442;
	setp.le.s32 	%p476, %r1424, %r432;
	setp.le.s32 	%p477, %r587, %r470;
	or.pred  	%p478, %p475, %p476;
	or.pred  	%p480, %p352, %p478;
	or.pred  	%p481, %p477, %p480;
	mov.f64 	%fd11017, 0d0000000000000000;
	mov.f64 	%fd11018, 0d0000000000000000;
	mov.f64 	%fd11019, 0d0000000000000000;
	mov.f64 	%fd11020, 0d0000000000000000;
	mov.f64 	%fd11021, 0d0000000000000000;
	mov.f64 	%fd11022, 0d0000000000000000;
	mov.f64 	%fd11023, 0d0000000000000000;
	mov.f64 	%fd11024, 0d0000000000000000;
	mov.f64 	%fd11025, 0d0000000000000000;
	@%p481 bra 	$L__BB1_292;
	bra.uni 	$L__BB1_290;

$L__BB1_292:
	add.s32 	%r1865, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1865, %r432};
	add.s32 	%r1866, %r403, 7;
	st.local.v2.u32 	[%rd26+8], {%r1866, %r1423};
	st.local.v2.u32 	[%rd26+16], {%r1424, %r587};
	mov.u64 	%rd1759, $str$1;
	cvta.global.u64 	%rd1760, %rd1759;
	{ // callseq 404, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1760;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1425, [retval0+0];
	} // callseq 404
	bra.uni 	$L__BB1_293;

$L__BB1_290:
	setp.eq.s64 	%p482, %rd217, 0;
	@%p482 bra 	$L__BB1_293;

	cvta.to.global.u64 	%rd1756, %rd217;
	mul.wide.s32 	%rd1757, %r586, %r470;
	add.s64 	%rd1758, %rd1756, %rd1757;
	ld.global.f64 	%fd5122, [%rd1758];
	add.f64 	%fd11017, %fd5122, 0d0000000000000000;
	ld.global.f64 	%fd5123, [%rd1758+8];
	add.f64 	%fd11018, %fd5123, 0d0000000000000000;
	ld.global.f64 	%fd5124, [%rd1758+16];
	add.f64 	%fd11019, %fd5124, 0d0000000000000000;
	ld.global.f64 	%fd5125, [%rd1758+24];
	add.f64 	%fd11020, %fd5125, 0d0000000000000000;
	ld.global.f64 	%fd5126, [%rd1758+32];
	add.f64 	%fd11021, %fd5126, 0d0000000000000000;
	ld.global.f64 	%fd5127, [%rd1758+40];
	add.f64 	%fd11022, %fd5127, 0d0000000000000000;
	ld.global.f64 	%fd5128, [%rd1758+48];
	add.f64 	%fd11023, %fd5128, 0d0000000000000000;
	ld.global.f64 	%fd5129, [%rd1758+56];
	add.f64 	%fd11024, %fd5129, 0d0000000000000000;
	ld.global.f64 	%fd5130, [%rd1758+64];
	add.f64 	%fd11025, %fd5130, 0d0000000000000000;

$L__BB1_293:
	add.f64 	%fd1387, %fd11025, 0d0000000000000000;
	add.f64 	%fd1388, %fd11024, 0d0000000000000000;
	add.f64 	%fd1389, %fd11023, 0d0000000000000000;
	add.f64 	%fd1390, %fd11022, 0d0000000000000000;
	add.f64 	%fd1391, %fd11021, 0d0000000000000000;
	add.f64 	%fd1392, %fd11020, 0d0000000000000000;
	add.f64 	%fd1393, %fd11019, 0d0000000000000000;
	add.f64 	%fd1394, %fd11018, 0d0000000000000000;
	add.f64 	%fd1395, %fd11017, 0d0000000000000000;
	ld.param.u64 	%rd218, [%rd27+120];
	ld.param.u32 	%r591, [%rd27+144];
	ld.param.u32 	%r592, [%rd27+172];
	ld.param.v2.u32 	{%r1426, %r1427}, [%rd27+176];
	setp.le.s32 	%p483, %r1426, %r442;
	setp.le.s32 	%p484, %r1427, %r422;
	setp.le.s32 	%p485, %r592, %r461;
	or.pred  	%p486, %p483, %p484;
	or.pred  	%p488, %p345, %p486;
	or.pred  	%p489, %p485, %p488;
	mov.f64 	%fd11026, 0d0000000000000000;
	mov.f64 	%fd11027, 0d0000000000000000;
	mov.f64 	%fd11028, 0d0000000000000000;
	mov.f64 	%fd11029, 0d0000000000000000;
	mov.f64 	%fd11030, 0d0000000000000000;
	mov.f64 	%fd11031, 0d0000000000000000;
	mov.f64 	%fd11032, 0d0000000000000000;
	mov.f64 	%fd11033, 0d0000000000000000;
	mov.f64 	%fd11034, 0d0000000000000000;
	@%p489 bra 	$L__BB1_296;
	bra.uni 	$L__BB1_294;

$L__BB1_296:
	add.s32 	%r1867, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1867, %r422};
	add.s32 	%r1868, %r403, 6;
	st.local.v2.u32 	[%rd26+8], {%r1868, %r1426};
	st.local.v2.u32 	[%rd26+16], {%r1427, %r592};
	mov.u64 	%rd1765, $str$1;
	cvta.global.u64 	%rd1766, %rd1765;
	{ // callseq 405, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1766;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1428, [retval0+0];
	} // callseq 405
	bra.uni 	$L__BB1_297;

$L__BB1_294:
	setp.eq.s64 	%p490, %rd218, 0;
	@%p490 bra 	$L__BB1_297;

	cvta.to.global.u64 	%rd1762, %rd218;
	mul.wide.s32 	%rd1763, %r591, %r461;
	add.s64 	%rd1764, %rd1762, %rd1763;
	ld.global.f64 	%fd5149, [%rd1764];
	add.f64 	%fd11026, %fd5149, 0d0000000000000000;
	ld.global.f64 	%fd5150, [%rd1764+8];
	add.f64 	%fd11027, %fd5150, 0d0000000000000000;
	ld.global.f64 	%fd5151, [%rd1764+16];
	add.f64 	%fd11028, %fd5151, 0d0000000000000000;
	ld.global.f64 	%fd5152, [%rd1764+24];
	add.f64 	%fd11029, %fd5152, 0d0000000000000000;
	ld.global.f64 	%fd5153, [%rd1764+32];
	add.f64 	%fd11030, %fd5153, 0d0000000000000000;
	ld.global.f64 	%fd5154, [%rd1764+40];
	add.f64 	%fd11031, %fd5154, 0d0000000000000000;
	ld.global.f64 	%fd5155, [%rd1764+48];
	add.f64 	%fd11032, %fd5155, 0d0000000000000000;
	ld.global.f64 	%fd5156, [%rd1764+56];
	add.f64 	%fd11033, %fd5156, 0d0000000000000000;
	ld.global.f64 	%fd5157, [%rd1764+64];
	add.f64 	%fd11034, %fd5157, 0d0000000000000000;

$L__BB1_297:
	add.f64 	%fd1414, %fd11034, 0d0000000000000000;
	add.f64 	%fd1415, %fd11033, 0d0000000000000000;
	add.f64 	%fd1416, %fd11032, 0d0000000000000000;
	add.f64 	%fd1417, %fd11031, 0d0000000000000000;
	add.f64 	%fd1418, %fd11030, 0d0000000000000000;
	add.f64 	%fd1419, %fd11029, 0d0000000000000000;
	add.f64 	%fd1420, %fd11028, 0d0000000000000000;
	add.f64 	%fd1421, %fd11027, 0d0000000000000000;
	add.f64 	%fd1422, %fd11026, 0d0000000000000000;
	ld.param.u64 	%rd219, [%rd27+120];
	ld.param.u32 	%r596, [%rd27+144];
	ld.param.u32 	%r597, [%rd27+172];
	ld.param.v2.u32 	{%r1429, %r1430}, [%rd27+176];
	setp.le.s32 	%p491, %r1429, %r442;
	setp.le.s32 	%p492, %r1430, %r412;
	setp.le.s32 	%p493, %r597, %r452;
	or.pred  	%p494, %p491, %p492;
	or.pred  	%p496, %p338, %p494;
	or.pred  	%p497, %p493, %p496;
	mov.f64 	%fd11035, 0d0000000000000000;
	mov.f64 	%fd11036, 0d0000000000000000;
	mov.f64 	%fd11037, 0d0000000000000000;
	mov.f64 	%fd11038, 0d0000000000000000;
	mov.f64 	%fd11039, 0d0000000000000000;
	mov.f64 	%fd11040, 0d0000000000000000;
	mov.f64 	%fd11041, 0d0000000000000000;
	mov.f64 	%fd11042, 0d0000000000000000;
	mov.f64 	%fd11043, 0d0000000000000000;
	@%p497 bra 	$L__BB1_300;
	bra.uni 	$L__BB1_298;

$L__BB1_300:
	add.s32 	%r1869, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1869, %r412};
	add.s32 	%r1870, %r403, 5;
	st.local.v2.u32 	[%rd26+8], {%r1870, %r1429};
	st.local.v2.u32 	[%rd26+16], {%r1430, %r597};
	mov.u64 	%rd1771, $str$1;
	cvta.global.u64 	%rd1772, %rd1771;
	{ // callseq 406, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1772;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1431, [retval0+0];
	} // callseq 406
	bra.uni 	$L__BB1_301;

$L__BB1_298:
	setp.eq.s64 	%p498, %rd219, 0;
	@%p498 bra 	$L__BB1_301;

	cvta.to.global.u64 	%rd1768, %rd219;
	mul.wide.s32 	%rd1769, %r596, %r452;
	add.s64 	%rd1770, %rd1768, %rd1769;
	ld.global.f64 	%fd5176, [%rd1770];
	add.f64 	%fd11035, %fd5176, 0d0000000000000000;
	ld.global.f64 	%fd5177, [%rd1770+8];
	add.f64 	%fd11036, %fd5177, 0d0000000000000000;
	ld.global.f64 	%fd5178, [%rd1770+16];
	add.f64 	%fd11037, %fd5178, 0d0000000000000000;
	ld.global.f64 	%fd5179, [%rd1770+24];
	add.f64 	%fd11038, %fd5179, 0d0000000000000000;
	ld.global.f64 	%fd5180, [%rd1770+32];
	add.f64 	%fd11039, %fd5180, 0d0000000000000000;
	ld.global.f64 	%fd5181, [%rd1770+40];
	add.f64 	%fd11040, %fd5181, 0d0000000000000000;
	ld.global.f64 	%fd5182, [%rd1770+48];
	add.f64 	%fd11041, %fd5182, 0d0000000000000000;
	ld.global.f64 	%fd5183, [%rd1770+56];
	add.f64 	%fd11042, %fd5183, 0d0000000000000000;
	ld.global.f64 	%fd5184, [%rd1770+64];
	add.f64 	%fd11043, %fd5184, 0d0000000000000000;

$L__BB1_301:
	add.f64 	%fd1441, %fd11043, 0d0000000000000000;
	add.f64 	%fd1442, %fd11042, 0d0000000000000000;
	add.f64 	%fd1443, %fd11041, 0d0000000000000000;
	add.f64 	%fd1444, %fd11040, 0d0000000000000000;
	add.f64 	%fd1445, %fd11039, 0d0000000000000000;
	add.f64 	%fd1446, %fd11038, 0d0000000000000000;
	add.f64 	%fd1447, %fd11037, 0d0000000000000000;
	add.f64 	%fd1448, %fd11036, 0d0000000000000000;
	add.f64 	%fd1449, %fd11035, 0d0000000000000000;
	ld.param.u64 	%rd220, [%rd27+120];
	ld.param.u32 	%r601, [%rd27+144];
	ld.param.u32 	%r602, [%rd27+172];
	ld.param.v2.u32 	{%r1432, %r1433}, [%rd27+176];
	setp.le.s32 	%p499, %r1432, %r442;
	setp.le.s32 	%p500, %r1433, %r402;
	setp.le.s32 	%p501, %r602, %r443;
	or.pred  	%p502, %p499, %p500;
	or.pred  	%p504, %p331, %p502;
	or.pred  	%p505, %p501, %p504;
	mov.f64 	%fd11044, 0d0000000000000000;
	mov.f64 	%fd11045, 0d0000000000000000;
	mov.f64 	%fd11046, 0d0000000000000000;
	mov.f64 	%fd11047, 0d0000000000000000;
	mov.f64 	%fd11048, 0d0000000000000000;
	mov.f64 	%fd11049, 0d0000000000000000;
	mov.f64 	%fd11050, 0d0000000000000000;
	mov.f64 	%fd11051, 0d0000000000000000;
	mov.f64 	%fd11052, 0d0000000000000000;
	@%p505 bra 	$L__BB1_304;
	bra.uni 	$L__BB1_302;

$L__BB1_304:
	add.s32 	%r1871, %r401, 1;
	st.local.v2.u32 	[%rd26], {%r1871, %r402};
	add.s32 	%r1872, %r403, 4;
	st.local.v2.u32 	[%rd26+8], {%r1872, %r1432};
	st.local.v2.u32 	[%rd26+16], {%r1433, %r602};
	mov.u64 	%rd1777, $str$1;
	cvta.global.u64 	%rd1778, %rd1777;
	{ // callseq 407, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1778;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1434, [retval0+0];
	} // callseq 407
	bra.uni 	$L__BB1_305;

$L__BB1_302:
	setp.eq.s64 	%p506, %rd220, 0;
	@%p506 bra 	$L__BB1_305;

	cvta.to.global.u64 	%rd1774, %rd220;
	mul.wide.s32 	%rd1775, %r601, %r443;
	add.s64 	%rd1776, %rd1774, %rd1775;
	ld.global.f64 	%fd5203, [%rd1776];
	add.f64 	%fd11044, %fd5203, 0d0000000000000000;
	ld.global.f64 	%fd5204, [%rd1776+8];
	add.f64 	%fd11045, %fd5204, 0d0000000000000000;
	ld.global.f64 	%fd5205, [%rd1776+16];
	add.f64 	%fd11046, %fd5205, 0d0000000000000000;
	ld.global.f64 	%fd5206, [%rd1776+24];
	add.f64 	%fd11047, %fd5206, 0d0000000000000000;
	ld.global.f64 	%fd5207, [%rd1776+32];
	add.f64 	%fd11048, %fd5207, 0d0000000000000000;
	ld.global.f64 	%fd5208, [%rd1776+40];
	add.f64 	%fd11049, %fd5208, 0d0000000000000000;
	ld.global.f64 	%fd5209, [%rd1776+48];
	add.f64 	%fd11050, %fd5209, 0d0000000000000000;
	ld.global.f64 	%fd5210, [%rd1776+56];
	add.f64 	%fd11051, %fd5210, 0d0000000000000000;
	ld.global.f64 	%fd5211, [%rd1776+64];
	add.f64 	%fd11052, %fd5211, 0d0000000000000000;

$L__BB1_305:
	add.f64 	%fd1468, %fd11052, 0d0000000000000000;
	add.f64 	%fd1469, %fd11051, 0d0000000000000000;
	add.f64 	%fd1470, %fd11050, 0d0000000000000000;
	add.f64 	%fd1471, %fd11049, 0d0000000000000000;
	add.f64 	%fd1472, %fd11048, 0d0000000000000000;
	add.f64 	%fd1473, %fd11047, 0d0000000000000000;
	add.f64 	%fd1474, %fd11046, 0d0000000000000000;
	add.f64 	%fd1475, %fd11045, 0d0000000000000000;
	add.f64 	%fd1476, %fd11044, 0d0000000000000000;
	ld.param.u64 	%rd221, [%rd27+120];
	ld.param.u32 	%r606, [%rd27+144];
	ld.param.u32 	%r607, [%rd27+172];
	ld.param.v2.u32 	{%r1435, %r1436}, [%rd27+176];
	setp.le.s32 	%p507, %r1435, %r401;
	setp.le.s32 	%p508, %r1436, %r432;
	setp.le.s32 	%p509, %r607, %r433;
	or.pred  	%p510, %p507, %p508;
	or.pred  	%p512, %p324, %p510;
	or.pred  	%p513, %p509, %p512;
	mov.f64 	%fd11053, 0d0000000000000000;
	mov.f64 	%fd11054, 0d0000000000000000;
	mov.f64 	%fd11055, 0d0000000000000000;
	mov.f64 	%fd11056, 0d0000000000000000;
	mov.f64 	%fd11057, 0d0000000000000000;
	mov.f64 	%fd11058, 0d0000000000000000;
	mov.f64 	%fd11059, 0d0000000000000000;
	mov.f64 	%fd11060, 0d0000000000000000;
	mov.f64 	%fd11061, 0d0000000000000000;
	@%p513 bra 	$L__BB1_308;
	bra.uni 	$L__BB1_306;

$L__BB1_308:
	add.s32 	%r1873, %r402, 3;
	st.local.v2.u32 	[%rd26], {%r401, %r1873};
	add.s32 	%r1874, %r403, 3;
	st.local.v2.u32 	[%rd26+8], {%r1874, %r1435};
	st.local.v2.u32 	[%rd26+16], {%r1436, %r607};
	mov.u64 	%rd1783, $str$1;
	cvta.global.u64 	%rd1784, %rd1783;
	{ // callseq 408, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1784;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1437, [retval0+0];
	} // callseq 408
	bra.uni 	$L__BB1_309;

$L__BB1_306:
	setp.eq.s64 	%p514, %rd221, 0;
	@%p514 bra 	$L__BB1_309;

	cvta.to.global.u64 	%rd1780, %rd221;
	mul.wide.s32 	%rd1781, %r606, %r433;
	add.s64 	%rd1782, %rd1780, %rd1781;
	ld.global.f64 	%fd5230, [%rd1782];
	add.f64 	%fd11053, %fd5230, 0d0000000000000000;
	ld.global.f64 	%fd5231, [%rd1782+8];
	add.f64 	%fd11054, %fd5231, 0d0000000000000000;
	ld.global.f64 	%fd5232, [%rd1782+16];
	add.f64 	%fd11055, %fd5232, 0d0000000000000000;
	ld.global.f64 	%fd5233, [%rd1782+24];
	add.f64 	%fd11056, %fd5233, 0d0000000000000000;
	ld.global.f64 	%fd5234, [%rd1782+32];
	add.f64 	%fd11057, %fd5234, 0d0000000000000000;
	ld.global.f64 	%fd5235, [%rd1782+40];
	add.f64 	%fd11058, %fd5235, 0d0000000000000000;
	ld.global.f64 	%fd5236, [%rd1782+48];
	add.f64 	%fd11059, %fd5236, 0d0000000000000000;
	ld.global.f64 	%fd5237, [%rd1782+56];
	add.f64 	%fd11060, %fd5237, 0d0000000000000000;
	ld.global.f64 	%fd5238, [%rd1782+64];
	add.f64 	%fd11061, %fd5238, 0d0000000000000000;

$L__BB1_309:
	add.f64 	%fd1495, %fd11061, 0d0000000000000000;
	add.f64 	%fd1496, %fd11060, 0d0000000000000000;
	add.f64 	%fd1497, %fd11059, 0d0000000000000000;
	add.f64 	%fd1498, %fd11058, 0d0000000000000000;
	add.f64 	%fd1499, %fd11057, 0d0000000000000000;
	add.f64 	%fd1500, %fd11056, 0d0000000000000000;
	add.f64 	%fd1501, %fd11055, 0d0000000000000000;
	add.f64 	%fd1502, %fd11054, 0d0000000000000000;
	add.f64 	%fd1503, %fd11053, 0d0000000000000000;
	ld.param.u64 	%rd222, [%rd27+120];
	ld.param.u32 	%r611, [%rd27+144];
	ld.param.u32 	%r612, [%rd27+172];
	ld.param.v2.u32 	{%r1438, %r1439}, [%rd27+176];
	setp.le.s32 	%p515, %r1438, %r401;
	setp.le.s32 	%p516, %r1439, %r422;
	setp.le.s32 	%p517, %r612, %r423;
	or.pred  	%p518, %p515, %p516;
	or.pred  	%p520, %p317, %p518;
	or.pred  	%p521, %p517, %p520;
	mov.f64 	%fd11062, 0d0000000000000000;
	mov.f64 	%fd11063, 0d0000000000000000;
	mov.f64 	%fd11064, 0d0000000000000000;
	mov.f64 	%fd11065, 0d0000000000000000;
	mov.f64 	%fd11066, 0d0000000000000000;
	mov.f64 	%fd11067, 0d0000000000000000;
	mov.f64 	%fd11068, 0d0000000000000000;
	mov.f64 	%fd11069, 0d0000000000000000;
	mov.f64 	%fd11070, 0d0000000000000000;
	@%p521 bra 	$L__BB1_312;
	bra.uni 	$L__BB1_310;

$L__BB1_312:
	add.s32 	%r1875, %r402, 2;
	st.local.v2.u32 	[%rd26], {%r401, %r1875};
	add.s32 	%r1876, %r403, 2;
	st.local.v2.u32 	[%rd26+8], {%r1876, %r1438};
	st.local.v2.u32 	[%rd26+16], {%r1439, %r612};
	mov.u64 	%rd1789, $str$1;
	cvta.global.u64 	%rd1790, %rd1789;
	{ // callseq 409, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1790;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1440, [retval0+0];
	} // callseq 409
	bra.uni 	$L__BB1_313;

$L__BB1_310:
	setp.eq.s64 	%p522, %rd222, 0;
	@%p522 bra 	$L__BB1_313;

	cvta.to.global.u64 	%rd1786, %rd222;
	mul.wide.s32 	%rd1787, %r611, %r423;
	add.s64 	%rd1788, %rd1786, %rd1787;
	ld.global.f64 	%fd5257, [%rd1788];
	add.f64 	%fd11062, %fd5257, 0d0000000000000000;
	ld.global.f64 	%fd5258, [%rd1788+8];
	add.f64 	%fd11063, %fd5258, 0d0000000000000000;
	ld.global.f64 	%fd5259, [%rd1788+16];
	add.f64 	%fd11064, %fd5259, 0d0000000000000000;
	ld.global.f64 	%fd5260, [%rd1788+24];
	add.f64 	%fd11065, %fd5260, 0d0000000000000000;
	ld.global.f64 	%fd5261, [%rd1788+32];
	add.f64 	%fd11066, %fd5261, 0d0000000000000000;
	ld.global.f64 	%fd5262, [%rd1788+40];
	add.f64 	%fd11067, %fd5262, 0d0000000000000000;
	ld.global.f64 	%fd5263, [%rd1788+48];
	add.f64 	%fd11068, %fd5263, 0d0000000000000000;
	ld.global.f64 	%fd5264, [%rd1788+56];
	add.f64 	%fd11069, %fd5264, 0d0000000000000000;
	ld.global.f64 	%fd5265, [%rd1788+64];
	add.f64 	%fd11070, %fd5265, 0d0000000000000000;

$L__BB1_313:
	add.s32 	%r2229, %r403, 1;
	or.b32  	%r2228, %r2229, %r401;
	or.b32  	%r2227, %r2228, %r412;
	setp.lt.s32 	%p946, %r2227, 0;
	add.f64 	%fd1522, %fd11070, 0d0000000000000000;
	add.f64 	%fd1523, %fd11069, 0d0000000000000000;
	add.f64 	%fd1524, %fd11068, 0d0000000000000000;
	add.f64 	%fd1525, %fd11067, 0d0000000000000000;
	add.f64 	%fd1526, %fd11066, 0d0000000000000000;
	add.f64 	%fd1527, %fd11065, 0d0000000000000000;
	add.f64 	%fd1528, %fd11064, 0d0000000000000000;
	add.f64 	%fd1529, %fd11063, 0d0000000000000000;
	add.f64 	%fd1530, %fd11062, 0d0000000000000000;
	ld.param.u64 	%rd223, [%rd27+120];
	ld.param.u32 	%r616, [%rd27+144];
	ld.param.u32 	%r617, [%rd27+172];
	ld.param.v2.u32 	{%r1441, %r1442}, [%rd27+176];
	setp.le.s32 	%p523, %r1441, %r401;
	setp.le.s32 	%p524, %r1442, %r412;
	setp.le.s32 	%p525, %r617, %r2229;
	or.pred  	%p526, %p523, %p524;
	or.pred  	%p528, %p946, %p526;
	or.pred  	%p529, %p525, %p528;
	mov.f64 	%fd11071, 0d0000000000000000;
	mov.f64 	%fd11072, 0d0000000000000000;
	mov.f64 	%fd11073, 0d0000000000000000;
	mov.f64 	%fd11074, 0d0000000000000000;
	mov.f64 	%fd11075, 0d0000000000000000;
	mov.f64 	%fd11076, 0d0000000000000000;
	mov.f64 	%fd11077, 0d0000000000000000;
	mov.f64 	%fd11078, 0d0000000000000000;
	mov.f64 	%fd11079, 0d0000000000000000;
	@%p529 bra 	$L__BB1_316;
	bra.uni 	$L__BB1_314;

$L__BB1_316:
	add.s32 	%r1877, %r402, 1;
	st.local.v2.u32 	[%rd26], {%r401, %r1877};
	add.s32 	%r1878, %r403, 1;
	st.local.v2.u32 	[%rd26+8], {%r1878, %r1441};
	st.local.v2.u32 	[%rd26+16], {%r1442, %r617};
	mov.u64 	%rd1795, $str$1;
	cvta.global.u64 	%rd1796, %rd1795;
	{ // callseq 410, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1796;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1443, [retval0+0];
	} // callseq 410
	bra.uni 	$L__BB1_317;

$L__BB1_314:
	setp.eq.s64 	%p530, %rd223, 0;
	@%p530 bra 	$L__BB1_317;

	add.s32 	%r2230, %r403, 1;
	cvta.to.global.u64 	%rd1792, %rd223;
	mul.wide.s32 	%rd1793, %r616, %r2230;
	add.s64 	%rd1794, %rd1792, %rd1793;
	ld.global.f64 	%fd5284, [%rd1794];
	add.f64 	%fd11071, %fd5284, 0d0000000000000000;
	ld.global.f64 	%fd5285, [%rd1794+8];
	add.f64 	%fd11072, %fd5285, 0d0000000000000000;
	ld.global.f64 	%fd5286, [%rd1794+16];
	add.f64 	%fd11073, %fd5286, 0d0000000000000000;
	ld.global.f64 	%fd5287, [%rd1794+24];
	add.f64 	%fd11074, %fd5287, 0d0000000000000000;
	ld.global.f64 	%fd5288, [%rd1794+32];
	add.f64 	%fd11075, %fd5288, 0d0000000000000000;
	ld.global.f64 	%fd5289, [%rd1794+40];
	add.f64 	%fd11076, %fd5289, 0d0000000000000000;
	ld.global.f64 	%fd5290, [%rd1794+48];
	add.f64 	%fd11077, %fd5290, 0d0000000000000000;
	ld.global.f64 	%fd5291, [%rd1794+56];
	add.f64 	%fd11078, %fd5291, 0d0000000000000000;
	ld.global.f64 	%fd5292, [%rd1794+64];
	add.f64 	%fd11079, %fd5292, 0d0000000000000000;

$L__BB1_317:
	or.b32  	%r2185, %r401, %r403;
	or.b32  	%r2184, %r2185, %r402;
	setp.lt.s32 	%p944, %r2184, 0;
	add.f64 	%fd1549, %fd11079, 0d0000000000000000;
	add.f64 	%fd1550, %fd11078, 0d0000000000000000;
	add.f64 	%fd1551, %fd11077, 0d0000000000000000;
	add.f64 	%fd1552, %fd11076, 0d0000000000000000;
	add.f64 	%fd1553, %fd11075, 0d0000000000000000;
	add.f64 	%fd1554, %fd11074, 0d0000000000000000;
	add.f64 	%fd1555, %fd11073, 0d0000000000000000;
	add.f64 	%fd1556, %fd11072, 0d0000000000000000;
	add.f64 	%fd1557, %fd11071, 0d0000000000000000;
	ld.param.u32 	%r622, [%rd27+172];
	ld.param.v2.u32 	{%r1444, %r1445}, [%rd27+176];
	setp.le.s32 	%p531, %r1444, %r401;
	setp.le.s32 	%p532, %r1445, %r402;
	setp.le.s32 	%p533, %r622, %r403;
	or.pred  	%p534, %p531, %p532;
	or.pred  	%p536, %p944, %p534;
	or.pred  	%p537, %p533, %p536;
	mov.f64 	%fd11080, 0d0000000000000000;
	mov.f64 	%fd11081, 0d0000000000000000;
	mov.f64 	%fd11082, 0d0000000000000000;
	mov.f64 	%fd11083, 0d0000000000000000;
	mov.f64 	%fd11084, 0d0000000000000000;
	mov.f64 	%fd11085, 0d0000000000000000;
	mov.f64 	%fd11086, 0d0000000000000000;
	mov.f64 	%fd11087, 0d0000000000000000;
	mov.f64 	%fd11088, 0d0000000000000000;
	@%p537 bra 	$L__BB1_320;
	bra.uni 	$L__BB1_318;

$L__BB1_320:
	st.local.v2.u32 	[%rd26], {%r401, %r402};
	st.local.v2.u32 	[%rd26+8], {%r403, %r1444};
	st.local.v2.u32 	[%rd26+16], {%r1445, %r622};
	mov.u64 	%rd1801, $str$1;
	cvta.global.u64 	%rd1802, %rd1801;
	{ // callseq 411, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1802;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1446, [retval0+0];
	} // callseq 411
	bra.uni 	$L__BB1_321;

$L__BB1_318:
	ld.param.u64 	%rd4072, [%rd27+120];
	setp.eq.s64 	%p538, %rd4072, 0;
	@%p538 bra 	$L__BB1_321;

	ld.param.u32 	%r2186, [%rd27+144];
	ld.param.u64 	%rd4073, [%rd27+120];
	cvta.to.global.u64 	%rd1798, %rd4073;
	mul.wide.s32 	%rd1799, %r2186, %r403;
	add.s64 	%rd1800, %rd1798, %rd1799;
	ld.global.f64 	%fd5311, [%rd1800];
	add.f64 	%fd11080, %fd5311, 0d0000000000000000;
	ld.global.f64 	%fd5312, [%rd1800+8];
	add.f64 	%fd11081, %fd5312, 0d0000000000000000;
	ld.global.f64 	%fd5313, [%rd1800+16];
	add.f64 	%fd11082, %fd5313, 0d0000000000000000;
	ld.global.f64 	%fd5314, [%rd1800+24];
	add.f64 	%fd11083, %fd5314, 0d0000000000000000;
	ld.global.f64 	%fd5315, [%rd1800+32];
	add.f64 	%fd11084, %fd5315, 0d0000000000000000;
	ld.global.f64 	%fd5316, [%rd1800+40];
	add.f64 	%fd11085, %fd5316, 0d0000000000000000;
	ld.global.f64 	%fd5317, [%rd1800+48];
	add.f64 	%fd11086, %fd5317, 0d0000000000000000;
	ld.global.f64 	%fd5318, [%rd1800+56];
	add.f64 	%fd11087, %fd5318, 0d0000000000000000;
	ld.global.f64 	%fd5319, [%rd1800+64];
	add.f64 	%fd11088, %fd5319, 0d0000000000000000;

$L__BB1_321:
	add.f64 	%fd1576, %fd11088, 0d0000000000000000;
	add.f64 	%fd1577, %fd11087, 0d0000000000000000;
	add.f64 	%fd1578, %fd11086, 0d0000000000000000;
	add.f64 	%fd1579, %fd11085, 0d0000000000000000;
	add.f64 	%fd1580, %fd11084, 0d0000000000000000;
	add.f64 	%fd1581, %fd11083, 0d0000000000000000;
	add.f64 	%fd1582, %fd11082, 0d0000000000000000;
	add.f64 	%fd1583, %fd11081, 0d0000000000000000;
	add.f64 	%fd1584, %fd11080, 0d0000000000000000;
	ld.param.u32 	%r629, [%rd27+172];
	ld.param.v2.u32 	{%r1447, %r1448}, [%rd27+176];
	shl.b32 	%r633, %r2256, 2;
	setp.le.s32 	%p539, %r1447, %r633;
	shl.b32 	%r634, %r2257, 2;
	setp.le.s32 	%p540, %r1448, %r634;
	shl.b32 	%r635, %r2255, 4;
	setp.le.s32 	%p541, %r629, %r635;
	or.pred  	%p542, %p539, %p540;
	or.b32  	%r1449, %r633, %r635;
	or.b32  	%r636, %r1449, %r634;
	setp.lt.s32 	%p543, %r636, 0;
	or.pred  	%p544, %p543, %p542;
	or.pred  	%p545, %p541, %p544;
	@%p545 bra 	$L__BB1_323;
	bra.uni 	$L__BB1_322;

$L__BB1_323:
	st.local.v2.u32 	[%rd26], {%r633, %r634};
	st.local.v2.u32 	[%rd26+8], {%r635, %r1447};
	st.local.v2.u32 	[%rd26+16], {%r1448, %r629};
	mov.u64 	%rd1820, $str$1;
	cvta.global.u64 	%rd1821, %rd1820;
	{ // callseq 412, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1821;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1450, [retval0+0];
	} // callseq 412
	bra.uni 	$L__BB1_324;

$L__BB1_322:
	ld.param.u32 	%r2189, [%rd27+88];
	ld.param.u64 	%rd4076, [%rd27];
	ld.param.u32 	%r2188, [%rd27+32];
	ld.param.u64 	%rd4075, [%rd27+112];
	ld.param.u32 	%r2187, [%rd27+144];
	ld.param.u64 	%rd4074, [%rd27+56];
	cvta.to.global.u64 	%rd1813, %rd4074;
	mul.wide.s32 	%rd1814, %r2187, %r635;
	add.s64 	%rd1804, %rd4075, %rd1814;
	// begin inline asm
	{ atom.add.f64 %fd5329,[%rd1804],%fd10800; }

	// end inline asm
	add.s64 	%rd1805, %rd1804, 8;
	// begin inline asm
	{ atom.add.f64 %fd5331,[%rd1805],%fd10799; }

	// end inline asm
	add.s64 	%rd1806, %rd1804, 16;
	// begin inline asm
	{ atom.add.f64 %fd5333,[%rd1806],%fd10798; }

	// end inline asm
	add.s64 	%rd1807, %rd1804, 24;
	// begin inline asm
	{ atom.add.f64 %fd5335,[%rd1807],%fd10788; }

	// end inline asm
	add.s64 	%rd1808, %rd1804, 32;
	// begin inline asm
	{ atom.add.f64 %fd5337,[%rd1808],%fd10787; }

	// end inline asm
	add.s64 	%rd1809, %rd1804, 40;
	// begin inline asm
	{ atom.add.f64 %fd5339,[%rd1809],%fd10786; }

	// end inline asm
	add.s64 	%rd1810, %rd1804, 48;
	// begin inline asm
	{ atom.add.f64 %fd5341,[%rd1810],%fd10776; }

	// end inline asm
	add.s64 	%rd1811, %rd1804, 56;
	// begin inline asm
	{ atom.add.f64 %fd5343,[%rd1811],%fd10775; }

	// end inline asm
	add.s64 	%rd1812, %rd1804, 64;
	// begin inline asm
	{ atom.add.f64 %fd5345,[%rd1812],%fd10774; }

	// end inline asm
	mul.wide.s32 	%rd1815, %r2188, %r635;
	cvta.to.global.u64 	%rd1816, %rd4076;
	add.s64 	%rd1817, %rd1816, %rd1815;
	mul.wide.s32 	%rd1818, %r2189, %r635;
	add.s64 	%rd1819, %rd1813, %rd1818;
	st.global.u32 	[%rd1817], %r633;
	st.global.u32 	[%rd1819], %r634;

$L__BB1_324:
	ld.param.u32 	%r640, [%rd27+172];
	ld.param.v2.u32 	{%r1451, %r1452}, [%rd27+176];
	setp.le.s32 	%p546, %r1451, %r633;
	add.s32 	%r644, %r634, 1;
	setp.le.s32 	%p547, %r1452, %r644;
	add.s32 	%r645, %r635, 1;
	setp.le.s32 	%p548, %r640, %r645;
	or.pred  	%p549, %p546, %p547;
	or.b32  	%r1453, %r645, %r633;
	or.b32  	%r646, %r1453, %r644;
	setp.lt.s32 	%p550, %r646, 0;
	or.pred  	%p551, %p550, %p549;
	or.pred  	%p552, %p548, %p551;
	@%p552 bra 	$L__BB1_326;
	bra.uni 	$L__BB1_325;

$L__BB1_326:
	add.s32 	%r1880, %r634, 1;
	st.local.v2.u32 	[%rd26], {%r633, %r1880};
	add.s32 	%r1881, %r635, 1;
	st.local.v2.u32 	[%rd26+8], {%r1881, %r1451};
	st.local.v2.u32 	[%rd26+16], {%r1452, %r640};
	mov.u64 	%rd1839, $str$1;
	cvta.global.u64 	%rd1840, %rd1839;
	{ // callseq 413, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1840;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1454, [retval0+0];
	} // callseq 413
	bra.uni 	$L__BB1_327;

$L__BB1_325:
	ld.param.u32 	%r2234, [%rd27+88];
	ld.param.u64 	%rd4114, [%rd27];
	ld.param.u32 	%r2233, [%rd27+32];
	ld.param.u64 	%rd4113, [%rd27+112];
	add.s32 	%r2232, %r635, 1;
	ld.param.u32 	%r2231, [%rd27+144];
	ld.param.u64 	%rd4112, [%rd27+56];
	cvta.to.global.u64 	%rd1832, %rd4112;
	mul.wide.s32 	%rd1833, %r2231, %r2232;
	add.s64 	%rd1823, %rd4113, %rd1833;
	// begin inline asm
	{ atom.add.f64 %fd5347,[%rd1823],%fd10797; }

	// end inline asm
	add.s64 	%rd1824, %rd1823, 8;
	// begin inline asm
	{ atom.add.f64 %fd5349,[%rd1824],%fd10796; }

	// end inline asm
	add.s64 	%rd1825, %rd1823, 16;
	// begin inline asm
	{ atom.add.f64 %fd5351,[%rd1825],%fd10795; }

	// end inline asm
	add.s64 	%rd1826, %rd1823, 24;
	// begin inline asm
	{ atom.add.f64 %fd5353,[%rd1826],%fd10785; }

	// end inline asm
	add.s64 	%rd1827, %rd1823, 32;
	// begin inline asm
	{ atom.add.f64 %fd5355,[%rd1827],%fd10784; }

	// end inline asm
	add.s64 	%rd1828, %rd1823, 40;
	// begin inline asm
	{ atom.add.f64 %fd5357,[%rd1828],%fd10783; }

	// end inline asm
	add.s64 	%rd1829, %rd1823, 48;
	// begin inline asm
	{ atom.add.f64 %fd5359,[%rd1829],%fd10773; }

	// end inline asm
	add.s64 	%rd1830, %rd1823, 56;
	// begin inline asm
	{ atom.add.f64 %fd5361,[%rd1830],%fd10772; }

	// end inline asm
	add.s64 	%rd1831, %rd1823, 64;
	// begin inline asm
	{ atom.add.f64 %fd5363,[%rd1831],%fd10771; }

	// end inline asm
	mul.wide.s32 	%rd1834, %r2233, %r2232;
	cvta.to.global.u64 	%rd1835, %rd4114;
	add.s64 	%rd1836, %rd1835, %rd1834;
	mul.wide.s32 	%rd1837, %r2234, %r2232;
	add.s64 	%rd1838, %rd1832, %rd1837;
	st.global.u32 	[%rd1836], %r633;
	add.s32 	%r1879, %r634, 1;
	st.global.u32 	[%rd1838], %r1879;

$L__BB1_327:
	ld.param.u64 	%rd231, [%rd27];
	ld.param.u32 	%r647, [%rd27+32];
	ld.param.u64 	%rd232, [%rd27+56];
	ld.param.u32 	%r648, [%rd27+88];
	ld.param.u64 	%rd233, [%rd27+112];
	ld.param.u32 	%r649, [%rd27+144];
	ld.param.u32 	%r650, [%rd27+172];
	ld.param.v2.u32 	{%r1455, %r1456}, [%rd27+176];
	setp.le.s32 	%p553, %r1455, %r633;
	add.s32 	%r654, %r634, 2;
	setp.le.s32 	%p554, %r1456, %r654;
	add.s32 	%r655, %r635, 2;
	setp.le.s32 	%p555, %r650, %r655;
	or.pred  	%p556, %p553, %p554;
	or.b32  	%r1457, %r655, %r633;
	or.b32  	%r656, %r1457, %r654;
	setp.lt.s32 	%p557, %r656, 0;
	or.pred  	%p558, %p557, %p556;
	or.pred  	%p559, %p555, %p558;
	@%p559 bra 	$L__BB1_329;
	bra.uni 	$L__BB1_328;

$L__BB1_329:
	add.s32 	%r1883, %r634, 2;
	st.local.v2.u32 	[%rd26], {%r633, %r1883};
	add.s32 	%r1884, %r635, 2;
	st.local.v2.u32 	[%rd26+8], {%r1884, %r1455};
	st.local.v2.u32 	[%rd26+16], {%r1456, %r650};
	mov.u64 	%rd1858, $str$1;
	cvta.global.u64 	%rd1859, %rd1858;
	{ // callseq 414, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1859;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1458, [retval0+0];
	} // callseq 414
	bra.uni 	$L__BB1_330;

$L__BB1_328:
	cvta.to.global.u64 	%rd1851, %rd232;
	mul.wide.s32 	%rd1852, %r649, %r655;
	add.s64 	%rd1842, %rd233, %rd1852;
	// begin inline asm
	{ atom.add.f64 %fd5365,[%rd1842],%fd10794; }

	// end inline asm
	add.s64 	%rd1843, %rd1842, 8;
	// begin inline asm
	{ atom.add.f64 %fd5367,[%rd1843],%fd10793; }

	// end inline asm
	add.s64 	%rd1844, %rd1842, 16;
	// begin inline asm
	{ atom.add.f64 %fd5369,[%rd1844],%fd10792; }

	// end inline asm
	add.s64 	%rd1845, %rd1842, 24;
	// begin inline asm
	{ atom.add.f64 %fd5371,[%rd1845],%fd10782; }

	// end inline asm
	add.s64 	%rd1846, %rd1842, 32;
	// begin inline asm
	{ atom.add.f64 %fd5373,[%rd1846],%fd10781; }

	// end inline asm
	add.s64 	%rd1847, %rd1842, 40;
	// begin inline asm
	{ atom.add.f64 %fd5375,[%rd1847],%fd10780; }

	// end inline asm
	add.s64 	%rd1848, %rd1842, 48;
	// begin inline asm
	{ atom.add.f64 %fd5377,[%rd1848],%fd10770; }

	// end inline asm
	add.s64 	%rd1849, %rd1842, 56;
	// begin inline asm
	{ atom.add.f64 %fd5379,[%rd1849],%fd10769; }

	// end inline asm
	add.s64 	%rd1850, %rd1842, 64;
	// begin inline asm
	{ atom.add.f64 %fd5381,[%rd1850],%fd10768; }

	// end inline asm
	mul.wide.s32 	%rd1853, %r647, %r655;
	cvta.to.global.u64 	%rd1854, %rd231;
	add.s64 	%rd1855, %rd1854, %rd1853;
	mul.wide.s32 	%rd1856, %r648, %r655;
	add.s64 	%rd1857, %rd1851, %rd1856;
	st.global.u32 	[%rd1855], %r633;
	add.s32 	%r1882, %r634, 2;
	st.global.u32 	[%rd1857], %r1882;

$L__BB1_330:
	ld.param.u64 	%rd234, [%rd27];
	ld.param.u32 	%r657, [%rd27+32];
	ld.param.u64 	%rd235, [%rd27+56];
	ld.param.u32 	%r658, [%rd27+88];
	ld.param.u64 	%rd236, [%rd27+112];
	ld.param.u32 	%r659, [%rd27+144];
	ld.param.u32 	%r660, [%rd27+172];
	ld.param.v2.u32 	{%r1459, %r1460}, [%rd27+176];
	setp.le.s32 	%p560, %r1459, %r633;
	add.s32 	%r664, %r634, 3;
	setp.le.s32 	%p561, %r1460, %r664;
	add.s32 	%r665, %r635, 3;
	setp.le.s32 	%p562, %r660, %r665;
	or.pred  	%p563, %p560, %p561;
	or.b32  	%r1461, %r665, %r633;
	or.b32  	%r666, %r1461, %r664;
	setp.lt.s32 	%p564, %r666, 0;
	or.pred  	%p565, %p564, %p563;
	or.pred  	%p566, %p562, %p565;
	@%p566 bra 	$L__BB1_332;
	bra.uni 	$L__BB1_331;

$L__BB1_332:
	add.s32 	%r1886, %r634, 3;
	st.local.v2.u32 	[%rd26], {%r633, %r1886};
	add.s32 	%r1887, %r635, 3;
	st.local.v2.u32 	[%rd26+8], {%r1887, %r1459};
	st.local.v2.u32 	[%rd26+16], {%r1460, %r660};
	mov.u64 	%rd1877, $str$1;
	cvta.global.u64 	%rd1878, %rd1877;
	{ // callseq 415, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1878;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1462, [retval0+0];
	} // callseq 415
	bra.uni 	$L__BB1_333;

$L__BB1_331:
	cvta.to.global.u64 	%rd1870, %rd235;
	mul.wide.s32 	%rd1871, %r659, %r665;
	add.s64 	%rd1861, %rd236, %rd1871;
	// begin inline asm
	{ atom.add.f64 %fd5383,[%rd1861],%fd10791; }

	// end inline asm
	add.s64 	%rd1862, %rd1861, 8;
	// begin inline asm
	{ atom.add.f64 %fd5385,[%rd1862],%fd10790; }

	// end inline asm
	add.s64 	%rd1863, %rd1861, 16;
	// begin inline asm
	{ atom.add.f64 %fd5387,[%rd1863],%fd10789; }

	// end inline asm
	add.s64 	%rd1864, %rd1861, 24;
	// begin inline asm
	{ atom.add.f64 %fd5389,[%rd1864],%fd10779; }

	// end inline asm
	add.s64 	%rd1865, %rd1861, 32;
	// begin inline asm
	{ atom.add.f64 %fd5391,[%rd1865],%fd10778; }

	// end inline asm
	add.s64 	%rd1866, %rd1861, 40;
	// begin inline asm
	{ atom.add.f64 %fd5393,[%rd1866],%fd10777; }

	// end inline asm
	add.s64 	%rd1867, %rd1861, 48;
	// begin inline asm
	{ atom.add.f64 %fd5395,[%rd1867],%fd10767; }

	// end inline asm
	add.s64 	%rd1868, %rd1861, 56;
	// begin inline asm
	{ atom.add.f64 %fd5397,[%rd1868],%fd10766; }

	// end inline asm
	add.s64 	%rd1869, %rd1861, 64;
	// begin inline asm
	{ atom.add.f64 %fd5399,[%rd1869],%fd10765; }

	// end inline asm
	mul.wide.s32 	%rd1872, %r657, %r665;
	cvta.to.global.u64 	%rd1873, %rd234;
	add.s64 	%rd1874, %rd1873, %rd1872;
	mul.wide.s32 	%rd1875, %r658, %r665;
	add.s64 	%rd1876, %rd1870, %rd1875;
	st.global.u32 	[%rd1874], %r633;
	add.s32 	%r1885, %r634, 3;
	st.global.u32 	[%rd1876], %r1885;

$L__BB1_333:
	ld.param.u64 	%rd237, [%rd27];
	ld.param.u32 	%r667, [%rd27+32];
	ld.param.u64 	%rd238, [%rd27+56];
	ld.param.u32 	%r668, [%rd27+88];
	ld.param.u64 	%rd239, [%rd27+112];
	ld.param.u32 	%r669, [%rd27+144];
	ld.param.u32 	%r670, [%rd27+172];
	ld.param.v2.u32 	{%r1463, %r1464}, [%rd27+176];
	add.s32 	%r674, %r633, 1;
	setp.le.s32 	%p567, %r1463, %r674;
	setp.le.s32 	%p568, %r1464, %r634;
	add.s32 	%r675, %r635, 4;
	setp.le.s32 	%p569, %r670, %r675;
	or.pred  	%p570, %p567, %p568;
	or.b32  	%r1465, %r634, %r675;
	or.b32  	%r676, %r1465, %r674;
	setp.lt.s32 	%p571, %r676, 0;
	or.pred  	%p572, %p571, %p570;
	or.pred  	%p573, %p569, %p572;
	@%p573 bra 	$L__BB1_335;
	bra.uni 	$L__BB1_334;

$L__BB1_335:
	add.s32 	%r1889, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1889, %r634};
	add.s32 	%r1890, %r635, 4;
	st.local.v2.u32 	[%rd26+8], {%r1890, %r1463};
	st.local.v2.u32 	[%rd26+16], {%r1464, %r670};
	mov.u64 	%rd1896, $str$1;
	cvta.global.u64 	%rd1897, %rd1896;
	{ // callseq 416, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1897;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1466, [retval0+0];
	} // callseq 416
	bra.uni 	$L__BB1_336;

$L__BB1_334:
	cvta.to.global.u64 	%rd1889, %rd238;
	mul.wide.s32 	%rd1890, %r669, %r675;
	add.s64 	%rd1880, %rd239, %rd1890;
	// begin inline asm
	{ atom.add.f64 %fd5401,[%rd1880],%fd10764; }

	// end inline asm
	add.s64 	%rd1881, %rd1880, 8;
	// begin inline asm
	{ atom.add.f64 %fd5403,[%rd1881],%fd10763; }

	// end inline asm
	add.s64 	%rd1882, %rd1880, 16;
	// begin inline asm
	{ atom.add.f64 %fd5405,[%rd1882],%fd10762; }

	// end inline asm
	add.s64 	%rd1883, %rd1880, 24;
	// begin inline asm
	{ atom.add.f64 %fd5407,[%rd1883],%fd10752; }

	// end inline asm
	add.s64 	%rd1884, %rd1880, 32;
	// begin inline asm
	{ atom.add.f64 %fd5409,[%rd1884],%fd10751; }

	// end inline asm
	add.s64 	%rd1885, %rd1880, 40;
	// begin inline asm
	{ atom.add.f64 %fd5411,[%rd1885],%fd10750; }

	// end inline asm
	add.s64 	%rd1886, %rd1880, 48;
	// begin inline asm
	{ atom.add.f64 %fd5413,[%rd1886],%fd10740; }

	// end inline asm
	add.s64 	%rd1887, %rd1880, 56;
	// begin inline asm
	{ atom.add.f64 %fd5415,[%rd1887],%fd10739; }

	// end inline asm
	add.s64 	%rd1888, %rd1880, 64;
	// begin inline asm
	{ atom.add.f64 %fd5417,[%rd1888],%fd10738; }

	// end inline asm
	mul.wide.s32 	%rd1891, %r667, %r675;
	cvta.to.global.u64 	%rd1892, %rd237;
	add.s64 	%rd1893, %rd1892, %rd1891;
	mul.wide.s32 	%rd1894, %r668, %r675;
	add.s64 	%rd1895, %rd1889, %rd1894;
	add.s32 	%r1888, %r633, 1;
	st.global.u32 	[%rd1893], %r1888;
	st.global.u32 	[%rd1895], %r634;

$L__BB1_336:
	ld.param.u64 	%rd240, [%rd27];
	ld.param.u32 	%r677, [%rd27+32];
	ld.param.u64 	%rd241, [%rd27+56];
	ld.param.u32 	%r678, [%rd27+88];
	ld.param.u64 	%rd242, [%rd27+112];
	ld.param.u32 	%r679, [%rd27+144];
	ld.param.u32 	%r680, [%rd27+172];
	ld.param.v2.u32 	{%r1467, %r1468}, [%rd27+176];
	setp.le.s32 	%p574, %r1467, %r674;
	setp.le.s32 	%p575, %r1468, %r644;
	add.s32 	%r684, %r635, 5;
	setp.le.s32 	%p576, %r680, %r684;
	or.pred  	%p577, %p574, %p575;
	or.b32  	%r1469, %r674, %r684;
	or.b32  	%r685, %r1469, %r644;
	setp.lt.s32 	%p578, %r685, 0;
	or.pred  	%p579, %p578, %p577;
	or.pred  	%p580, %p576, %p579;
	@%p580 bra 	$L__BB1_338;
	bra.uni 	$L__BB1_337;

$L__BB1_338:
	add.s32 	%r1893, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1893, %r644};
	add.s32 	%r1894, %r635, 5;
	st.local.v2.u32 	[%rd26+8], {%r1894, %r1467};
	st.local.v2.u32 	[%rd26+16], {%r1468, %r680};
	mov.u64 	%rd1915, $str$1;
	cvta.global.u64 	%rd1916, %rd1915;
	{ // callseq 417, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1916;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1470, [retval0+0];
	} // callseq 417
	bra.uni 	$L__BB1_339;

$L__BB1_337:
	cvta.to.global.u64 	%rd1908, %rd241;
	mul.wide.s32 	%rd1909, %r679, %r684;
	add.s64 	%rd1899, %rd242, %rd1909;
	// begin inline asm
	{ atom.add.f64 %fd5419,[%rd1899],%fd10761; }

	// end inline asm
	add.s64 	%rd1900, %rd1899, 8;
	// begin inline asm
	{ atom.add.f64 %fd5421,[%rd1900],%fd10760; }

	// end inline asm
	add.s64 	%rd1901, %rd1899, 16;
	// begin inline asm
	{ atom.add.f64 %fd5423,[%rd1901],%fd10759; }

	// end inline asm
	add.s64 	%rd1902, %rd1899, 24;
	// begin inline asm
	{ atom.add.f64 %fd5425,[%rd1902],%fd10749; }

	// end inline asm
	add.s64 	%rd1903, %rd1899, 32;
	// begin inline asm
	{ atom.add.f64 %fd5427,[%rd1903],%fd10748; }

	// end inline asm
	add.s64 	%rd1904, %rd1899, 40;
	// begin inline asm
	{ atom.add.f64 %fd5429,[%rd1904],%fd10747; }

	// end inline asm
	add.s64 	%rd1905, %rd1899, 48;
	// begin inline asm
	{ atom.add.f64 %fd5431,[%rd1905],%fd10737; }

	// end inline asm
	add.s64 	%rd1906, %rd1899, 56;
	// begin inline asm
	{ atom.add.f64 %fd5433,[%rd1906],%fd10736; }

	// end inline asm
	add.s64 	%rd1907, %rd1899, 64;
	// begin inline asm
	{ atom.add.f64 %fd5435,[%rd1907],%fd10735; }

	// end inline asm
	mul.wide.s32 	%rd1910, %r677, %r684;
	cvta.to.global.u64 	%rd1911, %rd240;
	add.s64 	%rd1912, %rd1911, %rd1910;
	mul.wide.s32 	%rd1913, %r678, %r684;
	add.s64 	%rd1914, %rd1908, %rd1913;
	add.s32 	%r1891, %r633, 1;
	st.global.u32 	[%rd1912], %r1891;
	add.s32 	%r1892, %r634, 1;
	st.global.u32 	[%rd1914], %r1892;

$L__BB1_339:
	ld.param.u64 	%rd243, [%rd27];
	ld.param.u32 	%r686, [%rd27+32];
	ld.param.u64 	%rd244, [%rd27+56];
	ld.param.u32 	%r687, [%rd27+88];
	ld.param.u64 	%rd245, [%rd27+112];
	ld.param.u32 	%r688, [%rd27+144];
	ld.param.u32 	%r689, [%rd27+172];
	ld.param.v2.u32 	{%r1471, %r1472}, [%rd27+176];
	setp.le.s32 	%p581, %r1471, %r674;
	setp.le.s32 	%p582, %r1472, %r654;
	add.s32 	%r693, %r635, 6;
	setp.le.s32 	%p583, %r689, %r693;
	or.pred  	%p584, %p581, %p582;
	or.b32  	%r1473, %r674, %r693;
	or.b32  	%r694, %r1473, %r654;
	setp.lt.s32 	%p585, %r694, 0;
	or.pred  	%p586, %p585, %p584;
	or.pred  	%p587, %p583, %p586;
	@%p587 bra 	$L__BB1_341;
	bra.uni 	$L__BB1_340;

$L__BB1_341:
	add.s32 	%r1897, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1897, %r654};
	add.s32 	%r1898, %r635, 6;
	st.local.v2.u32 	[%rd26+8], {%r1898, %r1471};
	st.local.v2.u32 	[%rd26+16], {%r1472, %r689};
	mov.u64 	%rd1934, $str$1;
	cvta.global.u64 	%rd1935, %rd1934;
	{ // callseq 418, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1935;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1474, [retval0+0];
	} // callseq 418
	bra.uni 	$L__BB1_342;

$L__BB1_340:
	cvta.to.global.u64 	%rd1927, %rd244;
	mul.wide.s32 	%rd1928, %r688, %r693;
	add.s64 	%rd1918, %rd245, %rd1928;
	// begin inline asm
	{ atom.add.f64 %fd5437,[%rd1918],%fd10758; }

	// end inline asm
	add.s64 	%rd1919, %rd1918, 8;
	// begin inline asm
	{ atom.add.f64 %fd5439,[%rd1919],%fd10757; }

	// end inline asm
	add.s64 	%rd1920, %rd1918, 16;
	// begin inline asm
	{ atom.add.f64 %fd5441,[%rd1920],%fd10756; }

	// end inline asm
	add.s64 	%rd1921, %rd1918, 24;
	// begin inline asm
	{ atom.add.f64 %fd5443,[%rd1921],%fd10746; }

	// end inline asm
	add.s64 	%rd1922, %rd1918, 32;
	// begin inline asm
	{ atom.add.f64 %fd5445,[%rd1922],%fd10745; }

	// end inline asm
	add.s64 	%rd1923, %rd1918, 40;
	// begin inline asm
	{ atom.add.f64 %fd5447,[%rd1923],%fd10744; }

	// end inline asm
	add.s64 	%rd1924, %rd1918, 48;
	// begin inline asm
	{ atom.add.f64 %fd5449,[%rd1924],%fd10734; }

	// end inline asm
	add.s64 	%rd1925, %rd1918, 56;
	// begin inline asm
	{ atom.add.f64 %fd5451,[%rd1925],%fd10733; }

	// end inline asm
	add.s64 	%rd1926, %rd1918, 64;
	// begin inline asm
	{ atom.add.f64 %fd5453,[%rd1926],%fd10732; }

	// end inline asm
	mul.wide.s32 	%rd1929, %r686, %r693;
	cvta.to.global.u64 	%rd1930, %rd243;
	add.s64 	%rd1931, %rd1930, %rd1929;
	mul.wide.s32 	%rd1932, %r687, %r693;
	add.s64 	%rd1933, %rd1927, %rd1932;
	add.s32 	%r1895, %r633, 1;
	st.global.u32 	[%rd1931], %r1895;
	add.s32 	%r1896, %r634, 2;
	st.global.u32 	[%rd1933], %r1896;

$L__BB1_342:
	ld.param.u64 	%rd246, [%rd27];
	ld.param.u32 	%r695, [%rd27+32];
	ld.param.u64 	%rd247, [%rd27+56];
	ld.param.u32 	%r696, [%rd27+88];
	ld.param.u64 	%rd248, [%rd27+112];
	ld.param.u32 	%r697, [%rd27+144];
	ld.param.u32 	%r698, [%rd27+172];
	ld.param.v2.u32 	{%r1475, %r1476}, [%rd27+176];
	setp.le.s32 	%p588, %r1475, %r674;
	setp.le.s32 	%p589, %r1476, %r664;
	add.s32 	%r702, %r635, 7;
	setp.le.s32 	%p590, %r698, %r702;
	or.pred  	%p591, %p588, %p589;
	or.b32  	%r1477, %r674, %r702;
	or.b32  	%r703, %r1477, %r664;
	setp.lt.s32 	%p592, %r703, 0;
	or.pred  	%p593, %p592, %p591;
	or.pred  	%p594, %p590, %p593;
	@%p594 bra 	$L__BB1_344;
	bra.uni 	$L__BB1_343;

$L__BB1_344:
	add.s32 	%r1901, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1901, %r664};
	add.s32 	%r1902, %r635, 7;
	st.local.v2.u32 	[%rd26+8], {%r1902, %r1475};
	st.local.v2.u32 	[%rd26+16], {%r1476, %r698};
	mov.u64 	%rd1953, $str$1;
	cvta.global.u64 	%rd1954, %rd1953;
	{ // callseq 419, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1954;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1478, [retval0+0];
	} // callseq 419
	bra.uni 	$L__BB1_345;

$L__BB1_343:
	cvta.to.global.u64 	%rd1946, %rd247;
	mul.wide.s32 	%rd1947, %r697, %r702;
	add.s64 	%rd1937, %rd248, %rd1947;
	// begin inline asm
	{ atom.add.f64 %fd5455,[%rd1937],%fd10755; }

	// end inline asm
	add.s64 	%rd1938, %rd1937, 8;
	// begin inline asm
	{ atom.add.f64 %fd5457,[%rd1938],%fd10754; }

	// end inline asm
	add.s64 	%rd1939, %rd1937, 16;
	// begin inline asm
	{ atom.add.f64 %fd5459,[%rd1939],%fd10753; }

	// end inline asm
	add.s64 	%rd1940, %rd1937, 24;
	// begin inline asm
	{ atom.add.f64 %fd5461,[%rd1940],%fd10743; }

	// end inline asm
	add.s64 	%rd1941, %rd1937, 32;
	// begin inline asm
	{ atom.add.f64 %fd5463,[%rd1941],%fd10742; }

	// end inline asm
	add.s64 	%rd1942, %rd1937, 40;
	// begin inline asm
	{ atom.add.f64 %fd5465,[%rd1942],%fd10741; }

	// end inline asm
	add.s64 	%rd1943, %rd1937, 48;
	// begin inline asm
	{ atom.add.f64 %fd5467,[%rd1943],%fd10731; }

	// end inline asm
	add.s64 	%rd1944, %rd1937, 56;
	// begin inline asm
	{ atom.add.f64 %fd5469,[%rd1944],%fd10730; }

	// end inline asm
	add.s64 	%rd1945, %rd1937, 64;
	// begin inline asm
	{ atom.add.f64 %fd5471,[%rd1945],%fd10729; }

	// end inline asm
	mul.wide.s32 	%rd1948, %r695, %r702;
	cvta.to.global.u64 	%rd1949, %rd246;
	add.s64 	%rd1950, %rd1949, %rd1948;
	mul.wide.s32 	%rd1951, %r696, %r702;
	add.s64 	%rd1952, %rd1946, %rd1951;
	add.s32 	%r1899, %r633, 1;
	st.global.u32 	[%rd1950], %r1899;
	add.s32 	%r1900, %r634, 3;
	st.global.u32 	[%rd1952], %r1900;

$L__BB1_345:
	ld.param.u64 	%rd249, [%rd27];
	ld.param.u32 	%r704, [%rd27+32];
	ld.param.u64 	%rd250, [%rd27+56];
	ld.param.u32 	%r705, [%rd27+88];
	ld.param.u64 	%rd251, [%rd27+112];
	ld.param.u32 	%r706, [%rd27+144];
	ld.param.u32 	%r707, [%rd27+172];
	ld.param.v2.u32 	{%r1479, %r1480}, [%rd27+176];
	add.s32 	%r711, %r633, 2;
	setp.le.s32 	%p595, %r1479, %r711;
	setp.le.s32 	%p596, %r1480, %r634;
	add.s32 	%r712, %r635, 8;
	setp.le.s32 	%p597, %r707, %r712;
	or.pred  	%p598, %p595, %p596;
	or.b32  	%r1481, %r634, %r712;
	or.b32  	%r713, %r1481, %r711;
	setp.lt.s32 	%p599, %r713, 0;
	or.pred  	%p600, %p599, %p598;
	or.pred  	%p601, %p597, %p600;
	@%p601 bra 	$L__BB1_347;
	bra.uni 	$L__BB1_346;

$L__BB1_347:
	add.s32 	%r1904, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1904, %r634};
	add.s32 	%r1905, %r635, 8;
	st.local.v2.u32 	[%rd26+8], {%r1905, %r1479};
	st.local.v2.u32 	[%rd26+16], {%r1480, %r707};
	mov.u64 	%rd1972, $str$1;
	cvta.global.u64 	%rd1973, %rd1972;
	{ // callseq 420, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1973;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1482, [retval0+0];
	} // callseq 420
	bra.uni 	$L__BB1_348;

$L__BB1_346:
	cvta.to.global.u64 	%rd1965, %rd250;
	mul.wide.s32 	%rd1966, %r706, %r712;
	add.s64 	%rd1956, %rd251, %rd1966;
	// begin inline asm
	{ atom.add.f64 %fd5473,[%rd1956],%fd10728; }

	// end inline asm
	add.s64 	%rd1957, %rd1956, 8;
	// begin inline asm
	{ atom.add.f64 %fd5475,[%rd1957],%fd10727; }

	// end inline asm
	add.s64 	%rd1958, %rd1956, 16;
	// begin inline asm
	{ atom.add.f64 %fd5477,[%rd1958],%fd10726; }

	// end inline asm
	add.s64 	%rd1959, %rd1956, 24;
	// begin inline asm
	{ atom.add.f64 %fd5479,[%rd1959],%fd10716; }

	// end inline asm
	add.s64 	%rd1960, %rd1956, 32;
	// begin inline asm
	{ atom.add.f64 %fd5481,[%rd1960],%fd10715; }

	// end inline asm
	add.s64 	%rd1961, %rd1956, 40;
	// begin inline asm
	{ atom.add.f64 %fd5483,[%rd1961],%fd10714; }

	// end inline asm
	add.s64 	%rd1962, %rd1956, 48;
	// begin inline asm
	{ atom.add.f64 %fd5485,[%rd1962],%fd10704; }

	// end inline asm
	add.s64 	%rd1963, %rd1956, 56;
	// begin inline asm
	{ atom.add.f64 %fd5487,[%rd1963],%fd10703; }

	// end inline asm
	add.s64 	%rd1964, %rd1956, 64;
	// begin inline asm
	{ atom.add.f64 %fd5489,[%rd1964],%fd10702; }

	// end inline asm
	mul.wide.s32 	%rd1967, %r704, %r712;
	cvta.to.global.u64 	%rd1968, %rd249;
	add.s64 	%rd1969, %rd1968, %rd1967;
	mul.wide.s32 	%rd1970, %r705, %r712;
	add.s64 	%rd1971, %rd1965, %rd1970;
	add.s32 	%r1903, %r633, 2;
	st.global.u32 	[%rd1969], %r1903;
	st.global.u32 	[%rd1971], %r634;

$L__BB1_348:
	ld.param.u64 	%rd252, [%rd27];
	ld.param.u32 	%r714, [%rd27+32];
	ld.param.u64 	%rd253, [%rd27+56];
	ld.param.u32 	%r715, [%rd27+88];
	ld.param.u64 	%rd254, [%rd27+112];
	ld.param.u32 	%r716, [%rd27+144];
	ld.param.u32 	%r717, [%rd27+172];
	ld.param.v2.u32 	{%r1483, %r1484}, [%rd27+176];
	setp.le.s32 	%p602, %r1483, %r711;
	setp.le.s32 	%p603, %r1484, %r644;
	add.s32 	%r721, %r635, 9;
	setp.le.s32 	%p604, %r717, %r721;
	or.pred  	%p605, %p602, %p603;
	or.b32  	%r1485, %r711, %r721;
	or.b32  	%r722, %r1485, %r644;
	setp.lt.s32 	%p606, %r722, 0;
	or.pred  	%p607, %p606, %p605;
	or.pred  	%p608, %p604, %p607;
	@%p608 bra 	$L__BB1_350;
	bra.uni 	$L__BB1_349;

$L__BB1_350:
	add.s32 	%r1908, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1908, %r644};
	add.s32 	%r1909, %r635, 9;
	st.local.v2.u32 	[%rd26+8], {%r1909, %r1483};
	st.local.v2.u32 	[%rd26+16], {%r1484, %r717};
	mov.u64 	%rd1991, $str$1;
	cvta.global.u64 	%rd1992, %rd1991;
	{ // callseq 421, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1992;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1486, [retval0+0];
	} // callseq 421
	bra.uni 	$L__BB1_351;

$L__BB1_349:
	cvta.to.global.u64 	%rd1984, %rd253;
	mul.wide.s32 	%rd1985, %r716, %r721;
	add.s64 	%rd1975, %rd254, %rd1985;
	// begin inline asm
	{ atom.add.f64 %fd5491,[%rd1975],%fd10725; }

	// end inline asm
	add.s64 	%rd1976, %rd1975, 8;
	// begin inline asm
	{ atom.add.f64 %fd5493,[%rd1976],%fd10724; }

	// end inline asm
	add.s64 	%rd1977, %rd1975, 16;
	// begin inline asm
	{ atom.add.f64 %fd5495,[%rd1977],%fd10723; }

	// end inline asm
	add.s64 	%rd1978, %rd1975, 24;
	// begin inline asm
	{ atom.add.f64 %fd5497,[%rd1978],%fd10713; }

	// end inline asm
	add.s64 	%rd1979, %rd1975, 32;
	// begin inline asm
	{ atom.add.f64 %fd5499,[%rd1979],%fd10712; }

	// end inline asm
	add.s64 	%rd1980, %rd1975, 40;
	// begin inline asm
	{ atom.add.f64 %fd5501,[%rd1980],%fd10711; }

	// end inline asm
	add.s64 	%rd1981, %rd1975, 48;
	// begin inline asm
	{ atom.add.f64 %fd5503,[%rd1981],%fd10701; }

	// end inline asm
	add.s64 	%rd1982, %rd1975, 56;
	// begin inline asm
	{ atom.add.f64 %fd5505,[%rd1982],%fd10700; }

	// end inline asm
	add.s64 	%rd1983, %rd1975, 64;
	// begin inline asm
	{ atom.add.f64 %fd5507,[%rd1983],%fd10699; }

	// end inline asm
	mul.wide.s32 	%rd1986, %r714, %r721;
	cvta.to.global.u64 	%rd1987, %rd252;
	add.s64 	%rd1988, %rd1987, %rd1986;
	mul.wide.s32 	%rd1989, %r715, %r721;
	add.s64 	%rd1990, %rd1984, %rd1989;
	add.s32 	%r1906, %r633, 2;
	st.global.u32 	[%rd1988], %r1906;
	add.s32 	%r1907, %r634, 1;
	st.global.u32 	[%rd1990], %r1907;

$L__BB1_351:
	ld.param.u64 	%rd255, [%rd27];
	ld.param.u32 	%r723, [%rd27+32];
	ld.param.u64 	%rd256, [%rd27+56];
	ld.param.u32 	%r724, [%rd27+88];
	ld.param.u64 	%rd257, [%rd27+112];
	ld.param.u32 	%r725, [%rd27+144];
	ld.param.u32 	%r726, [%rd27+172];
	ld.param.v2.u32 	{%r1487, %r1488}, [%rd27+176];
	setp.le.s32 	%p609, %r1487, %r711;
	setp.le.s32 	%p610, %r1488, %r654;
	add.s32 	%r730, %r635, 10;
	setp.le.s32 	%p611, %r726, %r730;
	or.pred  	%p612, %p609, %p610;
	or.b32  	%r1489, %r711, %r730;
	or.b32  	%r731, %r1489, %r654;
	setp.lt.s32 	%p613, %r731, 0;
	or.pred  	%p614, %p613, %p612;
	or.pred  	%p615, %p611, %p614;
	@%p615 bra 	$L__BB1_353;
	bra.uni 	$L__BB1_352;

$L__BB1_353:
	add.s32 	%r1912, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1912, %r654};
	add.s32 	%r1913, %r635, 10;
	st.local.v2.u32 	[%rd26+8], {%r1913, %r1487};
	st.local.v2.u32 	[%rd26+16], {%r1488, %r726};
	mov.u64 	%rd2010, $str$1;
	cvta.global.u64 	%rd2011, %rd2010;
	{ // callseq 422, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2011;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1490, [retval0+0];
	} // callseq 422
	bra.uni 	$L__BB1_354;

$L__BB1_352:
	cvta.to.global.u64 	%rd2003, %rd256;
	mul.wide.s32 	%rd2004, %r725, %r730;
	add.s64 	%rd1994, %rd257, %rd2004;
	// begin inline asm
	{ atom.add.f64 %fd5509,[%rd1994],%fd10722; }

	// end inline asm
	add.s64 	%rd1995, %rd1994, 8;
	// begin inline asm
	{ atom.add.f64 %fd5511,[%rd1995],%fd10721; }

	// end inline asm
	add.s64 	%rd1996, %rd1994, 16;
	// begin inline asm
	{ atom.add.f64 %fd5513,[%rd1996],%fd10720; }

	// end inline asm
	add.s64 	%rd1997, %rd1994, 24;
	// begin inline asm
	{ atom.add.f64 %fd5515,[%rd1997],%fd10710; }

	// end inline asm
	add.s64 	%rd1998, %rd1994, 32;
	// begin inline asm
	{ atom.add.f64 %fd5517,[%rd1998],%fd10709; }

	// end inline asm
	add.s64 	%rd1999, %rd1994, 40;
	// begin inline asm
	{ atom.add.f64 %fd5519,[%rd1999],%fd10708; }

	// end inline asm
	add.s64 	%rd2000, %rd1994, 48;
	// begin inline asm
	{ atom.add.f64 %fd5521,[%rd2000],%fd10698; }

	// end inline asm
	add.s64 	%rd2001, %rd1994, 56;
	// begin inline asm
	{ atom.add.f64 %fd5523,[%rd2001],%fd10697; }

	// end inline asm
	add.s64 	%rd2002, %rd1994, 64;
	// begin inline asm
	{ atom.add.f64 %fd5525,[%rd2002],%fd10696; }

	// end inline asm
	mul.wide.s32 	%rd2005, %r723, %r730;
	cvta.to.global.u64 	%rd2006, %rd255;
	add.s64 	%rd2007, %rd2006, %rd2005;
	mul.wide.s32 	%rd2008, %r724, %r730;
	add.s64 	%rd2009, %rd2003, %rd2008;
	add.s32 	%r1910, %r633, 2;
	st.global.u32 	[%rd2007], %r1910;
	add.s32 	%r1911, %r634, 2;
	st.global.u32 	[%rd2009], %r1911;

$L__BB1_354:
	ld.param.u64 	%rd258, [%rd27];
	ld.param.u32 	%r732, [%rd27+32];
	ld.param.u64 	%rd259, [%rd27+56];
	ld.param.u32 	%r733, [%rd27+88];
	ld.param.u64 	%rd260, [%rd27+112];
	ld.param.u32 	%r734, [%rd27+144];
	ld.param.u32 	%r735, [%rd27+172];
	ld.param.v2.u32 	{%r1491, %r1492}, [%rd27+176];
	setp.le.s32 	%p616, %r1491, %r711;
	setp.le.s32 	%p617, %r1492, %r664;
	add.s32 	%r739, %r635, 11;
	setp.le.s32 	%p618, %r735, %r739;
	or.pred  	%p619, %p616, %p617;
	or.b32  	%r1493, %r711, %r739;
	or.b32  	%r740, %r1493, %r664;
	setp.lt.s32 	%p620, %r740, 0;
	or.pred  	%p621, %p620, %p619;
	or.pred  	%p622, %p618, %p621;
	@%p622 bra 	$L__BB1_356;
	bra.uni 	$L__BB1_355;

$L__BB1_356:
	add.s32 	%r1916, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1916, %r664};
	add.s32 	%r1917, %r635, 11;
	st.local.v2.u32 	[%rd26+8], {%r1917, %r1491};
	st.local.v2.u32 	[%rd26+16], {%r1492, %r735};
	mov.u64 	%rd2029, $str$1;
	cvta.global.u64 	%rd2030, %rd2029;
	{ // callseq 423, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2030;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1494, [retval0+0];
	} // callseq 423
	bra.uni 	$L__BB1_357;

$L__BB1_355:
	cvta.to.global.u64 	%rd2022, %rd259;
	mul.wide.s32 	%rd2023, %r734, %r739;
	add.s64 	%rd2013, %rd260, %rd2023;
	// begin inline asm
	{ atom.add.f64 %fd5527,[%rd2013],%fd10719; }

	// end inline asm
	add.s64 	%rd2014, %rd2013, 8;
	// begin inline asm
	{ atom.add.f64 %fd5529,[%rd2014],%fd10718; }

	// end inline asm
	add.s64 	%rd2015, %rd2013, 16;
	// begin inline asm
	{ atom.add.f64 %fd5531,[%rd2015],%fd10717; }

	// end inline asm
	add.s64 	%rd2016, %rd2013, 24;
	// begin inline asm
	{ atom.add.f64 %fd5533,[%rd2016],%fd10707; }

	// end inline asm
	add.s64 	%rd2017, %rd2013, 32;
	// begin inline asm
	{ atom.add.f64 %fd5535,[%rd2017],%fd10706; }

	// end inline asm
	add.s64 	%rd2018, %rd2013, 40;
	// begin inline asm
	{ atom.add.f64 %fd5537,[%rd2018],%fd10705; }

	// end inline asm
	add.s64 	%rd2019, %rd2013, 48;
	// begin inline asm
	{ atom.add.f64 %fd5539,[%rd2019],%fd10695; }

	// end inline asm
	add.s64 	%rd2020, %rd2013, 56;
	// begin inline asm
	{ atom.add.f64 %fd5541,[%rd2020],%fd10694; }

	// end inline asm
	add.s64 	%rd2021, %rd2013, 64;
	// begin inline asm
	{ atom.add.f64 %fd5543,[%rd2021],%fd10693; }

	// end inline asm
	mul.wide.s32 	%rd2024, %r732, %r739;
	cvta.to.global.u64 	%rd2025, %rd258;
	add.s64 	%rd2026, %rd2025, %rd2024;
	mul.wide.s32 	%rd2027, %r733, %r739;
	add.s64 	%rd2028, %rd2022, %rd2027;
	add.s32 	%r1914, %r633, 2;
	st.global.u32 	[%rd2026], %r1914;
	add.s32 	%r1915, %r634, 3;
	st.global.u32 	[%rd2028], %r1915;

$L__BB1_357:
	ld.param.u64 	%rd261, [%rd27];
	ld.param.u32 	%r741, [%rd27+32];
	ld.param.u64 	%rd262, [%rd27+56];
	ld.param.u32 	%r742, [%rd27+88];
	ld.param.u64 	%rd263, [%rd27+112];
	ld.param.u32 	%r743, [%rd27+144];
	ld.param.u32 	%r744, [%rd27+172];
	ld.param.v2.u32 	{%r1495, %r1496}, [%rd27+176];
	add.s32 	%r748, %r633, 3;
	setp.le.s32 	%p623, %r1495, %r748;
	setp.le.s32 	%p624, %r1496, %r634;
	add.s32 	%r749, %r635, 12;
	setp.le.s32 	%p625, %r744, %r749;
	or.pred  	%p626, %p623, %p624;
	or.b32  	%r1497, %r634, %r749;
	or.b32  	%r750, %r1497, %r748;
	setp.lt.s32 	%p627, %r750, 0;
	or.pred  	%p628, %p627, %p626;
	or.pred  	%p629, %p625, %p628;
	@%p629 bra 	$L__BB1_359;
	bra.uni 	$L__BB1_358;

$L__BB1_359:
	add.s32 	%r1919, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1919, %r634};
	add.s32 	%r1920, %r635, 12;
	st.local.v2.u32 	[%rd26+8], {%r1920, %r1495};
	st.local.v2.u32 	[%rd26+16], {%r1496, %r744};
	mov.u64 	%rd2048, $str$1;
	cvta.global.u64 	%rd2049, %rd2048;
	{ // callseq 424, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2049;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1498, [retval0+0];
	} // callseq 424
	bra.uni 	$L__BB1_360;

$L__BB1_358:
	cvta.to.global.u64 	%rd2041, %rd262;
	mul.wide.s32 	%rd2042, %r743, %r749;
	add.s64 	%rd2032, %rd263, %rd2042;
	// begin inline asm
	{ atom.add.f64 %fd5545,[%rd2032],%fd10692; }

	// end inline asm
	add.s64 	%rd2033, %rd2032, 8;
	// begin inline asm
	{ atom.add.f64 %fd5547,[%rd2033],%fd10691; }

	// end inline asm
	add.s64 	%rd2034, %rd2032, 16;
	// begin inline asm
	{ atom.add.f64 %fd5549,[%rd2034],%fd10690; }

	// end inline asm
	add.s64 	%rd2035, %rd2032, 24;
	// begin inline asm
	{ atom.add.f64 %fd5551,[%rd2035],%fd10680; }

	// end inline asm
	add.s64 	%rd2036, %rd2032, 32;
	// begin inline asm
	{ atom.add.f64 %fd5553,[%rd2036],%fd10679; }

	// end inline asm
	add.s64 	%rd2037, %rd2032, 40;
	// begin inline asm
	{ atom.add.f64 %fd5555,[%rd2037],%fd10678; }

	// end inline asm
	add.s64 	%rd2038, %rd2032, 48;
	// begin inline asm
	{ atom.add.f64 %fd5557,[%rd2038],%fd10668; }

	// end inline asm
	add.s64 	%rd2039, %rd2032, 56;
	// begin inline asm
	{ atom.add.f64 %fd5559,[%rd2039],%fd10667; }

	// end inline asm
	add.s64 	%rd2040, %rd2032, 64;
	// begin inline asm
	{ atom.add.f64 %fd5561,[%rd2040],%fd10666; }

	// end inline asm
	mul.wide.s32 	%rd2043, %r741, %r749;
	cvta.to.global.u64 	%rd2044, %rd261;
	add.s64 	%rd2045, %rd2044, %rd2043;
	mul.wide.s32 	%rd2046, %r742, %r749;
	add.s64 	%rd2047, %rd2041, %rd2046;
	add.s32 	%r1918, %r633, 3;
	st.global.u32 	[%rd2045], %r1918;
	st.global.u32 	[%rd2047], %r634;

$L__BB1_360:
	ld.param.u64 	%rd264, [%rd27];
	ld.param.u32 	%r751, [%rd27+32];
	ld.param.u64 	%rd265, [%rd27+56];
	ld.param.u32 	%r752, [%rd27+88];
	ld.param.u64 	%rd266, [%rd27+112];
	ld.param.u32 	%r753, [%rd27+144];
	ld.param.u32 	%r754, [%rd27+172];
	ld.param.v2.u32 	{%r1499, %r1500}, [%rd27+176];
	setp.le.s32 	%p630, %r1499, %r748;
	setp.le.s32 	%p631, %r1500, %r644;
	add.s32 	%r758, %r635, 13;
	setp.le.s32 	%p632, %r754, %r758;
	or.pred  	%p633, %p630, %p631;
	or.b32  	%r1501, %r748, %r758;
	or.b32  	%r759, %r1501, %r644;
	setp.lt.s32 	%p634, %r759, 0;
	or.pred  	%p635, %p634, %p633;
	or.pred  	%p636, %p632, %p635;
	@%p636 bra 	$L__BB1_362;
	bra.uni 	$L__BB1_361;

$L__BB1_362:
	add.s32 	%r1923, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1923, %r644};
	add.s32 	%r1924, %r635, 13;
	st.local.v2.u32 	[%rd26+8], {%r1924, %r1499};
	st.local.v2.u32 	[%rd26+16], {%r1500, %r754};
	mov.u64 	%rd2067, $str$1;
	cvta.global.u64 	%rd2068, %rd2067;
	{ // callseq 425, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2068;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1502, [retval0+0];
	} // callseq 425
	bra.uni 	$L__BB1_363;

$L__BB1_361:
	cvta.to.global.u64 	%rd2060, %rd265;
	mul.wide.s32 	%rd2061, %r753, %r758;
	add.s64 	%rd2051, %rd266, %rd2061;
	// begin inline asm
	{ atom.add.f64 %fd5563,[%rd2051],%fd10689; }

	// end inline asm
	add.s64 	%rd2052, %rd2051, 8;
	// begin inline asm
	{ atom.add.f64 %fd5565,[%rd2052],%fd10688; }

	// end inline asm
	add.s64 	%rd2053, %rd2051, 16;
	// begin inline asm
	{ atom.add.f64 %fd5567,[%rd2053],%fd10687; }

	// end inline asm
	add.s64 	%rd2054, %rd2051, 24;
	// begin inline asm
	{ atom.add.f64 %fd5569,[%rd2054],%fd10677; }

	// end inline asm
	add.s64 	%rd2055, %rd2051, 32;
	// begin inline asm
	{ atom.add.f64 %fd5571,[%rd2055],%fd10676; }

	// end inline asm
	add.s64 	%rd2056, %rd2051, 40;
	// begin inline asm
	{ atom.add.f64 %fd5573,[%rd2056],%fd10675; }

	// end inline asm
	add.s64 	%rd2057, %rd2051, 48;
	// begin inline asm
	{ atom.add.f64 %fd5575,[%rd2057],%fd10665; }

	// end inline asm
	add.s64 	%rd2058, %rd2051, 56;
	// begin inline asm
	{ atom.add.f64 %fd5577,[%rd2058],%fd10664; }

	// end inline asm
	add.s64 	%rd2059, %rd2051, 64;
	// begin inline asm
	{ atom.add.f64 %fd5579,[%rd2059],%fd10663; }

	// end inline asm
	mul.wide.s32 	%rd2062, %r751, %r758;
	cvta.to.global.u64 	%rd2063, %rd264;
	add.s64 	%rd2064, %rd2063, %rd2062;
	mul.wide.s32 	%rd2065, %r752, %r758;
	add.s64 	%rd2066, %rd2060, %rd2065;
	add.s32 	%r1921, %r633, 3;
	st.global.u32 	[%rd2064], %r1921;
	add.s32 	%r1922, %r634, 1;
	st.global.u32 	[%rd2066], %r1922;

$L__BB1_363:
	ld.param.u64 	%rd267, [%rd27];
	ld.param.u32 	%r760, [%rd27+32];
	ld.param.u64 	%rd268, [%rd27+56];
	ld.param.u32 	%r761, [%rd27+88];
	ld.param.u64 	%rd269, [%rd27+112];
	ld.param.u32 	%r762, [%rd27+144];
	ld.param.u32 	%r763, [%rd27+172];
	ld.param.v2.u32 	{%r1503, %r1504}, [%rd27+176];
	setp.le.s32 	%p637, %r1503, %r748;
	setp.le.s32 	%p638, %r1504, %r654;
	add.s32 	%r767, %r635, 14;
	setp.le.s32 	%p639, %r763, %r767;
	or.pred  	%p640, %p637, %p638;
	or.b32  	%r1505, %r748, %r767;
	or.b32  	%r768, %r1505, %r654;
	setp.lt.s32 	%p641, %r768, 0;
	or.pred  	%p642, %p641, %p640;
	or.pred  	%p643, %p639, %p642;
	@%p643 bra 	$L__BB1_365;
	bra.uni 	$L__BB1_364;

$L__BB1_365:
	add.s32 	%r1927, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1927, %r654};
	add.s32 	%r1928, %r635, 14;
	st.local.v2.u32 	[%rd26+8], {%r1928, %r1503};
	st.local.v2.u32 	[%rd26+16], {%r1504, %r763};
	mov.u64 	%rd2086, $str$1;
	cvta.global.u64 	%rd2087, %rd2086;
	{ // callseq 426, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2087;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1506, [retval0+0];
	} // callseq 426
	bra.uni 	$L__BB1_366;

$L__BB1_364:
	cvta.to.global.u64 	%rd2079, %rd268;
	mul.wide.s32 	%rd2080, %r762, %r767;
	add.s64 	%rd2070, %rd269, %rd2080;
	// begin inline asm
	{ atom.add.f64 %fd5581,[%rd2070],%fd10686; }

	// end inline asm
	add.s64 	%rd2071, %rd2070, 8;
	// begin inline asm
	{ atom.add.f64 %fd5583,[%rd2071],%fd10685; }

	// end inline asm
	add.s64 	%rd2072, %rd2070, 16;
	// begin inline asm
	{ atom.add.f64 %fd5585,[%rd2072],%fd10684; }

	// end inline asm
	add.s64 	%rd2073, %rd2070, 24;
	// begin inline asm
	{ atom.add.f64 %fd5587,[%rd2073],%fd10674; }

	// end inline asm
	add.s64 	%rd2074, %rd2070, 32;
	// begin inline asm
	{ atom.add.f64 %fd5589,[%rd2074],%fd10673; }

	// end inline asm
	add.s64 	%rd2075, %rd2070, 40;
	// begin inline asm
	{ atom.add.f64 %fd5591,[%rd2075],%fd10672; }

	// end inline asm
	add.s64 	%rd2076, %rd2070, 48;
	// begin inline asm
	{ atom.add.f64 %fd5593,[%rd2076],%fd10662; }

	// end inline asm
	add.s64 	%rd2077, %rd2070, 56;
	// begin inline asm
	{ atom.add.f64 %fd5595,[%rd2077],%fd10661; }

	// end inline asm
	add.s64 	%rd2078, %rd2070, 64;
	// begin inline asm
	{ atom.add.f64 %fd5597,[%rd2078],%fd10660; }

	// end inline asm
	mul.wide.s32 	%rd2081, %r760, %r767;
	cvta.to.global.u64 	%rd2082, %rd267;
	add.s64 	%rd2083, %rd2082, %rd2081;
	mul.wide.s32 	%rd2084, %r761, %r767;
	add.s64 	%rd2085, %rd2079, %rd2084;
	add.s32 	%r1925, %r633, 3;
	st.global.u32 	[%rd2083], %r1925;
	add.s32 	%r1926, %r634, 2;
	st.global.u32 	[%rd2085], %r1926;

$L__BB1_366:
	ld.param.u64 	%rd270, [%rd27];
	ld.param.u32 	%r769, [%rd27+32];
	ld.param.u64 	%rd271, [%rd27+56];
	ld.param.u32 	%r770, [%rd27+88];
	ld.param.u64 	%rd272, [%rd27+112];
	ld.param.u32 	%r771, [%rd27+144];
	ld.param.u32 	%r772, [%rd27+172];
	ld.param.v2.u32 	{%r1507, %r1508}, [%rd27+176];
	setp.le.s32 	%p644, %r1507, %r748;
	setp.le.s32 	%p645, %r1508, %r664;
	add.s32 	%r776, %r635, 15;
	setp.le.s32 	%p646, %r772, %r776;
	or.pred  	%p647, %p644, %p645;
	or.b32  	%r1509, %r748, %r776;
	or.b32  	%r777, %r1509, %r664;
	setp.lt.s32 	%p648, %r777, 0;
	or.pred  	%p649, %p648, %p647;
	or.pred  	%p650, %p646, %p649;
	@%p650 bra 	$L__BB1_368;
	bra.uni 	$L__BB1_367;

$L__BB1_368:
	add.s32 	%r1931, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1931, %r664};
	add.s32 	%r1932, %r635, 15;
	st.local.v2.u32 	[%rd26+8], {%r1932, %r1507};
	st.local.v2.u32 	[%rd26+16], {%r1508, %r772};
	mov.u64 	%rd2105, $str$1;
	cvta.global.u64 	%rd2106, %rd2105;
	{ // callseq 427, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2106;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1510, [retval0+0];
	} // callseq 427
	bra.uni 	$L__BB1_369;

$L__BB1_367:
	cvta.to.global.u64 	%rd2098, %rd271;
	mul.wide.s32 	%rd2099, %r771, %r776;
	add.s64 	%rd2089, %rd272, %rd2099;
	// begin inline asm
	{ atom.add.f64 %fd5599,[%rd2089],%fd10683; }

	// end inline asm
	add.s64 	%rd2090, %rd2089, 8;
	// begin inline asm
	{ atom.add.f64 %fd5601,[%rd2090],%fd10682; }

	// end inline asm
	add.s64 	%rd2091, %rd2089, 16;
	// begin inline asm
	{ atom.add.f64 %fd5603,[%rd2091],%fd10681; }

	// end inline asm
	add.s64 	%rd2092, %rd2089, 24;
	// begin inline asm
	{ atom.add.f64 %fd5605,[%rd2092],%fd10671; }

	// end inline asm
	add.s64 	%rd2093, %rd2089, 32;
	// begin inline asm
	{ atom.add.f64 %fd5607,[%rd2093],%fd10670; }

	// end inline asm
	add.s64 	%rd2094, %rd2089, 40;
	// begin inline asm
	{ atom.add.f64 %fd5609,[%rd2094],%fd10669; }

	// end inline asm
	add.s64 	%rd2095, %rd2089, 48;
	// begin inline asm
	{ atom.add.f64 %fd5611,[%rd2095],%fd10659; }

	// end inline asm
	add.s64 	%rd2096, %rd2089, 56;
	// begin inline asm
	{ atom.add.f64 %fd5613,[%rd2096],%fd10658; }

	// end inline asm
	add.s64 	%rd2097, %rd2089, 64;
	// begin inline asm
	{ atom.add.f64 %fd5615,[%rd2097],%fd10657; }

	// end inline asm
	mul.wide.s32 	%rd2100, %r769, %r776;
	cvta.to.global.u64 	%rd2101, %rd270;
	add.s64 	%rd2102, %rd2101, %rd2100;
	mul.wide.s32 	%rd2103, %r770, %r776;
	add.s64 	%rd2104, %rd2098, %rd2103;
	add.s32 	%r1929, %r633, 3;
	st.global.u32 	[%rd2102], %r1929;
	add.s32 	%r1930, %r634, 3;
	st.global.u32 	[%rd2104], %r1930;

$L__BB1_369:
	ld.param.u64 	%rd273, [%rd27+120];
	ld.param.u32 	%r778, [%rd27+144];
	ld.param.u32 	%r779, [%rd27+172];
	ld.param.v2.u32 	{%r1511, %r1512}, [%rd27+176];
	setp.le.s32 	%p652, %r1511, %r748;
	setp.le.s32 	%p653, %r1512, %r664;
	setp.le.s32 	%p654, %r779, %r776;
	or.pred  	%p655, %p652, %p653;
	or.pred  	%p656, %p648, %p655;
	or.pred  	%p657, %p654, %p656;
	mov.f64 	%fd11089, 0d0000000000000000;
	mov.f64 	%fd11090, 0d0000000000000000;
	mov.f64 	%fd11091, 0d0000000000000000;
	mov.f64 	%fd11092, 0d0000000000000000;
	mov.f64 	%fd11093, 0d0000000000000000;
	mov.f64 	%fd11094, 0d0000000000000000;
	mov.f64 	%fd11095, 0d0000000000000000;
	mov.f64 	%fd11096, 0d0000000000000000;
	mov.f64 	%fd11097, 0d0000000000000000;
	@%p657 bra 	$L__BB1_372;
	bra.uni 	$L__BB1_370;

$L__BB1_372:
	add.s32 	%r1933, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1933, %r664};
	add.s32 	%r1934, %r635, 15;
	st.local.v2.u32 	[%rd26+8], {%r1934, %r1511};
	st.local.v2.u32 	[%rd26+16], {%r1512, %r779};
	mov.u64 	%rd2111, $str$1;
	cvta.global.u64 	%rd2112, %rd2111;
	{ // callseq 428, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2112;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1513, [retval0+0];
	} // callseq 428
	bra.uni 	$L__BB1_373;

$L__BB1_370:
	setp.eq.s64 	%p658, %rd273, 0;
	@%p658 bra 	$L__BB1_373;

	cvta.to.global.u64 	%rd2108, %rd273;
	mul.wide.s32 	%rd2109, %r778, %r776;
	add.s64 	%rd2110, %rd2108, %rd2109;
	ld.global.f64 	%fd5626, [%rd2110];
	add.f64 	%fd11089, %fd5626, 0d0000000000000000;
	ld.global.f64 	%fd5627, [%rd2110+8];
	add.f64 	%fd11090, %fd5627, 0d0000000000000000;
	ld.global.f64 	%fd5628, [%rd2110+16];
	add.f64 	%fd11091, %fd5628, 0d0000000000000000;
	ld.global.f64 	%fd5629, [%rd2110+24];
	add.f64 	%fd11092, %fd5629, 0d0000000000000000;
	ld.global.f64 	%fd5630, [%rd2110+32];
	add.f64 	%fd11093, %fd5630, 0d0000000000000000;
	ld.global.f64 	%fd5631, [%rd2110+40];
	add.f64 	%fd11094, %fd5631, 0d0000000000000000;
	ld.global.f64 	%fd5632, [%rd2110+48];
	add.f64 	%fd11095, %fd5632, 0d0000000000000000;
	ld.global.f64 	%fd5633, [%rd2110+56];
	add.f64 	%fd11096, %fd5633, 0d0000000000000000;
	ld.global.f64 	%fd5634, [%rd2110+64];
	add.f64 	%fd11097, %fd5634, 0d0000000000000000;

$L__BB1_373:
	add.f64 	%fd1603, %fd11097, 0d0000000000000000;
	add.f64 	%fd1604, %fd11096, 0d0000000000000000;
	add.f64 	%fd1605, %fd11095, 0d0000000000000000;
	add.f64 	%fd1606, %fd11094, 0d0000000000000000;
	add.f64 	%fd1607, %fd11093, 0d0000000000000000;
	add.f64 	%fd1608, %fd11092, 0d0000000000000000;
	add.f64 	%fd1609, %fd11091, 0d0000000000000000;
	add.f64 	%fd1610, %fd11090, 0d0000000000000000;
	add.f64 	%fd1611, %fd11089, 0d0000000000000000;
	ld.param.u64 	%rd274, [%rd27+120];
	ld.param.u32 	%r783, [%rd27+144];
	ld.param.u32 	%r784, [%rd27+172];
	ld.param.v2.u32 	{%r1514, %r1515}, [%rd27+176];
	setp.le.s32 	%p659, %r1514, %r748;
	setp.le.s32 	%p660, %r1515, %r654;
	setp.le.s32 	%p661, %r784, %r767;
	or.pred  	%p662, %p659, %p660;
	or.pred  	%p664, %p641, %p662;
	or.pred  	%p665, %p661, %p664;
	mov.f64 	%fd11098, 0d0000000000000000;
	mov.f64 	%fd11099, 0d0000000000000000;
	mov.f64 	%fd11100, 0d0000000000000000;
	mov.f64 	%fd11101, 0d0000000000000000;
	mov.f64 	%fd11102, 0d0000000000000000;
	mov.f64 	%fd11103, 0d0000000000000000;
	mov.f64 	%fd11104, 0d0000000000000000;
	mov.f64 	%fd11105, 0d0000000000000000;
	mov.f64 	%fd11106, 0d0000000000000000;
	@%p665 bra 	$L__BB1_376;
	bra.uni 	$L__BB1_374;

$L__BB1_376:
	add.s32 	%r1935, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1935, %r654};
	add.s32 	%r1936, %r635, 14;
	st.local.v2.u32 	[%rd26+8], {%r1936, %r1514};
	st.local.v2.u32 	[%rd26+16], {%r1515, %r784};
	mov.u64 	%rd2117, $str$1;
	cvta.global.u64 	%rd2118, %rd2117;
	{ // callseq 429, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2118;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1516, [retval0+0];
	} // callseq 429
	bra.uni 	$L__BB1_377;

$L__BB1_374:
	setp.eq.s64 	%p666, %rd274, 0;
	@%p666 bra 	$L__BB1_377;

	cvta.to.global.u64 	%rd2114, %rd274;
	mul.wide.s32 	%rd2115, %r783, %r767;
	add.s64 	%rd2116, %rd2114, %rd2115;
	ld.global.f64 	%fd5653, [%rd2116];
	add.f64 	%fd11098, %fd5653, 0d0000000000000000;
	ld.global.f64 	%fd5654, [%rd2116+8];
	add.f64 	%fd11099, %fd5654, 0d0000000000000000;
	ld.global.f64 	%fd5655, [%rd2116+16];
	add.f64 	%fd11100, %fd5655, 0d0000000000000000;
	ld.global.f64 	%fd5656, [%rd2116+24];
	add.f64 	%fd11101, %fd5656, 0d0000000000000000;
	ld.global.f64 	%fd5657, [%rd2116+32];
	add.f64 	%fd11102, %fd5657, 0d0000000000000000;
	ld.global.f64 	%fd5658, [%rd2116+40];
	add.f64 	%fd11103, %fd5658, 0d0000000000000000;
	ld.global.f64 	%fd5659, [%rd2116+48];
	add.f64 	%fd11104, %fd5659, 0d0000000000000000;
	ld.global.f64 	%fd5660, [%rd2116+56];
	add.f64 	%fd11105, %fd5660, 0d0000000000000000;
	ld.global.f64 	%fd5661, [%rd2116+64];
	add.f64 	%fd11106, %fd5661, 0d0000000000000000;

$L__BB1_377:
	add.f64 	%fd1630, %fd11106, 0d0000000000000000;
	add.f64 	%fd1631, %fd11105, 0d0000000000000000;
	add.f64 	%fd1632, %fd11104, 0d0000000000000000;
	add.f64 	%fd1633, %fd11103, 0d0000000000000000;
	add.f64 	%fd1634, %fd11102, 0d0000000000000000;
	add.f64 	%fd1635, %fd11101, 0d0000000000000000;
	add.f64 	%fd1636, %fd11100, 0d0000000000000000;
	add.f64 	%fd1637, %fd11099, 0d0000000000000000;
	add.f64 	%fd1638, %fd11098, 0d0000000000000000;
	ld.param.u64 	%rd275, [%rd27+120];
	ld.param.u32 	%r788, [%rd27+144];
	ld.param.u32 	%r789, [%rd27+172];
	ld.param.v2.u32 	{%r1517, %r1518}, [%rd27+176];
	setp.le.s32 	%p667, %r1517, %r748;
	setp.le.s32 	%p668, %r1518, %r644;
	setp.le.s32 	%p669, %r789, %r758;
	or.pred  	%p670, %p667, %p668;
	or.pred  	%p672, %p634, %p670;
	or.pred  	%p673, %p669, %p672;
	mov.f64 	%fd11107, 0d0000000000000000;
	mov.f64 	%fd11108, 0d0000000000000000;
	mov.f64 	%fd11109, 0d0000000000000000;
	mov.f64 	%fd11110, 0d0000000000000000;
	mov.f64 	%fd11111, 0d0000000000000000;
	mov.f64 	%fd11112, 0d0000000000000000;
	mov.f64 	%fd11113, 0d0000000000000000;
	mov.f64 	%fd11114, 0d0000000000000000;
	mov.f64 	%fd11115, 0d0000000000000000;
	@%p673 bra 	$L__BB1_380;
	bra.uni 	$L__BB1_378;

$L__BB1_380:
	add.s32 	%r1937, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1937, %r644};
	add.s32 	%r1938, %r635, 13;
	st.local.v2.u32 	[%rd26+8], {%r1938, %r1517};
	st.local.v2.u32 	[%rd26+16], {%r1518, %r789};
	mov.u64 	%rd2123, $str$1;
	cvta.global.u64 	%rd2124, %rd2123;
	{ // callseq 430, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2124;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1519, [retval0+0];
	} // callseq 430
	bra.uni 	$L__BB1_381;

$L__BB1_378:
	setp.eq.s64 	%p674, %rd275, 0;
	@%p674 bra 	$L__BB1_381;

	cvta.to.global.u64 	%rd2120, %rd275;
	mul.wide.s32 	%rd2121, %r788, %r758;
	add.s64 	%rd2122, %rd2120, %rd2121;
	ld.global.f64 	%fd5680, [%rd2122];
	add.f64 	%fd11107, %fd5680, 0d0000000000000000;
	ld.global.f64 	%fd5681, [%rd2122+8];
	add.f64 	%fd11108, %fd5681, 0d0000000000000000;
	ld.global.f64 	%fd5682, [%rd2122+16];
	add.f64 	%fd11109, %fd5682, 0d0000000000000000;
	ld.global.f64 	%fd5683, [%rd2122+24];
	add.f64 	%fd11110, %fd5683, 0d0000000000000000;
	ld.global.f64 	%fd5684, [%rd2122+32];
	add.f64 	%fd11111, %fd5684, 0d0000000000000000;
	ld.global.f64 	%fd5685, [%rd2122+40];
	add.f64 	%fd11112, %fd5685, 0d0000000000000000;
	ld.global.f64 	%fd5686, [%rd2122+48];
	add.f64 	%fd11113, %fd5686, 0d0000000000000000;
	ld.global.f64 	%fd5687, [%rd2122+56];
	add.f64 	%fd11114, %fd5687, 0d0000000000000000;
	ld.global.f64 	%fd5688, [%rd2122+64];
	add.f64 	%fd11115, %fd5688, 0d0000000000000000;

$L__BB1_381:
	add.f64 	%fd1657, %fd11115, 0d0000000000000000;
	add.f64 	%fd1658, %fd11114, 0d0000000000000000;
	add.f64 	%fd1659, %fd11113, 0d0000000000000000;
	add.f64 	%fd1660, %fd11112, 0d0000000000000000;
	add.f64 	%fd1661, %fd11111, 0d0000000000000000;
	add.f64 	%fd1662, %fd11110, 0d0000000000000000;
	add.f64 	%fd1663, %fd11109, 0d0000000000000000;
	add.f64 	%fd1664, %fd11108, 0d0000000000000000;
	add.f64 	%fd1665, %fd11107, 0d0000000000000000;
	ld.param.u64 	%rd276, [%rd27+120];
	ld.param.u32 	%r793, [%rd27+144];
	ld.param.u32 	%r794, [%rd27+172];
	ld.param.v2.u32 	{%r1520, %r1521}, [%rd27+176];
	setp.le.s32 	%p675, %r1520, %r748;
	setp.le.s32 	%p676, %r1521, %r634;
	setp.le.s32 	%p677, %r794, %r749;
	or.pred  	%p678, %p675, %p676;
	or.pred  	%p680, %p627, %p678;
	or.pred  	%p681, %p677, %p680;
	mov.f64 	%fd11116, 0d0000000000000000;
	mov.f64 	%fd11117, 0d0000000000000000;
	mov.f64 	%fd11118, 0d0000000000000000;
	mov.f64 	%fd11119, 0d0000000000000000;
	mov.f64 	%fd11120, 0d0000000000000000;
	mov.f64 	%fd11121, 0d0000000000000000;
	mov.f64 	%fd11122, 0d0000000000000000;
	mov.f64 	%fd11123, 0d0000000000000000;
	mov.f64 	%fd11124, 0d0000000000000000;
	@%p681 bra 	$L__BB1_384;
	bra.uni 	$L__BB1_382;

$L__BB1_384:
	add.s32 	%r1939, %r633, 3;
	st.local.v2.u32 	[%rd26], {%r1939, %r634};
	add.s32 	%r1940, %r635, 12;
	st.local.v2.u32 	[%rd26+8], {%r1940, %r1520};
	st.local.v2.u32 	[%rd26+16], {%r1521, %r794};
	mov.u64 	%rd2129, $str$1;
	cvta.global.u64 	%rd2130, %rd2129;
	{ // callseq 431, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2130;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1522, [retval0+0];
	} // callseq 431
	bra.uni 	$L__BB1_385;

$L__BB1_382:
	setp.eq.s64 	%p682, %rd276, 0;
	@%p682 bra 	$L__BB1_385;

	cvta.to.global.u64 	%rd2126, %rd276;
	mul.wide.s32 	%rd2127, %r793, %r749;
	add.s64 	%rd2128, %rd2126, %rd2127;
	ld.global.f64 	%fd5707, [%rd2128];
	add.f64 	%fd11116, %fd5707, 0d0000000000000000;
	ld.global.f64 	%fd5708, [%rd2128+8];
	add.f64 	%fd11117, %fd5708, 0d0000000000000000;
	ld.global.f64 	%fd5709, [%rd2128+16];
	add.f64 	%fd11118, %fd5709, 0d0000000000000000;
	ld.global.f64 	%fd5710, [%rd2128+24];
	add.f64 	%fd11119, %fd5710, 0d0000000000000000;
	ld.global.f64 	%fd5711, [%rd2128+32];
	add.f64 	%fd11120, %fd5711, 0d0000000000000000;
	ld.global.f64 	%fd5712, [%rd2128+40];
	add.f64 	%fd11121, %fd5712, 0d0000000000000000;
	ld.global.f64 	%fd5713, [%rd2128+48];
	add.f64 	%fd11122, %fd5713, 0d0000000000000000;
	ld.global.f64 	%fd5714, [%rd2128+56];
	add.f64 	%fd11123, %fd5714, 0d0000000000000000;
	ld.global.f64 	%fd5715, [%rd2128+64];
	add.f64 	%fd11124, %fd5715, 0d0000000000000000;

$L__BB1_385:
	add.f64 	%fd1684, %fd11124, 0d0000000000000000;
	add.f64 	%fd1685, %fd11123, 0d0000000000000000;
	add.f64 	%fd1686, %fd11122, 0d0000000000000000;
	add.f64 	%fd1687, %fd11121, 0d0000000000000000;
	add.f64 	%fd1688, %fd11120, 0d0000000000000000;
	add.f64 	%fd1689, %fd11119, 0d0000000000000000;
	add.f64 	%fd1690, %fd11118, 0d0000000000000000;
	add.f64 	%fd1691, %fd11117, 0d0000000000000000;
	add.f64 	%fd1692, %fd11116, 0d0000000000000000;
	ld.param.u64 	%rd277, [%rd27+120];
	ld.param.u32 	%r798, [%rd27+144];
	ld.param.u32 	%r799, [%rd27+172];
	ld.param.v2.u32 	{%r1523, %r1524}, [%rd27+176];
	setp.le.s32 	%p683, %r1523, %r711;
	setp.le.s32 	%p684, %r1524, %r664;
	setp.le.s32 	%p685, %r799, %r739;
	or.pred  	%p686, %p683, %p684;
	or.pred  	%p688, %p620, %p686;
	or.pred  	%p689, %p685, %p688;
	mov.f64 	%fd11125, 0d0000000000000000;
	mov.f64 	%fd11126, 0d0000000000000000;
	mov.f64 	%fd11127, 0d0000000000000000;
	mov.f64 	%fd11128, 0d0000000000000000;
	mov.f64 	%fd11129, 0d0000000000000000;
	mov.f64 	%fd11130, 0d0000000000000000;
	mov.f64 	%fd11131, 0d0000000000000000;
	mov.f64 	%fd11132, 0d0000000000000000;
	mov.f64 	%fd11133, 0d0000000000000000;
	@%p689 bra 	$L__BB1_388;
	bra.uni 	$L__BB1_386;

$L__BB1_388:
	add.s32 	%r1941, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1941, %r664};
	add.s32 	%r1942, %r635, 11;
	st.local.v2.u32 	[%rd26+8], {%r1942, %r1523};
	st.local.v2.u32 	[%rd26+16], {%r1524, %r799};
	mov.u64 	%rd2135, $str$1;
	cvta.global.u64 	%rd2136, %rd2135;
	{ // callseq 432, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2136;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1525, [retval0+0];
	} // callseq 432
	bra.uni 	$L__BB1_389;

$L__BB1_386:
	setp.eq.s64 	%p690, %rd277, 0;
	@%p690 bra 	$L__BB1_389;

	cvta.to.global.u64 	%rd2132, %rd277;
	mul.wide.s32 	%rd2133, %r798, %r739;
	add.s64 	%rd2134, %rd2132, %rd2133;
	ld.global.f64 	%fd5734, [%rd2134];
	add.f64 	%fd11125, %fd5734, 0d0000000000000000;
	ld.global.f64 	%fd5735, [%rd2134+8];
	add.f64 	%fd11126, %fd5735, 0d0000000000000000;
	ld.global.f64 	%fd5736, [%rd2134+16];
	add.f64 	%fd11127, %fd5736, 0d0000000000000000;
	ld.global.f64 	%fd5737, [%rd2134+24];
	add.f64 	%fd11128, %fd5737, 0d0000000000000000;
	ld.global.f64 	%fd5738, [%rd2134+32];
	add.f64 	%fd11129, %fd5738, 0d0000000000000000;
	ld.global.f64 	%fd5739, [%rd2134+40];
	add.f64 	%fd11130, %fd5739, 0d0000000000000000;
	ld.global.f64 	%fd5740, [%rd2134+48];
	add.f64 	%fd11131, %fd5740, 0d0000000000000000;
	ld.global.f64 	%fd5741, [%rd2134+56];
	add.f64 	%fd11132, %fd5741, 0d0000000000000000;
	ld.global.f64 	%fd5742, [%rd2134+64];
	add.f64 	%fd11133, %fd5742, 0d0000000000000000;

$L__BB1_389:
	add.f64 	%fd1711, %fd11133, 0d0000000000000000;
	add.f64 	%fd1712, %fd11132, 0d0000000000000000;
	add.f64 	%fd1713, %fd11131, 0d0000000000000000;
	add.f64 	%fd1714, %fd11130, 0d0000000000000000;
	add.f64 	%fd1715, %fd11129, 0d0000000000000000;
	add.f64 	%fd1716, %fd11128, 0d0000000000000000;
	add.f64 	%fd1717, %fd11127, 0d0000000000000000;
	add.f64 	%fd1718, %fd11126, 0d0000000000000000;
	add.f64 	%fd1719, %fd11125, 0d0000000000000000;
	ld.param.u64 	%rd278, [%rd27+120];
	ld.param.u32 	%r803, [%rd27+144];
	ld.param.u32 	%r804, [%rd27+172];
	ld.param.v2.u32 	{%r1526, %r1527}, [%rd27+176];
	setp.le.s32 	%p691, %r1526, %r711;
	setp.le.s32 	%p692, %r1527, %r654;
	setp.le.s32 	%p693, %r804, %r730;
	or.pred  	%p694, %p691, %p692;
	or.pred  	%p696, %p613, %p694;
	or.pred  	%p697, %p693, %p696;
	mov.f64 	%fd11134, 0d0000000000000000;
	mov.f64 	%fd11135, 0d0000000000000000;
	mov.f64 	%fd11136, 0d0000000000000000;
	mov.f64 	%fd11137, 0d0000000000000000;
	mov.f64 	%fd11138, 0d0000000000000000;
	mov.f64 	%fd11139, 0d0000000000000000;
	mov.f64 	%fd11140, 0d0000000000000000;
	mov.f64 	%fd11141, 0d0000000000000000;
	mov.f64 	%fd11142, 0d0000000000000000;
	@%p697 bra 	$L__BB1_392;
	bra.uni 	$L__BB1_390;

$L__BB1_392:
	add.s32 	%r1943, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1943, %r654};
	add.s32 	%r1944, %r635, 10;
	st.local.v2.u32 	[%rd26+8], {%r1944, %r1526};
	st.local.v2.u32 	[%rd26+16], {%r1527, %r804};
	mov.u64 	%rd2141, $str$1;
	cvta.global.u64 	%rd2142, %rd2141;
	{ // callseq 433, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2142;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1528, [retval0+0];
	} // callseq 433
	bra.uni 	$L__BB1_393;

$L__BB1_390:
	setp.eq.s64 	%p698, %rd278, 0;
	@%p698 bra 	$L__BB1_393;

	cvta.to.global.u64 	%rd2138, %rd278;
	mul.wide.s32 	%rd2139, %r803, %r730;
	add.s64 	%rd2140, %rd2138, %rd2139;
	ld.global.f64 	%fd5761, [%rd2140];
	add.f64 	%fd11134, %fd5761, 0d0000000000000000;
	ld.global.f64 	%fd5762, [%rd2140+8];
	add.f64 	%fd11135, %fd5762, 0d0000000000000000;
	ld.global.f64 	%fd5763, [%rd2140+16];
	add.f64 	%fd11136, %fd5763, 0d0000000000000000;
	ld.global.f64 	%fd5764, [%rd2140+24];
	add.f64 	%fd11137, %fd5764, 0d0000000000000000;
	ld.global.f64 	%fd5765, [%rd2140+32];
	add.f64 	%fd11138, %fd5765, 0d0000000000000000;
	ld.global.f64 	%fd5766, [%rd2140+40];
	add.f64 	%fd11139, %fd5766, 0d0000000000000000;
	ld.global.f64 	%fd5767, [%rd2140+48];
	add.f64 	%fd11140, %fd5767, 0d0000000000000000;
	ld.global.f64 	%fd5768, [%rd2140+56];
	add.f64 	%fd11141, %fd5768, 0d0000000000000000;
	ld.global.f64 	%fd5769, [%rd2140+64];
	add.f64 	%fd11142, %fd5769, 0d0000000000000000;

$L__BB1_393:
	add.f64 	%fd1738, %fd11142, 0d0000000000000000;
	add.f64 	%fd1739, %fd11141, 0d0000000000000000;
	add.f64 	%fd1740, %fd11140, 0d0000000000000000;
	add.f64 	%fd1741, %fd11139, 0d0000000000000000;
	add.f64 	%fd1742, %fd11138, 0d0000000000000000;
	add.f64 	%fd1743, %fd11137, 0d0000000000000000;
	add.f64 	%fd1744, %fd11136, 0d0000000000000000;
	add.f64 	%fd1745, %fd11135, 0d0000000000000000;
	add.f64 	%fd1746, %fd11134, 0d0000000000000000;
	ld.param.u64 	%rd279, [%rd27+120];
	ld.param.u32 	%r808, [%rd27+144];
	ld.param.u32 	%r809, [%rd27+172];
	ld.param.v2.u32 	{%r1529, %r1530}, [%rd27+176];
	setp.le.s32 	%p699, %r1529, %r711;
	setp.le.s32 	%p700, %r1530, %r644;
	setp.le.s32 	%p701, %r809, %r721;
	or.pred  	%p702, %p699, %p700;
	or.pred  	%p704, %p606, %p702;
	or.pred  	%p705, %p701, %p704;
	mov.f64 	%fd11143, 0d0000000000000000;
	mov.f64 	%fd11144, 0d0000000000000000;
	mov.f64 	%fd11145, 0d0000000000000000;
	mov.f64 	%fd11146, 0d0000000000000000;
	mov.f64 	%fd11147, 0d0000000000000000;
	mov.f64 	%fd11148, 0d0000000000000000;
	mov.f64 	%fd11149, 0d0000000000000000;
	mov.f64 	%fd11150, 0d0000000000000000;
	mov.f64 	%fd11151, 0d0000000000000000;
	@%p705 bra 	$L__BB1_396;
	bra.uni 	$L__BB1_394;

$L__BB1_396:
	add.s32 	%r1945, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1945, %r644};
	add.s32 	%r1946, %r635, 9;
	st.local.v2.u32 	[%rd26+8], {%r1946, %r1529};
	st.local.v2.u32 	[%rd26+16], {%r1530, %r809};
	mov.u64 	%rd2147, $str$1;
	cvta.global.u64 	%rd2148, %rd2147;
	{ // callseq 434, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2148;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1531, [retval0+0];
	} // callseq 434
	bra.uni 	$L__BB1_397;

$L__BB1_394:
	setp.eq.s64 	%p706, %rd279, 0;
	@%p706 bra 	$L__BB1_397;

	cvta.to.global.u64 	%rd2144, %rd279;
	mul.wide.s32 	%rd2145, %r808, %r721;
	add.s64 	%rd2146, %rd2144, %rd2145;
	ld.global.f64 	%fd5788, [%rd2146];
	add.f64 	%fd11143, %fd5788, 0d0000000000000000;
	ld.global.f64 	%fd5789, [%rd2146+8];
	add.f64 	%fd11144, %fd5789, 0d0000000000000000;
	ld.global.f64 	%fd5790, [%rd2146+16];
	add.f64 	%fd11145, %fd5790, 0d0000000000000000;
	ld.global.f64 	%fd5791, [%rd2146+24];
	add.f64 	%fd11146, %fd5791, 0d0000000000000000;
	ld.global.f64 	%fd5792, [%rd2146+32];
	add.f64 	%fd11147, %fd5792, 0d0000000000000000;
	ld.global.f64 	%fd5793, [%rd2146+40];
	add.f64 	%fd11148, %fd5793, 0d0000000000000000;
	ld.global.f64 	%fd5794, [%rd2146+48];
	add.f64 	%fd11149, %fd5794, 0d0000000000000000;
	ld.global.f64 	%fd5795, [%rd2146+56];
	add.f64 	%fd11150, %fd5795, 0d0000000000000000;
	ld.global.f64 	%fd5796, [%rd2146+64];
	add.f64 	%fd11151, %fd5796, 0d0000000000000000;

$L__BB1_397:
	add.f64 	%fd1765, %fd11151, 0d0000000000000000;
	add.f64 	%fd1766, %fd11150, 0d0000000000000000;
	add.f64 	%fd1767, %fd11149, 0d0000000000000000;
	add.f64 	%fd1768, %fd11148, 0d0000000000000000;
	add.f64 	%fd1769, %fd11147, 0d0000000000000000;
	add.f64 	%fd1770, %fd11146, 0d0000000000000000;
	add.f64 	%fd1771, %fd11145, 0d0000000000000000;
	add.f64 	%fd1772, %fd11144, 0d0000000000000000;
	add.f64 	%fd1773, %fd11143, 0d0000000000000000;
	ld.param.u64 	%rd280, [%rd27+120];
	ld.param.u32 	%r813, [%rd27+144];
	ld.param.u32 	%r814, [%rd27+172];
	ld.param.v2.u32 	{%r1532, %r1533}, [%rd27+176];
	setp.le.s32 	%p707, %r1532, %r711;
	setp.le.s32 	%p708, %r1533, %r634;
	setp.le.s32 	%p709, %r814, %r712;
	or.pred  	%p710, %p707, %p708;
	or.pred  	%p712, %p599, %p710;
	or.pred  	%p713, %p709, %p712;
	mov.f64 	%fd11152, 0d0000000000000000;
	mov.f64 	%fd11153, 0d0000000000000000;
	mov.f64 	%fd11154, 0d0000000000000000;
	mov.f64 	%fd11155, 0d0000000000000000;
	mov.f64 	%fd11156, 0d0000000000000000;
	mov.f64 	%fd11157, 0d0000000000000000;
	mov.f64 	%fd11158, 0d0000000000000000;
	mov.f64 	%fd11159, 0d0000000000000000;
	mov.f64 	%fd11160, 0d0000000000000000;
	@%p713 bra 	$L__BB1_400;
	bra.uni 	$L__BB1_398;

$L__BB1_400:
	add.s32 	%r1947, %r633, 2;
	st.local.v2.u32 	[%rd26], {%r1947, %r634};
	add.s32 	%r1948, %r635, 8;
	st.local.v2.u32 	[%rd26+8], {%r1948, %r1532};
	st.local.v2.u32 	[%rd26+16], {%r1533, %r814};
	mov.u64 	%rd2153, $str$1;
	cvta.global.u64 	%rd2154, %rd2153;
	{ // callseq 435, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2154;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1534, [retval0+0];
	} // callseq 435
	bra.uni 	$L__BB1_401;

$L__BB1_398:
	setp.eq.s64 	%p714, %rd280, 0;
	@%p714 bra 	$L__BB1_401;

	cvta.to.global.u64 	%rd2150, %rd280;
	mul.wide.s32 	%rd2151, %r813, %r712;
	add.s64 	%rd2152, %rd2150, %rd2151;
	ld.global.f64 	%fd5815, [%rd2152];
	add.f64 	%fd11152, %fd5815, 0d0000000000000000;
	ld.global.f64 	%fd5816, [%rd2152+8];
	add.f64 	%fd11153, %fd5816, 0d0000000000000000;
	ld.global.f64 	%fd5817, [%rd2152+16];
	add.f64 	%fd11154, %fd5817, 0d0000000000000000;
	ld.global.f64 	%fd5818, [%rd2152+24];
	add.f64 	%fd11155, %fd5818, 0d0000000000000000;
	ld.global.f64 	%fd5819, [%rd2152+32];
	add.f64 	%fd11156, %fd5819, 0d0000000000000000;
	ld.global.f64 	%fd5820, [%rd2152+40];
	add.f64 	%fd11157, %fd5820, 0d0000000000000000;
	ld.global.f64 	%fd5821, [%rd2152+48];
	add.f64 	%fd11158, %fd5821, 0d0000000000000000;
	ld.global.f64 	%fd5822, [%rd2152+56];
	add.f64 	%fd11159, %fd5822, 0d0000000000000000;
	ld.global.f64 	%fd5823, [%rd2152+64];
	add.f64 	%fd11160, %fd5823, 0d0000000000000000;

$L__BB1_401:
	add.f64 	%fd1792, %fd11160, 0d0000000000000000;
	add.f64 	%fd1793, %fd11159, 0d0000000000000000;
	add.f64 	%fd1794, %fd11158, 0d0000000000000000;
	add.f64 	%fd1795, %fd11157, 0d0000000000000000;
	add.f64 	%fd1796, %fd11156, 0d0000000000000000;
	add.f64 	%fd1797, %fd11155, 0d0000000000000000;
	add.f64 	%fd1798, %fd11154, 0d0000000000000000;
	add.f64 	%fd1799, %fd11153, 0d0000000000000000;
	add.f64 	%fd1800, %fd11152, 0d0000000000000000;
	ld.param.u64 	%rd281, [%rd27+120];
	ld.param.u32 	%r818, [%rd27+144];
	ld.param.u32 	%r819, [%rd27+172];
	ld.param.v2.u32 	{%r1535, %r1536}, [%rd27+176];
	setp.le.s32 	%p715, %r1535, %r674;
	setp.le.s32 	%p716, %r1536, %r664;
	setp.le.s32 	%p717, %r819, %r702;
	or.pred  	%p718, %p715, %p716;
	or.pred  	%p720, %p592, %p718;
	or.pred  	%p721, %p717, %p720;
	mov.f64 	%fd11161, 0d0000000000000000;
	mov.f64 	%fd11162, 0d0000000000000000;
	mov.f64 	%fd11163, 0d0000000000000000;
	mov.f64 	%fd11164, 0d0000000000000000;
	mov.f64 	%fd11165, 0d0000000000000000;
	mov.f64 	%fd11166, 0d0000000000000000;
	mov.f64 	%fd11167, 0d0000000000000000;
	mov.f64 	%fd11168, 0d0000000000000000;
	mov.f64 	%fd11169, 0d0000000000000000;
	@%p721 bra 	$L__BB1_404;
	bra.uni 	$L__BB1_402;

$L__BB1_404:
	add.s32 	%r1949, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1949, %r664};
	add.s32 	%r1950, %r635, 7;
	st.local.v2.u32 	[%rd26+8], {%r1950, %r1535};
	st.local.v2.u32 	[%rd26+16], {%r1536, %r819};
	mov.u64 	%rd2159, $str$1;
	cvta.global.u64 	%rd2160, %rd2159;
	{ // callseq 436, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2160;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1537, [retval0+0];
	} // callseq 436
	bra.uni 	$L__BB1_405;

$L__BB1_402:
	setp.eq.s64 	%p722, %rd281, 0;
	@%p722 bra 	$L__BB1_405;

	cvta.to.global.u64 	%rd2156, %rd281;
	mul.wide.s32 	%rd2157, %r818, %r702;
	add.s64 	%rd2158, %rd2156, %rd2157;
	ld.global.f64 	%fd5842, [%rd2158];
	add.f64 	%fd11161, %fd5842, 0d0000000000000000;
	ld.global.f64 	%fd5843, [%rd2158+8];
	add.f64 	%fd11162, %fd5843, 0d0000000000000000;
	ld.global.f64 	%fd5844, [%rd2158+16];
	add.f64 	%fd11163, %fd5844, 0d0000000000000000;
	ld.global.f64 	%fd5845, [%rd2158+24];
	add.f64 	%fd11164, %fd5845, 0d0000000000000000;
	ld.global.f64 	%fd5846, [%rd2158+32];
	add.f64 	%fd11165, %fd5846, 0d0000000000000000;
	ld.global.f64 	%fd5847, [%rd2158+40];
	add.f64 	%fd11166, %fd5847, 0d0000000000000000;
	ld.global.f64 	%fd5848, [%rd2158+48];
	add.f64 	%fd11167, %fd5848, 0d0000000000000000;
	ld.global.f64 	%fd5849, [%rd2158+56];
	add.f64 	%fd11168, %fd5849, 0d0000000000000000;
	ld.global.f64 	%fd5850, [%rd2158+64];
	add.f64 	%fd11169, %fd5850, 0d0000000000000000;

$L__BB1_405:
	add.f64 	%fd1819, %fd11169, 0d0000000000000000;
	add.f64 	%fd1820, %fd11168, 0d0000000000000000;
	add.f64 	%fd1821, %fd11167, 0d0000000000000000;
	add.f64 	%fd1822, %fd11166, 0d0000000000000000;
	add.f64 	%fd1823, %fd11165, 0d0000000000000000;
	add.f64 	%fd1824, %fd11164, 0d0000000000000000;
	add.f64 	%fd1825, %fd11163, 0d0000000000000000;
	add.f64 	%fd1826, %fd11162, 0d0000000000000000;
	add.f64 	%fd1827, %fd11161, 0d0000000000000000;
	ld.param.u64 	%rd282, [%rd27+120];
	ld.param.u32 	%r823, [%rd27+144];
	ld.param.u32 	%r824, [%rd27+172];
	ld.param.v2.u32 	{%r1538, %r1539}, [%rd27+176];
	setp.le.s32 	%p723, %r1538, %r674;
	setp.le.s32 	%p724, %r1539, %r654;
	setp.le.s32 	%p725, %r824, %r693;
	or.pred  	%p726, %p723, %p724;
	or.pred  	%p728, %p585, %p726;
	or.pred  	%p729, %p725, %p728;
	mov.f64 	%fd11170, 0d0000000000000000;
	mov.f64 	%fd11171, 0d0000000000000000;
	mov.f64 	%fd11172, 0d0000000000000000;
	mov.f64 	%fd11173, 0d0000000000000000;
	mov.f64 	%fd11174, 0d0000000000000000;
	mov.f64 	%fd11175, 0d0000000000000000;
	mov.f64 	%fd11176, 0d0000000000000000;
	mov.f64 	%fd11177, 0d0000000000000000;
	mov.f64 	%fd11178, 0d0000000000000000;
	@%p729 bra 	$L__BB1_408;
	bra.uni 	$L__BB1_406;

$L__BB1_408:
	add.s32 	%r1951, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1951, %r654};
	add.s32 	%r1952, %r635, 6;
	st.local.v2.u32 	[%rd26+8], {%r1952, %r1538};
	st.local.v2.u32 	[%rd26+16], {%r1539, %r824};
	mov.u64 	%rd2165, $str$1;
	cvta.global.u64 	%rd2166, %rd2165;
	{ // callseq 437, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2166;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1540, [retval0+0];
	} // callseq 437
	bra.uni 	$L__BB1_409;

$L__BB1_406:
	setp.eq.s64 	%p730, %rd282, 0;
	@%p730 bra 	$L__BB1_409;

	cvta.to.global.u64 	%rd2162, %rd282;
	mul.wide.s32 	%rd2163, %r823, %r693;
	add.s64 	%rd2164, %rd2162, %rd2163;
	ld.global.f64 	%fd5869, [%rd2164];
	add.f64 	%fd11170, %fd5869, 0d0000000000000000;
	ld.global.f64 	%fd5870, [%rd2164+8];
	add.f64 	%fd11171, %fd5870, 0d0000000000000000;
	ld.global.f64 	%fd5871, [%rd2164+16];
	add.f64 	%fd11172, %fd5871, 0d0000000000000000;
	ld.global.f64 	%fd5872, [%rd2164+24];
	add.f64 	%fd11173, %fd5872, 0d0000000000000000;
	ld.global.f64 	%fd5873, [%rd2164+32];
	add.f64 	%fd11174, %fd5873, 0d0000000000000000;
	ld.global.f64 	%fd5874, [%rd2164+40];
	add.f64 	%fd11175, %fd5874, 0d0000000000000000;
	ld.global.f64 	%fd5875, [%rd2164+48];
	add.f64 	%fd11176, %fd5875, 0d0000000000000000;
	ld.global.f64 	%fd5876, [%rd2164+56];
	add.f64 	%fd11177, %fd5876, 0d0000000000000000;
	ld.global.f64 	%fd5877, [%rd2164+64];
	add.f64 	%fd11178, %fd5877, 0d0000000000000000;

$L__BB1_409:
	add.f64 	%fd1846, %fd11178, 0d0000000000000000;
	add.f64 	%fd1847, %fd11177, 0d0000000000000000;
	add.f64 	%fd1848, %fd11176, 0d0000000000000000;
	add.f64 	%fd1849, %fd11175, 0d0000000000000000;
	add.f64 	%fd1850, %fd11174, 0d0000000000000000;
	add.f64 	%fd1851, %fd11173, 0d0000000000000000;
	add.f64 	%fd1852, %fd11172, 0d0000000000000000;
	add.f64 	%fd1853, %fd11171, 0d0000000000000000;
	add.f64 	%fd1854, %fd11170, 0d0000000000000000;
	ld.param.u64 	%rd283, [%rd27+120];
	ld.param.u32 	%r828, [%rd27+144];
	ld.param.u32 	%r829, [%rd27+172];
	ld.param.v2.u32 	{%r1541, %r1542}, [%rd27+176];
	setp.le.s32 	%p731, %r1541, %r674;
	setp.le.s32 	%p732, %r1542, %r644;
	setp.le.s32 	%p733, %r829, %r684;
	or.pred  	%p734, %p731, %p732;
	or.pred  	%p736, %p578, %p734;
	or.pred  	%p737, %p733, %p736;
	mov.f64 	%fd11179, 0d0000000000000000;
	mov.f64 	%fd11180, 0d0000000000000000;
	mov.f64 	%fd11181, 0d0000000000000000;
	mov.f64 	%fd11182, 0d0000000000000000;
	mov.f64 	%fd11183, 0d0000000000000000;
	mov.f64 	%fd11184, 0d0000000000000000;
	mov.f64 	%fd11185, 0d0000000000000000;
	mov.f64 	%fd11186, 0d0000000000000000;
	mov.f64 	%fd11187, 0d0000000000000000;
	@%p737 bra 	$L__BB1_412;
	bra.uni 	$L__BB1_410;

$L__BB1_412:
	add.s32 	%r1953, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1953, %r644};
	add.s32 	%r1954, %r635, 5;
	st.local.v2.u32 	[%rd26+8], {%r1954, %r1541};
	st.local.v2.u32 	[%rd26+16], {%r1542, %r829};
	mov.u64 	%rd2171, $str$1;
	cvta.global.u64 	%rd2172, %rd2171;
	{ // callseq 438, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2172;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1543, [retval0+0];
	} // callseq 438
	bra.uni 	$L__BB1_413;

$L__BB1_410:
	setp.eq.s64 	%p738, %rd283, 0;
	@%p738 bra 	$L__BB1_413;

	cvta.to.global.u64 	%rd2168, %rd283;
	mul.wide.s32 	%rd2169, %r828, %r684;
	add.s64 	%rd2170, %rd2168, %rd2169;
	ld.global.f64 	%fd5896, [%rd2170];
	add.f64 	%fd11179, %fd5896, 0d0000000000000000;
	ld.global.f64 	%fd5897, [%rd2170+8];
	add.f64 	%fd11180, %fd5897, 0d0000000000000000;
	ld.global.f64 	%fd5898, [%rd2170+16];
	add.f64 	%fd11181, %fd5898, 0d0000000000000000;
	ld.global.f64 	%fd5899, [%rd2170+24];
	add.f64 	%fd11182, %fd5899, 0d0000000000000000;
	ld.global.f64 	%fd5900, [%rd2170+32];
	add.f64 	%fd11183, %fd5900, 0d0000000000000000;
	ld.global.f64 	%fd5901, [%rd2170+40];
	add.f64 	%fd11184, %fd5901, 0d0000000000000000;
	ld.global.f64 	%fd5902, [%rd2170+48];
	add.f64 	%fd11185, %fd5902, 0d0000000000000000;
	ld.global.f64 	%fd5903, [%rd2170+56];
	add.f64 	%fd11186, %fd5903, 0d0000000000000000;
	ld.global.f64 	%fd5904, [%rd2170+64];
	add.f64 	%fd11187, %fd5904, 0d0000000000000000;

$L__BB1_413:
	add.f64 	%fd1873, %fd11187, 0d0000000000000000;
	add.f64 	%fd1874, %fd11186, 0d0000000000000000;
	add.f64 	%fd1875, %fd11185, 0d0000000000000000;
	add.f64 	%fd1876, %fd11184, 0d0000000000000000;
	add.f64 	%fd1877, %fd11183, 0d0000000000000000;
	add.f64 	%fd1878, %fd11182, 0d0000000000000000;
	add.f64 	%fd1879, %fd11181, 0d0000000000000000;
	add.f64 	%fd1880, %fd11180, 0d0000000000000000;
	add.f64 	%fd1881, %fd11179, 0d0000000000000000;
	ld.param.u64 	%rd284, [%rd27+120];
	ld.param.u32 	%r833, [%rd27+144];
	ld.param.u32 	%r834, [%rd27+172];
	ld.param.v2.u32 	{%r1544, %r1545}, [%rd27+176];
	setp.le.s32 	%p739, %r1544, %r674;
	setp.le.s32 	%p740, %r1545, %r634;
	setp.le.s32 	%p741, %r834, %r675;
	or.pred  	%p742, %p739, %p740;
	or.pred  	%p744, %p571, %p742;
	or.pred  	%p745, %p741, %p744;
	mov.f64 	%fd11188, 0d0000000000000000;
	mov.f64 	%fd11189, 0d0000000000000000;
	mov.f64 	%fd11190, 0d0000000000000000;
	mov.f64 	%fd11191, 0d0000000000000000;
	mov.f64 	%fd11192, 0d0000000000000000;
	mov.f64 	%fd11193, 0d0000000000000000;
	mov.f64 	%fd11194, 0d0000000000000000;
	mov.f64 	%fd11195, 0d0000000000000000;
	mov.f64 	%fd11196, 0d0000000000000000;
	@%p745 bra 	$L__BB1_416;
	bra.uni 	$L__BB1_414;

$L__BB1_416:
	add.s32 	%r1955, %r633, 1;
	st.local.v2.u32 	[%rd26], {%r1955, %r634};
	add.s32 	%r1956, %r635, 4;
	st.local.v2.u32 	[%rd26+8], {%r1956, %r1544};
	st.local.v2.u32 	[%rd26+16], {%r1545, %r834};
	mov.u64 	%rd2177, $str$1;
	cvta.global.u64 	%rd2178, %rd2177;
	{ // callseq 439, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2178;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1546, [retval0+0];
	} // callseq 439
	bra.uni 	$L__BB1_417;

$L__BB1_414:
	setp.eq.s64 	%p746, %rd284, 0;
	@%p746 bra 	$L__BB1_417;

	cvta.to.global.u64 	%rd2174, %rd284;
	mul.wide.s32 	%rd2175, %r833, %r675;
	add.s64 	%rd2176, %rd2174, %rd2175;
	ld.global.f64 	%fd5923, [%rd2176];
	add.f64 	%fd11188, %fd5923, 0d0000000000000000;
	ld.global.f64 	%fd5924, [%rd2176+8];
	add.f64 	%fd11189, %fd5924, 0d0000000000000000;
	ld.global.f64 	%fd5925, [%rd2176+16];
	add.f64 	%fd11190, %fd5925, 0d0000000000000000;
	ld.global.f64 	%fd5926, [%rd2176+24];
	add.f64 	%fd11191, %fd5926, 0d0000000000000000;
	ld.global.f64 	%fd5927, [%rd2176+32];
	add.f64 	%fd11192, %fd5927, 0d0000000000000000;
	ld.global.f64 	%fd5928, [%rd2176+40];
	add.f64 	%fd11193, %fd5928, 0d0000000000000000;
	ld.global.f64 	%fd5929, [%rd2176+48];
	add.f64 	%fd11194, %fd5929, 0d0000000000000000;
	ld.global.f64 	%fd5930, [%rd2176+56];
	add.f64 	%fd11195, %fd5930, 0d0000000000000000;
	ld.global.f64 	%fd5931, [%rd2176+64];
	add.f64 	%fd11196, %fd5931, 0d0000000000000000;

$L__BB1_417:
	add.f64 	%fd1900, %fd11196, 0d0000000000000000;
	add.f64 	%fd1901, %fd11195, 0d0000000000000000;
	add.f64 	%fd1902, %fd11194, 0d0000000000000000;
	add.f64 	%fd1903, %fd11193, 0d0000000000000000;
	add.f64 	%fd1904, %fd11192, 0d0000000000000000;
	add.f64 	%fd1905, %fd11191, 0d0000000000000000;
	add.f64 	%fd1906, %fd11190, 0d0000000000000000;
	add.f64 	%fd1907, %fd11189, 0d0000000000000000;
	add.f64 	%fd1908, %fd11188, 0d0000000000000000;
	ld.param.u64 	%rd285, [%rd27+120];
	ld.param.u32 	%r838, [%rd27+144];
	ld.param.u32 	%r839, [%rd27+172];
	ld.param.v2.u32 	{%r1547, %r1548}, [%rd27+176];
	setp.le.s32 	%p747, %r1547, %r633;
	setp.le.s32 	%p748, %r1548, %r664;
	setp.le.s32 	%p749, %r839, %r665;
	or.pred  	%p750, %p747, %p748;
	or.pred  	%p752, %p564, %p750;
	or.pred  	%p753, %p749, %p752;
	mov.f64 	%fd11197, 0d0000000000000000;
	mov.f64 	%fd11198, 0d0000000000000000;
	mov.f64 	%fd11199, 0d0000000000000000;
	mov.f64 	%fd11200, 0d0000000000000000;
	mov.f64 	%fd11201, 0d0000000000000000;
	mov.f64 	%fd11202, 0d0000000000000000;
	mov.f64 	%fd11203, 0d0000000000000000;
	mov.f64 	%fd11204, 0d0000000000000000;
	mov.f64 	%fd11205, 0d0000000000000000;
	@%p753 bra 	$L__BB1_420;
	bra.uni 	$L__BB1_418;

$L__BB1_420:
	add.s32 	%r1957, %r634, 3;
	st.local.v2.u32 	[%rd26], {%r633, %r1957};
	add.s32 	%r1958, %r635, 3;
	st.local.v2.u32 	[%rd26+8], {%r1958, %r1547};
	st.local.v2.u32 	[%rd26+16], {%r1548, %r839};
	mov.u64 	%rd2183, $str$1;
	cvta.global.u64 	%rd2184, %rd2183;
	{ // callseq 440, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2184;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1549, [retval0+0];
	} // callseq 440
	bra.uni 	$L__BB1_421;

$L__BB1_418:
	setp.eq.s64 	%p754, %rd285, 0;
	@%p754 bra 	$L__BB1_421;

	cvta.to.global.u64 	%rd2180, %rd285;
	mul.wide.s32 	%rd2181, %r838, %r665;
	add.s64 	%rd2182, %rd2180, %rd2181;
	ld.global.f64 	%fd5950, [%rd2182];
	add.f64 	%fd11197, %fd5950, 0d0000000000000000;
	ld.global.f64 	%fd5951, [%rd2182+8];
	add.f64 	%fd11198, %fd5951, 0d0000000000000000;
	ld.global.f64 	%fd5952, [%rd2182+16];
	add.f64 	%fd11199, %fd5952, 0d0000000000000000;
	ld.global.f64 	%fd5953, [%rd2182+24];
	add.f64 	%fd11200, %fd5953, 0d0000000000000000;
	ld.global.f64 	%fd5954, [%rd2182+32];
	add.f64 	%fd11201, %fd5954, 0d0000000000000000;
	ld.global.f64 	%fd5955, [%rd2182+40];
	add.f64 	%fd11202, %fd5955, 0d0000000000000000;
	ld.global.f64 	%fd5956, [%rd2182+48];
	add.f64 	%fd11203, %fd5956, 0d0000000000000000;
	ld.global.f64 	%fd5957, [%rd2182+56];
	add.f64 	%fd11204, %fd5957, 0d0000000000000000;
	ld.global.f64 	%fd5958, [%rd2182+64];
	add.f64 	%fd11205, %fd5958, 0d0000000000000000;

$L__BB1_421:
	add.f64 	%fd1927, %fd11205, 0d0000000000000000;
	add.f64 	%fd1928, %fd11204, 0d0000000000000000;
	add.f64 	%fd1929, %fd11203, 0d0000000000000000;
	add.f64 	%fd1930, %fd11202, 0d0000000000000000;
	add.f64 	%fd1931, %fd11201, 0d0000000000000000;
	add.f64 	%fd1932, %fd11200, 0d0000000000000000;
	add.f64 	%fd1933, %fd11199, 0d0000000000000000;
	add.f64 	%fd1934, %fd11198, 0d0000000000000000;
	add.f64 	%fd1935, %fd11197, 0d0000000000000000;
	ld.param.u64 	%rd286, [%rd27+120];
	ld.param.u32 	%r843, [%rd27+144];
	ld.param.u32 	%r844, [%rd27+172];
	ld.param.v2.u32 	{%r1550, %r1551}, [%rd27+176];
	setp.le.s32 	%p755, %r1550, %r633;
	setp.le.s32 	%p756, %r1551, %r654;
	setp.le.s32 	%p757, %r844, %r655;
	or.pred  	%p758, %p755, %p756;
	or.pred  	%p760, %p557, %p758;
	or.pred  	%p761, %p757, %p760;
	mov.f64 	%fd11206, 0d0000000000000000;
	mov.f64 	%fd11207, 0d0000000000000000;
	mov.f64 	%fd11208, 0d0000000000000000;
	mov.f64 	%fd11209, 0d0000000000000000;
	mov.f64 	%fd11210, 0d0000000000000000;
	mov.f64 	%fd11211, 0d0000000000000000;
	mov.f64 	%fd11212, 0d0000000000000000;
	mov.f64 	%fd11213, 0d0000000000000000;
	mov.f64 	%fd11214, 0d0000000000000000;
	@%p761 bra 	$L__BB1_424;
	bra.uni 	$L__BB1_422;

$L__BB1_424:
	add.s32 	%r1959, %r634, 2;
	st.local.v2.u32 	[%rd26], {%r633, %r1959};
	add.s32 	%r1960, %r635, 2;
	st.local.v2.u32 	[%rd26+8], {%r1960, %r1550};
	st.local.v2.u32 	[%rd26+16], {%r1551, %r844};
	mov.u64 	%rd2189, $str$1;
	cvta.global.u64 	%rd2190, %rd2189;
	{ // callseq 441, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2190;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1552, [retval0+0];
	} // callseq 441
	bra.uni 	$L__BB1_425;

$L__BB1_422:
	setp.eq.s64 	%p762, %rd286, 0;
	@%p762 bra 	$L__BB1_425;

	cvta.to.global.u64 	%rd2186, %rd286;
	mul.wide.s32 	%rd2187, %r843, %r655;
	add.s64 	%rd2188, %rd2186, %rd2187;
	ld.global.f64 	%fd5977, [%rd2188];
	add.f64 	%fd11206, %fd5977, 0d0000000000000000;
	ld.global.f64 	%fd5978, [%rd2188+8];
	add.f64 	%fd11207, %fd5978, 0d0000000000000000;
	ld.global.f64 	%fd5979, [%rd2188+16];
	add.f64 	%fd11208, %fd5979, 0d0000000000000000;
	ld.global.f64 	%fd5980, [%rd2188+24];
	add.f64 	%fd11209, %fd5980, 0d0000000000000000;
	ld.global.f64 	%fd5981, [%rd2188+32];
	add.f64 	%fd11210, %fd5981, 0d0000000000000000;
	ld.global.f64 	%fd5982, [%rd2188+40];
	add.f64 	%fd11211, %fd5982, 0d0000000000000000;
	ld.global.f64 	%fd5983, [%rd2188+48];
	add.f64 	%fd11212, %fd5983, 0d0000000000000000;
	ld.global.f64 	%fd5984, [%rd2188+56];
	add.f64 	%fd11213, %fd5984, 0d0000000000000000;
	ld.global.f64 	%fd5985, [%rd2188+64];
	add.f64 	%fd11214, %fd5985, 0d0000000000000000;

$L__BB1_425:
	add.s32 	%r2237, %r635, 1;
	or.b32  	%r2236, %r2237, %r633;
	or.b32  	%r2235, %r2236, %r644;
	setp.lt.s32 	%p947, %r2235, 0;
	add.f64 	%fd1954, %fd11214, 0d0000000000000000;
	add.f64 	%fd1955, %fd11213, 0d0000000000000000;
	add.f64 	%fd1956, %fd11212, 0d0000000000000000;
	add.f64 	%fd1957, %fd11211, 0d0000000000000000;
	add.f64 	%fd1958, %fd11210, 0d0000000000000000;
	add.f64 	%fd1959, %fd11209, 0d0000000000000000;
	add.f64 	%fd1960, %fd11208, 0d0000000000000000;
	add.f64 	%fd1961, %fd11207, 0d0000000000000000;
	add.f64 	%fd1962, %fd11206, 0d0000000000000000;
	ld.param.u64 	%rd287, [%rd27+120];
	ld.param.u32 	%r848, [%rd27+144];
	ld.param.u32 	%r849, [%rd27+172];
	ld.param.v2.u32 	{%r1553, %r1554}, [%rd27+176];
	setp.le.s32 	%p763, %r1553, %r633;
	setp.le.s32 	%p764, %r1554, %r644;
	setp.le.s32 	%p765, %r849, %r2237;
	or.pred  	%p766, %p763, %p764;
	or.pred  	%p768, %p947, %p766;
	or.pred  	%p769, %p765, %p768;
	mov.f64 	%fd11215, 0d0000000000000000;
	mov.f64 	%fd11216, 0d0000000000000000;
	mov.f64 	%fd11217, 0d0000000000000000;
	mov.f64 	%fd11218, 0d0000000000000000;
	mov.f64 	%fd11219, 0d0000000000000000;
	mov.f64 	%fd11220, 0d0000000000000000;
	mov.f64 	%fd11221, 0d0000000000000000;
	mov.f64 	%fd11222, 0d0000000000000000;
	mov.f64 	%fd11223, 0d0000000000000000;
	@%p769 bra 	$L__BB1_428;
	bra.uni 	$L__BB1_426;

$L__BB1_428:
	add.s32 	%r1961, %r634, 1;
	st.local.v2.u32 	[%rd26], {%r633, %r1961};
	add.s32 	%r1962, %r635, 1;
	st.local.v2.u32 	[%rd26+8], {%r1962, %r1553};
	st.local.v2.u32 	[%rd26+16], {%r1554, %r849};
	mov.u64 	%rd2195, $str$1;
	cvta.global.u64 	%rd2196, %rd2195;
	{ // callseq 442, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2196;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1555, [retval0+0];
	} // callseq 442
	bra.uni 	$L__BB1_429;

$L__BB1_426:
	setp.eq.s64 	%p770, %rd287, 0;
	@%p770 bra 	$L__BB1_429;

	add.s32 	%r2238, %r635, 1;
	cvta.to.global.u64 	%rd2192, %rd287;
	mul.wide.s32 	%rd2193, %r848, %r2238;
	add.s64 	%rd2194, %rd2192, %rd2193;
	ld.global.f64 	%fd6004, [%rd2194];
	add.f64 	%fd11215, %fd6004, 0d0000000000000000;
	ld.global.f64 	%fd6005, [%rd2194+8];
	add.f64 	%fd11216, %fd6005, 0d0000000000000000;
	ld.global.f64 	%fd6006, [%rd2194+16];
	add.f64 	%fd11217, %fd6006, 0d0000000000000000;
	ld.global.f64 	%fd6007, [%rd2194+24];
	add.f64 	%fd11218, %fd6007, 0d0000000000000000;
	ld.global.f64 	%fd6008, [%rd2194+32];
	add.f64 	%fd11219, %fd6008, 0d0000000000000000;
	ld.global.f64 	%fd6009, [%rd2194+40];
	add.f64 	%fd11220, %fd6009, 0d0000000000000000;
	ld.global.f64 	%fd6010, [%rd2194+48];
	add.f64 	%fd11221, %fd6010, 0d0000000000000000;
	ld.global.f64 	%fd6011, [%rd2194+56];
	add.f64 	%fd11222, %fd6011, 0d0000000000000000;
	ld.global.f64 	%fd6012, [%rd2194+64];
	add.f64 	%fd11223, %fd6012, 0d0000000000000000;

$L__BB1_429:
	or.b32  	%r2191, %r633, %r635;
	or.b32  	%r2190, %r2191, %r634;
	setp.lt.s32 	%p945, %r2190, 0;
	add.f64 	%fd1981, %fd11223, 0d0000000000000000;
	add.f64 	%fd1982, %fd11222, 0d0000000000000000;
	add.f64 	%fd1983, %fd11221, 0d0000000000000000;
	add.f64 	%fd1984, %fd11220, 0d0000000000000000;
	add.f64 	%fd1985, %fd11219, 0d0000000000000000;
	add.f64 	%fd1986, %fd11218, 0d0000000000000000;
	add.f64 	%fd1987, %fd11217, 0d0000000000000000;
	add.f64 	%fd1988, %fd11216, 0d0000000000000000;
	add.f64 	%fd1989, %fd11215, 0d0000000000000000;
	ld.param.u32 	%r854, [%rd27+172];
	ld.param.v2.u32 	{%r1556, %r1557}, [%rd27+176];
	setp.le.s32 	%p771, %r1556, %r633;
	setp.le.s32 	%p772, %r1557, %r634;
	setp.le.s32 	%p773, %r854, %r635;
	or.pred  	%p774, %p771, %p772;
	or.pred  	%p776, %p945, %p774;
	or.pred  	%p777, %p773, %p776;
	mov.f64 	%fd11224, 0d0000000000000000;
	mov.f64 	%fd11225, 0d0000000000000000;
	mov.f64 	%fd11226, 0d0000000000000000;
	mov.f64 	%fd11227, 0d0000000000000000;
	mov.f64 	%fd11228, 0d0000000000000000;
	mov.f64 	%fd11229, 0d0000000000000000;
	mov.f64 	%fd11230, 0d0000000000000000;
	mov.f64 	%fd11231, 0d0000000000000000;
	mov.f64 	%fd11232, 0d0000000000000000;
	@%p777 bra 	$L__BB1_432;
	bra.uni 	$L__BB1_430;

$L__BB1_432:
	st.local.v2.u32 	[%rd26], {%r633, %r634};
	st.local.v2.u32 	[%rd26+8], {%r635, %r1556};
	st.local.v2.u32 	[%rd26+16], {%r1557, %r854};
	mov.u64 	%rd2201, $str$1;
	cvta.global.u64 	%rd2202, %rd2201;
	{ // callseq 443, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2202;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1558, [retval0+0];
	} // callseq 443
	bra.uni 	$L__BB1_433;

$L__BB1_430:
	ld.param.u64 	%rd4077, [%rd27+120];
	setp.eq.s64 	%p778, %rd4077, 0;
	@%p778 bra 	$L__BB1_433;

	ld.param.u32 	%r2192, [%rd27+144];
	ld.param.u64 	%rd4078, [%rd27+120];
	cvta.to.global.u64 	%rd2198, %rd4078;
	mul.wide.s32 	%rd2199, %r2192, %r635;
	add.s64 	%rd2200, %rd2198, %rd2199;
	ld.global.f64 	%fd6031, [%rd2200];
	add.f64 	%fd11224, %fd6031, 0d0000000000000000;
	ld.global.f64 	%fd6032, [%rd2200+8];
	add.f64 	%fd11225, %fd6032, 0d0000000000000000;
	ld.global.f64 	%fd6033, [%rd2200+16];
	add.f64 	%fd11226, %fd6033, 0d0000000000000000;
	ld.global.f64 	%fd6034, [%rd2200+24];
	add.f64 	%fd11227, %fd6034, 0d0000000000000000;
	ld.global.f64 	%fd6035, [%rd2200+32];
	add.f64 	%fd11228, %fd6035, 0d0000000000000000;
	ld.global.f64 	%fd6036, [%rd2200+40];
	add.f64 	%fd11229, %fd6036, 0d0000000000000000;
	ld.global.f64 	%fd6037, [%rd2200+48];
	add.f64 	%fd11230, %fd6037, 0d0000000000000000;
	ld.global.f64 	%fd6038, [%rd2200+56];
	add.f64 	%fd11231, %fd6038, 0d0000000000000000;
	ld.global.f64 	%fd6039, [%rd2200+64];
	add.f64 	%fd11232, %fd6039, 0d0000000000000000;

$L__BB1_433:
	add.f64 	%fd2008, %fd11232, 0d0000000000000000;
	add.f64 	%fd2009, %fd11231, 0d0000000000000000;
	add.f64 	%fd2010, %fd11230, 0d0000000000000000;
	add.f64 	%fd2011, %fd11229, 0d0000000000000000;
	add.f64 	%fd2012, %fd11228, 0d0000000000000000;
	add.f64 	%fd2013, %fd11227, 0d0000000000000000;
	add.f64 	%fd2014, %fd11226, 0d0000000000000000;
	add.f64 	%fd2015, %fd11225, 0d0000000000000000;
	add.f64 	%fd2016, %fd11224, 0d0000000000000000;
	ld.param.u32 	%r859, [%rd25+60];
	shl.b32 	%r860, %r2254, 4;
	setp.le.s32 	%p779, %r859, %r860;
	selp.u16 	%rs163, 1, 0, %p779;
	shr.u32 	%r1559, %r2254, 27;
	cvt.u16.u32 	%rs164, %r1559;
	and.b16  	%rs1, %rs164, 1;
	or.b16  	%rs165, %rs1, %rs163;
	setp.eq.s16 	%p780, %rs165, 0;
	@%p780 bra 	$L__BB1_435;

	st.local.v2.u32 	[%rd26], {%r860, %r859};
	mov.u64 	%rd2204, $str;
	cvta.global.u64 	%rd2205, %rd2204;
	{ // callseq 444, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2205;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1560, [retval0+0];
	} // callseq 444
	bra.uni 	$L__BB1_436;

$L__BB1_435:
	ld.param.u64 	%rd4115, [%rd25];
	ld.param.u32 	%r2240, [%rd25+32];
	mul.wide.s32 	%rd2216, %r2240, %r860;
	add.s64 	%rd2207, %rd4115, %rd2216;
	// begin inline asm
	{ atom.add.f64 %fd6049,[%rd2207],%fd10656; }

	// end inline asm
	add.s64 	%rd2208, %rd2207, 8;
	// begin inline asm
	{ atom.add.f64 %fd6051,[%rd2208],%fd10655; }

	// end inline asm
	add.s64 	%rd2209, %rd2207, 16;
	// begin inline asm
	{ atom.add.f64 %fd6053,[%rd2209],%fd10654; }

	// end inline asm
	add.s64 	%rd2210, %rd2207, 24;
	// begin inline asm
	{ atom.add.f64 %fd6055,[%rd2210],%fd10644; }

	// end inline asm
	add.s64 	%rd2211, %rd2207, 32;
	// begin inline asm
	{ atom.add.f64 %fd6057,[%rd2211],%fd10643; }

	// end inline asm
	add.s64 	%rd2212, %rd2207, 40;
	// begin inline asm
	{ atom.add.f64 %fd6059,[%rd2212],%fd10642; }

	// end inline asm
	add.s64 	%rd2213, %rd2207, 48;
	// begin inline asm
	{ atom.add.f64 %fd6061,[%rd2213],%fd10632; }

	// end inline asm
	add.s64 	%rd2214, %rd2207, 56;
	// begin inline asm
	{ atom.add.f64 %fd6063,[%rd2214],%fd10631; }

	// end inline asm
	add.s64 	%rd2215, %rd2207, 64;
	// begin inline asm
	{ atom.add.f64 %fd6065,[%rd2215],%fd10630; }

	// end inline asm

$L__BB1_436:
	ld.param.u64 	%rd290, [%rd25];
	ld.param.u32 	%r861, [%rd25+32];
	ld.param.u32 	%r862, [%rd25+60];
	add.s32 	%r863, %r860, 1;
	setp.le.s32 	%p781, %r862, %r863;
	selp.u16 	%rs166, 1, 0, %p781;
	shr.u32 	%r1561, %r863, 31;
	cvt.u16.u32 	%rs2, %r1561;
	or.b16  	%rs167, %rs166, %rs2;
	setp.eq.s16 	%p782, %rs167, 0;
	@%p782 bra 	$L__BB1_438;

	add.s32 	%r1963, %r860, 1;
	st.local.v2.u32 	[%rd26], {%r1963, %r862};
	mov.u64 	%rd2217, $str;
	cvta.global.u64 	%rd2218, %rd2217;
	{ // callseq 445, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2218;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1562, [retval0+0];
	} // callseq 445
	bra.uni 	$L__BB1_439;

$L__BB1_438:
	mul.wide.s32 	%rd2229, %r861, %r863;
	add.s64 	%rd2220, %rd290, %rd2229;
	// begin inline asm
	{ atom.add.f64 %fd6067,[%rd2220],%fd10653; }

	// end inline asm
	add.s64 	%rd2221, %rd2220, 8;
	// begin inline asm
	{ atom.add.f64 %fd6069,[%rd2221],%fd10652; }

	// end inline asm
	add.s64 	%rd2222, %rd2220, 16;
	// begin inline asm
	{ atom.add.f64 %fd6071,[%rd2222],%fd10651; }

	// end inline asm
	add.s64 	%rd2223, %rd2220, 24;
	// begin inline asm
	{ atom.add.f64 %fd6073,[%rd2223],%fd10641; }

	// end inline asm
	add.s64 	%rd2224, %rd2220, 32;
	// begin inline asm
	{ atom.add.f64 %fd6075,[%rd2224],%fd10640; }

	// end inline asm
	add.s64 	%rd2225, %rd2220, 40;
	// begin inline asm
	{ atom.add.f64 %fd6077,[%rd2225],%fd10639; }

	// end inline asm
	add.s64 	%rd2226, %rd2220, 48;
	// begin inline asm
	{ atom.add.f64 %fd6079,[%rd2226],%fd10629; }

	// end inline asm
	add.s64 	%rd2227, %rd2220, 56;
	// begin inline asm
	{ atom.add.f64 %fd6081,[%rd2227],%fd10628; }

	// end inline asm
	add.s64 	%rd2228, %rd2220, 64;
	// begin inline asm
	{ atom.add.f64 %fd6083,[%rd2228],%fd10627; }

	// end inline asm

$L__BB1_439:
	ld.param.u64 	%rd291, [%rd25];
	ld.param.u32 	%r864, [%rd25+32];
	ld.param.u32 	%r865, [%rd25+60];
	add.s32 	%r866, %r860, 2;
	setp.le.s32 	%p783, %r865, %r866;
	selp.u16 	%rs168, 1, 0, %p783;
	shr.u32 	%r1563, %r866, 31;
	cvt.u16.u32 	%rs3, %r1563;
	or.b16  	%rs169, %rs168, %rs3;
	setp.eq.s16 	%p784, %rs169, 0;
	@%p784 bra 	$L__BB1_441;

	add.s32 	%r1964, %r860, 2;
	st.local.v2.u32 	[%rd26], {%r1964, %r865};
	mov.u64 	%rd2230, $str;
	cvta.global.u64 	%rd2231, %rd2230;
	{ // callseq 446, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2231;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1564, [retval0+0];
	} // callseq 446
	bra.uni 	$L__BB1_442;

$L__BB1_441:
	mul.wide.s32 	%rd2242, %r864, %r866;
	add.s64 	%rd2233, %rd291, %rd2242;
	// begin inline asm
	{ atom.add.f64 %fd6085,[%rd2233],%fd10650; }

	// end inline asm
	add.s64 	%rd2234, %rd2233, 8;
	// begin inline asm
	{ atom.add.f64 %fd6087,[%rd2234],%fd10649; }

	// end inline asm
	add.s64 	%rd2235, %rd2233, 16;
	// begin inline asm
	{ atom.add.f64 %fd6089,[%rd2235],%fd10648; }

	// end inline asm
	add.s64 	%rd2236, %rd2233, 24;
	// begin inline asm
	{ atom.add.f64 %fd6091,[%rd2236],%fd10638; }

	// end inline asm
	add.s64 	%rd2237, %rd2233, 32;
	// begin inline asm
	{ atom.add.f64 %fd6093,[%rd2237],%fd10637; }

	// end inline asm
	add.s64 	%rd2238, %rd2233, 40;
	// begin inline asm
	{ atom.add.f64 %fd6095,[%rd2238],%fd10636; }

	// end inline asm
	add.s64 	%rd2239, %rd2233, 48;
	// begin inline asm
	{ atom.add.f64 %fd6097,[%rd2239],%fd10626; }

	// end inline asm
	add.s64 	%rd2240, %rd2233, 56;
	// begin inline asm
	{ atom.add.f64 %fd6099,[%rd2240],%fd10625; }

	// end inline asm
	add.s64 	%rd2241, %rd2233, 64;
	// begin inline asm
	{ atom.add.f64 %fd6101,[%rd2241],%fd10624; }

	// end inline asm

$L__BB1_442:
	ld.param.u64 	%rd292, [%rd25];
	ld.param.u32 	%r867, [%rd25+32];
	ld.param.u32 	%r868, [%rd25+60];
	add.s32 	%r869, %r860, 3;
	setp.le.s32 	%p785, %r868, %r869;
	selp.u16 	%rs170, 1, 0, %p785;
	shr.u32 	%r1565, %r869, 31;
	cvt.u16.u32 	%rs4, %r1565;
	or.b16  	%rs171, %rs170, %rs4;
	setp.eq.s16 	%p786, %rs171, 0;
	@%p786 bra 	$L__BB1_444;

	add.s32 	%r1965, %r860, 3;
	st.local.v2.u32 	[%rd26], {%r1965, %r868};
	mov.u64 	%rd2243, $str;
	cvta.global.u64 	%rd2244, %rd2243;
	{ // callseq 447, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2244;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1566, [retval0+0];
	} // callseq 447
	bra.uni 	$L__BB1_445;

$L__BB1_444:
	mul.wide.s32 	%rd2255, %r867, %r869;
	add.s64 	%rd2246, %rd292, %rd2255;
	// begin inline asm
	{ atom.add.f64 %fd6103,[%rd2246],%fd10647; }

	// end inline asm
	add.s64 	%rd2247, %rd2246, 8;
	// begin inline asm
	{ atom.add.f64 %fd6105,[%rd2247],%fd10646; }

	// end inline asm
	add.s64 	%rd2248, %rd2246, 16;
	// begin inline asm
	{ atom.add.f64 %fd6107,[%rd2248],%fd10645; }

	// end inline asm
	add.s64 	%rd2249, %rd2246, 24;
	// begin inline asm
	{ atom.add.f64 %fd6109,[%rd2249],%fd10635; }

	// end inline asm
	add.s64 	%rd2250, %rd2246, 32;
	// begin inline asm
	{ atom.add.f64 %fd6111,[%rd2250],%fd10634; }

	// end inline asm
	add.s64 	%rd2251, %rd2246, 40;
	// begin inline asm
	{ atom.add.f64 %fd6113,[%rd2251],%fd10633; }

	// end inline asm
	add.s64 	%rd2252, %rd2246, 48;
	// begin inline asm
	{ atom.add.f64 %fd6115,[%rd2252],%fd10623; }

	// end inline asm
	add.s64 	%rd2253, %rd2246, 56;
	// begin inline asm
	{ atom.add.f64 %fd6117,[%rd2253],%fd10622; }

	// end inline asm
	add.s64 	%rd2254, %rd2246, 64;
	// begin inline asm
	{ atom.add.f64 %fd6119,[%rd2254],%fd10621; }

	// end inline asm

$L__BB1_445:
	ld.param.u64 	%rd293, [%rd25];
	ld.param.u32 	%r870, [%rd25+32];
	ld.param.u32 	%r871, [%rd25+60];
	add.s32 	%r872, %r860, 4;
	setp.le.s32 	%p787, %r871, %r872;
	selp.u16 	%rs172, 1, 0, %p787;
	shr.u32 	%r1567, %r872, 31;
	cvt.u16.u32 	%rs5, %r1567;
	or.b16  	%rs173, %rs172, %rs5;
	setp.eq.s16 	%p788, %rs173, 0;
	@%p788 bra 	$L__BB1_447;

	add.s32 	%r1966, %r860, 4;
	st.local.v2.u32 	[%rd26], {%r1966, %r871};
	mov.u64 	%rd2256, $str;
	cvta.global.u64 	%rd2257, %rd2256;
	{ // callseq 448, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2257;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1568, [retval0+0];
	} // callseq 448
	bra.uni 	$L__BB1_448;

$L__BB1_447:
	mul.wide.s32 	%rd2268, %r870, %r872;
	add.s64 	%rd2259, %rd293, %rd2268;
	// begin inline asm
	{ atom.add.f64 %fd6121,[%rd2259],%fd10620; }

	// end inline asm
	add.s64 	%rd2260, %rd2259, 8;
	// begin inline asm
	{ atom.add.f64 %fd6123,[%rd2260],%fd10619; }

	// end inline asm
	add.s64 	%rd2261, %rd2259, 16;
	// begin inline asm
	{ atom.add.f64 %fd6125,[%rd2261],%fd10618; }

	// end inline asm
	add.s64 	%rd2262, %rd2259, 24;
	// begin inline asm
	{ atom.add.f64 %fd6127,[%rd2262],%fd10608; }

	// end inline asm
	add.s64 	%rd2263, %rd2259, 32;
	// begin inline asm
	{ atom.add.f64 %fd6129,[%rd2263],%fd10607; }

	// end inline asm
	add.s64 	%rd2264, %rd2259, 40;
	// begin inline asm
	{ atom.add.f64 %fd6131,[%rd2264],%fd10606; }

	// end inline asm
	add.s64 	%rd2265, %rd2259, 48;
	// begin inline asm
	{ atom.add.f64 %fd6133,[%rd2265],%fd10596; }

	// end inline asm
	add.s64 	%rd2266, %rd2259, 56;
	// begin inline asm
	{ atom.add.f64 %fd6135,[%rd2266],%fd10595; }

	// end inline asm
	add.s64 	%rd2267, %rd2259, 64;
	// begin inline asm
	{ atom.add.f64 %fd6137,[%rd2267],%fd10594; }

	// end inline asm

$L__BB1_448:
	ld.param.u64 	%rd294, [%rd25];
	ld.param.u32 	%r873, [%rd25+32];
	ld.param.u32 	%r874, [%rd25+60];
	add.s32 	%r875, %r860, 5;
	setp.le.s32 	%p789, %r874, %r875;
	selp.u16 	%rs174, 1, 0, %p789;
	shr.u32 	%r1569, %r875, 31;
	cvt.u16.u32 	%rs6, %r1569;
	or.b16  	%rs175, %rs174, %rs6;
	setp.eq.s16 	%p790, %rs175, 0;
	@%p790 bra 	$L__BB1_450;

	add.s32 	%r1967, %r860, 5;
	st.local.v2.u32 	[%rd26], {%r1967, %r874};
	mov.u64 	%rd2269, $str;
	cvta.global.u64 	%rd2270, %rd2269;
	{ // callseq 449, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2270;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1570, [retval0+0];
	} // callseq 449
	bra.uni 	$L__BB1_451;

$L__BB1_450:
	mul.wide.s32 	%rd2281, %r873, %r875;
	add.s64 	%rd2272, %rd294, %rd2281;
	// begin inline asm
	{ atom.add.f64 %fd6139,[%rd2272],%fd10617; }

	// end inline asm
	add.s64 	%rd2273, %rd2272, 8;
	// begin inline asm
	{ atom.add.f64 %fd6141,[%rd2273],%fd10616; }

	// end inline asm
	add.s64 	%rd2274, %rd2272, 16;
	// begin inline asm
	{ atom.add.f64 %fd6143,[%rd2274],%fd10615; }

	// end inline asm
	add.s64 	%rd2275, %rd2272, 24;
	// begin inline asm
	{ atom.add.f64 %fd6145,[%rd2275],%fd10605; }

	// end inline asm
	add.s64 	%rd2276, %rd2272, 32;
	// begin inline asm
	{ atom.add.f64 %fd6147,[%rd2276],%fd10604; }

	// end inline asm
	add.s64 	%rd2277, %rd2272, 40;
	// begin inline asm
	{ atom.add.f64 %fd6149,[%rd2277],%fd10603; }

	// end inline asm
	add.s64 	%rd2278, %rd2272, 48;
	// begin inline asm
	{ atom.add.f64 %fd6151,[%rd2278],%fd10593; }

	// end inline asm
	add.s64 	%rd2279, %rd2272, 56;
	// begin inline asm
	{ atom.add.f64 %fd6153,[%rd2279],%fd10592; }

	// end inline asm
	add.s64 	%rd2280, %rd2272, 64;
	// begin inline asm
	{ atom.add.f64 %fd6155,[%rd2280],%fd10591; }

	// end inline asm

$L__BB1_451:
	ld.param.u64 	%rd295, [%rd25];
	ld.param.u32 	%r876, [%rd25+32];
	ld.param.u32 	%r877, [%rd25+60];
	add.s32 	%r878, %r860, 6;
	setp.le.s32 	%p791, %r877, %r878;
	selp.u16 	%rs176, 1, 0, %p791;
	shr.u32 	%r1571, %r878, 31;
	cvt.u16.u32 	%rs7, %r1571;
	or.b16  	%rs177, %rs176, %rs7;
	setp.eq.s16 	%p792, %rs177, 0;
	@%p792 bra 	$L__BB1_453;

	add.s32 	%r1968, %r860, 6;
	st.local.v2.u32 	[%rd26], {%r1968, %r877};
	mov.u64 	%rd2282, $str;
	cvta.global.u64 	%rd2283, %rd2282;
	{ // callseq 450, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2283;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1572, [retval0+0];
	} // callseq 450
	bra.uni 	$L__BB1_454;

$L__BB1_453:
	mul.wide.s32 	%rd2294, %r876, %r878;
	add.s64 	%rd2285, %rd295, %rd2294;
	// begin inline asm
	{ atom.add.f64 %fd6157,[%rd2285],%fd10614; }

	// end inline asm
	add.s64 	%rd2286, %rd2285, 8;
	// begin inline asm
	{ atom.add.f64 %fd6159,[%rd2286],%fd10613; }

	// end inline asm
	add.s64 	%rd2287, %rd2285, 16;
	// begin inline asm
	{ atom.add.f64 %fd6161,[%rd2287],%fd10612; }

	// end inline asm
	add.s64 	%rd2288, %rd2285, 24;
	// begin inline asm
	{ atom.add.f64 %fd6163,[%rd2288],%fd10602; }

	// end inline asm
	add.s64 	%rd2289, %rd2285, 32;
	// begin inline asm
	{ atom.add.f64 %fd6165,[%rd2289],%fd10601; }

	// end inline asm
	add.s64 	%rd2290, %rd2285, 40;
	// begin inline asm
	{ atom.add.f64 %fd6167,[%rd2290],%fd10600; }

	// end inline asm
	add.s64 	%rd2291, %rd2285, 48;
	// begin inline asm
	{ atom.add.f64 %fd6169,[%rd2291],%fd10590; }

	// end inline asm
	add.s64 	%rd2292, %rd2285, 56;
	// begin inline asm
	{ atom.add.f64 %fd6171,[%rd2292],%fd10589; }

	// end inline asm
	add.s64 	%rd2293, %rd2285, 64;
	// begin inline asm
	{ atom.add.f64 %fd6173,[%rd2293],%fd10588; }

	// end inline asm

$L__BB1_454:
	ld.param.u64 	%rd296, [%rd25];
	ld.param.u32 	%r879, [%rd25+32];
	ld.param.u32 	%r880, [%rd25+60];
	add.s32 	%r881, %r860, 7;
	setp.le.s32 	%p793, %r880, %r881;
	selp.u16 	%rs178, 1, 0, %p793;
	shr.u32 	%r1573, %r881, 31;
	cvt.u16.u32 	%rs8, %r1573;
	or.b16  	%rs179, %rs178, %rs8;
	setp.eq.s16 	%p794, %rs179, 0;
	@%p794 bra 	$L__BB1_456;

	add.s32 	%r1969, %r860, 7;
	st.local.v2.u32 	[%rd26], {%r1969, %r880};
	mov.u64 	%rd2295, $str;
	cvta.global.u64 	%rd2296, %rd2295;
	{ // callseq 451, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2296;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1574, [retval0+0];
	} // callseq 451
	bra.uni 	$L__BB1_457;

$L__BB1_456:
	mul.wide.s32 	%rd2307, %r879, %r881;
	add.s64 	%rd2298, %rd296, %rd2307;
	// begin inline asm
	{ atom.add.f64 %fd6175,[%rd2298],%fd10611; }

	// end inline asm
	add.s64 	%rd2299, %rd2298, 8;
	// begin inline asm
	{ atom.add.f64 %fd6177,[%rd2299],%fd10610; }

	// end inline asm
	add.s64 	%rd2300, %rd2298, 16;
	// begin inline asm
	{ atom.add.f64 %fd6179,[%rd2300],%fd10609; }

	// end inline asm
	add.s64 	%rd2301, %rd2298, 24;
	// begin inline asm
	{ atom.add.f64 %fd6181,[%rd2301],%fd10599; }

	// end inline asm
	add.s64 	%rd2302, %rd2298, 32;
	// begin inline asm
	{ atom.add.f64 %fd6183,[%rd2302],%fd10598; }

	// end inline asm
	add.s64 	%rd2303, %rd2298, 40;
	// begin inline asm
	{ atom.add.f64 %fd6185,[%rd2303],%fd10597; }

	// end inline asm
	add.s64 	%rd2304, %rd2298, 48;
	// begin inline asm
	{ atom.add.f64 %fd6187,[%rd2304],%fd10587; }

	// end inline asm
	add.s64 	%rd2305, %rd2298, 56;
	// begin inline asm
	{ atom.add.f64 %fd6189,[%rd2305],%fd10586; }

	// end inline asm
	add.s64 	%rd2306, %rd2298, 64;
	// begin inline asm
	{ atom.add.f64 %fd6191,[%rd2306],%fd10585; }

	// end inline asm

$L__BB1_457:
	ld.param.u64 	%rd297, [%rd25];
	ld.param.u32 	%r882, [%rd25+32];
	ld.param.u32 	%r883, [%rd25+60];
	add.s32 	%r884, %r860, 8;
	setp.le.s32 	%p795, %r883, %r884;
	selp.u16 	%rs180, 1, 0, %p795;
	shr.u32 	%r1575, %r884, 31;
	cvt.u16.u32 	%rs9, %r1575;
	or.b16  	%rs181, %rs180, %rs9;
	setp.eq.s16 	%p796, %rs181, 0;
	@%p796 bra 	$L__BB1_459;

	add.s32 	%r1970, %r860, 8;
	st.local.v2.u32 	[%rd26], {%r1970, %r883};
	mov.u64 	%rd2308, $str;
	cvta.global.u64 	%rd2309, %rd2308;
	{ // callseq 452, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2309;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1576, [retval0+0];
	} // callseq 452
	bra.uni 	$L__BB1_460;

$L__BB1_459:
	mul.wide.s32 	%rd2320, %r882, %r884;
	add.s64 	%rd2311, %rd297, %rd2320;
	// begin inline asm
	{ atom.add.f64 %fd6193,[%rd2311],%fd10584; }

	// end inline asm
	add.s64 	%rd2312, %rd2311, 8;
	// begin inline asm
	{ atom.add.f64 %fd6195,[%rd2312],%fd10583; }

	// end inline asm
	add.s64 	%rd2313, %rd2311, 16;
	// begin inline asm
	{ atom.add.f64 %fd6197,[%rd2313],%fd10582; }

	// end inline asm
	add.s64 	%rd2314, %rd2311, 24;
	// begin inline asm
	{ atom.add.f64 %fd6199,[%rd2314],%fd10572; }

	// end inline asm
	add.s64 	%rd2315, %rd2311, 32;
	// begin inline asm
	{ atom.add.f64 %fd6201,[%rd2315],%fd10571; }

	// end inline asm
	add.s64 	%rd2316, %rd2311, 40;
	// begin inline asm
	{ atom.add.f64 %fd6203,[%rd2316],%fd10570; }

	// end inline asm
	add.s64 	%rd2317, %rd2311, 48;
	// begin inline asm
	{ atom.add.f64 %fd6205,[%rd2317],%fd10560; }

	// end inline asm
	add.s64 	%rd2318, %rd2311, 56;
	// begin inline asm
	{ atom.add.f64 %fd6207,[%rd2318],%fd10559; }

	// end inline asm
	add.s64 	%rd2319, %rd2311, 64;
	// begin inline asm
	{ atom.add.f64 %fd6209,[%rd2319],%fd10558; }

	// end inline asm

$L__BB1_460:
	ld.param.u64 	%rd298, [%rd25];
	ld.param.u32 	%r885, [%rd25+32];
	ld.param.u32 	%r886, [%rd25+60];
	add.s32 	%r887, %r860, 9;
	setp.le.s32 	%p797, %r886, %r887;
	selp.u16 	%rs182, 1, 0, %p797;
	shr.u32 	%r1577, %r887, 31;
	cvt.u16.u32 	%rs10, %r1577;
	or.b16  	%rs183, %rs182, %rs10;
	setp.eq.s16 	%p798, %rs183, 0;
	@%p798 bra 	$L__BB1_462;

	add.s32 	%r1971, %r860, 9;
	st.local.v2.u32 	[%rd26], {%r1971, %r886};
	mov.u64 	%rd2321, $str;
	cvta.global.u64 	%rd2322, %rd2321;
	{ // callseq 453, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2322;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1578, [retval0+0];
	} // callseq 453
	bra.uni 	$L__BB1_463;

$L__BB1_462:
	mul.wide.s32 	%rd2333, %r885, %r887;
	add.s64 	%rd2324, %rd298, %rd2333;
	// begin inline asm
	{ atom.add.f64 %fd6211,[%rd2324],%fd10581; }

	// end inline asm
	add.s64 	%rd2325, %rd2324, 8;
	// begin inline asm
	{ atom.add.f64 %fd6213,[%rd2325],%fd10580; }

	// end inline asm
	add.s64 	%rd2326, %rd2324, 16;
	// begin inline asm
	{ atom.add.f64 %fd6215,[%rd2326],%fd10579; }

	// end inline asm
	add.s64 	%rd2327, %rd2324, 24;
	// begin inline asm
	{ atom.add.f64 %fd6217,[%rd2327],%fd10569; }

	// end inline asm
	add.s64 	%rd2328, %rd2324, 32;
	// begin inline asm
	{ atom.add.f64 %fd6219,[%rd2328],%fd10568; }

	// end inline asm
	add.s64 	%rd2329, %rd2324, 40;
	// begin inline asm
	{ atom.add.f64 %fd6221,[%rd2329],%fd10567; }

	// end inline asm
	add.s64 	%rd2330, %rd2324, 48;
	// begin inline asm
	{ atom.add.f64 %fd6223,[%rd2330],%fd10557; }

	// end inline asm
	add.s64 	%rd2331, %rd2324, 56;
	// begin inline asm
	{ atom.add.f64 %fd6225,[%rd2331],%fd10556; }

	// end inline asm
	add.s64 	%rd2332, %rd2324, 64;
	// begin inline asm
	{ atom.add.f64 %fd6227,[%rd2332],%fd10555; }

	// end inline asm

$L__BB1_463:
	ld.param.u64 	%rd299, [%rd25];
	ld.param.u32 	%r888, [%rd25+32];
	ld.param.u32 	%r889, [%rd25+60];
	add.s32 	%r890, %r860, 10;
	setp.le.s32 	%p799, %r889, %r890;
	selp.u16 	%rs184, 1, 0, %p799;
	shr.u32 	%r1579, %r890, 31;
	cvt.u16.u32 	%rs11, %r1579;
	or.b16  	%rs185, %rs184, %rs11;
	setp.eq.s16 	%p800, %rs185, 0;
	@%p800 bra 	$L__BB1_465;

	add.s32 	%r1972, %r860, 10;
	st.local.v2.u32 	[%rd26], {%r1972, %r889};
	mov.u64 	%rd2334, $str;
	cvta.global.u64 	%rd2335, %rd2334;
	{ // callseq 454, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2335;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1580, [retval0+0];
	} // callseq 454
	bra.uni 	$L__BB1_466;

$L__BB1_465:
	mul.wide.s32 	%rd2346, %r888, %r890;
	add.s64 	%rd2337, %rd299, %rd2346;
	// begin inline asm
	{ atom.add.f64 %fd6229,[%rd2337],%fd10578; }

	// end inline asm
	add.s64 	%rd2338, %rd2337, 8;
	// begin inline asm
	{ atom.add.f64 %fd6231,[%rd2338],%fd10577; }

	// end inline asm
	add.s64 	%rd2339, %rd2337, 16;
	// begin inline asm
	{ atom.add.f64 %fd6233,[%rd2339],%fd10576; }

	// end inline asm
	add.s64 	%rd2340, %rd2337, 24;
	// begin inline asm
	{ atom.add.f64 %fd6235,[%rd2340],%fd10566; }

	// end inline asm
	add.s64 	%rd2341, %rd2337, 32;
	// begin inline asm
	{ atom.add.f64 %fd6237,[%rd2341],%fd10565; }

	// end inline asm
	add.s64 	%rd2342, %rd2337, 40;
	// begin inline asm
	{ atom.add.f64 %fd6239,[%rd2342],%fd10564; }

	// end inline asm
	add.s64 	%rd2343, %rd2337, 48;
	// begin inline asm
	{ atom.add.f64 %fd6241,[%rd2343],%fd10554; }

	// end inline asm
	add.s64 	%rd2344, %rd2337, 56;
	// begin inline asm
	{ atom.add.f64 %fd6243,[%rd2344],%fd10553; }

	// end inline asm
	add.s64 	%rd2345, %rd2337, 64;
	// begin inline asm
	{ atom.add.f64 %fd6245,[%rd2345],%fd10552; }

	// end inline asm

$L__BB1_466:
	ld.param.u64 	%rd300, [%rd25];
	ld.param.u32 	%r891, [%rd25+32];
	ld.param.u32 	%r892, [%rd25+60];
	add.s32 	%r893, %r860, 11;
	setp.le.s32 	%p801, %r892, %r893;
	selp.u16 	%rs186, 1, 0, %p801;
	shr.u32 	%r1581, %r893, 31;
	cvt.u16.u32 	%rs12, %r1581;
	or.b16  	%rs187, %rs186, %rs12;
	setp.eq.s16 	%p802, %rs187, 0;
	@%p802 bra 	$L__BB1_468;

	add.s32 	%r1973, %r860, 11;
	st.local.v2.u32 	[%rd26], {%r1973, %r892};
	mov.u64 	%rd2347, $str;
	cvta.global.u64 	%rd2348, %rd2347;
	{ // callseq 455, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2348;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1582, [retval0+0];
	} // callseq 455
	bra.uni 	$L__BB1_469;

$L__BB1_468:
	mul.wide.s32 	%rd2359, %r891, %r893;
	add.s64 	%rd2350, %rd300, %rd2359;
	// begin inline asm
	{ atom.add.f64 %fd6247,[%rd2350],%fd10575; }

	// end inline asm
	add.s64 	%rd2351, %rd2350, 8;
	// begin inline asm
	{ atom.add.f64 %fd6249,[%rd2351],%fd10574; }

	// end inline asm
	add.s64 	%rd2352, %rd2350, 16;
	// begin inline asm
	{ atom.add.f64 %fd6251,[%rd2352],%fd10573; }

	// end inline asm
	add.s64 	%rd2353, %rd2350, 24;
	// begin inline asm
	{ atom.add.f64 %fd6253,[%rd2353],%fd10563; }

	// end inline asm
	add.s64 	%rd2354, %rd2350, 32;
	// begin inline asm
	{ atom.add.f64 %fd6255,[%rd2354],%fd10562; }

	// end inline asm
	add.s64 	%rd2355, %rd2350, 40;
	// begin inline asm
	{ atom.add.f64 %fd6257,[%rd2355],%fd10561; }

	// end inline asm
	add.s64 	%rd2356, %rd2350, 48;
	// begin inline asm
	{ atom.add.f64 %fd6259,[%rd2356],%fd10551; }

	// end inline asm
	add.s64 	%rd2357, %rd2350, 56;
	// begin inline asm
	{ atom.add.f64 %fd6261,[%rd2357],%fd10550; }

	// end inline asm
	add.s64 	%rd2358, %rd2350, 64;
	// begin inline asm
	{ atom.add.f64 %fd6263,[%rd2358],%fd10549; }

	// end inline asm

$L__BB1_469:
	ld.param.u64 	%rd301, [%rd25];
	ld.param.u32 	%r894, [%rd25+32];
	ld.param.u32 	%r895, [%rd25+60];
	add.s32 	%r896, %r860, 12;
	setp.le.s32 	%p803, %r895, %r896;
	selp.u16 	%rs188, 1, 0, %p803;
	shr.u32 	%r1583, %r896, 31;
	cvt.u16.u32 	%rs13, %r1583;
	or.b16  	%rs189, %rs188, %rs13;
	setp.eq.s16 	%p804, %rs189, 0;
	@%p804 bra 	$L__BB1_471;

	add.s32 	%r1974, %r860, 12;
	st.local.v2.u32 	[%rd26], {%r1974, %r895};
	mov.u64 	%rd2360, $str;
	cvta.global.u64 	%rd2361, %rd2360;
	{ // callseq 456, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2361;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1584, [retval0+0];
	} // callseq 456
	bra.uni 	$L__BB1_472;

$L__BB1_471:
	mul.wide.s32 	%rd2372, %r894, %r896;
	add.s64 	%rd2363, %rd301, %rd2372;
	// begin inline asm
	{ atom.add.f64 %fd6265,[%rd2363],%fd10548; }

	// end inline asm
	add.s64 	%rd2364, %rd2363, 8;
	// begin inline asm
	{ atom.add.f64 %fd6267,[%rd2364],%fd10547; }

	// end inline asm
	add.s64 	%rd2365, %rd2363, 16;
	// begin inline asm
	{ atom.add.f64 %fd6269,[%rd2365],%fd10546; }

	// end inline asm
	add.s64 	%rd2366, %rd2363, 24;
	// begin inline asm
	{ atom.add.f64 %fd6271,[%rd2366],%fd10536; }

	// end inline asm
	add.s64 	%rd2367, %rd2363, 32;
	// begin inline asm
	{ atom.add.f64 %fd6273,[%rd2367],%fd10535; }

	// end inline asm
	add.s64 	%rd2368, %rd2363, 40;
	// begin inline asm
	{ atom.add.f64 %fd6275,[%rd2368],%fd10534; }

	// end inline asm
	add.s64 	%rd2369, %rd2363, 48;
	// begin inline asm
	{ atom.add.f64 %fd6277,[%rd2369],%fd10524; }

	// end inline asm
	add.s64 	%rd2370, %rd2363, 56;
	// begin inline asm
	{ atom.add.f64 %fd6279,[%rd2370],%fd10523; }

	// end inline asm
	add.s64 	%rd2371, %rd2363, 64;
	// begin inline asm
	{ atom.add.f64 %fd6281,[%rd2371],%fd10522; }

	// end inline asm

$L__BB1_472:
	ld.param.u64 	%rd302, [%rd25];
	ld.param.u32 	%r897, [%rd25+32];
	ld.param.u32 	%r898, [%rd25+60];
	add.s32 	%r899, %r860, 13;
	setp.le.s32 	%p805, %r898, %r899;
	selp.u16 	%rs190, 1, 0, %p805;
	shr.u32 	%r1585, %r899, 31;
	cvt.u16.u32 	%rs14, %r1585;
	or.b16  	%rs191, %rs190, %rs14;
	setp.eq.s16 	%p806, %rs191, 0;
	@%p806 bra 	$L__BB1_474;

	add.s32 	%r1975, %r860, 13;
	st.local.v2.u32 	[%rd26], {%r1975, %r898};
	mov.u64 	%rd2373, $str;
	cvta.global.u64 	%rd2374, %rd2373;
	{ // callseq 457, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2374;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1586, [retval0+0];
	} // callseq 457
	bra.uni 	$L__BB1_475;

$L__BB1_474:
	mul.wide.s32 	%rd2385, %r897, %r899;
	add.s64 	%rd2376, %rd302, %rd2385;
	// begin inline asm
	{ atom.add.f64 %fd6283,[%rd2376],%fd10545; }

	// end inline asm
	add.s64 	%rd2377, %rd2376, 8;
	// begin inline asm
	{ atom.add.f64 %fd6285,[%rd2377],%fd10544; }

	// end inline asm
	add.s64 	%rd2378, %rd2376, 16;
	// begin inline asm
	{ atom.add.f64 %fd6287,[%rd2378],%fd10543; }

	// end inline asm
	add.s64 	%rd2379, %rd2376, 24;
	// begin inline asm
	{ atom.add.f64 %fd6289,[%rd2379],%fd10533; }

	// end inline asm
	add.s64 	%rd2380, %rd2376, 32;
	// begin inline asm
	{ atom.add.f64 %fd6291,[%rd2380],%fd10532; }

	// end inline asm
	add.s64 	%rd2381, %rd2376, 40;
	// begin inline asm
	{ atom.add.f64 %fd6293,[%rd2381],%fd10531; }

	// end inline asm
	add.s64 	%rd2382, %rd2376, 48;
	// begin inline asm
	{ atom.add.f64 %fd6295,[%rd2382],%fd10521; }

	// end inline asm
	add.s64 	%rd2383, %rd2376, 56;
	// begin inline asm
	{ atom.add.f64 %fd6297,[%rd2383],%fd10520; }

	// end inline asm
	add.s64 	%rd2384, %rd2376, 64;
	// begin inline asm
	{ atom.add.f64 %fd6299,[%rd2384],%fd10519; }

	// end inline asm

$L__BB1_475:
	ld.param.u64 	%rd303, [%rd25];
	ld.param.u32 	%r900, [%rd25+32];
	ld.param.u32 	%r901, [%rd25+60];
	add.s32 	%r902, %r860, 14;
	setp.le.s32 	%p807, %r901, %r902;
	selp.u16 	%rs192, 1, 0, %p807;
	shr.u32 	%r1587, %r902, 31;
	cvt.u16.u32 	%rs15, %r1587;
	or.b16  	%rs193, %rs192, %rs15;
	setp.eq.s16 	%p808, %rs193, 0;
	@%p808 bra 	$L__BB1_477;

	add.s32 	%r1976, %r860, 14;
	st.local.v2.u32 	[%rd26], {%r1976, %r901};
	mov.u64 	%rd2386, $str;
	cvta.global.u64 	%rd2387, %rd2386;
	{ // callseq 458, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2387;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1588, [retval0+0];
	} // callseq 458
	bra.uni 	$L__BB1_478;

$L__BB1_477:
	mul.wide.s32 	%rd2398, %r900, %r902;
	add.s64 	%rd2389, %rd303, %rd2398;
	// begin inline asm
	{ atom.add.f64 %fd6301,[%rd2389],%fd10542; }

	// end inline asm
	add.s64 	%rd2390, %rd2389, 8;
	// begin inline asm
	{ atom.add.f64 %fd6303,[%rd2390],%fd10541; }

	// end inline asm
	add.s64 	%rd2391, %rd2389, 16;
	// begin inline asm
	{ atom.add.f64 %fd6305,[%rd2391],%fd10540; }

	// end inline asm
	add.s64 	%rd2392, %rd2389, 24;
	// begin inline asm
	{ atom.add.f64 %fd6307,[%rd2392],%fd10530; }

	// end inline asm
	add.s64 	%rd2393, %rd2389, 32;
	// begin inline asm
	{ atom.add.f64 %fd6309,[%rd2393],%fd10529; }

	// end inline asm
	add.s64 	%rd2394, %rd2389, 40;
	// begin inline asm
	{ atom.add.f64 %fd6311,[%rd2394],%fd10528; }

	// end inline asm
	add.s64 	%rd2395, %rd2389, 48;
	// begin inline asm
	{ atom.add.f64 %fd6313,[%rd2395],%fd10518; }

	// end inline asm
	add.s64 	%rd2396, %rd2389, 56;
	// begin inline asm
	{ atom.add.f64 %fd6315,[%rd2396],%fd10517; }

	// end inline asm
	add.s64 	%rd2397, %rd2389, 64;
	// begin inline asm
	{ atom.add.f64 %fd6317,[%rd2397],%fd10516; }

	// end inline asm

$L__BB1_478:
	ld.param.u64 	%rd304, [%rd25];
	ld.param.u32 	%r903, [%rd25+32];
	ld.param.u32 	%r904, [%rd25+60];
	add.s32 	%r905, %r860, 15;
	setp.le.s32 	%p809, %r904, %r905;
	selp.u16 	%rs194, 1, 0, %p809;
	shr.u32 	%r1589, %r905, 31;
	cvt.u16.u32 	%rs16, %r1589;
	or.b16  	%rs195, %rs194, %rs16;
	setp.eq.s16 	%p810, %rs195, 0;
	@%p810 bra 	$L__BB1_480;

	add.s32 	%r1977, %r860, 15;
	st.local.v2.u32 	[%rd26], {%r1977, %r904};
	mov.u64 	%rd2399, $str;
	cvta.global.u64 	%rd2400, %rd2399;
	{ // callseq 459, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2400;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1590, [retval0+0];
	} // callseq 459
	bra.uni 	$L__BB1_481;

$L__BB1_480:
	mul.wide.s32 	%rd2411, %r903, %r905;
	add.s64 	%rd2402, %rd304, %rd2411;
	// begin inline asm
	{ atom.add.f64 %fd6319,[%rd2402],%fd10539; }

	// end inline asm
	add.s64 	%rd2403, %rd2402, 8;
	// begin inline asm
	{ atom.add.f64 %fd6321,[%rd2403],%fd10538; }

	// end inline asm
	add.s64 	%rd2404, %rd2402, 16;
	// begin inline asm
	{ atom.add.f64 %fd6323,[%rd2404],%fd10537; }

	// end inline asm
	add.s64 	%rd2405, %rd2402, 24;
	// begin inline asm
	{ atom.add.f64 %fd6325,[%rd2405],%fd10527; }

	// end inline asm
	add.s64 	%rd2406, %rd2402, 32;
	// begin inline asm
	{ atom.add.f64 %fd6327,[%rd2406],%fd10526; }

	// end inline asm
	add.s64 	%rd2407, %rd2402, 40;
	// begin inline asm
	{ atom.add.f64 %fd6329,[%rd2407],%fd10525; }

	// end inline asm
	add.s64 	%rd2408, %rd2402, 48;
	// begin inline asm
	{ atom.add.f64 %fd6331,[%rd2408],%fd10515; }

	// end inline asm
	add.s64 	%rd2409, %rd2402, 56;
	// begin inline asm
	{ atom.add.f64 %fd6333,[%rd2409],%fd10514; }

	// end inline asm
	add.s64 	%rd2410, %rd2402, 64;
	// begin inline asm
	{ atom.add.f64 %fd6335,[%rd2410],%fd10513; }

	// end inline asm

$L__BB1_481:
	ld.param.u64 	%rd305, [%rd25+8];
	ld.param.u32 	%r906, [%rd25+32];
	ld.param.u32 	%r907, [%rd25+60];
	setp.le.s32 	%p811, %r907, %r905;
	selp.u16 	%rs196, 1, 0, %p811;
	or.b16  	%rs197, %rs196, %rs16;
	setp.eq.s16 	%p812, %rs197, 0;
	mov.f64 	%fd11233, 0d0000000000000000;
	mov.f64 	%fd11234, 0d0000000000000000;
	mov.f64 	%fd11235, 0d0000000000000000;
	mov.f64 	%fd11236, 0d0000000000000000;
	mov.f64 	%fd11237, 0d0000000000000000;
	mov.f64 	%fd11238, 0d0000000000000000;
	mov.f64 	%fd11239, 0d0000000000000000;
	mov.f64 	%fd11240, 0d0000000000000000;
	mov.f64 	%fd11241, 0d0000000000000000;
	@%p812 bra 	$L__BB1_483;

	add.s32 	%r1978, %r860, 15;
	st.local.v2.u32 	[%rd26], {%r1978, %r907};
	mov.u64 	%rd2412, $str;
	cvta.global.u64 	%rd2413, %rd2412;
	{ // callseq 460, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2413;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1591, [retval0+0];
	} // callseq 460
	bra.uni 	$L__BB1_485;

$L__BB1_483:
	setp.eq.s64 	%p813, %rd305, 0;
	@%p813 bra 	$L__BB1_485;

	cvta.to.global.u64 	%rd2415, %rd305;
	mul.wide.s32 	%rd2416, %r906, %r905;
	add.s64 	%rd2417, %rd2415, %rd2416;
	ld.global.f64 	%fd6355, [%rd2417];
	add.f64 	%fd11233, %fd6355, 0d0000000000000000;
	ld.global.f64 	%fd6356, [%rd2417+8];
	add.f64 	%fd11234, %fd6356, 0d0000000000000000;
	ld.global.f64 	%fd6357, [%rd2417+16];
	add.f64 	%fd11235, %fd6357, 0d0000000000000000;
	ld.global.f64 	%fd6358, [%rd2417+24];
	add.f64 	%fd11236, %fd6358, 0d0000000000000000;
	ld.global.f64 	%fd6359, [%rd2417+32];
	add.f64 	%fd11237, %fd6359, 0d0000000000000000;
	ld.global.f64 	%fd6360, [%rd2417+40];
	add.f64 	%fd11238, %fd6360, 0d0000000000000000;
	ld.global.f64 	%fd6361, [%rd2417+48];
	add.f64 	%fd11239, %fd6361, 0d0000000000000000;
	ld.global.f64 	%fd6362, [%rd2417+56];
	add.f64 	%fd11240, %fd6362, 0d0000000000000000;
	ld.global.f64 	%fd6363, [%rd2417+64];
	add.f64 	%fd11241, %fd6363, 0d0000000000000000;

$L__BB1_485:
	add.f64 	%fd2035, %fd11241, 0d0000000000000000;
	add.f64 	%fd2036, %fd11240, 0d0000000000000000;
	add.f64 	%fd2037, %fd11239, 0d0000000000000000;
	add.f64 	%fd2038, %fd11238, 0d0000000000000000;
	add.f64 	%fd2039, %fd11237, 0d0000000000000000;
	add.f64 	%fd2040, %fd11236, 0d0000000000000000;
	add.f64 	%fd2041, %fd11235, 0d0000000000000000;
	add.f64 	%fd2042, %fd11234, 0d0000000000000000;
	add.f64 	%fd2043, %fd11233, 0d0000000000000000;
	ld.param.u64 	%rd306, [%rd25+8];
	ld.param.u32 	%r908, [%rd25+32];
	ld.param.u32 	%r909, [%rd25+60];
	setp.le.s32 	%p814, %r909, %r902;
	selp.u16 	%rs198, 1, 0, %p814;
	or.b16  	%rs199, %rs198, %rs15;
	setp.eq.s16 	%p815, %rs199, 0;
	mov.f64 	%fd11242, 0d0000000000000000;
	mov.f64 	%fd11243, 0d0000000000000000;
	mov.f64 	%fd11244, 0d0000000000000000;
	mov.f64 	%fd11245, 0d0000000000000000;
	mov.f64 	%fd11246, 0d0000000000000000;
	mov.f64 	%fd11247, 0d0000000000000000;
	mov.f64 	%fd11248, 0d0000000000000000;
	mov.f64 	%fd11249, 0d0000000000000000;
	mov.f64 	%fd11250, 0d0000000000000000;
	@%p815 bra 	$L__BB1_487;

	add.s32 	%r1979, %r860, 14;
	st.local.v2.u32 	[%rd26], {%r1979, %r909};
	mov.u64 	%rd2418, $str;
	cvta.global.u64 	%rd2419, %rd2418;
	{ // callseq 461, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2419;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1592, [retval0+0];
	} // callseq 461
	bra.uni 	$L__BB1_489;

$L__BB1_487:
	setp.eq.s64 	%p816, %rd306, 0;
	@%p816 bra 	$L__BB1_489;

	cvta.to.global.u64 	%rd2421, %rd306;
	mul.wide.s32 	%rd2422, %r908, %r902;
	add.s64 	%rd2423, %rd2421, %rd2422;
	ld.global.f64 	%fd6382, [%rd2423];
	add.f64 	%fd11242, %fd6382, 0d0000000000000000;
	ld.global.f64 	%fd6383, [%rd2423+8];
	add.f64 	%fd11243, %fd6383, 0d0000000000000000;
	ld.global.f64 	%fd6384, [%rd2423+16];
	add.f64 	%fd11244, %fd6384, 0d0000000000000000;
	ld.global.f64 	%fd6385, [%rd2423+24];
	add.f64 	%fd11245, %fd6385, 0d0000000000000000;
	ld.global.f64 	%fd6386, [%rd2423+32];
	add.f64 	%fd11246, %fd6386, 0d0000000000000000;
	ld.global.f64 	%fd6387, [%rd2423+40];
	add.f64 	%fd11247, %fd6387, 0d0000000000000000;
	ld.global.f64 	%fd6388, [%rd2423+48];
	add.f64 	%fd11248, %fd6388, 0d0000000000000000;
	ld.global.f64 	%fd6389, [%rd2423+56];
	add.f64 	%fd11249, %fd6389, 0d0000000000000000;
	ld.global.f64 	%fd6390, [%rd2423+64];
	add.f64 	%fd11250, %fd6390, 0d0000000000000000;

$L__BB1_489:
	add.f64 	%fd2062, %fd11250, 0d0000000000000000;
	add.f64 	%fd2063, %fd11249, 0d0000000000000000;
	add.f64 	%fd2064, %fd11248, 0d0000000000000000;
	add.f64 	%fd2065, %fd11247, 0d0000000000000000;
	add.f64 	%fd2066, %fd11246, 0d0000000000000000;
	add.f64 	%fd2067, %fd11245, 0d0000000000000000;
	add.f64 	%fd2068, %fd11244, 0d0000000000000000;
	add.f64 	%fd2069, %fd11243, 0d0000000000000000;
	add.f64 	%fd2070, %fd11242, 0d0000000000000000;
	ld.param.u64 	%rd307, [%rd25+8];
	ld.param.u32 	%r910, [%rd25+32];
	ld.param.u32 	%r911, [%rd25+60];
	setp.le.s32 	%p817, %r911, %r899;
	selp.u16 	%rs200, 1, 0, %p817;
	or.b16  	%rs201, %rs200, %rs14;
	setp.eq.s16 	%p818, %rs201, 0;
	mov.f64 	%fd11251, 0d0000000000000000;
	mov.f64 	%fd11252, 0d0000000000000000;
	mov.f64 	%fd11253, 0d0000000000000000;
	mov.f64 	%fd11254, 0d0000000000000000;
	mov.f64 	%fd11255, 0d0000000000000000;
	mov.f64 	%fd11256, 0d0000000000000000;
	mov.f64 	%fd11257, 0d0000000000000000;
	mov.f64 	%fd11258, 0d0000000000000000;
	mov.f64 	%fd11259, 0d0000000000000000;
	@%p818 bra 	$L__BB1_491;

	add.s32 	%r1980, %r860, 13;
	st.local.v2.u32 	[%rd26], {%r1980, %r911};
	mov.u64 	%rd2424, $str;
	cvta.global.u64 	%rd2425, %rd2424;
	{ // callseq 462, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2425;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1593, [retval0+0];
	} // callseq 462
	bra.uni 	$L__BB1_493;

$L__BB1_491:
	setp.eq.s64 	%p819, %rd307, 0;
	@%p819 bra 	$L__BB1_493;

	cvta.to.global.u64 	%rd2427, %rd307;
	mul.wide.s32 	%rd2428, %r910, %r899;
	add.s64 	%rd2429, %rd2427, %rd2428;
	ld.global.f64 	%fd6409, [%rd2429];
	add.f64 	%fd11251, %fd6409, 0d0000000000000000;
	ld.global.f64 	%fd6410, [%rd2429+8];
	add.f64 	%fd11252, %fd6410, 0d0000000000000000;
	ld.global.f64 	%fd6411, [%rd2429+16];
	add.f64 	%fd11253, %fd6411, 0d0000000000000000;
	ld.global.f64 	%fd6412, [%rd2429+24];
	add.f64 	%fd11254, %fd6412, 0d0000000000000000;
	ld.global.f64 	%fd6413, [%rd2429+32];
	add.f64 	%fd11255, %fd6413, 0d0000000000000000;
	ld.global.f64 	%fd6414, [%rd2429+40];
	add.f64 	%fd11256, %fd6414, 0d0000000000000000;
	ld.global.f64 	%fd6415, [%rd2429+48];
	add.f64 	%fd11257, %fd6415, 0d0000000000000000;
	ld.global.f64 	%fd6416, [%rd2429+56];
	add.f64 	%fd11258, %fd6416, 0d0000000000000000;
	ld.global.f64 	%fd6417, [%rd2429+64];
	add.f64 	%fd11259, %fd6417, 0d0000000000000000;

$L__BB1_493:
	add.f64 	%fd2089, %fd11259, 0d0000000000000000;
	add.f64 	%fd2090, %fd11258, 0d0000000000000000;
	add.f64 	%fd2091, %fd11257, 0d0000000000000000;
	add.f64 	%fd2092, %fd11256, 0d0000000000000000;
	add.f64 	%fd2093, %fd11255, 0d0000000000000000;
	add.f64 	%fd2094, %fd11254, 0d0000000000000000;
	add.f64 	%fd2095, %fd11253, 0d0000000000000000;
	add.f64 	%fd2096, %fd11252, 0d0000000000000000;
	add.f64 	%fd2097, %fd11251, 0d0000000000000000;
	ld.param.u64 	%rd308, [%rd25+8];
	ld.param.u32 	%r912, [%rd25+32];
	ld.param.u32 	%r913, [%rd25+60];
	setp.le.s32 	%p820, %r913, %r896;
	selp.u16 	%rs202, 1, 0, %p820;
	or.b16  	%rs203, %rs202, %rs13;
	setp.eq.s16 	%p821, %rs203, 0;
	mov.f64 	%fd11260, 0d0000000000000000;
	mov.f64 	%fd11261, 0d0000000000000000;
	mov.f64 	%fd11262, 0d0000000000000000;
	mov.f64 	%fd11263, 0d0000000000000000;
	mov.f64 	%fd11264, 0d0000000000000000;
	mov.f64 	%fd11265, 0d0000000000000000;
	mov.f64 	%fd11266, 0d0000000000000000;
	mov.f64 	%fd11267, 0d0000000000000000;
	mov.f64 	%fd11268, 0d0000000000000000;
	@%p821 bra 	$L__BB1_495;

	add.s32 	%r1981, %r860, 12;
	st.local.v2.u32 	[%rd26], {%r1981, %r913};
	mov.u64 	%rd2430, $str;
	cvta.global.u64 	%rd2431, %rd2430;
	{ // callseq 463, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2431;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1594, [retval0+0];
	} // callseq 463
	bra.uni 	$L__BB1_497;

$L__BB1_495:
	setp.eq.s64 	%p822, %rd308, 0;
	@%p822 bra 	$L__BB1_497;

	cvta.to.global.u64 	%rd2433, %rd308;
	mul.wide.s32 	%rd2434, %r912, %r896;
	add.s64 	%rd2435, %rd2433, %rd2434;
	ld.global.f64 	%fd6436, [%rd2435];
	add.f64 	%fd11260, %fd6436, 0d0000000000000000;
	ld.global.f64 	%fd6437, [%rd2435+8];
	add.f64 	%fd11261, %fd6437, 0d0000000000000000;
	ld.global.f64 	%fd6438, [%rd2435+16];
	add.f64 	%fd11262, %fd6438, 0d0000000000000000;
	ld.global.f64 	%fd6439, [%rd2435+24];
	add.f64 	%fd11263, %fd6439, 0d0000000000000000;
	ld.global.f64 	%fd6440, [%rd2435+32];
	add.f64 	%fd11264, %fd6440, 0d0000000000000000;
	ld.global.f64 	%fd6441, [%rd2435+40];
	add.f64 	%fd11265, %fd6441, 0d0000000000000000;
	ld.global.f64 	%fd6442, [%rd2435+48];
	add.f64 	%fd11266, %fd6442, 0d0000000000000000;
	ld.global.f64 	%fd6443, [%rd2435+56];
	add.f64 	%fd11267, %fd6443, 0d0000000000000000;
	ld.global.f64 	%fd6444, [%rd2435+64];
	add.f64 	%fd11268, %fd6444, 0d0000000000000000;

$L__BB1_497:
	add.f64 	%fd2116, %fd11268, 0d0000000000000000;
	add.f64 	%fd2117, %fd11267, 0d0000000000000000;
	add.f64 	%fd2118, %fd11266, 0d0000000000000000;
	add.f64 	%fd2119, %fd11265, 0d0000000000000000;
	add.f64 	%fd2120, %fd11264, 0d0000000000000000;
	add.f64 	%fd2121, %fd11263, 0d0000000000000000;
	add.f64 	%fd2122, %fd11262, 0d0000000000000000;
	add.f64 	%fd2123, %fd11261, 0d0000000000000000;
	add.f64 	%fd2124, %fd11260, 0d0000000000000000;
	ld.param.u64 	%rd309, [%rd25+8];
	ld.param.u32 	%r914, [%rd25+32];
	ld.param.u32 	%r915, [%rd25+60];
	setp.le.s32 	%p823, %r915, %r893;
	selp.u16 	%rs204, 1, 0, %p823;
	or.b16  	%rs205, %rs204, %rs12;
	setp.eq.s16 	%p824, %rs205, 0;
	mov.f64 	%fd11269, 0d0000000000000000;
	mov.f64 	%fd11270, 0d0000000000000000;
	mov.f64 	%fd11271, 0d0000000000000000;
	mov.f64 	%fd11272, 0d0000000000000000;
	mov.f64 	%fd11273, 0d0000000000000000;
	mov.f64 	%fd11274, 0d0000000000000000;
	mov.f64 	%fd11275, 0d0000000000000000;
	mov.f64 	%fd11276, 0d0000000000000000;
	mov.f64 	%fd11277, 0d0000000000000000;
	@%p824 bra 	$L__BB1_499;

	add.s32 	%r1982, %r860, 11;
	st.local.v2.u32 	[%rd26], {%r1982, %r915};
	mov.u64 	%rd2436, $str;
	cvta.global.u64 	%rd2437, %rd2436;
	{ // callseq 464, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2437;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1595, [retval0+0];
	} // callseq 464
	bra.uni 	$L__BB1_501;

$L__BB1_499:
	setp.eq.s64 	%p825, %rd309, 0;
	@%p825 bra 	$L__BB1_501;

	cvta.to.global.u64 	%rd2439, %rd309;
	mul.wide.s32 	%rd2440, %r914, %r893;
	add.s64 	%rd2441, %rd2439, %rd2440;
	ld.global.f64 	%fd6463, [%rd2441];
	add.f64 	%fd11269, %fd6463, 0d0000000000000000;
	ld.global.f64 	%fd6464, [%rd2441+8];
	add.f64 	%fd11270, %fd6464, 0d0000000000000000;
	ld.global.f64 	%fd6465, [%rd2441+16];
	add.f64 	%fd11271, %fd6465, 0d0000000000000000;
	ld.global.f64 	%fd6466, [%rd2441+24];
	add.f64 	%fd11272, %fd6466, 0d0000000000000000;
	ld.global.f64 	%fd6467, [%rd2441+32];
	add.f64 	%fd11273, %fd6467, 0d0000000000000000;
	ld.global.f64 	%fd6468, [%rd2441+40];
	add.f64 	%fd11274, %fd6468, 0d0000000000000000;
	ld.global.f64 	%fd6469, [%rd2441+48];
	add.f64 	%fd11275, %fd6469, 0d0000000000000000;
	ld.global.f64 	%fd6470, [%rd2441+56];
	add.f64 	%fd11276, %fd6470, 0d0000000000000000;
	ld.global.f64 	%fd6471, [%rd2441+64];
	add.f64 	%fd11277, %fd6471, 0d0000000000000000;

$L__BB1_501:
	add.f64 	%fd2143, %fd11277, 0d0000000000000000;
	add.f64 	%fd2144, %fd11276, 0d0000000000000000;
	add.f64 	%fd2145, %fd11275, 0d0000000000000000;
	add.f64 	%fd2146, %fd11274, 0d0000000000000000;
	add.f64 	%fd2147, %fd11273, 0d0000000000000000;
	add.f64 	%fd2148, %fd11272, 0d0000000000000000;
	add.f64 	%fd2149, %fd11271, 0d0000000000000000;
	add.f64 	%fd2150, %fd11270, 0d0000000000000000;
	add.f64 	%fd2151, %fd11269, 0d0000000000000000;
	ld.param.u64 	%rd310, [%rd25+8];
	ld.param.u32 	%r916, [%rd25+32];
	ld.param.u32 	%r917, [%rd25+60];
	setp.le.s32 	%p826, %r917, %r890;
	selp.u16 	%rs206, 1, 0, %p826;
	or.b16  	%rs207, %rs206, %rs11;
	setp.eq.s16 	%p827, %rs207, 0;
	mov.f64 	%fd11278, 0d0000000000000000;
	mov.f64 	%fd11279, 0d0000000000000000;
	mov.f64 	%fd11280, 0d0000000000000000;
	mov.f64 	%fd11281, 0d0000000000000000;
	mov.f64 	%fd11282, 0d0000000000000000;
	mov.f64 	%fd11283, 0d0000000000000000;
	mov.f64 	%fd11284, 0d0000000000000000;
	mov.f64 	%fd11285, 0d0000000000000000;
	mov.f64 	%fd11286, 0d0000000000000000;
	@%p827 bra 	$L__BB1_503;

	add.s32 	%r1983, %r860, 10;
	st.local.v2.u32 	[%rd26], {%r1983, %r917};
	mov.u64 	%rd2442, $str;
	cvta.global.u64 	%rd2443, %rd2442;
	{ // callseq 465, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2443;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1596, [retval0+0];
	} // callseq 465
	bra.uni 	$L__BB1_505;

$L__BB1_503:
	setp.eq.s64 	%p828, %rd310, 0;
	@%p828 bra 	$L__BB1_505;

	cvta.to.global.u64 	%rd2445, %rd310;
	mul.wide.s32 	%rd2446, %r916, %r890;
	add.s64 	%rd2447, %rd2445, %rd2446;
	ld.global.f64 	%fd6490, [%rd2447];
	add.f64 	%fd11278, %fd6490, 0d0000000000000000;
	ld.global.f64 	%fd6491, [%rd2447+8];
	add.f64 	%fd11279, %fd6491, 0d0000000000000000;
	ld.global.f64 	%fd6492, [%rd2447+16];
	add.f64 	%fd11280, %fd6492, 0d0000000000000000;
	ld.global.f64 	%fd6493, [%rd2447+24];
	add.f64 	%fd11281, %fd6493, 0d0000000000000000;
	ld.global.f64 	%fd6494, [%rd2447+32];
	add.f64 	%fd11282, %fd6494, 0d0000000000000000;
	ld.global.f64 	%fd6495, [%rd2447+40];
	add.f64 	%fd11283, %fd6495, 0d0000000000000000;
	ld.global.f64 	%fd6496, [%rd2447+48];
	add.f64 	%fd11284, %fd6496, 0d0000000000000000;
	ld.global.f64 	%fd6497, [%rd2447+56];
	add.f64 	%fd11285, %fd6497, 0d0000000000000000;
	ld.global.f64 	%fd6498, [%rd2447+64];
	add.f64 	%fd11286, %fd6498, 0d0000000000000000;

$L__BB1_505:
	add.f64 	%fd2170, %fd11286, 0d0000000000000000;
	add.f64 	%fd2171, %fd11285, 0d0000000000000000;
	add.f64 	%fd2172, %fd11284, 0d0000000000000000;
	add.f64 	%fd2173, %fd11283, 0d0000000000000000;
	add.f64 	%fd2174, %fd11282, 0d0000000000000000;
	add.f64 	%fd2175, %fd11281, 0d0000000000000000;
	add.f64 	%fd2176, %fd11280, 0d0000000000000000;
	add.f64 	%fd2177, %fd11279, 0d0000000000000000;
	add.f64 	%fd2178, %fd11278, 0d0000000000000000;
	ld.param.u64 	%rd311, [%rd25+8];
	ld.param.u32 	%r918, [%rd25+32];
	ld.param.u32 	%r919, [%rd25+60];
	setp.le.s32 	%p829, %r919, %r887;
	selp.u16 	%rs208, 1, 0, %p829;
	or.b16  	%rs209, %rs208, %rs10;
	setp.eq.s16 	%p830, %rs209, 0;
	mov.f64 	%fd11287, 0d0000000000000000;
	mov.f64 	%fd11288, 0d0000000000000000;
	mov.f64 	%fd11289, 0d0000000000000000;
	mov.f64 	%fd11290, 0d0000000000000000;
	mov.f64 	%fd11291, 0d0000000000000000;
	mov.f64 	%fd11292, 0d0000000000000000;
	mov.f64 	%fd11293, 0d0000000000000000;
	mov.f64 	%fd11294, 0d0000000000000000;
	mov.f64 	%fd11295, 0d0000000000000000;
	@%p830 bra 	$L__BB1_507;

	add.s32 	%r1984, %r860, 9;
	st.local.v2.u32 	[%rd26], {%r1984, %r919};
	mov.u64 	%rd2448, $str;
	cvta.global.u64 	%rd2449, %rd2448;
	{ // callseq 466, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2449;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1597, [retval0+0];
	} // callseq 466
	bra.uni 	$L__BB1_509;

$L__BB1_507:
	setp.eq.s64 	%p831, %rd311, 0;
	@%p831 bra 	$L__BB1_509;

	cvta.to.global.u64 	%rd2451, %rd311;
	mul.wide.s32 	%rd2452, %r918, %r887;
	add.s64 	%rd2453, %rd2451, %rd2452;
	ld.global.f64 	%fd6517, [%rd2453];
	add.f64 	%fd11287, %fd6517, 0d0000000000000000;
	ld.global.f64 	%fd6518, [%rd2453+8];
	add.f64 	%fd11288, %fd6518, 0d0000000000000000;
	ld.global.f64 	%fd6519, [%rd2453+16];
	add.f64 	%fd11289, %fd6519, 0d0000000000000000;
	ld.global.f64 	%fd6520, [%rd2453+24];
	add.f64 	%fd11290, %fd6520, 0d0000000000000000;
	ld.global.f64 	%fd6521, [%rd2453+32];
	add.f64 	%fd11291, %fd6521, 0d0000000000000000;
	ld.global.f64 	%fd6522, [%rd2453+40];
	add.f64 	%fd11292, %fd6522, 0d0000000000000000;
	ld.global.f64 	%fd6523, [%rd2453+48];
	add.f64 	%fd11293, %fd6523, 0d0000000000000000;
	ld.global.f64 	%fd6524, [%rd2453+56];
	add.f64 	%fd11294, %fd6524, 0d0000000000000000;
	ld.global.f64 	%fd6525, [%rd2453+64];
	add.f64 	%fd11295, %fd6525, 0d0000000000000000;

$L__BB1_509:
	add.f64 	%fd2197, %fd11295, 0d0000000000000000;
	add.f64 	%fd2198, %fd11294, 0d0000000000000000;
	add.f64 	%fd2199, %fd11293, 0d0000000000000000;
	add.f64 	%fd2200, %fd11292, 0d0000000000000000;
	add.f64 	%fd2201, %fd11291, 0d0000000000000000;
	add.f64 	%fd2202, %fd11290, 0d0000000000000000;
	add.f64 	%fd2203, %fd11289, 0d0000000000000000;
	add.f64 	%fd2204, %fd11288, 0d0000000000000000;
	add.f64 	%fd2205, %fd11287, 0d0000000000000000;
	ld.param.u64 	%rd312, [%rd25+8];
	ld.param.u32 	%r920, [%rd25+32];
	ld.param.u32 	%r921, [%rd25+60];
	setp.le.s32 	%p832, %r921, %r884;
	selp.u16 	%rs210, 1, 0, %p832;
	or.b16  	%rs211, %rs210, %rs9;
	setp.eq.s16 	%p833, %rs211, 0;
	mov.f64 	%fd11296, 0d0000000000000000;
	mov.f64 	%fd11297, 0d0000000000000000;
	mov.f64 	%fd11298, 0d0000000000000000;
	mov.f64 	%fd11299, 0d0000000000000000;
	mov.f64 	%fd11300, 0d0000000000000000;
	mov.f64 	%fd11301, 0d0000000000000000;
	mov.f64 	%fd11302, 0d0000000000000000;
	mov.f64 	%fd11303, 0d0000000000000000;
	mov.f64 	%fd11304, 0d0000000000000000;
	@%p833 bra 	$L__BB1_511;

	add.s32 	%r1985, %r860, 8;
	st.local.v2.u32 	[%rd26], {%r1985, %r921};
	mov.u64 	%rd2454, $str;
	cvta.global.u64 	%rd2455, %rd2454;
	{ // callseq 467, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2455;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1598, [retval0+0];
	} // callseq 467
	bra.uni 	$L__BB1_513;

$L__BB1_511:
	setp.eq.s64 	%p834, %rd312, 0;
	@%p834 bra 	$L__BB1_513;

	cvta.to.global.u64 	%rd2457, %rd312;
	mul.wide.s32 	%rd2458, %r920, %r884;
	add.s64 	%rd2459, %rd2457, %rd2458;
	ld.global.f64 	%fd6544, [%rd2459];
	add.f64 	%fd11296, %fd6544, 0d0000000000000000;
	ld.global.f64 	%fd6545, [%rd2459+8];
	add.f64 	%fd11297, %fd6545, 0d0000000000000000;
	ld.global.f64 	%fd6546, [%rd2459+16];
	add.f64 	%fd11298, %fd6546, 0d0000000000000000;
	ld.global.f64 	%fd6547, [%rd2459+24];
	add.f64 	%fd11299, %fd6547, 0d0000000000000000;
	ld.global.f64 	%fd6548, [%rd2459+32];
	add.f64 	%fd11300, %fd6548, 0d0000000000000000;
	ld.global.f64 	%fd6549, [%rd2459+40];
	add.f64 	%fd11301, %fd6549, 0d0000000000000000;
	ld.global.f64 	%fd6550, [%rd2459+48];
	add.f64 	%fd11302, %fd6550, 0d0000000000000000;
	ld.global.f64 	%fd6551, [%rd2459+56];
	add.f64 	%fd11303, %fd6551, 0d0000000000000000;
	ld.global.f64 	%fd6552, [%rd2459+64];
	add.f64 	%fd11304, %fd6552, 0d0000000000000000;

$L__BB1_513:
	add.f64 	%fd2224, %fd11304, 0d0000000000000000;
	add.f64 	%fd2225, %fd11303, 0d0000000000000000;
	add.f64 	%fd2226, %fd11302, 0d0000000000000000;
	add.f64 	%fd2227, %fd11301, 0d0000000000000000;
	add.f64 	%fd2228, %fd11300, 0d0000000000000000;
	add.f64 	%fd2229, %fd11299, 0d0000000000000000;
	add.f64 	%fd2230, %fd11298, 0d0000000000000000;
	add.f64 	%fd2231, %fd11297, 0d0000000000000000;
	add.f64 	%fd2232, %fd11296, 0d0000000000000000;
	ld.param.u64 	%rd313, [%rd25+8];
	ld.param.u32 	%r922, [%rd25+32];
	ld.param.u32 	%r923, [%rd25+60];
	setp.le.s32 	%p835, %r923, %r881;
	selp.u16 	%rs212, 1, 0, %p835;
	or.b16  	%rs213, %rs212, %rs8;
	setp.eq.s16 	%p836, %rs213, 0;
	mov.f64 	%fd11305, 0d0000000000000000;
	mov.f64 	%fd11306, 0d0000000000000000;
	mov.f64 	%fd11307, 0d0000000000000000;
	mov.f64 	%fd11308, 0d0000000000000000;
	mov.f64 	%fd11309, 0d0000000000000000;
	mov.f64 	%fd11310, 0d0000000000000000;
	mov.f64 	%fd11311, 0d0000000000000000;
	mov.f64 	%fd11312, 0d0000000000000000;
	mov.f64 	%fd11313, 0d0000000000000000;
	@%p836 bra 	$L__BB1_515;

	add.s32 	%r1986, %r860, 7;
	st.local.v2.u32 	[%rd26], {%r1986, %r923};
	mov.u64 	%rd2460, $str;
	cvta.global.u64 	%rd2461, %rd2460;
	{ // callseq 468, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2461;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1599, [retval0+0];
	} // callseq 468
	bra.uni 	$L__BB1_517;

$L__BB1_515:
	setp.eq.s64 	%p837, %rd313, 0;
	@%p837 bra 	$L__BB1_517;

	cvta.to.global.u64 	%rd2463, %rd313;
	mul.wide.s32 	%rd2464, %r922, %r881;
	add.s64 	%rd2465, %rd2463, %rd2464;
	ld.global.f64 	%fd6571, [%rd2465];
	add.f64 	%fd11305, %fd6571, 0d0000000000000000;
	ld.global.f64 	%fd6572, [%rd2465+8];
	add.f64 	%fd11306, %fd6572, 0d0000000000000000;
	ld.global.f64 	%fd6573, [%rd2465+16];
	add.f64 	%fd11307, %fd6573, 0d0000000000000000;
	ld.global.f64 	%fd6574, [%rd2465+24];
	add.f64 	%fd11308, %fd6574, 0d0000000000000000;
	ld.global.f64 	%fd6575, [%rd2465+32];
	add.f64 	%fd11309, %fd6575, 0d0000000000000000;
	ld.global.f64 	%fd6576, [%rd2465+40];
	add.f64 	%fd11310, %fd6576, 0d0000000000000000;
	ld.global.f64 	%fd6577, [%rd2465+48];
	add.f64 	%fd11311, %fd6577, 0d0000000000000000;
	ld.global.f64 	%fd6578, [%rd2465+56];
	add.f64 	%fd11312, %fd6578, 0d0000000000000000;
	ld.global.f64 	%fd6579, [%rd2465+64];
	add.f64 	%fd11313, %fd6579, 0d0000000000000000;

$L__BB1_517:
	add.f64 	%fd2251, %fd11313, 0d0000000000000000;
	add.f64 	%fd2252, %fd11312, 0d0000000000000000;
	add.f64 	%fd2253, %fd11311, 0d0000000000000000;
	add.f64 	%fd2254, %fd11310, 0d0000000000000000;
	add.f64 	%fd2255, %fd11309, 0d0000000000000000;
	add.f64 	%fd2256, %fd11308, 0d0000000000000000;
	add.f64 	%fd2257, %fd11307, 0d0000000000000000;
	add.f64 	%fd2258, %fd11306, 0d0000000000000000;
	add.f64 	%fd2259, %fd11305, 0d0000000000000000;
	ld.param.u64 	%rd314, [%rd25+8];
	ld.param.u32 	%r924, [%rd25+32];
	ld.param.u32 	%r925, [%rd25+60];
	setp.le.s32 	%p838, %r925, %r878;
	selp.u16 	%rs214, 1, 0, %p838;
	or.b16  	%rs215, %rs214, %rs7;
	setp.eq.s16 	%p839, %rs215, 0;
	mov.f64 	%fd11314, 0d0000000000000000;
	mov.f64 	%fd11315, 0d0000000000000000;
	mov.f64 	%fd11316, 0d0000000000000000;
	mov.f64 	%fd11317, 0d0000000000000000;
	mov.f64 	%fd11318, 0d0000000000000000;
	mov.f64 	%fd11319, 0d0000000000000000;
	mov.f64 	%fd11320, 0d0000000000000000;
	mov.f64 	%fd11321, 0d0000000000000000;
	mov.f64 	%fd11322, 0d0000000000000000;
	@%p839 bra 	$L__BB1_519;

	add.s32 	%r1987, %r860, 6;
	st.local.v2.u32 	[%rd26], {%r1987, %r925};
	mov.u64 	%rd2466, $str;
	cvta.global.u64 	%rd2467, %rd2466;
	{ // callseq 469, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2467;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1600, [retval0+0];
	} // callseq 469
	bra.uni 	$L__BB1_521;

$L__BB1_519:
	setp.eq.s64 	%p840, %rd314, 0;
	@%p840 bra 	$L__BB1_521;

	cvta.to.global.u64 	%rd2469, %rd314;
	mul.wide.s32 	%rd2470, %r924, %r878;
	add.s64 	%rd2471, %rd2469, %rd2470;
	ld.global.f64 	%fd6598, [%rd2471];
	add.f64 	%fd11314, %fd6598, 0d0000000000000000;
	ld.global.f64 	%fd6599, [%rd2471+8];
	add.f64 	%fd11315, %fd6599, 0d0000000000000000;
	ld.global.f64 	%fd6600, [%rd2471+16];
	add.f64 	%fd11316, %fd6600, 0d0000000000000000;
	ld.global.f64 	%fd6601, [%rd2471+24];
	add.f64 	%fd11317, %fd6601, 0d0000000000000000;
	ld.global.f64 	%fd6602, [%rd2471+32];
	add.f64 	%fd11318, %fd6602, 0d0000000000000000;
	ld.global.f64 	%fd6603, [%rd2471+40];
	add.f64 	%fd11319, %fd6603, 0d0000000000000000;
	ld.global.f64 	%fd6604, [%rd2471+48];
	add.f64 	%fd11320, %fd6604, 0d0000000000000000;
	ld.global.f64 	%fd6605, [%rd2471+56];
	add.f64 	%fd11321, %fd6605, 0d0000000000000000;
	ld.global.f64 	%fd6606, [%rd2471+64];
	add.f64 	%fd11322, %fd6606, 0d0000000000000000;

$L__BB1_521:
	add.f64 	%fd2278, %fd11322, 0d0000000000000000;
	add.f64 	%fd2279, %fd11321, 0d0000000000000000;
	add.f64 	%fd2280, %fd11320, 0d0000000000000000;
	add.f64 	%fd2281, %fd11319, 0d0000000000000000;
	add.f64 	%fd2282, %fd11318, 0d0000000000000000;
	add.f64 	%fd2283, %fd11317, 0d0000000000000000;
	add.f64 	%fd2284, %fd11316, 0d0000000000000000;
	add.f64 	%fd2285, %fd11315, 0d0000000000000000;
	add.f64 	%fd2286, %fd11314, 0d0000000000000000;
	ld.param.u64 	%rd315, [%rd25+8];
	ld.param.u32 	%r926, [%rd25+32];
	ld.param.u32 	%r927, [%rd25+60];
	setp.le.s32 	%p841, %r927, %r875;
	selp.u16 	%rs216, 1, 0, %p841;
	or.b16  	%rs217, %rs216, %rs6;
	setp.eq.s16 	%p842, %rs217, 0;
	mov.f64 	%fd11323, 0d0000000000000000;
	mov.f64 	%fd11324, 0d0000000000000000;
	mov.f64 	%fd11325, 0d0000000000000000;
	mov.f64 	%fd11326, 0d0000000000000000;
	mov.f64 	%fd11327, 0d0000000000000000;
	mov.f64 	%fd11328, 0d0000000000000000;
	mov.f64 	%fd11329, 0d0000000000000000;
	mov.f64 	%fd11330, 0d0000000000000000;
	mov.f64 	%fd11331, 0d0000000000000000;
	@%p842 bra 	$L__BB1_523;

	add.s32 	%r1988, %r860, 5;
	st.local.v2.u32 	[%rd26], {%r1988, %r927};
	mov.u64 	%rd2472, $str;
	cvta.global.u64 	%rd2473, %rd2472;
	{ // callseq 470, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2473;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1601, [retval0+0];
	} // callseq 470
	bra.uni 	$L__BB1_525;

$L__BB1_523:
	setp.eq.s64 	%p843, %rd315, 0;
	@%p843 bra 	$L__BB1_525;

	cvta.to.global.u64 	%rd2475, %rd315;
	mul.wide.s32 	%rd2476, %r926, %r875;
	add.s64 	%rd2477, %rd2475, %rd2476;
	ld.global.f64 	%fd6625, [%rd2477];
	add.f64 	%fd11323, %fd6625, 0d0000000000000000;
	ld.global.f64 	%fd6626, [%rd2477+8];
	add.f64 	%fd11324, %fd6626, 0d0000000000000000;
	ld.global.f64 	%fd6627, [%rd2477+16];
	add.f64 	%fd11325, %fd6627, 0d0000000000000000;
	ld.global.f64 	%fd6628, [%rd2477+24];
	add.f64 	%fd11326, %fd6628, 0d0000000000000000;
	ld.global.f64 	%fd6629, [%rd2477+32];
	add.f64 	%fd11327, %fd6629, 0d0000000000000000;
	ld.global.f64 	%fd6630, [%rd2477+40];
	add.f64 	%fd11328, %fd6630, 0d0000000000000000;
	ld.global.f64 	%fd6631, [%rd2477+48];
	add.f64 	%fd11329, %fd6631, 0d0000000000000000;
	ld.global.f64 	%fd6632, [%rd2477+56];
	add.f64 	%fd11330, %fd6632, 0d0000000000000000;
	ld.global.f64 	%fd6633, [%rd2477+64];
	add.f64 	%fd11331, %fd6633, 0d0000000000000000;

$L__BB1_525:
	add.f64 	%fd2305, %fd11331, 0d0000000000000000;
	add.f64 	%fd2306, %fd11330, 0d0000000000000000;
	add.f64 	%fd2307, %fd11329, 0d0000000000000000;
	add.f64 	%fd2308, %fd11328, 0d0000000000000000;
	add.f64 	%fd2309, %fd11327, 0d0000000000000000;
	add.f64 	%fd2310, %fd11326, 0d0000000000000000;
	add.f64 	%fd2311, %fd11325, 0d0000000000000000;
	add.f64 	%fd2312, %fd11324, 0d0000000000000000;
	add.f64 	%fd2313, %fd11323, 0d0000000000000000;
	ld.param.u64 	%rd316, [%rd25+8];
	ld.param.u32 	%r928, [%rd25+32];
	ld.param.u32 	%r929, [%rd25+60];
	setp.le.s32 	%p844, %r929, %r872;
	selp.u16 	%rs218, 1, 0, %p844;
	or.b16  	%rs219, %rs218, %rs5;
	setp.eq.s16 	%p845, %rs219, 0;
	mov.f64 	%fd11332, 0d0000000000000000;
	mov.f64 	%fd11333, 0d0000000000000000;
	mov.f64 	%fd11334, 0d0000000000000000;
	mov.f64 	%fd11335, 0d0000000000000000;
	mov.f64 	%fd11336, 0d0000000000000000;
	mov.f64 	%fd11337, 0d0000000000000000;
	mov.f64 	%fd11338, 0d0000000000000000;
	mov.f64 	%fd11339, 0d0000000000000000;
	mov.f64 	%fd11340, 0d0000000000000000;
	@%p845 bra 	$L__BB1_527;

	add.s32 	%r1989, %r860, 4;
	st.local.v2.u32 	[%rd26], {%r1989, %r929};
	mov.u64 	%rd2478, $str;
	cvta.global.u64 	%rd2479, %rd2478;
	{ // callseq 471, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2479;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1602, [retval0+0];
	} // callseq 471
	bra.uni 	$L__BB1_529;

$L__BB1_527:
	setp.eq.s64 	%p846, %rd316, 0;
	@%p846 bra 	$L__BB1_529;

	cvta.to.global.u64 	%rd2481, %rd316;
	mul.wide.s32 	%rd2482, %r928, %r872;
	add.s64 	%rd2483, %rd2481, %rd2482;
	ld.global.f64 	%fd6652, [%rd2483];
	add.f64 	%fd11332, %fd6652, 0d0000000000000000;
	ld.global.f64 	%fd6653, [%rd2483+8];
	add.f64 	%fd11333, %fd6653, 0d0000000000000000;
	ld.global.f64 	%fd6654, [%rd2483+16];
	add.f64 	%fd11334, %fd6654, 0d0000000000000000;
	ld.global.f64 	%fd6655, [%rd2483+24];
	add.f64 	%fd11335, %fd6655, 0d0000000000000000;
	ld.global.f64 	%fd6656, [%rd2483+32];
	add.f64 	%fd11336, %fd6656, 0d0000000000000000;
	ld.global.f64 	%fd6657, [%rd2483+40];
	add.f64 	%fd11337, %fd6657, 0d0000000000000000;
	ld.global.f64 	%fd6658, [%rd2483+48];
	add.f64 	%fd11338, %fd6658, 0d0000000000000000;
	ld.global.f64 	%fd6659, [%rd2483+56];
	add.f64 	%fd11339, %fd6659, 0d0000000000000000;
	ld.global.f64 	%fd6660, [%rd2483+64];
	add.f64 	%fd11340, %fd6660, 0d0000000000000000;

$L__BB1_529:
	add.f64 	%fd2332, %fd11340, 0d0000000000000000;
	add.f64 	%fd2333, %fd11339, 0d0000000000000000;
	add.f64 	%fd2334, %fd11338, 0d0000000000000000;
	add.f64 	%fd2335, %fd11337, 0d0000000000000000;
	add.f64 	%fd2336, %fd11336, 0d0000000000000000;
	add.f64 	%fd2337, %fd11335, 0d0000000000000000;
	add.f64 	%fd2338, %fd11334, 0d0000000000000000;
	add.f64 	%fd2339, %fd11333, 0d0000000000000000;
	add.f64 	%fd2340, %fd11332, 0d0000000000000000;
	ld.param.u64 	%rd317, [%rd25+8];
	ld.param.u32 	%r930, [%rd25+32];
	ld.param.u32 	%r931, [%rd25+60];
	setp.le.s32 	%p847, %r931, %r869;
	selp.u16 	%rs220, 1, 0, %p847;
	or.b16  	%rs221, %rs220, %rs4;
	setp.eq.s16 	%p848, %rs221, 0;
	mov.f64 	%fd11341, 0d0000000000000000;
	mov.f64 	%fd11342, 0d0000000000000000;
	mov.f64 	%fd11343, 0d0000000000000000;
	mov.f64 	%fd11344, 0d0000000000000000;
	mov.f64 	%fd11345, 0d0000000000000000;
	mov.f64 	%fd11346, 0d0000000000000000;
	mov.f64 	%fd11347, 0d0000000000000000;
	mov.f64 	%fd11348, 0d0000000000000000;
	mov.f64 	%fd11349, 0d0000000000000000;
	@%p848 bra 	$L__BB1_531;

	add.s32 	%r1990, %r860, 3;
	st.local.v2.u32 	[%rd26], {%r1990, %r931};
	mov.u64 	%rd2484, $str;
	cvta.global.u64 	%rd2485, %rd2484;
	{ // callseq 472, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2485;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1603, [retval0+0];
	} // callseq 472
	bra.uni 	$L__BB1_533;

$L__BB1_531:
	setp.eq.s64 	%p849, %rd317, 0;
	@%p849 bra 	$L__BB1_533;

	cvta.to.global.u64 	%rd2487, %rd317;
	mul.wide.s32 	%rd2488, %r930, %r869;
	add.s64 	%rd2489, %rd2487, %rd2488;
	ld.global.f64 	%fd6679, [%rd2489];
	add.f64 	%fd11341, %fd6679, 0d0000000000000000;
	ld.global.f64 	%fd6680, [%rd2489+8];
	add.f64 	%fd11342, %fd6680, 0d0000000000000000;
	ld.global.f64 	%fd6681, [%rd2489+16];
	add.f64 	%fd11343, %fd6681, 0d0000000000000000;
	ld.global.f64 	%fd6682, [%rd2489+24];
	add.f64 	%fd11344, %fd6682, 0d0000000000000000;
	ld.global.f64 	%fd6683, [%rd2489+32];
	add.f64 	%fd11345, %fd6683, 0d0000000000000000;
	ld.global.f64 	%fd6684, [%rd2489+40];
	add.f64 	%fd11346, %fd6684, 0d0000000000000000;
	ld.global.f64 	%fd6685, [%rd2489+48];
	add.f64 	%fd11347, %fd6685, 0d0000000000000000;
	ld.global.f64 	%fd6686, [%rd2489+56];
	add.f64 	%fd11348, %fd6686, 0d0000000000000000;
	ld.global.f64 	%fd6687, [%rd2489+64];
	add.f64 	%fd11349, %fd6687, 0d0000000000000000;

$L__BB1_533:
	add.f64 	%fd2359, %fd11349, 0d0000000000000000;
	add.f64 	%fd2360, %fd11348, 0d0000000000000000;
	add.f64 	%fd2361, %fd11347, 0d0000000000000000;
	add.f64 	%fd2362, %fd11346, 0d0000000000000000;
	add.f64 	%fd2363, %fd11345, 0d0000000000000000;
	add.f64 	%fd2364, %fd11344, 0d0000000000000000;
	add.f64 	%fd2365, %fd11343, 0d0000000000000000;
	add.f64 	%fd2366, %fd11342, 0d0000000000000000;
	add.f64 	%fd2367, %fd11341, 0d0000000000000000;
	ld.param.u64 	%rd318, [%rd25+8];
	ld.param.u32 	%r932, [%rd25+32];
	ld.param.u32 	%r933, [%rd25+60];
	setp.le.s32 	%p850, %r933, %r866;
	selp.u16 	%rs222, 1, 0, %p850;
	or.b16  	%rs223, %rs222, %rs3;
	setp.eq.s16 	%p851, %rs223, 0;
	mov.f64 	%fd11350, 0d0000000000000000;
	mov.f64 	%fd11351, 0d0000000000000000;
	mov.f64 	%fd11352, 0d0000000000000000;
	mov.f64 	%fd11353, 0d0000000000000000;
	mov.f64 	%fd11354, 0d0000000000000000;
	mov.f64 	%fd11355, 0d0000000000000000;
	mov.f64 	%fd11356, 0d0000000000000000;
	mov.f64 	%fd11357, 0d0000000000000000;
	mov.f64 	%fd11358, 0d0000000000000000;
	@%p851 bra 	$L__BB1_535;

	add.s32 	%r1991, %r860, 2;
	st.local.v2.u32 	[%rd26], {%r1991, %r933};
	mov.u64 	%rd2490, $str;
	cvta.global.u64 	%rd2491, %rd2490;
	{ // callseq 473, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2491;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1604, [retval0+0];
	} // callseq 473
	bra.uni 	$L__BB1_537;

$L__BB1_535:
	setp.eq.s64 	%p852, %rd318, 0;
	@%p852 bra 	$L__BB1_537;

	cvta.to.global.u64 	%rd2493, %rd318;
	mul.wide.s32 	%rd2494, %r932, %r866;
	add.s64 	%rd2495, %rd2493, %rd2494;
	ld.global.f64 	%fd6706, [%rd2495];
	add.f64 	%fd11350, %fd6706, 0d0000000000000000;
	ld.global.f64 	%fd6707, [%rd2495+8];
	add.f64 	%fd11351, %fd6707, 0d0000000000000000;
	ld.global.f64 	%fd6708, [%rd2495+16];
	add.f64 	%fd11352, %fd6708, 0d0000000000000000;
	ld.global.f64 	%fd6709, [%rd2495+24];
	add.f64 	%fd11353, %fd6709, 0d0000000000000000;
	ld.global.f64 	%fd6710, [%rd2495+32];
	add.f64 	%fd11354, %fd6710, 0d0000000000000000;
	ld.global.f64 	%fd6711, [%rd2495+40];
	add.f64 	%fd11355, %fd6711, 0d0000000000000000;
	ld.global.f64 	%fd6712, [%rd2495+48];
	add.f64 	%fd11356, %fd6712, 0d0000000000000000;
	ld.global.f64 	%fd6713, [%rd2495+56];
	add.f64 	%fd11357, %fd6713, 0d0000000000000000;
	ld.global.f64 	%fd6714, [%rd2495+64];
	add.f64 	%fd11358, %fd6714, 0d0000000000000000;

$L__BB1_537:
	add.f64 	%fd2386, %fd11358, 0d0000000000000000;
	add.f64 	%fd2387, %fd11357, 0d0000000000000000;
	add.f64 	%fd2388, %fd11356, 0d0000000000000000;
	add.f64 	%fd2389, %fd11355, 0d0000000000000000;
	add.f64 	%fd2390, %fd11354, 0d0000000000000000;
	add.f64 	%fd2391, %fd11353, 0d0000000000000000;
	add.f64 	%fd2392, %fd11352, 0d0000000000000000;
	add.f64 	%fd2393, %fd11351, 0d0000000000000000;
	add.f64 	%fd2394, %fd11350, 0d0000000000000000;
	ld.param.u64 	%rd319, [%rd25+8];
	ld.param.u32 	%r934, [%rd25+32];
	ld.param.u32 	%r935, [%rd25+60];
	setp.le.s32 	%p853, %r935, %r863;
	selp.u16 	%rs224, 1, 0, %p853;
	or.b16  	%rs225, %rs224, %rs2;
	setp.eq.s16 	%p854, %rs225, 0;
	mov.f64 	%fd11359, 0d0000000000000000;
	mov.f64 	%fd11360, 0d0000000000000000;
	mov.f64 	%fd11361, 0d0000000000000000;
	mov.f64 	%fd11362, 0d0000000000000000;
	mov.f64 	%fd11363, 0d0000000000000000;
	mov.f64 	%fd11364, 0d0000000000000000;
	mov.f64 	%fd11365, 0d0000000000000000;
	mov.f64 	%fd11366, 0d0000000000000000;
	mov.f64 	%fd11367, 0d0000000000000000;
	@%p854 bra 	$L__BB1_539;

	add.s32 	%r1992, %r860, 1;
	st.local.v2.u32 	[%rd26], {%r1992, %r935};
	mov.u64 	%rd2496, $str;
	cvta.global.u64 	%rd2497, %rd2496;
	{ // callseq 474, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2497;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1605, [retval0+0];
	} // callseq 474
	bra.uni 	$L__BB1_541;

$L__BB1_539:
	setp.eq.s64 	%p855, %rd319, 0;
	@%p855 bra 	$L__BB1_541;

	cvta.to.global.u64 	%rd2499, %rd319;
	mul.wide.s32 	%rd2500, %r934, %r863;
	add.s64 	%rd2501, %rd2499, %rd2500;
	ld.global.f64 	%fd6733, [%rd2501];
	add.f64 	%fd11359, %fd6733, 0d0000000000000000;
	ld.global.f64 	%fd6734, [%rd2501+8];
	add.f64 	%fd11360, %fd6734, 0d0000000000000000;
	ld.global.f64 	%fd6735, [%rd2501+16];
	add.f64 	%fd11361, %fd6735, 0d0000000000000000;
	ld.global.f64 	%fd6736, [%rd2501+24];
	add.f64 	%fd11362, %fd6736, 0d0000000000000000;
	ld.global.f64 	%fd6737, [%rd2501+32];
	add.f64 	%fd11363, %fd6737, 0d0000000000000000;
	ld.global.f64 	%fd6738, [%rd2501+40];
	add.f64 	%fd11364, %fd6738, 0d0000000000000000;
	ld.global.f64 	%fd6739, [%rd2501+48];
	add.f64 	%fd11365, %fd6739, 0d0000000000000000;
	ld.global.f64 	%fd6740, [%rd2501+56];
	add.f64 	%fd11366, %fd6740, 0d0000000000000000;
	ld.global.f64 	%fd6741, [%rd2501+64];
	add.f64 	%fd11367, %fd6741, 0d0000000000000000;

$L__BB1_541:
	shr.u32 	%r2239, %r2254, 27;
	cvt.u16.u32 	%rs294, %r2239;
	and.b16  	%rs293, %rs294, 1;
	add.f64 	%fd2413, %fd11367, 0d0000000000000000;
	add.f64 	%fd2414, %fd11366, 0d0000000000000000;
	add.f64 	%fd2415, %fd11365, 0d0000000000000000;
	add.f64 	%fd2416, %fd11364, 0d0000000000000000;
	add.f64 	%fd2417, %fd11363, 0d0000000000000000;
	add.f64 	%fd2418, %fd11362, 0d0000000000000000;
	add.f64 	%fd2419, %fd11361, 0d0000000000000000;
	add.f64 	%fd2420, %fd11360, 0d0000000000000000;
	add.f64 	%fd2421, %fd11359, 0d0000000000000000;
	ld.param.u32 	%r937, [%rd25+60];
	setp.le.s32 	%p856, %r937, %r860;
	selp.u16 	%rs226, 1, 0, %p856;
	or.b16  	%rs227, %rs293, %rs226;
	setp.eq.s16 	%p857, %rs227, 0;
	mov.f64 	%fd11368, 0d0000000000000000;
	mov.f64 	%fd11369, 0d0000000000000000;
	mov.f64 	%fd11370, 0d0000000000000000;
	mov.f64 	%fd11371, 0d0000000000000000;
	mov.f64 	%fd11372, 0d0000000000000000;
	mov.f64 	%fd11373, 0d0000000000000000;
	mov.f64 	%fd11374, 0d0000000000000000;
	mov.f64 	%fd11375, 0d0000000000000000;
	mov.f64 	%fd11376, 0d0000000000000000;
	@%p857 bra 	$L__BB1_543;

	st.local.v2.u32 	[%rd26], {%r860, %r937};
	mov.u64 	%rd2502, $str;
	cvta.global.u64 	%rd2503, %rd2502;
	{ // callseq 475, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2503;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1606, [retval0+0];
	} // callseq 475
	bra.uni 	$L__BB1_545;

$L__BB1_543:
	ld.param.u64 	%rd4116, [%rd25+8];
	setp.eq.s64 	%p858, %rd4116, 0;
	@%p858 bra 	$L__BB1_545;

	ld.param.u32 	%r2241, [%rd25+32];
	ld.param.u64 	%rd4117, [%rd25+8];
	cvta.to.global.u64 	%rd2505, %rd4117;
	mul.wide.s32 	%rd2506, %r2241, %r860;
	add.s64 	%rd2507, %rd2505, %rd2506;
	ld.global.f64 	%fd6760, [%rd2507];
	add.f64 	%fd11368, %fd6760, 0d0000000000000000;
	ld.global.f64 	%fd6761, [%rd2507+8];
	add.f64 	%fd11369, %fd6761, 0d0000000000000000;
	ld.global.f64 	%fd6762, [%rd2507+16];
	add.f64 	%fd11370, %fd6762, 0d0000000000000000;
	ld.global.f64 	%fd6763, [%rd2507+24];
	add.f64 	%fd11371, %fd6763, 0d0000000000000000;
	ld.global.f64 	%fd6764, [%rd2507+32];
	add.f64 	%fd11372, %fd6764, 0d0000000000000000;
	ld.global.f64 	%fd6765, [%rd2507+40];
	add.f64 	%fd11373, %fd6765, 0d0000000000000000;
	ld.global.f64 	%fd6766, [%rd2507+48];
	add.f64 	%fd11374, %fd6766, 0d0000000000000000;
	ld.global.f64 	%fd6767, [%rd2507+56];
	add.f64 	%fd11375, %fd6767, 0d0000000000000000;
	ld.global.f64 	%fd6768, [%rd2507+64];
	add.f64 	%fd11376, %fd6768, 0d0000000000000000;

$L__BB1_545:
	add.f64 	%fd2440, %fd11376, 0d0000000000000000;
	add.f64 	%fd2441, %fd11375, 0d0000000000000000;
	add.f64 	%fd2442, %fd11374, 0d0000000000000000;
	add.f64 	%fd2443, %fd11373, 0d0000000000000000;
	add.f64 	%fd2444, %fd11372, 0d0000000000000000;
	add.f64 	%fd2445, %fd11371, 0d0000000000000000;
	add.f64 	%fd2446, %fd11370, 0d0000000000000000;
	add.f64 	%fd2447, %fd11369, 0d0000000000000000;
	add.f64 	%fd2448, %fd11368, 0d0000000000000000;
	ld.param.u32 	%r939, [%rd25+60];
	shl.b32 	%r940, %r2253, 4;
	setp.le.s32 	%p859, %r939, %r940;
	selp.u16 	%rs228, 1, 0, %p859;
	shr.u32 	%r1607, %r2253, 27;
	cvt.u16.u32 	%rs229, %r1607;
	and.b16  	%rs17, %rs229, 1;
	or.b16  	%rs230, %rs17, %rs228;
	setp.eq.s16 	%p860, %rs230, 0;
	@%p860 bra 	$L__BB1_547;

	st.local.v2.u32 	[%rd26], {%r940, %r939};
	mov.u64 	%rd2508, $str;
	cvta.global.u64 	%rd2509, %rd2508;
	{ // callseq 476, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2509;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1608, [retval0+0];
	} // callseq 476
	bra.uni 	$L__BB1_548;

$L__BB1_547:
	ld.param.u64 	%rd4118, [%rd25];
	ld.param.u32 	%r2243, [%rd25+32];
	mul.wide.s32 	%rd2520, %r2243, %r940;
	add.s64 	%rd2511, %rd4118, %rd2520;
	// begin inline asm
	{ atom.add.f64 %fd6769,[%rd2511],%fd10512; }

	// end inline asm
	add.s64 	%rd2512, %rd2511, 8;
	// begin inline asm
	{ atom.add.f64 %fd6771,[%rd2512],%fd10511; }

	// end inline asm
	add.s64 	%rd2513, %rd2511, 16;
	// begin inline asm
	{ atom.add.f64 %fd6773,[%rd2513],%fd10510; }

	// end inline asm
	add.s64 	%rd2514, %rd2511, 24;
	// begin inline asm
	{ atom.add.f64 %fd6775,[%rd2514],%fd10500; }

	// end inline asm
	add.s64 	%rd2515, %rd2511, 32;
	// begin inline asm
	{ atom.add.f64 %fd6777,[%rd2515],%fd10499; }

	// end inline asm
	add.s64 	%rd2516, %rd2511, 40;
	// begin inline asm
	{ atom.add.f64 %fd6779,[%rd2516],%fd10498; }

	// end inline asm
	add.s64 	%rd2517, %rd2511, 48;
	// begin inline asm
	{ atom.add.f64 %fd6781,[%rd2517],%fd10488; }

	// end inline asm
	add.s64 	%rd2518, %rd2511, 56;
	// begin inline asm
	{ atom.add.f64 %fd6783,[%rd2518],%fd10487; }

	// end inline asm
	add.s64 	%rd2519, %rd2511, 64;
	// begin inline asm
	{ atom.add.f64 %fd6785,[%rd2519],%fd10486; }

	// end inline asm

$L__BB1_548:
	ld.param.u64 	%rd322, [%rd25];
	ld.param.u32 	%r941, [%rd25+32];
	ld.param.u32 	%r942, [%rd25+60];
	add.s32 	%r943, %r940, 1;
	setp.le.s32 	%p861, %r942, %r943;
	selp.u16 	%rs231, 1, 0, %p861;
	shr.u32 	%r1609, %r943, 31;
	cvt.u16.u32 	%rs18, %r1609;
	or.b16  	%rs232, %rs231, %rs18;
	setp.eq.s16 	%p862, %rs232, 0;
	@%p862 bra 	$L__BB1_550;

	add.s32 	%r1993, %r940, 1;
	st.local.v2.u32 	[%rd26], {%r1993, %r942};
	mov.u64 	%rd2521, $str;
	cvta.global.u64 	%rd2522, %rd2521;
	{ // callseq 477, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2522;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1610, [retval0+0];
	} // callseq 477
	bra.uni 	$L__BB1_551;

$L__BB1_550:
	mul.wide.s32 	%rd2533, %r941, %r943;
	add.s64 	%rd2524, %rd322, %rd2533;
	// begin inline asm
	{ atom.add.f64 %fd6787,[%rd2524],%fd10509; }

	// end inline asm
	add.s64 	%rd2525, %rd2524, 8;
	// begin inline asm
	{ atom.add.f64 %fd6789,[%rd2525],%fd10508; }

	// end inline asm
	add.s64 	%rd2526, %rd2524, 16;
	// begin inline asm
	{ atom.add.f64 %fd6791,[%rd2526],%fd10507; }

	// end inline asm
	add.s64 	%rd2527, %rd2524, 24;
	// begin inline asm
	{ atom.add.f64 %fd6793,[%rd2527],%fd10497; }

	// end inline asm
	add.s64 	%rd2528, %rd2524, 32;
	// begin inline asm
	{ atom.add.f64 %fd6795,[%rd2528],%fd10496; }

	// end inline asm
	add.s64 	%rd2529, %rd2524, 40;
	// begin inline asm
	{ atom.add.f64 %fd6797,[%rd2529],%fd10495; }

	// end inline asm
	add.s64 	%rd2530, %rd2524, 48;
	// begin inline asm
	{ atom.add.f64 %fd6799,[%rd2530],%fd10485; }

	// end inline asm
	add.s64 	%rd2531, %rd2524, 56;
	// begin inline asm
	{ atom.add.f64 %fd6801,[%rd2531],%fd10484; }

	// end inline asm
	add.s64 	%rd2532, %rd2524, 64;
	// begin inline asm
	{ atom.add.f64 %fd6803,[%rd2532],%fd10483; }

	// end inline asm

$L__BB1_551:
	ld.param.u64 	%rd323, [%rd25];
	ld.param.u32 	%r944, [%rd25+32];
	ld.param.u32 	%r945, [%rd25+60];
	add.s32 	%r946, %r940, 2;
	setp.le.s32 	%p863, %r945, %r946;
	selp.u16 	%rs233, 1, 0, %p863;
	shr.u32 	%r1611, %r946, 31;
	cvt.u16.u32 	%rs19, %r1611;
	or.b16  	%rs234, %rs233, %rs19;
	setp.eq.s16 	%p864, %rs234, 0;
	@%p864 bra 	$L__BB1_553;

	add.s32 	%r1994, %r940, 2;
	st.local.v2.u32 	[%rd26], {%r1994, %r945};
	mov.u64 	%rd2534, $str;
	cvta.global.u64 	%rd2535, %rd2534;
	{ // callseq 478, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2535;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1612, [retval0+0];
	} // callseq 478
	bra.uni 	$L__BB1_554;

$L__BB1_553:
	mul.wide.s32 	%rd2546, %r944, %r946;
	add.s64 	%rd2537, %rd323, %rd2546;
	// begin inline asm
	{ atom.add.f64 %fd6805,[%rd2537],%fd10506; }

	// end inline asm
	add.s64 	%rd2538, %rd2537, 8;
	// begin inline asm
	{ atom.add.f64 %fd6807,[%rd2538],%fd10505; }

	// end inline asm
	add.s64 	%rd2539, %rd2537, 16;
	// begin inline asm
	{ atom.add.f64 %fd6809,[%rd2539],%fd10504; }

	// end inline asm
	add.s64 	%rd2540, %rd2537, 24;
	// begin inline asm
	{ atom.add.f64 %fd6811,[%rd2540],%fd10494; }

	// end inline asm
	add.s64 	%rd2541, %rd2537, 32;
	// begin inline asm
	{ atom.add.f64 %fd6813,[%rd2541],%fd10493; }

	// end inline asm
	add.s64 	%rd2542, %rd2537, 40;
	// begin inline asm
	{ atom.add.f64 %fd6815,[%rd2542],%fd10492; }

	// end inline asm
	add.s64 	%rd2543, %rd2537, 48;
	// begin inline asm
	{ atom.add.f64 %fd6817,[%rd2543],%fd10482; }

	// end inline asm
	add.s64 	%rd2544, %rd2537, 56;
	// begin inline asm
	{ atom.add.f64 %fd6819,[%rd2544],%fd10481; }

	// end inline asm
	add.s64 	%rd2545, %rd2537, 64;
	// begin inline asm
	{ atom.add.f64 %fd6821,[%rd2545],%fd10480; }

	// end inline asm

$L__BB1_554:
	ld.param.u64 	%rd324, [%rd25];
	ld.param.u32 	%r947, [%rd25+32];
	ld.param.u32 	%r948, [%rd25+60];
	add.s32 	%r949, %r940, 3;
	setp.le.s32 	%p865, %r948, %r949;
	selp.u16 	%rs235, 1, 0, %p865;
	shr.u32 	%r1613, %r949, 31;
	cvt.u16.u32 	%rs20, %r1613;
	or.b16  	%rs236, %rs235, %rs20;
	setp.eq.s16 	%p866, %rs236, 0;
	@%p866 bra 	$L__BB1_556;

	add.s32 	%r1995, %r940, 3;
	st.local.v2.u32 	[%rd26], {%r1995, %r948};
	mov.u64 	%rd2547, $str;
	cvta.global.u64 	%rd2548, %rd2547;
	{ // callseq 479, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2548;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1614, [retval0+0];
	} // callseq 479
	bra.uni 	$L__BB1_557;

$L__BB1_556:
	mul.wide.s32 	%rd2559, %r947, %r949;
	add.s64 	%rd2550, %rd324, %rd2559;
	// begin inline asm
	{ atom.add.f64 %fd6823,[%rd2550],%fd10503; }

	// end inline asm
	add.s64 	%rd2551, %rd2550, 8;
	// begin inline asm
	{ atom.add.f64 %fd6825,[%rd2551],%fd10502; }

	// end inline asm
	add.s64 	%rd2552, %rd2550, 16;
	// begin inline asm
	{ atom.add.f64 %fd6827,[%rd2552],%fd10501; }

	// end inline asm
	add.s64 	%rd2553, %rd2550, 24;
	// begin inline asm
	{ atom.add.f64 %fd6829,[%rd2553],%fd10491; }

	// end inline asm
	add.s64 	%rd2554, %rd2550, 32;
	// begin inline asm
	{ atom.add.f64 %fd6831,[%rd2554],%fd10490; }

	// end inline asm
	add.s64 	%rd2555, %rd2550, 40;
	// begin inline asm
	{ atom.add.f64 %fd6833,[%rd2555],%fd10489; }

	// end inline asm
	add.s64 	%rd2556, %rd2550, 48;
	// begin inline asm
	{ atom.add.f64 %fd6835,[%rd2556],%fd10479; }

	// end inline asm
	add.s64 	%rd2557, %rd2550, 56;
	// begin inline asm
	{ atom.add.f64 %fd6837,[%rd2557],%fd10478; }

	// end inline asm
	add.s64 	%rd2558, %rd2550, 64;
	// begin inline asm
	{ atom.add.f64 %fd6839,[%rd2558],%fd10477; }

	// end inline asm

$L__BB1_557:
	ld.param.u64 	%rd325, [%rd25];
	ld.param.u32 	%r950, [%rd25+32];
	ld.param.u32 	%r951, [%rd25+60];
	add.s32 	%r952, %r940, 4;
	setp.le.s32 	%p867, %r951, %r952;
	selp.u16 	%rs237, 1, 0, %p867;
	shr.u32 	%r1615, %r952, 31;
	cvt.u16.u32 	%rs21, %r1615;
	or.b16  	%rs238, %rs237, %rs21;
	setp.eq.s16 	%p868, %rs238, 0;
	@%p868 bra 	$L__BB1_559;

	add.s32 	%r1996, %r940, 4;
	st.local.v2.u32 	[%rd26], {%r1996, %r951};
	mov.u64 	%rd2560, $str;
	cvta.global.u64 	%rd2561, %rd2560;
	{ // callseq 480, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2561;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1616, [retval0+0];
	} // callseq 480
	bra.uni 	$L__BB1_560;

$L__BB1_559:
	mul.wide.s32 	%rd2572, %r950, %r952;
	add.s64 	%rd2563, %rd325, %rd2572;
	// begin inline asm
	{ atom.add.f64 %fd6841,[%rd2563],%fd10476; }

	// end inline asm
	add.s64 	%rd2564, %rd2563, 8;
	// begin inline asm
	{ atom.add.f64 %fd6843,[%rd2564],%fd10475; }

	// end inline asm
	add.s64 	%rd2565, %rd2563, 16;
	// begin inline asm
	{ atom.add.f64 %fd6845,[%rd2565],%fd10474; }

	// end inline asm
	add.s64 	%rd2566, %rd2563, 24;
	// begin inline asm
	{ atom.add.f64 %fd6847,[%rd2566],%fd10464; }

	// end inline asm
	add.s64 	%rd2567, %rd2563, 32;
	// begin inline asm
	{ atom.add.f64 %fd6849,[%rd2567],%fd10463; }

	// end inline asm
	add.s64 	%rd2568, %rd2563, 40;
	// begin inline asm
	{ atom.add.f64 %fd6851,[%rd2568],%fd10462; }

	// end inline asm
	add.s64 	%rd2569, %rd2563, 48;
	// begin inline asm
	{ atom.add.f64 %fd6853,[%rd2569],%fd10452; }

	// end inline asm
	add.s64 	%rd2570, %rd2563, 56;
	// begin inline asm
	{ atom.add.f64 %fd6855,[%rd2570],%fd10451; }

	// end inline asm
	add.s64 	%rd2571, %rd2563, 64;
	// begin inline asm
	{ atom.add.f64 %fd6857,[%rd2571],%fd10450; }

	// end inline asm

$L__BB1_560:
	ld.param.u64 	%rd326, [%rd25];
	ld.param.u32 	%r953, [%rd25+32];
	ld.param.u32 	%r954, [%rd25+60];
	add.s32 	%r955, %r940, 5;
	setp.le.s32 	%p869, %r954, %r955;
	selp.u16 	%rs239, 1, 0, %p869;
	shr.u32 	%r1617, %r955, 31;
	cvt.u16.u32 	%rs22, %r1617;
	or.b16  	%rs240, %rs239, %rs22;
	setp.eq.s16 	%p870, %rs240, 0;
	@%p870 bra 	$L__BB1_562;

	add.s32 	%r1997, %r940, 5;
	st.local.v2.u32 	[%rd26], {%r1997, %r954};
	mov.u64 	%rd2573, $str;
	cvta.global.u64 	%rd2574, %rd2573;
	{ // callseq 481, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2574;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1618, [retval0+0];
	} // callseq 481
	bra.uni 	$L__BB1_563;

$L__BB1_562:
	mul.wide.s32 	%rd2585, %r953, %r955;
	add.s64 	%rd2576, %rd326, %rd2585;
	// begin inline asm
	{ atom.add.f64 %fd6859,[%rd2576],%fd10473; }

	// end inline asm
	add.s64 	%rd2577, %rd2576, 8;
	// begin inline asm
	{ atom.add.f64 %fd6861,[%rd2577],%fd10472; }

	// end inline asm
	add.s64 	%rd2578, %rd2576, 16;
	// begin inline asm
	{ atom.add.f64 %fd6863,[%rd2578],%fd10471; }

	// end inline asm
	add.s64 	%rd2579, %rd2576, 24;
	// begin inline asm
	{ atom.add.f64 %fd6865,[%rd2579],%fd10461; }

	// end inline asm
	add.s64 	%rd2580, %rd2576, 32;
	// begin inline asm
	{ atom.add.f64 %fd6867,[%rd2580],%fd10460; }

	// end inline asm
	add.s64 	%rd2581, %rd2576, 40;
	// begin inline asm
	{ atom.add.f64 %fd6869,[%rd2581],%fd10459; }

	// end inline asm
	add.s64 	%rd2582, %rd2576, 48;
	// begin inline asm
	{ atom.add.f64 %fd6871,[%rd2582],%fd10449; }

	// end inline asm
	add.s64 	%rd2583, %rd2576, 56;
	// begin inline asm
	{ atom.add.f64 %fd6873,[%rd2583],%fd10448; }

	// end inline asm
	add.s64 	%rd2584, %rd2576, 64;
	// begin inline asm
	{ atom.add.f64 %fd6875,[%rd2584],%fd10447; }

	// end inline asm

$L__BB1_563:
	ld.param.u64 	%rd327, [%rd25];
	ld.param.u32 	%r956, [%rd25+32];
	ld.param.u32 	%r957, [%rd25+60];
	add.s32 	%r958, %r940, 6;
	setp.le.s32 	%p871, %r957, %r958;
	selp.u16 	%rs241, 1, 0, %p871;
	shr.u32 	%r1619, %r958, 31;
	cvt.u16.u32 	%rs23, %r1619;
	or.b16  	%rs242, %rs241, %rs23;
	setp.eq.s16 	%p872, %rs242, 0;
	@%p872 bra 	$L__BB1_565;

	add.s32 	%r1998, %r940, 6;
	st.local.v2.u32 	[%rd26], {%r1998, %r957};
	mov.u64 	%rd2586, $str;
	cvta.global.u64 	%rd2587, %rd2586;
	{ // callseq 482, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2587;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1620, [retval0+0];
	} // callseq 482
	bra.uni 	$L__BB1_566;

$L__BB1_565:
	mul.wide.s32 	%rd2598, %r956, %r958;
	add.s64 	%rd2589, %rd327, %rd2598;
	// begin inline asm
	{ atom.add.f64 %fd6877,[%rd2589],%fd10470; }

	// end inline asm
	add.s64 	%rd2590, %rd2589, 8;
	// begin inline asm
	{ atom.add.f64 %fd6879,[%rd2590],%fd10469; }

	// end inline asm
	add.s64 	%rd2591, %rd2589, 16;
	// begin inline asm
	{ atom.add.f64 %fd6881,[%rd2591],%fd10468; }

	// end inline asm
	add.s64 	%rd2592, %rd2589, 24;
	// begin inline asm
	{ atom.add.f64 %fd6883,[%rd2592],%fd10458; }

	// end inline asm
	add.s64 	%rd2593, %rd2589, 32;
	// begin inline asm
	{ atom.add.f64 %fd6885,[%rd2593],%fd10457; }

	// end inline asm
	add.s64 	%rd2594, %rd2589, 40;
	// begin inline asm
	{ atom.add.f64 %fd6887,[%rd2594],%fd10456; }

	// end inline asm
	add.s64 	%rd2595, %rd2589, 48;
	// begin inline asm
	{ atom.add.f64 %fd6889,[%rd2595],%fd10446; }

	// end inline asm
	add.s64 	%rd2596, %rd2589, 56;
	// begin inline asm
	{ atom.add.f64 %fd6891,[%rd2596],%fd10445; }

	// end inline asm
	add.s64 	%rd2597, %rd2589, 64;
	// begin inline asm
	{ atom.add.f64 %fd6893,[%rd2597],%fd10444; }

	// end inline asm

$L__BB1_566:
	ld.param.u64 	%rd328, [%rd25];
	ld.param.u32 	%r959, [%rd25+32];
	ld.param.u32 	%r960, [%rd25+60];
	add.s32 	%r961, %r940, 7;
	setp.le.s32 	%p873, %r960, %r961;
	selp.u16 	%rs243, 1, 0, %p873;
	shr.u32 	%r1621, %r961, 31;
	cvt.u16.u32 	%rs24, %r1621;
	or.b16  	%rs244, %rs243, %rs24;
	setp.eq.s16 	%p874, %rs244, 0;
	@%p874 bra 	$L__BB1_568;

	add.s32 	%r1999, %r940, 7;
	st.local.v2.u32 	[%rd26], {%r1999, %r960};
	mov.u64 	%rd2599, $str;
	cvta.global.u64 	%rd2600, %rd2599;
	{ // callseq 483, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2600;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1622, [retval0+0];
	} // callseq 483
	bra.uni 	$L__BB1_569;

$L__BB1_568:
	mul.wide.s32 	%rd2611, %r959, %r961;
	add.s64 	%rd2602, %rd328, %rd2611;
	// begin inline asm
	{ atom.add.f64 %fd6895,[%rd2602],%fd10467; }

	// end inline asm
	add.s64 	%rd2603, %rd2602, 8;
	// begin inline asm
	{ atom.add.f64 %fd6897,[%rd2603],%fd10466; }

	// end inline asm
	add.s64 	%rd2604, %rd2602, 16;
	// begin inline asm
	{ atom.add.f64 %fd6899,[%rd2604],%fd10465; }

	// end inline asm
	add.s64 	%rd2605, %rd2602, 24;
	// begin inline asm
	{ atom.add.f64 %fd6901,[%rd2605],%fd10455; }

	// end inline asm
	add.s64 	%rd2606, %rd2602, 32;
	// begin inline asm
	{ atom.add.f64 %fd6903,[%rd2606],%fd10454; }

	// end inline asm
	add.s64 	%rd2607, %rd2602, 40;
	// begin inline asm
	{ atom.add.f64 %fd6905,[%rd2607],%fd10453; }

	// end inline asm
	add.s64 	%rd2608, %rd2602, 48;
	// begin inline asm
	{ atom.add.f64 %fd6907,[%rd2608],%fd10443; }

	// end inline asm
	add.s64 	%rd2609, %rd2602, 56;
	// begin inline asm
	{ atom.add.f64 %fd6909,[%rd2609],%fd10442; }

	// end inline asm
	add.s64 	%rd2610, %rd2602, 64;
	// begin inline asm
	{ atom.add.f64 %fd6911,[%rd2610],%fd10441; }

	// end inline asm

$L__BB1_569:
	ld.param.u64 	%rd329, [%rd25];
	ld.param.u32 	%r962, [%rd25+32];
	ld.param.u32 	%r963, [%rd25+60];
	add.s32 	%r964, %r940, 8;
	setp.le.s32 	%p875, %r963, %r964;
	selp.u16 	%rs245, 1, 0, %p875;
	shr.u32 	%r1623, %r964, 31;
	cvt.u16.u32 	%rs25, %r1623;
	or.b16  	%rs246, %rs245, %rs25;
	setp.eq.s16 	%p876, %rs246, 0;
	@%p876 bra 	$L__BB1_571;

	add.s32 	%r2000, %r940, 8;
	st.local.v2.u32 	[%rd26], {%r2000, %r963};
	mov.u64 	%rd2612, $str;
	cvta.global.u64 	%rd2613, %rd2612;
	{ // callseq 484, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2613;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1624, [retval0+0];
	} // callseq 484
	bra.uni 	$L__BB1_572;

$L__BB1_571:
	mul.wide.s32 	%rd2624, %r962, %r964;
	add.s64 	%rd2615, %rd329, %rd2624;
	// begin inline asm
	{ atom.add.f64 %fd6913,[%rd2615],%fd10440; }

	// end inline asm
	add.s64 	%rd2616, %rd2615, 8;
	// begin inline asm
	{ atom.add.f64 %fd6915,[%rd2616],%fd10439; }

	// end inline asm
	add.s64 	%rd2617, %rd2615, 16;
	// begin inline asm
	{ atom.add.f64 %fd6917,[%rd2617],%fd10438; }

	// end inline asm
	add.s64 	%rd2618, %rd2615, 24;
	// begin inline asm
	{ atom.add.f64 %fd6919,[%rd2618],%fd10428; }

	// end inline asm
	add.s64 	%rd2619, %rd2615, 32;
	// begin inline asm
	{ atom.add.f64 %fd6921,[%rd2619],%fd10427; }

	// end inline asm
	add.s64 	%rd2620, %rd2615, 40;
	// begin inline asm
	{ atom.add.f64 %fd6923,[%rd2620],%fd10426; }

	// end inline asm
	add.s64 	%rd2621, %rd2615, 48;
	// begin inline asm
	{ atom.add.f64 %fd6925,[%rd2621],%fd10416; }

	// end inline asm
	add.s64 	%rd2622, %rd2615, 56;
	// begin inline asm
	{ atom.add.f64 %fd6927,[%rd2622],%fd10415; }

	// end inline asm
	add.s64 	%rd2623, %rd2615, 64;
	// begin inline asm
	{ atom.add.f64 %fd6929,[%rd2623],%fd10414; }

	// end inline asm

$L__BB1_572:
	ld.param.u64 	%rd330, [%rd25];
	ld.param.u32 	%r965, [%rd25+32];
	ld.param.u32 	%r966, [%rd25+60];
	add.s32 	%r967, %r940, 9;
	setp.le.s32 	%p877, %r966, %r967;
	selp.u16 	%rs247, 1, 0, %p877;
	shr.u32 	%r1625, %r967, 31;
	cvt.u16.u32 	%rs26, %r1625;
	or.b16  	%rs248, %rs247, %rs26;
	setp.eq.s16 	%p878, %rs248, 0;
	@%p878 bra 	$L__BB1_574;

	add.s32 	%r2001, %r940, 9;
	st.local.v2.u32 	[%rd26], {%r2001, %r966};
	mov.u64 	%rd2625, $str;
	cvta.global.u64 	%rd2626, %rd2625;
	{ // callseq 485, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2626;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1626, [retval0+0];
	} // callseq 485
	bra.uni 	$L__BB1_575;

$L__BB1_574:
	mul.wide.s32 	%rd2637, %r965, %r967;
	add.s64 	%rd2628, %rd330, %rd2637;
	// begin inline asm
	{ atom.add.f64 %fd6931,[%rd2628],%fd10437; }

	// end inline asm
	add.s64 	%rd2629, %rd2628, 8;
	// begin inline asm
	{ atom.add.f64 %fd6933,[%rd2629],%fd10436; }

	// end inline asm
	add.s64 	%rd2630, %rd2628, 16;
	// begin inline asm
	{ atom.add.f64 %fd6935,[%rd2630],%fd10435; }

	// end inline asm
	add.s64 	%rd2631, %rd2628, 24;
	// begin inline asm
	{ atom.add.f64 %fd6937,[%rd2631],%fd10425; }

	// end inline asm
	add.s64 	%rd2632, %rd2628, 32;
	// begin inline asm
	{ atom.add.f64 %fd6939,[%rd2632],%fd10424; }

	// end inline asm
	add.s64 	%rd2633, %rd2628, 40;
	// begin inline asm
	{ atom.add.f64 %fd6941,[%rd2633],%fd10423; }

	// end inline asm
	add.s64 	%rd2634, %rd2628, 48;
	// begin inline asm
	{ atom.add.f64 %fd6943,[%rd2634],%fd10413; }

	// end inline asm
	add.s64 	%rd2635, %rd2628, 56;
	// begin inline asm
	{ atom.add.f64 %fd6945,[%rd2635],%fd10412; }

	// end inline asm
	add.s64 	%rd2636, %rd2628, 64;
	// begin inline asm
	{ atom.add.f64 %fd6947,[%rd2636],%fd10411; }

	// end inline asm

$L__BB1_575:
	ld.param.u64 	%rd331, [%rd25];
	ld.param.u32 	%r968, [%rd25+32];
	ld.param.u32 	%r969, [%rd25+60];
	add.s32 	%r970, %r940, 10;
	setp.le.s32 	%p879, %r969, %r970;
	selp.u16 	%rs249, 1, 0, %p879;
	shr.u32 	%r1627, %r970, 31;
	cvt.u16.u32 	%rs27, %r1627;
	or.b16  	%rs250, %rs249, %rs27;
	setp.eq.s16 	%p880, %rs250, 0;
	@%p880 bra 	$L__BB1_577;

	add.s32 	%r2002, %r940, 10;
	st.local.v2.u32 	[%rd26], {%r2002, %r969};
	mov.u64 	%rd2638, $str;
	cvta.global.u64 	%rd2639, %rd2638;
	{ // callseq 486, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2639;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1628, [retval0+0];
	} // callseq 486
	bra.uni 	$L__BB1_578;

$L__BB1_577:
	mul.wide.s32 	%rd2650, %r968, %r970;
	add.s64 	%rd2641, %rd331, %rd2650;
	// begin inline asm
	{ atom.add.f64 %fd6949,[%rd2641],%fd10434; }

	// end inline asm
	add.s64 	%rd2642, %rd2641, 8;
	// begin inline asm
	{ atom.add.f64 %fd6951,[%rd2642],%fd10433; }

	// end inline asm
	add.s64 	%rd2643, %rd2641, 16;
	// begin inline asm
	{ atom.add.f64 %fd6953,[%rd2643],%fd10432; }

	// end inline asm
	add.s64 	%rd2644, %rd2641, 24;
	// begin inline asm
	{ atom.add.f64 %fd6955,[%rd2644],%fd10422; }

	// end inline asm
	add.s64 	%rd2645, %rd2641, 32;
	// begin inline asm
	{ atom.add.f64 %fd6957,[%rd2645],%fd10421; }

	// end inline asm
	add.s64 	%rd2646, %rd2641, 40;
	// begin inline asm
	{ atom.add.f64 %fd6959,[%rd2646],%fd10420; }

	// end inline asm
	add.s64 	%rd2647, %rd2641, 48;
	// begin inline asm
	{ atom.add.f64 %fd6961,[%rd2647],%fd10410; }

	// end inline asm
	add.s64 	%rd2648, %rd2641, 56;
	// begin inline asm
	{ atom.add.f64 %fd6963,[%rd2648],%fd10409; }

	// end inline asm
	add.s64 	%rd2649, %rd2641, 64;
	// begin inline asm
	{ atom.add.f64 %fd6965,[%rd2649],%fd10408; }

	// end inline asm

$L__BB1_578:
	ld.param.u64 	%rd332, [%rd25];
	ld.param.u32 	%r971, [%rd25+32];
	ld.param.u32 	%r972, [%rd25+60];
	add.s32 	%r973, %r940, 11;
	setp.le.s32 	%p881, %r972, %r973;
	selp.u16 	%rs251, 1, 0, %p881;
	shr.u32 	%r1629, %r973, 31;
	cvt.u16.u32 	%rs28, %r1629;
	or.b16  	%rs252, %rs251, %rs28;
	setp.eq.s16 	%p882, %rs252, 0;
	@%p882 bra 	$L__BB1_580;

	add.s32 	%r2003, %r940, 11;
	st.local.v2.u32 	[%rd26], {%r2003, %r972};
	mov.u64 	%rd2651, $str;
	cvta.global.u64 	%rd2652, %rd2651;
	{ // callseq 487, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2652;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1630, [retval0+0];
	} // callseq 487
	bra.uni 	$L__BB1_581;

$L__BB1_580:
	mul.wide.s32 	%rd2663, %r971, %r973;
	add.s64 	%rd2654, %rd332, %rd2663;
	// begin inline asm
	{ atom.add.f64 %fd6967,[%rd2654],%fd10431; }

	// end inline asm
	add.s64 	%rd2655, %rd2654, 8;
	// begin inline asm
	{ atom.add.f64 %fd6969,[%rd2655],%fd10430; }

	// end inline asm
	add.s64 	%rd2656, %rd2654, 16;
	// begin inline asm
	{ atom.add.f64 %fd6971,[%rd2656],%fd10429; }

	// end inline asm
	add.s64 	%rd2657, %rd2654, 24;
	// begin inline asm
	{ atom.add.f64 %fd6973,[%rd2657],%fd10419; }

	// end inline asm
	add.s64 	%rd2658, %rd2654, 32;
	// begin inline asm
	{ atom.add.f64 %fd6975,[%rd2658],%fd10418; }

	// end inline asm
	add.s64 	%rd2659, %rd2654, 40;
	// begin inline asm
	{ atom.add.f64 %fd6977,[%rd2659],%fd10417; }

	// end inline asm
	add.s64 	%rd2660, %rd2654, 48;
	// begin inline asm
	{ atom.add.f64 %fd6979,[%rd2660],%fd10407; }

	// end inline asm
	add.s64 	%rd2661, %rd2654, 56;
	// begin inline asm
	{ atom.add.f64 %fd6981,[%rd2661],%fd10406; }

	// end inline asm
	add.s64 	%rd2662, %rd2654, 64;
	// begin inline asm
	{ atom.add.f64 %fd6983,[%rd2662],%fd10405; }

	// end inline asm

$L__BB1_581:
	ld.param.u64 	%rd333, [%rd25];
	ld.param.u32 	%r974, [%rd25+32];
	ld.param.u32 	%r975, [%rd25+60];
	add.s32 	%r976, %r940, 12;
	setp.le.s32 	%p883, %r975, %r976;
	selp.u16 	%rs253, 1, 0, %p883;
	shr.u32 	%r1631, %r976, 31;
	cvt.u16.u32 	%rs29, %r1631;
	or.b16  	%rs254, %rs253, %rs29;
	setp.eq.s16 	%p884, %rs254, 0;
	@%p884 bra 	$L__BB1_583;

	add.s32 	%r2004, %r940, 12;
	st.local.v2.u32 	[%rd26], {%r2004, %r975};
	mov.u64 	%rd2664, $str;
	cvta.global.u64 	%rd2665, %rd2664;
	{ // callseq 488, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2665;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1632, [retval0+0];
	} // callseq 488
	bra.uni 	$L__BB1_584;

$L__BB1_583:
	mul.wide.s32 	%rd2676, %r974, %r976;
	add.s64 	%rd2667, %rd333, %rd2676;
	// begin inline asm
	{ atom.add.f64 %fd6985,[%rd2667],%fd10404; }

	// end inline asm
	add.s64 	%rd2668, %rd2667, 8;
	// begin inline asm
	{ atom.add.f64 %fd6987,[%rd2668],%fd10403; }

	// end inline asm
	add.s64 	%rd2669, %rd2667, 16;
	// begin inline asm
	{ atom.add.f64 %fd6989,[%rd2669],%fd10402; }

	// end inline asm
	add.s64 	%rd2670, %rd2667, 24;
	// begin inline asm
	{ atom.add.f64 %fd6991,[%rd2670],%fd10392; }

	// end inline asm
	add.s64 	%rd2671, %rd2667, 32;
	// begin inline asm
	{ atom.add.f64 %fd6993,[%rd2671],%fd10391; }

	// end inline asm
	add.s64 	%rd2672, %rd2667, 40;
	// begin inline asm
	{ atom.add.f64 %fd6995,[%rd2672],%fd10390; }

	// end inline asm
	add.s64 	%rd2673, %rd2667, 48;
	// begin inline asm
	{ atom.add.f64 %fd6997,[%rd2673],%fd10380; }

	// end inline asm
	add.s64 	%rd2674, %rd2667, 56;
	// begin inline asm
	{ atom.add.f64 %fd6999,[%rd2674],%fd10379; }

	// end inline asm
	add.s64 	%rd2675, %rd2667, 64;
	// begin inline asm
	{ atom.add.f64 %fd7001,[%rd2675],%fd10378; }

	// end inline asm

$L__BB1_584:
	ld.param.u64 	%rd334, [%rd25];
	ld.param.u32 	%r977, [%rd25+32];
	ld.param.u32 	%r978, [%rd25+60];
	add.s32 	%r979, %r940, 13;
	setp.le.s32 	%p885, %r978, %r979;
	selp.u16 	%rs255, 1, 0, %p885;
	shr.u32 	%r1633, %r979, 31;
	cvt.u16.u32 	%rs30, %r1633;
	or.b16  	%rs256, %rs255, %rs30;
	setp.eq.s16 	%p886, %rs256, 0;
	@%p886 bra 	$L__BB1_586;

	add.s32 	%r2005, %r940, 13;
	st.local.v2.u32 	[%rd26], {%r2005, %r978};
	mov.u64 	%rd2677, $str;
	cvta.global.u64 	%rd2678, %rd2677;
	{ // callseq 489, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2678;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1634, [retval0+0];
	} // callseq 489
	bra.uni 	$L__BB1_587;

$L__BB1_586:
	mul.wide.s32 	%rd2689, %r977, %r979;
	add.s64 	%rd2680, %rd334, %rd2689;
	// begin inline asm
	{ atom.add.f64 %fd7003,[%rd2680],%fd10401; }

	// end inline asm
	add.s64 	%rd2681, %rd2680, 8;
	// begin inline asm
	{ atom.add.f64 %fd7005,[%rd2681],%fd10400; }

	// end inline asm
	add.s64 	%rd2682, %rd2680, 16;
	// begin inline asm
	{ atom.add.f64 %fd7007,[%rd2682],%fd10399; }

	// end inline asm
	add.s64 	%rd2683, %rd2680, 24;
	// begin inline asm
	{ atom.add.f64 %fd7009,[%rd2683],%fd10389; }

	// end inline asm
	add.s64 	%rd2684, %rd2680, 32;
	// begin inline asm
	{ atom.add.f64 %fd7011,[%rd2684],%fd10388; }

	// end inline asm
	add.s64 	%rd2685, %rd2680, 40;
	// begin inline asm
	{ atom.add.f64 %fd7013,[%rd2685],%fd10387; }

	// end inline asm
	add.s64 	%rd2686, %rd2680, 48;
	// begin inline asm
	{ atom.add.f64 %fd7015,[%rd2686],%fd10377; }

	// end inline asm
	add.s64 	%rd2687, %rd2680, 56;
	// begin inline asm
	{ atom.add.f64 %fd7017,[%rd2687],%fd10376; }

	// end inline asm
	add.s64 	%rd2688, %rd2680, 64;
	// begin inline asm
	{ atom.add.f64 %fd7019,[%rd2688],%fd10375; }

	// end inline asm

$L__BB1_587:
	ld.param.u64 	%rd335, [%rd25];
	ld.param.u32 	%r980, [%rd25+32];
	ld.param.u32 	%r981, [%rd25+60];
	add.s32 	%r982, %r940, 14;
	setp.le.s32 	%p887, %r981, %r982;
	selp.u16 	%rs257, 1, 0, %p887;
	shr.u32 	%r1635, %r982, 31;
	cvt.u16.u32 	%rs31, %r1635;
	or.b16  	%rs258, %rs257, %rs31;
	setp.eq.s16 	%p888, %rs258, 0;
	@%p888 bra 	$L__BB1_589;

	add.s32 	%r2006, %r940, 14;
	st.local.v2.u32 	[%rd26], {%r2006, %r981};
	mov.u64 	%rd2690, $str;
	cvta.global.u64 	%rd2691, %rd2690;
	{ // callseq 490, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2691;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1636, [retval0+0];
	} // callseq 490
	bra.uni 	$L__BB1_590;

$L__BB1_589:
	mul.wide.s32 	%rd2702, %r980, %r982;
	add.s64 	%rd2693, %rd335, %rd2702;
	// begin inline asm
	{ atom.add.f64 %fd7021,[%rd2693],%fd10398; }

	// end inline asm
	add.s64 	%rd2694, %rd2693, 8;
	// begin inline asm
	{ atom.add.f64 %fd7023,[%rd2694],%fd10397; }

	// end inline asm
	add.s64 	%rd2695, %rd2693, 16;
	// begin inline asm
	{ atom.add.f64 %fd7025,[%rd2695],%fd10396; }

	// end inline asm
	add.s64 	%rd2696, %rd2693, 24;
	// begin inline asm
	{ atom.add.f64 %fd7027,[%rd2696],%fd10386; }

	// end inline asm
	add.s64 	%rd2697, %rd2693, 32;
	// begin inline asm
	{ atom.add.f64 %fd7029,[%rd2697],%fd10385; }

	// end inline asm
	add.s64 	%rd2698, %rd2693, 40;
	// begin inline asm
	{ atom.add.f64 %fd7031,[%rd2698],%fd10384; }

	// end inline asm
	add.s64 	%rd2699, %rd2693, 48;
	// begin inline asm
	{ atom.add.f64 %fd7033,[%rd2699],%fd10374; }

	// end inline asm
	add.s64 	%rd2700, %rd2693, 56;
	// begin inline asm
	{ atom.add.f64 %fd7035,[%rd2700],%fd10373; }

	// end inline asm
	add.s64 	%rd2701, %rd2693, 64;
	// begin inline asm
	{ atom.add.f64 %fd7037,[%rd2701],%fd10372; }

	// end inline asm

$L__BB1_590:
	ld.param.u64 	%rd336, [%rd25];
	ld.param.u32 	%r983, [%rd25+32];
	ld.param.u32 	%r984, [%rd25+60];
	add.s32 	%r985, %r940, 15;
	setp.le.s32 	%p889, %r984, %r985;
	selp.u16 	%rs259, 1, 0, %p889;
	shr.u32 	%r1637, %r985, 31;
	cvt.u16.u32 	%rs32, %r1637;
	or.b16  	%rs260, %rs259, %rs32;
	setp.eq.s16 	%p890, %rs260, 0;
	@%p890 bra 	$L__BB1_592;

	add.s32 	%r2007, %r940, 15;
	st.local.v2.u32 	[%rd26], {%r2007, %r984};
	mov.u64 	%rd2703, $str;
	cvta.global.u64 	%rd2704, %rd2703;
	{ // callseq 491, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2704;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1638, [retval0+0];
	} // callseq 491
	bra.uni 	$L__BB1_593;

$L__BB1_592:
	mul.wide.s32 	%rd2715, %r983, %r985;
	add.s64 	%rd2706, %rd336, %rd2715;
	// begin inline asm
	{ atom.add.f64 %fd7039,[%rd2706],%fd10395; }

	// end inline asm
	add.s64 	%rd2707, %rd2706, 8;
	// begin inline asm
	{ atom.add.f64 %fd7041,[%rd2707],%fd10394; }

	// end inline asm
	add.s64 	%rd2708, %rd2706, 16;
	// begin inline asm
	{ atom.add.f64 %fd7043,[%rd2708],%fd10393; }

	// end inline asm
	add.s64 	%rd2709, %rd2706, 24;
	// begin inline asm
	{ atom.add.f64 %fd7045,[%rd2709],%fd10383; }

	// end inline asm
	add.s64 	%rd2710, %rd2706, 32;
	// begin inline asm
	{ atom.add.f64 %fd7047,[%rd2710],%fd10382; }

	// end inline asm
	add.s64 	%rd2711, %rd2706, 40;
	// begin inline asm
	{ atom.add.f64 %fd7049,[%rd2711],%fd10381; }

	// end inline asm
	add.s64 	%rd2712, %rd2706, 48;
	// begin inline asm
	{ atom.add.f64 %fd7051,[%rd2712],%fd10371; }

	// end inline asm
	add.s64 	%rd2713, %rd2706, 56;
	// begin inline asm
	{ atom.add.f64 %fd7053,[%rd2713],%fd10370; }

	// end inline asm
	add.s64 	%rd2714, %rd2706, 64;
	// begin inline asm
	{ atom.add.f64 %fd7055,[%rd2714],%fd10369; }

	// end inline asm

$L__BB1_593:
	ld.param.u64 	%rd337, [%rd25+8];
	ld.param.u32 	%r986, [%rd25+32];
	ld.param.u32 	%r987, [%rd25+60];
	setp.le.s32 	%p891, %r987, %r985;
	selp.u16 	%rs261, 1, 0, %p891;
	or.b16  	%rs262, %rs261, %rs32;
	setp.eq.s16 	%p892, %rs262, 0;
	mov.f64 	%fd11377, 0d0000000000000000;
	mov.f64 	%fd11378, 0d0000000000000000;
	mov.f64 	%fd11379, 0d0000000000000000;
	mov.f64 	%fd11380, 0d0000000000000000;
	mov.f64 	%fd11381, 0d0000000000000000;
	mov.f64 	%fd11382, 0d0000000000000000;
	mov.f64 	%fd11383, 0d0000000000000000;
	mov.f64 	%fd11384, 0d0000000000000000;
	mov.f64 	%fd11385, 0d0000000000000000;
	@%p892 bra 	$L__BB1_595;

	add.s32 	%r2008, %r940, 15;
	st.local.v2.u32 	[%rd26], {%r2008, %r987};
	mov.u64 	%rd2716, $str;
	cvta.global.u64 	%rd2717, %rd2716;
	{ // callseq 492, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2717;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1639, [retval0+0];
	} // callseq 492
	bra.uni 	$L__BB1_597;

$L__BB1_595:
	setp.eq.s64 	%p893, %rd337, 0;
	@%p893 bra 	$L__BB1_597;

	cvta.to.global.u64 	%rd2719, %rd337;
	mul.wide.s32 	%rd2720, %r986, %r985;
	add.s64 	%rd2721, %rd2719, %rd2720;
	ld.global.f64 	%fd7075, [%rd2721];
	add.f64 	%fd11377, %fd7075, 0d0000000000000000;
	ld.global.f64 	%fd7076, [%rd2721+8];
	add.f64 	%fd11378, %fd7076, 0d0000000000000000;
	ld.global.f64 	%fd7077, [%rd2721+16];
	add.f64 	%fd11379, %fd7077, 0d0000000000000000;
	ld.global.f64 	%fd7078, [%rd2721+24];
	add.f64 	%fd11380, %fd7078, 0d0000000000000000;
	ld.global.f64 	%fd7079, [%rd2721+32];
	add.f64 	%fd11381, %fd7079, 0d0000000000000000;
	ld.global.f64 	%fd7080, [%rd2721+40];
	add.f64 	%fd11382, %fd7080, 0d0000000000000000;
	ld.global.f64 	%fd7081, [%rd2721+48];
	add.f64 	%fd11383, %fd7081, 0d0000000000000000;
	ld.global.f64 	%fd7082, [%rd2721+56];
	add.f64 	%fd11384, %fd7082, 0d0000000000000000;
	ld.global.f64 	%fd7083, [%rd2721+64];
	add.f64 	%fd11385, %fd7083, 0d0000000000000000;

$L__BB1_597:
	add.f64 	%fd2467, %fd11385, 0d0000000000000000;
	add.f64 	%fd2468, %fd11384, 0d0000000000000000;
	add.f64 	%fd2469, %fd11383, 0d0000000000000000;
	add.f64 	%fd2470, %fd11382, 0d0000000000000000;
	add.f64 	%fd2471, %fd11381, 0d0000000000000000;
	add.f64 	%fd2472, %fd11380, 0d0000000000000000;
	add.f64 	%fd2473, %fd11379, 0d0000000000000000;
	add.f64 	%fd2474, %fd11378, 0d0000000000000000;
	add.f64 	%fd2475, %fd11377, 0d0000000000000000;
	ld.param.u64 	%rd338, [%rd25+8];
	ld.param.u32 	%r988, [%rd25+32];
	ld.param.u32 	%r989, [%rd25+60];
	setp.le.s32 	%p894, %r989, %r982;
	selp.u16 	%rs263, 1, 0, %p894;
	or.b16  	%rs264, %rs263, %rs31;
	setp.eq.s16 	%p895, %rs264, 0;
	mov.f64 	%fd11386, 0d0000000000000000;
	mov.f64 	%fd11387, 0d0000000000000000;
	mov.f64 	%fd11388, 0d0000000000000000;
	mov.f64 	%fd11389, 0d0000000000000000;
	mov.f64 	%fd11390, 0d0000000000000000;
	mov.f64 	%fd11391, 0d0000000000000000;
	mov.f64 	%fd11392, 0d0000000000000000;
	mov.f64 	%fd11393, 0d0000000000000000;
	mov.f64 	%fd11394, 0d0000000000000000;
	@%p895 bra 	$L__BB1_599;

	add.s32 	%r2009, %r940, 14;
	st.local.v2.u32 	[%rd26], {%r2009, %r989};
	mov.u64 	%rd2722, $str;
	cvta.global.u64 	%rd2723, %rd2722;
	{ // callseq 493, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2723;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1640, [retval0+0];
	} // callseq 493
	bra.uni 	$L__BB1_601;

$L__BB1_599:
	setp.eq.s64 	%p896, %rd338, 0;
	@%p896 bra 	$L__BB1_601;

	cvta.to.global.u64 	%rd2725, %rd338;
	mul.wide.s32 	%rd2726, %r988, %r982;
	add.s64 	%rd2727, %rd2725, %rd2726;
	ld.global.f64 	%fd7102, [%rd2727];
	add.f64 	%fd11386, %fd7102, 0d0000000000000000;
	ld.global.f64 	%fd7103, [%rd2727+8];
	add.f64 	%fd11387, %fd7103, 0d0000000000000000;
	ld.global.f64 	%fd7104, [%rd2727+16];
	add.f64 	%fd11388, %fd7104, 0d0000000000000000;
	ld.global.f64 	%fd7105, [%rd2727+24];
	add.f64 	%fd11389, %fd7105, 0d0000000000000000;
	ld.global.f64 	%fd7106, [%rd2727+32];
	add.f64 	%fd11390, %fd7106, 0d0000000000000000;
	ld.global.f64 	%fd7107, [%rd2727+40];
	add.f64 	%fd11391, %fd7107, 0d0000000000000000;
	ld.global.f64 	%fd7108, [%rd2727+48];
	add.f64 	%fd11392, %fd7108, 0d0000000000000000;
	ld.global.f64 	%fd7109, [%rd2727+56];
	add.f64 	%fd11393, %fd7109, 0d0000000000000000;
	ld.global.f64 	%fd7110, [%rd2727+64];
	add.f64 	%fd11394, %fd7110, 0d0000000000000000;

$L__BB1_601:
	add.f64 	%fd2494, %fd11394, 0d0000000000000000;
	add.f64 	%fd2495, %fd11393, 0d0000000000000000;
	add.f64 	%fd2496, %fd11392, 0d0000000000000000;
	add.f64 	%fd2497, %fd11391, 0d0000000000000000;
	add.f64 	%fd2498, %fd11390, 0d0000000000000000;
	add.f64 	%fd2499, %fd11389, 0d0000000000000000;
	add.f64 	%fd2500, %fd11388, 0d0000000000000000;
	add.f64 	%fd2501, %fd11387, 0d0000000000000000;
	add.f64 	%fd2502, %fd11386, 0d0000000000000000;
	ld.param.u64 	%rd339, [%rd25+8];
	ld.param.u32 	%r990, [%rd25+32];
	ld.param.u32 	%r991, [%rd25+60];
	setp.le.s32 	%p897, %r991, %r979;
	selp.u16 	%rs265, 1, 0, %p897;
	or.b16  	%rs266, %rs265, %rs30;
	setp.eq.s16 	%p898, %rs266, 0;
	mov.f64 	%fd11395, 0d0000000000000000;
	mov.f64 	%fd11396, 0d0000000000000000;
	mov.f64 	%fd11397, 0d0000000000000000;
	mov.f64 	%fd11398, 0d0000000000000000;
	mov.f64 	%fd11399, 0d0000000000000000;
	mov.f64 	%fd11400, 0d0000000000000000;
	mov.f64 	%fd11401, 0d0000000000000000;
	mov.f64 	%fd11402, 0d0000000000000000;
	mov.f64 	%fd11403, 0d0000000000000000;
	@%p898 bra 	$L__BB1_603;

	add.s32 	%r2010, %r940, 13;
	st.local.v2.u32 	[%rd26], {%r2010, %r991};
	mov.u64 	%rd2728, $str;
	cvta.global.u64 	%rd2729, %rd2728;
	{ // callseq 494, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2729;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1641, [retval0+0];
	} // callseq 494
	bra.uni 	$L__BB1_605;

$L__BB1_603:
	setp.eq.s64 	%p899, %rd339, 0;
	@%p899 bra 	$L__BB1_605;

	cvta.to.global.u64 	%rd2731, %rd339;
	mul.wide.s32 	%rd2732, %r990, %r979;
	add.s64 	%rd2733, %rd2731, %rd2732;
	ld.global.f64 	%fd7129, [%rd2733];
	add.f64 	%fd11395, %fd7129, 0d0000000000000000;
	ld.global.f64 	%fd7130, [%rd2733+8];
	add.f64 	%fd11396, %fd7130, 0d0000000000000000;
	ld.global.f64 	%fd7131, [%rd2733+16];
	add.f64 	%fd11397, %fd7131, 0d0000000000000000;
	ld.global.f64 	%fd7132, [%rd2733+24];
	add.f64 	%fd11398, %fd7132, 0d0000000000000000;
	ld.global.f64 	%fd7133, [%rd2733+32];
	add.f64 	%fd11399, %fd7133, 0d0000000000000000;
	ld.global.f64 	%fd7134, [%rd2733+40];
	add.f64 	%fd11400, %fd7134, 0d0000000000000000;
	ld.global.f64 	%fd7135, [%rd2733+48];
	add.f64 	%fd11401, %fd7135, 0d0000000000000000;
	ld.global.f64 	%fd7136, [%rd2733+56];
	add.f64 	%fd11402, %fd7136, 0d0000000000000000;
	ld.global.f64 	%fd7137, [%rd2733+64];
	add.f64 	%fd11403, %fd7137, 0d0000000000000000;

$L__BB1_605:
	add.f64 	%fd2521, %fd11403, 0d0000000000000000;
	add.f64 	%fd2522, %fd11402, 0d0000000000000000;
	add.f64 	%fd2523, %fd11401, 0d0000000000000000;
	add.f64 	%fd2524, %fd11400, 0d0000000000000000;
	add.f64 	%fd2525, %fd11399, 0d0000000000000000;
	add.f64 	%fd2526, %fd11398, 0d0000000000000000;
	add.f64 	%fd2527, %fd11397, 0d0000000000000000;
	add.f64 	%fd2528, %fd11396, 0d0000000000000000;
	add.f64 	%fd2529, %fd11395, 0d0000000000000000;
	ld.param.u64 	%rd340, [%rd25+8];
	ld.param.u32 	%r992, [%rd25+32];
	ld.param.u32 	%r993, [%rd25+60];
	setp.le.s32 	%p900, %r993, %r976;
	selp.u16 	%rs267, 1, 0, %p900;
	or.b16  	%rs268, %rs267, %rs29;
	setp.eq.s16 	%p901, %rs268, 0;
	mov.f64 	%fd11404, 0d0000000000000000;
	mov.f64 	%fd11405, 0d0000000000000000;
	mov.f64 	%fd11406, 0d0000000000000000;
	mov.f64 	%fd11407, 0d0000000000000000;
	mov.f64 	%fd11408, 0d0000000000000000;
	mov.f64 	%fd11409, 0d0000000000000000;
	mov.f64 	%fd11410, 0d0000000000000000;
	mov.f64 	%fd11411, 0d0000000000000000;
	mov.f64 	%fd11412, 0d0000000000000000;
	@%p901 bra 	$L__BB1_607;

	add.s32 	%r2011, %r940, 12;
	st.local.v2.u32 	[%rd26], {%r2011, %r993};
	mov.u64 	%rd2734, $str;
	cvta.global.u64 	%rd2735, %rd2734;
	{ // callseq 495, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2735;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1642, [retval0+0];
	} // callseq 495
	bra.uni 	$L__BB1_609;

$L__BB1_607:
	setp.eq.s64 	%p902, %rd340, 0;
	@%p902 bra 	$L__BB1_609;

	cvta.to.global.u64 	%rd2737, %rd340;
	mul.wide.s32 	%rd2738, %r992, %r976;
	add.s64 	%rd2739, %rd2737, %rd2738;
	ld.global.f64 	%fd7156, [%rd2739];
	add.f64 	%fd11404, %fd7156, 0d0000000000000000;
	ld.global.f64 	%fd7157, [%rd2739+8];
	add.f64 	%fd11405, %fd7157, 0d0000000000000000;
	ld.global.f64 	%fd7158, [%rd2739+16];
	add.f64 	%fd11406, %fd7158, 0d0000000000000000;
	ld.global.f64 	%fd7159, [%rd2739+24];
	add.f64 	%fd11407, %fd7159, 0d0000000000000000;
	ld.global.f64 	%fd7160, [%rd2739+32];
	add.f64 	%fd11408, %fd7160, 0d0000000000000000;
	ld.global.f64 	%fd7161, [%rd2739+40];
	add.f64 	%fd11409, %fd7161, 0d0000000000000000;
	ld.global.f64 	%fd7162, [%rd2739+48];
	add.f64 	%fd11410, %fd7162, 0d0000000000000000;
	ld.global.f64 	%fd7163, [%rd2739+56];
	add.f64 	%fd11411, %fd7163, 0d0000000000000000;
	ld.global.f64 	%fd7164, [%rd2739+64];
	add.f64 	%fd11412, %fd7164, 0d0000000000000000;

$L__BB1_609:
	add.f64 	%fd2548, %fd11412, 0d0000000000000000;
	add.f64 	%fd2549, %fd11411, 0d0000000000000000;
	add.f64 	%fd2550, %fd11410, 0d0000000000000000;
	add.f64 	%fd2551, %fd11409, 0d0000000000000000;
	add.f64 	%fd2552, %fd11408, 0d0000000000000000;
	add.f64 	%fd2553, %fd11407, 0d0000000000000000;
	add.f64 	%fd2554, %fd11406, 0d0000000000000000;
	add.f64 	%fd2555, %fd11405, 0d0000000000000000;
	add.f64 	%fd2556, %fd11404, 0d0000000000000000;
	ld.param.u64 	%rd341, [%rd25+8];
	ld.param.u32 	%r994, [%rd25+32];
	ld.param.u32 	%r995, [%rd25+60];
	setp.le.s32 	%p903, %r995, %r973;
	selp.u16 	%rs269, 1, 0, %p903;
	or.b16  	%rs270, %rs269, %rs28;
	setp.eq.s16 	%p904, %rs270, 0;
	mov.f64 	%fd11413, 0d0000000000000000;
	mov.f64 	%fd11414, 0d0000000000000000;
	mov.f64 	%fd11415, 0d0000000000000000;
	mov.f64 	%fd11416, 0d0000000000000000;
	mov.f64 	%fd11417, 0d0000000000000000;
	mov.f64 	%fd11418, 0d0000000000000000;
	mov.f64 	%fd11419, 0d0000000000000000;
	mov.f64 	%fd11420, 0d0000000000000000;
	mov.f64 	%fd11421, 0d0000000000000000;
	@%p904 bra 	$L__BB1_611;

	add.s32 	%r2012, %r940, 11;
	st.local.v2.u32 	[%rd26], {%r2012, %r995};
	mov.u64 	%rd2740, $str;
	cvta.global.u64 	%rd2741, %rd2740;
	{ // callseq 496, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2741;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1643, [retval0+0];
	} // callseq 496
	bra.uni 	$L__BB1_613;

$L__BB1_611:
	setp.eq.s64 	%p905, %rd341, 0;
	@%p905 bra 	$L__BB1_613;

	cvta.to.global.u64 	%rd2743, %rd341;
	mul.wide.s32 	%rd2744, %r994, %r973;
	add.s64 	%rd2745, %rd2743, %rd2744;
	ld.global.f64 	%fd7183, [%rd2745];
	add.f64 	%fd11413, %fd7183, 0d0000000000000000;
	ld.global.f64 	%fd7184, [%rd2745+8];
	add.f64 	%fd11414, %fd7184, 0d0000000000000000;
	ld.global.f64 	%fd7185, [%rd2745+16];
	add.f64 	%fd11415, %fd7185, 0d0000000000000000;
	ld.global.f64 	%fd7186, [%rd2745+24];
	add.f64 	%fd11416, %fd7186, 0d0000000000000000;
	ld.global.f64 	%fd7187, [%rd2745+32];
	add.f64 	%fd11417, %fd7187, 0d0000000000000000;
	ld.global.f64 	%fd7188, [%rd2745+40];
	add.f64 	%fd11418, %fd7188, 0d0000000000000000;
	ld.global.f64 	%fd7189, [%rd2745+48];
	add.f64 	%fd11419, %fd7189, 0d0000000000000000;
	ld.global.f64 	%fd7190, [%rd2745+56];
	add.f64 	%fd11420, %fd7190, 0d0000000000000000;
	ld.global.f64 	%fd7191, [%rd2745+64];
	add.f64 	%fd11421, %fd7191, 0d0000000000000000;

$L__BB1_613:
	add.f64 	%fd2575, %fd11421, 0d0000000000000000;
	add.f64 	%fd2576, %fd11420, 0d0000000000000000;
	add.f64 	%fd2577, %fd11419, 0d0000000000000000;
	add.f64 	%fd2578, %fd11418, 0d0000000000000000;
	add.f64 	%fd2579, %fd11417, 0d0000000000000000;
	add.f64 	%fd2580, %fd11416, 0d0000000000000000;
	add.f64 	%fd2581, %fd11415, 0d0000000000000000;
	add.f64 	%fd2582, %fd11414, 0d0000000000000000;
	add.f64 	%fd2583, %fd11413, 0d0000000000000000;
	ld.param.u64 	%rd342, [%rd25+8];
	ld.param.u32 	%r996, [%rd25+32];
	ld.param.u32 	%r997, [%rd25+60];
	setp.le.s32 	%p906, %r997, %r970;
	selp.u16 	%rs271, 1, 0, %p906;
	or.b16  	%rs272, %rs271, %rs27;
	setp.eq.s16 	%p907, %rs272, 0;
	mov.f64 	%fd11422, 0d0000000000000000;
	mov.f64 	%fd11423, 0d0000000000000000;
	mov.f64 	%fd11424, 0d0000000000000000;
	mov.f64 	%fd11425, 0d0000000000000000;
	mov.f64 	%fd11426, 0d0000000000000000;
	mov.f64 	%fd11427, 0d0000000000000000;
	mov.f64 	%fd11428, 0d0000000000000000;
	mov.f64 	%fd11429, 0d0000000000000000;
	mov.f64 	%fd11430, 0d0000000000000000;
	@%p907 bra 	$L__BB1_615;

	add.s32 	%r2013, %r940, 10;
	st.local.v2.u32 	[%rd26], {%r2013, %r997};
	mov.u64 	%rd2746, $str;
	cvta.global.u64 	%rd2747, %rd2746;
	{ // callseq 497, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2747;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1644, [retval0+0];
	} // callseq 497
	bra.uni 	$L__BB1_617;

$L__BB1_615:
	setp.eq.s64 	%p908, %rd342, 0;
	@%p908 bra 	$L__BB1_617;

	cvta.to.global.u64 	%rd2749, %rd342;
	mul.wide.s32 	%rd2750, %r996, %r970;
	add.s64 	%rd2751, %rd2749, %rd2750;
	ld.global.f64 	%fd7210, [%rd2751];
	add.f64 	%fd11422, %fd7210, 0d0000000000000000;
	ld.global.f64 	%fd7211, [%rd2751+8];
	add.f64 	%fd11423, %fd7211, 0d0000000000000000;
	ld.global.f64 	%fd7212, [%rd2751+16];
	add.f64 	%fd11424, %fd7212, 0d0000000000000000;
	ld.global.f64 	%fd7213, [%rd2751+24];
	add.f64 	%fd11425, %fd7213, 0d0000000000000000;
	ld.global.f64 	%fd7214, [%rd2751+32];
	add.f64 	%fd11426, %fd7214, 0d0000000000000000;
	ld.global.f64 	%fd7215, [%rd2751+40];
	add.f64 	%fd11427, %fd7215, 0d0000000000000000;
	ld.global.f64 	%fd7216, [%rd2751+48];
	add.f64 	%fd11428, %fd7216, 0d0000000000000000;
	ld.global.f64 	%fd7217, [%rd2751+56];
	add.f64 	%fd11429, %fd7217, 0d0000000000000000;
	ld.global.f64 	%fd7218, [%rd2751+64];
	add.f64 	%fd11430, %fd7218, 0d0000000000000000;

$L__BB1_617:
	add.f64 	%fd2602, %fd11430, 0d0000000000000000;
	add.f64 	%fd2603, %fd11429, 0d0000000000000000;
	add.f64 	%fd2604, %fd11428, 0d0000000000000000;
	add.f64 	%fd2605, %fd11427, 0d0000000000000000;
	add.f64 	%fd2606, %fd11426, 0d0000000000000000;
	add.f64 	%fd2607, %fd11425, 0d0000000000000000;
	add.f64 	%fd2608, %fd11424, 0d0000000000000000;
	add.f64 	%fd2609, %fd11423, 0d0000000000000000;
	add.f64 	%fd2610, %fd11422, 0d0000000000000000;
	ld.param.u64 	%rd343, [%rd25+8];
	ld.param.u32 	%r998, [%rd25+32];
	ld.param.u32 	%r999, [%rd25+60];
	setp.le.s32 	%p909, %r999, %r967;
	selp.u16 	%rs273, 1, 0, %p909;
	or.b16  	%rs274, %rs273, %rs26;
	setp.eq.s16 	%p910, %rs274, 0;
	mov.f64 	%fd11431, 0d0000000000000000;
	mov.f64 	%fd11432, 0d0000000000000000;
	mov.f64 	%fd11433, 0d0000000000000000;
	mov.f64 	%fd11434, 0d0000000000000000;
	mov.f64 	%fd11435, 0d0000000000000000;
	mov.f64 	%fd11436, 0d0000000000000000;
	mov.f64 	%fd11437, 0d0000000000000000;
	mov.f64 	%fd11438, 0d0000000000000000;
	mov.f64 	%fd11439, 0d0000000000000000;
	@%p910 bra 	$L__BB1_619;

	add.s32 	%r2014, %r940, 9;
	st.local.v2.u32 	[%rd26], {%r2014, %r999};
	mov.u64 	%rd2752, $str;
	cvta.global.u64 	%rd2753, %rd2752;
	{ // callseq 498, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2753;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1645, [retval0+0];
	} // callseq 498
	bra.uni 	$L__BB1_621;

$L__BB1_619:
	setp.eq.s64 	%p911, %rd343, 0;
	@%p911 bra 	$L__BB1_621;

	cvta.to.global.u64 	%rd2755, %rd343;
	mul.wide.s32 	%rd2756, %r998, %r967;
	add.s64 	%rd2757, %rd2755, %rd2756;
	ld.global.f64 	%fd7237, [%rd2757];
	add.f64 	%fd11431, %fd7237, 0d0000000000000000;
	ld.global.f64 	%fd7238, [%rd2757+8];
	add.f64 	%fd11432, %fd7238, 0d0000000000000000;
	ld.global.f64 	%fd7239, [%rd2757+16];
	add.f64 	%fd11433, %fd7239, 0d0000000000000000;
	ld.global.f64 	%fd7240, [%rd2757+24];
	add.f64 	%fd11434, %fd7240, 0d0000000000000000;
	ld.global.f64 	%fd7241, [%rd2757+32];
	add.f64 	%fd11435, %fd7241, 0d0000000000000000;
	ld.global.f64 	%fd7242, [%rd2757+40];
	add.f64 	%fd11436, %fd7242, 0d0000000000000000;
	ld.global.f64 	%fd7243, [%rd2757+48];
	add.f64 	%fd11437, %fd7243, 0d0000000000000000;
	ld.global.f64 	%fd7244, [%rd2757+56];
	add.f64 	%fd11438, %fd7244, 0d0000000000000000;
	ld.global.f64 	%fd7245, [%rd2757+64];
	add.f64 	%fd11439, %fd7245, 0d0000000000000000;

$L__BB1_621:
	add.f64 	%fd2629, %fd11439, 0d0000000000000000;
	add.f64 	%fd2630, %fd11438, 0d0000000000000000;
	add.f64 	%fd2631, %fd11437, 0d0000000000000000;
	add.f64 	%fd2632, %fd11436, 0d0000000000000000;
	add.f64 	%fd2633, %fd11435, 0d0000000000000000;
	add.f64 	%fd2634, %fd11434, 0d0000000000000000;
	add.f64 	%fd2635, %fd11433, 0d0000000000000000;
	add.f64 	%fd2636, %fd11432, 0d0000000000000000;
	add.f64 	%fd2637, %fd11431, 0d0000000000000000;
	ld.param.u64 	%rd344, [%rd25+8];
	ld.param.u32 	%r1000, [%rd25+32];
	ld.param.u32 	%r1001, [%rd25+60];
	setp.le.s32 	%p912, %r1001, %r964;
	selp.u16 	%rs275, 1, 0, %p912;
	or.b16  	%rs276, %rs275, %rs25;
	setp.eq.s16 	%p913, %rs276, 0;
	mov.f64 	%fd11440, 0d0000000000000000;
	mov.f64 	%fd11441, 0d0000000000000000;
	mov.f64 	%fd11442, 0d0000000000000000;
	mov.f64 	%fd11443, 0d0000000000000000;
	mov.f64 	%fd11444, 0d0000000000000000;
	mov.f64 	%fd11445, 0d0000000000000000;
	mov.f64 	%fd11446, 0d0000000000000000;
	mov.f64 	%fd11447, 0d0000000000000000;
	mov.f64 	%fd11448, 0d0000000000000000;
	@%p913 bra 	$L__BB1_623;

	add.s32 	%r2015, %r940, 8;
	st.local.v2.u32 	[%rd26], {%r2015, %r1001};
	mov.u64 	%rd2758, $str;
	cvta.global.u64 	%rd2759, %rd2758;
	{ // callseq 499, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2759;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1646, [retval0+0];
	} // callseq 499
	bra.uni 	$L__BB1_625;

$L__BB1_623:
	setp.eq.s64 	%p914, %rd344, 0;
	@%p914 bra 	$L__BB1_625;

	cvta.to.global.u64 	%rd2761, %rd344;
	mul.wide.s32 	%rd2762, %r1000, %r964;
	add.s64 	%rd2763, %rd2761, %rd2762;
	ld.global.f64 	%fd7264, [%rd2763];
	add.f64 	%fd11440, %fd7264, 0d0000000000000000;
	ld.global.f64 	%fd7265, [%rd2763+8];
	add.f64 	%fd11441, %fd7265, 0d0000000000000000;
	ld.global.f64 	%fd7266, [%rd2763+16];
	add.f64 	%fd11442, %fd7266, 0d0000000000000000;
	ld.global.f64 	%fd7267, [%rd2763+24];
	add.f64 	%fd11443, %fd7267, 0d0000000000000000;
	ld.global.f64 	%fd7268, [%rd2763+32];
	add.f64 	%fd11444, %fd7268, 0d0000000000000000;
	ld.global.f64 	%fd7269, [%rd2763+40];
	add.f64 	%fd11445, %fd7269, 0d0000000000000000;
	ld.global.f64 	%fd7270, [%rd2763+48];
	add.f64 	%fd11446, %fd7270, 0d0000000000000000;
	ld.global.f64 	%fd7271, [%rd2763+56];
	add.f64 	%fd11447, %fd7271, 0d0000000000000000;
	ld.global.f64 	%fd7272, [%rd2763+64];
	add.f64 	%fd11448, %fd7272, 0d0000000000000000;

$L__BB1_625:
	add.f64 	%fd2656, %fd11448, 0d0000000000000000;
	add.f64 	%fd2657, %fd11447, 0d0000000000000000;
	add.f64 	%fd2658, %fd11446, 0d0000000000000000;
	add.f64 	%fd2659, %fd11445, 0d0000000000000000;
	add.f64 	%fd2660, %fd11444, 0d0000000000000000;
	add.f64 	%fd2661, %fd11443, 0d0000000000000000;
	add.f64 	%fd2662, %fd11442, 0d0000000000000000;
	add.f64 	%fd2663, %fd11441, 0d0000000000000000;
	add.f64 	%fd2664, %fd11440, 0d0000000000000000;
	ld.param.u64 	%rd345, [%rd25+8];
	ld.param.u32 	%r1002, [%rd25+32];
	ld.param.u32 	%r1003, [%rd25+60];
	setp.le.s32 	%p915, %r1003, %r961;
	selp.u16 	%rs277, 1, 0, %p915;
	or.b16  	%rs278, %rs277, %rs24;
	setp.eq.s16 	%p916, %rs278, 0;
	mov.f64 	%fd11449, 0d0000000000000000;
	mov.f64 	%fd11450, 0d0000000000000000;
	mov.f64 	%fd11451, 0d0000000000000000;
	mov.f64 	%fd11452, 0d0000000000000000;
	mov.f64 	%fd11453, 0d0000000000000000;
	mov.f64 	%fd11454, 0d0000000000000000;
	mov.f64 	%fd11455, 0d0000000000000000;
	mov.f64 	%fd11456, 0d0000000000000000;
	mov.f64 	%fd11457, 0d0000000000000000;
	@%p916 bra 	$L__BB1_627;

	add.s32 	%r2016, %r940, 7;
	st.local.v2.u32 	[%rd26], {%r2016, %r1003};
	mov.u64 	%rd2764, $str;
	cvta.global.u64 	%rd2765, %rd2764;
	{ // callseq 500, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2765;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1647, [retval0+0];
	} // callseq 500
	bra.uni 	$L__BB1_629;

$L__BB1_627:
	setp.eq.s64 	%p917, %rd345, 0;
	@%p917 bra 	$L__BB1_629;

	cvta.to.global.u64 	%rd2767, %rd345;
	mul.wide.s32 	%rd2768, %r1002, %r961;
	add.s64 	%rd2769, %rd2767, %rd2768;
	ld.global.f64 	%fd7291, [%rd2769];
	add.f64 	%fd11449, %fd7291, 0d0000000000000000;
	ld.global.f64 	%fd7292, [%rd2769+8];
	add.f64 	%fd11450, %fd7292, 0d0000000000000000;
	ld.global.f64 	%fd7293, [%rd2769+16];
	add.f64 	%fd11451, %fd7293, 0d0000000000000000;
	ld.global.f64 	%fd7294, [%rd2769+24];
	add.f64 	%fd11452, %fd7294, 0d0000000000000000;
	ld.global.f64 	%fd7295, [%rd2769+32];
	add.f64 	%fd11453, %fd7295, 0d0000000000000000;
	ld.global.f64 	%fd7296, [%rd2769+40];
	add.f64 	%fd11454, %fd7296, 0d0000000000000000;
	ld.global.f64 	%fd7297, [%rd2769+48];
	add.f64 	%fd11455, %fd7297, 0d0000000000000000;
	ld.global.f64 	%fd7298, [%rd2769+56];
	add.f64 	%fd11456, %fd7298, 0d0000000000000000;
	ld.global.f64 	%fd7299, [%rd2769+64];
	add.f64 	%fd11457, %fd7299, 0d0000000000000000;

$L__BB1_629:
	add.f64 	%fd2683, %fd11457, 0d0000000000000000;
	add.f64 	%fd2684, %fd11456, 0d0000000000000000;
	add.f64 	%fd2685, %fd11455, 0d0000000000000000;
	add.f64 	%fd2686, %fd11454, 0d0000000000000000;
	add.f64 	%fd2687, %fd11453, 0d0000000000000000;
	add.f64 	%fd2688, %fd11452, 0d0000000000000000;
	add.f64 	%fd2689, %fd11451, 0d0000000000000000;
	add.f64 	%fd2690, %fd11450, 0d0000000000000000;
	add.f64 	%fd2691, %fd11449, 0d0000000000000000;
	ld.param.u64 	%rd346, [%rd25+8];
	ld.param.u32 	%r1004, [%rd25+32];
	ld.param.u32 	%r1005, [%rd25+60];
	setp.le.s32 	%p918, %r1005, %r958;
	selp.u16 	%rs279, 1, 0, %p918;
	or.b16  	%rs280, %rs279, %rs23;
	setp.eq.s16 	%p919, %rs280, 0;
	mov.f64 	%fd11458, 0d0000000000000000;
	mov.f64 	%fd11459, 0d0000000000000000;
	mov.f64 	%fd11460, 0d0000000000000000;
	mov.f64 	%fd11461, 0d0000000000000000;
	mov.f64 	%fd11462, 0d0000000000000000;
	mov.f64 	%fd11463, 0d0000000000000000;
	mov.f64 	%fd11464, 0d0000000000000000;
	mov.f64 	%fd11465, 0d0000000000000000;
	mov.f64 	%fd11466, 0d0000000000000000;
	@%p919 bra 	$L__BB1_631;

	add.s32 	%r2017, %r940, 6;
	st.local.v2.u32 	[%rd26], {%r2017, %r1005};
	mov.u64 	%rd2770, $str;
	cvta.global.u64 	%rd2771, %rd2770;
	{ // callseq 501, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2771;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1648, [retval0+0];
	} // callseq 501
	bra.uni 	$L__BB1_633;

$L__BB1_631:
	setp.eq.s64 	%p920, %rd346, 0;
	@%p920 bra 	$L__BB1_633;

	cvta.to.global.u64 	%rd2773, %rd346;
	mul.wide.s32 	%rd2774, %r1004, %r958;
	add.s64 	%rd2775, %rd2773, %rd2774;
	ld.global.f64 	%fd7318, [%rd2775];
	add.f64 	%fd11458, %fd7318, 0d0000000000000000;
	ld.global.f64 	%fd7319, [%rd2775+8];
	add.f64 	%fd11459, %fd7319, 0d0000000000000000;
	ld.global.f64 	%fd7320, [%rd2775+16];
	add.f64 	%fd11460, %fd7320, 0d0000000000000000;
	ld.global.f64 	%fd7321, [%rd2775+24];
	add.f64 	%fd11461, %fd7321, 0d0000000000000000;
	ld.global.f64 	%fd7322, [%rd2775+32];
	add.f64 	%fd11462, %fd7322, 0d0000000000000000;
	ld.global.f64 	%fd7323, [%rd2775+40];
	add.f64 	%fd11463, %fd7323, 0d0000000000000000;
	ld.global.f64 	%fd7324, [%rd2775+48];
	add.f64 	%fd11464, %fd7324, 0d0000000000000000;
	ld.global.f64 	%fd7325, [%rd2775+56];
	add.f64 	%fd11465, %fd7325, 0d0000000000000000;
	ld.global.f64 	%fd7326, [%rd2775+64];
	add.f64 	%fd11466, %fd7326, 0d0000000000000000;

$L__BB1_633:
	add.f64 	%fd2710, %fd11466, 0d0000000000000000;
	add.f64 	%fd2711, %fd11465, 0d0000000000000000;
	add.f64 	%fd2712, %fd11464, 0d0000000000000000;
	add.f64 	%fd2713, %fd11463, 0d0000000000000000;
	add.f64 	%fd2714, %fd11462, 0d0000000000000000;
	add.f64 	%fd2715, %fd11461, 0d0000000000000000;
	add.f64 	%fd2716, %fd11460, 0d0000000000000000;
	add.f64 	%fd2717, %fd11459, 0d0000000000000000;
	add.f64 	%fd2718, %fd11458, 0d0000000000000000;
	ld.param.u64 	%rd347, [%rd25+8];
	ld.param.u32 	%r1006, [%rd25+32];
	ld.param.u32 	%r1007, [%rd25+60];
	setp.le.s32 	%p921, %r1007, %r955;
	selp.u16 	%rs281, 1, 0, %p921;
	or.b16  	%rs282, %rs281, %rs22;
	setp.eq.s16 	%p922, %rs282, 0;
	mov.f64 	%fd11467, 0d0000000000000000;
	mov.f64 	%fd11468, 0d0000000000000000;
	mov.f64 	%fd11469, 0d0000000000000000;
	mov.f64 	%fd11470, 0d0000000000000000;
	mov.f64 	%fd11471, 0d0000000000000000;
	mov.f64 	%fd11472, 0d0000000000000000;
	mov.f64 	%fd11473, 0d0000000000000000;
	mov.f64 	%fd11474, 0d0000000000000000;
	mov.f64 	%fd11475, 0d0000000000000000;
	@%p922 bra 	$L__BB1_635;

	add.s32 	%r2018, %r940, 5;
	st.local.v2.u32 	[%rd26], {%r2018, %r1007};
	mov.u64 	%rd2776, $str;
	cvta.global.u64 	%rd2777, %rd2776;
	{ // callseq 502, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2777;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1649, [retval0+0];
	} // callseq 502
	bra.uni 	$L__BB1_637;

$L__BB1_635:
	setp.eq.s64 	%p923, %rd347, 0;
	@%p923 bra 	$L__BB1_637;

	cvta.to.global.u64 	%rd2779, %rd347;
	mul.wide.s32 	%rd2780, %r1006, %r955;
	add.s64 	%rd2781, %rd2779, %rd2780;
	ld.global.f64 	%fd7345, [%rd2781];
	add.f64 	%fd11467, %fd7345, 0d0000000000000000;
	ld.global.f64 	%fd7346, [%rd2781+8];
	add.f64 	%fd11468, %fd7346, 0d0000000000000000;
	ld.global.f64 	%fd7347, [%rd2781+16];
	add.f64 	%fd11469, %fd7347, 0d0000000000000000;
	ld.global.f64 	%fd7348, [%rd2781+24];
	add.f64 	%fd11470, %fd7348, 0d0000000000000000;
	ld.global.f64 	%fd7349, [%rd2781+32];
	add.f64 	%fd11471, %fd7349, 0d0000000000000000;
	ld.global.f64 	%fd7350, [%rd2781+40];
	add.f64 	%fd11472, %fd7350, 0d0000000000000000;
	ld.global.f64 	%fd7351, [%rd2781+48];
	add.f64 	%fd11473, %fd7351, 0d0000000000000000;
	ld.global.f64 	%fd7352, [%rd2781+56];
	add.f64 	%fd11474, %fd7352, 0d0000000000000000;
	ld.global.f64 	%fd7353, [%rd2781+64];
	add.f64 	%fd11475, %fd7353, 0d0000000000000000;

$L__BB1_637:
	add.f64 	%fd2737, %fd11475, 0d0000000000000000;
	add.f64 	%fd2738, %fd11474, 0d0000000000000000;
	add.f64 	%fd2739, %fd11473, 0d0000000000000000;
	add.f64 	%fd2740, %fd11472, 0d0000000000000000;
	add.f64 	%fd2741, %fd11471, 0d0000000000000000;
	add.f64 	%fd2742, %fd11470, 0d0000000000000000;
	add.f64 	%fd2743, %fd11469, 0d0000000000000000;
	add.f64 	%fd2744, %fd11468, 0d0000000000000000;
	add.f64 	%fd2745, %fd11467, 0d0000000000000000;
	ld.param.u64 	%rd348, [%rd25+8];
	ld.param.u32 	%r1008, [%rd25+32];
	ld.param.u32 	%r1009, [%rd25+60];
	setp.le.s32 	%p924, %r1009, %r952;
	selp.u16 	%rs283, 1, 0, %p924;
	or.b16  	%rs284, %rs283, %rs21;
	setp.eq.s16 	%p925, %rs284, 0;
	mov.f64 	%fd11476, 0d0000000000000000;
	mov.f64 	%fd11477, 0d0000000000000000;
	mov.f64 	%fd11478, 0d0000000000000000;
	mov.f64 	%fd11479, 0d0000000000000000;
	mov.f64 	%fd11480, 0d0000000000000000;
	mov.f64 	%fd11481, 0d0000000000000000;
	mov.f64 	%fd11482, 0d0000000000000000;
	mov.f64 	%fd11483, 0d0000000000000000;
	mov.f64 	%fd11484, 0d0000000000000000;
	@%p925 bra 	$L__BB1_639;

	add.s32 	%r2019, %r940, 4;
	st.local.v2.u32 	[%rd26], {%r2019, %r1009};
	mov.u64 	%rd2782, $str;
	cvta.global.u64 	%rd2783, %rd2782;
	{ // callseq 503, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2783;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1650, [retval0+0];
	} // callseq 503
	bra.uni 	$L__BB1_641;

$L__BB1_639:
	setp.eq.s64 	%p926, %rd348, 0;
	@%p926 bra 	$L__BB1_641;

	cvta.to.global.u64 	%rd2785, %rd348;
	mul.wide.s32 	%rd2786, %r1008, %r952;
	add.s64 	%rd2787, %rd2785, %rd2786;
	ld.global.f64 	%fd7372, [%rd2787];
	add.f64 	%fd11476, %fd7372, 0d0000000000000000;
	ld.global.f64 	%fd7373, [%rd2787+8];
	add.f64 	%fd11477, %fd7373, 0d0000000000000000;
	ld.global.f64 	%fd7374, [%rd2787+16];
	add.f64 	%fd11478, %fd7374, 0d0000000000000000;
	ld.global.f64 	%fd7375, [%rd2787+24];
	add.f64 	%fd11479, %fd7375, 0d0000000000000000;
	ld.global.f64 	%fd7376, [%rd2787+32];
	add.f64 	%fd11480, %fd7376, 0d0000000000000000;
	ld.global.f64 	%fd7377, [%rd2787+40];
	add.f64 	%fd11481, %fd7377, 0d0000000000000000;
	ld.global.f64 	%fd7378, [%rd2787+48];
	add.f64 	%fd11482, %fd7378, 0d0000000000000000;
	ld.global.f64 	%fd7379, [%rd2787+56];
	add.f64 	%fd11483, %fd7379, 0d0000000000000000;
	ld.global.f64 	%fd7380, [%rd2787+64];
	add.f64 	%fd11484, %fd7380, 0d0000000000000000;

$L__BB1_641:
	add.f64 	%fd2764, %fd11484, 0d0000000000000000;
	add.f64 	%fd2765, %fd11483, 0d0000000000000000;
	add.f64 	%fd2766, %fd11482, 0d0000000000000000;
	add.f64 	%fd2767, %fd11481, 0d0000000000000000;
	add.f64 	%fd2768, %fd11480, 0d0000000000000000;
	add.f64 	%fd2769, %fd11479, 0d0000000000000000;
	add.f64 	%fd2770, %fd11478, 0d0000000000000000;
	add.f64 	%fd2771, %fd11477, 0d0000000000000000;
	add.f64 	%fd2772, %fd11476, 0d0000000000000000;
	ld.param.u64 	%rd349, [%rd25+8];
	ld.param.u32 	%r1010, [%rd25+32];
	ld.param.u32 	%r1011, [%rd25+60];
	setp.le.s32 	%p927, %r1011, %r949;
	selp.u16 	%rs285, 1, 0, %p927;
	or.b16  	%rs286, %rs285, %rs20;
	setp.eq.s16 	%p928, %rs286, 0;
	mov.f64 	%fd11485, 0d0000000000000000;
	mov.f64 	%fd11486, 0d0000000000000000;
	mov.f64 	%fd11487, 0d0000000000000000;
	mov.f64 	%fd11488, 0d0000000000000000;
	mov.f64 	%fd11489, 0d0000000000000000;
	mov.f64 	%fd11490, 0d0000000000000000;
	mov.f64 	%fd11491, 0d0000000000000000;
	mov.f64 	%fd11492, 0d0000000000000000;
	mov.f64 	%fd11493, 0d0000000000000000;
	@%p928 bra 	$L__BB1_643;

	add.s32 	%r2020, %r940, 3;
	st.local.v2.u32 	[%rd26], {%r2020, %r1011};
	mov.u64 	%rd2788, $str;
	cvta.global.u64 	%rd2789, %rd2788;
	{ // callseq 504, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2789;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1651, [retval0+0];
	} // callseq 504
	bra.uni 	$L__BB1_645;

$L__BB1_643:
	setp.eq.s64 	%p929, %rd349, 0;
	@%p929 bra 	$L__BB1_645;

	cvta.to.global.u64 	%rd2791, %rd349;
	mul.wide.s32 	%rd2792, %r1010, %r949;
	add.s64 	%rd2793, %rd2791, %rd2792;
	ld.global.f64 	%fd7399, [%rd2793];
	add.f64 	%fd11485, %fd7399, 0d0000000000000000;
	ld.global.f64 	%fd7400, [%rd2793+8];
	add.f64 	%fd11486, %fd7400, 0d0000000000000000;
	ld.global.f64 	%fd7401, [%rd2793+16];
	add.f64 	%fd11487, %fd7401, 0d0000000000000000;
	ld.global.f64 	%fd7402, [%rd2793+24];
	add.f64 	%fd11488, %fd7402, 0d0000000000000000;
	ld.global.f64 	%fd7403, [%rd2793+32];
	add.f64 	%fd11489, %fd7403, 0d0000000000000000;
	ld.global.f64 	%fd7404, [%rd2793+40];
	add.f64 	%fd11490, %fd7404, 0d0000000000000000;
	ld.global.f64 	%fd7405, [%rd2793+48];
	add.f64 	%fd11491, %fd7405, 0d0000000000000000;
	ld.global.f64 	%fd7406, [%rd2793+56];
	add.f64 	%fd11492, %fd7406, 0d0000000000000000;
	ld.global.f64 	%fd7407, [%rd2793+64];
	add.f64 	%fd11493, %fd7407, 0d0000000000000000;

$L__BB1_645:
	add.f64 	%fd2791, %fd11493, 0d0000000000000000;
	add.f64 	%fd2792, %fd11492, 0d0000000000000000;
	add.f64 	%fd2793, %fd11491, 0d0000000000000000;
	add.f64 	%fd2794, %fd11490, 0d0000000000000000;
	add.f64 	%fd2795, %fd11489, 0d0000000000000000;
	add.f64 	%fd2796, %fd11488, 0d0000000000000000;
	add.f64 	%fd2797, %fd11487, 0d0000000000000000;
	add.f64 	%fd2798, %fd11486, 0d0000000000000000;
	add.f64 	%fd2799, %fd11485, 0d0000000000000000;
	ld.param.u64 	%rd350, [%rd25+8];
	ld.param.u32 	%r1012, [%rd25+32];
	ld.param.u32 	%r1013, [%rd25+60];
	setp.le.s32 	%p930, %r1013, %r946;
	selp.u16 	%rs287, 1, 0, %p930;
	or.b16  	%rs288, %rs287, %rs19;
	setp.eq.s16 	%p931, %rs288, 0;
	mov.f64 	%fd11494, 0d0000000000000000;
	mov.f64 	%fd11495, 0d0000000000000000;
	mov.f64 	%fd11496, 0d0000000000000000;
	mov.f64 	%fd11497, 0d0000000000000000;
	mov.f64 	%fd11498, 0d0000000000000000;
	mov.f64 	%fd11499, 0d0000000000000000;
	mov.f64 	%fd11500, 0d0000000000000000;
	mov.f64 	%fd11501, 0d0000000000000000;
	mov.f64 	%fd11502, 0d0000000000000000;
	@%p931 bra 	$L__BB1_647;

	add.s32 	%r2021, %r940, 2;
	st.local.v2.u32 	[%rd26], {%r2021, %r1013};
	mov.u64 	%rd2794, $str;
	cvta.global.u64 	%rd2795, %rd2794;
	{ // callseq 505, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2795;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1652, [retval0+0];
	} // callseq 505
	bra.uni 	$L__BB1_649;

$L__BB1_647:
	setp.eq.s64 	%p932, %rd350, 0;
	@%p932 bra 	$L__BB1_649;

	cvta.to.global.u64 	%rd2797, %rd350;
	mul.wide.s32 	%rd2798, %r1012, %r946;
	add.s64 	%rd2799, %rd2797, %rd2798;
	ld.global.f64 	%fd7426, [%rd2799];
	add.f64 	%fd11494, %fd7426, 0d0000000000000000;
	ld.global.f64 	%fd7427, [%rd2799+8];
	add.f64 	%fd11495, %fd7427, 0d0000000000000000;
	ld.global.f64 	%fd7428, [%rd2799+16];
	add.f64 	%fd11496, %fd7428, 0d0000000000000000;
	ld.global.f64 	%fd7429, [%rd2799+24];
	add.f64 	%fd11497, %fd7429, 0d0000000000000000;
	ld.global.f64 	%fd7430, [%rd2799+32];
	add.f64 	%fd11498, %fd7430, 0d0000000000000000;
	ld.global.f64 	%fd7431, [%rd2799+40];
	add.f64 	%fd11499, %fd7431, 0d0000000000000000;
	ld.global.f64 	%fd7432, [%rd2799+48];
	add.f64 	%fd11500, %fd7432, 0d0000000000000000;
	ld.global.f64 	%fd7433, [%rd2799+56];
	add.f64 	%fd11501, %fd7433, 0d0000000000000000;
	ld.global.f64 	%fd7434, [%rd2799+64];
	add.f64 	%fd11502, %fd7434, 0d0000000000000000;

$L__BB1_649:
	add.f64 	%fd2818, %fd11502, 0d0000000000000000;
	add.f64 	%fd2819, %fd11501, 0d0000000000000000;
	add.f64 	%fd2820, %fd11500, 0d0000000000000000;
	add.f64 	%fd2821, %fd11499, 0d0000000000000000;
	add.f64 	%fd2822, %fd11498, 0d0000000000000000;
	add.f64 	%fd2823, %fd11497, 0d0000000000000000;
	add.f64 	%fd2824, %fd11496, 0d0000000000000000;
	add.f64 	%fd2825, %fd11495, 0d0000000000000000;
	add.f64 	%fd2826, %fd11494, 0d0000000000000000;
	ld.param.u64 	%rd351, [%rd25+8];
	ld.param.u32 	%r1014, [%rd25+32];
	ld.param.u32 	%r1015, [%rd25+60];
	setp.le.s32 	%p933, %r1015, %r943;
	selp.u16 	%rs289, 1, 0, %p933;
	or.b16  	%rs290, %rs289, %rs18;
	setp.eq.s16 	%p934, %rs290, 0;
	mov.f64 	%fd11503, 0d0000000000000000;
	mov.f64 	%fd11504, 0d0000000000000000;
	mov.f64 	%fd11505, 0d0000000000000000;
	mov.f64 	%fd11506, 0d0000000000000000;
	mov.f64 	%fd11507, 0d0000000000000000;
	mov.f64 	%fd11508, 0d0000000000000000;
	mov.f64 	%fd11509, 0d0000000000000000;
	mov.f64 	%fd11510, 0d0000000000000000;
	mov.f64 	%fd11511, 0d0000000000000000;
	@%p934 bra 	$L__BB1_651;

	add.s32 	%r2022, %r940, 1;
	st.local.v2.u32 	[%rd26], {%r2022, %r1015};
	mov.u64 	%rd2800, $str;
	cvta.global.u64 	%rd2801, %rd2800;
	{ // callseq 506, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2801;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1653, [retval0+0];
	} // callseq 506
	bra.uni 	$L__BB1_653;

$L__BB1_651:
	setp.eq.s64 	%p935, %rd351, 0;
	@%p935 bra 	$L__BB1_653;

	cvta.to.global.u64 	%rd2803, %rd351;
	mul.wide.s32 	%rd2804, %r1014, %r943;
	add.s64 	%rd2805, %rd2803, %rd2804;
	ld.global.f64 	%fd7453, [%rd2805];
	add.f64 	%fd11503, %fd7453, 0d0000000000000000;
	ld.global.f64 	%fd7454, [%rd2805+8];
	add.f64 	%fd11504, %fd7454, 0d0000000000000000;
	ld.global.f64 	%fd7455, [%rd2805+16];
	add.f64 	%fd11505, %fd7455, 0d0000000000000000;
	ld.global.f64 	%fd7456, [%rd2805+24];
	add.f64 	%fd11506, %fd7456, 0d0000000000000000;
	ld.global.f64 	%fd7457, [%rd2805+32];
	add.f64 	%fd11507, %fd7457, 0d0000000000000000;
	ld.global.f64 	%fd7458, [%rd2805+40];
	add.f64 	%fd11508, %fd7458, 0d0000000000000000;
	ld.global.f64 	%fd7459, [%rd2805+48];
	add.f64 	%fd11509, %fd7459, 0d0000000000000000;
	ld.global.f64 	%fd7460, [%rd2805+56];
	add.f64 	%fd11510, %fd7460, 0d0000000000000000;
	ld.global.f64 	%fd7461, [%rd2805+64];
	add.f64 	%fd11511, %fd7461, 0d0000000000000000;

$L__BB1_653:
	shr.u32 	%r2242, %r2253, 27;
	cvt.u16.u32 	%rs296, %r2242;
	and.b16  	%rs295, %rs296, 1;
	add.f64 	%fd2845, %fd11511, 0d0000000000000000;
	add.f64 	%fd2846, %fd11510, 0d0000000000000000;
	add.f64 	%fd2847, %fd11509, 0d0000000000000000;
	add.f64 	%fd2848, %fd11508, 0d0000000000000000;
	add.f64 	%fd2849, %fd11507, 0d0000000000000000;
	add.f64 	%fd2850, %fd11506, 0d0000000000000000;
	add.f64 	%fd2851, %fd11505, 0d0000000000000000;
	add.f64 	%fd2852, %fd11504, 0d0000000000000000;
	add.f64 	%fd2853, %fd11503, 0d0000000000000000;
	ld.param.u32 	%r1017, [%rd25+60];
	setp.le.s32 	%p936, %r1017, %r940;
	selp.u16 	%rs291, 1, 0, %p936;
	or.b16  	%rs292, %rs295, %rs291;
	setp.eq.s16 	%p937, %rs292, 0;
	mov.f64 	%fd11512, 0d0000000000000000;
	mov.f64 	%fd11513, 0d0000000000000000;
	mov.f64 	%fd11514, 0d0000000000000000;
	mov.f64 	%fd11515, 0d0000000000000000;
	mov.f64 	%fd11516, 0d0000000000000000;
	mov.f64 	%fd11517, 0d0000000000000000;
	mov.f64 	%fd11518, 0d0000000000000000;
	mov.f64 	%fd11519, 0d0000000000000000;
	mov.f64 	%fd11520, 0d0000000000000000;
	@%p937 bra 	$L__BB1_655;

	st.local.v2.u32 	[%rd26], {%r940, %r1017};
	mov.u64 	%rd2806, $str;
	cvta.global.u64 	%rd2807, %rd2806;
	{ // callseq 507, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2807;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd374;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1654, [retval0+0];
	} // callseq 507
	bra.uni 	$L__BB1_657;

$L__BB1_655:
	ld.param.u64 	%rd4119, [%rd25+8];
	setp.eq.s64 	%p938, %rd4119, 0;
	@%p938 bra 	$L__BB1_657;

	ld.param.u32 	%r2244, [%rd25+32];
	ld.param.u64 	%rd4120, [%rd25+8];
	cvta.to.global.u64 	%rd2809, %rd4120;
	mul.wide.s32 	%rd2810, %r2244, %r940;
	add.s64 	%rd2811, %rd2809, %rd2810;
	ld.global.f64 	%fd7480, [%rd2811];
	add.f64 	%fd11512, %fd7480, 0d0000000000000000;
	ld.global.f64 	%fd7481, [%rd2811+8];
	add.f64 	%fd11513, %fd7481, 0d0000000000000000;
	ld.global.f64 	%fd7482, [%rd2811+16];
	add.f64 	%fd11514, %fd7482, 0d0000000000000000;
	ld.global.f64 	%fd7483, [%rd2811+24];
	add.f64 	%fd11515, %fd7483, 0d0000000000000000;
	ld.global.f64 	%fd7484, [%rd2811+32];
	add.f64 	%fd11516, %fd7484, 0d0000000000000000;
	ld.global.f64 	%fd7485, [%rd2811+40];
	add.f64 	%fd11517, %fd7485, 0d0000000000000000;
	ld.global.f64 	%fd7486, [%rd2811+48];
	add.f64 	%fd11518, %fd7486, 0d0000000000000000;
	ld.global.f64 	%fd7487, [%rd2811+56];
	add.f64 	%fd11519, %fd7487, 0d0000000000000000;
	ld.global.f64 	%fd7488, [%rd2811+64];
	add.f64 	%fd11520, %fd7488, 0d0000000000000000;

$L__BB1_657:
	add.f64 	%fd2872, %fd11520, 0d0000000000000000;
	add.f64 	%fd2873, %fd11519, 0d0000000000000000;
	add.f64 	%fd2874, %fd11518, 0d0000000000000000;
	add.f64 	%fd2875, %fd11517, 0d0000000000000000;
	add.f64 	%fd2876, %fd11516, 0d0000000000000000;
	add.f64 	%fd2877, %fd11515, 0d0000000000000000;
	add.f64 	%fd2878, %fd11514, 0d0000000000000000;
	add.f64 	%fd2879, %fd11513, 0d0000000000000000;
	add.f64 	%fd2880, %fd11512, 0d0000000000000000;
	setp.eq.s64 	%p939, %rd363, 0;
	@%p939 bra 	$L__BB1_659;

	cvt.s64.s32 	%rd3388, %r10;
	mul.lo.s64 	%rd3389, %rd3388, %rd14;
	add.s64 	%rd2812, %rd363, %rd3389;
	// begin inline asm
	{ atom.add.f64 %fd7489,[%rd2812],%fd2880; }

	// end inline asm
	add.s64 	%rd2813, %rd2812, 8;
	// begin inline asm
	{ atom.add.f64 %fd7491,[%rd2813],%fd2879; }

	// end inline asm
	add.s64 	%rd2814, %rd2812, 16;
	// begin inline asm
	{ atom.add.f64 %fd7493,[%rd2814],%fd2878; }

	// end inline asm
	add.s64 	%rd2815, %rd2812, 24;
	// begin inline asm
	{ atom.add.f64 %fd7495,[%rd2815],%fd2853; }

	// end inline asm
	add.s64 	%rd2816, %rd2812, 32;
	// begin inline asm
	{ atom.add.f64 %fd7497,[%rd2816],%fd2852; }

	// end inline asm
	add.s64 	%rd2817, %rd2812, 40;
	// begin inline asm
	{ atom.add.f64 %fd7499,[%rd2817],%fd2851; }

	// end inline asm
	add.s64 	%rd2818, %rd2812, 48;
	// begin inline asm
	{ atom.add.f64 %fd7501,[%rd2818],%fd2826; }

	// end inline asm
	add.s64 	%rd2819, %rd2812, 56;
	// begin inline asm
	{ atom.add.f64 %fd7503,[%rd2819],%fd2825; }

	// end inline asm
	add.s64 	%rd2820, %rd2812, 64;
	// begin inline asm
	{ atom.add.f64 %fd7505,[%rd2820],%fd2824; }

	// end inline asm
	add.s64 	%rd2821, %rd2812, 72;
	// begin inline asm
	{ atom.add.f64 %fd7507,[%rd2821],%fd2799; }

	// end inline asm
	add.s64 	%rd2822, %rd2812, 80;
	// begin inline asm
	{ atom.add.f64 %fd7509,[%rd2822],%fd2798; }

	// end inline asm
	add.s64 	%rd2823, %rd2812, 88;
	// begin inline asm
	{ atom.add.f64 %fd7511,[%rd2823],%fd2797; }

	// end inline asm
	add.s64 	%rd2824, %rd2812, 96;
	// begin inline asm
	{ atom.add.f64 %fd7513,[%rd2824],%fd2016; }

	// end inline asm
	add.s64 	%rd2825, %rd2812, 104;
	// begin inline asm
	{ atom.add.f64 %fd7515,[%rd2825],%fd2015; }

	// end inline asm
	add.s64 	%rd2826, %rd2812, 112;
	// begin inline asm
	{ atom.add.f64 %fd7517,[%rd2826],%fd2014; }

	// end inline asm
	add.s64 	%rd2827, %rd2812, 120;
	// begin inline asm
	{ atom.add.f64 %fd7519,[%rd2827],%fd1989; }

	// end inline asm
	add.s64 	%rd2828, %rd2812, 128;
	// begin inline asm
	{ atom.add.f64 %fd7521,[%rd2828],%fd1988; }

	// end inline asm
	add.s64 	%rd2829, %rd2812, 136;
	// begin inline asm
	{ atom.add.f64 %fd7523,[%rd2829],%fd1987; }

	// end inline asm
	add.s64 	%rd2830, %rd2812, 144;
	// begin inline asm
	{ atom.add.f64 %fd7525,[%rd2830],%fd1962; }

	// end inline asm
	add.s64 	%rd2831, %rd2812, 152;
	// begin inline asm
	{ atom.add.f64 %fd7527,[%rd2831],%fd1961; }

	// end inline asm
	add.s64 	%rd2832, %rd2812, 160;
	// begin inline asm
	{ atom.add.f64 %fd7529,[%rd2832],%fd1960; }

	// end inline asm
	add.s64 	%rd2833, %rd2812, 168;
	// begin inline asm
	{ atom.add.f64 %fd7531,[%rd2833],%fd1935; }

	// end inline asm
	add.s64 	%rd2834, %rd2812, 176;
	// begin inline asm
	{ atom.add.f64 %fd7533,[%rd2834],%fd1934; }

	// end inline asm
	add.s64 	%rd2835, %rd2812, 184;
	// begin inline asm
	{ atom.add.f64 %fd7535,[%rd2835],%fd1933; }

	// end inline asm
	add.s64 	%rd2836, %rd2812, 192;
	// begin inline asm
	{ atom.add.f64 %fd7537,[%rd2836],%fd2877; }

	// end inline asm
	add.s64 	%rd2837, %rd2812, 200;
	// begin inline asm
	{ atom.add.f64 %fd7539,[%rd2837],%fd2876; }

	// end inline asm
	add.s64 	%rd2838, %rd2812, 208;
	// begin inline asm
	{ atom.add.f64 %fd7541,[%rd2838],%fd2875; }

	// end inline asm
	add.s64 	%rd2839, %rd2812, 216;
	// begin inline asm
	{ atom.add.f64 %fd7543,[%rd2839],%fd2850; }

	// end inline asm
	add.s64 	%rd2840, %rd2812, 224;
	// begin inline asm
	{ atom.add.f64 %fd7545,[%rd2840],%fd2849; }

	// end inline asm
	add.s64 	%rd2841, %rd2812, 232;
	// begin inline asm
	{ atom.add.f64 %fd7547,[%rd2841],%fd2848; }

	// end inline asm
	add.s64 	%rd2842, %rd2812, 240;
	// begin inline asm
	{ atom.add.f64 %fd7549,[%rd2842],%fd2823; }

	// end inline asm
	add.s64 	%rd2843, %rd2812, 248;
	// begin inline asm
	{ atom.add.f64 %fd7551,[%rd2843],%fd2822; }

	// end inline asm
	add.s64 	%rd2844, %rd2812, 256;
	// begin inline asm
	{ atom.add.f64 %fd7553,[%rd2844],%fd2821; }

	// end inline asm
	add.s64 	%rd2845, %rd2812, 264;
	// begin inline asm
	{ atom.add.f64 %fd7555,[%rd2845],%fd2796; }

	// end inline asm
	add.s64 	%rd2846, %rd2812, 272;
	// begin inline asm
	{ atom.add.f64 %fd7557,[%rd2846],%fd2795; }

	// end inline asm
	add.s64 	%rd2847, %rd2812, 280;
	// begin inline asm
	{ atom.add.f64 %fd7559,[%rd2847],%fd2794; }

	// end inline asm
	add.s64 	%rd2848, %rd2812, 288;
	// begin inline asm
	{ atom.add.f64 %fd7561,[%rd2848],%fd2013; }

	// end inline asm
	add.s64 	%rd2849, %rd2812, 296;
	// begin inline asm
	{ atom.add.f64 %fd7563,[%rd2849],%fd2012; }

	// end inline asm
	add.s64 	%rd2850, %rd2812, 304;
	// begin inline asm
	{ atom.add.f64 %fd7565,[%rd2850],%fd2011; }

	// end inline asm
	add.s64 	%rd2851, %rd2812, 312;
	// begin inline asm
	{ atom.add.f64 %fd7567,[%rd2851],%fd1986; }

	// end inline asm
	add.s64 	%rd2852, %rd2812, 320;
	// begin inline asm
	{ atom.add.f64 %fd7569,[%rd2852],%fd1985; }

	// end inline asm
	add.s64 	%rd2853, %rd2812, 328;
	// begin inline asm
	{ atom.add.f64 %fd7571,[%rd2853],%fd1984; }

	// end inline asm
	add.s64 	%rd2854, %rd2812, 336;
	// begin inline asm
	{ atom.add.f64 %fd7573,[%rd2854],%fd1959; }

	// end inline asm
	add.s64 	%rd2855, %rd2812, 344;
	// begin inline asm
	{ atom.add.f64 %fd7575,[%rd2855],%fd1958; }

	// end inline asm
	add.s64 	%rd2856, %rd2812, 352;
	// begin inline asm
	{ atom.add.f64 %fd7577,[%rd2856],%fd1957; }

	// end inline asm
	add.s64 	%rd2857, %rd2812, 360;
	// begin inline asm
	{ atom.add.f64 %fd7579,[%rd2857],%fd1932; }

	// end inline asm
	add.s64 	%rd2858, %rd2812, 368;
	// begin inline asm
	{ atom.add.f64 %fd7581,[%rd2858],%fd1931; }

	// end inline asm
	add.s64 	%rd2859, %rd2812, 376;
	// begin inline asm
	{ atom.add.f64 %fd7583,[%rd2859],%fd1930; }

	// end inline asm
	add.s64 	%rd2860, %rd2812, 384;
	// begin inline asm
	{ atom.add.f64 %fd7585,[%rd2860],%fd2874; }

	// end inline asm
	add.s64 	%rd2861, %rd2812, 392;
	// begin inline asm
	{ atom.add.f64 %fd7587,[%rd2861],%fd2873; }

	// end inline asm
	add.s64 	%rd2862, %rd2812, 400;
	// begin inline asm
	{ atom.add.f64 %fd7589,[%rd2862],%fd2872; }

	// end inline asm
	add.s64 	%rd2863, %rd2812, 408;
	// begin inline asm
	{ atom.add.f64 %fd7591,[%rd2863],%fd2847; }

	// end inline asm
	add.s64 	%rd2864, %rd2812, 416;
	// begin inline asm
	{ atom.add.f64 %fd7593,[%rd2864],%fd2846; }

	// end inline asm
	add.s64 	%rd2865, %rd2812, 424;
	// begin inline asm
	{ atom.add.f64 %fd7595,[%rd2865],%fd2845; }

	// end inline asm
	add.s64 	%rd2866, %rd2812, 432;
	// begin inline asm
	{ atom.add.f64 %fd7597,[%rd2866],%fd2820; }

	// end inline asm
	add.s64 	%rd2867, %rd2812, 440;
	// begin inline asm
	{ atom.add.f64 %fd7599,[%rd2867],%fd2819; }

	// end inline asm
	add.s64 	%rd2868, %rd2812, 448;
	// begin inline asm
	{ atom.add.f64 %fd7601,[%rd2868],%fd2818; }

	// end inline asm
	add.s64 	%rd2869, %rd2812, 456;
	// begin inline asm
	{ atom.add.f64 %fd7603,[%rd2869],%fd2793; }

	// end inline asm
	add.s64 	%rd2870, %rd2812, 464;
	// begin inline asm
	{ atom.add.f64 %fd7605,[%rd2870],%fd2792; }

	// end inline asm
	add.s64 	%rd2871, %rd2812, 472;
	// begin inline asm
	{ atom.add.f64 %fd7607,[%rd2871],%fd2791; }

	// end inline asm
	add.s64 	%rd2872, %rd2812, 480;
	// begin inline asm
	{ atom.add.f64 %fd7609,[%rd2872],%fd2010; }

	// end inline asm
	add.s64 	%rd2873, %rd2812, 488;
	// begin inline asm
	{ atom.add.f64 %fd7611,[%rd2873],%fd2009; }

	// end inline asm
	add.s64 	%rd2874, %rd2812, 496;
	// begin inline asm
	{ atom.add.f64 %fd7613,[%rd2874],%fd2008; }

	// end inline asm
	add.s64 	%rd2875, %rd2812, 504;
	// begin inline asm
	{ atom.add.f64 %fd7615,[%rd2875],%fd1983; }

	// end inline asm
	add.s64 	%rd2876, %rd2812, 512;
	// begin inline asm
	{ atom.add.f64 %fd7617,[%rd2876],%fd1982; }

	// end inline asm
	add.s64 	%rd2877, %rd2812, 520;
	// begin inline asm
	{ atom.add.f64 %fd7619,[%rd2877],%fd1981; }

	// end inline asm
	add.s64 	%rd2878, %rd2812, 528;
	// begin inline asm
	{ atom.add.f64 %fd7621,[%rd2878],%fd1956; }

	// end inline asm
	add.s64 	%rd2879, %rd2812, 536;
	// begin inline asm
	{ atom.add.f64 %fd7623,[%rd2879],%fd1955; }

	// end inline asm
	add.s64 	%rd2880, %rd2812, 544;
	// begin inline asm
	{ atom.add.f64 %fd7625,[%rd2880],%fd1954; }

	// end inline asm
	add.s64 	%rd2881, %rd2812, 552;
	// begin inline asm
	{ atom.add.f64 %fd7627,[%rd2881],%fd1929; }

	// end inline asm
	add.s64 	%rd2882, %rd2812, 560;
	// begin inline asm
	{ atom.add.f64 %fd7629,[%rd2882],%fd1928; }

	// end inline asm
	add.s64 	%rd2883, %rd2812, 568;
	// begin inline asm
	{ atom.add.f64 %fd7631,[%rd2883],%fd1927; }

	// end inline asm
	add.s64 	%rd2884, %rd2812, 576;
	// begin inline asm
	{ atom.add.f64 %fd7633,[%rd2884],%fd2772; }

	// end inline asm
	add.s64 	%rd2885, %rd2812, 584;
	// begin inline asm
	{ atom.add.f64 %fd7635,[%rd2885],%fd2771; }

	// end inline asm
	add.s64 	%rd2886, %rd2812, 592;
	// begin inline asm
	{ atom.add.f64 %fd7637,[%rd2886],%fd2770; }

	// end inline asm
	add.s64 	%rd2887, %rd2812, 600;
	// begin inline asm
	{ atom.add.f64 %fd7639,[%rd2887],%fd2745; }

	// end inline asm
	add.s64 	%rd2888, %rd2812, 608;
	// begin inline asm
	{ atom.add.f64 %fd7641,[%rd2888],%fd2744; }

	// end inline asm
	add.s64 	%rd2889, %rd2812, 616;
	// begin inline asm
	{ atom.add.f64 %fd7643,[%rd2889],%fd2743; }

	// end inline asm
	add.s64 	%rd2890, %rd2812, 624;
	// begin inline asm
	{ atom.add.f64 %fd7645,[%rd2890],%fd2718; }

	// end inline asm
	add.s64 	%rd2891, %rd2812, 632;
	// begin inline asm
	{ atom.add.f64 %fd7647,[%rd2891],%fd2717; }

	// end inline asm
	add.s64 	%rd2892, %rd2812, 640;
	// begin inline asm
	{ atom.add.f64 %fd7649,[%rd2892],%fd2716; }

	// end inline asm
	add.s64 	%rd2893, %rd2812, 648;
	// begin inline asm
	{ atom.add.f64 %fd7651,[%rd2893],%fd2691; }

	// end inline asm
	add.s64 	%rd2894, %rd2812, 656;
	// begin inline asm
	{ atom.add.f64 %fd7653,[%rd2894],%fd2690; }

	// end inline asm
	add.s64 	%rd2895, %rd2812, 664;
	// begin inline asm
	{ atom.add.f64 %fd7655,[%rd2895],%fd2689; }

	// end inline asm
	add.s64 	%rd2896, %rd2812, 672;
	// begin inline asm
	{ atom.add.f64 %fd7657,[%rd2896],%fd1908; }

	// end inline asm
	add.s64 	%rd2897, %rd2812, 680;
	// begin inline asm
	{ atom.add.f64 %fd7659,[%rd2897],%fd1907; }

	// end inline asm
	add.s64 	%rd2898, %rd2812, 688;
	// begin inline asm
	{ atom.add.f64 %fd7661,[%rd2898],%fd1906; }

	// end inline asm
	add.s64 	%rd2899, %rd2812, 696;
	// begin inline asm
	{ atom.add.f64 %fd7663,[%rd2899],%fd1881; }

	// end inline asm
	add.s64 	%rd2900, %rd2812, 704;
	// begin inline asm
	{ atom.add.f64 %fd7665,[%rd2900],%fd1880; }

	// end inline asm
	add.s64 	%rd2901, %rd2812, 712;
	// begin inline asm
	{ atom.add.f64 %fd7667,[%rd2901],%fd1879; }

	// end inline asm
	add.s64 	%rd2902, %rd2812, 720;
	// begin inline asm
	{ atom.add.f64 %fd7669,[%rd2902],%fd1854; }

	// end inline asm
	add.s64 	%rd2903, %rd2812, 728;
	// begin inline asm
	{ atom.add.f64 %fd7671,[%rd2903],%fd1853; }

	// end inline asm
	add.s64 	%rd2904, %rd2812, 736;
	// begin inline asm
	{ atom.add.f64 %fd7673,[%rd2904],%fd1852; }

	// end inline asm
	add.s64 	%rd2905, %rd2812, 744;
	// begin inline asm
	{ atom.add.f64 %fd7675,[%rd2905],%fd1827; }

	// end inline asm
	add.s64 	%rd2906, %rd2812, 752;
	// begin inline asm
	{ atom.add.f64 %fd7677,[%rd2906],%fd1826; }

	// end inline asm
	add.s64 	%rd2907, %rd2812, 760;
	// begin inline asm
	{ atom.add.f64 %fd7679,[%rd2907],%fd1825; }

	// end inline asm
	add.s64 	%rd2908, %rd2812, 768;
	// begin inline asm
	{ atom.add.f64 %fd7681,[%rd2908],%fd2769; }

	// end inline asm
	add.s64 	%rd2909, %rd2812, 776;
	// begin inline asm
	{ atom.add.f64 %fd7683,[%rd2909],%fd2768; }

	// end inline asm
	add.s64 	%rd2910, %rd2812, 784;
	// begin inline asm
	{ atom.add.f64 %fd7685,[%rd2910],%fd2767; }

	// end inline asm
	add.s64 	%rd2911, %rd2812, 792;
	// begin inline asm
	{ atom.add.f64 %fd7687,[%rd2911],%fd2742; }

	// end inline asm
	add.s64 	%rd2912, %rd2812, 800;
	// begin inline asm
	{ atom.add.f64 %fd7689,[%rd2912],%fd2741; }

	// end inline asm
	add.s64 	%rd2913, %rd2812, 808;
	// begin inline asm
	{ atom.add.f64 %fd7691,[%rd2913],%fd2740; }

	// end inline asm
	add.s64 	%rd2914, %rd2812, 816;
	// begin inline asm
	{ atom.add.f64 %fd7693,[%rd2914],%fd2715; }

	// end inline asm
	add.s64 	%rd2915, %rd2812, 824;
	// begin inline asm
	{ atom.add.f64 %fd7695,[%rd2915],%fd2714; }

	// end inline asm
	add.s64 	%rd2916, %rd2812, 832;
	// begin inline asm
	{ atom.add.f64 %fd7697,[%rd2916],%fd2713; }

	// end inline asm
	add.s64 	%rd2917, %rd2812, 840;
	// begin inline asm
	{ atom.add.f64 %fd7699,[%rd2917],%fd2688; }

	// end inline asm
	add.s64 	%rd2918, %rd2812, 848;
	// begin inline asm
	{ atom.add.f64 %fd7701,[%rd2918],%fd2687; }

	// end inline asm
	add.s64 	%rd2919, %rd2812, 856;
	// begin inline asm
	{ atom.add.f64 %fd7703,[%rd2919],%fd2686; }

	// end inline asm
	add.s64 	%rd2920, %rd2812, 864;
	// begin inline asm
	{ atom.add.f64 %fd7705,[%rd2920],%fd1905; }

	// end inline asm
	add.s64 	%rd2921, %rd2812, 872;
	// begin inline asm
	{ atom.add.f64 %fd7707,[%rd2921],%fd1904; }

	// end inline asm
	add.s64 	%rd2922, %rd2812, 880;
	// begin inline asm
	{ atom.add.f64 %fd7709,[%rd2922],%fd1903; }

	// end inline asm
	add.s64 	%rd2923, %rd2812, 888;
	// begin inline asm
	{ atom.add.f64 %fd7711,[%rd2923],%fd1878; }

	// end inline asm
	add.s64 	%rd2924, %rd2812, 896;
	// begin inline asm
	{ atom.add.f64 %fd7713,[%rd2924],%fd1877; }

	// end inline asm
	add.s64 	%rd2925, %rd2812, 904;
	// begin inline asm
	{ atom.add.f64 %fd7715,[%rd2925],%fd1876; }

	// end inline asm
	add.s64 	%rd2926, %rd2812, 912;
	// begin inline asm
	{ atom.add.f64 %fd7717,[%rd2926],%fd1851; }

	// end inline asm
	add.s64 	%rd2927, %rd2812, 920;
	// begin inline asm
	{ atom.add.f64 %fd7719,[%rd2927],%fd1850; }

	// end inline asm
	add.s64 	%rd2928, %rd2812, 928;
	// begin inline asm
	{ atom.add.f64 %fd7721,[%rd2928],%fd1849; }

	// end inline asm
	add.s64 	%rd2929, %rd2812, 936;
	// begin inline asm
	{ atom.add.f64 %fd7723,[%rd2929],%fd1824; }

	// end inline asm
	add.s64 	%rd2930, %rd2812, 944;
	// begin inline asm
	{ atom.add.f64 %fd7725,[%rd2930],%fd1823; }

	// end inline asm
	add.s64 	%rd2931, %rd2812, 952;
	// begin inline asm
	{ atom.add.f64 %fd7727,[%rd2931],%fd1822; }

	// end inline asm
	add.s64 	%rd2932, %rd2812, 960;
	// begin inline asm
	{ atom.add.f64 %fd7729,[%rd2932],%fd2766; }

	// end inline asm
	add.s64 	%rd2933, %rd2812, 968;
	// begin inline asm
	{ atom.add.f64 %fd7731,[%rd2933],%fd2765; }

	// end inline asm
	add.s64 	%rd2934, %rd2812, 976;
	// begin inline asm
	{ atom.add.f64 %fd7733,[%rd2934],%fd2764; }

	// end inline asm
	add.s64 	%rd2935, %rd2812, 984;
	// begin inline asm
	{ atom.add.f64 %fd7735,[%rd2935],%fd2739; }

	// end inline asm
	add.s64 	%rd2936, %rd2812, 992;
	// begin inline asm
	{ atom.add.f64 %fd7737,[%rd2936],%fd2738; }

	// end inline asm
	add.s64 	%rd2937, %rd2812, 1000;
	// begin inline asm
	{ atom.add.f64 %fd7739,[%rd2937],%fd2737; }

	// end inline asm
	add.s64 	%rd2938, %rd2812, 1008;
	// begin inline asm
	{ atom.add.f64 %fd7741,[%rd2938],%fd2712; }

	// end inline asm
	add.s64 	%rd2939, %rd2812, 1016;
	// begin inline asm
	{ atom.add.f64 %fd7743,[%rd2939],%fd2711; }

	// end inline asm
	add.s64 	%rd2940, %rd2812, 1024;
	// begin inline asm
	{ atom.add.f64 %fd7745,[%rd2940],%fd2710; }

	// end inline asm
	add.s64 	%rd2941, %rd2812, 1032;
	// begin inline asm
	{ atom.add.f64 %fd7747,[%rd2941],%fd2685; }

	// end inline asm
	add.s64 	%rd2942, %rd2812, 1040;
	// begin inline asm
	{ atom.add.f64 %fd7749,[%rd2942],%fd2684; }

	// end inline asm
	add.s64 	%rd2943, %rd2812, 1048;
	// begin inline asm
	{ atom.add.f64 %fd7751,[%rd2943],%fd2683; }

	// end inline asm
	add.s64 	%rd2944, %rd2812, 1056;
	// begin inline asm
	{ atom.add.f64 %fd7753,[%rd2944],%fd1902; }

	// end inline asm
	add.s64 	%rd2945, %rd2812, 1064;
	// begin inline asm
	{ atom.add.f64 %fd7755,[%rd2945],%fd1901; }

	// end inline asm
	add.s64 	%rd2946, %rd2812, 1072;
	// begin inline asm
	{ atom.add.f64 %fd7757,[%rd2946],%fd1900; }

	// end inline asm
	add.s64 	%rd2947, %rd2812, 1080;
	// begin inline asm
	{ atom.add.f64 %fd7759,[%rd2947],%fd1875; }

	// end inline asm
	add.s64 	%rd2948, %rd2812, 1088;
	// begin inline asm
	{ atom.add.f64 %fd7761,[%rd2948],%fd1874; }

	// end inline asm
	add.s64 	%rd2949, %rd2812, 1096;
	// begin inline asm
	{ atom.add.f64 %fd7763,[%rd2949],%fd1873; }

	// end inline asm
	add.s64 	%rd2950, %rd2812, 1104;
	// begin inline asm
	{ atom.add.f64 %fd7765,[%rd2950],%fd1848; }

	// end inline asm
	add.s64 	%rd2951, %rd2812, 1112;
	// begin inline asm
	{ atom.add.f64 %fd7767,[%rd2951],%fd1847; }

	// end inline asm
	add.s64 	%rd2952, %rd2812, 1120;
	// begin inline asm
	{ atom.add.f64 %fd7769,[%rd2952],%fd1846; }

	// end inline asm
	add.s64 	%rd2953, %rd2812, 1128;
	// begin inline asm
	{ atom.add.f64 %fd7771,[%rd2953],%fd1821; }

	// end inline asm
	add.s64 	%rd2954, %rd2812, 1136;
	// begin inline asm
	{ atom.add.f64 %fd7773,[%rd2954],%fd1820; }

	// end inline asm
	add.s64 	%rd2955, %rd2812, 1144;
	// begin inline asm
	{ atom.add.f64 %fd7775,[%rd2955],%fd1819; }

	// end inline asm
	add.s64 	%rd2956, %rd2812, 1152;
	// begin inline asm
	{ atom.add.f64 %fd7777,[%rd2956],%fd2664; }

	// end inline asm
	add.s64 	%rd2957, %rd2812, 1160;
	// begin inline asm
	{ atom.add.f64 %fd7779,[%rd2957],%fd2663; }

	// end inline asm
	add.s64 	%rd2958, %rd2812, 1168;
	// begin inline asm
	{ atom.add.f64 %fd7781,[%rd2958],%fd2662; }

	// end inline asm
	add.s64 	%rd2959, %rd2812, 1176;
	// begin inline asm
	{ atom.add.f64 %fd7783,[%rd2959],%fd2637; }

	// end inline asm
	add.s64 	%rd2960, %rd2812, 1184;
	// begin inline asm
	{ atom.add.f64 %fd7785,[%rd2960],%fd2636; }

	// end inline asm
	add.s64 	%rd2961, %rd2812, 1192;
	// begin inline asm
	{ atom.add.f64 %fd7787,[%rd2961],%fd2635; }

	// end inline asm
	add.s64 	%rd2962, %rd2812, 1200;
	// begin inline asm
	{ atom.add.f64 %fd7789,[%rd2962],%fd2610; }

	// end inline asm
	add.s64 	%rd2963, %rd2812, 1208;
	// begin inline asm
	{ atom.add.f64 %fd7791,[%rd2963],%fd2609; }

	// end inline asm
	add.s64 	%rd2964, %rd2812, 1216;
	// begin inline asm
	{ atom.add.f64 %fd7793,[%rd2964],%fd2608; }

	// end inline asm
	add.s64 	%rd2965, %rd2812, 1224;
	// begin inline asm
	{ atom.add.f64 %fd7795,[%rd2965],%fd2583; }

	// end inline asm
	add.s64 	%rd2966, %rd2812, 1232;
	// begin inline asm
	{ atom.add.f64 %fd7797,[%rd2966],%fd2582; }

	// end inline asm
	add.s64 	%rd2967, %rd2812, 1240;
	// begin inline asm
	{ atom.add.f64 %fd7799,[%rd2967],%fd2581; }

	// end inline asm
	add.s64 	%rd2968, %rd2812, 1248;
	// begin inline asm
	{ atom.add.f64 %fd7801,[%rd2968],%fd1800; }

	// end inline asm
	add.s64 	%rd2969, %rd2812, 1256;
	// begin inline asm
	{ atom.add.f64 %fd7803,[%rd2969],%fd1799; }

	// end inline asm
	add.s64 	%rd2970, %rd2812, 1264;
	// begin inline asm
	{ atom.add.f64 %fd7805,[%rd2970],%fd1798; }

	// end inline asm
	add.s64 	%rd2971, %rd2812, 1272;
	// begin inline asm
	{ atom.add.f64 %fd7807,[%rd2971],%fd1773; }

	// end inline asm
	add.s64 	%rd2972, %rd2812, 1280;
	// begin inline asm
	{ atom.add.f64 %fd7809,[%rd2972],%fd1772; }

	// end inline asm
	add.s64 	%rd2973, %rd2812, 1288;
	// begin inline asm
	{ atom.add.f64 %fd7811,[%rd2973],%fd1771; }

	// end inline asm
	add.s64 	%rd2974, %rd2812, 1296;
	// begin inline asm
	{ atom.add.f64 %fd7813,[%rd2974],%fd1746; }

	// end inline asm
	add.s64 	%rd2975, %rd2812, 1304;
	// begin inline asm
	{ atom.add.f64 %fd7815,[%rd2975],%fd1745; }

	// end inline asm
	add.s64 	%rd2976, %rd2812, 1312;
	// begin inline asm
	{ atom.add.f64 %fd7817,[%rd2976],%fd1744; }

	// end inline asm
	add.s64 	%rd2977, %rd2812, 1320;
	// begin inline asm
	{ atom.add.f64 %fd7819,[%rd2977],%fd1719; }

	// end inline asm
	add.s64 	%rd2978, %rd2812, 1328;
	// begin inline asm
	{ atom.add.f64 %fd7821,[%rd2978],%fd1718; }

	// end inline asm
	add.s64 	%rd2979, %rd2812, 1336;
	// begin inline asm
	{ atom.add.f64 %fd7823,[%rd2979],%fd1717; }

	// end inline asm
	add.s64 	%rd2980, %rd2812, 1344;
	// begin inline asm
	{ atom.add.f64 %fd7825,[%rd2980],%fd2661; }

	// end inline asm
	add.s64 	%rd2981, %rd2812, 1352;
	// begin inline asm
	{ atom.add.f64 %fd7827,[%rd2981],%fd2660; }

	// end inline asm
	add.s64 	%rd2982, %rd2812, 1360;
	// begin inline asm
	{ atom.add.f64 %fd7829,[%rd2982],%fd2659; }

	// end inline asm
	add.s64 	%rd2983, %rd2812, 1368;
	// begin inline asm
	{ atom.add.f64 %fd7831,[%rd2983],%fd2634; }

	// end inline asm
	add.s64 	%rd2984, %rd2812, 1376;
	// begin inline asm
	{ atom.add.f64 %fd7833,[%rd2984],%fd2633; }

	// end inline asm
	add.s64 	%rd2985, %rd2812, 1384;
	// begin inline asm
	{ atom.add.f64 %fd7835,[%rd2985],%fd2632; }

	// end inline asm
	add.s64 	%rd2986, %rd2812, 1392;
	// begin inline asm
	{ atom.add.f64 %fd7837,[%rd2986],%fd2607; }

	// end inline asm
	add.s64 	%rd2987, %rd2812, 1400;
	// begin inline asm
	{ atom.add.f64 %fd7839,[%rd2987],%fd2606; }

	// end inline asm
	add.s64 	%rd2988, %rd2812, 1408;
	// begin inline asm
	{ atom.add.f64 %fd7841,[%rd2988],%fd2605; }

	// end inline asm
	add.s64 	%rd2989, %rd2812, 1416;
	// begin inline asm
	{ atom.add.f64 %fd7843,[%rd2989],%fd2580; }

	// end inline asm
	add.s64 	%rd2990, %rd2812, 1424;
	// begin inline asm
	{ atom.add.f64 %fd7845,[%rd2990],%fd2579; }

	// end inline asm
	add.s64 	%rd2991, %rd2812, 1432;
	// begin inline asm
	{ atom.add.f64 %fd7847,[%rd2991],%fd2578; }

	// end inline asm
	add.s64 	%rd2992, %rd2812, 1440;
	// begin inline asm
	{ atom.add.f64 %fd7849,[%rd2992],%fd1797; }

	// end inline asm
	add.s64 	%rd2993, %rd2812, 1448;
	// begin inline asm
	{ atom.add.f64 %fd7851,[%rd2993],%fd1796; }

	// end inline asm
	add.s64 	%rd2994, %rd2812, 1456;
	// begin inline asm
	{ atom.add.f64 %fd7853,[%rd2994],%fd1795; }

	// end inline asm
	add.s64 	%rd2995, %rd2812, 1464;
	// begin inline asm
	{ atom.add.f64 %fd7855,[%rd2995],%fd1770; }

	// end inline asm
	add.s64 	%rd2996, %rd2812, 1472;
	// begin inline asm
	{ atom.add.f64 %fd7857,[%rd2996],%fd1769; }

	// end inline asm
	add.s64 	%rd2997, %rd2812, 1480;
	// begin inline asm
	{ atom.add.f64 %fd7859,[%rd2997],%fd1768; }

	// end inline asm
	add.s64 	%rd2998, %rd2812, 1488;
	// begin inline asm
	{ atom.add.f64 %fd7861,[%rd2998],%fd1743; }

	// end inline asm
	add.s64 	%rd2999, %rd2812, 1496;
	// begin inline asm
	{ atom.add.f64 %fd7863,[%rd2999],%fd1742; }

	// end inline asm
	add.s64 	%rd3000, %rd2812, 1504;
	// begin inline asm
	{ atom.add.f64 %fd7865,[%rd3000],%fd1741; }

	// end inline asm
	add.s64 	%rd3001, %rd2812, 1512;
	// begin inline asm
	{ atom.add.f64 %fd7867,[%rd3001],%fd1716; }

	// end inline asm
	add.s64 	%rd3002, %rd2812, 1520;
	// begin inline asm
	{ atom.add.f64 %fd7869,[%rd3002],%fd1715; }

	// end inline asm
	add.s64 	%rd3003, %rd2812, 1528;
	// begin inline asm
	{ atom.add.f64 %fd7871,[%rd3003],%fd1714; }

	// end inline asm
	add.s64 	%rd3004, %rd2812, 1536;
	// begin inline asm
	{ atom.add.f64 %fd7873,[%rd3004],%fd2658; }

	// end inline asm
	add.s64 	%rd3005, %rd2812, 1544;
	// begin inline asm
	{ atom.add.f64 %fd7875,[%rd3005],%fd2657; }

	// end inline asm
	add.s64 	%rd3006, %rd2812, 1552;
	// begin inline asm
	{ atom.add.f64 %fd7877,[%rd3006],%fd2656; }

	// end inline asm
	add.s64 	%rd3007, %rd2812, 1560;
	// begin inline asm
	{ atom.add.f64 %fd7879,[%rd3007],%fd2631; }

	// end inline asm
	add.s64 	%rd3008, %rd2812, 1568;
	// begin inline asm
	{ atom.add.f64 %fd7881,[%rd3008],%fd2630; }

	// end inline asm
	add.s64 	%rd3009, %rd2812, 1576;
	// begin inline asm
	{ atom.add.f64 %fd7883,[%rd3009],%fd2629; }

	// end inline asm
	add.s64 	%rd3010, %rd2812, 1584;
	// begin inline asm
	{ atom.add.f64 %fd7885,[%rd3010],%fd2604; }

	// end inline asm
	add.s64 	%rd3011, %rd2812, 1592;
	// begin inline asm
	{ atom.add.f64 %fd7887,[%rd3011],%fd2603; }

	// end inline asm
	add.s64 	%rd3012, %rd2812, 1600;
	// begin inline asm
	{ atom.add.f64 %fd7889,[%rd3012],%fd2602; }

	// end inline asm
	add.s64 	%rd3013, %rd2812, 1608;
	// begin inline asm
	{ atom.add.f64 %fd7891,[%rd3013],%fd2577; }

	// end inline asm
	add.s64 	%rd3014, %rd2812, 1616;
	// begin inline asm
	{ atom.add.f64 %fd7893,[%rd3014],%fd2576; }

	// end inline asm
	add.s64 	%rd3015, %rd2812, 1624;
	// begin inline asm
	{ atom.add.f64 %fd7895,[%rd3015],%fd2575; }

	// end inline asm
	add.s64 	%rd3016, %rd2812, 1632;
	// begin inline asm
	{ atom.add.f64 %fd7897,[%rd3016],%fd1794; }

	// end inline asm
	add.s64 	%rd3017, %rd2812, 1640;
	// begin inline asm
	{ atom.add.f64 %fd7899,[%rd3017],%fd1793; }

	// end inline asm
	add.s64 	%rd3018, %rd2812, 1648;
	// begin inline asm
	{ atom.add.f64 %fd7901,[%rd3018],%fd1792; }

	// end inline asm
	add.s64 	%rd3019, %rd2812, 1656;
	// begin inline asm
	{ atom.add.f64 %fd7903,[%rd3019],%fd1767; }

	// end inline asm
	add.s64 	%rd3020, %rd2812, 1664;
	// begin inline asm
	{ atom.add.f64 %fd7905,[%rd3020],%fd1766; }

	// end inline asm
	add.s64 	%rd3021, %rd2812, 1672;
	// begin inline asm
	{ atom.add.f64 %fd7907,[%rd3021],%fd1765; }

	// end inline asm
	add.s64 	%rd3022, %rd2812, 1680;
	// begin inline asm
	{ atom.add.f64 %fd7909,[%rd3022],%fd1740; }

	// end inline asm
	add.s64 	%rd3023, %rd2812, 1688;
	// begin inline asm
	{ atom.add.f64 %fd7911,[%rd3023],%fd1739; }

	// end inline asm
	add.s64 	%rd3024, %rd2812, 1696;
	// begin inline asm
	{ atom.add.f64 %fd7913,[%rd3024],%fd1738; }

	// end inline asm
	add.s64 	%rd3025, %rd2812, 1704;
	// begin inline asm
	{ atom.add.f64 %fd7915,[%rd3025],%fd1713; }

	// end inline asm
	add.s64 	%rd3026, %rd2812, 1712;
	// begin inline asm
	{ atom.add.f64 %fd7917,[%rd3026],%fd1712; }

	// end inline asm
	add.s64 	%rd3027, %rd2812, 1720;
	// begin inline asm
	{ atom.add.f64 %fd7919,[%rd3027],%fd1711; }

	// end inline asm
	add.s64 	%rd3028, %rd2812, 1728;
	// begin inline asm
	{ atom.add.f64 %fd7921,[%rd3028],%fd2556; }

	// end inline asm
	add.s64 	%rd3029, %rd2812, 1736;
	// begin inline asm
	{ atom.add.f64 %fd7923,[%rd3029],%fd2555; }

	// end inline asm
	add.s64 	%rd3030, %rd2812, 1744;
	// begin inline asm
	{ atom.add.f64 %fd7925,[%rd3030],%fd2554; }

	// end inline asm
	add.s64 	%rd3031, %rd2812, 1752;
	// begin inline asm
	{ atom.add.f64 %fd7927,[%rd3031],%fd2529; }

	// end inline asm
	add.s64 	%rd3032, %rd2812, 1760;
	// begin inline asm
	{ atom.add.f64 %fd7929,[%rd3032],%fd2528; }

	// end inline asm
	add.s64 	%rd3033, %rd2812, 1768;
	// begin inline asm
	{ atom.add.f64 %fd7931,[%rd3033],%fd2527; }

	// end inline asm
	add.s64 	%rd3034, %rd2812, 1776;
	// begin inline asm
	{ atom.add.f64 %fd7933,[%rd3034],%fd2502; }

	// end inline asm
	add.s64 	%rd3035, %rd2812, 1784;
	// begin inline asm
	{ atom.add.f64 %fd7935,[%rd3035],%fd2501; }

	// end inline asm
	add.s64 	%rd3036, %rd2812, 1792;
	// begin inline asm
	{ atom.add.f64 %fd7937,[%rd3036],%fd2500; }

	// end inline asm
	add.s64 	%rd3037, %rd2812, 1800;
	// begin inline asm
	{ atom.add.f64 %fd7939,[%rd3037],%fd2475; }

	// end inline asm
	add.s64 	%rd3038, %rd2812, 1808;
	// begin inline asm
	{ atom.add.f64 %fd7941,[%rd3038],%fd2474; }

	// end inline asm
	add.s64 	%rd3039, %rd2812, 1816;
	// begin inline asm
	{ atom.add.f64 %fd7943,[%rd3039],%fd2473; }

	// end inline asm
	add.s64 	%rd3040, %rd2812, 1824;
	// begin inline asm
	{ atom.add.f64 %fd7945,[%rd3040],%fd1692; }

	// end inline asm
	add.s64 	%rd3041, %rd2812, 1832;
	// begin inline asm
	{ atom.add.f64 %fd7947,[%rd3041],%fd1691; }

	// end inline asm
	add.s64 	%rd3042, %rd2812, 1840;
	// begin inline asm
	{ atom.add.f64 %fd7949,[%rd3042],%fd1690; }

	// end inline asm
	add.s64 	%rd3043, %rd2812, 1848;
	// begin inline asm
	{ atom.add.f64 %fd7951,[%rd3043],%fd1665; }

	// end inline asm
	add.s64 	%rd3044, %rd2812, 1856;
	// begin inline asm
	{ atom.add.f64 %fd7953,[%rd3044],%fd1664; }

	// end inline asm
	add.s64 	%rd3045, %rd2812, 1864;
	// begin inline asm
	{ atom.add.f64 %fd7955,[%rd3045],%fd1663; }

	// end inline asm
	add.s64 	%rd3046, %rd2812, 1872;
	// begin inline asm
	{ atom.add.f64 %fd7957,[%rd3046],%fd1638; }

	// end inline asm
	add.s64 	%rd3047, %rd2812, 1880;
	// begin inline asm
	{ atom.add.f64 %fd7959,[%rd3047],%fd1637; }

	// end inline asm
	add.s64 	%rd3048, %rd2812, 1888;
	// begin inline asm
	{ atom.add.f64 %fd7961,[%rd3048],%fd1636; }

	// end inline asm
	add.s64 	%rd3049, %rd2812, 1896;
	// begin inline asm
	{ atom.add.f64 %fd7963,[%rd3049],%fd1611; }

	// end inline asm
	add.s64 	%rd3050, %rd2812, 1904;
	// begin inline asm
	{ atom.add.f64 %fd7965,[%rd3050],%fd1610; }

	// end inline asm
	add.s64 	%rd3051, %rd2812, 1912;
	// begin inline asm
	{ atom.add.f64 %fd7967,[%rd3051],%fd1609; }

	// end inline asm
	add.s64 	%rd3052, %rd2812, 1920;
	// begin inline asm
	{ atom.add.f64 %fd7969,[%rd3052],%fd2553; }

	// end inline asm
	add.s64 	%rd3053, %rd2812, 1928;
	// begin inline asm
	{ atom.add.f64 %fd7971,[%rd3053],%fd2552; }

	// end inline asm
	add.s64 	%rd3054, %rd2812, 1936;
	// begin inline asm
	{ atom.add.f64 %fd7973,[%rd3054],%fd2551; }

	// end inline asm
	add.s64 	%rd3055, %rd2812, 1944;
	// begin inline asm
	{ atom.add.f64 %fd7975,[%rd3055],%fd2526; }

	// end inline asm
	add.s64 	%rd3056, %rd2812, 1952;
	// begin inline asm
	{ atom.add.f64 %fd7977,[%rd3056],%fd2525; }

	// end inline asm
	add.s64 	%rd3057, %rd2812, 1960;
	// begin inline asm
	{ atom.add.f64 %fd7979,[%rd3057],%fd2524; }

	// end inline asm
	add.s64 	%rd3058, %rd2812, 1968;
	// begin inline asm
	{ atom.add.f64 %fd7981,[%rd3058],%fd2499; }

	// end inline asm
	add.s64 	%rd3059, %rd2812, 1976;
	// begin inline asm
	{ atom.add.f64 %fd7983,[%rd3059],%fd2498; }

	// end inline asm
	add.s64 	%rd3060, %rd2812, 1984;
	// begin inline asm
	{ atom.add.f64 %fd7985,[%rd3060],%fd2497; }

	// end inline asm
	add.s64 	%rd3061, %rd2812, 1992;
	// begin inline asm
	{ atom.add.f64 %fd7987,[%rd3061],%fd2472; }

	// end inline asm
	add.s64 	%rd3062, %rd2812, 2000;
	// begin inline asm
	{ atom.add.f64 %fd7989,[%rd3062],%fd2471; }

	// end inline asm
	add.s64 	%rd3063, %rd2812, 2008;
	// begin inline asm
	{ atom.add.f64 %fd7991,[%rd3063],%fd2470; }

	// end inline asm
	add.s64 	%rd3064, %rd2812, 2016;
	// begin inline asm
	{ atom.add.f64 %fd7993,[%rd3064],%fd1689; }

	// end inline asm
	add.s64 	%rd3065, %rd2812, 2024;
	// begin inline asm
	{ atom.add.f64 %fd7995,[%rd3065],%fd1688; }

	// end inline asm
	add.s64 	%rd3066, %rd2812, 2032;
	// begin inline asm
	{ atom.add.f64 %fd7997,[%rd3066],%fd1687; }

	// end inline asm
	add.s64 	%rd3067, %rd2812, 2040;
	// begin inline asm
	{ atom.add.f64 %fd7999,[%rd3067],%fd1662; }

	// end inline asm
	add.s64 	%rd3068, %rd2812, 2048;
	// begin inline asm
	{ atom.add.f64 %fd8001,[%rd3068],%fd1661; }

	// end inline asm
	add.s64 	%rd3069, %rd2812, 2056;
	// begin inline asm
	{ atom.add.f64 %fd8003,[%rd3069],%fd1660; }

	// end inline asm
	add.s64 	%rd3070, %rd2812, 2064;
	// begin inline asm
	{ atom.add.f64 %fd8005,[%rd3070],%fd1635; }

	// end inline asm
	add.s64 	%rd3071, %rd2812, 2072;
	// begin inline asm
	{ atom.add.f64 %fd8007,[%rd3071],%fd1634; }

	// end inline asm
	add.s64 	%rd3072, %rd2812, 2080;
	// begin inline asm
	{ atom.add.f64 %fd8009,[%rd3072],%fd1633; }

	// end inline asm
	add.s64 	%rd3073, %rd2812, 2088;
	// begin inline asm
	{ atom.add.f64 %fd8011,[%rd3073],%fd1608; }

	// end inline asm
	add.s64 	%rd3074, %rd2812, 2096;
	// begin inline asm
	{ atom.add.f64 %fd8013,[%rd3074],%fd1607; }

	// end inline asm
	add.s64 	%rd3075, %rd2812, 2104;
	// begin inline asm
	{ atom.add.f64 %fd8015,[%rd3075],%fd1606; }

	// end inline asm
	add.s64 	%rd3076, %rd2812, 2112;
	// begin inline asm
	{ atom.add.f64 %fd8017,[%rd3076],%fd2550; }

	// end inline asm
	add.s64 	%rd3077, %rd2812, 2120;
	// begin inline asm
	{ atom.add.f64 %fd8019,[%rd3077],%fd2549; }

	// end inline asm
	add.s64 	%rd3078, %rd2812, 2128;
	// begin inline asm
	{ atom.add.f64 %fd8021,[%rd3078],%fd2548; }

	// end inline asm
	add.s64 	%rd3079, %rd2812, 2136;
	// begin inline asm
	{ atom.add.f64 %fd8023,[%rd3079],%fd2523; }

	// end inline asm
	add.s64 	%rd3080, %rd2812, 2144;
	// begin inline asm
	{ atom.add.f64 %fd8025,[%rd3080],%fd2522; }

	// end inline asm
	add.s64 	%rd3081, %rd2812, 2152;
	// begin inline asm
	{ atom.add.f64 %fd8027,[%rd3081],%fd2521; }

	// end inline asm
	add.s64 	%rd3082, %rd2812, 2160;
	// begin inline asm
	{ atom.add.f64 %fd8029,[%rd3082],%fd2496; }

	// end inline asm
	add.s64 	%rd3083, %rd2812, 2168;
	// begin inline asm
	{ atom.add.f64 %fd8031,[%rd3083],%fd2495; }

	// end inline asm
	add.s64 	%rd3084, %rd2812, 2176;
	// begin inline asm
	{ atom.add.f64 %fd8033,[%rd3084],%fd2494; }

	// end inline asm
	add.s64 	%rd3085, %rd2812, 2184;
	// begin inline asm
	{ atom.add.f64 %fd8035,[%rd3085],%fd2469; }

	// end inline asm
	add.s64 	%rd3086, %rd2812, 2192;
	// begin inline asm
	{ atom.add.f64 %fd8037,[%rd3086],%fd2468; }

	// end inline asm
	add.s64 	%rd3087, %rd2812, 2200;
	// begin inline asm
	{ atom.add.f64 %fd8039,[%rd3087],%fd2467; }

	// end inline asm
	add.s64 	%rd3088, %rd2812, 2208;
	// begin inline asm
	{ atom.add.f64 %fd8041,[%rd3088],%fd1686; }

	// end inline asm
	add.s64 	%rd3089, %rd2812, 2216;
	// begin inline asm
	{ atom.add.f64 %fd8043,[%rd3089],%fd1685; }

	// end inline asm
	add.s64 	%rd3090, %rd2812, 2224;
	// begin inline asm
	{ atom.add.f64 %fd8045,[%rd3090],%fd1684; }

	// end inline asm
	add.s64 	%rd3091, %rd2812, 2232;
	// begin inline asm
	{ atom.add.f64 %fd8047,[%rd3091],%fd1659; }

	// end inline asm
	add.s64 	%rd3092, %rd2812, 2240;
	// begin inline asm
	{ atom.add.f64 %fd8049,[%rd3092],%fd1658; }

	// end inline asm
	add.s64 	%rd3093, %rd2812, 2248;
	// begin inline asm
	{ atom.add.f64 %fd8051,[%rd3093],%fd1657; }

	// end inline asm
	add.s64 	%rd3094, %rd2812, 2256;
	// begin inline asm
	{ atom.add.f64 %fd8053,[%rd3094],%fd1632; }

	// end inline asm
	add.s64 	%rd3095, %rd2812, 2264;
	// begin inline asm
	{ atom.add.f64 %fd8055,[%rd3095],%fd1631; }

	// end inline asm
	add.s64 	%rd3096, %rd2812, 2272;
	// begin inline asm
	{ atom.add.f64 %fd8057,[%rd3096],%fd1630; }

	// end inline asm
	add.s64 	%rd3097, %rd2812, 2280;
	// begin inline asm
	{ atom.add.f64 %fd8059,[%rd3097],%fd1605; }

	// end inline asm
	add.s64 	%rd3098, %rd2812, 2288;
	// begin inline asm
	{ atom.add.f64 %fd8061,[%rd3098],%fd1604; }

	// end inline asm
	add.s64 	%rd3099, %rd2812, 2296;
	// begin inline asm
	{ atom.add.f64 %fd8063,[%rd3099],%fd1603; }

	// end inline asm
	add.s64 	%rd3100, %rd2812, 2304;
	// begin inline asm
	{ atom.add.f64 %fd8065,[%rd3100],%fd1584; }

	// end inline asm
	add.s64 	%rd3101, %rd2812, 2312;
	// begin inline asm
	{ atom.add.f64 %fd8067,[%rd3101],%fd1583; }

	// end inline asm
	add.s64 	%rd3102, %rd2812, 2320;
	// begin inline asm
	{ atom.add.f64 %fd8069,[%rd3102],%fd1582; }

	// end inline asm
	add.s64 	%rd3103, %rd2812, 2328;
	// begin inline asm
	{ atom.add.f64 %fd8071,[%rd3103],%fd1557; }

	// end inline asm
	add.s64 	%rd3104, %rd2812, 2336;
	// begin inline asm
	{ atom.add.f64 %fd8073,[%rd3104],%fd1556; }

	// end inline asm
	add.s64 	%rd3105, %rd2812, 2344;
	// begin inline asm
	{ atom.add.f64 %fd8075,[%rd3105],%fd1555; }

	// end inline asm
	add.s64 	%rd3106, %rd2812, 2352;
	// begin inline asm
	{ atom.add.f64 %fd8077,[%rd3106],%fd1530; }

	// end inline asm
	add.s64 	%rd3107, %rd2812, 2360;
	// begin inline asm
	{ atom.add.f64 %fd8079,[%rd3107],%fd1529; }

	// end inline asm
	add.s64 	%rd3108, %rd2812, 2368;
	// begin inline asm
	{ atom.add.f64 %fd8081,[%rd3108],%fd1528; }

	// end inline asm
	add.s64 	%rd3109, %rd2812, 2376;
	// begin inline asm
	{ atom.add.f64 %fd8083,[%rd3109],%fd1503; }

	// end inline asm
	add.s64 	%rd3110, %rd2812, 2384;
	// begin inline asm
	{ atom.add.f64 %fd8085,[%rd3110],%fd1502; }

	// end inline asm
	add.s64 	%rd3111, %rd2812, 2392;
	// begin inline asm
	{ atom.add.f64 %fd8087,[%rd3111],%fd1501; }

	// end inline asm
	add.s64 	%rd3112, %rd2812, 2400;
	// begin inline asm
	{ atom.add.f64 %fd8089,[%rd3112],%fd2448; }

	// end inline asm
	add.s64 	%rd3113, %rd2812, 2408;
	// begin inline asm
	{ atom.add.f64 %fd8091,[%rd3113],%fd2447; }

	// end inline asm
	add.s64 	%rd3114, %rd2812, 2416;
	// begin inline asm
	{ atom.add.f64 %fd8093,[%rd3114],%fd2446; }

	// end inline asm
	add.s64 	%rd3115, %rd2812, 2424;
	// begin inline asm
	{ atom.add.f64 %fd8095,[%rd3115],%fd2421; }

	// end inline asm
	add.s64 	%rd3116, %rd2812, 2432;
	// begin inline asm
	{ atom.add.f64 %fd8097,[%rd3116],%fd2420; }

	// end inline asm
	add.s64 	%rd3117, %rd2812, 2440;
	// begin inline asm
	{ atom.add.f64 %fd8099,[%rd3117],%fd2419; }

	// end inline asm
	add.s64 	%rd3118, %rd2812, 2448;
	// begin inline asm
	{ atom.add.f64 %fd8101,[%rd3118],%fd2394; }

	// end inline asm
	add.s64 	%rd3119, %rd2812, 2456;
	// begin inline asm
	{ atom.add.f64 %fd8103,[%rd3119],%fd2393; }

	// end inline asm
	add.s64 	%rd3120, %rd2812, 2464;
	// begin inline asm
	{ atom.add.f64 %fd8105,[%rd3120],%fd2392; }

	// end inline asm
	add.s64 	%rd3121, %rd2812, 2472;
	// begin inline asm
	{ atom.add.f64 %fd8107,[%rd3121],%fd2367; }

	// end inline asm
	add.s64 	%rd3122, %rd2812, 2480;
	// begin inline asm
	{ atom.add.f64 %fd8109,[%rd3122],%fd2366; }

	// end inline asm
	add.s64 	%rd3123, %rd2812, 2488;
	// begin inline asm
	{ atom.add.f64 %fd8111,[%rd3123],%fd2365; }

	// end inline asm
	add.s64 	%rd3124, %rd2812, 2496;
	// begin inline asm
	{ atom.add.f64 %fd8113,[%rd3124],%fd1581; }

	// end inline asm
	add.s64 	%rd3125, %rd2812, 2504;
	// begin inline asm
	{ atom.add.f64 %fd8115,[%rd3125],%fd1580; }

	// end inline asm
	add.s64 	%rd3126, %rd2812, 2512;
	// begin inline asm
	{ atom.add.f64 %fd8117,[%rd3126],%fd1579; }

	// end inline asm
	add.s64 	%rd3127, %rd2812, 2520;
	// begin inline asm
	{ atom.add.f64 %fd8119,[%rd3127],%fd1554; }

	// end inline asm
	add.s64 	%rd3128, %rd2812, 2528;
	// begin inline asm
	{ atom.add.f64 %fd8121,[%rd3128],%fd1553; }

	// end inline asm
	add.s64 	%rd3129, %rd2812, 2536;
	// begin inline asm
	{ atom.add.f64 %fd8123,[%rd3129],%fd1552; }

	// end inline asm
	add.s64 	%rd3130, %rd2812, 2544;
	// begin inline asm
	{ atom.add.f64 %fd8125,[%rd3130],%fd1527; }

	// end inline asm
	add.s64 	%rd3131, %rd2812, 2552;
	// begin inline asm
	{ atom.add.f64 %fd8127,[%rd3131],%fd1526; }

	// end inline asm
	add.s64 	%rd3132, %rd2812, 2560;
	// begin inline asm
	{ atom.add.f64 %fd8129,[%rd3132],%fd1525; }

	// end inline asm
	add.s64 	%rd3133, %rd2812, 2568;
	// begin inline asm
	{ atom.add.f64 %fd8131,[%rd3133],%fd1500; }

	// end inline asm
	add.s64 	%rd3134, %rd2812, 2576;
	// begin inline asm
	{ atom.add.f64 %fd8133,[%rd3134],%fd1499; }

	// end inline asm
	add.s64 	%rd3135, %rd2812, 2584;
	// begin inline asm
	{ atom.add.f64 %fd8135,[%rd3135],%fd1498; }

	// end inline asm
	add.s64 	%rd3136, %rd2812, 2592;
	// begin inline asm
	{ atom.add.f64 %fd8137,[%rd3136],%fd2445; }

	// end inline asm
	add.s64 	%rd3137, %rd2812, 2600;
	// begin inline asm
	{ atom.add.f64 %fd8139,[%rd3137],%fd2444; }

	// end inline asm
	add.s64 	%rd3138, %rd2812, 2608;
	// begin inline asm
	{ atom.add.f64 %fd8141,[%rd3138],%fd2443; }

	// end inline asm
	add.s64 	%rd3139, %rd2812, 2616;
	// begin inline asm
	{ atom.add.f64 %fd8143,[%rd3139],%fd2418; }

	// end inline asm
	add.s64 	%rd3140, %rd2812, 2624;
	// begin inline asm
	{ atom.add.f64 %fd8145,[%rd3140],%fd2417; }

	// end inline asm
	add.s64 	%rd3141, %rd2812, 2632;
	// begin inline asm
	{ atom.add.f64 %fd8147,[%rd3141],%fd2416; }

	// end inline asm
	add.s64 	%rd3142, %rd2812, 2640;
	// begin inline asm
	{ atom.add.f64 %fd8149,[%rd3142],%fd2391; }

	// end inline asm
	add.s64 	%rd3143, %rd2812, 2648;
	// begin inline asm
	{ atom.add.f64 %fd8151,[%rd3143],%fd2390; }

	// end inline asm
	add.s64 	%rd3144, %rd2812, 2656;
	// begin inline asm
	{ atom.add.f64 %fd8153,[%rd3144],%fd2389; }

	// end inline asm
	add.s64 	%rd3145, %rd2812, 2664;
	// begin inline asm
	{ atom.add.f64 %fd8155,[%rd3145],%fd2364; }

	// end inline asm
	add.s64 	%rd3146, %rd2812, 2672;
	// begin inline asm
	{ atom.add.f64 %fd8157,[%rd3146],%fd2363; }

	// end inline asm
	add.s64 	%rd3147, %rd2812, 2680;
	// begin inline asm
	{ atom.add.f64 %fd8159,[%rd3147],%fd2362; }

	// end inline asm
	add.s64 	%rd3148, %rd2812, 2688;
	// begin inline asm
	{ atom.add.f64 %fd8161,[%rd3148],%fd1578; }

	// end inline asm
	add.s64 	%rd3149, %rd2812, 2696;
	// begin inline asm
	{ atom.add.f64 %fd8163,[%rd3149],%fd1577; }

	// end inline asm
	add.s64 	%rd3150, %rd2812, 2704;
	// begin inline asm
	{ atom.add.f64 %fd8165,[%rd3150],%fd1576; }

	// end inline asm
	add.s64 	%rd3151, %rd2812, 2712;
	// begin inline asm
	{ atom.add.f64 %fd8167,[%rd3151],%fd1551; }

	// end inline asm
	add.s64 	%rd3152, %rd2812, 2720;
	// begin inline asm
	{ atom.add.f64 %fd8169,[%rd3152],%fd1550; }

	// end inline asm
	add.s64 	%rd3153, %rd2812, 2728;
	// begin inline asm
	{ atom.add.f64 %fd8171,[%rd3153],%fd1549; }

	// end inline asm
	add.s64 	%rd3154, %rd2812, 2736;
	// begin inline asm
	{ atom.add.f64 %fd8173,[%rd3154],%fd1524; }

	// end inline asm
	add.s64 	%rd3155, %rd2812, 2744;
	// begin inline asm
	{ atom.add.f64 %fd8175,[%rd3155],%fd1523; }

	// end inline asm
	add.s64 	%rd3156, %rd2812, 2752;
	// begin inline asm
	{ atom.add.f64 %fd8177,[%rd3156],%fd1522; }

	// end inline asm
	add.s64 	%rd3157, %rd2812, 2760;
	// begin inline asm
	{ atom.add.f64 %fd8179,[%rd3157],%fd1497; }

	// end inline asm
	add.s64 	%rd3158, %rd2812, 2768;
	// begin inline asm
	{ atom.add.f64 %fd8181,[%rd3158],%fd1496; }

	// end inline asm
	add.s64 	%rd3159, %rd2812, 2776;
	// begin inline asm
	{ atom.add.f64 %fd8183,[%rd3159],%fd1495; }

	// end inline asm
	add.s64 	%rd3160, %rd2812, 2784;
	// begin inline asm
	{ atom.add.f64 %fd8185,[%rd3160],%fd2442; }

	// end inline asm
	add.s64 	%rd3161, %rd2812, 2792;
	// begin inline asm
	{ atom.add.f64 %fd8187,[%rd3161],%fd2441; }

	// end inline asm
	add.s64 	%rd3162, %rd2812, 2800;
	// begin inline asm
	{ atom.add.f64 %fd8189,[%rd3162],%fd2440; }

	// end inline asm
	add.s64 	%rd3163, %rd2812, 2808;
	// begin inline asm
	{ atom.add.f64 %fd8191,[%rd3163],%fd2415; }

	// end inline asm
	add.s64 	%rd3164, %rd2812, 2816;
	// begin inline asm
	{ atom.add.f64 %fd8193,[%rd3164],%fd2414; }

	// end inline asm
	add.s64 	%rd3165, %rd2812, 2824;
	// begin inline asm
	{ atom.add.f64 %fd8195,[%rd3165],%fd2413; }

	// end inline asm
	add.s64 	%rd3166, %rd2812, 2832;
	// begin inline asm
	{ atom.add.f64 %fd8197,[%rd3166],%fd2388; }

	// end inline asm
	add.s64 	%rd3167, %rd2812, 2840;
	// begin inline asm
	{ atom.add.f64 %fd8199,[%rd3167],%fd2387; }

	// end inline asm
	add.s64 	%rd3168, %rd2812, 2848;
	// begin inline asm
	{ atom.add.f64 %fd8201,[%rd3168],%fd2386; }

	// end inline asm
	add.s64 	%rd3169, %rd2812, 2856;
	// begin inline asm
	{ atom.add.f64 %fd8203,[%rd3169],%fd2361; }

	// end inline asm
	add.s64 	%rd3170, %rd2812, 2864;
	// begin inline asm
	{ atom.add.f64 %fd8205,[%rd3170],%fd2360; }

	// end inline asm
	add.s64 	%rd3171, %rd2812, 2872;
	// begin inline asm
	{ atom.add.f64 %fd8207,[%rd3171],%fd2359; }

	// end inline asm
	add.s64 	%rd3172, %rd2812, 2880;
	// begin inline asm
	{ atom.add.f64 %fd8209,[%rd3172],%fd1476; }

	// end inline asm
	add.s64 	%rd3173, %rd2812, 2888;
	// begin inline asm
	{ atom.add.f64 %fd8211,[%rd3173],%fd1475; }

	// end inline asm
	add.s64 	%rd3174, %rd2812, 2896;
	// begin inline asm
	{ atom.add.f64 %fd8213,[%rd3174],%fd1474; }

	// end inline asm
	add.s64 	%rd3175, %rd2812, 2904;
	// begin inline asm
	{ atom.add.f64 %fd8215,[%rd3175],%fd1449; }

	// end inline asm
	add.s64 	%rd3176, %rd2812, 2912;
	// begin inline asm
	{ atom.add.f64 %fd8217,[%rd3176],%fd1448; }

	// end inline asm
	add.s64 	%rd3177, %rd2812, 2920;
	// begin inline asm
	{ atom.add.f64 %fd8219,[%rd3177],%fd1447; }

	// end inline asm
	add.s64 	%rd3178, %rd2812, 2928;
	// begin inline asm
	{ atom.add.f64 %fd8221,[%rd3178],%fd1422; }

	// end inline asm
	add.s64 	%rd3179, %rd2812, 2936;
	// begin inline asm
	{ atom.add.f64 %fd8223,[%rd3179],%fd1421; }

	// end inline asm
	add.s64 	%rd3180, %rd2812, 2944;
	// begin inline asm
	{ atom.add.f64 %fd8225,[%rd3180],%fd1420; }

	// end inline asm
	add.s64 	%rd3181, %rd2812, 2952;
	// begin inline asm
	{ atom.add.f64 %fd8227,[%rd3181],%fd1395; }

	// end inline asm
	add.s64 	%rd3182, %rd2812, 2960;
	// begin inline asm
	{ atom.add.f64 %fd8229,[%rd3182],%fd1394; }

	// end inline asm
	add.s64 	%rd3183, %rd2812, 2968;
	// begin inline asm
	{ atom.add.f64 %fd8231,[%rd3183],%fd1393; }

	// end inline asm
	add.s64 	%rd3184, %rd2812, 2976;
	// begin inline asm
	{ atom.add.f64 %fd8233,[%rd3184],%fd2340; }

	// end inline asm
	add.s64 	%rd3185, %rd2812, 2984;
	// begin inline asm
	{ atom.add.f64 %fd8235,[%rd3185],%fd2339; }

	// end inline asm
	add.s64 	%rd3186, %rd2812, 2992;
	// begin inline asm
	{ atom.add.f64 %fd8237,[%rd3186],%fd2338; }

	// end inline asm
	add.s64 	%rd3187, %rd2812, 3000;
	// begin inline asm
	{ atom.add.f64 %fd8239,[%rd3187],%fd2313; }

	// end inline asm
	add.s64 	%rd3188, %rd2812, 3008;
	// begin inline asm
	{ atom.add.f64 %fd8241,[%rd3188],%fd2312; }

	// end inline asm
	add.s64 	%rd3189, %rd2812, 3016;
	// begin inline asm
	{ atom.add.f64 %fd8243,[%rd3189],%fd2311; }

	// end inline asm
	add.s64 	%rd3190, %rd2812, 3024;
	// begin inline asm
	{ atom.add.f64 %fd8245,[%rd3190],%fd2286; }

	// end inline asm
	add.s64 	%rd3191, %rd2812, 3032;
	// begin inline asm
	{ atom.add.f64 %fd8247,[%rd3191],%fd2285; }

	// end inline asm
	add.s64 	%rd3192, %rd2812, 3040;
	// begin inline asm
	{ atom.add.f64 %fd8249,[%rd3192],%fd2284; }

	// end inline asm
	add.s64 	%rd3193, %rd2812, 3048;
	// begin inline asm
	{ atom.add.f64 %fd8251,[%rd3193],%fd2259; }

	// end inline asm
	add.s64 	%rd3194, %rd2812, 3056;
	// begin inline asm
	{ atom.add.f64 %fd8253,[%rd3194],%fd2258; }

	// end inline asm
	add.s64 	%rd3195, %rd2812, 3064;
	// begin inline asm
	{ atom.add.f64 %fd8255,[%rd3195],%fd2257; }

	// end inline asm
	add.s64 	%rd3196, %rd2812, 3072;
	// begin inline asm
	{ atom.add.f64 %fd8257,[%rd3196],%fd1473; }

	// end inline asm
	add.s64 	%rd3197, %rd2812, 3080;
	// begin inline asm
	{ atom.add.f64 %fd8259,[%rd3197],%fd1472; }

	// end inline asm
	add.s64 	%rd3198, %rd2812, 3088;
	// begin inline asm
	{ atom.add.f64 %fd8261,[%rd3198],%fd1471; }

	// end inline asm
	add.s64 	%rd3199, %rd2812, 3096;
	// begin inline asm
	{ atom.add.f64 %fd8263,[%rd3199],%fd1446; }

	// end inline asm
	add.s64 	%rd3200, %rd2812, 3104;
	// begin inline asm
	{ atom.add.f64 %fd8265,[%rd3200],%fd1445; }

	// end inline asm
	add.s64 	%rd3201, %rd2812, 3112;
	// begin inline asm
	{ atom.add.f64 %fd8267,[%rd3201],%fd1444; }

	// end inline asm
	add.s64 	%rd3202, %rd2812, 3120;
	// begin inline asm
	{ atom.add.f64 %fd8269,[%rd3202],%fd1419; }

	// end inline asm
	add.s64 	%rd3203, %rd2812, 3128;
	// begin inline asm
	{ atom.add.f64 %fd8271,[%rd3203],%fd1418; }

	// end inline asm
	add.s64 	%rd3204, %rd2812, 3136;
	// begin inline asm
	{ atom.add.f64 %fd8273,[%rd3204],%fd1417; }

	// end inline asm
	add.s64 	%rd3205, %rd2812, 3144;
	// begin inline asm
	{ atom.add.f64 %fd8275,[%rd3205],%fd1392; }

	// end inline asm
	add.s64 	%rd3206, %rd2812, 3152;
	// begin inline asm
	{ atom.add.f64 %fd8277,[%rd3206],%fd1391; }

	// end inline asm
	add.s64 	%rd3207, %rd2812, 3160;
	// begin inline asm
	{ atom.add.f64 %fd8279,[%rd3207],%fd1390; }

	// end inline asm
	add.s64 	%rd3208, %rd2812, 3168;
	// begin inline asm
	{ atom.add.f64 %fd8281,[%rd3208],%fd2337; }

	// end inline asm
	add.s64 	%rd3209, %rd2812, 3176;
	// begin inline asm
	{ atom.add.f64 %fd8283,[%rd3209],%fd2336; }

	// end inline asm
	add.s64 	%rd3210, %rd2812, 3184;
	// begin inline asm
	{ atom.add.f64 %fd8285,[%rd3210],%fd2335; }

	// end inline asm
	add.s64 	%rd3211, %rd2812, 3192;
	// begin inline asm
	{ atom.add.f64 %fd8287,[%rd3211],%fd2310; }

	// end inline asm
	add.s64 	%rd3212, %rd2812, 3200;
	// begin inline asm
	{ atom.add.f64 %fd8289,[%rd3212],%fd2309; }

	// end inline asm
	add.s64 	%rd3213, %rd2812, 3208;
	// begin inline asm
	{ atom.add.f64 %fd8291,[%rd3213],%fd2308; }

	// end inline asm
	add.s64 	%rd3214, %rd2812, 3216;
	// begin inline asm
	{ atom.add.f64 %fd8293,[%rd3214],%fd2283; }

	// end inline asm
	add.s64 	%rd3215, %rd2812, 3224;
	// begin inline asm
	{ atom.add.f64 %fd8295,[%rd3215],%fd2282; }

	// end inline asm
	add.s64 	%rd3216, %rd2812, 3232;
	// begin inline asm
	{ atom.add.f64 %fd8297,[%rd3216],%fd2281; }

	// end inline asm
	add.s64 	%rd3217, %rd2812, 3240;
	// begin inline asm
	{ atom.add.f64 %fd8299,[%rd3217],%fd2256; }

	// end inline asm
	add.s64 	%rd3218, %rd2812, 3248;
	// begin inline asm
	{ atom.add.f64 %fd8301,[%rd3218],%fd2255; }

	// end inline asm
	add.s64 	%rd3219, %rd2812, 3256;
	// begin inline asm
	{ atom.add.f64 %fd8303,[%rd3219],%fd2254; }

	// end inline asm
	add.s64 	%rd3220, %rd2812, 3264;
	// begin inline asm
	{ atom.add.f64 %fd8305,[%rd3220],%fd1470; }

	// end inline asm
	add.s64 	%rd3221, %rd2812, 3272;
	// begin inline asm
	{ atom.add.f64 %fd8307,[%rd3221],%fd1469; }

	// end inline asm
	add.s64 	%rd3222, %rd2812, 3280;
	// begin inline asm
	{ atom.add.f64 %fd8309,[%rd3222],%fd1468; }

	// end inline asm
	add.s64 	%rd3223, %rd2812, 3288;
	// begin inline asm
	{ atom.add.f64 %fd8311,[%rd3223],%fd1443; }

	// end inline asm
	add.s64 	%rd3224, %rd2812, 3296;
	// begin inline asm
	{ atom.add.f64 %fd8313,[%rd3224],%fd1442; }

	// end inline asm
	add.s64 	%rd3225, %rd2812, 3304;
	// begin inline asm
	{ atom.add.f64 %fd8315,[%rd3225],%fd1441; }

	// end inline asm
	add.s64 	%rd3226, %rd2812, 3312;
	// begin inline asm
	{ atom.add.f64 %fd8317,[%rd3226],%fd1416; }

	// end inline asm
	add.s64 	%rd3227, %rd2812, 3320;
	// begin inline asm
	{ atom.add.f64 %fd8319,[%rd3227],%fd1415; }

	// end inline asm
	add.s64 	%rd3228, %rd2812, 3328;
	// begin inline asm
	{ atom.add.f64 %fd8321,[%rd3228],%fd1414; }

	// end inline asm
	add.s64 	%rd3229, %rd2812, 3336;
	// begin inline asm
	{ atom.add.f64 %fd8323,[%rd3229],%fd1389; }

	// end inline asm
	add.s64 	%rd3230, %rd2812, 3344;
	// begin inline asm
	{ atom.add.f64 %fd8325,[%rd3230],%fd1388; }

	// end inline asm
	add.s64 	%rd3231, %rd2812, 3352;
	// begin inline asm
	{ atom.add.f64 %fd8327,[%rd3231],%fd1387; }

	// end inline asm
	add.s64 	%rd3232, %rd2812, 3360;
	// begin inline asm
	{ atom.add.f64 %fd8329,[%rd3232],%fd2334; }

	// end inline asm
	add.s64 	%rd3233, %rd2812, 3368;
	// begin inline asm
	{ atom.add.f64 %fd8331,[%rd3233],%fd2333; }

	// end inline asm
	add.s64 	%rd3234, %rd2812, 3376;
	// begin inline asm
	{ atom.add.f64 %fd8333,[%rd3234],%fd2332; }

	// end inline asm
	add.s64 	%rd3235, %rd2812, 3384;
	// begin inline asm
	{ atom.add.f64 %fd8335,[%rd3235],%fd2307; }

	// end inline asm
	add.s64 	%rd3236, %rd2812, 3392;
	// begin inline asm
	{ atom.add.f64 %fd8337,[%rd3236],%fd2306; }

	// end inline asm
	add.s64 	%rd3237, %rd2812, 3400;
	// begin inline asm
	{ atom.add.f64 %fd8339,[%rd3237],%fd2305; }

	// end inline asm
	add.s64 	%rd3238, %rd2812, 3408;
	// begin inline asm
	{ atom.add.f64 %fd8341,[%rd3238],%fd2280; }

	// end inline asm
	add.s64 	%rd3239, %rd2812, 3416;
	// begin inline asm
	{ atom.add.f64 %fd8343,[%rd3239],%fd2279; }

	// end inline asm
	add.s64 	%rd3240, %rd2812, 3424;
	// begin inline asm
	{ atom.add.f64 %fd8345,[%rd3240],%fd2278; }

	// end inline asm
	add.s64 	%rd3241, %rd2812, 3432;
	// begin inline asm
	{ atom.add.f64 %fd8347,[%rd3241],%fd2253; }

	// end inline asm
	add.s64 	%rd3242, %rd2812, 3440;
	// begin inline asm
	{ atom.add.f64 %fd8349,[%rd3242],%fd2252; }

	// end inline asm
	add.s64 	%rd3243, %rd2812, 3448;
	// begin inline asm
	{ atom.add.f64 %fd8351,[%rd3243],%fd2251; }

	// end inline asm
	add.s64 	%rd3244, %rd2812, 3456;
	// begin inline asm
	{ atom.add.f64 %fd8353,[%rd3244],%fd1368; }

	// end inline asm
	add.s64 	%rd3245, %rd2812, 3464;
	// begin inline asm
	{ atom.add.f64 %fd8355,[%rd3245],%fd1367; }

	// end inline asm
	add.s64 	%rd3246, %rd2812, 3472;
	// begin inline asm
	{ atom.add.f64 %fd8357,[%rd3246],%fd1366; }

	// end inline asm
	add.s64 	%rd3247, %rd2812, 3480;
	// begin inline asm
	{ atom.add.f64 %fd8359,[%rd3247],%fd1341; }

	// end inline asm
	add.s64 	%rd3248, %rd2812, 3488;
	// begin inline asm
	{ atom.add.f64 %fd8361,[%rd3248],%fd1340; }

	// end inline asm
	add.s64 	%rd3249, %rd2812, 3496;
	// begin inline asm
	{ atom.add.f64 %fd8363,[%rd3249],%fd1339; }

	// end inline asm
	add.s64 	%rd3250, %rd2812, 3504;
	// begin inline asm
	{ atom.add.f64 %fd8365,[%rd3250],%fd1314; }

	// end inline asm
	add.s64 	%rd3251, %rd2812, 3512;
	// begin inline asm
	{ atom.add.f64 %fd8367,[%rd3251],%fd1313; }

	// end inline asm
	add.s64 	%rd3252, %rd2812, 3520;
	// begin inline asm
	{ atom.add.f64 %fd8369,[%rd3252],%fd1312; }

	// end inline asm
	add.s64 	%rd3253, %rd2812, 3528;
	// begin inline asm
	{ atom.add.f64 %fd8371,[%rd3253],%fd1287; }

	// end inline asm
	add.s64 	%rd3254, %rd2812, 3536;
	// begin inline asm
	{ atom.add.f64 %fd8373,[%rd3254],%fd1286; }

	// end inline asm
	add.s64 	%rd3255, %rd2812, 3544;
	// begin inline asm
	{ atom.add.f64 %fd8375,[%rd3255],%fd1285; }

	// end inline asm
	add.s64 	%rd3256, %rd2812, 3552;
	// begin inline asm
	{ atom.add.f64 %fd8377,[%rd3256],%fd2232; }

	// end inline asm
	add.s64 	%rd3257, %rd2812, 3560;
	// begin inline asm
	{ atom.add.f64 %fd8379,[%rd3257],%fd2231; }

	// end inline asm
	add.s64 	%rd3258, %rd2812, 3568;
	// begin inline asm
	{ atom.add.f64 %fd8381,[%rd3258],%fd2230; }

	// end inline asm
	add.s64 	%rd3259, %rd2812, 3576;
	// begin inline asm
	{ atom.add.f64 %fd8383,[%rd3259],%fd2205; }

	// end inline asm
	add.s64 	%rd3260, %rd2812, 3584;
	// begin inline asm
	{ atom.add.f64 %fd8385,[%rd3260],%fd2204; }

	// end inline asm
	add.s64 	%rd3261, %rd2812, 3592;
	// begin inline asm
	{ atom.add.f64 %fd8387,[%rd3261],%fd2203; }

	// end inline asm
	add.s64 	%rd3262, %rd2812, 3600;
	// begin inline asm
	{ atom.add.f64 %fd8389,[%rd3262],%fd2178; }

	// end inline asm
	add.s64 	%rd3263, %rd2812, 3608;
	// begin inline asm
	{ atom.add.f64 %fd8391,[%rd3263],%fd2177; }

	// end inline asm
	add.s64 	%rd3264, %rd2812, 3616;
	// begin inline asm
	{ atom.add.f64 %fd8393,[%rd3264],%fd2176; }

	// end inline asm
	add.s64 	%rd3265, %rd2812, 3624;
	// begin inline asm
	{ atom.add.f64 %fd8395,[%rd3265],%fd2151; }

	// end inline asm
	add.s64 	%rd3266, %rd2812, 3632;
	// begin inline asm
	{ atom.add.f64 %fd8397,[%rd3266],%fd2150; }

	// end inline asm
	add.s64 	%rd3267, %rd2812, 3640;
	// begin inline asm
	{ atom.add.f64 %fd8399,[%rd3267],%fd2149; }

	// end inline asm
	add.s64 	%rd3268, %rd2812, 3648;
	// begin inline asm
	{ atom.add.f64 %fd8401,[%rd3268],%fd1365; }

	// end inline asm
	add.s64 	%rd3269, %rd2812, 3656;
	// begin inline asm
	{ atom.add.f64 %fd8403,[%rd3269],%fd1364; }

	// end inline asm
	add.s64 	%rd3270, %rd2812, 3664;
	// begin inline asm
	{ atom.add.f64 %fd8405,[%rd3270],%fd1363; }

	// end inline asm
	add.s64 	%rd3271, %rd2812, 3672;
	// begin inline asm
	{ atom.add.f64 %fd8407,[%rd3271],%fd1338; }

	// end inline asm
	add.s64 	%rd3272, %rd2812, 3680;
	// begin inline asm
	{ atom.add.f64 %fd8409,[%rd3272],%fd1337; }

	// end inline asm
	add.s64 	%rd3273, %rd2812, 3688;
	// begin inline asm
	{ atom.add.f64 %fd8411,[%rd3273],%fd1336; }

	// end inline asm
	add.s64 	%rd3274, %rd2812, 3696;
	// begin inline asm
	{ atom.add.f64 %fd8413,[%rd3274],%fd1311; }

	// end inline asm
	add.s64 	%rd3275, %rd2812, 3704;
	// begin inline asm
	{ atom.add.f64 %fd8415,[%rd3275],%fd1310; }

	// end inline asm
	add.s64 	%rd3276, %rd2812, 3712;
	// begin inline asm
	{ atom.add.f64 %fd8417,[%rd3276],%fd1309; }

	// end inline asm
	add.s64 	%rd3277, %rd2812, 3720;
	// begin inline asm
	{ atom.add.f64 %fd8419,[%rd3277],%fd1284; }

	// end inline asm
	add.s64 	%rd3278, %rd2812, 3728;
	// begin inline asm
	{ atom.add.f64 %fd8421,[%rd3278],%fd1283; }

	// end inline asm
	add.s64 	%rd3279, %rd2812, 3736;
	// begin inline asm
	{ atom.add.f64 %fd8423,[%rd3279],%fd1282; }

	// end inline asm
	add.s64 	%rd3280, %rd2812, 3744;
	// begin inline asm
	{ atom.add.f64 %fd8425,[%rd3280],%fd2229; }

	// end inline asm
	add.s64 	%rd3281, %rd2812, 3752;
	// begin inline asm
	{ atom.add.f64 %fd8427,[%rd3281],%fd2228; }

	// end inline asm
	add.s64 	%rd3282, %rd2812, 3760;
	// begin inline asm
	{ atom.add.f64 %fd8429,[%rd3282],%fd2227; }

	// end inline asm
	add.s64 	%rd3283, %rd2812, 3768;
	// begin inline asm
	{ atom.add.f64 %fd8431,[%rd3283],%fd2202; }

	// end inline asm
	add.s64 	%rd3284, %rd2812, 3776;
	// begin inline asm
	{ atom.add.f64 %fd8433,[%rd3284],%fd2201; }

	// end inline asm
	add.s64 	%rd3285, %rd2812, 3784;
	// begin inline asm
	{ atom.add.f64 %fd8435,[%rd3285],%fd2200; }

	// end inline asm
	add.s64 	%rd3286, %rd2812, 3792;
	// begin inline asm
	{ atom.add.f64 %fd8437,[%rd3286],%fd2175; }

	// end inline asm
	add.s64 	%rd3287, %rd2812, 3800;
	// begin inline asm
	{ atom.add.f64 %fd8439,[%rd3287],%fd2174; }

	// end inline asm
	add.s64 	%rd3288, %rd2812, 3808;
	// begin inline asm
	{ atom.add.f64 %fd8441,[%rd3288],%fd2173; }

	// end inline asm
	add.s64 	%rd3289, %rd2812, 3816;
	// begin inline asm
	{ atom.add.f64 %fd8443,[%rd3289],%fd2148; }

	// end inline asm
	add.s64 	%rd3290, %rd2812, 3824;
	// begin inline asm
	{ atom.add.f64 %fd8445,[%rd3290],%fd2147; }

	// end inline asm
	add.s64 	%rd3291, %rd2812, 3832;
	// begin inline asm
	{ atom.add.f64 %fd8447,[%rd3291],%fd2146; }

	// end inline asm
	add.s64 	%rd3292, %rd2812, 3840;
	// begin inline asm
	{ atom.add.f64 %fd8449,[%rd3292],%fd1362; }

	// end inline asm
	add.s64 	%rd3293, %rd2812, 3848;
	// begin inline asm
	{ atom.add.f64 %fd8451,[%rd3293],%fd1361; }

	// end inline asm
	add.s64 	%rd3294, %rd2812, 3856;
	// begin inline asm
	{ atom.add.f64 %fd8453,[%rd3294],%fd1360; }

	// end inline asm
	add.s64 	%rd3295, %rd2812, 3864;
	// begin inline asm
	{ atom.add.f64 %fd8455,[%rd3295],%fd1335; }

	// end inline asm
	add.s64 	%rd3296, %rd2812, 3872;
	// begin inline asm
	{ atom.add.f64 %fd8457,[%rd3296],%fd1334; }

	// end inline asm
	add.s64 	%rd3297, %rd2812, 3880;
	// begin inline asm
	{ atom.add.f64 %fd8459,[%rd3297],%fd1333; }

	// end inline asm
	add.s64 	%rd3298, %rd2812, 3888;
	// begin inline asm
	{ atom.add.f64 %fd8461,[%rd3298],%fd1308; }

	// end inline asm
	add.s64 	%rd3299, %rd2812, 3896;
	// begin inline asm
	{ atom.add.f64 %fd8463,[%rd3299],%fd1307; }

	// end inline asm
	add.s64 	%rd3300, %rd2812, 3904;
	// begin inline asm
	{ atom.add.f64 %fd8465,[%rd3300],%fd1306; }

	// end inline asm
	add.s64 	%rd3301, %rd2812, 3912;
	// begin inline asm
	{ atom.add.f64 %fd8467,[%rd3301],%fd1281; }

	// end inline asm
	add.s64 	%rd3302, %rd2812, 3920;
	// begin inline asm
	{ atom.add.f64 %fd8469,[%rd3302],%fd1280; }

	// end inline asm
	add.s64 	%rd3303, %rd2812, 3928;
	// begin inline asm
	{ atom.add.f64 %fd8471,[%rd3303],%fd1279; }

	// end inline asm
	add.s64 	%rd3304, %rd2812, 3936;
	// begin inline asm
	{ atom.add.f64 %fd8473,[%rd3304],%fd2226; }

	// end inline asm
	add.s64 	%rd3305, %rd2812, 3944;
	// begin inline asm
	{ atom.add.f64 %fd8475,[%rd3305],%fd2225; }

	// end inline asm
	add.s64 	%rd3306, %rd2812, 3952;
	// begin inline asm
	{ atom.add.f64 %fd8477,[%rd3306],%fd2224; }

	// end inline asm
	add.s64 	%rd3307, %rd2812, 3960;
	// begin inline asm
	{ atom.add.f64 %fd8479,[%rd3307],%fd2199; }

	// end inline asm
	add.s64 	%rd3308, %rd2812, 3968;
	// begin inline asm
	{ atom.add.f64 %fd8481,[%rd3308],%fd2198; }

	// end inline asm
	add.s64 	%rd3309, %rd2812, 3976;
	// begin inline asm
	{ atom.add.f64 %fd8483,[%rd3309],%fd2197; }

	// end inline asm
	add.s64 	%rd3310, %rd2812, 3984;
	// begin inline asm
	{ atom.add.f64 %fd8485,[%rd3310],%fd2172; }

	// end inline asm
	add.s64 	%rd3311, %rd2812, 3992;
	// begin inline asm
	{ atom.add.f64 %fd8487,[%rd3311],%fd2171; }

	// end inline asm
	add.s64 	%rd3312, %rd2812, 4000;
	// begin inline asm
	{ atom.add.f64 %fd8489,[%rd3312],%fd2170; }

	// end inline asm
	add.s64 	%rd3313, %rd2812, 4008;
	// begin inline asm
	{ atom.add.f64 %fd8491,[%rd3313],%fd2145; }

	// end inline asm
	add.s64 	%rd3314, %rd2812, 4016;
	// begin inline asm
	{ atom.add.f64 %fd8493,[%rd3314],%fd2144; }

	// end inline asm
	add.s64 	%rd3315, %rd2812, 4024;
	// begin inline asm
	{ atom.add.f64 %fd8495,[%rd3315],%fd2143; }

	// end inline asm
	add.s64 	%rd3316, %rd2812, 4032;
	// begin inline asm
	{ atom.add.f64 %fd8497,[%rd3316],%fd1260; }

	// end inline asm
	add.s64 	%rd3317, %rd2812, 4040;
	// begin inline asm
	{ atom.add.f64 %fd8499,[%rd3317],%fd1259; }

	// end inline asm
	add.s64 	%rd3318, %rd2812, 4048;
	// begin inline asm
	{ atom.add.f64 %fd8501,[%rd3318],%fd1258; }

	// end inline asm
	add.s64 	%rd3319, %rd2812, 4056;
	// begin inline asm
	{ atom.add.f64 %fd8503,[%rd3319],%fd1233; }

	// end inline asm
	add.s64 	%rd3320, %rd2812, 4064;
	// begin inline asm
	{ atom.add.f64 %fd8505,[%rd3320],%fd1232; }

	// end inline asm
	add.s64 	%rd3321, %rd2812, 4072;
	// begin inline asm
	{ atom.add.f64 %fd8507,[%rd3321],%fd1231; }

	// end inline asm
	add.s64 	%rd3322, %rd2812, 4080;
	// begin inline asm
	{ atom.add.f64 %fd8509,[%rd3322],%fd1206; }

	// end inline asm
	add.s64 	%rd3323, %rd2812, 4088;
	// begin inline asm
	{ atom.add.f64 %fd8511,[%rd3323],%fd1205; }

	// end inline asm
	add.s64 	%rd3324, %rd2812, 4096;
	// begin inline asm
	{ atom.add.f64 %fd8513,[%rd3324],%fd1204; }

	// end inline asm
	add.s64 	%rd3325, %rd2812, 4104;
	// begin inline asm
	{ atom.add.f64 %fd8515,[%rd3325],%fd1179; }

	// end inline asm
	add.s64 	%rd3326, %rd2812, 4112;
	// begin inline asm
	{ atom.add.f64 %fd8517,[%rd3326],%fd1178; }

	// end inline asm
	add.s64 	%rd3327, %rd2812, 4120;
	// begin inline asm
	{ atom.add.f64 %fd8519,[%rd3327],%fd1177; }

	// end inline asm
	add.s64 	%rd3328, %rd2812, 4128;
	// begin inline asm
	{ atom.add.f64 %fd8521,[%rd3328],%fd2124; }

	// end inline asm
	add.s64 	%rd3329, %rd2812, 4136;
	// begin inline asm
	{ atom.add.f64 %fd8523,[%rd3329],%fd2123; }

	// end inline asm
	add.s64 	%rd3330, %rd2812, 4144;
	// begin inline asm
	{ atom.add.f64 %fd8525,[%rd3330],%fd2122; }

	// end inline asm
	add.s64 	%rd3331, %rd2812, 4152;
	// begin inline asm
	{ atom.add.f64 %fd8527,[%rd3331],%fd2097; }

	// end inline asm
	add.s64 	%rd3332, %rd2812, 4160;
	// begin inline asm
	{ atom.add.f64 %fd8529,[%rd3332],%fd2096; }

	// end inline asm
	add.s64 	%rd3333, %rd2812, 4168;
	// begin inline asm
	{ atom.add.f64 %fd8531,[%rd3333],%fd2095; }

	// end inline asm
	add.s64 	%rd3334, %rd2812, 4176;
	// begin inline asm
	{ atom.add.f64 %fd8533,[%rd3334],%fd2070; }

	// end inline asm
	add.s64 	%rd3335, %rd2812, 4184;
	// begin inline asm
	{ atom.add.f64 %fd8535,[%rd3335],%fd2069; }

	// end inline asm
	add.s64 	%rd3336, %rd2812, 4192;
	// begin inline asm
	{ atom.add.f64 %fd8537,[%rd3336],%fd2068; }

	// end inline asm
	add.s64 	%rd3337, %rd2812, 4200;
	// begin inline asm
	{ atom.add.f64 %fd8539,[%rd3337],%fd2043; }

	// end inline asm
	add.s64 	%rd3338, %rd2812, 4208;
	// begin inline asm
	{ atom.add.f64 %fd8541,[%rd3338],%fd2042; }

	// end inline asm
	add.s64 	%rd3339, %rd2812, 4216;
	// begin inline asm
	{ atom.add.f64 %fd8543,[%rd3339],%fd2041; }

	// end inline asm
	add.s64 	%rd3340, %rd2812, 4224;
	// begin inline asm
	{ atom.add.f64 %fd8545,[%rd3340],%fd1257; }

	// end inline asm
	add.s64 	%rd3341, %rd2812, 4232;
	// begin inline asm
	{ atom.add.f64 %fd8547,[%rd3341],%fd1256; }

	// end inline asm
	add.s64 	%rd3342, %rd2812, 4240;
	// begin inline asm
	{ atom.add.f64 %fd8549,[%rd3342],%fd1255; }

	// end inline asm
	add.s64 	%rd3343, %rd2812, 4248;
	// begin inline asm
	{ atom.add.f64 %fd8551,[%rd3343],%fd1230; }

	// end inline asm
	add.s64 	%rd3344, %rd2812, 4256;
	// begin inline asm
	{ atom.add.f64 %fd8553,[%rd3344],%fd1229; }

	// end inline asm
	add.s64 	%rd3345, %rd2812, 4264;
	// begin inline asm
	{ atom.add.f64 %fd8555,[%rd3345],%fd1228; }

	// end inline asm
	add.s64 	%rd3346, %rd2812, 4272;
	// begin inline asm
	{ atom.add.f64 %fd8557,[%rd3346],%fd1203; }

	// end inline asm
	add.s64 	%rd3347, %rd2812, 4280;
	// begin inline asm
	{ atom.add.f64 %fd8559,[%rd3347],%fd1202; }

	// end inline asm
	add.s64 	%rd3348, %rd2812, 4288;
	// begin inline asm
	{ atom.add.f64 %fd8561,[%rd3348],%fd1201; }

	// end inline asm
	add.s64 	%rd3349, %rd2812, 4296;
	// begin inline asm
	{ atom.add.f64 %fd8563,[%rd3349],%fd1176; }

	// end inline asm
	add.s64 	%rd3350, %rd2812, 4304;
	// begin inline asm
	{ atom.add.f64 %fd8565,[%rd3350],%fd1175; }

	// end inline asm
	add.s64 	%rd3351, %rd2812, 4312;
	// begin inline asm
	{ atom.add.f64 %fd8567,[%rd3351],%fd1174; }

	// end inline asm
	add.s64 	%rd3352, %rd2812, 4320;
	// begin inline asm
	{ atom.add.f64 %fd8569,[%rd3352],%fd2121; }

	// end inline asm
	add.s64 	%rd3353, %rd2812, 4328;
	// begin inline asm
	{ atom.add.f64 %fd8571,[%rd3353],%fd2120; }

	// end inline asm
	add.s64 	%rd3354, %rd2812, 4336;
	// begin inline asm
	{ atom.add.f64 %fd8573,[%rd3354],%fd2119; }

	// end inline asm
	add.s64 	%rd3355, %rd2812, 4344;
	// begin inline asm
	{ atom.add.f64 %fd8575,[%rd3355],%fd2094; }

	// end inline asm
	add.s64 	%rd3356, %rd2812, 4352;
	// begin inline asm
	{ atom.add.f64 %fd8577,[%rd3356],%fd2093; }

	// end inline asm
	add.s64 	%rd3357, %rd2812, 4360;
	// begin inline asm
	{ atom.add.f64 %fd8579,[%rd3357],%fd2092; }

	// end inline asm
	add.s64 	%rd3358, %rd2812, 4368;
	// begin inline asm
	{ atom.add.f64 %fd8581,[%rd3358],%fd2067; }

	// end inline asm
	add.s64 	%rd3359, %rd2812, 4376;
	// begin inline asm
	{ atom.add.f64 %fd8583,[%rd3359],%fd2066; }

	// end inline asm
	add.s64 	%rd3360, %rd2812, 4384;
	// begin inline asm
	{ atom.add.f64 %fd8585,[%rd3360],%fd2065; }

	// end inline asm
	add.s64 	%rd3361, %rd2812, 4392;
	// begin inline asm
	{ atom.add.f64 %fd8587,[%rd3361],%fd2040; }

	// end inline asm
	add.s64 	%rd3362, %rd2812, 4400;
	// begin inline asm
	{ atom.add.f64 %fd8589,[%rd3362],%fd2039; }

	// end inline asm
	add.s64 	%rd3363, %rd2812, 4408;
	// begin inline asm
	{ atom.add.f64 %fd8591,[%rd3363],%fd2038; }

	// end inline asm
	add.s64 	%rd3364, %rd2812, 4416;
	// begin inline asm
	{ atom.add.f64 %fd8593,[%rd3364],%fd1254; }

	// end inline asm
	add.s64 	%rd3365, %rd2812, 4424;
	// begin inline asm
	{ atom.add.f64 %fd8595,[%rd3365],%fd1253; }

	// end inline asm
	add.s64 	%rd3366, %rd2812, 4432;
	// begin inline asm
	{ atom.add.f64 %fd8597,[%rd3366],%fd1252; }

	// end inline asm
	add.s64 	%rd3367, %rd2812, 4440;
	// begin inline asm
	{ atom.add.f64 %fd8599,[%rd3367],%fd1227; }

	// end inline asm
	add.s64 	%rd3368, %rd2812, 4448;
	// begin inline asm
	{ atom.add.f64 %fd8601,[%rd3368],%fd1226; }

	// end inline asm
	add.s64 	%rd3369, %rd2812, 4456;
	// begin inline asm
	{ atom.add.f64 %fd8603,[%rd3369],%fd1225; }

	// end inline asm
	add.s64 	%rd3370, %rd2812, 4464;
	// begin inline asm
	{ atom.add.f64 %fd8605,[%rd3370],%fd1200; }

	// end inline asm
	add.s64 	%rd3371, %rd2812, 4472;
	// begin inline asm
	{ atom.add.f64 %fd8607,[%rd3371],%fd1199; }

	// end inline asm
	add.s64 	%rd3372, %rd2812, 4480;
	// begin inline asm
	{ atom.add.f64 %fd8609,[%rd3372],%fd1198; }

	// end inline asm
	add.s64 	%rd3373, %rd2812, 4488;
	// begin inline asm
	{ atom.add.f64 %fd8611,[%rd3373],%fd1173; }

	// end inline asm
	add.s64 	%rd3374, %rd2812, 4496;
	// begin inline asm
	{ atom.add.f64 %fd8613,[%rd3374],%fd1172; }

	// end inline asm
	add.s64 	%rd3375, %rd2812, 4504;
	// begin inline asm
	{ atom.add.f64 %fd8615,[%rd3375],%fd1171; }

	// end inline asm
	add.s64 	%rd3376, %rd2812, 4512;
	// begin inline asm
	{ atom.add.f64 %fd8617,[%rd3376],%fd2118; }

	// end inline asm
	add.s64 	%rd3377, %rd2812, 4520;
	// begin inline asm
	{ atom.add.f64 %fd8619,[%rd3377],%fd2117; }

	// end inline asm
	add.s64 	%rd3378, %rd2812, 4528;
	// begin inline asm
	{ atom.add.f64 %fd8621,[%rd3378],%fd2116; }

	// end inline asm
	add.s64 	%rd3379, %rd2812, 4536;
	// begin inline asm
	{ atom.add.f64 %fd8623,[%rd3379],%fd2091; }

	// end inline asm
	add.s64 	%rd3380, %rd2812, 4544;
	// begin inline asm
	{ atom.add.f64 %fd8625,[%rd3380],%fd2090; }

	// end inline asm
	add.s64 	%rd3381, %rd2812, 4552;
	// begin inline asm
	{ atom.add.f64 %fd8627,[%rd3381],%fd2089; }

	// end inline asm
	add.s64 	%rd3382, %rd2812, 4560;
	// begin inline asm
	{ atom.add.f64 %fd8629,[%rd3382],%fd2064; }

	// end inline asm
	add.s64 	%rd3383, %rd2812, 4568;
	// begin inline asm
	{ atom.add.f64 %fd8631,[%rd3383],%fd2063; }

	// end inline asm
	add.s64 	%rd3384, %rd2812, 4576;
	// begin inline asm
	{ atom.add.f64 %fd8633,[%rd3384],%fd2062; }

	// end inline asm
	add.s64 	%rd3385, %rd2812, 4584;
	// begin inline asm
	{ atom.add.f64 %fd8635,[%rd3385],%fd2037; }

	// end inline asm
	add.s64 	%rd3386, %rd2812, 4592;
	// begin inline asm
	{ atom.add.f64 %fd8637,[%rd3386],%fd2036; }

	// end inline asm
	add.s64 	%rd3387, %rd2812, 4600;
	// begin inline asm
	{ atom.add.f64 %fd8639,[%rd3387],%fd2035; }

	// end inline asm
	bra.uni 	$L__BB1_661;

$L__BB1_659:
	setp.eq.s64 	%p940, %rd362, 0;
	@%p940 bra 	$L__BB1_661;

	cvt.s64.s32 	%rd3966, %r10;
	mul.lo.s64 	%rd3967, %rd3966, %rd11;
	add.s64 	%rd3390, %rd362, %rd3967;
	// begin inline asm
	{ atom.add.f64 %fd8641,[%rd3390],%fd2880; }

	// end inline asm
	add.s64 	%rd3391, %rd3390, 8;
	// begin inline asm
	{ atom.add.f64 %fd8643,[%rd3391],%fd2879; }

	// end inline asm
	add.s64 	%rd3392, %rd3390, 16;
	// begin inline asm
	{ atom.add.f64 %fd8645,[%rd3392],%fd2878; }

	// end inline asm
	add.s64 	%rd3393, %rd3390, 24;
	// begin inline asm
	{ atom.add.f64 %fd8647,[%rd3393],%fd2853; }

	// end inline asm
	add.s64 	%rd3394, %rd3390, 32;
	// begin inline asm
	{ atom.add.f64 %fd8649,[%rd3394],%fd2852; }

	// end inline asm
	add.s64 	%rd3395, %rd3390, 40;
	// begin inline asm
	{ atom.add.f64 %fd8651,[%rd3395],%fd2851; }

	// end inline asm
	add.s64 	%rd3396, %rd3390, 48;
	// begin inline asm
	{ atom.add.f64 %fd8653,[%rd3396],%fd2826; }

	// end inline asm
	add.s64 	%rd3397, %rd3390, 56;
	// begin inline asm
	{ atom.add.f64 %fd8655,[%rd3397],%fd2825; }

	// end inline asm
	add.s64 	%rd3398, %rd3390, 64;
	// begin inline asm
	{ atom.add.f64 %fd8657,[%rd3398],%fd2824; }

	// end inline asm
	add.s64 	%rd3399, %rd3390, 72;
	// begin inline asm
	{ atom.add.f64 %fd8659,[%rd3399],%fd2799; }

	// end inline asm
	add.s64 	%rd3400, %rd3390, 80;
	// begin inline asm
	{ atom.add.f64 %fd8661,[%rd3400],%fd2798; }

	// end inline asm
	add.s64 	%rd3401, %rd3390, 88;
	// begin inline asm
	{ atom.add.f64 %fd8663,[%rd3401],%fd2797; }

	// end inline asm
	add.s64 	%rd3402, %rd3390, 96;
	// begin inline asm
	{ atom.add.f64 %fd8665,[%rd3402],%fd2016; }

	// end inline asm
	add.s64 	%rd3403, %rd3390, 104;
	// begin inline asm
	{ atom.add.f64 %fd8667,[%rd3403],%fd2015; }

	// end inline asm
	add.s64 	%rd3404, %rd3390, 112;
	// begin inline asm
	{ atom.add.f64 %fd8669,[%rd3404],%fd2014; }

	// end inline asm
	add.s64 	%rd3405, %rd3390, 120;
	// begin inline asm
	{ atom.add.f64 %fd8671,[%rd3405],%fd1989; }

	// end inline asm
	add.s64 	%rd3406, %rd3390, 128;
	// begin inline asm
	{ atom.add.f64 %fd8673,[%rd3406],%fd1988; }

	// end inline asm
	add.s64 	%rd3407, %rd3390, 136;
	// begin inline asm
	{ atom.add.f64 %fd8675,[%rd3407],%fd1987; }

	// end inline asm
	add.s64 	%rd3408, %rd3390, 144;
	// begin inline asm
	{ atom.add.f64 %fd8677,[%rd3408],%fd1962; }

	// end inline asm
	add.s64 	%rd3409, %rd3390, 152;
	// begin inline asm
	{ atom.add.f64 %fd8679,[%rd3409],%fd1961; }

	// end inline asm
	add.s64 	%rd3410, %rd3390, 160;
	// begin inline asm
	{ atom.add.f64 %fd8681,[%rd3410],%fd1960; }

	// end inline asm
	add.s64 	%rd3411, %rd3390, 168;
	// begin inline asm
	{ atom.add.f64 %fd8683,[%rd3411],%fd1935; }

	// end inline asm
	add.s64 	%rd3412, %rd3390, 176;
	// begin inline asm
	{ atom.add.f64 %fd8685,[%rd3412],%fd1934; }

	// end inline asm
	add.s64 	%rd3413, %rd3390, 184;
	// begin inline asm
	{ atom.add.f64 %fd8687,[%rd3413],%fd1933; }

	// end inline asm
	add.s64 	%rd3414, %rd3390, 192;
	// begin inline asm
	{ atom.add.f64 %fd8689,[%rd3414],%fd2877; }

	// end inline asm
	add.s64 	%rd3415, %rd3390, 200;
	// begin inline asm
	{ atom.add.f64 %fd8691,[%rd3415],%fd2876; }

	// end inline asm
	add.s64 	%rd3416, %rd3390, 208;
	// begin inline asm
	{ atom.add.f64 %fd8693,[%rd3416],%fd2875; }

	// end inline asm
	add.s64 	%rd3417, %rd3390, 216;
	// begin inline asm
	{ atom.add.f64 %fd8695,[%rd3417],%fd2850; }

	// end inline asm
	add.s64 	%rd3418, %rd3390, 224;
	// begin inline asm
	{ atom.add.f64 %fd8697,[%rd3418],%fd2849; }

	// end inline asm
	add.s64 	%rd3419, %rd3390, 232;
	// begin inline asm
	{ atom.add.f64 %fd8699,[%rd3419],%fd2848; }

	// end inline asm
	add.s64 	%rd3420, %rd3390, 240;
	// begin inline asm
	{ atom.add.f64 %fd8701,[%rd3420],%fd2823; }

	// end inline asm
	add.s64 	%rd3421, %rd3390, 248;
	// begin inline asm
	{ atom.add.f64 %fd8703,[%rd3421],%fd2822; }

	// end inline asm
	add.s64 	%rd3422, %rd3390, 256;
	// begin inline asm
	{ atom.add.f64 %fd8705,[%rd3422],%fd2821; }

	// end inline asm
	add.s64 	%rd3423, %rd3390, 264;
	// begin inline asm
	{ atom.add.f64 %fd8707,[%rd3423],%fd2796; }

	// end inline asm
	add.s64 	%rd3424, %rd3390, 272;
	// begin inline asm
	{ atom.add.f64 %fd8709,[%rd3424],%fd2795; }

	// end inline asm
	add.s64 	%rd3425, %rd3390, 280;
	// begin inline asm
	{ atom.add.f64 %fd8711,[%rd3425],%fd2794; }

	// end inline asm
	add.s64 	%rd3426, %rd3390, 288;
	// begin inline asm
	{ atom.add.f64 %fd8713,[%rd3426],%fd2013; }

	// end inline asm
	add.s64 	%rd3427, %rd3390, 296;
	// begin inline asm
	{ atom.add.f64 %fd8715,[%rd3427],%fd2012; }

	// end inline asm
	add.s64 	%rd3428, %rd3390, 304;
	// begin inline asm
	{ atom.add.f64 %fd8717,[%rd3428],%fd2011; }

	// end inline asm
	add.s64 	%rd3429, %rd3390, 312;
	// begin inline asm
	{ atom.add.f64 %fd8719,[%rd3429],%fd1986; }

	// end inline asm
	add.s64 	%rd3430, %rd3390, 320;
	// begin inline asm
	{ atom.add.f64 %fd8721,[%rd3430],%fd1985; }

	// end inline asm
	add.s64 	%rd3431, %rd3390, 328;
	// begin inline asm
	{ atom.add.f64 %fd8723,[%rd3431],%fd1984; }

	// end inline asm
	add.s64 	%rd3432, %rd3390, 336;
	// begin inline asm
	{ atom.add.f64 %fd8725,[%rd3432],%fd1959; }

	// end inline asm
	add.s64 	%rd3433, %rd3390, 344;
	// begin inline asm
	{ atom.add.f64 %fd8727,[%rd3433],%fd1958; }

	// end inline asm
	add.s64 	%rd3434, %rd3390, 352;
	// begin inline asm
	{ atom.add.f64 %fd8729,[%rd3434],%fd1957; }

	// end inline asm
	add.s64 	%rd3435, %rd3390, 360;
	// begin inline asm
	{ atom.add.f64 %fd8731,[%rd3435],%fd1932; }

	// end inline asm
	add.s64 	%rd3436, %rd3390, 368;
	// begin inline asm
	{ atom.add.f64 %fd8733,[%rd3436],%fd1931; }

	// end inline asm
	add.s64 	%rd3437, %rd3390, 376;
	// begin inline asm
	{ atom.add.f64 %fd8735,[%rd3437],%fd1930; }

	// end inline asm
	add.s64 	%rd3438, %rd3390, 384;
	// begin inline asm
	{ atom.add.f64 %fd8737,[%rd3438],%fd2874; }

	// end inline asm
	add.s64 	%rd3439, %rd3390, 392;
	// begin inline asm
	{ atom.add.f64 %fd8739,[%rd3439],%fd2873; }

	// end inline asm
	add.s64 	%rd3440, %rd3390, 400;
	// begin inline asm
	{ atom.add.f64 %fd8741,[%rd3440],%fd2872; }

	// end inline asm
	add.s64 	%rd3441, %rd3390, 408;
	// begin inline asm
	{ atom.add.f64 %fd8743,[%rd3441],%fd2847; }

	// end inline asm
	add.s64 	%rd3442, %rd3390, 416;
	// begin inline asm
	{ atom.add.f64 %fd8745,[%rd3442],%fd2846; }

	// end inline asm
	add.s64 	%rd3443, %rd3390, 424;
	// begin inline asm
	{ atom.add.f64 %fd8747,[%rd3443],%fd2845; }

	// end inline asm
	add.s64 	%rd3444, %rd3390, 432;
	// begin inline asm
	{ atom.add.f64 %fd8749,[%rd3444],%fd2820; }

	// end inline asm
	add.s64 	%rd3445, %rd3390, 440;
	// begin inline asm
	{ atom.add.f64 %fd8751,[%rd3445],%fd2819; }

	// end inline asm
	add.s64 	%rd3446, %rd3390, 448;
	// begin inline asm
	{ atom.add.f64 %fd8753,[%rd3446],%fd2818; }

	// end inline asm
	add.s64 	%rd3447, %rd3390, 456;
	// begin inline asm
	{ atom.add.f64 %fd8755,[%rd3447],%fd2793; }

	// end inline asm
	add.s64 	%rd3448, %rd3390, 464;
	// begin inline asm
	{ atom.add.f64 %fd8757,[%rd3448],%fd2792; }

	// end inline asm
	add.s64 	%rd3449, %rd3390, 472;
	// begin inline asm
	{ atom.add.f64 %fd8759,[%rd3449],%fd2791; }

	// end inline asm
	add.s64 	%rd3450, %rd3390, 480;
	// begin inline asm
	{ atom.add.f64 %fd8761,[%rd3450],%fd2010; }

	// end inline asm
	add.s64 	%rd3451, %rd3390, 488;
	// begin inline asm
	{ atom.add.f64 %fd8763,[%rd3451],%fd2009; }

	// end inline asm
	add.s64 	%rd3452, %rd3390, 496;
	// begin inline asm
	{ atom.add.f64 %fd8765,[%rd3452],%fd2008; }

	// end inline asm
	add.s64 	%rd3453, %rd3390, 504;
	// begin inline asm
	{ atom.add.f64 %fd8767,[%rd3453],%fd1983; }

	// end inline asm
	add.s64 	%rd3454, %rd3390, 512;
	// begin inline asm
	{ atom.add.f64 %fd8769,[%rd3454],%fd1982; }

	// end inline asm
	add.s64 	%rd3455, %rd3390, 520;
	// begin inline asm
	{ atom.add.f64 %fd8771,[%rd3455],%fd1981; }

	// end inline asm
	add.s64 	%rd3456, %rd3390, 528;
	// begin inline asm
	{ atom.add.f64 %fd8773,[%rd3456],%fd1956; }

	// end inline asm
	add.s64 	%rd3457, %rd3390, 536;
	// begin inline asm
	{ atom.add.f64 %fd8775,[%rd3457],%fd1955; }

	// end inline asm
	add.s64 	%rd3458, %rd3390, 544;
	// begin inline asm
	{ atom.add.f64 %fd8777,[%rd3458],%fd1954; }

	// end inline asm
	add.s64 	%rd3459, %rd3390, 552;
	// begin inline asm
	{ atom.add.f64 %fd8779,[%rd3459],%fd1929; }

	// end inline asm
	add.s64 	%rd3460, %rd3390, 560;
	// begin inline asm
	{ atom.add.f64 %fd8781,[%rd3460],%fd1928; }

	// end inline asm
	add.s64 	%rd3461, %rd3390, 568;
	// begin inline asm
	{ atom.add.f64 %fd8783,[%rd3461],%fd1927; }

	// end inline asm
	add.s64 	%rd3462, %rd3390, 576;
	// begin inline asm
	{ atom.add.f64 %fd8785,[%rd3462],%fd2772; }

	// end inline asm
	add.s64 	%rd3463, %rd3390, 584;
	// begin inline asm
	{ atom.add.f64 %fd8787,[%rd3463],%fd2771; }

	// end inline asm
	add.s64 	%rd3464, %rd3390, 592;
	// begin inline asm
	{ atom.add.f64 %fd8789,[%rd3464],%fd2770; }

	// end inline asm
	add.s64 	%rd3465, %rd3390, 600;
	// begin inline asm
	{ atom.add.f64 %fd8791,[%rd3465],%fd2745; }

	// end inline asm
	add.s64 	%rd3466, %rd3390, 608;
	// begin inline asm
	{ atom.add.f64 %fd8793,[%rd3466],%fd2744; }

	// end inline asm
	add.s64 	%rd3467, %rd3390, 616;
	// begin inline asm
	{ atom.add.f64 %fd8795,[%rd3467],%fd2743; }

	// end inline asm
	add.s64 	%rd3468, %rd3390, 624;
	// begin inline asm
	{ atom.add.f64 %fd8797,[%rd3468],%fd2718; }

	// end inline asm
	add.s64 	%rd3469, %rd3390, 632;
	// begin inline asm
	{ atom.add.f64 %fd8799,[%rd3469],%fd2717; }

	// end inline asm
	add.s64 	%rd3470, %rd3390, 640;
	// begin inline asm
	{ atom.add.f64 %fd8801,[%rd3470],%fd2716; }

	// end inline asm
	add.s64 	%rd3471, %rd3390, 648;
	// begin inline asm
	{ atom.add.f64 %fd8803,[%rd3471],%fd2691; }

	// end inline asm
	add.s64 	%rd3472, %rd3390, 656;
	// begin inline asm
	{ atom.add.f64 %fd8805,[%rd3472],%fd2690; }

	// end inline asm
	add.s64 	%rd3473, %rd3390, 664;
	// begin inline asm
	{ atom.add.f64 %fd8807,[%rd3473],%fd2689; }

	// end inline asm
	add.s64 	%rd3474, %rd3390, 672;
	// begin inline asm
	{ atom.add.f64 %fd8809,[%rd3474],%fd1908; }

	// end inline asm
	add.s64 	%rd3475, %rd3390, 680;
	// begin inline asm
	{ atom.add.f64 %fd8811,[%rd3475],%fd1907; }

	// end inline asm
	add.s64 	%rd3476, %rd3390, 688;
	// begin inline asm
	{ atom.add.f64 %fd8813,[%rd3476],%fd1906; }

	// end inline asm
	add.s64 	%rd3477, %rd3390, 696;
	// begin inline asm
	{ atom.add.f64 %fd8815,[%rd3477],%fd1881; }

	// end inline asm
	add.s64 	%rd3478, %rd3390, 704;
	// begin inline asm
	{ atom.add.f64 %fd8817,[%rd3478],%fd1880; }

	// end inline asm
	add.s64 	%rd3479, %rd3390, 712;
	// begin inline asm
	{ atom.add.f64 %fd8819,[%rd3479],%fd1879; }

	// end inline asm
	add.s64 	%rd3480, %rd3390, 720;
	// begin inline asm
	{ atom.add.f64 %fd8821,[%rd3480],%fd1854; }

	// end inline asm
	add.s64 	%rd3481, %rd3390, 728;
	// begin inline asm
	{ atom.add.f64 %fd8823,[%rd3481],%fd1853; }

	// end inline asm
	add.s64 	%rd3482, %rd3390, 736;
	// begin inline asm
	{ atom.add.f64 %fd8825,[%rd3482],%fd1852; }

	// end inline asm
	add.s64 	%rd3483, %rd3390, 744;
	// begin inline asm
	{ atom.add.f64 %fd8827,[%rd3483],%fd1827; }

	// end inline asm
	add.s64 	%rd3484, %rd3390, 752;
	// begin inline asm
	{ atom.add.f64 %fd8829,[%rd3484],%fd1826; }

	// end inline asm
	add.s64 	%rd3485, %rd3390, 760;
	// begin inline asm
	{ atom.add.f64 %fd8831,[%rd3485],%fd1825; }

	// end inline asm
	add.s64 	%rd3486, %rd3390, 768;
	// begin inline asm
	{ atom.add.f64 %fd8833,[%rd3486],%fd2769; }

	// end inline asm
	add.s64 	%rd3487, %rd3390, 776;
	// begin inline asm
	{ atom.add.f64 %fd8835,[%rd3487],%fd2768; }

	// end inline asm
	add.s64 	%rd3488, %rd3390, 784;
	// begin inline asm
	{ atom.add.f64 %fd8837,[%rd3488],%fd2767; }

	// end inline asm
	add.s64 	%rd3489, %rd3390, 792;
	// begin inline asm
	{ atom.add.f64 %fd8839,[%rd3489],%fd2742; }

	// end inline asm
	add.s64 	%rd3490, %rd3390, 800;
	// begin inline asm
	{ atom.add.f64 %fd8841,[%rd3490],%fd2741; }

	// end inline asm
	add.s64 	%rd3491, %rd3390, 808;
	// begin inline asm
	{ atom.add.f64 %fd8843,[%rd3491],%fd2740; }

	// end inline asm
	add.s64 	%rd3492, %rd3390, 816;
	// begin inline asm
	{ atom.add.f64 %fd8845,[%rd3492],%fd2715; }

	// end inline asm
	add.s64 	%rd3493, %rd3390, 824;
	// begin inline asm
	{ atom.add.f64 %fd8847,[%rd3493],%fd2714; }

	// end inline asm
	add.s64 	%rd3494, %rd3390, 832;
	// begin inline asm
	{ atom.add.f64 %fd8849,[%rd3494],%fd2713; }

	// end inline asm
	add.s64 	%rd3495, %rd3390, 840;
	// begin inline asm
	{ atom.add.f64 %fd8851,[%rd3495],%fd2688; }

	// end inline asm
	add.s64 	%rd3496, %rd3390, 848;
	// begin inline asm
	{ atom.add.f64 %fd8853,[%rd3496],%fd2687; }

	// end inline asm
	add.s64 	%rd3497, %rd3390, 856;
	// begin inline asm
	{ atom.add.f64 %fd8855,[%rd3497],%fd2686; }

	// end inline asm
	add.s64 	%rd3498, %rd3390, 864;
	// begin inline asm
	{ atom.add.f64 %fd8857,[%rd3498],%fd1905; }

	// end inline asm
	add.s64 	%rd3499, %rd3390, 872;
	// begin inline asm
	{ atom.add.f64 %fd8859,[%rd3499],%fd1904; }

	// end inline asm
	add.s64 	%rd3500, %rd3390, 880;
	// begin inline asm
	{ atom.add.f64 %fd8861,[%rd3500],%fd1903; }

	// end inline asm
	add.s64 	%rd3501, %rd3390, 888;
	// begin inline asm
	{ atom.add.f64 %fd8863,[%rd3501],%fd1878; }

	// end inline asm
	add.s64 	%rd3502, %rd3390, 896;
	// begin inline asm
	{ atom.add.f64 %fd8865,[%rd3502],%fd1877; }

	// end inline asm
	add.s64 	%rd3503, %rd3390, 904;
	// begin inline asm
	{ atom.add.f64 %fd8867,[%rd3503],%fd1876; }

	// end inline asm
	add.s64 	%rd3504, %rd3390, 912;
	// begin inline asm
	{ atom.add.f64 %fd8869,[%rd3504],%fd1851; }

	// end inline asm
	add.s64 	%rd3505, %rd3390, 920;
	// begin inline asm
	{ atom.add.f64 %fd8871,[%rd3505],%fd1850; }

	// end inline asm
	add.s64 	%rd3506, %rd3390, 928;
	// begin inline asm
	{ atom.add.f64 %fd8873,[%rd3506],%fd1849; }

	// end inline asm
	add.s64 	%rd3507, %rd3390, 936;
	// begin inline asm
	{ atom.add.f64 %fd8875,[%rd3507],%fd1824; }

	// end inline asm
	add.s64 	%rd3508, %rd3390, 944;
	// begin inline asm
	{ atom.add.f64 %fd8877,[%rd3508],%fd1823; }

	// end inline asm
	add.s64 	%rd3509, %rd3390, 952;
	// begin inline asm
	{ atom.add.f64 %fd8879,[%rd3509],%fd1822; }

	// end inline asm
	add.s64 	%rd3510, %rd3390, 960;
	// begin inline asm
	{ atom.add.f64 %fd8881,[%rd3510],%fd2766; }

	// end inline asm
	add.s64 	%rd3511, %rd3390, 968;
	// begin inline asm
	{ atom.add.f64 %fd8883,[%rd3511],%fd2765; }

	// end inline asm
	add.s64 	%rd3512, %rd3390, 976;
	// begin inline asm
	{ atom.add.f64 %fd8885,[%rd3512],%fd2764; }

	// end inline asm
	add.s64 	%rd3513, %rd3390, 984;
	// begin inline asm
	{ atom.add.f64 %fd8887,[%rd3513],%fd2739; }

	// end inline asm
	add.s64 	%rd3514, %rd3390, 992;
	// begin inline asm
	{ atom.add.f64 %fd8889,[%rd3514],%fd2738; }

	// end inline asm
	add.s64 	%rd3515, %rd3390, 1000;
	// begin inline asm
	{ atom.add.f64 %fd8891,[%rd3515],%fd2737; }

	// end inline asm
	add.s64 	%rd3516, %rd3390, 1008;
	// begin inline asm
	{ atom.add.f64 %fd8893,[%rd3516],%fd2712; }

	// end inline asm
	add.s64 	%rd3517, %rd3390, 1016;
	// begin inline asm
	{ atom.add.f64 %fd8895,[%rd3517],%fd2711; }

	// end inline asm
	add.s64 	%rd3518, %rd3390, 1024;
	// begin inline asm
	{ atom.add.f64 %fd8897,[%rd3518],%fd2710; }

	// end inline asm
	add.s64 	%rd3519, %rd3390, 1032;
	// begin inline asm
	{ atom.add.f64 %fd8899,[%rd3519],%fd2685; }

	// end inline asm
	add.s64 	%rd3520, %rd3390, 1040;
	// begin inline asm
	{ atom.add.f64 %fd8901,[%rd3520],%fd2684; }

	// end inline asm
	add.s64 	%rd3521, %rd3390, 1048;
	// begin inline asm
	{ atom.add.f64 %fd8903,[%rd3521],%fd2683; }

	// end inline asm
	add.s64 	%rd3522, %rd3390, 1056;
	// begin inline asm
	{ atom.add.f64 %fd8905,[%rd3522],%fd1902; }

	// end inline asm
	add.s64 	%rd3523, %rd3390, 1064;
	// begin inline asm
	{ atom.add.f64 %fd8907,[%rd3523],%fd1901; }

	// end inline asm
	add.s64 	%rd3524, %rd3390, 1072;
	// begin inline asm
	{ atom.add.f64 %fd8909,[%rd3524],%fd1900; }

	// end inline asm
	add.s64 	%rd3525, %rd3390, 1080;
	// begin inline asm
	{ atom.add.f64 %fd8911,[%rd3525],%fd1875; }

	// end inline asm
	add.s64 	%rd3526, %rd3390, 1088;
	// begin inline asm
	{ atom.add.f64 %fd8913,[%rd3526],%fd1874; }

	// end inline asm
	add.s64 	%rd3527, %rd3390, 1096;
	// begin inline asm
	{ atom.add.f64 %fd8915,[%rd3527],%fd1873; }

	// end inline asm
	add.s64 	%rd3528, %rd3390, 1104;
	// begin inline asm
	{ atom.add.f64 %fd8917,[%rd3528],%fd1848; }

	// end inline asm
	add.s64 	%rd3529, %rd3390, 1112;
	// begin inline asm
	{ atom.add.f64 %fd8919,[%rd3529],%fd1847; }

	// end inline asm
	add.s64 	%rd3530, %rd3390, 1120;
	// begin inline asm
	{ atom.add.f64 %fd8921,[%rd3530],%fd1846; }

	// end inline asm
	add.s64 	%rd3531, %rd3390, 1128;
	// begin inline asm
	{ atom.add.f64 %fd8923,[%rd3531],%fd1821; }

	// end inline asm
	add.s64 	%rd3532, %rd3390, 1136;
	// begin inline asm
	{ atom.add.f64 %fd8925,[%rd3532],%fd1820; }

	// end inline asm
	add.s64 	%rd3533, %rd3390, 1144;
	// begin inline asm
	{ atom.add.f64 %fd8927,[%rd3533],%fd1819; }

	// end inline asm
	add.s64 	%rd3534, %rd3390, 1152;
	// begin inline asm
	{ atom.add.f64 %fd8929,[%rd3534],%fd2664; }

	// end inline asm
	add.s64 	%rd3535, %rd3390, 1160;
	// begin inline asm
	{ atom.add.f64 %fd8931,[%rd3535],%fd2663; }

	// end inline asm
	add.s64 	%rd3536, %rd3390, 1168;
	// begin inline asm
	{ atom.add.f64 %fd8933,[%rd3536],%fd2662; }

	// end inline asm
	add.s64 	%rd3537, %rd3390, 1176;
	// begin inline asm
	{ atom.add.f64 %fd8935,[%rd3537],%fd2637; }

	// end inline asm
	add.s64 	%rd3538, %rd3390, 1184;
	// begin inline asm
	{ atom.add.f64 %fd8937,[%rd3538],%fd2636; }

	// end inline asm
	add.s64 	%rd3539, %rd3390, 1192;
	// begin inline asm
	{ atom.add.f64 %fd8939,[%rd3539],%fd2635; }

	// end inline asm
	add.s64 	%rd3540, %rd3390, 1200;
	// begin inline asm
	{ atom.add.f64 %fd8941,[%rd3540],%fd2610; }

	// end inline asm
	add.s64 	%rd3541, %rd3390, 1208;
	// begin inline asm
	{ atom.add.f64 %fd8943,[%rd3541],%fd2609; }

	// end inline asm
	add.s64 	%rd3542, %rd3390, 1216;
	// begin inline asm
	{ atom.add.f64 %fd8945,[%rd3542],%fd2608; }

	// end inline asm
	add.s64 	%rd3543, %rd3390, 1224;
	// begin inline asm
	{ atom.add.f64 %fd8947,[%rd3543],%fd2583; }

	// end inline asm
	add.s64 	%rd3544, %rd3390, 1232;
	// begin inline asm
	{ atom.add.f64 %fd8949,[%rd3544],%fd2582; }

	// end inline asm
	add.s64 	%rd3545, %rd3390, 1240;
	// begin inline asm
	{ atom.add.f64 %fd8951,[%rd3545],%fd2581; }

	// end inline asm
	add.s64 	%rd3546, %rd3390, 1248;
	// begin inline asm
	{ atom.add.f64 %fd8953,[%rd3546],%fd1800; }

	// end inline asm
	add.s64 	%rd3547, %rd3390, 1256;
	// begin inline asm
	{ atom.add.f64 %fd8955,[%rd3547],%fd1799; }

	// end inline asm
	add.s64 	%rd3548, %rd3390, 1264;
	// begin inline asm
	{ atom.add.f64 %fd8957,[%rd3548],%fd1798; }

	// end inline asm
	add.s64 	%rd3549, %rd3390, 1272;
	// begin inline asm
	{ atom.add.f64 %fd8959,[%rd3549],%fd1773; }

	// end inline asm
	add.s64 	%rd3550, %rd3390, 1280;
	// begin inline asm
	{ atom.add.f64 %fd8961,[%rd3550],%fd1772; }

	// end inline asm
	add.s64 	%rd3551, %rd3390, 1288;
	// begin inline asm
	{ atom.add.f64 %fd8963,[%rd3551],%fd1771; }

	// end inline asm
	add.s64 	%rd3552, %rd3390, 1296;
	// begin inline asm
	{ atom.add.f64 %fd8965,[%rd3552],%fd1746; }

	// end inline asm
	add.s64 	%rd3553, %rd3390, 1304;
	// begin inline asm
	{ atom.add.f64 %fd8967,[%rd3553],%fd1745; }

	// end inline asm
	add.s64 	%rd3554, %rd3390, 1312;
	// begin inline asm
	{ atom.add.f64 %fd8969,[%rd3554],%fd1744; }

	// end inline asm
	add.s64 	%rd3555, %rd3390, 1320;
	// begin inline asm
	{ atom.add.f64 %fd8971,[%rd3555],%fd1719; }

	// end inline asm
	add.s64 	%rd3556, %rd3390, 1328;
	// begin inline asm
	{ atom.add.f64 %fd8973,[%rd3556],%fd1718; }

	// end inline asm
	add.s64 	%rd3557, %rd3390, 1336;
	// begin inline asm
	{ atom.add.f64 %fd8975,[%rd3557],%fd1717; }

	// end inline asm
	add.s64 	%rd3558, %rd3390, 1344;
	// begin inline asm
	{ atom.add.f64 %fd8977,[%rd3558],%fd2661; }

	// end inline asm
	add.s64 	%rd3559, %rd3390, 1352;
	// begin inline asm
	{ atom.add.f64 %fd8979,[%rd3559],%fd2660; }

	// end inline asm
	add.s64 	%rd3560, %rd3390, 1360;
	// begin inline asm
	{ atom.add.f64 %fd8981,[%rd3560],%fd2659; }

	// end inline asm
	add.s64 	%rd3561, %rd3390, 1368;
	// begin inline asm
	{ atom.add.f64 %fd8983,[%rd3561],%fd2634; }

	// end inline asm
	add.s64 	%rd3562, %rd3390, 1376;
	// begin inline asm
	{ atom.add.f64 %fd8985,[%rd3562],%fd2633; }

	// end inline asm
	add.s64 	%rd3563, %rd3390, 1384;
	// begin inline asm
	{ atom.add.f64 %fd8987,[%rd3563],%fd2632; }

	// end inline asm
	add.s64 	%rd3564, %rd3390, 1392;
	// begin inline asm
	{ atom.add.f64 %fd8989,[%rd3564],%fd2607; }

	// end inline asm
	add.s64 	%rd3565, %rd3390, 1400;
	// begin inline asm
	{ atom.add.f64 %fd8991,[%rd3565],%fd2606; }

	// end inline asm
	add.s64 	%rd3566, %rd3390, 1408;
	// begin inline asm
	{ atom.add.f64 %fd8993,[%rd3566],%fd2605; }

	// end inline asm
	add.s64 	%rd3567, %rd3390, 1416;
	// begin inline asm
	{ atom.add.f64 %fd8995,[%rd3567],%fd2580; }

	// end inline asm
	add.s64 	%rd3568, %rd3390, 1424;
	// begin inline asm
	{ atom.add.f64 %fd8997,[%rd3568],%fd2579; }

	// end inline asm
	add.s64 	%rd3569, %rd3390, 1432;
	// begin inline asm
	{ atom.add.f64 %fd8999,[%rd3569],%fd2578; }

	// end inline asm
	add.s64 	%rd3570, %rd3390, 1440;
	// begin inline asm
	{ atom.add.f64 %fd9001,[%rd3570],%fd1797; }

	// end inline asm
	add.s64 	%rd3571, %rd3390, 1448;
	// begin inline asm
	{ atom.add.f64 %fd9003,[%rd3571],%fd1796; }

	// end inline asm
	add.s64 	%rd3572, %rd3390, 1456;
	// begin inline asm
	{ atom.add.f64 %fd9005,[%rd3572],%fd1795; }

	// end inline asm
	add.s64 	%rd3573, %rd3390, 1464;
	// begin inline asm
	{ atom.add.f64 %fd9007,[%rd3573],%fd1770; }

	// end inline asm
	add.s64 	%rd3574, %rd3390, 1472;
	// begin inline asm
	{ atom.add.f64 %fd9009,[%rd3574],%fd1769; }

	// end inline asm
	add.s64 	%rd3575, %rd3390, 1480;
	// begin inline asm
	{ atom.add.f64 %fd9011,[%rd3575],%fd1768; }

	// end inline asm
	add.s64 	%rd3576, %rd3390, 1488;
	// begin inline asm
	{ atom.add.f64 %fd9013,[%rd3576],%fd1743; }

	// end inline asm
	add.s64 	%rd3577, %rd3390, 1496;
	// begin inline asm
	{ atom.add.f64 %fd9015,[%rd3577],%fd1742; }

	// end inline asm
	add.s64 	%rd3578, %rd3390, 1504;
	// begin inline asm
	{ atom.add.f64 %fd9017,[%rd3578],%fd1741; }

	// end inline asm
	add.s64 	%rd3579, %rd3390, 1512;
	// begin inline asm
	{ atom.add.f64 %fd9019,[%rd3579],%fd1716; }

	// end inline asm
	add.s64 	%rd3580, %rd3390, 1520;
	// begin inline asm
	{ atom.add.f64 %fd9021,[%rd3580],%fd1715; }

	// end inline asm
	add.s64 	%rd3581, %rd3390, 1528;
	// begin inline asm
	{ atom.add.f64 %fd9023,[%rd3581],%fd1714; }

	// end inline asm
	add.s64 	%rd3582, %rd3390, 1536;
	// begin inline asm
	{ atom.add.f64 %fd9025,[%rd3582],%fd2658; }

	// end inline asm
	add.s64 	%rd3583, %rd3390, 1544;
	// begin inline asm
	{ atom.add.f64 %fd9027,[%rd3583],%fd2657; }

	// end inline asm
	add.s64 	%rd3584, %rd3390, 1552;
	// begin inline asm
	{ atom.add.f64 %fd9029,[%rd3584],%fd2656; }

	// end inline asm
	add.s64 	%rd3585, %rd3390, 1560;
	// begin inline asm
	{ atom.add.f64 %fd9031,[%rd3585],%fd2631; }

	// end inline asm
	add.s64 	%rd3586, %rd3390, 1568;
	// begin inline asm
	{ atom.add.f64 %fd9033,[%rd3586],%fd2630; }

	// end inline asm
	add.s64 	%rd3587, %rd3390, 1576;
	// begin inline asm
	{ atom.add.f64 %fd9035,[%rd3587],%fd2629; }

	// end inline asm
	add.s64 	%rd3588, %rd3390, 1584;
	// begin inline asm
	{ atom.add.f64 %fd9037,[%rd3588],%fd2604; }

	// end inline asm
	add.s64 	%rd3589, %rd3390, 1592;
	// begin inline asm
	{ atom.add.f64 %fd9039,[%rd3589],%fd2603; }

	// end inline asm
	add.s64 	%rd3590, %rd3390, 1600;
	// begin inline asm
	{ atom.add.f64 %fd9041,[%rd3590],%fd2602; }

	// end inline asm
	add.s64 	%rd3591, %rd3390, 1608;
	// begin inline asm
	{ atom.add.f64 %fd9043,[%rd3591],%fd2577; }

	// end inline asm
	add.s64 	%rd3592, %rd3390, 1616;
	// begin inline asm
	{ atom.add.f64 %fd9045,[%rd3592],%fd2576; }

	// end inline asm
	add.s64 	%rd3593, %rd3390, 1624;
	// begin inline asm
	{ atom.add.f64 %fd9047,[%rd3593],%fd2575; }

	// end inline asm
	add.s64 	%rd3594, %rd3390, 1632;
	// begin inline asm
	{ atom.add.f64 %fd9049,[%rd3594],%fd1794; }

	// end inline asm
	add.s64 	%rd3595, %rd3390, 1640;
	// begin inline asm
	{ atom.add.f64 %fd9051,[%rd3595],%fd1793; }

	// end inline asm
	add.s64 	%rd3596, %rd3390, 1648;
	// begin inline asm
	{ atom.add.f64 %fd9053,[%rd3596],%fd1792; }

	// end inline asm
	add.s64 	%rd3597, %rd3390, 1656;
	// begin inline asm
	{ atom.add.f64 %fd9055,[%rd3597],%fd1767; }

	// end inline asm
	add.s64 	%rd3598, %rd3390, 1664;
	// begin inline asm
	{ atom.add.f64 %fd9057,[%rd3598],%fd1766; }

	// end inline asm
	add.s64 	%rd3599, %rd3390, 1672;
	// begin inline asm
	{ atom.add.f64 %fd9059,[%rd3599],%fd1765; }

	// end inline asm
	add.s64 	%rd3600, %rd3390, 1680;
	// begin inline asm
	{ atom.add.f64 %fd9061,[%rd3600],%fd1740; }

	// end inline asm
	add.s64 	%rd3601, %rd3390, 1688;
	// begin inline asm
	{ atom.add.f64 %fd9063,[%rd3601],%fd1739; }

	// end inline asm
	add.s64 	%rd3602, %rd3390, 1696;
	// begin inline asm
	{ atom.add.f64 %fd9065,[%rd3602],%fd1738; }

	// end inline asm
	add.s64 	%rd3603, %rd3390, 1704;
	// begin inline asm
	{ atom.add.f64 %fd9067,[%rd3603],%fd1713; }

	// end inline asm
	add.s64 	%rd3604, %rd3390, 1712;
	// begin inline asm
	{ atom.add.f64 %fd9069,[%rd3604],%fd1712; }

	// end inline asm
	add.s64 	%rd3605, %rd3390, 1720;
	// begin inline asm
	{ atom.add.f64 %fd9071,[%rd3605],%fd1711; }

	// end inline asm
	add.s64 	%rd3606, %rd3390, 1728;
	// begin inline asm
	{ atom.add.f64 %fd9073,[%rd3606],%fd2556; }

	// end inline asm
	add.s64 	%rd3607, %rd3390, 1736;
	// begin inline asm
	{ atom.add.f64 %fd9075,[%rd3607],%fd2555; }

	// end inline asm
	add.s64 	%rd3608, %rd3390, 1744;
	// begin inline asm
	{ atom.add.f64 %fd9077,[%rd3608],%fd2554; }

	// end inline asm
	add.s64 	%rd3609, %rd3390, 1752;
	// begin inline asm
	{ atom.add.f64 %fd9079,[%rd3609],%fd2529; }

	// end inline asm
	add.s64 	%rd3610, %rd3390, 1760;
	// begin inline asm
	{ atom.add.f64 %fd9081,[%rd3610],%fd2528; }

	// end inline asm
	add.s64 	%rd3611, %rd3390, 1768;
	// begin inline asm
	{ atom.add.f64 %fd9083,[%rd3611],%fd2527; }

	// end inline asm
	add.s64 	%rd3612, %rd3390, 1776;
	// begin inline asm
	{ atom.add.f64 %fd9085,[%rd3612],%fd2502; }

	// end inline asm
	add.s64 	%rd3613, %rd3390, 1784;
	// begin inline asm
	{ atom.add.f64 %fd9087,[%rd3613],%fd2501; }

	// end inline asm
	add.s64 	%rd3614, %rd3390, 1792;
	// begin inline asm
	{ atom.add.f64 %fd9089,[%rd3614],%fd2500; }

	// end inline asm
	add.s64 	%rd3615, %rd3390, 1800;
	// begin inline asm
	{ atom.add.f64 %fd9091,[%rd3615],%fd2475; }

	// end inline asm
	add.s64 	%rd3616, %rd3390, 1808;
	// begin inline asm
	{ atom.add.f64 %fd9093,[%rd3616],%fd2474; }

	// end inline asm
	add.s64 	%rd3617, %rd3390, 1816;
	// begin inline asm
	{ atom.add.f64 %fd9095,[%rd3617],%fd2473; }

	// end inline asm
	add.s64 	%rd3618, %rd3390, 1824;
	// begin inline asm
	{ atom.add.f64 %fd9097,[%rd3618],%fd1692; }

	// end inline asm
	add.s64 	%rd3619, %rd3390, 1832;
	// begin inline asm
	{ atom.add.f64 %fd9099,[%rd3619],%fd1691; }

	// end inline asm
	add.s64 	%rd3620, %rd3390, 1840;
	// begin inline asm
	{ atom.add.f64 %fd9101,[%rd3620],%fd1690; }

	// end inline asm
	add.s64 	%rd3621, %rd3390, 1848;
	// begin inline asm
	{ atom.add.f64 %fd9103,[%rd3621],%fd1665; }

	// end inline asm
	add.s64 	%rd3622, %rd3390, 1856;
	// begin inline asm
	{ atom.add.f64 %fd9105,[%rd3622],%fd1664; }

	// end inline asm
	add.s64 	%rd3623, %rd3390, 1864;
	// begin inline asm
	{ atom.add.f64 %fd9107,[%rd3623],%fd1663; }

	// end inline asm
	add.s64 	%rd3624, %rd3390, 1872;
	// begin inline asm
	{ atom.add.f64 %fd9109,[%rd3624],%fd1638; }

	// end inline asm
	add.s64 	%rd3625, %rd3390, 1880;
	// begin inline asm
	{ atom.add.f64 %fd9111,[%rd3625],%fd1637; }

	// end inline asm
	add.s64 	%rd3626, %rd3390, 1888;
	// begin inline asm
	{ atom.add.f64 %fd9113,[%rd3626],%fd1636; }

	// end inline asm
	add.s64 	%rd3627, %rd3390, 1896;
	// begin inline asm
	{ atom.add.f64 %fd9115,[%rd3627],%fd1611; }

	// end inline asm
	add.s64 	%rd3628, %rd3390, 1904;
	// begin inline asm
	{ atom.add.f64 %fd9117,[%rd3628],%fd1610; }

	// end inline asm
	add.s64 	%rd3629, %rd3390, 1912;
	// begin inline asm
	{ atom.add.f64 %fd9119,[%rd3629],%fd1609; }

	// end inline asm
	add.s64 	%rd3630, %rd3390, 1920;
	// begin inline asm
	{ atom.add.f64 %fd9121,[%rd3630],%fd2553; }

	// end inline asm
	add.s64 	%rd3631, %rd3390, 1928;
	// begin inline asm
	{ atom.add.f64 %fd9123,[%rd3631],%fd2552; }

	// end inline asm
	add.s64 	%rd3632, %rd3390, 1936;
	// begin inline asm
	{ atom.add.f64 %fd9125,[%rd3632],%fd2551; }

	// end inline asm
	add.s64 	%rd3633, %rd3390, 1944;
	// begin inline asm
	{ atom.add.f64 %fd9127,[%rd3633],%fd2526; }

	// end inline asm
	add.s64 	%rd3634, %rd3390, 1952;
	// begin inline asm
	{ atom.add.f64 %fd9129,[%rd3634],%fd2525; }

	// end inline asm
	add.s64 	%rd3635, %rd3390, 1960;
	// begin inline asm
	{ atom.add.f64 %fd9131,[%rd3635],%fd2524; }

	// end inline asm
	add.s64 	%rd3636, %rd3390, 1968;
	// begin inline asm
	{ atom.add.f64 %fd9133,[%rd3636],%fd2499; }

	// end inline asm
	add.s64 	%rd3637, %rd3390, 1976;
	// begin inline asm
	{ atom.add.f64 %fd9135,[%rd3637],%fd2498; }

	// end inline asm
	add.s64 	%rd3638, %rd3390, 1984;
	// begin inline asm
	{ atom.add.f64 %fd9137,[%rd3638],%fd2497; }

	// end inline asm
	add.s64 	%rd3639, %rd3390, 1992;
	// begin inline asm
	{ atom.add.f64 %fd9139,[%rd3639],%fd2472; }

	// end inline asm
	add.s64 	%rd3640, %rd3390, 2000;
	// begin inline asm
	{ atom.add.f64 %fd9141,[%rd3640],%fd2471; }

	// end inline asm
	add.s64 	%rd3641, %rd3390, 2008;
	// begin inline asm
	{ atom.add.f64 %fd9143,[%rd3641],%fd2470; }

	// end inline asm
	add.s64 	%rd3642, %rd3390, 2016;
	// begin inline asm
	{ atom.add.f64 %fd9145,[%rd3642],%fd1689; }

	// end inline asm
	add.s64 	%rd3643, %rd3390, 2024;
	// begin inline asm
	{ atom.add.f64 %fd9147,[%rd3643],%fd1688; }

	// end inline asm
	add.s64 	%rd3644, %rd3390, 2032;
	// begin inline asm
	{ atom.add.f64 %fd9149,[%rd3644],%fd1687; }

	// end inline asm
	add.s64 	%rd3645, %rd3390, 2040;
	// begin inline asm
	{ atom.add.f64 %fd9151,[%rd3645],%fd1662; }

	// end inline asm
	add.s64 	%rd3646, %rd3390, 2048;
	// begin inline asm
	{ atom.add.f64 %fd9153,[%rd3646],%fd1661; }

	// end inline asm
	add.s64 	%rd3647, %rd3390, 2056;
	// begin inline asm
	{ atom.add.f64 %fd9155,[%rd3647],%fd1660; }

	// end inline asm
	add.s64 	%rd3648, %rd3390, 2064;
	// begin inline asm
	{ atom.add.f64 %fd9157,[%rd3648],%fd1635; }

	// end inline asm
	add.s64 	%rd3649, %rd3390, 2072;
	// begin inline asm
	{ atom.add.f64 %fd9159,[%rd3649],%fd1634; }

	// end inline asm
	add.s64 	%rd3650, %rd3390, 2080;
	// begin inline asm
	{ atom.add.f64 %fd9161,[%rd3650],%fd1633; }

	// end inline asm
	add.s64 	%rd3651, %rd3390, 2088;
	// begin inline asm
	{ atom.add.f64 %fd9163,[%rd3651],%fd1608; }

	// end inline asm
	add.s64 	%rd3652, %rd3390, 2096;
	// begin inline asm
	{ atom.add.f64 %fd9165,[%rd3652],%fd1607; }

	// end inline asm
	add.s64 	%rd3653, %rd3390, 2104;
	// begin inline asm
	{ atom.add.f64 %fd9167,[%rd3653],%fd1606; }

	// end inline asm
	add.s64 	%rd3654, %rd3390, 2112;
	// begin inline asm
	{ atom.add.f64 %fd9169,[%rd3654],%fd2550; }

	// end inline asm
	add.s64 	%rd3655, %rd3390, 2120;
	// begin inline asm
	{ atom.add.f64 %fd9171,[%rd3655],%fd2549; }

	// end inline asm
	add.s64 	%rd3656, %rd3390, 2128;
	// begin inline asm
	{ atom.add.f64 %fd9173,[%rd3656],%fd2548; }

	// end inline asm
	add.s64 	%rd3657, %rd3390, 2136;
	// begin inline asm
	{ atom.add.f64 %fd9175,[%rd3657],%fd2523; }

	// end inline asm
	add.s64 	%rd3658, %rd3390, 2144;
	// begin inline asm
	{ atom.add.f64 %fd9177,[%rd3658],%fd2522; }

	// end inline asm
	add.s64 	%rd3659, %rd3390, 2152;
	// begin inline asm
	{ atom.add.f64 %fd9179,[%rd3659],%fd2521; }

	// end inline asm
	add.s64 	%rd3660, %rd3390, 2160;
	// begin inline asm
	{ atom.add.f64 %fd9181,[%rd3660],%fd2496; }

	// end inline asm
	add.s64 	%rd3661, %rd3390, 2168;
	// begin inline asm
	{ atom.add.f64 %fd9183,[%rd3661],%fd2495; }

	// end inline asm
	add.s64 	%rd3662, %rd3390, 2176;
	// begin inline asm
	{ atom.add.f64 %fd9185,[%rd3662],%fd2494; }

	// end inline asm
	add.s64 	%rd3663, %rd3390, 2184;
	// begin inline asm
	{ atom.add.f64 %fd9187,[%rd3663],%fd2469; }

	// end inline asm
	add.s64 	%rd3664, %rd3390, 2192;
	// begin inline asm
	{ atom.add.f64 %fd9189,[%rd3664],%fd2468; }

	// end inline asm
	add.s64 	%rd3665, %rd3390, 2200;
	// begin inline asm
	{ atom.add.f64 %fd9191,[%rd3665],%fd2467; }

	// end inline asm
	add.s64 	%rd3666, %rd3390, 2208;
	// begin inline asm
	{ atom.add.f64 %fd9193,[%rd3666],%fd1686; }

	// end inline asm
	add.s64 	%rd3667, %rd3390, 2216;
	// begin inline asm
	{ atom.add.f64 %fd9195,[%rd3667],%fd1685; }

	// end inline asm
	add.s64 	%rd3668, %rd3390, 2224;
	// begin inline asm
	{ atom.add.f64 %fd9197,[%rd3668],%fd1684; }

	// end inline asm
	add.s64 	%rd3669, %rd3390, 2232;
	// begin inline asm
	{ atom.add.f64 %fd9199,[%rd3669],%fd1659; }

	// end inline asm
	add.s64 	%rd3670, %rd3390, 2240;
	// begin inline asm
	{ atom.add.f64 %fd9201,[%rd3670],%fd1658; }

	// end inline asm
	add.s64 	%rd3671, %rd3390, 2248;
	// begin inline asm
	{ atom.add.f64 %fd9203,[%rd3671],%fd1657; }

	// end inline asm
	add.s64 	%rd3672, %rd3390, 2256;
	// begin inline asm
	{ atom.add.f64 %fd9205,[%rd3672],%fd1632; }

	// end inline asm
	add.s64 	%rd3673, %rd3390, 2264;
	// begin inline asm
	{ atom.add.f64 %fd9207,[%rd3673],%fd1631; }

	// end inline asm
	add.s64 	%rd3674, %rd3390, 2272;
	// begin inline asm
	{ atom.add.f64 %fd9209,[%rd3674],%fd1630; }

	// end inline asm
	add.s64 	%rd3675, %rd3390, 2280;
	// begin inline asm
	{ atom.add.f64 %fd9211,[%rd3675],%fd1605; }

	// end inline asm
	add.s64 	%rd3676, %rd3390, 2288;
	// begin inline asm
	{ atom.add.f64 %fd9213,[%rd3676],%fd1604; }

	// end inline asm
	add.s64 	%rd3677, %rd3390, 2296;
	// begin inline asm
	{ atom.add.f64 %fd9215,[%rd3677],%fd1603; }

	// end inline asm
	add.s64 	%rd3678, %rd3390, 2304;
	// begin inline asm
	{ atom.add.f64 %fd9217,[%rd3678],%fd1584; }

	// end inline asm
	add.s64 	%rd3679, %rd3390, 2312;
	// begin inline asm
	{ atom.add.f64 %fd9219,[%rd3679],%fd1583; }

	// end inline asm
	add.s64 	%rd3680, %rd3390, 2320;
	// begin inline asm
	{ atom.add.f64 %fd9221,[%rd3680],%fd1582; }

	// end inline asm
	add.s64 	%rd3681, %rd3390, 2328;
	// begin inline asm
	{ atom.add.f64 %fd9223,[%rd3681],%fd1557; }

	// end inline asm
	add.s64 	%rd3682, %rd3390, 2336;
	// begin inline asm
	{ atom.add.f64 %fd9225,[%rd3682],%fd1556; }

	// end inline asm
	add.s64 	%rd3683, %rd3390, 2344;
	// begin inline asm
	{ atom.add.f64 %fd9227,[%rd3683],%fd1555; }

	// end inline asm
	add.s64 	%rd3684, %rd3390, 2352;
	// begin inline asm
	{ atom.add.f64 %fd9229,[%rd3684],%fd1530; }

	// end inline asm
	add.s64 	%rd3685, %rd3390, 2360;
	// begin inline asm
	{ atom.add.f64 %fd9231,[%rd3685],%fd1529; }

	// end inline asm
	add.s64 	%rd3686, %rd3390, 2368;
	// begin inline asm
	{ atom.add.f64 %fd9233,[%rd3686],%fd1528; }

	// end inline asm
	add.s64 	%rd3687, %rd3390, 2376;
	// begin inline asm
	{ atom.add.f64 %fd9235,[%rd3687],%fd1503; }

	// end inline asm
	add.s64 	%rd3688, %rd3390, 2384;
	// begin inline asm
	{ atom.add.f64 %fd9237,[%rd3688],%fd1502; }

	// end inline asm
	add.s64 	%rd3689, %rd3390, 2392;
	// begin inline asm
	{ atom.add.f64 %fd9239,[%rd3689],%fd1501; }

	// end inline asm
	add.s64 	%rd3690, %rd3390, 2400;
	// begin inline asm
	{ atom.add.f64 %fd9241,[%rd3690],%fd2448; }

	// end inline asm
	add.s64 	%rd3691, %rd3390, 2408;
	// begin inline asm
	{ atom.add.f64 %fd9243,[%rd3691],%fd2447; }

	// end inline asm
	add.s64 	%rd3692, %rd3390, 2416;
	// begin inline asm
	{ atom.add.f64 %fd9245,[%rd3692],%fd2446; }

	// end inline asm
	add.s64 	%rd3693, %rd3390, 2424;
	// begin inline asm
	{ atom.add.f64 %fd9247,[%rd3693],%fd2421; }

	// end inline asm
	add.s64 	%rd3694, %rd3390, 2432;
	// begin inline asm
	{ atom.add.f64 %fd9249,[%rd3694],%fd2420; }

	// end inline asm
	add.s64 	%rd3695, %rd3390, 2440;
	// begin inline asm
	{ atom.add.f64 %fd9251,[%rd3695],%fd2419; }

	// end inline asm
	add.s64 	%rd3696, %rd3390, 2448;
	// begin inline asm
	{ atom.add.f64 %fd9253,[%rd3696],%fd2394; }

	// end inline asm
	add.s64 	%rd3697, %rd3390, 2456;
	// begin inline asm
	{ atom.add.f64 %fd9255,[%rd3697],%fd2393; }

	// end inline asm
	add.s64 	%rd3698, %rd3390, 2464;
	// begin inline asm
	{ atom.add.f64 %fd9257,[%rd3698],%fd2392; }

	// end inline asm
	add.s64 	%rd3699, %rd3390, 2472;
	// begin inline asm
	{ atom.add.f64 %fd9259,[%rd3699],%fd2367; }

	// end inline asm
	add.s64 	%rd3700, %rd3390, 2480;
	// begin inline asm
	{ atom.add.f64 %fd9261,[%rd3700],%fd2366; }

	// end inline asm
	add.s64 	%rd3701, %rd3390, 2488;
	// begin inline asm
	{ atom.add.f64 %fd9263,[%rd3701],%fd2365; }

	// end inline asm
	add.s64 	%rd3702, %rd3390, 2496;
	// begin inline asm
	{ atom.add.f64 %fd9265,[%rd3702],%fd1581; }

	// end inline asm
	add.s64 	%rd3703, %rd3390, 2504;
	// begin inline asm
	{ atom.add.f64 %fd9267,[%rd3703],%fd1580; }

	// end inline asm
	add.s64 	%rd3704, %rd3390, 2512;
	// begin inline asm
	{ atom.add.f64 %fd9269,[%rd3704],%fd1579; }

	// end inline asm
	add.s64 	%rd3705, %rd3390, 2520;
	// begin inline asm
	{ atom.add.f64 %fd9271,[%rd3705],%fd1554; }

	// end inline asm
	add.s64 	%rd3706, %rd3390, 2528;
	// begin inline asm
	{ atom.add.f64 %fd9273,[%rd3706],%fd1553; }

	// end inline asm
	add.s64 	%rd3707, %rd3390, 2536;
	// begin inline asm
	{ atom.add.f64 %fd9275,[%rd3707],%fd1552; }

	// end inline asm
	add.s64 	%rd3708, %rd3390, 2544;
	// begin inline asm
	{ atom.add.f64 %fd9277,[%rd3708],%fd1527; }

	// end inline asm
	add.s64 	%rd3709, %rd3390, 2552;
	// begin inline asm
	{ atom.add.f64 %fd9279,[%rd3709],%fd1526; }

	// end inline asm
	add.s64 	%rd3710, %rd3390, 2560;
	// begin inline asm
	{ atom.add.f64 %fd9281,[%rd3710],%fd1525; }

	// end inline asm
	add.s64 	%rd3711, %rd3390, 2568;
	// begin inline asm
	{ atom.add.f64 %fd9283,[%rd3711],%fd1500; }

	// end inline asm
	add.s64 	%rd3712, %rd3390, 2576;
	// begin inline asm
	{ atom.add.f64 %fd9285,[%rd3712],%fd1499; }

	// end inline asm
	add.s64 	%rd3713, %rd3390, 2584;
	// begin inline asm
	{ atom.add.f64 %fd9287,[%rd3713],%fd1498; }

	// end inline asm
	add.s64 	%rd3714, %rd3390, 2592;
	// begin inline asm
	{ atom.add.f64 %fd9289,[%rd3714],%fd2445; }

	// end inline asm
	add.s64 	%rd3715, %rd3390, 2600;
	// begin inline asm
	{ atom.add.f64 %fd9291,[%rd3715],%fd2444; }

	// end inline asm
	add.s64 	%rd3716, %rd3390, 2608;
	// begin inline asm
	{ atom.add.f64 %fd9293,[%rd3716],%fd2443; }

	// end inline asm
	add.s64 	%rd3717, %rd3390, 2616;
	// begin inline asm
	{ atom.add.f64 %fd9295,[%rd3717],%fd2418; }

	// end inline asm
	add.s64 	%rd3718, %rd3390, 2624;
	// begin inline asm
	{ atom.add.f64 %fd9297,[%rd3718],%fd2417; }

	// end inline asm
	add.s64 	%rd3719, %rd3390, 2632;
	// begin inline asm
	{ atom.add.f64 %fd9299,[%rd3719],%fd2416; }

	// end inline asm
	add.s64 	%rd3720, %rd3390, 2640;
	// begin inline asm
	{ atom.add.f64 %fd9301,[%rd3720],%fd2391; }

	// end inline asm
	add.s64 	%rd3721, %rd3390, 2648;
	// begin inline asm
	{ atom.add.f64 %fd9303,[%rd3721],%fd2390; }

	// end inline asm
	add.s64 	%rd3722, %rd3390, 2656;
	// begin inline asm
	{ atom.add.f64 %fd9305,[%rd3722],%fd2389; }

	// end inline asm
	add.s64 	%rd3723, %rd3390, 2664;
	// begin inline asm
	{ atom.add.f64 %fd9307,[%rd3723],%fd2364; }

	// end inline asm
	add.s64 	%rd3724, %rd3390, 2672;
	// begin inline asm
	{ atom.add.f64 %fd9309,[%rd3724],%fd2363; }

	// end inline asm
	add.s64 	%rd3725, %rd3390, 2680;
	// begin inline asm
	{ atom.add.f64 %fd9311,[%rd3725],%fd2362; }

	// end inline asm
	add.s64 	%rd3726, %rd3390, 2688;
	// begin inline asm
	{ atom.add.f64 %fd9313,[%rd3726],%fd1578; }

	// end inline asm
	add.s64 	%rd3727, %rd3390, 2696;
	// begin inline asm
	{ atom.add.f64 %fd9315,[%rd3727],%fd1577; }

	// end inline asm
	add.s64 	%rd3728, %rd3390, 2704;
	// begin inline asm
	{ atom.add.f64 %fd9317,[%rd3728],%fd1576; }

	// end inline asm
	add.s64 	%rd3729, %rd3390, 2712;
	// begin inline asm
	{ atom.add.f64 %fd9319,[%rd3729],%fd1551; }

	// end inline asm
	add.s64 	%rd3730, %rd3390, 2720;
	// begin inline asm
	{ atom.add.f64 %fd9321,[%rd3730],%fd1550; }

	// end inline asm
	add.s64 	%rd3731, %rd3390, 2728;
	// begin inline asm
	{ atom.add.f64 %fd9323,[%rd3731],%fd1549; }

	// end inline asm
	add.s64 	%rd3732, %rd3390, 2736;
	// begin inline asm
	{ atom.add.f64 %fd9325,[%rd3732],%fd1524; }

	// end inline asm
	add.s64 	%rd3733, %rd3390, 2744;
	// begin inline asm
	{ atom.add.f64 %fd9327,[%rd3733],%fd1523; }

	// end inline asm
	add.s64 	%rd3734, %rd3390, 2752;
	// begin inline asm
	{ atom.add.f64 %fd9329,[%rd3734],%fd1522; }

	// end inline asm
	add.s64 	%rd3735, %rd3390, 2760;
	// begin inline asm
	{ atom.add.f64 %fd9331,[%rd3735],%fd1497; }

	// end inline asm
	add.s64 	%rd3736, %rd3390, 2768;
	// begin inline asm
	{ atom.add.f64 %fd9333,[%rd3736],%fd1496; }

	// end inline asm
	add.s64 	%rd3737, %rd3390, 2776;
	// begin inline asm
	{ atom.add.f64 %fd9335,[%rd3737],%fd1495; }

	// end inline asm
	add.s64 	%rd3738, %rd3390, 2784;
	// begin inline asm
	{ atom.add.f64 %fd9337,[%rd3738],%fd2442; }

	// end inline asm
	add.s64 	%rd3739, %rd3390, 2792;
	// begin inline asm
	{ atom.add.f64 %fd9339,[%rd3739],%fd2441; }

	// end inline asm
	add.s64 	%rd3740, %rd3390, 2800;
	// begin inline asm
	{ atom.add.f64 %fd9341,[%rd3740],%fd2440; }

	// end inline asm
	add.s64 	%rd3741, %rd3390, 2808;
	// begin inline asm
	{ atom.add.f64 %fd9343,[%rd3741],%fd2415; }

	// end inline asm
	add.s64 	%rd3742, %rd3390, 2816;
	// begin inline asm
	{ atom.add.f64 %fd9345,[%rd3742],%fd2414; }

	// end inline asm
	add.s64 	%rd3743, %rd3390, 2824;
	// begin inline asm
	{ atom.add.f64 %fd9347,[%rd3743],%fd2413; }

	// end inline asm
	add.s64 	%rd3744, %rd3390, 2832;
	// begin inline asm
	{ atom.add.f64 %fd9349,[%rd3744],%fd2388; }

	// end inline asm
	add.s64 	%rd3745, %rd3390, 2840;
	// begin inline asm
	{ atom.add.f64 %fd9351,[%rd3745],%fd2387; }

	// end inline asm
	add.s64 	%rd3746, %rd3390, 2848;
	// begin inline asm
	{ atom.add.f64 %fd9353,[%rd3746],%fd2386; }

	// end inline asm
	add.s64 	%rd3747, %rd3390, 2856;
	// begin inline asm
	{ atom.add.f64 %fd9355,[%rd3747],%fd2361; }

	// end inline asm
	add.s64 	%rd3748, %rd3390, 2864;
	// begin inline asm
	{ atom.add.f64 %fd9357,[%rd3748],%fd2360; }

	// end inline asm
	add.s64 	%rd3749, %rd3390, 2872;
	// begin inline asm
	{ atom.add.f64 %fd9359,[%rd3749],%fd2359; }

	// end inline asm
	add.s64 	%rd3750, %rd3390, 2880;
	// begin inline asm
	{ atom.add.f64 %fd9361,[%rd3750],%fd1476; }

	// end inline asm
	add.s64 	%rd3751, %rd3390, 2888;
	// begin inline asm
	{ atom.add.f64 %fd9363,[%rd3751],%fd1475; }

	// end inline asm
	add.s64 	%rd3752, %rd3390, 2896;
	// begin inline asm
	{ atom.add.f64 %fd9365,[%rd3752],%fd1474; }

	// end inline asm
	add.s64 	%rd3753, %rd3390, 2904;
	// begin inline asm
	{ atom.add.f64 %fd9367,[%rd3753],%fd1449; }

	// end inline asm
	add.s64 	%rd3754, %rd3390, 2912;
	// begin inline asm
	{ atom.add.f64 %fd9369,[%rd3754],%fd1448; }

	// end inline asm
	add.s64 	%rd3755, %rd3390, 2920;
	// begin inline asm
	{ atom.add.f64 %fd9371,[%rd3755],%fd1447; }

	// end inline asm
	add.s64 	%rd3756, %rd3390, 2928;
	// begin inline asm
	{ atom.add.f64 %fd9373,[%rd3756],%fd1422; }

	// end inline asm
	add.s64 	%rd3757, %rd3390, 2936;
	// begin inline asm
	{ atom.add.f64 %fd9375,[%rd3757],%fd1421; }

	// end inline asm
	add.s64 	%rd3758, %rd3390, 2944;
	// begin inline asm
	{ atom.add.f64 %fd9377,[%rd3758],%fd1420; }

	// end inline asm
	add.s64 	%rd3759, %rd3390, 2952;
	// begin inline asm
	{ atom.add.f64 %fd9379,[%rd3759],%fd1395; }

	// end inline asm
	add.s64 	%rd3760, %rd3390, 2960;
	// begin inline asm
	{ atom.add.f64 %fd9381,[%rd3760],%fd1394; }

	// end inline asm
	add.s64 	%rd3761, %rd3390, 2968;
	// begin inline asm
	{ atom.add.f64 %fd9383,[%rd3761],%fd1393; }

	// end inline asm
	add.s64 	%rd3762, %rd3390, 2976;
	// begin inline asm
	{ atom.add.f64 %fd9385,[%rd3762],%fd2340; }

	// end inline asm
	add.s64 	%rd3763, %rd3390, 2984;
	// begin inline asm
	{ atom.add.f64 %fd9387,[%rd3763],%fd2339; }

	// end inline asm
	add.s64 	%rd3764, %rd3390, 2992;
	// begin inline asm
	{ atom.add.f64 %fd9389,[%rd3764],%fd2338; }

	// end inline asm
	add.s64 	%rd3765, %rd3390, 3000;
	// begin inline asm
	{ atom.add.f64 %fd9391,[%rd3765],%fd2313; }

	// end inline asm
	add.s64 	%rd3766, %rd3390, 3008;
	// begin inline asm
	{ atom.add.f64 %fd9393,[%rd3766],%fd2312; }

	// end inline asm
	add.s64 	%rd3767, %rd3390, 3016;
	// begin inline asm
	{ atom.add.f64 %fd9395,[%rd3767],%fd2311; }

	// end inline asm
	add.s64 	%rd3768, %rd3390, 3024;
	// begin inline asm
	{ atom.add.f64 %fd9397,[%rd3768],%fd2286; }

	// end inline asm
	add.s64 	%rd3769, %rd3390, 3032;
	// begin inline asm
	{ atom.add.f64 %fd9399,[%rd3769],%fd2285; }

	// end inline asm
	add.s64 	%rd3770, %rd3390, 3040;
	// begin inline asm
	{ atom.add.f64 %fd9401,[%rd3770],%fd2284; }

	// end inline asm
	add.s64 	%rd3771, %rd3390, 3048;
	// begin inline asm
	{ atom.add.f64 %fd9403,[%rd3771],%fd2259; }

	// end inline asm
	add.s64 	%rd3772, %rd3390, 3056;
	// begin inline asm
	{ atom.add.f64 %fd9405,[%rd3772],%fd2258; }

	// end inline asm
	add.s64 	%rd3773, %rd3390, 3064;
	// begin inline asm
	{ atom.add.f64 %fd9407,[%rd3773],%fd2257; }

	// end inline asm
	add.s64 	%rd3774, %rd3390, 3072;
	// begin inline asm
	{ atom.add.f64 %fd9409,[%rd3774],%fd1473; }

	// end inline asm
	add.s64 	%rd3775, %rd3390, 3080;
	// begin inline asm
	{ atom.add.f64 %fd9411,[%rd3775],%fd1472; }

	// end inline asm
	add.s64 	%rd3776, %rd3390, 3088;
	// begin inline asm
	{ atom.add.f64 %fd9413,[%rd3776],%fd1471; }

	// end inline asm
	add.s64 	%rd3777, %rd3390, 3096;
	// begin inline asm
	{ atom.add.f64 %fd9415,[%rd3777],%fd1446; }

	// end inline asm
	add.s64 	%rd3778, %rd3390, 3104;
	// begin inline asm
	{ atom.add.f64 %fd9417,[%rd3778],%fd1445; }

	// end inline asm
	add.s64 	%rd3779, %rd3390, 3112;
	// begin inline asm
	{ atom.add.f64 %fd9419,[%rd3779],%fd1444; }

	// end inline asm
	add.s64 	%rd3780, %rd3390, 3120;
	// begin inline asm
	{ atom.add.f64 %fd9421,[%rd3780],%fd1419; }

	// end inline asm
	add.s64 	%rd3781, %rd3390, 3128;
	// begin inline asm
	{ atom.add.f64 %fd9423,[%rd3781],%fd1418; }

	// end inline asm
	add.s64 	%rd3782, %rd3390, 3136;
	// begin inline asm
	{ atom.add.f64 %fd9425,[%rd3782],%fd1417; }

	// end inline asm
	add.s64 	%rd3783, %rd3390, 3144;
	// begin inline asm
	{ atom.add.f64 %fd9427,[%rd3783],%fd1392; }

	// end inline asm
	add.s64 	%rd3784, %rd3390, 3152;
	// begin inline asm
	{ atom.add.f64 %fd9429,[%rd3784],%fd1391; }

	// end inline asm
	add.s64 	%rd3785, %rd3390, 3160;
	// begin inline asm
	{ atom.add.f64 %fd9431,[%rd3785],%fd1390; }

	// end inline asm
	add.s64 	%rd3786, %rd3390, 3168;
	// begin inline asm
	{ atom.add.f64 %fd9433,[%rd3786],%fd2337; }

	// end inline asm
	add.s64 	%rd3787, %rd3390, 3176;
	// begin inline asm
	{ atom.add.f64 %fd9435,[%rd3787],%fd2336; }

	// end inline asm
	add.s64 	%rd3788, %rd3390, 3184;
	// begin inline asm
	{ atom.add.f64 %fd9437,[%rd3788],%fd2335; }

	// end inline asm
	add.s64 	%rd3789, %rd3390, 3192;
	// begin inline asm
	{ atom.add.f64 %fd9439,[%rd3789],%fd2310; }

	// end inline asm
	add.s64 	%rd3790, %rd3390, 3200;
	// begin inline asm
	{ atom.add.f64 %fd9441,[%rd3790],%fd2309; }

	// end inline asm
	add.s64 	%rd3791, %rd3390, 3208;
	// begin inline asm
	{ atom.add.f64 %fd9443,[%rd3791],%fd2308; }

	// end inline asm
	add.s64 	%rd3792, %rd3390, 3216;
	// begin inline asm
	{ atom.add.f64 %fd9445,[%rd3792],%fd2283; }

	// end inline asm
	add.s64 	%rd3793, %rd3390, 3224;
	// begin inline asm
	{ atom.add.f64 %fd9447,[%rd3793],%fd2282; }

	// end inline asm
	add.s64 	%rd3794, %rd3390, 3232;
	// begin inline asm
	{ atom.add.f64 %fd9449,[%rd3794],%fd2281; }

	// end inline asm
	add.s64 	%rd3795, %rd3390, 3240;
	// begin inline asm
	{ atom.add.f64 %fd9451,[%rd3795],%fd2256; }

	// end inline asm
	add.s64 	%rd3796, %rd3390, 3248;
	// begin inline asm
	{ atom.add.f64 %fd9453,[%rd3796],%fd2255; }

	// end inline asm
	add.s64 	%rd3797, %rd3390, 3256;
	// begin inline asm
	{ atom.add.f64 %fd9455,[%rd3797],%fd2254; }

	// end inline asm
	add.s64 	%rd3798, %rd3390, 3264;
	// begin inline asm
	{ atom.add.f64 %fd9457,[%rd3798],%fd1470; }

	// end inline asm
	add.s64 	%rd3799, %rd3390, 3272;
	// begin inline asm
	{ atom.add.f64 %fd9459,[%rd3799],%fd1469; }

	// end inline asm
	add.s64 	%rd3800, %rd3390, 3280;
	// begin inline asm
	{ atom.add.f64 %fd9461,[%rd3800],%fd1468; }

	// end inline asm
	add.s64 	%rd3801, %rd3390, 3288;
	// begin inline asm
	{ atom.add.f64 %fd9463,[%rd3801],%fd1443; }

	// end inline asm
	add.s64 	%rd3802, %rd3390, 3296;
	// begin inline asm
	{ atom.add.f64 %fd9465,[%rd3802],%fd1442; }

	// end inline asm
	add.s64 	%rd3803, %rd3390, 3304;
	// begin inline asm
	{ atom.add.f64 %fd9467,[%rd3803],%fd1441; }

	// end inline asm
	add.s64 	%rd3804, %rd3390, 3312;
	// begin inline asm
	{ atom.add.f64 %fd9469,[%rd3804],%fd1416; }

	// end inline asm
	add.s64 	%rd3805, %rd3390, 3320;
	// begin inline asm
	{ atom.add.f64 %fd9471,[%rd3805],%fd1415; }

	// end inline asm
	add.s64 	%rd3806, %rd3390, 3328;
	// begin inline asm
	{ atom.add.f64 %fd9473,[%rd3806],%fd1414; }

	// end inline asm
	add.s64 	%rd3807, %rd3390, 3336;
	// begin inline asm
	{ atom.add.f64 %fd9475,[%rd3807],%fd1389; }

	// end inline asm
	add.s64 	%rd3808, %rd3390, 3344;
	// begin inline asm
	{ atom.add.f64 %fd9477,[%rd3808],%fd1388; }

	// end inline asm
	add.s64 	%rd3809, %rd3390, 3352;
	// begin inline asm
	{ atom.add.f64 %fd9479,[%rd3809],%fd1387; }

	// end inline asm
	add.s64 	%rd3810, %rd3390, 3360;
	// begin inline asm
	{ atom.add.f64 %fd9481,[%rd3810],%fd2334; }

	// end inline asm
	add.s64 	%rd3811, %rd3390, 3368;
	// begin inline asm
	{ atom.add.f64 %fd9483,[%rd3811],%fd2333; }

	// end inline asm
	add.s64 	%rd3812, %rd3390, 3376;
	// begin inline asm
	{ atom.add.f64 %fd9485,[%rd3812],%fd2332; }

	// end inline asm
	add.s64 	%rd3813, %rd3390, 3384;
	// begin inline asm
	{ atom.add.f64 %fd9487,[%rd3813],%fd2307; }

	// end inline asm
	add.s64 	%rd3814, %rd3390, 3392;
	// begin inline asm
	{ atom.add.f64 %fd9489,[%rd3814],%fd2306; }

	// end inline asm
	add.s64 	%rd3815, %rd3390, 3400;
	// begin inline asm
	{ atom.add.f64 %fd9491,[%rd3815],%fd2305; }

	// end inline asm
	add.s64 	%rd3816, %rd3390, 3408;
	// begin inline asm
	{ atom.add.f64 %fd9493,[%rd3816],%fd2280; }

	// end inline asm
	add.s64 	%rd3817, %rd3390, 3416;
	// begin inline asm
	{ atom.add.f64 %fd9495,[%rd3817],%fd2279; }

	// end inline asm
	add.s64 	%rd3818, %rd3390, 3424;
	// begin inline asm
	{ atom.add.f64 %fd9497,[%rd3818],%fd2278; }

	// end inline asm
	add.s64 	%rd3819, %rd3390, 3432;
	// begin inline asm
	{ atom.add.f64 %fd9499,[%rd3819],%fd2253; }

	// end inline asm
	add.s64 	%rd3820, %rd3390, 3440;
	// begin inline asm
	{ atom.add.f64 %fd9501,[%rd3820],%fd2252; }

	// end inline asm
	add.s64 	%rd3821, %rd3390, 3448;
	// begin inline asm
	{ atom.add.f64 %fd9503,[%rd3821],%fd2251; }

	// end inline asm
	add.s64 	%rd3822, %rd3390, 3456;
	// begin inline asm
	{ atom.add.f64 %fd9505,[%rd3822],%fd1368; }

	// end inline asm
	add.s64 	%rd3823, %rd3390, 3464;
	// begin inline asm
	{ atom.add.f64 %fd9507,[%rd3823],%fd1367; }

	// end inline asm
	add.s64 	%rd3824, %rd3390, 3472;
	// begin inline asm
	{ atom.add.f64 %fd9509,[%rd3824],%fd1366; }

	// end inline asm
	add.s64 	%rd3825, %rd3390, 3480;
	// begin inline asm
	{ atom.add.f64 %fd9511,[%rd3825],%fd1341; }

	// end inline asm
	add.s64 	%rd3826, %rd3390, 3488;
	// begin inline asm
	{ atom.add.f64 %fd9513,[%rd3826],%fd1340; }

	// end inline asm
	add.s64 	%rd3827, %rd3390, 3496;
	// begin inline asm
	{ atom.add.f64 %fd9515,[%rd3827],%fd1339; }

	// end inline asm
	add.s64 	%rd3828, %rd3390, 3504;
	// begin inline asm
	{ atom.add.f64 %fd9517,[%rd3828],%fd1314; }

	// end inline asm
	add.s64 	%rd3829, %rd3390, 3512;
	// begin inline asm
	{ atom.add.f64 %fd9519,[%rd3829],%fd1313; }

	// end inline asm
	add.s64 	%rd3830, %rd3390, 3520;
	// begin inline asm
	{ atom.add.f64 %fd9521,[%rd3830],%fd1312; }

	// end inline asm
	add.s64 	%rd3831, %rd3390, 3528;
	// begin inline asm
	{ atom.add.f64 %fd9523,[%rd3831],%fd1287; }

	// end inline asm
	add.s64 	%rd3832, %rd3390, 3536;
	// begin inline asm
	{ atom.add.f64 %fd9525,[%rd3832],%fd1286; }

	// end inline asm
	add.s64 	%rd3833, %rd3390, 3544;
	// begin inline asm
	{ atom.add.f64 %fd9527,[%rd3833],%fd1285; }

	// end inline asm
	add.s64 	%rd3834, %rd3390, 3552;
	// begin inline asm
	{ atom.add.f64 %fd9529,[%rd3834],%fd2232; }

	// end inline asm
	add.s64 	%rd3835, %rd3390, 3560;
	// begin inline asm
	{ atom.add.f64 %fd9531,[%rd3835],%fd2231; }

	// end inline asm
	add.s64 	%rd3836, %rd3390, 3568;
	// begin inline asm
	{ atom.add.f64 %fd9533,[%rd3836],%fd2230; }

	// end inline asm
	add.s64 	%rd3837, %rd3390, 3576;
	// begin inline asm
	{ atom.add.f64 %fd9535,[%rd3837],%fd2205; }

	// end inline asm
	add.s64 	%rd3838, %rd3390, 3584;
	// begin inline asm
	{ atom.add.f64 %fd9537,[%rd3838],%fd2204; }

	// end inline asm
	add.s64 	%rd3839, %rd3390, 3592;
	// begin inline asm
	{ atom.add.f64 %fd9539,[%rd3839],%fd2203; }

	// end inline asm
	add.s64 	%rd3840, %rd3390, 3600;
	// begin inline asm
	{ atom.add.f64 %fd9541,[%rd3840],%fd2178; }

	// end inline asm
	add.s64 	%rd3841, %rd3390, 3608;
	// begin inline asm
	{ atom.add.f64 %fd9543,[%rd3841],%fd2177; }

	// end inline asm
	add.s64 	%rd3842, %rd3390, 3616;
	// begin inline asm
	{ atom.add.f64 %fd9545,[%rd3842],%fd2176; }

	// end inline asm
	add.s64 	%rd3843, %rd3390, 3624;
	// begin inline asm
	{ atom.add.f64 %fd9547,[%rd3843],%fd2151; }

	// end inline asm
	add.s64 	%rd3844, %rd3390, 3632;
	// begin inline asm
	{ atom.add.f64 %fd9549,[%rd3844],%fd2150; }

	// end inline asm
	add.s64 	%rd3845, %rd3390, 3640;
	// begin inline asm
	{ atom.add.f64 %fd9551,[%rd3845],%fd2149; }

	// end inline asm
	add.s64 	%rd3846, %rd3390, 3648;
	// begin inline asm
	{ atom.add.f64 %fd9553,[%rd3846],%fd1365; }

	// end inline asm
	add.s64 	%rd3847, %rd3390, 3656;
	// begin inline asm
	{ atom.add.f64 %fd9555,[%rd3847],%fd1364; }

	// end inline asm
	add.s64 	%rd3848, %rd3390, 3664;
	// begin inline asm
	{ atom.add.f64 %fd9557,[%rd3848],%fd1363; }

	// end inline asm
	add.s64 	%rd3849, %rd3390, 3672;
	// begin inline asm
	{ atom.add.f64 %fd9559,[%rd3849],%fd1338; }

	// end inline asm
	add.s64 	%rd3850, %rd3390, 3680;
	// begin inline asm
	{ atom.add.f64 %fd9561,[%rd3850],%fd1337; }

	// end inline asm
	add.s64 	%rd3851, %rd3390, 3688;
	// begin inline asm
	{ atom.add.f64 %fd9563,[%rd3851],%fd1336; }

	// end inline asm
	add.s64 	%rd3852, %rd3390, 3696;
	// begin inline asm
	{ atom.add.f64 %fd9565,[%rd3852],%fd1311; }

	// end inline asm
	add.s64 	%rd3853, %rd3390, 3704;
	// begin inline asm
	{ atom.add.f64 %fd9567,[%rd3853],%fd1310; }

	// end inline asm
	add.s64 	%rd3854, %rd3390, 3712;
	// begin inline asm
	{ atom.add.f64 %fd9569,[%rd3854],%fd1309; }

	// end inline asm
	add.s64 	%rd3855, %rd3390, 3720;
	// begin inline asm
	{ atom.add.f64 %fd9571,[%rd3855],%fd1284; }

	// end inline asm
	add.s64 	%rd3856, %rd3390, 3728;
	// begin inline asm
	{ atom.add.f64 %fd9573,[%rd3856],%fd1283; }

	// end inline asm
	add.s64 	%rd3857, %rd3390, 3736;
	// begin inline asm
	{ atom.add.f64 %fd9575,[%rd3857],%fd1282; }

	// end inline asm
	add.s64 	%rd3858, %rd3390, 3744;
	// begin inline asm
	{ atom.add.f64 %fd9577,[%rd3858],%fd2229; }

	// end inline asm
	add.s64 	%rd3859, %rd3390, 3752;
	// begin inline asm
	{ atom.add.f64 %fd9579,[%rd3859],%fd2228; }

	// end inline asm
	add.s64 	%rd3860, %rd3390, 3760;
	// begin inline asm
	{ atom.add.f64 %fd9581,[%rd3860],%fd2227; }

	// end inline asm
	add.s64 	%rd3861, %rd3390, 3768;
	// begin inline asm
	{ atom.add.f64 %fd9583,[%rd3861],%fd2202; }

	// end inline asm
	add.s64 	%rd3862, %rd3390, 3776;
	// begin inline asm
	{ atom.add.f64 %fd9585,[%rd3862],%fd2201; }

	// end inline asm
	add.s64 	%rd3863, %rd3390, 3784;
	// begin inline asm
	{ atom.add.f64 %fd9587,[%rd3863],%fd2200; }

	// end inline asm
	add.s64 	%rd3864, %rd3390, 3792;
	// begin inline asm
	{ atom.add.f64 %fd9589,[%rd3864],%fd2175; }

	// end inline asm
	add.s64 	%rd3865, %rd3390, 3800;
	// begin inline asm
	{ atom.add.f64 %fd9591,[%rd3865],%fd2174; }

	// end inline asm
	add.s64 	%rd3866, %rd3390, 3808;
	// begin inline asm
	{ atom.add.f64 %fd9593,[%rd3866],%fd2173; }

	// end inline asm
	add.s64 	%rd3867, %rd3390, 3816;
	// begin inline asm
	{ atom.add.f64 %fd9595,[%rd3867],%fd2148; }

	// end inline asm
	add.s64 	%rd3868, %rd3390, 3824;
	// begin inline asm
	{ atom.add.f64 %fd9597,[%rd3868],%fd2147; }

	// end inline asm
	add.s64 	%rd3869, %rd3390, 3832;
	// begin inline asm
	{ atom.add.f64 %fd9599,[%rd3869],%fd2146; }

	// end inline asm
	add.s64 	%rd3870, %rd3390, 3840;
	// begin inline asm
	{ atom.add.f64 %fd9601,[%rd3870],%fd1362; }

	// end inline asm
	add.s64 	%rd3871, %rd3390, 3848;
	// begin inline asm
	{ atom.add.f64 %fd9603,[%rd3871],%fd1361; }

	// end inline asm
	add.s64 	%rd3872, %rd3390, 3856;
	// begin inline asm
	{ atom.add.f64 %fd9605,[%rd3872],%fd1360; }

	// end inline asm
	add.s64 	%rd3873, %rd3390, 3864;
	// begin inline asm
	{ atom.add.f64 %fd9607,[%rd3873],%fd1335; }

	// end inline asm
	add.s64 	%rd3874, %rd3390, 3872;
	// begin inline asm
	{ atom.add.f64 %fd9609,[%rd3874],%fd1334; }

	// end inline asm
	add.s64 	%rd3875, %rd3390, 3880;
	// begin inline asm
	{ atom.add.f64 %fd9611,[%rd3875],%fd1333; }

	// end inline asm
	add.s64 	%rd3876, %rd3390, 3888;
	// begin inline asm
	{ atom.add.f64 %fd9613,[%rd3876],%fd1308; }

	// end inline asm
	add.s64 	%rd3877, %rd3390, 3896;
	// begin inline asm
	{ atom.add.f64 %fd9615,[%rd3877],%fd1307; }

	// end inline asm
	add.s64 	%rd3878, %rd3390, 3904;
	// begin inline asm
	{ atom.add.f64 %fd9617,[%rd3878],%fd1306; }

	// end inline asm
	add.s64 	%rd3879, %rd3390, 3912;
	// begin inline asm
	{ atom.add.f64 %fd9619,[%rd3879],%fd1281; }

	// end inline asm
	add.s64 	%rd3880, %rd3390, 3920;
	// begin inline asm
	{ atom.add.f64 %fd9621,[%rd3880],%fd1280; }

	// end inline asm
	add.s64 	%rd3881, %rd3390, 3928;
	// begin inline asm
	{ atom.add.f64 %fd9623,[%rd3881],%fd1279; }

	// end inline asm
	add.s64 	%rd3882, %rd3390, 3936;
	// begin inline asm
	{ atom.add.f64 %fd9625,[%rd3882],%fd2226; }

	// end inline asm
	add.s64 	%rd3883, %rd3390, 3944;
	// begin inline asm
	{ atom.add.f64 %fd9627,[%rd3883],%fd2225; }

	// end inline asm
	add.s64 	%rd3884, %rd3390, 3952;
	// begin inline asm
	{ atom.add.f64 %fd9629,[%rd3884],%fd2224; }

	// end inline asm
	add.s64 	%rd3885, %rd3390, 3960;
	// begin inline asm
	{ atom.add.f64 %fd9631,[%rd3885],%fd2199; }

	// end inline asm
	add.s64 	%rd3886, %rd3390, 3968;
	// begin inline asm
	{ atom.add.f64 %fd9633,[%rd3886],%fd2198; }

	// end inline asm
	add.s64 	%rd3887, %rd3390, 3976;
	// begin inline asm
	{ atom.add.f64 %fd9635,[%rd3887],%fd2197; }

	// end inline asm
	add.s64 	%rd3888, %rd3390, 3984;
	// begin inline asm
	{ atom.add.f64 %fd9637,[%rd3888],%fd2172; }

	// end inline asm
	add.s64 	%rd3889, %rd3390, 3992;
	// begin inline asm
	{ atom.add.f64 %fd9639,[%rd3889],%fd2171; }

	// end inline asm
	add.s64 	%rd3890, %rd3390, 4000;
	// begin inline asm
	{ atom.add.f64 %fd9641,[%rd3890],%fd2170; }

	// end inline asm
	add.s64 	%rd3891, %rd3390, 4008;
	// begin inline asm
	{ atom.add.f64 %fd9643,[%rd3891],%fd2145; }

	// end inline asm
	add.s64 	%rd3892, %rd3390, 4016;
	// begin inline asm
	{ atom.add.f64 %fd9645,[%rd3892],%fd2144; }

	// end inline asm
	add.s64 	%rd3893, %rd3390, 4024;
	// begin inline asm
	{ atom.add.f64 %fd9647,[%rd3893],%fd2143; }

	// end inline asm
	add.s64 	%rd3894, %rd3390, 4032;
	// begin inline asm
	{ atom.add.f64 %fd9649,[%rd3894],%fd1260; }

	// end inline asm
	add.s64 	%rd3895, %rd3390, 4040;
	// begin inline asm
	{ atom.add.f64 %fd9651,[%rd3895],%fd1259; }

	// end inline asm
	add.s64 	%rd3896, %rd3390, 4048;
	// begin inline asm
	{ atom.add.f64 %fd9653,[%rd3896],%fd1258; }

	// end inline asm
	add.s64 	%rd3897, %rd3390, 4056;
	// begin inline asm
	{ atom.add.f64 %fd9655,[%rd3897],%fd1233; }

	// end inline asm
	add.s64 	%rd3898, %rd3390, 4064;
	// begin inline asm
	{ atom.add.f64 %fd9657,[%rd3898],%fd1232; }

	// end inline asm
	add.s64 	%rd3899, %rd3390, 4072;
	// begin inline asm
	{ atom.add.f64 %fd9659,[%rd3899],%fd1231; }

	// end inline asm
	add.s64 	%rd3900, %rd3390, 4080;
	// begin inline asm
	{ atom.add.f64 %fd9661,[%rd3900],%fd1206; }

	// end inline asm
	add.s64 	%rd3901, %rd3390, 4088;
	// begin inline asm
	{ atom.add.f64 %fd9663,[%rd3901],%fd1205; }

	// end inline asm
	add.s64 	%rd3902, %rd3390, 4096;
	// begin inline asm
	{ atom.add.f64 %fd9665,[%rd3902],%fd1204; }

	// end inline asm
	add.s64 	%rd3903, %rd3390, 4104;
	// begin inline asm
	{ atom.add.f64 %fd9667,[%rd3903],%fd1179; }

	// end inline asm
	add.s64 	%rd3904, %rd3390, 4112;
	// begin inline asm
	{ atom.add.f64 %fd9669,[%rd3904],%fd1178; }

	// end inline asm
	add.s64 	%rd3905, %rd3390, 4120;
	// begin inline asm
	{ atom.add.f64 %fd9671,[%rd3905],%fd1177; }

	// end inline asm
	add.s64 	%rd3906, %rd3390, 4128;
	// begin inline asm
	{ atom.add.f64 %fd9673,[%rd3906],%fd2124; }

	// end inline asm
	add.s64 	%rd3907, %rd3390, 4136;
	// begin inline asm
	{ atom.add.f64 %fd9675,[%rd3907],%fd2123; }

	// end inline asm
	add.s64 	%rd3908, %rd3390, 4144;
	// begin inline asm
	{ atom.add.f64 %fd9677,[%rd3908],%fd2122; }

	// end inline asm
	add.s64 	%rd3909, %rd3390, 4152;
	// begin inline asm
	{ atom.add.f64 %fd9679,[%rd3909],%fd2097; }

	// end inline asm
	add.s64 	%rd3910, %rd3390, 4160;
	// begin inline asm
	{ atom.add.f64 %fd9681,[%rd3910],%fd2096; }

	// end inline asm
	add.s64 	%rd3911, %rd3390, 4168;
	// begin inline asm
	{ atom.add.f64 %fd9683,[%rd3911],%fd2095; }

	// end inline asm
	add.s64 	%rd3912, %rd3390, 4176;
	// begin inline asm
	{ atom.add.f64 %fd9685,[%rd3912],%fd2070; }

	// end inline asm
	add.s64 	%rd3913, %rd3390, 4184;
	// begin inline asm
	{ atom.add.f64 %fd9687,[%rd3913],%fd2069; }

	// end inline asm
	add.s64 	%rd3914, %rd3390, 4192;
	// begin inline asm
	{ atom.add.f64 %fd9689,[%rd3914],%fd2068; }

	// end inline asm
	add.s64 	%rd3915, %rd3390, 4200;
	// begin inline asm
	{ atom.add.f64 %fd9691,[%rd3915],%fd2043; }

	// end inline asm
	add.s64 	%rd3916, %rd3390, 4208;
	// begin inline asm
	{ atom.add.f64 %fd9693,[%rd3916],%fd2042; }

	// end inline asm
	add.s64 	%rd3917, %rd3390, 4216;
	// begin inline asm
	{ atom.add.f64 %fd9695,[%rd3917],%fd2041; }

	// end inline asm
	add.s64 	%rd3918, %rd3390, 4224;
	// begin inline asm
	{ atom.add.f64 %fd9697,[%rd3918],%fd1257; }

	// end inline asm
	add.s64 	%rd3919, %rd3390, 4232;
	// begin inline asm
	{ atom.add.f64 %fd9699,[%rd3919],%fd1256; }

	// end inline asm
	add.s64 	%rd3920, %rd3390, 4240;
	// begin inline asm
	{ atom.add.f64 %fd9701,[%rd3920],%fd1255; }

	// end inline asm
	add.s64 	%rd3921, %rd3390, 4248;
	// begin inline asm
	{ atom.add.f64 %fd9703,[%rd3921],%fd1230; }

	// end inline asm
	add.s64 	%rd3922, %rd3390, 4256;
	// begin inline asm
	{ atom.add.f64 %fd9705,[%rd3922],%fd1229; }

	// end inline asm
	add.s64 	%rd3923, %rd3390, 4264;
	// begin inline asm
	{ atom.add.f64 %fd9707,[%rd3923],%fd1228; }

	// end inline asm
	add.s64 	%rd3924, %rd3390, 4272;
	// begin inline asm
	{ atom.add.f64 %fd9709,[%rd3924],%fd1203; }

	// end inline asm
	add.s64 	%rd3925, %rd3390, 4280;
	// begin inline asm
	{ atom.add.f64 %fd9711,[%rd3925],%fd1202; }

	// end inline asm
	add.s64 	%rd3926, %rd3390, 4288;
	// begin inline asm
	{ atom.add.f64 %fd9713,[%rd3926],%fd1201; }

	// end inline asm
	add.s64 	%rd3927, %rd3390, 4296;
	// begin inline asm
	{ atom.add.f64 %fd9715,[%rd3927],%fd1176; }

	// end inline asm
	add.s64 	%rd3928, %rd3390, 4304;
	// begin inline asm
	{ atom.add.f64 %fd9717,[%rd3928],%fd1175; }

	// end inline asm
	add.s64 	%rd3929, %rd3390, 4312;
	// begin inline asm
	{ atom.add.f64 %fd9719,[%rd3929],%fd1174; }

	// end inline asm
	add.s64 	%rd3930, %rd3390, 4320;
	// begin inline asm
	{ atom.add.f64 %fd9721,[%rd3930],%fd2121; }

	// end inline asm
	add.s64 	%rd3931, %rd3390, 4328;
	// begin inline asm
	{ atom.add.f64 %fd9723,[%rd3931],%fd2120; }

	// end inline asm
	add.s64 	%rd3932, %rd3390, 4336;
	// begin inline asm
	{ atom.add.f64 %fd9725,[%rd3932],%fd2119; }

	// end inline asm
	add.s64 	%rd3933, %rd3390, 4344;
	// begin inline asm
	{ atom.add.f64 %fd9727,[%rd3933],%fd2094; }

	// end inline asm
	add.s64 	%rd3934, %rd3390, 4352;
	// begin inline asm
	{ atom.add.f64 %fd9729,[%rd3934],%fd2093; }

	// end inline asm
	add.s64 	%rd3935, %rd3390, 4360;
	// begin inline asm
	{ atom.add.f64 %fd9731,[%rd3935],%fd2092; }

	// end inline asm
	add.s64 	%rd3936, %rd3390, 4368;
	// begin inline asm
	{ atom.add.f64 %fd9733,[%rd3936],%fd2067; }

	// end inline asm
	add.s64 	%rd3937, %rd3390, 4376;
	// begin inline asm
	{ atom.add.f64 %fd9735,[%rd3937],%fd2066; }

	// end inline asm
	add.s64 	%rd3938, %rd3390, 4384;
	// begin inline asm
	{ atom.add.f64 %fd9737,[%rd3938],%fd2065; }

	// end inline asm
	add.s64 	%rd3939, %rd3390, 4392;
	// begin inline asm
	{ atom.add.f64 %fd9739,[%rd3939],%fd2040; }

	// end inline asm
	add.s64 	%rd3940, %rd3390, 4400;
	// begin inline asm
	{ atom.add.f64 %fd9741,[%rd3940],%fd2039; }

	// end inline asm
	add.s64 	%rd3941, %rd3390, 4408;
	// begin inline asm
	{ atom.add.f64 %fd9743,[%rd3941],%fd2038; }

	// end inline asm
	add.s64 	%rd3942, %rd3390, 4416;
	// begin inline asm
	{ atom.add.f64 %fd9745,[%rd3942],%fd1254; }

	// end inline asm
	add.s64 	%rd3943, %rd3390, 4424;
	// begin inline asm
	{ atom.add.f64 %fd9747,[%rd3943],%fd1253; }

	// end inline asm
	add.s64 	%rd3944, %rd3390, 4432;
	// begin inline asm
	{ atom.add.f64 %fd9749,[%rd3944],%fd1252; }

	// end inline asm
	add.s64 	%rd3945, %rd3390, 4440;
	// begin inline asm
	{ atom.add.f64 %fd9751,[%rd3945],%fd1227; }

	// end inline asm
	add.s64 	%rd3946, %rd3390, 4448;
	// begin inline asm
	{ atom.add.f64 %fd9753,[%rd3946],%fd1226; }

	// end inline asm
	add.s64 	%rd3947, %rd3390, 4456;
	// begin inline asm
	{ atom.add.f64 %fd9755,[%rd3947],%fd1225; }

	// end inline asm
	add.s64 	%rd3948, %rd3390, 4464;
	// begin inline asm
	{ atom.add.f64 %fd9757,[%rd3948],%fd1200; }

	// end inline asm
	add.s64 	%rd3949, %rd3390, 4472;
	// begin inline asm
	{ atom.add.f64 %fd9759,[%rd3949],%fd1199; }

	// end inline asm
	add.s64 	%rd3950, %rd3390, 4480;
	// begin inline asm
	{ atom.add.f64 %fd9761,[%rd3950],%fd1198; }

	// end inline asm
	add.s64 	%rd3951, %rd3390, 4488;
	// begin inline asm
	{ atom.add.f64 %fd9763,[%rd3951],%fd1173; }

	// end inline asm
	add.s64 	%rd3952, %rd3390, 4496;
	// begin inline asm
	{ atom.add.f64 %fd9765,[%rd3952],%fd1172; }

	// end inline asm
	add.s64 	%rd3953, %rd3390, 4504;
	// begin inline asm
	{ atom.add.f64 %fd9767,[%rd3953],%fd1171; }

	// end inline asm
	add.s64 	%rd3954, %rd3390, 4512;
	// begin inline asm
	{ atom.add.f64 %fd9769,[%rd3954],%fd2118; }

	// end inline asm
	add.s64 	%rd3955, %rd3390, 4520;
	// begin inline asm
	{ atom.add.f64 %fd9771,[%rd3955],%fd2117; }

	// end inline asm
	add.s64 	%rd3956, %rd3390, 4528;
	// begin inline asm
	{ atom.add.f64 %fd9773,[%rd3956],%fd2116; }

	// end inline asm
	add.s64 	%rd3957, %rd3390, 4536;
	// begin inline asm
	{ atom.add.f64 %fd9775,[%rd3957],%fd2091; }

	// end inline asm
	add.s64 	%rd3958, %rd3390, 4544;
	// begin inline asm
	{ atom.add.f64 %fd9777,[%rd3958],%fd2090; }

	// end inline asm
	add.s64 	%rd3959, %rd3390, 4552;
	// begin inline asm
	{ atom.add.f64 %fd9779,[%rd3959],%fd2089; }

	// end inline asm
	add.s64 	%rd3960, %rd3390, 4560;
	// begin inline asm
	{ atom.add.f64 %fd9781,[%rd3960],%fd2064; }

	// end inline asm
	add.s64 	%rd3961, %rd3390, 4568;
	// begin inline asm
	{ atom.add.f64 %fd9783,[%rd3961],%fd2063; }

	// end inline asm
	add.s64 	%rd3962, %rd3390, 4576;
	// begin inline asm
	{ atom.add.f64 %fd9785,[%rd3962],%fd2062; }

	// end inline asm
	add.s64 	%rd3963, %rd3390, 4584;
	// begin inline asm
	{ atom.add.f64 %fd9787,[%rd3963],%fd2037; }

	// end inline asm
	add.s64 	%rd3964, %rd3390, 4592;
	// begin inline asm
	{ atom.add.f64 %fd9789,[%rd3964],%fd2036; }

	// end inline asm
	add.s64 	%rd3965, %rd3390, 4600;
	// begin inline asm
	{ atom.add.f64 %fd9791,[%rd3965],%fd2035; }

	// end inline asm

$L__BB1_661:
	ld.param.u64 	%rd3972, [assemble_matrix_cuda_kernel_backward_param_0+24];
	mov.u32 	%r2038, %ntid.x;
	mov.u32 	%r1656, %nctaid.x;
	mul.wide.u32 	%rd3968, %r2038, %r1656;
	add.s64 	%rd4121, %rd4121, %rd3968;
	setp.lt.u64 	%p941, %rd4121, %rd3972;
	@%p941 bra 	$L__BB1_2;

$L__BB1_662:
	ret;

}
	// .globl	add_projected_x_to_y_cuda_kernel_forward
.visible .entry add_projected_x_to_y_cuda_kernel_forward(
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_4[56]
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<92>;
	.reg .f64 	%fd<35>;
	.reg .b64 	%rd<71>;


	ld.param.v2.u32 	{%r43, %r44}, [add_projected_x_to_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [add_projected_x_to_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [add_projected_x_to_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [add_projected_x_to_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [add_projected_x_to_y_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [add_projected_x_to_y_cuda_kernel_forward_param_4+32];
	ld.param.u64 	%rd34, [add_projected_x_to_y_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd32, [add_projected_x_to_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd30, [add_projected_x_to_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd28, [add_projected_x_to_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd27, [add_projected_x_to_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [add_projected_x_to_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd36, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd37, %r81;
	add.s64 	%rd67, %rd36, %rd37;
	setp.ge.u64 	%p1, %rd67, %rd27;
	@%p1 bra 	$L__BB2_15;

	cvta.to.global.u64 	%rd4, %rd34;
	cvta.to.global.u64 	%rd5, %rd32;
	cvta.to.global.u64 	%rd6, %rd30;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r67;
	cvt.s64.s32 	%rd12, %r59;
	cvt.s64.s32 	%rd13, %r75;
	cvt.s64.s32 	%rd14, %r51;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd38, %r82;
	mul.lo.s64 	%rd15, %rd1, %rd38;

$L__BB2_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd68, %rd67;
	@%p2 bra 	$L__BB2_6;

	or.b64  	%rd39, %rd67, %rd8;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p3, %rd40, 0;
	@%p3 bra 	$L__BB2_5;

	div.u64 	%rd68, %rd67, %rd8;
	bra.uni 	$L__BB2_6;

$L__BB2_5:
	cvt.u32.u64 	%r83, %rd8;
	cvt.u32.u64 	%r84, %rd67;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd68, %r85;

$L__BB2_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB2_10;

	or.b64  	%rd41, %rd68, %rd9;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd68, %rd68, %rd9;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r86, %rd9;
	cvt.u32.u64 	%r87, %rd68;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd68, %r88;

$L__BB2_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB2_14;

	or.b64  	%rd43, %rd68, %rd10;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p7, %rd44, 0;
	@%p7 bra 	$L__BB2_13;

	div.u64 	%rd68, %rd68, %rd10;
	bra.uni 	$L__BB2_14;

$L__BB2_13:
	cvt.u32.u64 	%r89, %rd10;
	cvt.u32.u64 	%r90, %rd68;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd68, %r91;

$L__BB2_14:
	cvt.s64.s32 	%rd57, %rd68;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd58, %rd57, 0, %p8;
	mul.lo.s64 	%rd59, %rd58, %rd11;
	add.s64 	%rd60, %rd5, %rd59;
	mul.lo.s64 	%rd61, %rd58, %rd12;
	add.s64 	%rd62, %rd6, %rd61;
	mul.lo.s64 	%rd63, %rd58, %rd13;
	add.s64 	%rd64, %rd4, %rd63;
	ld.global.f64 	%fd25, [%rd64];
	mov.f64 	%fd26, 0d3FF0000000000000;
	sub.f64 	%fd27, %fd26, %fd25;
	ld.global.f64 	%fd28, [%rd64+8];
	sub.f64 	%fd29, %fd27, %fd28;
	ld.global.f64 	%fd30, [%rd64+16];
	sub.f64 	%fd31, %fd29, %fd30;
	ld.global.f64 	%fd32, [%rd62];
	mul.f64 	%fd2, %fd32, %fd31;
	ld.global.f64 	%fd33, [%rd62+8];
	mul.f64 	%fd4, %fd33, %fd31;
	ld.global.f64 	%fd34, [%rd62+16];
	mul.f64 	%fd6, %fd34, %fd31;
	mul.f64 	%fd8, %fd32, %fd25;
	mul.f64 	%fd10, %fd33, %fd25;
	mul.f64 	%fd12, %fd34, %fd25;
	mul.f64 	%fd14, %fd32, %fd28;
	mul.f64 	%fd16, %fd33, %fd28;
	mul.f64 	%fd18, %fd34, %fd28;
	mul.f64 	%fd20, %fd32, %fd30;
	mul.f64 	%fd22, %fd33, %fd30;
	mul.f64 	%fd24, %fd34, %fd30;
	ld.global.s32 	%rd65, [%rd60];
	mul.lo.s64 	%rd66, %rd65, %rd14;
	add.s64 	%rd45, %rd28, %rd66;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd45],%fd2; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd46],%fd4; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd47],%fd6; }

	// end inline asm
	add.s64 	%rd48, %rd45, 24;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd48],%fd8; }

	// end inline asm
	add.s64 	%rd49, %rd45, 32;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd49],%fd10; }

	// end inline asm
	add.s64 	%rd50, %rd45, 40;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd50],%fd12; }

	// end inline asm
	add.s64 	%rd51, %rd45, 48;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd51],%fd14; }

	// end inline asm
	add.s64 	%rd52, %rd45, 56;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd52],%fd16; }

	// end inline asm
	add.s64 	%rd53, %rd45, 64;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd53],%fd18; }

	// end inline asm
	add.s64 	%rd54, %rd45, 72;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd54],%fd20; }

	// end inline asm
	add.s64 	%rd55, %rd45, 80;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd55],%fd22; }

	// end inline asm
	add.s64 	%rd56, %rd45, 88;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd56],%fd24; }

	// end inline asm
	add.s64 	%rd67, %rd67, %rd15;
	setp.lt.u64 	%p9, %rd67, %rd27;
	@%p9 bra 	$L__BB2_2;

$L__BB2_15:
	ret;

}
	// .globl	add_projected_x_to_y_cuda_kernel_backward
.visible .entry add_projected_x_to_y_cuda_kernel_backward(
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_8[56]
)
{
	.reg .pred 	%p<26>;
	.reg .b16 	%rs<57>;
	.reg .b32 	%r<143>;
	.reg .f64 	%fd<242>;
	.reg .b64 	%rd<130>;


	ld.param.v2.u32 	{%r70, %r71}, [add_projected_x_to_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [add_projected_x_to_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [add_projected_x_to_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [add_projected_x_to_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [add_projected_x_to_y_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r102, %r103}, [add_projected_x_to_y_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r110, %r111}, [add_projected_x_to_y_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r118, %r119}, [add_projected_x_to_y_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r126, %r127}, [add_projected_x_to_y_cuda_kernel_backward_param_8+32];
	ld.param.u64 	%rd54, [add_projected_x_to_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd52, [add_projected_x_to_y_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd50, [add_projected_x_to_y_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd49, [add_projected_x_to_y_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd48, [add_projected_x_to_y_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd46, [add_projected_x_to_y_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd45, [add_projected_x_to_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd44, [add_projected_x_to_y_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd43, [add_projected_x_to_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd41, [add_projected_x_to_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [add_projected_x_to_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd56, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd57, %r132;
	add.s64 	%rd126, %rd56, %rd57;
	setp.ge.u64 	%p1, %rd126, %rd41;
	@%p1 bra 	$L__BB3_47;

	cvta.to.global.u64 	%rd11, %rd48;
	cvta.to.global.u64 	%rd12, %rd46;
	cvta.to.global.u64 	%rd13, %rd44;
	cvt.s64.s32 	%rd15, %r73;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r71;
	cvt.s64.s32 	%rd18, %r94;
	cvt.s64.s32 	%rd19, %r86;
	cvt.s64.s32 	%rd20, %r102;
	cvt.s64.s32 	%rd21, %r110;
	cvt.s64.s32 	%rd22, %r78;
	cvt.s64.s32 	%rd23, %r126;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd58, %r133;
	mul.lo.s64 	%rd24, %rd1, %rd58;
	cvt.s64.s32 	%rd25, %r118;

$L__BB3_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd127, %rd126;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd59, %rd126, %rd15;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p3, %rd60, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd127, %rd126, %rd15;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r134, %rd15;
	cvt.u32.u64 	%r135, %rd126;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd127, %r136;

$L__BB3_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd61, %rd127, %rd16;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p5, %rd62, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd127, %rd127, %rd16;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r137, %rd16;
	cvt.u32.u64 	%r138, %rd127;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd127, %r139;

$L__BB3_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd63, %rd127, %rd17;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p7, %rd64, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd127, %rd127, %rd17;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r140, %rd17;
	cvt.u32.u64 	%r141, %rd127;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd127, %r142;

$L__BB3_14:
	cvt.s64.s32 	%rd65, %rd127;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd36, %rd65, 0, %p8;
	mul.lo.s64 	%rd66, %rd36, %rd18;
	add.s64 	%rd67, %rd12, %rd66;
	ld.global.s32 	%rd37, [%rd67];
	mul.lo.s64 	%rd38, %rd36, %rd19;
	add.s64 	%rd68, %rd13, %rd38;
	ld.global.f64 	%fd1, [%rd68];
	ld.global.f64 	%fd2, [%rd68+8];
	ld.global.f64 	%fd3, [%rd68+16];
	mul.lo.s64 	%rd39, %rd36, %rd20;
	add.s64 	%rd69, %rd11, %rd39;
	ld.global.f64 	%fd4, [%rd69];
	ld.global.f64 	%fd5, [%rd69+8];
	ld.global.f64 	%fd6, [%rd69+16];
	setp.eq.s64 	%p9, %rd50, 0;
	@%p9 bra 	$L__BB3_16;

	cvta.to.global.u64 	%rd124, %rd50;
	mul.lo.s64 	%rd70, %rd37, %rd21;
	add.s64 	%rd71, %rd124, %rd70;
	ld.global.f64 	%fd54, [%rd71];
	add.f64 	%fd241, %fd54, 0d0000000000000000;
	ld.global.f64 	%fd55, [%rd71+8];
	add.f64 	%fd240, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd71+16];
	add.f64 	%fd239, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd71+24];
	add.f64 	%fd238, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd71+32];
	add.f64 	%fd237, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd71+40];
	add.f64 	%fd236, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd71+48];
	add.f64 	%fd235, %fd60, 0d0000000000000000;
	ld.global.f64 	%fd61, [%rd71+56];
	add.f64 	%fd234, %fd61, 0d0000000000000000;
	ld.global.f64 	%fd62, [%rd71+64];
	add.f64 	%fd233, %fd62, 0d0000000000000000;
	ld.global.f64 	%fd63, [%rd71+72];
	add.f64 	%fd232, %fd63, 0d0000000000000000;
	ld.global.f64 	%fd64, [%rd71+80];
	add.f64 	%fd231, %fd64, 0d0000000000000000;
	ld.global.f64 	%fd65, [%rd71+88];
	add.f64 	%fd230, %fd65, 0d0000000000000000;
	bra.uni 	$L__BB3_18;

$L__BB3_16:
	setp.eq.s64 	%p10, %rd43, 0;
	mov.f64 	%fd230, 0d0000000000000000;
	mov.f64 	%fd231, %fd230;
	mov.f64 	%fd232, %fd230;
	mov.f64 	%fd233, %fd230;
	mov.f64 	%fd234, %fd230;
	mov.f64 	%fd235, %fd230;
	mov.f64 	%fd236, %fd230;
	mov.f64 	%fd237, %fd230;
	mov.f64 	%fd238, %fd230;
	mov.f64 	%fd239, %fd230;
	mov.f64 	%fd240, %fd230;
	mov.f64 	%fd241, %fd230;
	@%p10 bra 	$L__BB3_18;

	cvta.to.global.u64 	%rd125, %rd43;
	mul.lo.s64 	%rd72, %rd37, %rd22;
	add.s64 	%rd73, %rd125, %rd72;
	ld.global.f64 	%fd78, [%rd73];
	add.f64 	%fd241, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd73+8];
	add.f64 	%fd240, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd73+16];
	add.f64 	%fd239, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd73+24];
	add.f64 	%fd238, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd73+32];
	add.f64 	%fd237, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd73+40];
	add.f64 	%fd236, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd73+48];
	add.f64 	%fd235, %fd84, 0d0000000000000000;
	ld.global.f64 	%fd85, [%rd73+56];
	add.f64 	%fd234, %fd85, 0d0000000000000000;
	ld.global.f64 	%fd86, [%rd73+64];
	add.f64 	%fd233, %fd86, 0d0000000000000000;
	ld.global.f64 	%fd87, [%rd73+72];
	add.f64 	%fd232, %fd87, 0d0000000000000000;
	ld.global.f64 	%fd88, [%rd73+80];
	add.f64 	%fd231, %fd88, 0d0000000000000000;
	ld.global.f64 	%fd89, [%rd73+88];
	add.f64 	%fd230, %fd89, 0d0000000000000000;

$L__BB3_18:
	mov.f64 	%fd90, 0d3FF0000000000000;
	sub.f64 	%fd91, %fd90, %fd4;
	sub.f64 	%fd92, %fd91, %fd5;
	sub.f64 	%fd93, %fd92, %fd6;
	add.f64 	%fd94, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd95, %fd3, %fd94, 0d0000000000000000;
	fma.rn.f64 	%fd96, %fd6, %fd94, 0d0000000000000000;
	add.f64 	%fd97, %fd231, 0d0000000000000000;
	fma.rn.f64 	%fd98, %fd2, %fd97, 0d0000000000000000;
	fma.rn.f64 	%fd99, %fd6, %fd97, 0d0000000000000000;
	add.f64 	%fd100, %fd95, %fd98;
	add.f64 	%fd101, %fd232, 0d0000000000000000;
	fma.rn.f64 	%fd102, %fd1, %fd101, 0d0000000000000000;
	fma.rn.f64 	%fd103, %fd6, %fd101, 0d0000000000000000;
	add.f64 	%fd104, %fd100, %fd102;
	add.f64 	%fd105, %fd233, 0d0000000000000000;
	fma.rn.f64 	%fd106, %fd3, %fd105, 0d0000000000000000;
	fma.rn.f64 	%fd107, %fd5, %fd105, 0d0000000000000000;
	add.f64 	%fd108, %fd96, %fd107;
	add.f64 	%fd109, %fd234, 0d0000000000000000;
	fma.rn.f64 	%fd110, %fd2, %fd109, 0d0000000000000000;
	fma.rn.f64 	%fd111, %fd5, %fd109, 0d0000000000000000;
	add.f64 	%fd112, %fd99, %fd111;
	add.f64 	%fd113, %fd106, %fd110;
	add.f64 	%fd114, %fd235, 0d0000000000000000;
	fma.rn.f64 	%fd115, %fd1, %fd114, 0d0000000000000000;
	fma.rn.f64 	%fd116, %fd5, %fd114, 0d0000000000000000;
	add.f64 	%fd117, %fd103, %fd116;
	add.f64 	%fd118, %fd113, %fd115;
	add.f64 	%fd119, %fd236, 0d0000000000000000;
	fma.rn.f64 	%fd120, %fd3, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd121, %fd4, %fd119, 0d0000000000000000;
	add.f64 	%fd122, %fd108, %fd121;
	add.f64 	%fd123, %fd237, 0d0000000000000000;
	fma.rn.f64 	%fd124, %fd2, %fd123, 0d0000000000000000;
	fma.rn.f64 	%fd125, %fd4, %fd123, 0d0000000000000000;
	add.f64 	%fd126, %fd112, %fd125;
	add.f64 	%fd127, %fd120, %fd124;
	add.f64 	%fd128, %fd238, 0d0000000000000000;
	fma.rn.f64 	%fd129, %fd1, %fd128, 0d0000000000000000;
	fma.rn.f64 	%fd130, %fd4, %fd128, 0d0000000000000000;
	add.f64 	%fd131, %fd117, %fd130;
	add.f64 	%fd132, %fd127, %fd129;
	add.f64 	%fd133, %fd239, 0d0000000000000000;
	fma.rn.f64 	%fd134, %fd3, %fd133, 0d0000000000000000;
	fma.rn.f64 	%fd135, %fd93, %fd133, 0d0000000000000000;
	add.f64 	%fd43, %fd122, %fd135;
	add.f64 	%fd136, %fd240, 0d0000000000000000;
	fma.rn.f64 	%fd137, %fd2, %fd136, 0d0000000000000000;
	fma.rn.f64 	%fd138, %fd93, %fd136, 0d0000000000000000;
	add.f64 	%fd44, %fd126, %fd138;
	add.f64 	%fd139, %fd134, %fd137;
	add.f64 	%fd140, %fd241, 0d0000000000000000;
	fma.rn.f64 	%fd141, %fd1, %fd140, 0d0000000000000000;
	fma.rn.f64 	%fd142, %fd93, %fd140, 0d0000000000000000;
	add.f64 	%fd45, %fd131, %fd142;
	add.f64 	%fd46, %fd139, %fd141;
	add.f64 	%fd47, %fd132, 0d0000000000000000;
	add.f64 	%fd48, %fd118, 0d0000000000000000;
	add.f64 	%fd49, %fd104, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd54, 0;
	@%p11 bra 	$L__BB3_20;

	mul.lo.s64 	%rd77, %rd36, %rd23;
	add.s64 	%rd74, %rd54, %rd77;
	mov.f64 	%fd146, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd74],%fd146; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd75],%fd146; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd76],%fd49; }

	// end inline asm
	bra.uni 	$L__BB3_22;

$L__BB3_20:
	setp.eq.s64 	%p12, %rd49, 0;
	@%p12 bra 	$L__BB3_22;

	add.s64 	%rd78, %rd49, %rd39;
	mov.f64 	%fd152, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd78],%fd152; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd79],%fd152; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd80],%fd49; }

	// end inline asm

$L__BB3_22:
	@%p11 bra 	$L__BB3_24;

	mul.lo.s64 	%rd84, %rd36, %rd23;
	add.s64 	%rd81, %rd54, %rd84;
	mov.f64 	%fd160, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd81],%fd160; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd82],%fd48; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd83],%fd160; }

	// end inline asm
	bra.uni 	$L__BB3_26;

$L__BB3_24:
	setp.eq.s64 	%p14, %rd49, 0;
	@%p14 bra 	$L__BB3_26;

	add.s64 	%rd85, %rd49, %rd39;
	mov.f64 	%fd166, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd85],%fd166; }

	// end inline asm
	add.s64 	%rd86, %rd85, 8;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd86],%fd48; }

	// end inline asm
	add.s64 	%rd87, %rd85, 16;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd87],%fd166; }

	// end inline asm

$L__BB3_26:
	@%p11 bra 	$L__BB3_28;

	mul.lo.s64 	%rd91, %rd36, %rd23;
	add.s64 	%rd88, %rd54, %rd91;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd88],%fd47; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	mov.f64 	%fd172, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd89],%fd172; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd90],%fd172; }

	// end inline asm
	bra.uni 	$L__BB3_30;

$L__BB3_28:
	setp.eq.s64 	%p16, %rd49, 0;
	@%p16 bra 	$L__BB3_30;

	add.s64 	%rd92, %rd49, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd92],%fd47; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	mov.f64 	%fd178, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd93],%fd178; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd94],%fd178; }

	// end inline asm

$L__BB3_30:
	add.f64 	%fd179, %fd46, 0d0000000000000000;
	mov.f64 	%fd180, 0d0000000000000000;
	sub.f64 	%fd181, %fd180, %fd179;
	add.f64 	%fd50, %fd181, 0d0000000000000000;
	@%p11 bra 	$L__BB3_32;

	mul.lo.s64 	%rd98, %rd36, %rd23;
	add.s64 	%rd95, %rd54, %rd98;
	// begin inline asm
	{ atom.add.f64 %fd182,[%rd95],%fd180; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd184,[%rd96],%fd180; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd186,[%rd97],%fd50; }

	// end inline asm
	bra.uni 	$L__BB3_34;

$L__BB3_32:
	setp.eq.s64 	%p18, %rd49, 0;
	@%p18 bra 	$L__BB3_34;

	add.s64 	%rd99, %rd49, %rd39;
	mov.f64 	%fd191, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd99],%fd191; }

	// end inline asm
	add.s64 	%rd100, %rd99, 8;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd100],%fd191; }

	// end inline asm
	add.s64 	%rd101, %rd99, 16;
	// begin inline asm
	{ atom.add.f64 %fd192,[%rd101],%fd50; }

	// end inline asm

$L__BB3_34:
	@%p11 bra 	$L__BB3_36;

	mul.lo.s64 	%rd105, %rd36, %rd23;
	add.s64 	%rd102, %rd54, %rd105;
	mov.f64 	%fd199, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd194,[%rd102],%fd199; }

	// end inline asm
	add.s64 	%rd103, %rd102, 8;
	// begin inline asm
	{ atom.add.f64 %fd196,[%rd103],%fd50; }

	// end inline asm
	add.s64 	%rd104, %rd102, 16;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd104],%fd199; }

	// end inline asm
	bra.uni 	$L__BB3_38;

$L__BB3_36:
	setp.eq.s64 	%p20, %rd49, 0;
	@%p20 bra 	$L__BB3_38;

	add.s64 	%rd106, %rd49, %rd39;
	mov.f64 	%fd205, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd106],%fd205; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd202,[%rd107],%fd50; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd204,[%rd108],%fd205; }

	// end inline asm

$L__BB3_38:
	@%p11 bra 	$L__BB3_40;

	mul.lo.s64 	%rd112, %rd36, %rd23;
	add.s64 	%rd109, %rd54, %rd112;
	// begin inline asm
	{ atom.add.f64 %fd206,[%rd109],%fd50; }

	// end inline asm
	add.s64 	%rd110, %rd109, 8;
	mov.f64 	%fd211, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd208,[%rd110],%fd211; }

	// end inline asm
	add.s64 	%rd111, %rd109, 16;
	// begin inline asm
	{ atom.add.f64 %fd210,[%rd111],%fd211; }

	// end inline asm
	bra.uni 	$L__BB3_42;

$L__BB3_40:
	setp.eq.s64 	%p22, %rd49, 0;
	@%p22 bra 	$L__BB3_42;

	add.s64 	%rd113, %rd49, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd212,[%rd113],%fd50; }

	// end inline asm
	add.s64 	%rd114, %rd113, 8;
	mov.f64 	%fd217, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd214,[%rd114],%fd217; }

	// end inline asm
	add.s64 	%rd115, %rd113, 16;
	// begin inline asm
	{ atom.add.f64 %fd216,[%rd115],%fd217; }

	// end inline asm

$L__BB3_42:
	setp.eq.s64 	%p23, %rd52, 0;
	add.f64 	%fd51, %fd45, 0d0000000000000000;
	add.f64 	%fd52, %fd44, 0d0000000000000000;
	add.f64 	%fd53, %fd43, 0d0000000000000000;
	@%p23 bra 	$L__BB3_44;

	mul.lo.s64 	%rd119, %rd36, %rd25;
	add.s64 	%rd116, %rd52, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd218,[%rd116],%fd51; }

	// end inline asm
	add.s64 	%rd117, %rd116, 8;
	// begin inline asm
	{ atom.add.f64 %fd220,[%rd117],%fd52; }

	// end inline asm
	add.s64 	%rd118, %rd116, 16;
	// begin inline asm
	{ atom.add.f64 %fd222,[%rd118],%fd53; }

	// end inline asm
	bra.uni 	$L__BB3_46;

$L__BB3_44:
	setp.eq.s64 	%p24, %rd45, 0;
	@%p24 bra 	$L__BB3_46;

	add.s64 	%rd120, %rd45, %rd38;
	// begin inline asm
	{ atom.add.f64 %fd224,[%rd120],%fd51; }

	// end inline asm
	add.s64 	%rd121, %rd120, 8;
	// begin inline asm
	{ atom.add.f64 %fd226,[%rd121],%fd52; }

	// end inline asm
	add.s64 	%rd122, %rd120, 16;
	// begin inline asm
	{ atom.add.f64 %fd228,[%rd122],%fd53; }

	// end inline asm

$L__BB3_46:
	ld.param.u64 	%rd123, [add_projected_x_to_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd126, %rd126, %rd24;
	setp.lt.u64 	%p25, %rd126, %rd123;
	@%p25 bra 	$L__BB3_2;

$L__BB3_47:
	ret;

}
	// .globl	val_IPC_hs_cuda_kernel_forward
.visible .entry val_IPC_hs_cuda_kernel_forward(
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_11[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_12[56],
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_13,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_14,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_15,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_16,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_17,
	.param .u32 val_IPC_hs_cuda_kernel_forward_param_18
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<97>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<257>;
	.reg .f64 	%fd<155>;
	.reg .b64 	%rd<113>;


	ld.param.v2.u32 	{%r126, %r127}, [val_IPC_hs_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r128, %r129}, [val_IPC_hs_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r134, %r135}, [val_IPC_hs_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r142, %r143}, [val_IPC_hs_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r150, %r151}, [val_IPC_hs_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r158, %r159}, [val_IPC_hs_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r166, %r167}, [val_IPC_hs_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r174, %r175}, [val_IPC_hs_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r182, %r183}, [val_IPC_hs_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r190, %r191}, [val_IPC_hs_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r198, %r199}, [val_IPC_hs_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r206, %r207}, [val_IPC_hs_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r214, %r215}, [val_IPC_hs_cuda_kernel_forward_param_11+32];
	ld.param.v2.u32 	{%r222, %r223}, [val_IPC_hs_cuda_kernel_forward_param_12+32];
	ld.param.f64 	%fd28, [val_IPC_hs_cuda_kernel_forward_param_13];
	ld.param.f64 	%fd29, [val_IPC_hs_cuda_kernel_forward_param_14];
	ld.param.f64 	%fd30, [val_IPC_hs_cuda_kernel_forward_param_15];
	ld.param.f64 	%fd31, [val_IPC_hs_cuda_kernel_forward_param_16];
	ld.param.f64 	%fd32, [val_IPC_hs_cuda_kernel_forward_param_17];
	ld.param.u64 	%rd70, [val_IPC_hs_cuda_kernel_forward_param_12];
	ld.param.u64 	%rd68, [val_IPC_hs_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd66, [val_IPC_hs_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd64, [val_IPC_hs_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd62, [val_IPC_hs_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd60, [val_IPC_hs_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd58, [val_IPC_hs_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd56, [val_IPC_hs_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd54, [val_IPC_hs_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd52, [val_IPC_hs_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd50, [val_IPC_hs_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd48, [val_IPC_hs_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd47, [val_IPC_hs_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r16, [val_IPC_hs_cuda_kernel_forward_param_0+16];
	mov.u32 	%r226, %ntid.x;
	cvt.u64.u32 	%rd1, %r226;
	mov.u32 	%r227, %ctaid.x;
	mul.wide.u32 	%rd72, %r226, %r227;
	mov.u32 	%r228, %tid.x;
	cvt.u64.u32 	%rd73, %r228;
	add.s64 	%rd109, %rd72, %rd73;
	setp.ge.u64 	%p1, %rd109, %rd47;
	@%p1 bra 	$L__BB4_29;

	cvta.to.global.u64 	%rd5, %rd70;
	cvta.to.global.u64 	%rd6, %rd68;
	cvta.to.global.u64 	%rd7, %rd66;
	cvta.to.global.u64 	%rd8, %rd64;
	cvta.to.global.u64 	%rd9, %rd62;
	cvta.to.global.u64 	%rd10, %rd60;
	cvta.to.global.u64 	%rd11, %rd58;
	cvta.to.global.u64 	%rd12, %rd56;
	cvta.to.global.u64 	%rd13, %rd54;
	cvta.to.global.u64 	%rd14, %rd52;
	cvta.to.global.u64 	%rd15, %rd50;
	cvt.s64.s32 	%rd16, %r129;
	cvt.s64.s32 	%rd17, %r128;
	cvt.s64.s32 	%rd18, %r127;
	cvt.s64.s32 	%rd19, %r150;
	cvt.s64.s32 	%rd20, %r190;
	cvt.s64.s32 	%rd21, %r174;
	cvt.s64.s32 	%rd22, %r158;
	cvt.s64.s32 	%rd23, %r206;
	cvt.s64.s32 	%rd24, %r214;
	cvt.s64.s32 	%rd25, %r166;
	cvt.s64.s32 	%rd26, %r182;
	cvt.s64.s32 	%rd27, %r198;
	mov.u32 	%r229, %nctaid.x;
	cvt.u64.u32 	%rd74, %r229;
	mul.lo.s64 	%rd28, %rd1, %rd74;
	cvt.s64.s32 	%rd29, %r222;
	cvt.s64.s32 	%rd30, %r142;
	mov.f64 	%fd33, 0d0000000000000000;
	sub.f64 	%fd1, %fd33, %fd30;
	cvt.s64.s32 	%rd31, %r134;
	mul.f64 	%fd2, %fd29, %fd32;
	mul.f64 	%fd3, %fd2, %fd2;
	div.rn.f64 	%fd4, %fd2, 0d4008000000000000;

$L__BB4_2:
	setp.lt.s32 	%p2, %r16, 4;
	mov.u64 	%rd110, %rd109;
	@%p2 bra 	$L__BB4_6;

	or.b64  	%rd75, %rd109, %rd16;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p3, %rd76, 0;
	@%p3 bra 	$L__BB4_5;

	div.u64 	%rd110, %rd109, %rd16;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r230, %rd16;
	cvt.u32.u64 	%r231, %rd109;
	div.u32 	%r232, %r231, %r230;
	cvt.u64.u32 	%rd110, %r232;

$L__BB4_6:
	setp.lt.s32 	%p4, %r16, 3;
	@%p4 bra 	$L__BB4_10;

	or.b64  	%rd77, %rd110, %rd17;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p5, %rd78, 0;
	@%p5 bra 	$L__BB4_9;

	div.u64 	%rd110, %rd110, %rd17;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r233, %rd17;
	cvt.u32.u64 	%r234, %rd110;
	div.u32 	%r235, %r234, %r233;
	cvt.u64.u32 	%rd110, %r235;

$L__BB4_10:
	setp.lt.s32 	%p6, %r16, 2;
	@%p6 bra 	$L__BB4_14;

	or.b64  	%rd79, %rd110, %rd18;
	and.b64  	%rd80, %rd79, -4294967296;
	setp.eq.s64 	%p7, %rd80, 0;
	@%p7 bra 	$L__BB4_13;

	div.u64 	%rd110, %rd110, %rd18;
	bra.uni 	$L__BB4_14;

$L__BB4_13:
	cvt.u32.u64 	%r236, %rd18;
	cvt.u32.u64 	%r237, %rd110;
	div.u32 	%r238, %r237, %r236;
	cvt.u64.u32 	%rd110, %r238;

$L__BB4_14:
	ld.param.u32 	%r252, [val_IPC_hs_cuda_kernel_forward_param_18];
	cvt.s64.s32 	%rd81, %rd110;
	setp.gt.s32 	%p8, %r16, 0;
	selp.b64 	%rd42, %rd81, 0, %p8;
	mul.lo.s64 	%rd82, %rd42, %rd19;
	add.s64 	%rd83, %rd14, %rd82;
	ld.global.s32 	%rd43, [%rd83];
	mul.lo.s64 	%rd84, %rd43, %rd20;
	add.s64 	%rd85, %rd9, %rd84;
	mul.lo.s64 	%rd86, %rd43, %rd21;
	add.s64 	%rd87, %rd11, %rd86;
	mul.lo.s64 	%rd88, %rd42, %rd22;
	add.s64 	%rd89, %rd13, %rd88;
	ld.global.s32 	%rd44, [%rd89];
	mul.lo.s64 	%rd90, %rd44, %rd23;
	add.s64 	%rd91, %rd7, %rd90;
	mul.lo.s64 	%rd92, %rd44, %rd24;
	add.s64 	%rd93, %rd6, %rd92;
	ld.global.s32 	%rd45, [%rd87];
	mul.lo.s64 	%rd94, %rd45, %rd25;
	add.s64 	%rd95, %rd12, %rd94;
	ld.global.f64 	%fd5, [%rd95];
	ld.global.f64 	%fd34, [%rd93];
	sub.f64 	%fd35, %fd5, %fd34;
	ld.global.f64 	%fd6, [%rd95+8];
	ld.global.f64 	%fd36, [%rd93+8];
	sub.f64 	%fd37, %fd6, %fd36;
	ld.global.f64 	%fd7, [%rd95+16];
	ld.global.f64 	%fd38, [%rd93+16];
	sub.f64 	%fd39, %fd7, %fd38;
	ld.global.f64 	%fd8, [%rd91];
	ld.global.f64 	%fd9, [%rd91+8];
	mul.f64 	%fd40, %fd9, %fd37;
	fma.rn.f64 	%fd41, %fd8, %fd35, %fd40;
	ld.global.f64 	%fd10, [%rd91+16];
	fma.rn.f64 	%fd42, %fd10, %fd39, %fd41;
	ld.global.f64 	%fd43, [%rd85];
	sub.f64 	%fd11, %fd42, %fd43;
	setp.eq.s32 	%p9, %r252, 0;
	@%p9 bra 	$L__BB4_18;

	ld.param.f64 	%fd149, [val_IPC_hs_cuda_kernel_forward_param_17];
	ld.param.f64 	%fd148, [val_IPC_hs_cuda_kernel_forward_param_14];
	mul.lo.s64 	%rd96, %rd45, %rd26;
	add.s64 	%rd97, %rd10, %rd96;
	mul.lo.s64 	%rd98, %rd44, %rd29;
	add.s64 	%rd99, %rd5, %rd98;
	mul.lo.s64 	%rd100, %rd42, %rd30;
	add.s64 	%rd101, %rd15, %rd100;
	ld.global.f64 	%fd44, [%rd101];
	ld.global.f64 	%fd45, [%rd99];
	mul.f64 	%fd12, %fd45, %fd44;
	mul.f64 	%fd46, %fd8, %fd8;
	mov.f64 	%fd47, 0d3FF0000000000000;
	sub.f64 	%fd48, %fd47, %fd46;
	mul.f64 	%fd49, %fd8, %fd9;
	mov.f64 	%fd50, 0d0000000000000000;
	sub.f64 	%fd51, %fd50, %fd49;
	mul.f64 	%fd52, %fd8, %fd10;
	sub.f64 	%fd53, %fd50, %fd52;
	mul.f64 	%fd54, %fd9, %fd9;
	sub.f64 	%fd55, %fd47, %fd54;
	mul.f64 	%fd56, %fd9, %fd10;
	sub.f64 	%fd57, %fd50, %fd56;
	mul.f64 	%fd58, %fd10, %fd10;
	sub.f64 	%fd59, %fd47, %fd58;
	ld.global.f64 	%fd60, [%rd97];
	sub.f64 	%fd61, %fd5, %fd60;
	ld.global.f64 	%fd62, [%rd97+8];
	sub.f64 	%fd63, %fd6, %fd62;
	ld.global.f64 	%fd64, [%rd97+16];
	sub.f64 	%fd65, %fd7, %fd64;
	mul.f64 	%fd66, %fd51, %fd63;
	mul.f64 	%fd67, %fd55, %fd63;
	mul.f64 	%fd68, %fd57, %fd63;
	fma.rn.f64 	%fd69, %fd48, %fd61, %fd66;
	fma.rn.f64 	%fd70, %fd51, %fd61, %fd67;
	fma.rn.f64 	%fd71, %fd53, %fd61, %fd68;
	fma.rn.f64 	%fd72, %fd53, %fd65, %fd69;
	fma.rn.f64 	%fd73, %fd57, %fd65, %fd70;
	fma.rn.f64 	%fd74, %fd59, %fd65, %fd71;
	div.rn.f64 	%fd75, %fd72, %fd148;
	div.rn.f64 	%fd76, %fd73, %fd148;
	div.rn.f64 	%fd77, %fd74, %fd148;
	mul.f64 	%fd78, %fd76, %fd76;
	fma.rn.f64 	%fd79, %fd75, %fd75, %fd78;
	fma.rn.f64 	%fd80, %fd77, %fd77, %fd79;
	sqrt.rn.f64 	%fd81, %fd80;
	setp.ge.f64 	%p10, %fd81, %fd149;
	mul.f64 	%fd150, %fd81, %fd148;
	@%p10 bra 	$L__BB4_17;

	mul.f64 	%fd82, %fd150, %fd150;
	sub.f64 	%fd84, %fd50, %fd150;
	div.rn.f64 	%fd85, %fd84, 0d4008000000000000;
	add.f64 	%fd86, %fd2, %fd85;
	mul.f64 	%fd87, %fd82, %fd86;
	div.rn.f64 	%fd88, %fd87, %fd3;
	add.f64 	%fd150, %fd4, %fd88;

$L__BB4_17:
	mul.lo.s64 	%rd103, %rd45, %rd31;
	add.s64 	%rd102, %rd48, %rd103;
	mul.f64 	%fd91, %fd12, %fd31;
	mul.f64 	%fd90, %fd91, %fd150;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd102],%fd90; }

	// end inline asm
	bra.uni 	$L__BB4_28;

$L__BB4_18:
	mul.lo.s64 	%rd104, %rd43, %rd27;
	add.s64 	%rd105, %rd8, %rd104;
	ld.global.f64 	%fd16, [%rd105];
	setp.geu.f64 	%p11, %fd11, %fd28;
	@%p11 bra 	$L__BB4_27;

	div.rn.f64 	%fd151, %fd11, %fd28;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r253}, %fd151;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r254, %temp}, %fd151;
	}
	setp.gt.s32 	%p12, %r253, 1048575;
	mov.u32 	%r255, -1023;
	@%p12 bra 	$L__BB4_21;

	mul.f64 	%fd151, %fd151, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r253}, %fd151;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r254, %temp}, %fd151;
	}
	mov.u32 	%r255, -1077;

$L__BB4_21:
	add.s32 	%r241, %r253, -1;
	setp.lt.u32 	%p13, %r241, 2146435071;
	@%p13 bra 	$L__BB4_23;
	bra.uni 	$L__BB4_22;

$L__BB4_23:
	shr.u32 	%r243, %r253, 20;
	add.s32 	%r256, %r255, %r243;
	and.b32  	%r244, %r253, -2146435073;
	or.b32  	%r245, %r244, 1072693248;
	mov.b64 	%fd152, {%r254, %r245};
	setp.lt.s32 	%p15, %r245, 1073127583;
	@%p15 bra 	$L__BB4_25;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r246, %temp}, %fd152;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r247}, %fd152;
	}
	add.s32 	%r248, %r247, -1048576;
	mov.b64 	%fd152, {%r246, %r248};
	add.s32 	%r256, %r256, 1;

$L__BB4_25:
	add.f64 	%fd95, %fd152, 0d3FF0000000000000;
	mov.f64 	%fd96, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd97, %fd95;
	neg.f64 	%fd98, %fd95;
	fma.rn.f64 	%fd99, %fd98, %fd97, %fd96;
	fma.rn.f64 	%fd100, %fd99, %fd99, %fd99;
	fma.rn.f64 	%fd101, %fd100, %fd97, %fd97;
	add.f64 	%fd102, %fd152, 0dBFF0000000000000;
	mul.f64 	%fd103, %fd102, %fd101;
	fma.rn.f64 	%fd104, %fd102, %fd101, %fd103;
	mul.f64 	%fd105, %fd104, %fd104;
	mov.f64 	%fd106, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd107, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd108, %fd107, %fd105, %fd106;
	mov.f64 	%fd109, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd110, %fd108, %fd105, %fd109;
	mov.f64 	%fd111, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd112, %fd110, %fd105, %fd111;
	mov.f64 	%fd113, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd114, %fd112, %fd105, %fd113;
	mov.f64 	%fd115, 0d3F624924923BE72D;
	fma.rn.f64 	%fd116, %fd114, %fd105, %fd115;
	mov.f64 	%fd117, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd118, %fd116, %fd105, %fd117;
	mov.f64 	%fd119, 0d3FB5555555555554;
	fma.rn.f64 	%fd120, %fd118, %fd105, %fd119;
	sub.f64 	%fd121, %fd102, %fd104;
	add.f64 	%fd122, %fd121, %fd121;
	neg.f64 	%fd123, %fd104;
	fma.rn.f64 	%fd124, %fd123, %fd102, %fd122;
	mul.f64 	%fd125, %fd101, %fd124;
	mul.f64 	%fd126, %fd105, %fd120;
	fma.rn.f64 	%fd127, %fd126, %fd104, %fd125;
	xor.b32  	%r249, %r256, -2147483648;
	mov.u32 	%r250, -2147483648;
	mov.u32 	%r251, 1127219200;
	mov.b64 	%fd128, {%r249, %r251};
	mov.b64 	%fd129, {%r250, %r251};
	sub.f64 	%fd130, %fd128, %fd129;
	mov.f64 	%fd131, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd132, %fd130, %fd131, %fd104;
	neg.f64 	%fd133, %fd130;
	fma.rn.f64 	%fd134, %fd133, %fd131, %fd132;
	sub.f64 	%fd135, %fd134, %fd104;
	sub.f64 	%fd136, %fd127, %fd135;
	mov.f64 	%fd137, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd138, %fd130, %fd137, %fd136;
	add.f64 	%fd153, %fd132, %fd138;
	bra.uni 	$L__BB4_26;

$L__BB4_22:
	mov.f64 	%fd93, 0d7FF0000000000000;
	fma.rn.f64 	%fd94, %fd151, %fd93, %fd93;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r242}, %fd151;
	}
	mov.b32 	%f1, %r242;
	setp.eq.f32 	%p14, %f1, 0f00000000;
	selp.f64 	%fd153, 0dFFF0000000000000, %fd94, %p14;

$L__BB4_26:
	sub.f64 	%fd139, %fd11, %fd28;
	div.rn.f64 	%fd140, %fd139, %fd28;
	mul.f64 	%fd141, %fd1, %fd140;
	mul.f64 	%fd142, %fd140, %fd141;
	mul.f64 	%fd154, %fd142, %fd153;

$L__BB4_27:
	setp.lt.f64 	%p16, %fd11, %fd28;
	selp.f64 	%fd145, %fd154, 0d0000000000000000, %p16;
	mul.f64 	%fd146, %fd16, %fd28;
	mul.f64 	%fd147, %fd146, %fd145;
	mul.f64 	%fd144, %fd147, %fd31;
	mul.lo.s64 	%rd107, %rd45, %rd31;
	add.s64 	%rd106, %rd48, %rd107;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd106],%fd144; }

	// end inline asm

$L__BB4_28:
	ld.param.u64 	%rd108, [val_IPC_hs_cuda_kernel_forward_param_0+24];
	add.s64 	%rd109, %rd109, %rd28;
	setp.lt.u64 	%p17, %rd109, %rd108;
	@%p17 bra 	$L__BB4_2;

$L__BB4_29:
	ret;

}
	// .globl	val_IPC_hs_cuda_kernel_backward
.visible .entry val_IPC_hs_cuda_kernel_backward(
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_12[56],
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_13,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_14,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_15,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_16,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_17,
	.param .u32 val_IPC_hs_cuda_kernel_backward_param_18,
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_21[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_22[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_23[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_24[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_25[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_26[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_27[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_28[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_29[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_30[56],
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_31,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_32,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_33,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_34,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_35,
	.param .u32 val_IPC_hs_cuda_kernel_backward_param_36
)
{
	.reg .pred 	%p<51>;
	.reg .b16 	%rs<169>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<436>;
	.reg .f64 	%fd<526>;
	.reg .b64 	%rd<214>;


	ld.param.v2.u32 	{%r217, %r218}, [val_IPC_hs_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r219, %r220}, [val_IPC_hs_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r225, %r226}, [val_IPC_hs_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r233, %r234}, [val_IPC_hs_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r241, %r242}, [val_IPC_hs_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r249, %r250}, [val_IPC_hs_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r257, %r258}, [val_IPC_hs_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r265, %r266}, [val_IPC_hs_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r273, %r274}, [val_IPC_hs_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r281, %r282}, [val_IPC_hs_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r289, %r290}, [val_IPC_hs_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r297, %r298}, [val_IPC_hs_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r305, %r306}, [val_IPC_hs_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r313, %r314}, [val_IPC_hs_cuda_kernel_backward_param_12+32];
	ld.param.f64 	%fd146, [val_IPC_hs_cuda_kernel_backward_param_13];
	ld.param.f64 	%fd147, [val_IPC_hs_cuda_kernel_backward_param_14];
	ld.param.f64 	%fd148, [val_IPC_hs_cuda_kernel_backward_param_15];
	ld.param.f64 	%fd149, [val_IPC_hs_cuda_kernel_backward_param_16];
	ld.param.f64 	%fd150, [val_IPC_hs_cuda_kernel_backward_param_17];
	ld.param.u32 	%r135, [val_IPC_hs_cuda_kernel_backward_param_18];
	ld.param.v2.u32 	{%r321, %r322}, [val_IPC_hs_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r329, %r330}, [val_IPC_hs_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r337, %r338}, [val_IPC_hs_cuda_kernel_backward_param_23+32];
	ld.param.v2.u32 	{%r345, %r346}, [val_IPC_hs_cuda_kernel_backward_param_25+32];
	ld.param.v2.u32 	{%r353, %r354}, [val_IPC_hs_cuda_kernel_backward_param_26+32];
	ld.param.v2.u32 	{%r361, %r362}, [val_IPC_hs_cuda_kernel_backward_param_27+32];
	ld.param.v2.u32 	{%r369, %r370}, [val_IPC_hs_cuda_kernel_backward_param_28+32];
	ld.param.v2.u32 	{%r377, %r378}, [val_IPC_hs_cuda_kernel_backward_param_29+32];
	ld.param.v2.u32 	{%r385, %r386}, [val_IPC_hs_cuda_kernel_backward_param_30+32];
	ld.param.u64 	%rd123, [val_IPC_hs_cuda_kernel_backward_param_30];
	ld.param.u64 	%rd121, [val_IPC_hs_cuda_kernel_backward_param_29];
	ld.param.u64 	%rd119, [val_IPC_hs_cuda_kernel_backward_param_28];
	ld.param.u64 	%rd117, [val_IPC_hs_cuda_kernel_backward_param_27];
	ld.param.u64 	%rd115, [val_IPC_hs_cuda_kernel_backward_param_26];
	ld.param.u64 	%rd113, [val_IPC_hs_cuda_kernel_backward_param_25];
	ld.param.u64 	%rd111, [val_IPC_hs_cuda_kernel_backward_param_23];
	ld.param.u64 	%rd109, [val_IPC_hs_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd107, [val_IPC_hs_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd106, [val_IPC_hs_cuda_kernel_backward_param_12+8];
	ld.param.u64 	%rd105, [val_IPC_hs_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd104, [val_IPC_hs_cuda_kernel_backward_param_11+8];
	ld.param.u64 	%rd103, [val_IPC_hs_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd102, [val_IPC_hs_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd101, [val_IPC_hs_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd100, [val_IPC_hs_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd99, [val_IPC_hs_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd98, [val_IPC_hs_cuda_kernel_backward_param_8+8];
	ld.param.u64 	%rd97, [val_IPC_hs_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd96, [val_IPC_hs_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd95, [val_IPC_hs_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd93, [val_IPC_hs_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd92, [val_IPC_hs_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd91, [val_IPC_hs_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd89, [val_IPC_hs_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd87, [val_IPC_hs_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd86, [val_IPC_hs_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd85, [val_IPC_hs_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd84, [val_IPC_hs_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd82, [val_IPC_hs_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r26, [val_IPC_hs_cuda_kernel_backward_param_0+16];
	mov.u32 	%r389, %ntid.x;
	cvt.u64.u32 	%rd1, %r389;
	mov.u32 	%r390, %ctaid.x;
	mul.wide.u32 	%rd125, %r389, %r390;
	mov.u32 	%r391, %tid.x;
	cvt.u64.u32 	%rd126, %r391;
	add.s64 	%rd210, %rd125, %rd126;
	setp.ge.u64 	%p1, %rd210, %rd82;
	@%p1 bra 	$L__BB5_86;

	cvta.to.global.u64 	%rd24, %rd105;
	cvta.to.global.u64 	%rd25, %rd103;
	cvta.to.global.u64 	%rd26, %rd101;
	cvta.to.global.u64 	%rd27, %rd99;
	cvta.to.global.u64 	%rd28, %rd97;
	cvta.to.global.u64 	%rd29, %rd95;
	cvta.to.global.u64 	%rd30, %rd93;
	cvta.to.global.u64 	%rd31, %rd91;
	cvta.to.global.u64 	%rd32, %rd89;
	cvta.to.global.u64 	%rd33, %rd87;
	cvta.to.global.u64 	%rd34, %rd85;
	cvt.s64.s32 	%rd35, %r220;
	cvt.s64.s32 	%rd36, %r219;
	cvt.s64.s32 	%rd37, %r218;
	cvt.s64.s32 	%rd38, %r241;
	cvt.s64.s32 	%rd39, %r281;
	cvt.s64.s32 	%rd40, %r265;
	cvt.s64.s32 	%rd41, %r249;
	cvt.s64.s32 	%rd42, %r297;
	cvt.s64.s32 	%rd43, %r305;
	cvt.s64.s32 	%rd44, %r257;
	cvt.s64.s32 	%rd45, %r273;
	cvt.s64.s32 	%rd46, %r289;
	mov.f64 	%fd152, 0d0000000000000000;
	sub.f64 	%fd1, %fd152, %fd148;
	cvt.s64.s32 	%rd47, %r313;
	cvt.s64.s32 	%rd48, %r233;
	cvt.s64.s32 	%rd49, %r321;
	cvt.s64.s32 	%rd50, %r345;
	cvt.s64.s32 	%rd51, %r225;
	cvt.s64.s32 	%rd52, %r337;
	cvt.s64.s32 	%rd53, %r377;
	cvt.s64.s32 	%rd54, %r361;
	cvt.s64.s32 	%rd55, %r369;
	mov.u32 	%r392, %nctaid.x;
	cvt.u64.u32 	%rd127, %r392;
	mul.lo.s64 	%rd56, %rd1, %rd127;
	cvt.s64.s32 	%rd57, %r353;
	mul.f64 	%fd2, %fd147, %fd150;
	mul.f64 	%fd3, %fd2, %fd2;
	div.rn.f64 	%fd4, %fd2, 0d4008000000000000;
	cvt.s64.s32 	%rd58, %r329;
	cvt.s64.s32 	%rd59, %r385;

$L__BB5_2:
	setp.lt.s32 	%p2, %r26, 4;
	mov.u64 	%rd211, %rd210;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd128, %rd210, %rd35;
	and.b64  	%rd129, %rd128, -4294967296;
	setp.eq.s64 	%p3, %rd129, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd211, %rd210, %rd35;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r393, %rd35;
	cvt.u32.u64 	%r394, %rd210;
	div.u32 	%r395, %r394, %r393;
	cvt.u64.u32 	%rd211, %r395;

$L__BB5_6:
	setp.lt.s32 	%p4, %r26, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd130, %rd211, %rd36;
	and.b64  	%rd131, %rd130, -4294967296;
	setp.eq.s64 	%p5, %rd131, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd211, %rd211, %rd36;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r396, %rd36;
	cvt.u32.u64 	%r397, %rd211;
	div.u32 	%r398, %r397, %r396;
	cvt.u64.u32 	%rd211, %r398;

$L__BB5_10:
	setp.lt.s32 	%p6, %r26, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd132, %rd211, %rd37;
	and.b64  	%rd133, %rd132, -4294967296;
	setp.eq.s64 	%p7, %rd133, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd211, %rd211, %rd37;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r399, %rd37;
	cvt.u32.u64 	%r400, %rd211;
	div.u32 	%r401, %r400, %r399;
	cvt.u64.u32 	%rd211, %r401;

$L__BB5_14:
	cvt.s64.s32 	%rd202, %r281;
	cvt.s64.s32 	%rd201, %r297;
	cvt.s64.s32 	%rd200, %r305;
	cvt.s64.s32 	%rd199, %r257;
	cvt.s64.s32 	%rd198, %r273;
	cvt.s64.s32 	%rd134, %rd211;
	setp.gt.s32 	%p8, %r26, 0;
	selp.b64 	%rd70, %rd134, 0, %p8;
	mul.lo.s64 	%rd135, %rd70, %rd38;
	add.s64 	%rd136, %rd33, %rd135;
	ld.global.s32 	%rd71, [%rd136];
	mul.lo.s64 	%rd72, %rd71, %rd202;
	add.s64 	%rd137, %rd28, %rd72;
	mul.lo.s64 	%rd138, %rd71, %rd40;
	add.s64 	%rd139, %rd30, %rd138;
	mul.lo.s64 	%rd140, %rd70, %rd41;
	add.s64 	%rd141, %rd32, %rd140;
	ld.global.s32 	%rd73, [%rd141];
	mul.lo.s64 	%rd74, %rd73, %rd201;
	add.s64 	%rd142, %rd26, %rd74;
	mul.lo.s64 	%rd75, %rd73, %rd200;
	add.s64 	%rd143, %rd25, %rd75;
	ld.global.s32 	%rd76, [%rd139];
	mul.lo.s64 	%rd77, %rd76, %rd199;
	add.s64 	%rd144, %rd31, %rd77;
	mul.lo.s64 	%rd78, %rd76, %rd198;
	add.s64 	%rd145, %rd29, %rd78;
	ld.global.f64 	%fd10, [%rd145];
	ld.global.f64 	%fd11, [%rd145+8];
	ld.global.f64 	%fd12, [%rd145+16];
	ld.global.f64 	%fd13, [%rd144];
	ld.global.f64 	%fd153, [%rd143];
	sub.f64 	%fd14, %fd13, %fd153;
	ld.global.f64 	%fd15, [%rd144+8];
	ld.global.f64 	%fd154, [%rd143+8];
	sub.f64 	%fd16, %fd15, %fd154;
	ld.global.f64 	%fd17, [%rd144+16];
	ld.global.f64 	%fd155, [%rd143+16];
	sub.f64 	%fd18, %fd17, %fd155;
	ld.global.f64 	%fd19, [%rd142];
	ld.global.f64 	%fd20, [%rd142+8];
	mul.f64 	%fd156, %fd20, %fd16;
	fma.rn.f64 	%fd157, %fd19, %fd14, %fd156;
	ld.global.f64 	%fd21, [%rd142+16];
	fma.rn.f64 	%fd158, %fd21, %fd18, %fd157;
	ld.global.f64 	%fd159, [%rd137];
	sub.f64 	%fd22, %fd158, %fd159;
	setp.ne.s32 	%p9, %r135, 0;
	@%p9 bra 	$L__BB5_25;

	mul.lo.s64 	%rd146, %rd71, %rd46;
	add.s64 	%rd147, %rd27, %rd146;
	ld.global.f64 	%fd23, [%rd147];
	setp.geu.f64 	%p10, %fd22, %fd146;
	@%p10 bra 	$L__BB5_24;

	div.rn.f64 	%fd492, %fd22, %fd146;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r428}, %fd492;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r429, %temp}, %fd492;
	}
	setp.gt.s32 	%p11, %r428, 1048575;
	mov.u32 	%r430, -1023;
	@%p11 bra 	$L__BB5_18;

	mul.f64 	%fd492, %fd492, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r428}, %fd492;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r429, %temp}, %fd492;
	}
	mov.u32 	%r430, -1077;

$L__BB5_18:
	add.s32 	%r404, %r428, -1;
	setp.lt.u32 	%p12, %r404, 2146435071;
	@%p12 bra 	$L__BB5_20;
	bra.uni 	$L__BB5_19;

$L__BB5_20:
	shr.u32 	%r406, %r428, 20;
	add.s32 	%r431, %r430, %r406;
	and.b32  	%r407, %r428, -2146435073;
	or.b32  	%r408, %r407, 1072693248;
	mov.b64 	%fd493, {%r429, %r408};
	setp.lt.s32 	%p14, %r408, 1073127583;
	@%p14 bra 	$L__BB5_22;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r409, %temp}, %fd493;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r410}, %fd493;
	}
	add.s32 	%r411, %r410, -1048576;
	mov.b64 	%fd493, {%r409, %r411};
	add.s32 	%r431, %r431, 1;

$L__BB5_22:
	add.f64 	%fd163, %fd493, 0d3FF0000000000000;
	mov.f64 	%fd164, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd165, %fd163;
	neg.f64 	%fd166, %fd163;
	fma.rn.f64 	%fd167, %fd166, %fd165, %fd164;
	fma.rn.f64 	%fd168, %fd167, %fd167, %fd167;
	fma.rn.f64 	%fd169, %fd168, %fd165, %fd165;
	add.f64 	%fd170, %fd493, 0dBFF0000000000000;
	mul.f64 	%fd171, %fd170, %fd169;
	fma.rn.f64 	%fd172, %fd170, %fd169, %fd171;
	mul.f64 	%fd173, %fd172, %fd172;
	mov.f64 	%fd174, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd175, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd176, %fd175, %fd173, %fd174;
	mov.f64 	%fd177, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd178, %fd176, %fd173, %fd177;
	mov.f64 	%fd179, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd180, %fd178, %fd173, %fd179;
	mov.f64 	%fd181, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd182, %fd180, %fd173, %fd181;
	mov.f64 	%fd183, 0d3F624924923BE72D;
	fma.rn.f64 	%fd184, %fd182, %fd173, %fd183;
	mov.f64 	%fd185, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd186, %fd184, %fd173, %fd185;
	mov.f64 	%fd187, 0d3FB5555555555554;
	fma.rn.f64 	%fd188, %fd186, %fd173, %fd187;
	sub.f64 	%fd189, %fd170, %fd172;
	add.f64 	%fd190, %fd189, %fd189;
	neg.f64 	%fd191, %fd172;
	fma.rn.f64 	%fd192, %fd191, %fd170, %fd190;
	mul.f64 	%fd193, %fd169, %fd192;
	mul.f64 	%fd194, %fd173, %fd188;
	fma.rn.f64 	%fd195, %fd194, %fd172, %fd193;
	xor.b32  	%r412, %r431, -2147483648;
	mov.u32 	%r413, -2147483648;
	mov.u32 	%r414, 1127219200;
	mov.b64 	%fd196, {%r412, %r414};
	mov.b64 	%fd197, {%r413, %r414};
	sub.f64 	%fd198, %fd196, %fd197;
	mov.f64 	%fd199, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd200, %fd198, %fd199, %fd172;
	neg.f64 	%fd201, %fd198;
	fma.rn.f64 	%fd202, %fd201, %fd199, %fd200;
	sub.f64 	%fd203, %fd202, %fd172;
	sub.f64 	%fd204, %fd195, %fd203;
	mov.f64 	%fd205, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd206, %fd198, %fd205, %fd204;
	add.f64 	%fd494, %fd200, %fd206;
	bra.uni 	$L__BB5_23;

$L__BB5_19:
	mov.f64 	%fd161, 0d7FF0000000000000;
	fma.rn.f64 	%fd162, %fd492, %fd161, %fd161;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r405}, %fd492;
	}
	mov.b32 	%f1, %r405;
	setp.eq.f32 	%p13, %f1, 0f00000000;
	selp.f64 	%fd494, 0dFFF0000000000000, %fd162, %p13;

$L__BB5_23:
	sub.f64 	%fd207, %fd22, %fd146;
	div.rn.f64 	%fd208, %fd207, %fd146;
	mul.f64 	%fd209, %fd1, %fd208;
	mul.f64 	%fd210, %fd208, %fd209;
	mul.f64 	%fd495, %fd210, %fd494;

$L__BB5_24:
	setp.lt.f64 	%p15, %fd22, %fd146;
	selp.f64 	%fd497, %fd495, 0d0000000000000000, %p15;
	mul.f64 	%fd496, %fd23, %fd146;

$L__BB5_25:
	setp.eq.s32 	%p16, %r135, 0;
	@%p16 bra 	$L__BB5_27;

	mul.lo.s64 	%rd148, %rd73, %rd47;
	add.s64 	%rd149, %rd24, %rd148;
	mul.lo.s64 	%rd150, %rd70, %rd48;
	add.s64 	%rd151, %rd34, %rd150;
	ld.global.f64 	%fd499, [%rd151];
	ld.global.f64 	%fd498, [%rd149];
	mul.f64 	%fd211, %fd498, %fd499;
	mul.f64 	%fd500, %fd211, %fd149;

$L__BB5_27:
	cvta.to.global.u64 	%rd204, %rd84;
	cvta.to.global.u64 	%rd203, %rd107;
	mul.lo.s64 	%rd152, %rd76, %rd49;
	add.s64 	%rd79, %rd203, %rd152;
	mul.lo.s64 	%rd153, %rd76, %rd51;
	add.s64 	%rd80, %rd204, %rd153;
	mov.f64 	%fd525, 0d0000000000000000;
	mov.f64 	%fd507, %fd525;
	mov.f64 	%fd508, %fd525;
	mov.f64 	%fd509, %fd525;
	mov.f64 	%fd510, %fd525;
	mov.f64 	%fd511, %fd525;
	mov.f64 	%fd512, %fd525;
	mov.f64 	%fd513, %fd525;
	mov.f64 	%fd514, %fd525;
	mov.f64 	%fd515, %fd525;
	@%p16 bra 	$L__BB5_46;

	setp.eq.s64 	%p18, %rd107, 0;
	@%p18 bra 	$L__BB5_30;

	ld.global.f64 	%fd221, [%rd79];
	add.f64 	%fd501, %fd221, 0d0000000000000000;
	bra.uni 	$L__BB5_32;

$L__BB5_30:
	setp.eq.s64 	%p19, %rd84, 0;
	mov.f64 	%fd501, 0d0000000000000000;
	@%p19 bra 	$L__BB5_32;

	ld.global.f64 	%fd223, [%rd80];
	add.f64 	%fd501, %fd223, 0d0000000000000000;

$L__BB5_32:
	ld.param.f64 	%fd475, [val_IPC_hs_cuda_kernel_backward_param_17];
	mul.f64 	%fd224, %fd19, %fd19;
	mov.f64 	%fd225, 0d3FF0000000000000;
	sub.f64 	%fd48, %fd225, %fd224;
	mul.f64 	%fd226, %fd19, %fd20;
	mov.f64 	%fd227, 0d0000000000000000;
	sub.f64 	%fd49, %fd227, %fd226;
	mul.f64 	%fd228, %fd19, %fd21;
	sub.f64 	%fd50, %fd227, %fd228;
	mul.f64 	%fd229, %fd20, %fd20;
	sub.f64 	%fd51, %fd225, %fd229;
	mul.f64 	%fd230, %fd20, %fd21;
	sub.f64 	%fd52, %fd227, %fd230;
	mul.f64 	%fd231, %fd21, %fd21;
	sub.f64 	%fd53, %fd225, %fd231;
	sub.f64 	%fd54, %fd13, %fd10;
	sub.f64 	%fd55, %fd15, %fd11;
	mul.f64 	%fd232, %fd49, %fd55;
	mul.f64 	%fd233, %fd51, %fd55;
	mul.f64 	%fd234, %fd52, %fd55;
	fma.rn.f64 	%fd235, %fd48, %fd54, %fd232;
	fma.rn.f64 	%fd236, %fd49, %fd54, %fd233;
	fma.rn.f64 	%fd237, %fd50, %fd54, %fd234;
	sub.f64 	%fd56, %fd17, %fd12;
	fma.rn.f64 	%fd238, %fd50, %fd56, %fd235;
	fma.rn.f64 	%fd239, %fd52, %fd56, %fd236;
	fma.rn.f64 	%fd240, %fd53, %fd56, %fd237;
	div.rn.f64 	%fd57, %fd238, %fd147;
	div.rn.f64 	%fd58, %fd239, %fd147;
	div.rn.f64 	%fd59, %fd240, %fd147;
	mul.f64 	%fd241, %fd58, %fd58;
	fma.rn.f64 	%fd242, %fd57, %fd57, %fd241;
	fma.rn.f64 	%fd243, %fd59, %fd59, %fd242;
	sqrt.rn.f64 	%fd60, %fd243;
	setp.ge.f64 	%p20, %fd60, %fd475;
	mul.f64 	%fd502, %fd60, %fd147;
	@%p20 bra 	$L__BB5_34;

	mov.f64 	%fd482, 0d0000000000000000;
	mul.f64 	%fd477, %fd60, %fd147;
	mul.f64 	%fd244, %fd477, %fd477;
	sub.f64 	%fd246, %fd482, %fd477;
	div.rn.f64 	%fd247, %fd246, 0d4008000000000000;
	add.f64 	%fd248, %fd2, %fd247;
	mul.f64 	%fd249, %fd244, %fd248;
	div.rn.f64 	%fd250, %fd249, %fd3;
	add.f64 	%fd502, %fd4, %fd250;

$L__BB5_34:
	add.f64 	%fd251, %fd501, 0d0000000000000000;
	fma.rn.f64 	%fd64, %fd251, %fd502, 0d0000000000000000;
	fma.rn.f64 	%fd503, %fd500, %fd251, 0d0000000000000000;
	@%p20 bra 	$L__BB5_36;

	mul.f64 	%fd476, %fd60, %fd147;
	mul.f64 	%fd252, %fd476, %fd476;
	mov.f64 	%fd253, 0d0000000000000000;
	sub.f64 	%fd254, %fd253, %fd476;
	div.rn.f64 	%fd255, %fd254, 0d4008000000000000;
	add.f64 	%fd256, %fd2, %fd255;
	div.rn.f64 	%fd257, %fd503, %fd3;
	add.f64 	%fd258, %fd257, 0d0000000000000000;
	fma.rn.f64 	%fd259, %fd258, %fd256, 0d0000000000000000;
	fma.rn.f64 	%fd260, %fd258, %fd252, 0d0000000000000000;
	div.rn.f64 	%fd261, %fd260, 0d4008000000000000;
	add.f64 	%fd262, %fd261, 0d0000000000000000;
	sub.f64 	%fd263, %fd253, %fd262;
	fma.rn.f64 	%fd264, %fd476, %fd259, %fd263;
	fma.rn.f64 	%fd503, %fd476, %fd259, %fd264;

$L__BB5_36:
	fma.rn.f64 	%fd68, %fd503, %fd147, 0d0000000000000000;
	mov.f64 	%fd267, 0d0000000000000000;
	setp.leu.f64 	%p22, %fd60, 0d0000000000000000;
	mov.f64 	%fd504, %fd267;
	mov.f64 	%fd505, %fd267;
	mov.f64 	%fd506, %fd267;
	@%p22 bra 	$L__BB5_38;

	div.rn.f64 	%fd268, %fd57, %fd60;
	div.rn.f64 	%fd269, %fd58, %fd60;
	div.rn.f64 	%fd270, %fd59, %fd60;
	fma.rn.f64 	%fd506, %fd268, %fd68, 0d0000000000000000;
	fma.rn.f64 	%fd505, %fd269, %fd68, 0d0000000000000000;
	fma.rn.f64 	%fd504, %fd270, %fd68, 0d0000000000000000;

$L__BB5_38:
	mov.f64 	%fd474, 0d3FF0000000000000;
	mul.f64 	%fd473, %fd21, %fd21;
	sub.f64 	%fd472, %fd474, %fd473;
	mul.f64 	%fd471, %fd19, %fd21;
	sub.f64 	%fd470, %fd227, %fd471;
	mul.f64 	%fd469, %fd19, %fd19;
	sub.f64 	%fd468, %fd474, %fd469;
	mul.f64 	%fd467, %fd20, %fd21;
	sub.f64 	%fd466, %fd227, %fd467;
	mul.f64 	%fd465, %fd20, %fd20;
	sub.f64 	%fd464, %fd474, %fd465;
	mul.f64 	%fd463, %fd19, %fd20;
	sub.f64 	%fd462, %fd227, %fd463;
	div.rn.f64 	%fd271, %fd506, %fd147;
	add.f64 	%fd272, %fd271, 0d0000000000000000;
	div.rn.f64 	%fd274, %fd505, %fd147;
	add.f64 	%fd275, %fd274, 0d0000000000000000;
	div.rn.f64 	%fd276, %fd504, %fd147;
	add.f64 	%fd277, %fd276, 0d0000000000000000;
	fma.rn.f64 	%fd278, %fd272, %fd54, 0d0000000000000000;
	fma.rn.f64 	%fd279, %fd272, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd280, %fd272, %fd56, 0d0000000000000000;
	fma.rn.f64 	%fd281, %fd275, %fd54, 0d0000000000000000;
	fma.rn.f64 	%fd282, %fd275, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd283, %fd275, %fd56, 0d0000000000000000;
	fma.rn.f64 	%fd284, %fd277, %fd54, 0d0000000000000000;
	fma.rn.f64 	%fd285, %fd277, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd286, %fd277, %fd56, 0d0000000000000000;
	mul.f64 	%fd287, %fd462, %fd275;
	mul.f64 	%fd288, %fd464, %fd275;
	mul.f64 	%fd289, %fd466, %fd275;
	fma.rn.f64 	%fd290, %fd468, %fd272, %fd287;
	fma.rn.f64 	%fd291, %fd462, %fd272, %fd288;
	fma.rn.f64 	%fd292, %fd470, %fd272, %fd289;
	fma.rn.f64 	%fd293, %fd466, %fd277, %fd291;
	fma.rn.f64 	%fd294, %fd472, %fd277, %fd292;
	add.f64 	%fd511, %fd293, 0d0000000000000000;
	fma.rn.f64 	%fd295, %fd470, %fd277, %fd290;
	add.f64 	%fd512, %fd295, 0d0000000000000000;
	add.f64 	%fd510, %fd294, 0d0000000000000000;
	sub.f64 	%fd296, %fd267, %fd278;
	sub.f64 	%fd297, %fd267, %fd281;
	sub.f64 	%fd298, %fd267, %fd284;
	sub.f64 	%fd299, %fd267, %fd279;
	sub.f64 	%fd300, %fd267, %fd282;
	sub.f64 	%fd301, %fd267, %fd285;
	sub.f64 	%fd302, %fd267, %fd280;
	sub.f64 	%fd303, %fd267, %fd283;
	sub.f64 	%fd304, %fd267, %fd286;
	mul.f64 	%fd305, %fd297, %fd20;
	mul.f64 	%fd306, %fd300, %fd20;
	mul.f64 	%fd307, %fd303, %fd20;
	fma.rn.f64 	%fd308, %fd296, %fd19, %fd305;
	fma.rn.f64 	%fd309, %fd299, %fd19, %fd306;
	fma.rn.f64 	%fd310, %fd302, %fd19, %fd307;
	fma.rn.f64 	%fd78, %fd301, %fd21, %fd309;
	fma.rn.f64 	%fd79, %fd304, %fd21, %fd310;
	fma.rn.f64 	%fd80, %fd298, %fd21, %fd308;
	mul.f64 	%fd311, %fd299, %fd20;
	mul.f64 	%fd312, %fd301, %fd20;
	fma.rn.f64 	%fd313, %fd296, %fd19, %fd311;
	fma.rn.f64 	%fd314, %fd297, %fd19, %fd306;
	fma.rn.f64 	%fd315, %fd298, %fd19, %fd312;
	fma.rn.f64 	%fd81, %fd302, %fd21, %fd313;
	fma.rn.f64 	%fd82, %fd303, %fd21, %fd314;
	fma.rn.f64 	%fd83, %fd304, %fd21, %fd315;
	fma.rn.f64 	%fd316, %fd64, %fd149, 0d0000000000000000;
	setp.eq.s64 	%p23, %rd109, 0;
	@%p23 bra 	$L__BB5_40;

	fma.rn.f64 	%fd483, %fd498, %fd316, 0d0000000000000000;
	mul.lo.s64 	%rd155, %rd70, %rd58;
	add.s64 	%rd154, %rd109, %rd155;
	// begin inline asm
	{ atom.add.f64 %fd317,[%rd154],%fd483; }

	// end inline asm
	bra.uni 	$L__BB5_42;

$L__BB5_40:
	setp.eq.s64 	%p24, %rd86, 0;
	@%p24 bra 	$L__BB5_42;

	fma.rn.f64 	%fd486, %fd498, %fd316, 0d0000000000000000;
	mul.lo.s64 	%rd157, %rd70, %rd48;
	add.s64 	%rd156, %rd86, %rd157;
	// begin inline asm
	{ atom.add.f64 %fd319,[%rd156],%fd486; }

	// end inline asm

$L__BB5_42:
	mov.f64 	%fd321, 0d0000000000000000;
	sub.f64 	%fd514, %fd321, %fd511;
	sub.f64 	%fd515, %fd321, %fd512;
	sub.f64 	%fd513, %fd321, %fd510;
	add.f64 	%fd322, %fd80, 0d0000000000000000;
	add.f64 	%fd509, %fd322, %fd81;
	add.f64 	%fd323, %fd78, 0d0000000000000000;
	add.f64 	%fd508, %fd323, %fd82;
	add.f64 	%fd324, %fd79, 0d0000000000000000;
	add.f64 	%fd507, %fd324, %fd83;
	setp.eq.s64 	%p25, %rd123, 0;
	@%p25 bra 	$L__BB5_44;

	fma.rn.f64 	%fd484, %fd499, %fd316, 0d0000000000000000;
	mul.lo.s64 	%rd159, %rd73, %rd59;
	add.s64 	%rd158, %rd123, %rd159;
	// begin inline asm
	{ atom.add.f64 %fd325,[%rd158],%fd484; }

	// end inline asm
	bra.uni 	$L__BB5_46;

$L__BB5_44:
	setp.eq.s64 	%p26, %rd106, 0;
	@%p26 bra 	$L__BB5_46;

	fma.rn.f64 	%fd485, %fd499, %fd316, 0d0000000000000000;
	mul.lo.s64 	%rd161, %rd73, %rd47;
	add.s64 	%rd160, %rd106, %rd161;
	// begin inline asm
	{ atom.add.f64 %fd327,[%rd160],%fd485; }

	// end inline asm

$L__BB5_46:
	setp.ne.s32 	%p50, %r135, 0;
	@%p50 bra 	$L__BB5_65;

	setp.eq.s64 	%p28, %rd107, 0;
	@%p28 bra 	$L__BB5_49;

	ld.global.f64 	%fd330, [%rd79];
	add.f64 	%fd516, %fd330, 0d0000000000000000;
	bra.uni 	$L__BB5_51;

$L__BB5_49:
	setp.eq.s64 	%p29, %rd84, 0;
	mov.f64 	%fd516, 0d0000000000000000;
	@%p29 bra 	$L__BB5_51;

	ld.global.f64 	%fd332, [%rd80];
	add.f64 	%fd516, %fd332, 0d0000000000000000;

$L__BB5_51:
	setp.geu.f64 	%p30, %fd22, %fd146;
	@%p30 bra 	$L__BB5_59;

	sub.f64 	%fd335, %fd22, %fd146;
	div.rn.f64 	%fd519, %fd335, %fd146;
	mul.f64 	%fd520, %fd1, %fd519;
	div.rn.f64 	%fd522, %fd22, %fd146;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r432}, %fd522;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r433, %temp}, %fd522;
	}
	setp.gt.s32 	%p31, %r432, 1048575;
	mov.u32 	%r434, -1023;
	mov.f64 	%fd517, %fd522;
	@%p31 bra 	$L__BB5_54;

	mul.f64 	%fd517, %fd522, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r432}, %fd517;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r433, %temp}, %fd517;
	}
	mov.u32 	%r434, -1077;

$L__BB5_54:
	mul.f64 	%fd521, %fd519, %fd520;
	add.s32 	%r417, %r432, -1;
	setp.lt.u32 	%p32, %r417, 2146435071;
	@%p32 bra 	$L__BB5_56;
	bra.uni 	$L__BB5_55;

$L__BB5_56:
	shr.u32 	%r419, %r432, 20;
	add.s32 	%r435, %r434, %r419;
	and.b32  	%r420, %r432, -2146435073;
	or.b32  	%r421, %r420, 1072693248;
	mov.b64 	%fd518, {%r433, %r421};
	setp.lt.s32 	%p34, %r421, 1073127583;
	@%p34 bra 	$L__BB5_58;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r422, %temp}, %fd518;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r423}, %fd518;
	}
	add.s32 	%r424, %r423, -1048576;
	mov.b64 	%fd518, {%r422, %r424};
	add.s32 	%r435, %r435, 1;

$L__BB5_58:
	add.f64 	%fd338, %fd518, 0d3FF0000000000000;
	mov.f64 	%fd339, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd340, %fd338;
	neg.f64 	%fd341, %fd338;
	fma.rn.f64 	%fd342, %fd341, %fd340, %fd339;
	fma.rn.f64 	%fd343, %fd342, %fd342, %fd342;
	fma.rn.f64 	%fd344, %fd343, %fd340, %fd340;
	add.f64 	%fd345, %fd518, 0dBFF0000000000000;
	mul.f64 	%fd346, %fd345, %fd344;
	fma.rn.f64 	%fd347, %fd345, %fd344, %fd346;
	mul.f64 	%fd348, %fd347, %fd347;
	mov.f64 	%fd349, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd350, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd351, %fd350, %fd348, %fd349;
	mov.f64 	%fd352, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd353, %fd351, %fd348, %fd352;
	mov.f64 	%fd354, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd355, %fd353, %fd348, %fd354;
	mov.f64 	%fd356, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd357, %fd355, %fd348, %fd356;
	mov.f64 	%fd358, 0d3F624924923BE72D;
	fma.rn.f64 	%fd359, %fd357, %fd348, %fd358;
	mov.f64 	%fd360, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd361, %fd359, %fd348, %fd360;
	mov.f64 	%fd362, 0d3FB5555555555554;
	fma.rn.f64 	%fd363, %fd361, %fd348, %fd362;
	sub.f64 	%fd364, %fd345, %fd347;
	add.f64 	%fd365, %fd364, %fd364;
	neg.f64 	%fd366, %fd347;
	fma.rn.f64 	%fd367, %fd366, %fd345, %fd365;
	mul.f64 	%fd368, %fd344, %fd367;
	mul.f64 	%fd369, %fd348, %fd363;
	fma.rn.f64 	%fd370, %fd369, %fd347, %fd368;
	xor.b32  	%r425, %r435, -2147483648;
	mov.u32 	%r426, -2147483648;
	mov.u32 	%r427, 1127219200;
	mov.b64 	%fd371, {%r425, %r427};
	mov.b64 	%fd372, {%r426, %r427};
	sub.f64 	%fd373, %fd371, %fd372;
	mov.f64 	%fd374, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd375, %fd373, %fd374, %fd347;
	neg.f64 	%fd376, %fd373;
	fma.rn.f64 	%fd377, %fd376, %fd374, %fd375;
	sub.f64 	%fd378, %fd377, %fd347;
	sub.f64 	%fd379, %fd370, %fd378;
	mov.f64 	%fd380, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd381, %fd373, %fd380, %fd379;
	add.f64 	%fd523, %fd375, %fd381;
	bra.uni 	$L__BB5_59;

$L__BB5_55:
	mov.f64 	%fd336, 0d7FF0000000000000;
	fma.rn.f64 	%fd337, %fd517, %fd336, %fd336;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r418}, %fd517;
	}
	mov.b32 	%f2, %r418;
	setp.eq.f32 	%p33, %f2, 0f00000000;
	selp.f64 	%fd523, 0dFFF0000000000000, %fd337, %p33;

$L__BB5_59:
	fma.rn.f64 	%fd479, %fd516, %fd149, 0d0000000000000000;
	fma.rn.f64 	%fd478, %fd496, %fd479, 0d0000000000000000;
	setp.lt.f64 	%p36, %fd22, %fd146;
	selp.f64 	%fd122, %fd478, 0d0000000000000000, %p36;
	mov.f64 	%fd525, 0d0000000000000000;
	@%p30 bra 	$L__BB5_61;

	fma.rn.f64 	%fd383, %fd122, %fd523, 0d0000000000000000;
	fma.rn.f64 	%fd384, %fd122, %fd521, 0d0000000000000000;
	rcp.rn.f64 	%fd385, %fd522;
	fma.rn.f64 	%fd386, %fd385, %fd384, 0d0000000000000000;
	div.rn.f64 	%fd387, %fd386, %fd146;
	add.f64 	%fd388, %fd387, 0d0000000000000000;
	fma.rn.f64 	%fd389, %fd519, %fd383, 0d0000000000000000;
	fma.rn.f64 	%fd390, %fd520, %fd383, 0d0000000000000000;
	div.rn.f64 	%fd391, %fd390, %fd146;
	add.f64 	%fd392, %fd391, 0d0000000000000000;
	fma.rn.f64 	%fd393, %fd1, %fd389, 0d0000000000000000;
	div.rn.f64 	%fd394, %fd393, %fd146;
	add.f64 	%fd395, %fd392, %fd394;
	add.f64 	%fd525, %fd388, %fd395;

$L__BB5_61:
	fma.rn.f64 	%fd481, %fd516, %fd149, 0d0000000000000000;
	fma.rn.f64 	%fd480, %fd497, %fd481, 0d0000000000000000;
	fma.rn.f64 	%fd125, %fd480, %fd146, 0d0000000000000000;
	setp.eq.s64 	%p37, %rd117, 0;
	@%p37 bra 	$L__BB5_63;

	mul.lo.s64 	%rd163, %rd71, %rd54;
	add.s64 	%rd162, %rd117, %rd163;
	// begin inline asm
	{ atom.add.f64 %fd396,[%rd162],%fd125; }

	// end inline asm
	bra.uni 	$L__BB5_65;

$L__BB5_63:
	setp.eq.s64 	%p38, %rd100, 0;
	@%p38 bra 	$L__BB5_65;

	mul.lo.s64 	%rd165, %rd71, %rd46;
	add.s64 	%rd164, %rd100, %rd165;
	// begin inline asm
	{ atom.add.f64 %fd398,[%rd164],%fd125; }

	// end inline asm

$L__BB5_65:
	add.f64 	%fd400, %fd525, 0d0000000000000000;
	fma.rn.f64 	%fd127, %fd19, %fd400, 0d0000000000000000;
	fma.rn.f64 	%fd128, %fd20, %fd400, 0d0000000000000000;
	fma.rn.f64 	%fd129, %fd21, %fd400, 0d0000000000000000;
	fma.rn.f64 	%fd130, %fd14, %fd400, %fd509;
	fma.rn.f64 	%fd131, %fd16, %fd400, %fd508;
	fma.rn.f64 	%fd132, %fd18, %fd400, %fd507;
	add.f64 	%fd133, %fd515, 0d0000000000000000;
	add.f64 	%fd134, %fd514, 0d0000000000000000;
	add.f64 	%fd135, %fd513, 0d0000000000000000;
	setp.eq.s64 	%p39, %rd113, 0;
	@%p39 bra 	$L__BB5_67;

	mul.lo.s64 	%rd169, %rd76, %rd50;
	add.s64 	%rd166, %rd113, %rd169;
	// begin inline asm
	{ atom.add.f64 %fd401,[%rd166],%fd133; }

	// end inline asm
	add.s64 	%rd167, %rd166, 8;
	// begin inline asm
	{ atom.add.f64 %fd403,[%rd167],%fd134; }

	// end inline asm
	add.s64 	%rd168, %rd166, 16;
	// begin inline asm
	{ atom.add.f64 %fd405,[%rd168],%fd135; }

	// end inline asm
	bra.uni 	$L__BB5_69;

$L__BB5_67:
	setp.eq.s64 	%p40, %rd96, 0;
	@%p40 bra 	$L__BB5_69;

	mul.lo.s64 	%rd209, %rd76, %rd198;
	add.s64 	%rd170, %rd96, %rd209;
	// begin inline asm
	{ atom.add.f64 %fd407,[%rd170],%fd133; }

	// end inline asm
	add.s64 	%rd171, %rd170, 8;
	// begin inline asm
	{ atom.add.f64 %fd409,[%rd171],%fd134; }

	// end inline asm
	add.s64 	%rd172, %rd170, 16;
	// begin inline asm
	{ atom.add.f64 %fd411,[%rd172],%fd135; }

	// end inline asm

$L__BB5_69:
	setp.eq.s64 	%p41, %rd111, 0;
	add.f64 	%fd413, %fd512, %fd127;
	add.f64 	%fd136, %fd413, 0d0000000000000000;
	add.f64 	%fd414, %fd511, %fd128;
	add.f64 	%fd137, %fd414, 0d0000000000000000;
	add.f64 	%fd415, %fd510, %fd129;
	add.f64 	%fd138, %fd415, 0d0000000000000000;
	@%p41 bra 	$L__BB5_71;

	mul.lo.s64 	%rd176, %rd76, %rd52;
	add.s64 	%rd173, %rd111, %rd176;
	// begin inline asm
	{ atom.add.f64 %fd416,[%rd173],%fd136; }

	// end inline asm
	add.s64 	%rd174, %rd173, 8;
	// begin inline asm
	{ atom.add.f64 %fd418,[%rd174],%fd137; }

	// end inline asm
	add.s64 	%rd175, %rd173, 16;
	// begin inline asm
	{ atom.add.f64 %fd420,[%rd175],%fd138; }

	// end inline asm
	bra.uni 	$L__BB5_73;

$L__BB5_71:
	setp.eq.s64 	%p42, %rd92, 0;
	@%p42 bra 	$L__BB5_73;

	mul.lo.s64 	%rd208, %rd76, %rd199;
	add.s64 	%rd177, %rd92, %rd208;
	// begin inline asm
	{ atom.add.f64 %fd422,[%rd177],%fd136; }

	// end inline asm
	add.s64 	%rd178, %rd177, 8;
	// begin inline asm
	{ atom.add.f64 %fd424,[%rd178],%fd137; }

	// end inline asm
	add.s64 	%rd179, %rd177, 16;
	// begin inline asm
	{ atom.add.f64 %fd426,[%rd179],%fd138; }

	// end inline asm

$L__BB5_73:
	setp.eq.s64 	%p43, %rd121, 0;
	mov.f64 	%fd428, 0d0000000000000000;
	sub.f64 	%fd429, %fd428, %fd127;
	add.f64 	%fd139, %fd429, 0d0000000000000000;
	sub.f64 	%fd430, %fd428, %fd128;
	add.f64 	%fd140, %fd430, 0d0000000000000000;
	sub.f64 	%fd431, %fd428, %fd129;
	add.f64 	%fd141, %fd431, 0d0000000000000000;
	@%p43 bra 	$L__BB5_75;

	mul.lo.s64 	%rd183, %rd73, %rd53;
	add.s64 	%rd180, %rd121, %rd183;
	// begin inline asm
	{ atom.add.f64 %fd432,[%rd180],%fd139; }

	// end inline asm
	add.s64 	%rd181, %rd180, 8;
	// begin inline asm
	{ atom.add.f64 %fd434,[%rd181],%fd140; }

	// end inline asm
	add.s64 	%rd182, %rd180, 16;
	// begin inline asm
	{ atom.add.f64 %fd436,[%rd182],%fd141; }

	// end inline asm
	bra.uni 	$L__BB5_77;

$L__BB5_75:
	setp.eq.s64 	%p44, %rd104, 0;
	@%p44 bra 	$L__BB5_77;

	mul.lo.s64 	%rd207, %rd73, %rd200;
	add.s64 	%rd184, %rd104, %rd207;
	// begin inline asm
	{ atom.add.f64 %fd438,[%rd184],%fd139; }

	// end inline asm
	add.s64 	%rd185, %rd184, 8;
	// begin inline asm
	{ atom.add.f64 %fd440,[%rd185],%fd140; }

	// end inline asm
	add.s64 	%rd186, %rd184, 16;
	// begin inline asm
	{ atom.add.f64 %fd442,[%rd186],%fd141; }

	// end inline asm

$L__BB5_77:
	setp.eq.s64 	%p45, %rd119, 0;
	add.f64 	%fd142, %fd130, 0d0000000000000000;
	add.f64 	%fd143, %fd131, 0d0000000000000000;
	add.f64 	%fd144, %fd132, 0d0000000000000000;
	@%p45 bra 	$L__BB5_79;

	mul.lo.s64 	%rd190, %rd73, %rd55;
	add.s64 	%rd187, %rd119, %rd190;
	// begin inline asm
	{ atom.add.f64 %fd444,[%rd187],%fd142; }

	// end inline asm
	add.s64 	%rd188, %rd187, 8;
	// begin inline asm
	{ atom.add.f64 %fd446,[%rd188],%fd143; }

	// end inline asm
	add.s64 	%rd189, %rd187, 16;
	// begin inline asm
	{ atom.add.f64 %fd448,[%rd189],%fd144; }

	// end inline asm
	bra.uni 	$L__BB5_81;

$L__BB5_79:
	setp.eq.s64 	%p46, %rd102, 0;
	@%p46 bra 	$L__BB5_81;

	mul.lo.s64 	%rd206, %rd73, %rd201;
	add.s64 	%rd191, %rd102, %rd206;
	// begin inline asm
	{ atom.add.f64 %fd450,[%rd191],%fd142; }

	// end inline asm
	add.s64 	%rd192, %rd191, 8;
	// begin inline asm
	{ atom.add.f64 %fd452,[%rd192],%fd143; }

	// end inline asm
	add.s64 	%rd193, %rd191, 16;
	// begin inline asm
	{ atom.add.f64 %fd454,[%rd193],%fd144; }

	// end inline asm

$L__BB5_81:
	setp.eq.s64 	%p47, %rd115, 0;
	mov.f64 	%fd456, 0d0000000000000000;
	sub.f64 	%fd457, %fd456, %fd525;
	add.f64 	%fd145, %fd457, 0d0000000000000000;
	@%p47 bra 	$L__BB5_83;

	mul.lo.s64 	%rd195, %rd71, %rd57;
	add.s64 	%rd194, %rd115, %rd195;
	// begin inline asm
	{ atom.add.f64 %fd458,[%rd194],%fd145; }

	// end inline asm
	bra.uni 	$L__BB5_85;

$L__BB5_83:
	setp.eq.s64 	%p48, %rd98, 0;
	@%p48 bra 	$L__BB5_85;

	mul.lo.s64 	%rd205, %rd71, %rd202;
	add.s64 	%rd196, %rd98, %rd205;
	// begin inline asm
	{ atom.add.f64 %fd460,[%rd196],%fd145; }

	// end inline asm

$L__BB5_85:
	ld.param.u64 	%rd197, [val_IPC_hs_cuda_kernel_backward_param_0+24];
	add.s64 	%rd210, %rd210, %rd56;
	setp.lt.u64 	%p49, %rd210, %rd197;
	@%p49 bra 	$L__BB5_2;

$L__BB5_86:
	ret;

}
	// .globl	initialize_friction_collisions_cuda_kernel_forward
.visible .entry initialize_friction_collisions_cuda_kernel_forward(
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_11[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_12[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_13[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_14[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_15[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_16[56],
	.param .f64 initialize_friction_collisions_cuda_kernel_forward_param_17,
	.param .f64 initialize_friction_collisions_cuda_kernel_forward_param_18,
	.param .f64 initialize_friction_collisions_cuda_kernel_forward_param_19
)
{
	.reg .pred 	%p<155>;
	.reg .b16 	%rs<129>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<412>;
	.reg .f64 	%fd<1226>;
	.reg .b64 	%rd<171>;


	ld.param.v2.u32 	{%r189, %r190}, [initialize_friction_collisions_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r191, %r192}, [initialize_friction_collisions_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r197, %r198}, [initialize_friction_collisions_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r205, %r206}, [initialize_friction_collisions_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r213, %r214}, [initialize_friction_collisions_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r221, %r222}, [initialize_friction_collisions_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r229, %r230}, [initialize_friction_collisions_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r237, %r238}, [initialize_friction_collisions_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r245, %r246}, [initialize_friction_collisions_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r253, %r254}, [initialize_friction_collisions_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r261, %r262}, [initialize_friction_collisions_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r269, %r270}, [initialize_friction_collisions_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r277, %r278}, [initialize_friction_collisions_cuda_kernel_forward_param_11+32];
	ld.param.v2.u32 	{%r285, %r286}, [initialize_friction_collisions_cuda_kernel_forward_param_12+32];
	ld.param.v2.u32 	{%r293, %r294}, [initialize_friction_collisions_cuda_kernel_forward_param_13+32];
	ld.param.v2.u32 	{%r301, %r302}, [initialize_friction_collisions_cuda_kernel_forward_param_14+32];
	ld.param.v2.u32 	{%r309, %r310}, [initialize_friction_collisions_cuda_kernel_forward_param_15+32];
	ld.param.v2.u32 	{%r317, %r318}, [initialize_friction_collisions_cuda_kernel_forward_param_16+32];
	ld.param.f64 	%fd306, [initialize_friction_collisions_cuda_kernel_forward_param_17];
	ld.param.f64 	%fd307, [initialize_friction_collisions_cuda_kernel_forward_param_18];
	ld.param.f64 	%fd308, [initialize_friction_collisions_cuda_kernel_forward_param_19];
	ld.param.u64 	%rd94, [initialize_friction_collisions_cuda_kernel_forward_param_16];
	ld.param.u64 	%rd92, [initialize_friction_collisions_cuda_kernel_forward_param_15];
	ld.param.u64 	%rd90, [initialize_friction_collisions_cuda_kernel_forward_param_14];
	ld.param.u64 	%rd88, [initialize_friction_collisions_cuda_kernel_forward_param_13];
	ld.param.u64 	%rd86, [initialize_friction_collisions_cuda_kernel_forward_param_12];
	ld.param.u64 	%rd84, [initialize_friction_collisions_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd82, [initialize_friction_collisions_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd80, [initialize_friction_collisions_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd78, [initialize_friction_collisions_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd76, [initialize_friction_collisions_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd74, [initialize_friction_collisions_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd72, [initialize_friction_collisions_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd70, [initialize_friction_collisions_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd68, [initialize_friction_collisions_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd66, [initialize_friction_collisions_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd64, [initialize_friction_collisions_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd63, [initialize_friction_collisions_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r44, [initialize_friction_collisions_cuda_kernel_forward_param_0+16];
	mov.u32 	%r321, %ntid.x;
	cvt.u64.u32 	%rd1, %r321;
	mov.u32 	%r322, %ctaid.x;
	mul.wide.u32 	%rd96, %r321, %r322;
	mov.u32 	%r323, %tid.x;
	cvt.u64.u32 	%rd97, %r323;
	add.s64 	%rd167, %rd96, %rd97;
	setp.ge.u64 	%p4, %rd167, %rd63;
	@%p4 bra 	$L__BB6_160;

	cvta.to.global.u64 	%rd4, %rd76;
	cvta.to.global.u64 	%rd5, %rd72;
	cvta.to.global.u64 	%rd6, %rd70;
	cvta.to.global.u64 	%rd7, %rd68;
	cvta.to.global.u64 	%rd8, %rd66;
	cvta.to.global.u64 	%rd9, %rd94;
	cvta.to.global.u64 	%rd10, %rd92;
	cvta.to.global.u64 	%rd11, %rd90;
	cvta.to.global.u64 	%rd12, %rd88;
	cvta.to.global.u64 	%rd13, %rd86;
	cvta.to.global.u64 	%rd14, %rd84;
	cvta.to.global.u64 	%rd15, %rd82;
	cvta.to.global.u64 	%rd16, %rd80;
	cvta.to.global.u64 	%rd17, %rd78;
	cvta.to.global.u64 	%rd18, %rd74;
	cvta.to.global.u64 	%rd19, %rd64;
	cvt.s64.s32 	%rd20, %r192;
	cvt.s64.s32 	%rd21, %r191;
	cvt.s64.s32 	%rd22, %r190;
	cvt.s64.s32 	%rd23, %r197;
	cvt.s64.s32 	%rd24, %r237;
	cvt.s64.s32 	%rd25, %r229;
	cvt.s64.s32 	%rd26, %r221;
	cvt.s64.s32 	%rd27, %r317;
	cvt.s64.s32 	%rd28, %r269;
	cvt.s64.s32 	%rd29, %r285;
	cvt.s64.s32 	%rd30, %r253;
	cvt.s64.s32 	%rd31, %r245;
	mov.u32 	%r324, %nctaid.x;
	cvt.u64.u32 	%rd98, %r324;
	mul.lo.s64 	%rd32, %rd1, %rd98;
	cvt.s64.s32 	%rd33, %r309;
	cvt.s64.s32 	%rd34, %r277;
	mul.f64 	%fd1, %fd306, %fd306;
	add.f64 	%fd2, %fd306, %fd306;
	cvt.s64.s32 	%rd35, %r293;
	cvt.s64.s32 	%rd36, %r205;
	cvt.s64.s32 	%rd37, %r213;
	cvt.s64.s32 	%rd38, %r261;
	cvt.s64.s32 	%rd39, %r301;

$L__BB6_2:
	setp.lt.s32 	%p5, %r44, 4;
	mov.u64 	%rd168, %rd167;
	@%p5 bra 	$L__BB6_6;

	or.b64  	%rd99, %rd167, %rd20;
	and.b64  	%rd100, %rd99, -4294967296;
	setp.eq.s64 	%p6, %rd100, 0;
	@%p6 bra 	$L__BB6_5;

	div.u64 	%rd168, %rd167, %rd20;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r325, %rd20;
	cvt.u32.u64 	%r326, %rd167;
	div.u32 	%r327, %r326, %r325;
	cvt.u64.u32 	%rd168, %r327;

$L__BB6_6:
	setp.lt.s32 	%p7, %r44, 3;
	@%p7 bra 	$L__BB6_10;

	or.b64  	%rd101, %rd168, %rd21;
	and.b64  	%rd102, %rd101, -4294967296;
	setp.eq.s64 	%p8, %rd102, 0;
	@%p8 bra 	$L__BB6_9;

	div.u64 	%rd168, %rd168, %rd21;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r328, %rd21;
	cvt.u32.u64 	%r329, %rd168;
	div.u32 	%r330, %r329, %r328;
	cvt.u64.u32 	%rd168, %r330;

$L__BB6_10:
	setp.lt.s32 	%p9, %r44, 2;
	@%p9 bra 	$L__BB6_14;

	or.b64  	%rd103, %rd168, %rd22;
	and.b64  	%rd104, %rd103, -4294967296;
	setp.eq.s64 	%p10, %rd104, 0;
	@%p10 bra 	$L__BB6_13;

	div.u64 	%rd168, %rd168, %rd22;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r331, %rd22;
	cvt.u32.u64 	%r332, %rd168;
	div.u32 	%r333, %r332, %r331;
	cvt.u64.u32 	%rd168, %r333;

$L__BB6_14:
	cvt.s64.s32 	%rd105, %rd168;
	setp.gt.s32 	%p11, %r44, 0;
	selp.b64 	%rd106, %rd105, 0, %p11;
	mov.u64 	%rd107, 0;
	mul.lo.s64 	%rd108, %rd106, %rd23;
	add.s64 	%rd50, %rd19, %rd108;
	st.global.u64 	[%rd50], %rd107;
	mul.lo.s64 	%rd109, %rd106, %rd24;
	add.s64 	%rd110, %rd18, %rd109;
	ld.global.u32 	%r2, [%rd110];
	setp.gt.u32 	%p12, %r2, 1;
	mul.lo.s64 	%rd111, %rd106, %rd25;
	add.s64 	%rd51, %rd5, %rd111;
	mul.lo.s64 	%rd112, %rd106, %rd26;
	add.s64 	%rd52, %rd6, %rd112;
	mul.lo.s64 	%rd113, %rd106, %rd36;
	add.s64 	%rd53, %rd8, %rd113;
	mul.lo.s64 	%rd114, %rd106, %rd37;
	add.s64 	%rd54, %rd7, %rd114;
	@%p12 bra 	$L__BB6_73;

	ld.global.u32 	%r335, [%rd52];
	ld.global.u32 	%r336, [%rd51];
	setp.eq.s32 	%p13, %r2, 1;
	selp.b32 	%r337, %r335, %r336, %p13;
	selp.b32 	%r338, %r336, %r335, %p13;
	cvt.s64.s32 	%rd115, %r338;
	mul.lo.s64 	%rd116, %rd115, %rd27;
	add.s64 	%rd117, %rd9, %rd116;
	cvt.s64.s32 	%rd55, %r337;
	mul.lo.s64 	%rd118, %rd55, %rd28;
	add.s64 	%rd119, %rd15, %rd118;
	mul.lo.s64 	%rd120, %rd115, %rd29;
	add.s64 	%rd121, %rd13, %rd120;
	ld.global.f64 	%fd309, [%rd121];
	ld.global.f64 	%fd310, [%rd119];
	add.f64 	%fd3, %fd310, %fd309;
	mul.lo.s64 	%rd122, %rd55, %rd30;
	add.s64 	%rd123, %rd17, %rd122;
	ld.global.s32 	%rd124, [%rd123];
	mul.lo.s64 	%rd125, %rd124, %rd31;
	add.s64 	%rd126, %rd4, %rd125;
	ld.global.s32 	%rd127, [%rd117];
	mul.lo.s64 	%rd128, %rd127, %rd31;
	add.s64 	%rd129, %rd4, %rd128;
	ld.global.s32 	%rd130, [%rd117+4];
	mul.lo.s64 	%rd131, %rd130, %rd31;
	add.s64 	%rd132, %rd4, %rd131;
	ld.global.s32 	%rd133, [%rd117+8];
	mul.lo.s64 	%rd134, %rd133, %rd31;
	add.s64 	%rd135, %rd4, %rd134;
	ld.global.f64 	%fd4, [%rd132];
	ld.global.f64 	%fd5, [%rd129];
	sub.f64 	%fd6, %fd4, %fd5;
	ld.global.f64 	%fd7, [%rd132+8];
	ld.global.f64 	%fd8, [%rd129+8];
	sub.f64 	%fd9, %fd7, %fd8;
	ld.global.f64 	%fd10, [%rd132+16];
	ld.global.f64 	%fd11, [%rd129+16];
	sub.f64 	%fd12, %fd10, %fd11;
	ld.global.f64 	%fd13, [%rd135];
	sub.f64 	%fd14, %fd13, %fd5;
	ld.global.f64 	%fd15, [%rd135+8];
	sub.f64 	%fd16, %fd15, %fd8;
	ld.global.f64 	%fd17, [%rd135+16];
	sub.f64 	%fd18, %fd17, %fd11;
	mul.f64 	%fd311, %fd9, %fd18;
	mul.f64 	%fd312, %fd12, %fd16;
	sub.f64 	%fd19, %fd311, %fd312;
	mul.f64 	%fd313, %fd12, %fd14;
	mul.f64 	%fd314, %fd6, %fd18;
	sub.f64 	%fd20, %fd313, %fd314;
	mul.f64 	%fd315, %fd6, %fd16;
	mul.f64 	%fd316, %fd9, %fd14;
	sub.f64 	%fd21, %fd315, %fd316;
	mul.f64 	%fd317, %fd9, %fd21;
	mul.f64 	%fd318, %fd12, %fd20;
	sub.f64 	%fd319, %fd317, %fd318;
	mul.f64 	%fd320, %fd12, %fd19;
	mul.f64 	%fd321, %fd6, %fd21;
	sub.f64 	%fd322, %fd320, %fd321;
	mul.f64 	%fd323, %fd6, %fd20;
	mul.f64 	%fd324, %fd9, %fd19;
	sub.f64 	%fd325, %fd323, %fd324;
	mul.f64 	%fd326, %fd9, %fd9;
	fma.rn.f64 	%fd327, %fd6, %fd6, %fd326;
	fma.rn.f64 	%fd22, %fd12, %fd12, %fd327;
	mul.f64 	%fd328, %fd9, %fd322;
	fma.rn.f64 	%fd329, %fd6, %fd319, %fd328;
	fma.rn.f64 	%fd330, %fd12, %fd325, %fd329;
	mul.f64 	%fd331, %fd322, %fd322;
	fma.rn.f64 	%fd332, %fd319, %fd319, %fd331;
	fma.rn.f64 	%fd333, %fd325, %fd325, %fd332;
	ld.global.f64 	%fd23, [%rd126];
	sub.f64 	%fd24, %fd23, %fd5;
	ld.global.f64 	%fd25, [%rd126+8];
	sub.f64 	%fd26, %fd25, %fd8;
	ld.global.f64 	%fd27, [%rd126+16];
	sub.f64 	%fd28, %fd27, %fd11;
	mul.f64 	%fd334, %fd26, %fd9;
	fma.rn.f64 	%fd335, %fd24, %fd6, %fd334;
	fma.rn.f64 	%fd29, %fd28, %fd12, %fd335;
	mul.f64 	%fd336, %fd26, %fd322;
	fma.rn.f64 	%fd337, %fd24, %fd319, %fd336;
	fma.rn.f64 	%fd338, %fd28, %fd325, %fd337;
	div.rn.f64 	%fd339, %fd330, %fd22;
	mul.f64 	%fd340, %fd339, %fd339;
	mul.f64 	%fd341, %fd22, %fd340;
	sub.f64 	%fd342, %fd333, %fd341;
	mul.f64 	%fd343, %fd29, %fd339;
	sub.f64 	%fd344, %fd338, %fd343;
	div.rn.f64 	%fd345, %fd344, %fd342;
	mul.f64 	%fd346, %fd22, %fd339;
	mul.f64 	%fd347, %fd346, %fd345;
	sub.f64 	%fd348, %fd29, %fd347;
	div.rn.f64 	%fd30, %fd348, %fd22;
	setp.gt.f64 	%p14, %fd30, 0d0000000000000000;
	setp.lt.f64 	%p15, %fd30, 0d3FF0000000000000;
	setp.ge.f64 	%p16, %fd345, 0d0000000000000000;
	and.pred  	%p17, %p14, %p15;
	and.pred  	%p1, %p16, %p17;
	mov.u32 	%r396, 3;
	@%p1 bra 	$L__BB6_21;

	sub.f64 	%fd349, %fd13, %fd4;
	sub.f64 	%fd350, %fd15, %fd7;
	mul.f64 	%fd351, %fd350, %fd21;
	sub.f64 	%fd352, %fd17, %fd10;
	mul.f64 	%fd353, %fd352, %fd20;
	sub.f64 	%fd354, %fd351, %fd353;
	mul.f64 	%fd355, %fd352, %fd19;
	mul.f64 	%fd356, %fd349, %fd21;
	sub.f64 	%fd357, %fd355, %fd356;
	mul.f64 	%fd358, %fd349, %fd20;
	mul.f64 	%fd359, %fd350, %fd19;
	sub.f64 	%fd360, %fd358, %fd359;
	mul.f64 	%fd361, %fd350, %fd350;
	fma.rn.f64 	%fd362, %fd349, %fd349, %fd361;
	fma.rn.f64 	%fd363, %fd352, %fd352, %fd362;
	mul.f64 	%fd364, %fd350, %fd357;
	fma.rn.f64 	%fd365, %fd349, %fd354, %fd364;
	fma.rn.f64 	%fd366, %fd352, %fd360, %fd365;
	mul.f64 	%fd367, %fd357, %fd357;
	fma.rn.f64 	%fd368, %fd354, %fd354, %fd367;
	fma.rn.f64 	%fd369, %fd360, %fd360, %fd368;
	sub.f64 	%fd370, %fd23, %fd4;
	sub.f64 	%fd371, %fd25, %fd7;
	mul.f64 	%fd372, %fd371, %fd350;
	fma.rn.f64 	%fd373, %fd370, %fd349, %fd372;
	sub.f64 	%fd374, %fd27, %fd10;
	fma.rn.f64 	%fd375, %fd374, %fd352, %fd373;
	mul.f64 	%fd376, %fd371, %fd357;
	fma.rn.f64 	%fd377, %fd370, %fd354, %fd376;
	fma.rn.f64 	%fd378, %fd374, %fd360, %fd377;
	div.rn.f64 	%fd379, %fd366, %fd363;
	mul.f64 	%fd380, %fd379, %fd379;
	mul.f64 	%fd381, %fd363, %fd380;
	sub.f64 	%fd382, %fd369, %fd381;
	mul.f64 	%fd383, %fd375, %fd379;
	sub.f64 	%fd384, %fd378, %fd383;
	div.rn.f64 	%fd385, %fd384, %fd382;
	mul.f64 	%fd386, %fd363, %fd379;
	mul.f64 	%fd387, %fd386, %fd385;
	sub.f64 	%fd388, %fd375, %fd387;
	div.rn.f64 	%fd31, %fd388, %fd363;
	setp.gt.f64 	%p18, %fd31, 0d0000000000000000;
	setp.lt.f64 	%p19, %fd31, 0d3FF0000000000000;
	setp.ge.f64 	%p20, %fd385, 0d0000000000000000;
	and.pred  	%p21, %p18, %p19;
	and.pred  	%p22, %p20, %p21;
	mov.u32 	%r396, 4;
	@%p22 bra 	$L__BB6_21;

	sub.f64 	%fd389, %fd5, %fd13;
	sub.f64 	%fd390, %fd8, %fd15;
	mul.f64 	%fd391, %fd390, %fd21;
	sub.f64 	%fd392, %fd11, %fd17;
	mul.f64 	%fd393, %fd392, %fd20;
	sub.f64 	%fd394, %fd391, %fd393;
	mul.f64 	%fd395, %fd392, %fd19;
	mul.f64 	%fd396, %fd389, %fd21;
	sub.f64 	%fd397, %fd395, %fd396;
	mul.f64 	%fd398, %fd389, %fd20;
	mul.f64 	%fd399, %fd390, %fd19;
	sub.f64 	%fd400, %fd398, %fd399;
	mul.f64 	%fd401, %fd390, %fd390;
	fma.rn.f64 	%fd402, %fd389, %fd389, %fd401;
	fma.rn.f64 	%fd403, %fd392, %fd392, %fd402;
	mul.f64 	%fd404, %fd390, %fd397;
	fma.rn.f64 	%fd405, %fd389, %fd394, %fd404;
	fma.rn.f64 	%fd406, %fd392, %fd400, %fd405;
	mul.f64 	%fd407, %fd397, %fd397;
	fma.rn.f64 	%fd408, %fd394, %fd394, %fd407;
	fma.rn.f64 	%fd409, %fd400, %fd400, %fd408;
	sub.f64 	%fd410, %fd23, %fd13;
	sub.f64 	%fd411, %fd25, %fd15;
	mul.f64 	%fd412, %fd390, %fd411;
	fma.rn.f64 	%fd413, %fd389, %fd410, %fd412;
	sub.f64 	%fd414, %fd27, %fd17;
	fma.rn.f64 	%fd415, %fd392, %fd414, %fd413;
	mul.f64 	%fd416, %fd411, %fd397;
	fma.rn.f64 	%fd417, %fd410, %fd394, %fd416;
	fma.rn.f64 	%fd418, %fd414, %fd400, %fd417;
	div.rn.f64 	%fd419, %fd406, %fd403;
	mul.f64 	%fd420, %fd419, %fd419;
	mul.f64 	%fd421, %fd403, %fd420;
	sub.f64 	%fd422, %fd409, %fd421;
	mul.f64 	%fd423, %fd415, %fd419;
	sub.f64 	%fd424, %fd418, %fd423;
	div.rn.f64 	%fd425, %fd424, %fd422;
	mul.f64 	%fd426, %fd403, %fd419;
	mul.f64 	%fd427, %fd426, %fd425;
	sub.f64 	%fd428, %fd415, %fd427;
	div.rn.f64 	%fd32, %fd428, %fd403;
	setp.gt.f64 	%p23, %fd32, 0d0000000000000000;
	setp.lt.f64 	%p24, %fd32, 0d3FF0000000000000;
	setp.ge.f64 	%p25, %fd425, 0d0000000000000000;
	and.pred  	%p26, %p23, %p24;
	and.pred  	%p27, %p25, %p26;
	mov.u32 	%r396, 5;
	@%p27 bra 	$L__BB6_21;

	setp.le.f64 	%p28, %fd30, 0d0000000000000000;
	setp.ge.f64 	%p29, %fd32, 0d3FF0000000000000;
	and.pred  	%p30, %p28, %p29;
	mov.u32 	%r396, 0;
	@%p30 bra 	$L__BB6_21;

	setp.le.f64 	%p31, %fd31, 0d0000000000000000;
	setp.ge.f64 	%p32, %fd30, 0d3FF0000000000000;
	and.pred  	%p33, %p31, %p32;
	mov.u32 	%r396, 1;
	@%p33 bra 	$L__BB6_21;

	setp.le.f64 	%p34, %fd32, 0d0000000000000000;
	setp.ge.f64 	%p35, %fd31, 0d3FF0000000000000;
	and.pred  	%p36, %p34, %p35;
	selp.b32 	%r396, 2, 6, %p36;

$L__BB6_21:
	setp.eq.s32 	%p37, %r396, 0;
	@%p37 bra 	$L__BB6_33;

	setp.eq.s32 	%p38, %r396, 1;
	@%p38 bra 	$L__BB6_32;
	bra.uni 	$L__BB6_23;

$L__BB6_32:
	sub.f64 	%fd507, %fd23, %fd4;
	sub.f64 	%fd508, %fd25, %fd7;
	mul.f64 	%fd509, %fd508, %fd508;
	fma.rn.f64 	%fd510, %fd507, %fd507, %fd509;
	sub.f64 	%fd511, %fd27, %fd10;
	fma.rn.f64 	%fd1147, %fd511, %fd511, %fd510;
	bra.uni 	$L__BB6_34;

$L__BB6_33:
	sub.f64 	%fd1119, %fd27, %fd11;
	sub.f64 	%fd1118, %fd23, %fd5;
	sub.f64 	%fd1117, %fd25, %fd8;
	mul.f64 	%fd512, %fd1117, %fd1117;
	fma.rn.f64 	%fd513, %fd1118, %fd1118, %fd512;
	fma.rn.f64 	%fd1147, %fd1119, %fd1119, %fd513;
	bra.uni 	$L__BB6_34;

$L__BB6_23:
	setp.eq.s32 	%p39, %r396, 2;
	@%p39 bra 	$L__BB6_31;
	bra.uni 	$L__BB6_24;

$L__BB6_31:
	sub.f64 	%fd502, %fd23, %fd13;
	sub.f64 	%fd503, %fd25, %fd15;
	mul.f64 	%fd504, %fd503, %fd503;
	fma.rn.f64 	%fd505, %fd502, %fd502, %fd504;
	sub.f64 	%fd506, %fd27, %fd17;
	fma.rn.f64 	%fd1147, %fd506, %fd506, %fd505;
	bra.uni 	$L__BB6_34;

$L__BB6_24:
	setp.eq.s32 	%p40, %r396, 3;
	@%p40 bra 	$L__BB6_30;
	bra.uni 	$L__BB6_25;

$L__BB6_30:
	sub.f64 	%fd484, %fd5, %fd23;
	sub.f64 	%fd485, %fd10, %fd27;
	sub.f64 	%fd486, %fd8, %fd25;
	mul.f64 	%fd487, %fd486, %fd485;
	sub.f64 	%fd488, %fd7, %fd25;
	sub.f64 	%fd489, %fd11, %fd27;
	mul.f64 	%fd490, %fd489, %fd488;
	sub.f64 	%fd491, %fd487, %fd490;
	sub.f64 	%fd492, %fd4, %fd23;
	mul.f64 	%fd493, %fd489, %fd492;
	mul.f64 	%fd494, %fd484, %fd485;
	sub.f64 	%fd495, %fd493, %fd494;
	mul.f64 	%fd496, %fd484, %fd488;
	mul.f64 	%fd497, %fd486, %fd492;
	sub.f64 	%fd498, %fd496, %fd497;
	mul.f64 	%fd499, %fd495, %fd495;
	fma.rn.f64 	%fd500, %fd491, %fd491, %fd499;
	fma.rn.f64 	%fd501, %fd498, %fd498, %fd500;
	div.rn.f64 	%fd1147, %fd501, %fd22;
	bra.uni 	$L__BB6_34;

$L__BB6_25:
	setp.eq.s32 	%p41, %r396, 4;
	@%p41 bra 	$L__BB6_29;
	bra.uni 	$L__BB6_26;

$L__BB6_29:
	sub.f64 	%fd460, %fd4, %fd23;
	sub.f64 	%fd461, %fd17, %fd27;
	sub.f64 	%fd462, %fd7, %fd25;
	mul.f64 	%fd463, %fd462, %fd461;
	sub.f64 	%fd464, %fd15, %fd25;
	sub.f64 	%fd465, %fd10, %fd27;
	mul.f64 	%fd466, %fd465, %fd464;
	sub.f64 	%fd467, %fd463, %fd466;
	sub.f64 	%fd468, %fd13, %fd23;
	mul.f64 	%fd469, %fd465, %fd468;
	mul.f64 	%fd470, %fd460, %fd461;
	sub.f64 	%fd471, %fd469, %fd470;
	mul.f64 	%fd472, %fd460, %fd464;
	mul.f64 	%fd473, %fd462, %fd468;
	sub.f64 	%fd474, %fd472, %fd473;
	mul.f64 	%fd475, %fd471, %fd471;
	fma.rn.f64 	%fd476, %fd467, %fd467, %fd475;
	fma.rn.f64 	%fd477, %fd474, %fd474, %fd476;
	sub.f64 	%fd478, %fd13, %fd4;
	sub.f64 	%fd479, %fd15, %fd7;
	mul.f64 	%fd480, %fd479, %fd479;
	fma.rn.f64 	%fd481, %fd478, %fd478, %fd480;
	sub.f64 	%fd482, %fd17, %fd10;
	fma.rn.f64 	%fd483, %fd482, %fd482, %fd481;
	div.rn.f64 	%fd1147, %fd477, %fd483;
	bra.uni 	$L__BB6_34;

$L__BB6_26:
	setp.eq.s32 	%p42, %r396, 5;
	@%p42 bra 	$L__BB6_28;
	bra.uni 	$L__BB6_27;

$L__BB6_28:
	sub.f64 	%fd436, %fd13, %fd23;
	sub.f64 	%fd437, %fd11, %fd27;
	sub.f64 	%fd438, %fd15, %fd25;
	mul.f64 	%fd439, %fd437, %fd438;
	sub.f64 	%fd440, %fd8, %fd25;
	sub.f64 	%fd441, %fd17, %fd27;
	mul.f64 	%fd442, %fd440, %fd441;
	sub.f64 	%fd443, %fd439, %fd442;
	sub.f64 	%fd444, %fd5, %fd23;
	mul.f64 	%fd445, %fd444, %fd441;
	mul.f64 	%fd446, %fd437, %fd436;
	sub.f64 	%fd447, %fd445, %fd446;
	mul.f64 	%fd448, %fd440, %fd436;
	mul.f64 	%fd449, %fd444, %fd438;
	sub.f64 	%fd450, %fd448, %fd449;
	mul.f64 	%fd451, %fd447, %fd447;
	fma.rn.f64 	%fd452, %fd443, %fd443, %fd451;
	fma.rn.f64 	%fd453, %fd450, %fd450, %fd452;
	sub.f64 	%fd454, %fd5, %fd13;
	sub.f64 	%fd455, %fd8, %fd15;
	mul.f64 	%fd456, %fd455, %fd455;
	fma.rn.f64 	%fd457, %fd454, %fd454, %fd456;
	sub.f64 	%fd458, %fd11, %fd17;
	fma.rn.f64 	%fd459, %fd458, %fd458, %fd457;
	div.rn.f64 	%fd1147, %fd453, %fd459;
	bra.uni 	$L__BB6_34;

$L__BB6_27:
	sub.f64 	%fd1101, %fd27, %fd11;
	sub.f64 	%fd1100, %fd23, %fd5;
	sub.f64 	%fd1099, %fd25, %fd8;
	mul.f64 	%fd429, %fd1099, %fd20;
	fma.rn.f64 	%fd430, %fd1100, %fd19, %fd429;
	fma.rn.f64 	%fd431, %fd1101, %fd21, %fd430;
	mul.f64 	%fd432, %fd431, %fd431;
	mul.f64 	%fd433, %fd20, %fd20;
	fma.rn.f64 	%fd434, %fd19, %fd19, %fd433;
	fma.rn.f64 	%fd435, %fd21, %fd21, %fd434;
	div.rn.f64 	%fd1147, %fd432, %fd435;

$L__BB6_34:
	mul.f64 	%fd514, %fd3, %fd3;
	sub.f64 	%fd41, %fd1147, %fd514;
	fma.rn.f64 	%fd42, %fd2, %fd3, %fd1;
	setp.geu.f64 	%p43, %fd41, %fd42;
	@%p43 bra 	$L__BB6_43;

	mul.lo.s64 	%rd136, %rd55, %rd35;
	add.s64 	%rd137, %rd12, %rd136;
	ld.global.f64 	%fd43, [%rd137];
	div.rn.f64 	%fd1148, %fd41, %fd42;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r397}, %fd1148;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r398, %temp}, %fd1148;
	}
	setp.gt.s32 	%p44, %r397, 1048575;
	mov.u32 	%r399, -1023;
	@%p44 bra 	$L__BB6_37;

	mul.f64 	%fd1148, %fd1148, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r397}, %fd1148;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r398, %temp}, %fd1148;
	}
	mov.u32 	%r399, -1077;

$L__BB6_37:
	add.s32 	%r345, %r397, -1;
	setp.lt.u32 	%p45, %r345, 2146435071;
	@%p45 bra 	$L__BB6_39;
	bra.uni 	$L__BB6_38;

$L__BB6_39:
	shr.u32 	%r347, %r397, 20;
	add.s32 	%r400, %r399, %r347;
	and.b32  	%r348, %r397, -2146435073;
	or.b32  	%r349, %r348, 1072693248;
	mov.b64 	%fd1149, {%r398, %r349};
	setp.lt.s32 	%p47, %r349, 1073127583;
	@%p47 bra 	$L__BB6_41;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r350, %temp}, %fd1149;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r351}, %fd1149;
	}
	add.s32 	%r352, %r351, -1048576;
	mov.b64 	%fd1149, {%r350, %r352};
	add.s32 	%r400, %r400, 1;

$L__BB6_41:
	add.f64 	%fd517, %fd1149, 0d3FF0000000000000;
	mov.f64 	%fd518, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd519, %fd517;
	neg.f64 	%fd520, %fd517;
	fma.rn.f64 	%fd521, %fd520, %fd519, %fd518;
	fma.rn.f64 	%fd522, %fd521, %fd521, %fd521;
	fma.rn.f64 	%fd523, %fd522, %fd519, %fd519;
	add.f64 	%fd524, %fd1149, 0dBFF0000000000000;
	mul.f64 	%fd525, %fd524, %fd523;
	fma.rn.f64 	%fd526, %fd524, %fd523, %fd525;
	mul.f64 	%fd527, %fd526, %fd526;
	mov.f64 	%fd528, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd529, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd530, %fd529, %fd527, %fd528;
	mov.f64 	%fd531, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd532, %fd530, %fd527, %fd531;
	mov.f64 	%fd533, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd534, %fd532, %fd527, %fd533;
	mov.f64 	%fd535, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd536, %fd534, %fd527, %fd535;
	mov.f64 	%fd537, 0d3F624924923BE72D;
	fma.rn.f64 	%fd538, %fd536, %fd527, %fd537;
	mov.f64 	%fd539, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd540, %fd538, %fd527, %fd539;
	mov.f64 	%fd541, 0d3FB5555555555554;
	fma.rn.f64 	%fd542, %fd540, %fd527, %fd541;
	sub.f64 	%fd543, %fd524, %fd526;
	add.f64 	%fd544, %fd543, %fd543;
	neg.f64 	%fd545, %fd526;
	fma.rn.f64 	%fd546, %fd545, %fd524, %fd544;
	mul.f64 	%fd547, %fd523, %fd546;
	mul.f64 	%fd548, %fd527, %fd542;
	fma.rn.f64 	%fd549, %fd548, %fd526, %fd547;
	xor.b32  	%r353, %r400, -2147483648;
	mov.u32 	%r354, -2147483648;
	mov.u32 	%r355, 1127219200;
	mov.b64 	%fd550, {%r353, %r355};
	mov.b64 	%fd551, {%r354, %r355};
	sub.f64 	%fd552, %fd550, %fd551;
	mov.f64 	%fd553, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd554, %fd552, %fd553, %fd526;
	neg.f64 	%fd555, %fd552;
	fma.rn.f64 	%fd556, %fd555, %fd553, %fd554;
	sub.f64 	%fd557, %fd556, %fd526;
	sub.f64 	%fd558, %fd549, %fd557;
	mov.f64 	%fd559, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd560, %fd552, %fd559, %fd558;
	add.f64 	%fd1150, %fd554, %fd560;
	bra.uni 	$L__BB6_42;

$L__BB6_38:
	mov.f64 	%fd515, 0d7FF0000000000000;
	fma.rn.f64 	%fd516, %fd1148, %fd515, %fd515;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r346}, %fd1148;
	}
	mov.b32 	%f1, %r346;
	setp.eq.f32 	%p46, %f1, 0f00000000;
	selp.f64 	%fd1150, 0dFFF0000000000000, %fd516, %p46;

$L__BB6_42:
	sub.f64 	%fd561, %fd41, %fd42;
	div.rn.f64 	%fd562, %fd561, %fd42;
	mul.f64 	%fd563, %fd562, %fd1150;
	mul.f64 	%fd564, %fd563, 0dC000000000000000;
	div.rn.f64 	%fd565, %fd564, %fd42;
	mul.f64 	%fd566, %fd562, %fd562;
	div.rn.f64 	%fd567, %fd566, %fd41;
	sub.f64 	%fd568, %fd565, %fd567;
	mul.f64 	%fd569, %fd568, %fd307;
	mul.f64 	%fd570, %fd43, %fd308;
	mul.f64 	%fd571, %fd570, %fd306;
	mul.f64 	%fd572, %fd571, %fd569;
	mul.f64 	%fd573, %fd572, 0dC000000000000000;
	sqrt.rn.f64 	%fd574, %fd41;
	fma.rn.f64 	%fd575, %fd574, %fd573, 0d0000000000000000;
	st.global.f64 	[%rd50], %fd575;

$L__BB6_43:
	mov.u32 	%r401, 3;
	@%p1 bra 	$L__BB6_49;

	sub.f64 	%fd576, %fd13, %fd4;
	sub.f64 	%fd577, %fd15, %fd7;
	mul.f64 	%fd578, %fd577, %fd21;
	sub.f64 	%fd579, %fd17, %fd10;
	mul.f64 	%fd580, %fd579, %fd20;
	sub.f64 	%fd581, %fd578, %fd580;
	mul.f64 	%fd582, %fd579, %fd19;
	mul.f64 	%fd583, %fd576, %fd21;
	sub.f64 	%fd584, %fd582, %fd583;
	mul.f64 	%fd585, %fd576, %fd20;
	mul.f64 	%fd586, %fd577, %fd19;
	sub.f64 	%fd587, %fd585, %fd586;
	mul.f64 	%fd588, %fd577, %fd577;
	fma.rn.f64 	%fd589, %fd576, %fd576, %fd588;
	fma.rn.f64 	%fd590, %fd579, %fd579, %fd589;
	mul.f64 	%fd591, %fd577, %fd584;
	fma.rn.f64 	%fd592, %fd576, %fd581, %fd591;
	fma.rn.f64 	%fd593, %fd579, %fd587, %fd592;
	mul.f64 	%fd594, %fd584, %fd584;
	fma.rn.f64 	%fd595, %fd581, %fd581, %fd594;
	fma.rn.f64 	%fd596, %fd587, %fd587, %fd595;
	sub.f64 	%fd597, %fd23, %fd4;
	sub.f64 	%fd598, %fd25, %fd7;
	mul.f64 	%fd599, %fd598, %fd577;
	fma.rn.f64 	%fd600, %fd597, %fd576, %fd599;
	sub.f64 	%fd601, %fd27, %fd10;
	fma.rn.f64 	%fd602, %fd601, %fd579, %fd600;
	mul.f64 	%fd603, %fd598, %fd584;
	fma.rn.f64 	%fd604, %fd597, %fd581, %fd603;
	fma.rn.f64 	%fd605, %fd601, %fd587, %fd604;
	div.rn.f64 	%fd606, %fd593, %fd590;
	mul.f64 	%fd607, %fd606, %fd606;
	mul.f64 	%fd608, %fd590, %fd607;
	sub.f64 	%fd609, %fd596, %fd608;
	mul.f64 	%fd610, %fd602, %fd606;
	sub.f64 	%fd611, %fd605, %fd610;
	div.rn.f64 	%fd612, %fd611, %fd609;
	mul.f64 	%fd613, %fd590, %fd606;
	mul.f64 	%fd614, %fd613, %fd612;
	sub.f64 	%fd615, %fd602, %fd614;
	div.rn.f64 	%fd53, %fd615, %fd590;
	setp.gt.f64 	%p48, %fd53, 0d0000000000000000;
	setp.lt.f64 	%p49, %fd53, 0d3FF0000000000000;
	setp.ge.f64 	%p50, %fd612, 0d0000000000000000;
	and.pred  	%p51, %p48, %p49;
	and.pred  	%p52, %p50, %p51;
	mov.u32 	%r401, 4;
	@%p52 bra 	$L__BB6_49;

	sub.f64 	%fd616, %fd5, %fd13;
	sub.f64 	%fd617, %fd8, %fd15;
	mul.f64 	%fd618, %fd617, %fd21;
	sub.f64 	%fd619, %fd11, %fd17;
	mul.f64 	%fd620, %fd619, %fd20;
	sub.f64 	%fd621, %fd618, %fd620;
	mul.f64 	%fd622, %fd619, %fd19;
	mul.f64 	%fd623, %fd616, %fd21;
	sub.f64 	%fd624, %fd622, %fd623;
	mul.f64 	%fd625, %fd616, %fd20;
	mul.f64 	%fd626, %fd617, %fd19;
	sub.f64 	%fd627, %fd625, %fd626;
	mul.f64 	%fd628, %fd617, %fd617;
	fma.rn.f64 	%fd629, %fd616, %fd616, %fd628;
	fma.rn.f64 	%fd630, %fd619, %fd619, %fd629;
	mul.f64 	%fd631, %fd617, %fd624;
	fma.rn.f64 	%fd632, %fd616, %fd621, %fd631;
	fma.rn.f64 	%fd633, %fd619, %fd627, %fd632;
	mul.f64 	%fd634, %fd624, %fd624;
	fma.rn.f64 	%fd635, %fd621, %fd621, %fd634;
	fma.rn.f64 	%fd636, %fd627, %fd627, %fd635;
	sub.f64 	%fd637, %fd23, %fd13;
	sub.f64 	%fd638, %fd25, %fd15;
	mul.f64 	%fd639, %fd617, %fd638;
	fma.rn.f64 	%fd640, %fd616, %fd637, %fd639;
	sub.f64 	%fd641, %fd27, %fd17;
	fma.rn.f64 	%fd642, %fd619, %fd641, %fd640;
	mul.f64 	%fd643, %fd638, %fd624;
	fma.rn.f64 	%fd644, %fd637, %fd621, %fd643;
	fma.rn.f64 	%fd645, %fd641, %fd627, %fd644;
	div.rn.f64 	%fd646, %fd633, %fd630;
	mul.f64 	%fd647, %fd646, %fd646;
	mul.f64 	%fd648, %fd630, %fd647;
	sub.f64 	%fd649, %fd636, %fd648;
	mul.f64 	%fd650, %fd642, %fd646;
	sub.f64 	%fd651, %fd645, %fd650;
	div.rn.f64 	%fd652, %fd651, %fd649;
	mul.f64 	%fd653, %fd630, %fd646;
	mul.f64 	%fd654, %fd653, %fd652;
	sub.f64 	%fd655, %fd642, %fd654;
	div.rn.f64 	%fd54, %fd655, %fd630;
	setp.gt.f64 	%p53, %fd54, 0d0000000000000000;
	setp.lt.f64 	%p54, %fd54, 0d3FF0000000000000;
	setp.ge.f64 	%p55, %fd652, 0d0000000000000000;
	and.pred  	%p56, %p53, %p54;
	and.pred  	%p57, %p55, %p56;
	mov.u32 	%r401, 5;
	@%p57 bra 	$L__BB6_49;

	setp.le.f64 	%p58, %fd30, 0d0000000000000000;
	setp.ge.f64 	%p59, %fd54, 0d3FF0000000000000;
	and.pred  	%p60, %p58, %p59;
	mov.u32 	%r401, 0;
	@%p60 bra 	$L__BB6_49;

	setp.le.f64 	%p61, %fd53, 0d0000000000000000;
	setp.ge.f64 	%p62, %fd30, 0d3FF0000000000000;
	and.pred  	%p63, %p61, %p62;
	mov.u32 	%r401, 1;
	@%p63 bra 	$L__BB6_49;

	setp.le.f64 	%p64, %fd54, 0d0000000000000000;
	setp.ge.f64 	%p65, %fd53, 0d3FF0000000000000;
	and.pred  	%p66, %p64, %p65;
	selp.b32 	%r401, 2, 6, %p66;

$L__BB6_49:
	setp.eq.s32 	%p67, %r401, 0;
	mov.f64 	%fd1175, 0d0000000000000000;
	mov.f64 	%fd1176, %fd1175;
	@%p67 bra 	$L__BB6_72;

	setp.eq.s32 	%p68, %r401, 1;
	selp.f64 	%fd1171, 0d3FF0000000000000, 0d0000000000000000, %p68;
	mov.f64 	%fd1172, 0d0000000000000000;
	@%p68 bra 	$L__BB6_70;
	bra.uni 	$L__BB6_51;

$L__BB6_70:
	mov.f64 	%fd1173, 0d0000000000000000;
	mov.f64 	%fd1174, %fd1173;
	bra.uni 	$L__BB6_71;

$L__BB6_51:
	setp.eq.s32 	%p69, %r401, 2;
	selp.f64 	%fd658, 0d3FF0000000000000, 0d0000000000000000, %p69;
	selp.f64 	%fd1168, %fd658, %fd1172, %p69;
	selp.f64 	%fd1167, 0d0000000000000000, %fd1171, %p69;
	@%p69 bra 	$L__BB6_68;
	bra.uni 	$L__BB6_52;

$L__BB6_68:
	mov.f64 	%fd1169, 0d0000000000000000;
	mov.f64 	%fd1170, %fd1169;
	bra.uni 	$L__BB6_69;

$L__BB6_52:
	setp.ne.s32 	%p70, %r401, 3;
	mov.f64 	%fd1151, 0d0000000000000000;
	@%p70 bra 	$L__BB6_54;

	div.rn.f64 	%fd1151, %fd29, %fd22;

$L__BB6_54:
	setp.eq.s32 	%p71, %r401, 3;
	selp.f64 	%fd1163, %fd1151, %fd1167, %p71;
	selp.f64 	%fd1164, 0d0000000000000000, %fd1168, %p71;
	@%p71 bra 	$L__BB6_66;
	bra.uni 	$L__BB6_55;

$L__BB6_66:
	mov.f64 	%fd1165, 0d0000000000000000;
	mov.f64 	%fd1166, %fd1165;
	bra.uni 	$L__BB6_67;

$L__BB6_55:
	setp.ne.s32 	%p72, %r401, 4;
	mov.f64 	%fd1152, 0d0000000000000000;
	mov.f64 	%fd1153, %fd1152;
	@%p72 bra 	$L__BB6_57;

	sub.f64 	%fd662, %fd23, %fd4;
	sub.f64 	%fd663, %fd13, %fd4;
	sub.f64 	%fd664, %fd15, %fd7;
	sub.f64 	%fd665, %fd25, %fd7;
	mul.f64 	%fd666, %fd665, %fd664;
	fma.rn.f64 	%fd667, %fd662, %fd663, %fd666;
	sub.f64 	%fd668, %fd17, %fd10;
	sub.f64 	%fd669, %fd27, %fd10;
	fma.rn.f64 	%fd670, %fd669, %fd668, %fd667;
	mul.f64 	%fd671, %fd664, %fd664;
	fma.rn.f64 	%fd672, %fd663, %fd663, %fd671;
	fma.rn.f64 	%fd673, %fd668, %fd668, %fd672;
	div.rn.f64 	%fd1153, %fd670, %fd673;
	mov.f64 	%fd674, 0d3FF0000000000000;
	sub.f64 	%fd1152, %fd674, %fd1153;

$L__BB6_57:
	setp.eq.s32 	%p73, %r401, 4;
	selp.f64 	%fd1159, %fd1152, %fd1163, %p73;
	selp.f64 	%fd1160, %fd1153, %fd1164, %p73;
	@%p73 bra 	$L__BB6_64;
	bra.uni 	$L__BB6_58;

$L__BB6_64:
	mov.f64 	%fd1161, 0d0000000000000000;
	mov.f64 	%fd1162, %fd1161;
	bra.uni 	$L__BB6_65;

$L__BB6_58:
	setp.ne.s32 	%p74, %r401, 5;
	mov.f64 	%fd1154, 0d0000000000000000;
	@%p74 bra 	$L__BB6_60;

	sub.f64 	%fd1116, %fd17, %fd11;
	sub.f64 	%fd1115, %fd13, %fd5;
	sub.f64 	%fd1114, %fd15, %fd8;
	sub.f64 	%fd1113, %fd27, %fd11;
	sub.f64 	%fd1112, %fd23, %fd5;
	sub.f64 	%fd1111, %fd25, %fd8;
	mul.f64 	%fd676, %fd1111, %fd1114;
	fma.rn.f64 	%fd677, %fd1112, %fd1115, %fd676;
	fma.rn.f64 	%fd678, %fd1113, %fd1116, %fd677;
	mul.f64 	%fd679, %fd1114, %fd1114;
	fma.rn.f64 	%fd680, %fd1115, %fd1115, %fd679;
	fma.rn.f64 	%fd681, %fd1116, %fd1116, %fd680;
	div.rn.f64 	%fd1154, %fd678, %fd681;

$L__BB6_60:
	setp.eq.s32 	%p75, %r401, 5;
	selp.f64 	%fd1156, %fd1154, %fd1160, %p75;
	selp.f64 	%fd1155, 0d0000000000000000, %fd1159, %p75;
	@%p75 bra 	$L__BB6_62;
	bra.uni 	$L__BB6_61;

$L__BB6_62:
	mov.f64 	%fd1157, 0d0000000000000000;
	mov.f64 	%fd1158, %fd1157;
	bra.uni 	$L__BB6_63;

$L__BB6_61:
	sub.f64 	%fd1110, %fd10, %fd11;
	sub.f64 	%fd1109, %fd4, %fd5;
	sub.f64 	%fd1108, %fd7, %fd8;
	sub.f64 	%fd1107, %fd17, %fd11;
	sub.f64 	%fd1106, %fd13, %fd5;
	sub.f64 	%fd1105, %fd15, %fd8;
	sub.f64 	%fd1104, %fd27, %fd11;
	sub.f64 	%fd1103, %fd23, %fd5;
	sub.f64 	%fd1102, %fd25, %fd8;
	mul.f64 	%fd682, %fd1108, %fd1105;
	fma.rn.f64 	%fd683, %fd1109, %fd1106, %fd682;
	fma.rn.f64 	%fd684, %fd1110, %fd1107, %fd683;
	mul.f64 	%fd685, %fd1105, %fd1105;
	fma.rn.f64 	%fd686, %fd1106, %fd1106, %fd685;
	fma.rn.f64 	%fd687, %fd1107, %fd1107, %fd686;
	mul.f64 	%fd688, %fd1102, %fd1105;
	fma.rn.f64 	%fd689, %fd1103, %fd1106, %fd688;
	fma.rn.f64 	%fd690, %fd1104, %fd1107, %fd689;
	div.rn.f64 	%fd691, %fd684, %fd22;
	mul.f64 	%fd692, %fd691, %fd691;
	mul.f64 	%fd693, %fd22, %fd692;
	sub.f64 	%fd694, %fd687, %fd693;
	mul.f64 	%fd695, %fd29, %fd691;
	sub.f64 	%fd696, %fd690, %fd695;
	div.rn.f64 	%fd1158, %fd696, %fd694;
	mul.f64 	%fd697, %fd22, %fd691;
	mul.f64 	%fd698, %fd697, %fd1158;
	sub.f64 	%fd699, %fd29, %fd698;
	div.rn.f64 	%fd1157, %fd699, %fd22;

$L__BB6_63:
	selp.f64 	%fd1161, %fd1155, %fd1157, %p75;
	selp.f64 	%fd1162, %fd1156, %fd1158, %p75;

$L__BB6_65:
	selp.f64 	%fd1165, %fd1159, %fd1161, %p73;
	selp.f64 	%fd1166, %fd1160, %fd1162, %p73;

$L__BB6_67:
	selp.f64 	%fd1169, %fd1163, %fd1165, %p71;
	selp.f64 	%fd1170, %fd1164, %fd1166, %p71;

$L__BB6_69:
	selp.f64 	%fd1173, %fd1167, %fd1169, %p69;
	selp.f64 	%fd1174, %fd1168, %fd1170, %p69;

$L__BB6_71:
	selp.f64 	%fd1175, %fd1171, %fd1173, %p68;
	selp.f64 	%fd1176, %fd1172, %fd1174, %p68;

$L__BB6_72:
	selp.f64 	%fd710, 0d0000000000000000, %fd1176, %p67;
	selp.f64 	%fd711, 0d0000000000000000, %fd1175, %p67;
	mov.f64 	%fd712, 0d3FF0000000000000;
	sub.f64 	%fd713, %fd712, %fd711;
	sub.f64 	%fd714, %fd713, %fd710;
	mul.f64 	%fd715, %fd5, %fd714;
	mul.f64 	%fd716, %fd8, %fd714;
	mul.f64 	%fd717, %fd11, %fd714;
	fma.rn.f64 	%fd718, %fd4, %fd711, %fd715;
	fma.rn.f64 	%fd719, %fd7, %fd711, %fd716;
	fma.rn.f64 	%fd720, %fd10, %fd711, %fd717;
	fma.rn.f64 	%fd721, %fd13, %fd710, %fd718;
	fma.rn.f64 	%fd722, %fd15, %fd710, %fd719;
	fma.rn.f64 	%fd723, %fd17, %fd710, %fd720;
	sub.f64 	%fd724, %fd23, %fd721;
	sub.f64 	%fd725, %fd25, %fd722;
	sub.f64 	%fd726, %fd27, %fd723;
	mul.f64 	%fd727, %fd725, %fd725;
	fma.rn.f64 	%fd728, %fd724, %fd724, %fd727;
	fma.rn.f64 	%fd729, %fd726, %fd726, %fd728;
	sqrt.rn.f64 	%fd730, %fd729;
	div.rn.f64 	%fd731, %fd724, %fd730;
	div.rn.f64 	%fd732, %fd725, %fd730;
	div.rn.f64 	%fd733, %fd726, %fd730;
	st.global.f64 	[%rd53], %fd711;
	st.global.f64 	[%rd53+8], %fd710;
	st.global.f64 	[%rd54], %fd731;
	st.global.f64 	[%rd54+8], %fd732;
	st.global.f64 	[%rd54+16], %fd733;

$L__BB6_73:
	setp.lt.u32 	%p82, %r2, 2;
	@%p82 bra 	$L__BB6_159;

	ld.global.s32 	%rd56, [%rd52];
	mul.lo.s64 	%rd138, %rd56, %rd33;
	add.s64 	%rd139, %rd10, %rd138;
	ld.global.s32 	%rd57, [%rd51];
	mul.lo.s64 	%rd140, %rd57, %rd33;
	add.s64 	%rd141, %rd10, %rd140;
	mul.lo.s64 	%rd142, %rd56, %rd34;
	add.s64 	%rd143, %rd14, %rd142;
	mul.lo.s64 	%rd144, %rd57, %rd34;
	add.s64 	%rd145, %rd14, %rd144;
	ld.global.f64 	%fd735, [%rd145];
	ld.global.f64 	%fd736, [%rd143];
	add.f64 	%fd127, %fd736, %fd735;
	ld.global.s32 	%rd58, [%rd139];
	mul.lo.s64 	%rd146, %rd58, %rd31;
	add.s64 	%rd147, %rd4, %rd146;
	ld.global.s32 	%rd59, [%rd139+4];
	mul.lo.s64 	%rd148, %rd59, %rd31;
	add.s64 	%rd149, %rd4, %rd148;
	ld.global.s32 	%rd60, [%rd141];
	mul.lo.s64 	%rd150, %rd60, %rd31;
	add.s64 	%rd151, %rd4, %rd150;
	ld.global.s32 	%rd61, [%rd141+4];
	mul.lo.s64 	%rd152, %rd61, %rd31;
	add.s64 	%rd153, %rd4, %rd152;
	ld.global.f64 	%fd128, [%rd149];
	ld.global.f64 	%fd129, [%rd147];
	sub.f64 	%fd130, %fd128, %fd129;
	ld.global.f64 	%fd131, [%rd149+8];
	ld.global.f64 	%fd132, [%rd147+8];
	sub.f64 	%fd133, %fd131, %fd132;
	ld.global.f64 	%fd134, [%rd149+16];
	ld.global.f64 	%fd135, [%rd147+16];
	sub.f64 	%fd136, %fd134, %fd135;
	ld.global.f64 	%fd137, [%rd153];
	ld.global.f64 	%fd138, [%rd151];
	sub.f64 	%fd139, %fd137, %fd138;
	ld.global.f64 	%fd140, [%rd153+8];
	ld.global.f64 	%fd141, [%rd151+8];
	sub.f64 	%fd142, %fd140, %fd141;
	ld.global.f64 	%fd143, [%rd153+16];
	ld.global.f64 	%fd144, [%rd151+16];
	sub.f64 	%fd145, %fd143, %fd144;
	sub.f64 	%fd146, %fd129, %fd138;
	sub.f64 	%fd147, %fd132, %fd141;
	sub.f64 	%fd148, %fd135, %fd144;
	mul.f64 	%fd737, %fd133, %fd133;
	fma.rn.f64 	%fd738, %fd130, %fd130, %fd737;
	fma.rn.f64 	%fd149, %fd136, %fd136, %fd738;
	mul.f64 	%fd739, %fd133, %fd142;
	fma.rn.f64 	%fd740, %fd130, %fd139, %fd739;
	fma.rn.f64 	%fd150, %fd136, %fd145, %fd740;
	mul.f64 	%fd741, %fd142, %fd142;
	fma.rn.f64 	%fd742, %fd139, %fd139, %fd741;
	fma.rn.f64 	%fd151, %fd145, %fd145, %fd742;
	mul.f64 	%fd743, %fd133, %fd147;
	fma.rn.f64 	%fd744, %fd130, %fd146, %fd743;
	fma.rn.f64 	%fd152, %fd136, %fd148, %fd744;
	mul.f64 	%fd745, %fd147, %fd142;
	fma.rn.f64 	%fd746, %fd146, %fd139, %fd745;
	fma.rn.f64 	%fd153, %fd148, %fd145, %fd746;
	mul.f64 	%fd747, %fd149, %fd151;
	mul.f64 	%fd748, %fd150, %fd150;
	sub.f64 	%fd154, %fd747, %fd748;
	mul.f64 	%fd749, %fd150, %fd153;
	mul.f64 	%fd750, %fd152, %fd151;
	sub.f64 	%fd155, %fd749, %fd750;
	setp.le.f64 	%p83, %fd155, 0d0000000000000000;
	@%p83 bra 	$L__BB6_78;

	setp.ge.f64 	%p2, %fd155, %fd154;
	add.f64 	%fd156, %fd153, %fd150;
	@%p2 bra 	$L__BB6_77;

	setp.le.f64 	%p153, %fd155, 0d0000000000000000;
	selp.b32 	%r394, 2, 8, %p153;
	sub.f64 	%fd1125, %fd135, %fd144;
	sub.f64 	%fd1124, %fd129, %fd138;
	sub.f64 	%fd1123, %fd132, %fd141;
	sub.f64 	%fd1122, %fd137, %fd138;
	sub.f64 	%fd1121, %fd143, %fd144;
	sub.f64 	%fd1120, %fd140, %fd141;
	selp.f64 	%fd752, %fd151, %fd154, %p2;
	mul.f64 	%fd753, %fd149, %fd153;
	mul.f64 	%fd754, %fd152, %fd150;
	sub.f64 	%fd755, %fd753, %fd754;
	mul.f64 	%fd756, %fd136, %fd1120;
	mul.f64 	%fd757, %fd133, %fd1121;
	sub.f64 	%fd758, %fd757, %fd756;
	mul.f64 	%fd759, %fd130, %fd1121;
	mul.f64 	%fd760, %fd136, %fd1122;
	sub.f64 	%fd761, %fd760, %fd759;
	mul.f64 	%fd762, %fd133, %fd1122;
	mul.f64 	%fd763, %fd130, %fd1120;
	sub.f64 	%fd764, %fd763, %fd762;
	setp.gt.f64 	%p84, %fd755, 0d0000000000000000;
	setp.lt.f64 	%p85, %fd755, %fd752;
	mul.f64 	%fd765, %fd1123, %fd761;
	fma.rn.f64 	%fd766, %fd1124, %fd758, %fd765;
	fma.rn.f64 	%fd767, %fd1125, %fd764, %fd766;
	setp.eq.f64 	%p86, %fd767, 0d0000000000000000;
	mul.f64 	%fd768, %fd761, %fd761;
	fma.rn.f64 	%fd769, %fd758, %fd758, %fd768;
	fma.rn.f64 	%fd770, %fd764, %fd764, %fd769;
	mul.f64 	%fd771, %fd149, 0d3BC79CA100000000;
	mul.f64 	%fd772, %fd771, %fd151;
	setp.lt.f64 	%p87, %fd770, %fd772;
	or.pred  	%p88, %p86, %p87;
	and.pred  	%p89, %p84, %p85;
	and.pred  	%p90, %p88, %p89;
	mul.f64 	%fd773, %fd154, 0d3FE0000000000000;
	setp.lt.f64 	%p91, %fd155, %fd773;
	selp.b32 	%r363, 2, 5, %p91;
	selp.f64 	%fd774, %fd153, %fd156, %p91;
	selp.f64 	%fd1178, %fd151, %fd752, %p90;
	selp.b32 	%r364, 5, %r394, %p2;
	selp.b32 	%r402, %r363, %r364, %p90;
	selp.f64 	%fd1177, %fd774, %fd755, %p90;

$L__BB6_77:
	selp.f64 	%fd1180, %fd151, %fd1178, %p2;
	selp.b32 	%r403, 5, %r402, %p2;
	selp.f64 	%fd1179, %fd156, %fd1177, %p2;

$L__BB6_78:
	selp.f64 	%fd165, %fd151, %fd1180, %p83;
	selp.b32 	%r404, 2, %r403, %p83;
	selp.f64 	%fd166, %fd153, %fd1179, %p83;
	setp.gtu.f64 	%p94, %fd166, 0d0000000000000000;
	@%p94 bra 	$L__BB6_82;
	bra.uni 	$L__BB6_79;

$L__BB6_82:
	setp.ltu.f64 	%p97, %fd166, %fd165;
	@%p97 bra 	$L__BB6_86;

	mov.f64 	%fd776, 0d0000000000000000;
	sub.f64 	%fd777, %fd776, %fd152;
	add.f64 	%fd168, %fd777, %fd150;
	setp.le.f64 	%p98, %fd168, 0d0000000000000000;
	mov.u32 	%r404, 1;
	@%p98 bra 	$L__BB6_86;

	setp.ge.f64 	%p99, %fd168, %fd149;
	mov.u32 	%r404, 4;
	@%p99 bra 	$L__BB6_86;

	mov.u32 	%r404, 7;
	bra.uni 	$L__BB6_86;

$L__BB6_79:
	mov.f64 	%fd775, 0d0000000000000000;
	sub.f64 	%fd167, %fd775, %fd152;
	setp.le.f64 	%p95, %fd167, 0d0000000000000000;
	mov.u32 	%r404, 0;
	@%p95 bra 	$L__BB6_86;

	setp.ge.f64 	%p96, %fd167, %fd149;
	mov.u32 	%r404, 3;
	@%p96 bra 	$L__BB6_86;

	mov.u32 	%r404, 6;

$L__BB6_86:
	setp.eq.s32 	%p100, %r404, 0;
	@%p100 bra 	$L__BB6_102;

	setp.eq.s32 	%p101, %r404, 1;
	@%p101 bra 	$L__BB6_101;
	bra.uni 	$L__BB6_88;

$L__BB6_101:
	sub.f64 	%fd876, %fd129, %fd137;
	sub.f64 	%fd877, %fd132, %fd140;
	mul.f64 	%fd878, %fd877, %fd877;
	fma.rn.f64 	%fd879, %fd876, %fd876, %fd878;
	sub.f64 	%fd880, %fd135, %fd143;
	fma.rn.f64 	%fd1181, %fd880, %fd880, %fd879;
	bra.uni 	$L__BB6_103;

$L__BB6_102:
	sub.f64 	%fd1146, %fd135, %fd144;
	sub.f64 	%fd1145, %fd129, %fd138;
	sub.f64 	%fd1144, %fd132, %fd141;
	mul.f64 	%fd881, %fd1144, %fd1144;
	fma.rn.f64 	%fd882, %fd1145, %fd1145, %fd881;
	fma.rn.f64 	%fd1181, %fd1146, %fd1146, %fd882;
	bra.uni 	$L__BB6_103;

$L__BB6_88:
	setp.eq.s32 	%p102, %r404, 2;
	@%p102 bra 	$L__BB6_100;
	bra.uni 	$L__BB6_89;

$L__BB6_100:
	sub.f64 	%fd858, %fd138, %fd129;
	sub.f64 	%fd859, %fd143, %fd135;
	sub.f64 	%fd860, %fd141, %fd132;
	mul.f64 	%fd861, %fd860, %fd859;
	sub.f64 	%fd862, %fd140, %fd132;
	sub.f64 	%fd863, %fd144, %fd135;
	mul.f64 	%fd864, %fd863, %fd862;
	sub.f64 	%fd865, %fd861, %fd864;
	sub.f64 	%fd866, %fd137, %fd129;
	mul.f64 	%fd867, %fd863, %fd866;
	mul.f64 	%fd868, %fd858, %fd859;
	sub.f64 	%fd869, %fd867, %fd868;
	mul.f64 	%fd870, %fd858, %fd862;
	mul.f64 	%fd871, %fd860, %fd866;
	sub.f64 	%fd872, %fd870, %fd871;
	mul.f64 	%fd873, %fd869, %fd869;
	fma.rn.f64 	%fd874, %fd865, %fd865, %fd873;
	fma.rn.f64 	%fd875, %fd872, %fd872, %fd874;
	div.rn.f64 	%fd1181, %fd875, %fd151;
	bra.uni 	$L__BB6_103;

$L__BB6_89:
	setp.eq.s32 	%p103, %r404, 3;
	@%p103 bra 	$L__BB6_99;
	bra.uni 	$L__BB6_90;

$L__BB6_99:
	sub.f64 	%fd853, %fd128, %fd138;
	sub.f64 	%fd854, %fd131, %fd141;
	mul.f64 	%fd855, %fd854, %fd854;
	fma.rn.f64 	%fd856, %fd853, %fd853, %fd855;
	sub.f64 	%fd857, %fd134, %fd144;
	fma.rn.f64 	%fd1181, %fd857, %fd857, %fd856;
	bra.uni 	$L__BB6_103;

$L__BB6_90:
	setp.eq.s32 	%p104, %r404, 4;
	@%p104 bra 	$L__BB6_98;
	bra.uni 	$L__BB6_91;

$L__BB6_98:
	sub.f64 	%fd848, %fd128, %fd137;
	sub.f64 	%fd849, %fd131, %fd140;
	mul.f64 	%fd850, %fd849, %fd849;
	fma.rn.f64 	%fd851, %fd848, %fd848, %fd850;
	sub.f64 	%fd852, %fd134, %fd143;
	fma.rn.f64 	%fd1181, %fd852, %fd852, %fd851;
	bra.uni 	$L__BB6_103;

$L__BB6_91:
	setp.eq.s32 	%p105, %r404, 5;
	@%p105 bra 	$L__BB6_97;
	bra.uni 	$L__BB6_92;

$L__BB6_97:
	sub.f64 	%fd830, %fd138, %fd128;
	sub.f64 	%fd831, %fd143, %fd134;
	sub.f64 	%fd832, %fd141, %fd131;
	mul.f64 	%fd833, %fd832, %fd831;
	sub.f64 	%fd834, %fd140, %fd131;
	sub.f64 	%fd835, %fd144, %fd134;
	mul.f64 	%fd836, %fd835, %fd834;
	sub.f64 	%fd837, %fd833, %fd836;
	sub.f64 	%fd838, %fd137, %fd128;
	mul.f64 	%fd839, %fd835, %fd838;
	mul.f64 	%fd840, %fd830, %fd831;
	sub.f64 	%fd841, %fd839, %fd840;
	mul.f64 	%fd842, %fd830, %fd834;
	mul.f64 	%fd843, %fd832, %fd838;
	sub.f64 	%fd844, %fd842, %fd843;
	mul.f64 	%fd845, %fd841, %fd841;
	fma.rn.f64 	%fd846, %fd837, %fd837, %fd845;
	fma.rn.f64 	%fd847, %fd844, %fd844, %fd846;
	div.rn.f64 	%fd1181, %fd847, %fd151;
	bra.uni 	$L__BB6_103;

$L__BB6_92:
	setp.eq.s32 	%p106, %r404, 6;
	@%p106 bra 	$L__BB6_96;
	bra.uni 	$L__BB6_93;

$L__BB6_96:
	sub.f64 	%fd1143, %fd135, %fd144;
	sub.f64 	%fd1142, %fd129, %fd138;
	sub.f64 	%fd1141, %fd132, %fd141;
	sub.f64 	%fd815, %fd128, %fd138;
	sub.f64 	%fd816, %fd134, %fd144;
	mul.f64 	%fd817, %fd1141, %fd816;
	sub.f64 	%fd818, %fd131, %fd141;
	mul.f64 	%fd819, %fd818, %fd1143;
	sub.f64 	%fd820, %fd817, %fd819;
	mul.f64 	%fd821, %fd815, %fd1143;
	mul.f64 	%fd822, %fd1142, %fd816;
	sub.f64 	%fd823, %fd821, %fd822;
	mul.f64 	%fd824, %fd1142, %fd818;
	mul.f64 	%fd825, %fd815, %fd1141;
	sub.f64 	%fd826, %fd824, %fd825;
	mul.f64 	%fd827, %fd823, %fd823;
	fma.rn.f64 	%fd828, %fd820, %fd820, %fd827;
	fma.rn.f64 	%fd829, %fd826, %fd826, %fd828;
	div.rn.f64 	%fd1181, %fd829, %fd149;
	bra.uni 	$L__BB6_103;

$L__BB6_93:
	setp.eq.s32 	%p107, %r404, 7;
	@%p107 bra 	$L__BB6_95;
	bra.uni 	$L__BB6_94;

$L__BB6_95:
	sub.f64 	%fd797, %fd129, %fd137;
	sub.f64 	%fd798, %fd134, %fd143;
	sub.f64 	%fd799, %fd132, %fd140;
	mul.f64 	%fd800, %fd799, %fd798;
	sub.f64 	%fd801, %fd131, %fd140;
	sub.f64 	%fd802, %fd135, %fd143;
	mul.f64 	%fd803, %fd801, %fd802;
	sub.f64 	%fd804, %fd800, %fd803;
	sub.f64 	%fd805, %fd128, %fd137;
	mul.f64 	%fd806, %fd805, %fd802;
	mul.f64 	%fd807, %fd797, %fd798;
	sub.f64 	%fd808, %fd806, %fd807;
	mul.f64 	%fd809, %fd797, %fd801;
	mul.f64 	%fd810, %fd805, %fd799;
	sub.f64 	%fd811, %fd809, %fd810;
	mul.f64 	%fd812, %fd808, %fd808;
	fma.rn.f64 	%fd813, %fd804, %fd804, %fd812;
	fma.rn.f64 	%fd814, %fd811, %fd811, %fd813;
	div.rn.f64 	%fd1181, %fd814, %fd149;
	bra.uni 	$L__BB6_103;

$L__BB6_94:
	sub.f64 	%fd1128, %fd137, %fd138;
	sub.f64 	%fd1127, %fd143, %fd144;
	sub.f64 	%fd1126, %fd140, %fd141;
	sub.f64 	%fd778, %fd138, %fd129;
	mul.f64 	%fd779, %fd136, %fd1126;
	mul.f64 	%fd780, %fd133, %fd1127;
	sub.f64 	%fd781, %fd780, %fd779;
	mul.f64 	%fd782, %fd130, %fd1127;
	mul.f64 	%fd783, %fd136, %fd1128;
	sub.f64 	%fd784, %fd783, %fd782;
	mul.f64 	%fd785, %fd133, %fd1128;
	mul.f64 	%fd786, %fd130, %fd1126;
	sub.f64 	%fd787, %fd786, %fd785;
	sub.f64 	%fd788, %fd141, %fd132;
	mul.f64 	%fd789, %fd788, %fd784;
	fma.rn.f64 	%fd790, %fd778, %fd781, %fd789;
	sub.f64 	%fd791, %fd144, %fd135;
	fma.rn.f64 	%fd792, %fd791, %fd787, %fd790;
	mul.f64 	%fd793, %fd792, %fd792;
	mul.f64 	%fd794, %fd784, %fd784;
	fma.rn.f64 	%fd795, %fd781, %fd781, %fd794;
	fma.rn.f64 	%fd796, %fd787, %fd787, %fd795;
	div.rn.f64 	%fd1181, %fd793, %fd796;

$L__BB6_103:
	mul.f64 	%fd883, %fd127, %fd127;
	sub.f64 	%fd179, %fd1181, %fd883;
	fma.rn.f64 	%fd180, %fd2, %fd127, %fd1;
	setp.geu.f64 	%p108, %fd179, %fd180;
	@%p108 bra 	$L__BB6_115;

	sub.f64 	%fd1140, %fd137, %fd138;
	sub.f64 	%fd1139, %fd143, %fd144;
	sub.f64 	%fd1138, %fd140, %fd141;
	mul.lo.s64 	%rd154, %rd58, %rd38;
	add.s64 	%rd155, %rd16, %rd154;
	mul.lo.s64 	%rd156, %rd59, %rd38;
	add.s64 	%rd157, %rd16, %rd156;
	mul.lo.s64 	%rd158, %rd60, %rd38;
	add.s64 	%rd159, %rd16, %rd158;
	mul.lo.s64 	%rd160, %rd61, %rd38;
	add.s64 	%rd161, %rd16, %rd160;
	ld.global.f64 	%fd885, [%rd157];
	ld.global.f64 	%fd886, [%rd155];
	sub.f64 	%fd887, %fd885, %fd886;
	ld.global.f64 	%fd888, [%rd157+8];
	ld.global.f64 	%fd889, [%rd155+8];
	sub.f64 	%fd890, %fd888, %fd889;
	ld.global.f64 	%fd891, [%rd157+16];
	ld.global.f64 	%fd892, [%rd155+16];
	sub.f64 	%fd893, %fd891, %fd892;
	ld.global.f64 	%fd894, [%rd161];
	ld.global.f64 	%fd895, [%rd159];
	sub.f64 	%fd896, %fd894, %fd895;
	ld.global.f64 	%fd897, [%rd161+8];
	ld.global.f64 	%fd898, [%rd159+8];
	sub.f64 	%fd899, %fd897, %fd898;
	ld.global.f64 	%fd900, [%rd161+16];
	ld.global.f64 	%fd901, [%rd159+16];
	sub.f64 	%fd902, %fd900, %fd901;
	mul.f64 	%fd903, %fd890, %fd890;
	fma.rn.f64 	%fd904, %fd887, %fd887, %fd903;
	fma.rn.f64 	%fd905, %fd893, %fd893, %fd904;
	mul.f64 	%fd906, %fd905, 0d3F50624DE0000000;
	mul.f64 	%fd907, %fd899, %fd899;
	fma.rn.f64 	%fd908, %fd896, %fd896, %fd907;
	fma.rn.f64 	%fd909, %fd902, %fd902, %fd908;
	mul.f64 	%fd181, %fd906, %fd909;
	mul.f64 	%fd910, %fd136, %fd1138;
	mul.f64 	%fd911, %fd133, %fd1139;
	sub.f64 	%fd912, %fd911, %fd910;
	mul.f64 	%fd913, %fd130, %fd1139;
	mul.f64 	%fd914, %fd136, %fd1140;
	sub.f64 	%fd915, %fd914, %fd913;
	mul.f64 	%fd916, %fd133, %fd1140;
	mul.f64 	%fd917, %fd130, %fd1138;
	sub.f64 	%fd918, %fd917, %fd916;
	mul.f64 	%fd919, %fd915, %fd915;
	fma.rn.f64 	%fd920, %fd912, %fd912, %fd919;
	fma.rn.f64 	%fd182, %fd918, %fd918, %fd920;
	setp.geu.f64 	%p109, %fd182, %fd181;
	mov.f64 	%fd1182, 0d3FF0000000000000;
	@%p109 bra 	$L__BB6_106;

	div.rn.f64 	%fd921, %fd182, %fd181;
	mov.f64 	%fd922, 0d0000000000000000;
	sub.f64 	%fd923, %fd922, %fd921;
	add.f64 	%fd924, %fd923, 0d4000000000000000;
	mul.f64 	%fd1182, %fd921, %fd924;

$L__BB6_106:
	setp.neu.f64 	%p110, %fd1182, 0d3FF0000000000000;
	@%p110 bra 	$L__BB6_115;

	mul.lo.s64 	%rd162, %rd56, %rd39;
	add.s64 	%rd163, %rd11, %rd162;
	mul.lo.s64 	%rd164, %rd57, %rd39;
	add.s64 	%rd165, %rd11, %rd164;
	ld.global.f64 	%fd925, [%rd165];
	ld.global.f64 	%fd926, [%rd163];
	add.f64 	%fd185, %fd926, %fd925;
	div.rn.f64 	%fd1183, %fd179, %fd180;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r405}, %fd1183;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r406, %temp}, %fd1183;
	}
	setp.gt.s32 	%p111, %r405, 1048575;
	mov.u32 	%r407, -1023;
	@%p111 bra 	$L__BB6_109;

	mul.f64 	%fd1183, %fd1183, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r405}, %fd1183;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r406, %temp}, %fd1183;
	}
	mov.u32 	%r407, -1077;

$L__BB6_109:
	add.s32 	%r373, %r405, -1;
	setp.lt.u32 	%p112, %r373, 2146435071;
	@%p112 bra 	$L__BB6_111;
	bra.uni 	$L__BB6_110;

$L__BB6_111:
	shr.u32 	%r375, %r405, 20;
	add.s32 	%r408, %r407, %r375;
	and.b32  	%r376, %r405, -2146435073;
	or.b32  	%r377, %r376, 1072693248;
	mov.b64 	%fd1184, {%r406, %r377};
	setp.lt.s32 	%p114, %r377, 1073127583;
	@%p114 bra 	$L__BB6_113;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r378, %temp}, %fd1184;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r379}, %fd1184;
	}
	add.s32 	%r380, %r379, -1048576;
	mov.b64 	%fd1184, {%r378, %r380};
	add.s32 	%r408, %r408, 1;

$L__BB6_113:
	add.f64 	%fd929, %fd1184, 0d3FF0000000000000;
	mov.f64 	%fd930, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd931, %fd929;
	neg.f64 	%fd932, %fd929;
	fma.rn.f64 	%fd933, %fd932, %fd931, %fd930;
	fma.rn.f64 	%fd934, %fd933, %fd933, %fd933;
	fma.rn.f64 	%fd935, %fd934, %fd931, %fd931;
	add.f64 	%fd936, %fd1184, 0dBFF0000000000000;
	mul.f64 	%fd937, %fd936, %fd935;
	fma.rn.f64 	%fd938, %fd936, %fd935, %fd937;
	mul.f64 	%fd939, %fd938, %fd938;
	mov.f64 	%fd940, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd941, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd942, %fd941, %fd939, %fd940;
	mov.f64 	%fd943, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd944, %fd942, %fd939, %fd943;
	mov.f64 	%fd945, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd946, %fd944, %fd939, %fd945;
	mov.f64 	%fd947, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd948, %fd946, %fd939, %fd947;
	mov.f64 	%fd949, 0d3F624924923BE72D;
	fma.rn.f64 	%fd950, %fd948, %fd939, %fd949;
	mov.f64 	%fd951, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd952, %fd950, %fd939, %fd951;
	mov.f64 	%fd953, 0d3FB5555555555554;
	fma.rn.f64 	%fd954, %fd952, %fd939, %fd953;
	sub.f64 	%fd955, %fd936, %fd938;
	add.f64 	%fd956, %fd955, %fd955;
	neg.f64 	%fd957, %fd938;
	fma.rn.f64 	%fd958, %fd957, %fd936, %fd956;
	mul.f64 	%fd959, %fd935, %fd958;
	mul.f64 	%fd960, %fd939, %fd954;
	fma.rn.f64 	%fd961, %fd960, %fd938, %fd959;
	xor.b32  	%r381, %r408, -2147483648;
	mov.u32 	%r382, -2147483648;
	mov.u32 	%r383, 1127219200;
	mov.b64 	%fd962, {%r381, %r383};
	mov.b64 	%fd963, {%r382, %r383};
	sub.f64 	%fd964, %fd962, %fd963;
	mov.f64 	%fd965, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd966, %fd964, %fd965, %fd938;
	neg.f64 	%fd967, %fd964;
	fma.rn.f64 	%fd968, %fd967, %fd965, %fd966;
	sub.f64 	%fd969, %fd968, %fd938;
	sub.f64 	%fd970, %fd961, %fd969;
	mov.f64 	%fd971, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd972, %fd964, %fd971, %fd970;
	add.f64 	%fd1185, %fd966, %fd972;
	bra.uni 	$L__BB6_114;

$L__BB6_110:
	mov.f64 	%fd927, 0d7FF0000000000000;
	fma.rn.f64 	%fd928, %fd1183, %fd927, %fd927;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r374}, %fd1183;
	}
	mov.b32 	%f2, %r374;
	setp.eq.f32 	%p113, %f2, 0f00000000;
	selp.f64 	%fd1185, 0dFFF0000000000000, %fd928, %p113;

$L__BB6_114:
	sub.f64 	%fd973, %fd179, %fd180;
	div.rn.f64 	%fd974, %fd973, %fd180;
	mul.f64 	%fd975, %fd974, %fd1185;
	mul.f64 	%fd976, %fd975, 0dC000000000000000;
	div.rn.f64 	%fd977, %fd976, %fd180;
	mul.f64 	%fd978, %fd974, %fd974;
	div.rn.f64 	%fd979, %fd978, %fd179;
	sub.f64 	%fd980, %fd977, %fd979;
	mul.f64 	%fd981, %fd980, %fd307;
	mul.f64 	%fd982, %fd185, %fd308;
	mul.f64 	%fd983, %fd982, %fd306;
	mul.f64 	%fd984, %fd983, %fd981;
	mul.f64 	%fd985, %fd984, 0dC000000000000000;
	sqrt.rn.f64 	%fd986, %fd179;
	fma.rn.f64 	%fd987, %fd986, %fd985, 0d0000000000000000;
	st.global.f64 	[%rd50], %fd987;

$L__BB6_115:
	@%p83 bra 	$L__BB6_119;

	setp.ge.f64 	%p3, %fd155, %fd154;
	add.f64 	%fd195, %fd153, %fd150;
	@%p3 bra 	$L__BB6_118;

	setp.le.f64 	%p154, %fd155, 0d0000000000000000;
	selp.b32 	%r395, 2, 8, %p154;
	sub.f64 	%fd1134, %fd135, %fd144;
	sub.f64 	%fd1133, %fd129, %fd138;
	sub.f64 	%fd1132, %fd132, %fd141;
	sub.f64 	%fd1131, %fd137, %fd138;
	sub.f64 	%fd1130, %fd143, %fd144;
	sub.f64 	%fd1129, %fd140, %fd141;
	selp.f64 	%fd990, %fd151, %fd154, %p3;
	mul.f64 	%fd991, %fd149, %fd153;
	mul.f64 	%fd992, %fd152, %fd150;
	sub.f64 	%fd993, %fd991, %fd992;
	mul.f64 	%fd994, %fd136, %fd1129;
	mul.f64 	%fd995, %fd133, %fd1130;
	sub.f64 	%fd996, %fd995, %fd994;
	mul.f64 	%fd997, %fd130, %fd1130;
	mul.f64 	%fd998, %fd136, %fd1131;
	sub.f64 	%fd999, %fd998, %fd997;
	mul.f64 	%fd1000, %fd133, %fd1131;
	mul.f64 	%fd1001, %fd130, %fd1129;
	sub.f64 	%fd1002, %fd1001, %fd1000;
	setp.gt.f64 	%p116, %fd993, 0d0000000000000000;
	setp.lt.f64 	%p117, %fd993, %fd990;
	mul.f64 	%fd1003, %fd1132, %fd999;
	fma.rn.f64 	%fd1004, %fd1133, %fd996, %fd1003;
	fma.rn.f64 	%fd1005, %fd1134, %fd1002, %fd1004;
	setp.eq.f64 	%p118, %fd1005, 0d0000000000000000;
	mul.f64 	%fd1006, %fd999, %fd999;
	fma.rn.f64 	%fd1007, %fd996, %fd996, %fd1006;
	fma.rn.f64 	%fd1008, %fd1002, %fd1002, %fd1007;
	mul.f64 	%fd1009, %fd149, 0d3BC79CA100000000;
	mul.f64 	%fd1010, %fd1009, %fd151;
	setp.lt.f64 	%p119, %fd1008, %fd1010;
	or.pred  	%p120, %p118, %p119;
	and.pred  	%p121, %p116, %p117;
	and.pred  	%p122, %p120, %p121;
	mul.f64 	%fd1011, %fd154, 0d3FE0000000000000;
	setp.lt.f64 	%p123, %fd155, %fd1011;
	selp.b32 	%r386, 2, 5, %p123;
	selp.f64 	%fd1012, %fd153, %fd195, %p123;
	selp.f64 	%fd1187, %fd151, %fd990, %p122;
	selp.b32 	%r387, 5, %r395, %p3;
	selp.b32 	%r409, %r386, %r387, %p122;
	selp.f64 	%fd1186, %fd1012, %fd993, %p122;

$L__BB6_118:
	selp.f64 	%fd1189, %fd151, %fd1187, %p3;
	selp.b32 	%r410, 5, %r409, %p3;
	selp.f64 	%fd1188, %fd195, %fd1186, %p3;

$L__BB6_119:
	selp.f64 	%fd204, %fd151, %fd1189, %p83;
	selp.b32 	%r411, 2, %r410, %p83;
	selp.f64 	%fd205, %fd153, %fd1188, %p83;
	setp.gtu.f64 	%p126, %fd205, 0d0000000000000000;
	@%p126 bra 	$L__BB6_123;
	bra.uni 	$L__BB6_120;

$L__BB6_123:
	setp.ltu.f64 	%p129, %fd205, %fd204;
	@%p129 bra 	$L__BB6_127;

	mov.f64 	%fd1014, 0d0000000000000000;
	sub.f64 	%fd1015, %fd1014, %fd152;
	add.f64 	%fd207, %fd1015, %fd150;
	setp.le.f64 	%p130, %fd207, 0d0000000000000000;
	mov.u32 	%r411, 1;
	@%p130 bra 	$L__BB6_127;

	setp.ge.f64 	%p131, %fd207, %fd149;
	mov.u32 	%r411, 4;
	@%p131 bra 	$L__BB6_127;

	mov.u32 	%r411, 7;
	bra.uni 	$L__BB6_127;

$L__BB6_120:
	mov.f64 	%fd1013, 0d0000000000000000;
	sub.f64 	%fd206, %fd1013, %fd152;
	setp.le.f64 	%p127, %fd206, 0d0000000000000000;
	mov.u32 	%r411, 0;
	@%p127 bra 	$L__BB6_127;

	setp.ge.f64 	%p128, %fd206, %fd149;
	mov.u32 	%r411, 3;
	@%p128 bra 	$L__BB6_127;

	mov.u32 	%r411, 6;

$L__BB6_127:
	setp.eq.s32 	%p132, %r411, 0;
	mov.f64 	%fd1224, 0d0000000000000000;
	mov.f64 	%fd1225, %fd1224;
	@%p132 bra 	$L__BB6_158;

	setp.eq.s32 	%p133, %r411, 1;
	selp.f64 	%fd1221, 0d3FF0000000000000, 0d0000000000000000, %p133;
	mov.f64 	%fd1220, 0d0000000000000000;
	@%p133 bra 	$L__BB6_156;
	bra.uni 	$L__BB6_129;

$L__BB6_156:
	mov.f64 	%fd1222, 0d0000000000000000;
	mov.f64 	%fd1223, %fd1222;
	bra.uni 	$L__BB6_157;

$L__BB6_129:
	setp.ne.s32 	%p134, %r411, 2;
	mov.f64 	%fd1190, 0d0000000000000000;
	@%p134 bra 	$L__BB6_131;

	div.rn.f64 	%fd1190, %fd153, %fd151;

$L__BB6_131:
	setp.eq.s32 	%p135, %r411, 2;
	selp.f64 	%fd1217, %fd1190, %fd1221, %p135;
	selp.f64 	%fd1216, 0d0000000000000000, %fd1220, %p135;
	@%p135 bra 	$L__BB6_154;
	bra.uni 	$L__BB6_132;

$L__BB6_154:
	mov.f64 	%fd1218, 0d0000000000000000;
	mov.f64 	%fd1219, %fd1218;
	bra.uni 	$L__BB6_155;

$L__BB6_132:
	setp.eq.s32 	%p136, %r411, 3;
	selp.f64 	%fd1019, 0d3FF0000000000000, 0d0000000000000000, %p136;
	selp.f64 	%fd1212, %fd1019, %fd1216, %p136;
	selp.f64 	%fd1213, 0d0000000000000000, %fd1217, %p136;
	@%p136 bra 	$L__BB6_152;
	bra.uni 	$L__BB6_133;

$L__BB6_152:
	mov.f64 	%fd1214, 0d0000000000000000;
	mov.f64 	%fd1215, %fd1214;
	bra.uni 	$L__BB6_153;

$L__BB6_133:
	setp.eq.s32 	%p137, %r411, 4;
	selp.f64 	%fd1020, 0d3FF0000000000000, 0d0000000000000000, %p137;
	selp.f64 	%fd1208, %fd1020, %fd1212, %p137;
	selp.f64 	%fd1209, %fd1020, %fd1213, %p137;
	@%p137 bra 	$L__BB6_150;
	bra.uni 	$L__BB6_134;

$L__BB6_150:
	mov.f64 	%fd1210, 0d0000000000000000;
	mov.f64 	%fd1211, %fd1210;
	bra.uni 	$L__BB6_151;

$L__BB6_134:
	setp.ne.s32 	%p138, %r411, 5;
	mov.f64 	%fd1191, 0d0000000000000000;
	mov.f64 	%fd1192, %fd1191;
	@%p138 bra 	$L__BB6_136;

	sub.f64 	%fd1137, %fd137, %fd138;
	sub.f64 	%fd1136, %fd143, %fd144;
	sub.f64 	%fd1135, %fd140, %fd141;
	sub.f64 	%fd1024, %fd128, %fd138;
	sub.f64 	%fd1025, %fd131, %fd141;
	mul.f64 	%fd1026, %fd1025, %fd1135;
	fma.rn.f64 	%fd1027, %fd1024, %fd1137, %fd1026;
	sub.f64 	%fd1028, %fd134, %fd144;
	fma.rn.f64 	%fd1029, %fd1028, %fd1136, %fd1027;
	div.rn.f64 	%fd1191, %fd1029, %fd151;
	mov.f64 	%fd1192, 0d3FF0000000000000;

$L__BB6_136:
	setp.eq.s32 	%p139, %r411, 5;
	selp.f64 	%fd1204, %fd1192, %fd1208, %p139;
	selp.f64 	%fd1205, %fd1191, %fd1209, %p139;
	@%p139 bra 	$L__BB6_148;
	bra.uni 	$L__BB6_137;

$L__BB6_148:
	mov.f64 	%fd1206, 0d0000000000000000;
	mov.f64 	%fd1207, %fd1206;
	bra.uni 	$L__BB6_149;

$L__BB6_137:
	setp.ne.s32 	%p140, %r411, 6;
	mov.f64 	%fd1193, 0d0000000000000000;
	@%p140 bra 	$L__BB6_139;

	sub.f64 	%fd1031, %fd138, %fd129;
	sub.f64 	%fd1032, %fd141, %fd132;
	mul.f64 	%fd1033, %fd133, %fd1032;
	fma.rn.f64 	%fd1034, %fd130, %fd1031, %fd1033;
	sub.f64 	%fd1035, %fd144, %fd135;
	fma.rn.f64 	%fd1036, %fd136, %fd1035, %fd1034;
	div.rn.f64 	%fd1193, %fd1036, %fd149;

$L__BB6_139:
	setp.eq.s32 	%p141, %r411, 6;
	selp.f64 	%fd1200, %fd1193, %fd1204, %p141;
	selp.f64 	%fd1201, 0d0000000000000000, %fd1205, %p141;
	@%p141 bra 	$L__BB6_146;
	bra.uni 	$L__BB6_140;

$L__BB6_146:
	mov.f64 	%fd1202, 0d0000000000000000;
	mov.f64 	%fd1203, %fd1202;
	bra.uni 	$L__BB6_147;

$L__BB6_140:
	setp.ne.s32 	%p142, %r411, 7;
	mov.f64 	%fd1194, 0d0000000000000000;
	mov.f64 	%fd1195, %fd1194;
	@%p142 bra 	$L__BB6_142;

	sub.f64 	%fd1040, %fd137, %fd129;
	sub.f64 	%fd1041, %fd140, %fd132;
	mul.f64 	%fd1042, %fd133, %fd1041;
	fma.rn.f64 	%fd1043, %fd130, %fd1040, %fd1042;
	sub.f64 	%fd1044, %fd143, %fd135;
	fma.rn.f64 	%fd1045, %fd136, %fd1044, %fd1043;
	div.rn.f64 	%fd1195, %fd1045, %fd149;
	mov.f64 	%fd1194, 0d3FF0000000000000;

$L__BB6_142:
	setp.eq.s32 	%p143, %r411, 7;
	selp.f64 	%fd1196, %fd1195, %fd1200, %p143;
	selp.f64 	%fd1197, %fd1194, %fd1201, %p143;
	@%p143 bra 	$L__BB6_144;
	bra.uni 	$L__BB6_143;

$L__BB6_144:
	mov.f64 	%fd1198, 0d0000000000000000;
	mov.f64 	%fd1199, %fd1198;
	bra.uni 	$L__BB6_145;

$L__BB6_143:
	mov.f64 	%fd1046, 0d0000000000000000;
	sub.f64 	%fd1047, %fd1046, %fd150;
	div.rn.f64 	%fd1048, %fd1047, %fd149;
	mul.f64 	%fd1049, %fd1048, %fd1048;
	mul.f64 	%fd1050, %fd149, %fd1049;
	sub.f64 	%fd1051, %fd151, %fd1050;
	sub.f64 	%fd1052, %fd1046, %fd152;
	mul.f64 	%fd1053, %fd1052, %fd1048;
	sub.f64 	%fd1054, %fd153, %fd1053;
	div.rn.f64 	%fd1198, %fd1054, %fd1051;
	mul.f64 	%fd1055, %fd149, %fd1048;
	mul.f64 	%fd1056, %fd1055, %fd1198;
	sub.f64 	%fd1057, %fd1052, %fd1056;
	div.rn.f64 	%fd1199, %fd1057, %fd149;

$L__BB6_145:
	selp.f64 	%fd1203, %fd1196, %fd1199, %p143;
	selp.f64 	%fd1202, %fd1197, %fd1198, %p143;

$L__BB6_147:
	selp.f64 	%fd1207, %fd1200, %fd1203, %p141;
	selp.f64 	%fd1206, %fd1201, %fd1202, %p141;

$L__BB6_149:
	selp.f64 	%fd1211, %fd1204, %fd1207, %p139;
	selp.f64 	%fd1210, %fd1205, %fd1206, %p139;

$L__BB6_151:
	selp.f64 	%fd1215, %fd1208, %fd1211, %p137;
	selp.f64 	%fd1214, %fd1209, %fd1210, %p137;

$L__BB6_153:
	selp.f64 	%fd1219, %fd1212, %fd1215, %p136;
	selp.f64 	%fd1218, %fd1213, %fd1214, %p136;

$L__BB6_155:
	selp.f64 	%fd1223, %fd1216, %fd1219, %p135;
	selp.f64 	%fd1222, %fd1217, %fd1218, %p135;

$L__BB6_157:
	selp.f64 	%fd1225, %fd1220, %fd1223, %p133;
	selp.f64 	%fd1224, %fd1221, %fd1222, %p133;

$L__BB6_158:
	selp.f64 	%fd1072, 0d0000000000000000, %fd1224, %p132;
	selp.f64 	%fd1073, 0d0000000000000000, %fd1225, %p132;
	mov.f64 	%fd1074, 0d3FF0000000000000;
	sub.f64 	%fd1075, %fd1074, %fd1073;
	mul.f64 	%fd1076, %fd129, %fd1075;
	mul.f64 	%fd1077, %fd132, %fd1075;
	mul.f64 	%fd1078, %fd135, %fd1075;
	fma.rn.f64 	%fd1079, %fd128, %fd1073, %fd1076;
	fma.rn.f64 	%fd1080, %fd131, %fd1073, %fd1077;
	fma.rn.f64 	%fd1081, %fd134, %fd1073, %fd1078;
	sub.f64 	%fd1082, %fd1074, %fd1072;
	mul.f64 	%fd1083, %fd138, %fd1082;
	mul.f64 	%fd1084, %fd141, %fd1082;
	mul.f64 	%fd1085, %fd144, %fd1082;
	fma.rn.f64 	%fd1086, %fd137, %fd1072, %fd1083;
	fma.rn.f64 	%fd1087, %fd140, %fd1072, %fd1084;
	fma.rn.f64 	%fd1088, %fd143, %fd1072, %fd1085;
	sub.f64 	%fd1089, %fd1079, %fd1086;
	sub.f64 	%fd1090, %fd1080, %fd1087;
	sub.f64 	%fd1091, %fd1081, %fd1088;
	mul.f64 	%fd1092, %fd1090, %fd1090;
	fma.rn.f64 	%fd1093, %fd1089, %fd1089, %fd1092;
	fma.rn.f64 	%fd1094, %fd1091, %fd1091, %fd1093;
	sqrt.rn.f64 	%fd1095, %fd1094;
	div.rn.f64 	%fd1096, %fd1089, %fd1095;
	div.rn.f64 	%fd1097, %fd1090, %fd1095;
	div.rn.f64 	%fd1098, %fd1091, %fd1095;
	st.global.f64 	[%rd53], %fd1073;
	st.global.f64 	[%rd53+8], %fd1072;
	st.global.f64 	[%rd54], %fd1096;
	st.global.f64 	[%rd54+8], %fd1097;
	st.global.f64 	[%rd54+16], %fd1098;

$L__BB6_159:
	ld.param.u64 	%rd166, [initialize_friction_collisions_cuda_kernel_forward_param_0+24];
	add.s64 	%rd167, %rd167, %rd32;
	setp.lt.u64 	%p152, %rd167, %rd166;
	@%p152 bra 	$L__BB6_2;

$L__BB6_160:
	ret;

}
	// .globl	initialize_friction_collisions_cuda_kernel_backward
.visible .entry initialize_friction_collisions_cuda_kernel_backward(
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_16[56],
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_17,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_18,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_19,
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_21[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_22[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_23[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_24[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_25[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_26[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_27[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_28[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_29[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_30[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_31[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_32[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_33[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_34[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_35[56],
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_36,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_37,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_38
)
{
	.reg .pred 	%p<345>;
	.reg .b16 	%rs<388>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<726>;
	.reg .f64 	%fd<6778>;
	.reg .b64 	%rd<407>;


	ld.param.v2.u32 	{%r348, %r349}, [initialize_friction_collisions_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r350, %r351}, [initialize_friction_collisions_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r356, %r357}, [initialize_friction_collisions_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r364, %r365}, [initialize_friction_collisions_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r372, %r373}, [initialize_friction_collisions_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r380, %r381}, [initialize_friction_collisions_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r388, %r389}, [initialize_friction_collisions_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r396, %r397}, [initialize_friction_collisions_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r404, %r405}, [initialize_friction_collisions_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r412, %r413}, [initialize_friction_collisions_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r420, %r421}, [initialize_friction_collisions_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r428, %r429}, [initialize_friction_collisions_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r436, %r437}, [initialize_friction_collisions_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r444, %r445}, [initialize_friction_collisions_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r452, %r453}, [initialize_friction_collisions_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r460, %r461}, [initialize_friction_collisions_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r468, %r469}, [initialize_friction_collisions_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r476, %r477}, [initialize_friction_collisions_cuda_kernel_backward_param_16+32];
	ld.param.f64 	%fd1867, [initialize_friction_collisions_cuda_kernel_backward_param_17];
	ld.param.f64 	%fd1868, [initialize_friction_collisions_cuda_kernel_backward_param_18];
	ld.param.f64 	%fd1869, [initialize_friction_collisions_cuda_kernel_backward_param_19];
	ld.param.v2.u32 	{%r484, %r485}, [initialize_friction_collisions_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r492, %r493}, [initialize_friction_collisions_cuda_kernel_backward_param_21+32];
	ld.param.v2.u32 	{%r500, %r501}, [initialize_friction_collisions_cuda_kernel_backward_param_22+32];
	ld.param.v2.u32 	{%r508, %r509}, [initialize_friction_collisions_cuda_kernel_backward_param_26+32];
	ld.param.v2.u32 	{%r516, %r517}, [initialize_friction_collisions_cuda_kernel_backward_param_28+32];
	ld.param.v2.u32 	{%r524, %r525}, [initialize_friction_collisions_cuda_kernel_backward_param_29+32];
	ld.param.v2.u32 	{%r532, %r533}, [initialize_friction_collisions_cuda_kernel_backward_param_30+32];
	ld.param.v2.u32 	{%r540, %r541}, [initialize_friction_collisions_cuda_kernel_backward_param_31+32];
	ld.param.v2.u32 	{%r548, %r549}, [initialize_friction_collisions_cuda_kernel_backward_param_32+32];
	ld.param.v2.u32 	{%r556, %r557}, [initialize_friction_collisions_cuda_kernel_backward_param_33+32];
	ld.param.u64 	%rd137, [initialize_friction_collisions_cuda_kernel_backward_param_33];
	ld.param.u64 	%rd135, [initialize_friction_collisions_cuda_kernel_backward_param_32];
	ld.param.u64 	%rd133, [initialize_friction_collisions_cuda_kernel_backward_param_31];
	ld.param.u64 	%rd131, [initialize_friction_collisions_cuda_kernel_backward_param_30];
	ld.param.u64 	%rd129, [initialize_friction_collisions_cuda_kernel_backward_param_29];
	ld.param.u64 	%rd127, [initialize_friction_collisions_cuda_kernel_backward_param_28];
	ld.param.u64 	%rd125, [initialize_friction_collisions_cuda_kernel_backward_param_26];
	ld.param.u64 	%rd123, [initialize_friction_collisions_cuda_kernel_backward_param_22];
	ld.param.u64 	%rd121, [initialize_friction_collisions_cuda_kernel_backward_param_21];
	ld.param.u64 	%rd119, [initialize_friction_collisions_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd117, [initialize_friction_collisions_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd115, [initialize_friction_collisions_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd114, [initialize_friction_collisions_cuda_kernel_backward_param_14+8];
	ld.param.u64 	%rd113, [initialize_friction_collisions_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd112, [initialize_friction_collisions_cuda_kernel_backward_param_13+8];
	ld.param.u64 	%rd111, [initialize_friction_collisions_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd110, [initialize_friction_collisions_cuda_kernel_backward_param_12+8];
	ld.param.u64 	%rd109, [initialize_friction_collisions_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd108, [initialize_friction_collisions_cuda_kernel_backward_param_11+8];
	ld.param.u64 	%rd107, [initialize_friction_collisions_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd106, [initialize_friction_collisions_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd105, [initialize_friction_collisions_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd104, [initialize_friction_collisions_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd103, [initialize_friction_collisions_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd101, [initialize_friction_collisions_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd100, [initialize_friction_collisions_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd99, [initialize_friction_collisions_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd97, [initialize_friction_collisions_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd95, [initialize_friction_collisions_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd93, [initialize_friction_collisions_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd92, [initialize_friction_collisions_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd90, [initialize_friction_collisions_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd88, [initialize_friction_collisions_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd86, [initialize_friction_collisions_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r113, [initialize_friction_collisions_cuda_kernel_backward_param_0+16];
	mov.u32 	%r560, %ntid.x;
	mov.u32 	%r561, %ctaid.x;
	mul.wide.u32 	%rd139, %r560, %r561;
	mov.u32 	%r562, %tid.x;
	cvt.u64.u32 	%rd140, %r562;
	add.s64 	%rd403, %rd139, %rd140;
	setp.ge.u64 	%p24, %rd403, %rd86;
	@%p24 bra 	$L__BB7_393;

	cvta.to.global.u64 	%rd23, %rd99;
	cvta.to.global.u64 	%rd24, %rd95;
	cvta.to.global.u64 	%rd25, %rd93;
	cvta.to.global.u64 	%rd26, %rd117;
	cvta.to.global.u64 	%rd27, %rd115;
	cvta.to.global.u64 	%rd28, %rd113;
	cvta.to.global.u64 	%rd29, %rd111;
	cvta.to.global.u64 	%rd30, %rd109;
	cvta.to.global.u64 	%rd31, %rd107;
	cvta.to.global.u64 	%rd32, %rd105;
	cvta.to.global.u64 	%rd33, %rd103;
	cvta.to.global.u64 	%rd34, %rd101;
	cvta.to.global.u64 	%rd35, %rd97;
	cvt.s64.s32 	%rd36, %r351;
	cvt.s64.s32 	%rd37, %r350;
	cvt.s64.s32 	%rd38, %r349;
	cvt.s64.s32 	%rd39, %r396;
	cvt.s64.s32 	%rd40, %r388;
	cvt.s64.s32 	%rd41, %r380;
	cvt.s64.s32 	%rd42, %r476;
	cvt.s64.s32 	%rd43, %r428;
	cvt.s64.s32 	%rd44, %r444;
	cvt.s64.s32 	%rd45, %r412;
	cvt.s64.s32 	%rd46, %r404;
	cvt.s64.s32 	%rd47, %r468;
	cvt.s64.s32 	%rd48, %r436;
	mul.f64 	%fd1, %fd1867, %fd1867;
	add.f64 	%fd5854, %fd1867, %fd1867;
	cvt.s64.s32 	%rd49, %r452;
	cvt.s64.s32 	%rd50, %r500;
	cvt.s64.s32 	%rd51, %r372;
	cvt.s64.s32 	%rd52, %r492;
	cvt.s64.s32 	%rd53, %r364;
	cvt.s64.s32 	%rd54, %r420;
	cvt.s64.s32 	%rd55, %r460;
	cvt.s64.s32 	%rd56, %r484;
	cvt.s64.s32 	%rd57, %r356;
	cvt.s64.s32 	%rd58, %r508;
	cvt.s64.s32 	%rd59, %r548;
	cvt.s64.s32 	%rd60, %r516;
	cvt.s64.s32 	%rd61, %r556;
	cvt.s64.s32 	%rd62, %r540;
	cvt.s64.s32 	%rd63, %r524;
	cvt.s64.s32 	%rd64, %r532;

$L__BB7_2:
	setp.lt.s32 	%p25, %r113, 4;
	mov.u64 	%rd404, %rd403;
	@%p25 bra 	$L__BB7_6;

	or.b64  	%rd141, %rd403, %rd36;
	and.b64  	%rd142, %rd141, -4294967296;
	setp.eq.s64 	%p26, %rd142, 0;
	@%p26 bra 	$L__BB7_5;

	div.u64 	%rd404, %rd403, %rd36;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r564, %rd36;
	cvt.u32.u64 	%r565, %rd403;
	div.u32 	%r566, %r565, %r564;
	cvt.u64.u32 	%rd404, %r566;

$L__BB7_6:
	setp.lt.s32 	%p27, %r113, 3;
	@%p27 bra 	$L__BB7_10;

	or.b64  	%rd143, %rd404, %rd37;
	and.b64  	%rd144, %rd143, -4294967296;
	setp.eq.s64 	%p28, %rd144, 0;
	@%p28 bra 	$L__BB7_9;

	div.u64 	%rd404, %rd404, %rd37;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r567, %rd37;
	cvt.u32.u64 	%r568, %rd404;
	div.u32 	%r569, %r568, %r567;
	cvt.u64.u32 	%rd404, %r569;

$L__BB7_10:
	setp.lt.s32 	%p29, %r113, 2;
	@%p29 bra 	$L__BB7_14;

	or.b64  	%rd145, %rd404, %rd38;
	and.b64  	%rd146, %rd145, -4294967296;
	setp.eq.s64 	%p30, %rd146, 0;
	@%p30 bra 	$L__BB7_13;

	div.u64 	%rd404, %rd404, %rd38;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r570, %rd38;
	cvt.u32.u64 	%r571, %rd404;
	div.u32 	%r572, %r571, %r570;
	cvt.u64.u32 	%rd404, %r572;

$L__BB7_14:
	cvt.s64.s32 	%rd147, %rd404;
	setp.gt.s32 	%p31, %r113, 0;
	selp.b64 	%rd75, %rd147, 0, %p31;
	mul.lo.s64 	%rd148, %rd75, %rd39;
	add.s64 	%rd149, %rd35, %rd148;
	ld.global.u32 	%r18, [%rd149];
	setp.gt.u32 	%p32, %r18, 1;
	mul.lo.s64 	%rd150, %rd75, %rd40;
	add.s64 	%rd76, %rd24, %rd150;
	mul.lo.s64 	%rd151, %rd75, %rd41;
	add.s64 	%rd77, %rd25, %rd151;
	mov.f64 	%fd5883, 0d0000000000000000;
	mov.f64 	%fd5840, %fd5883;
	mov.f64 	%fd5841, %fd5883;
	mov.f64 	%fd5842, %fd5883;
	mov.f64 	%fd5843, %fd5883;
	mov.f64 	%fd5844, %fd5883;
	mov.f64 	%fd5845, %fd5883;
	mov.f64 	%fd5846, %fd5883;
	mov.f64 	%fd5847, %fd5883;
	mov.f64 	%fd5848, %fd5883;
	mov.f64 	%fd5849, %fd5883;
	mov.f64 	%fd5850, %fd5883;
	mov.f64 	%fd5851, %fd5883;
	@%p32 bra 	$L__BB7_46;

	ld.global.u32 	%r573, [%rd77];
	ld.global.u32 	%r574, [%rd76];
	setp.eq.s32 	%p33, %r18, 1;
	selp.b32 	%r682, %r573, %r574, %p33;
	selp.b32 	%r681, %r574, %r573, %p33;
	cvt.s64.s32 	%rd152, %r681;
	mul.lo.s64 	%rd153, %rd152, %rd42;
	add.s64 	%rd154, %rd26, %rd153;
	cvt.s64.s32 	%rd78, %r682;
	mul.lo.s64 	%rd155, %rd78, %rd43;
	add.s64 	%rd156, %rd32, %rd155;
	mul.lo.s64 	%rd157, %rd152, %rd44;
	add.s64 	%rd158, %rd30, %rd157;
	ld.global.f64 	%fd1883, [%rd158];
	ld.global.f64 	%fd1884, [%rd156];
	add.f64 	%fd5834, %fd1884, %fd1883;
	mul.lo.s64 	%rd159, %rd78, %rd45;
	add.s64 	%rd160, %rd34, %rd159;
	ld.global.u32 	%r680, [%rd160];
	cvt.s64.s32 	%rd161, %r680;
	mul.lo.s64 	%rd162, %rd161, %rd46;
	add.s64 	%rd163, %rd23, %rd162;
	ld.global.u32 	%r679, [%rd154];
	cvt.s64.s32 	%rd164, %r679;
	mul.lo.s64 	%rd165, %rd164, %rd46;
	add.s64 	%rd166, %rd23, %rd165;
	ld.global.u32 	%r678, [%rd154+4];
	cvt.s64.s32 	%rd167, %r678;
	mul.lo.s64 	%rd168, %rd167, %rd46;
	add.s64 	%rd169, %rd23, %rd168;
	ld.global.u32 	%r677, [%rd154+8];
	cvt.s64.s32 	%rd170, %r677;
	mul.lo.s64 	%rd171, %rd170, %rd46;
	add.s64 	%rd172, %rd23, %rd171;
	ld.global.f64 	%fd5848, [%rd169];
	ld.global.f64 	%fd5845, [%rd166];
	sub.f64 	%fd1885, %fd5848, %fd5845;
	ld.global.f64 	%fd5847, [%rd169+8];
	ld.global.f64 	%fd5844, [%rd166+8];
	sub.f64 	%fd1886, %fd5847, %fd5844;
	ld.global.f64 	%fd5846, [%rd169+16];
	ld.global.f64 	%fd5843, [%rd166+16];
	sub.f64 	%fd1887, %fd5846, %fd5843;
	ld.global.f64 	%fd5851, [%rd172];
	sub.f64 	%fd1888, %fd5851, %fd5845;
	ld.global.f64 	%fd5850, [%rd172+8];
	sub.f64 	%fd1889, %fd5850, %fd5844;
	ld.global.f64 	%fd5849, [%rd172+16];
	sub.f64 	%fd1890, %fd5849, %fd5843;
	mul.f64 	%fd1891, %fd1886, %fd1890;
	mul.f64 	%fd1892, %fd1887, %fd1889;
	sub.f64 	%fd30, %fd1891, %fd1892;
	mul.f64 	%fd1893, %fd1887, %fd1888;
	mul.f64 	%fd1894, %fd1885, %fd1890;
	sub.f64 	%fd31, %fd1893, %fd1894;
	mul.f64 	%fd1895, %fd1885, %fd1889;
	mul.f64 	%fd1896, %fd1886, %fd1888;
	sub.f64 	%fd32, %fd1895, %fd1896;
	mul.f64 	%fd1897, %fd1886, %fd32;
	mul.f64 	%fd1898, %fd1887, %fd31;
	sub.f64 	%fd1899, %fd1897, %fd1898;
	mul.f64 	%fd1900, %fd1887, %fd30;
	mul.f64 	%fd1901, %fd1885, %fd32;
	sub.f64 	%fd1902, %fd1900, %fd1901;
	mul.f64 	%fd1903, %fd1885, %fd31;
	mul.f64 	%fd1904, %fd1886, %fd30;
	sub.f64 	%fd1905, %fd1903, %fd1904;
	mul.f64 	%fd1906, %fd1886, %fd1886;
	fma.rn.f64 	%fd1907, %fd1885, %fd1885, %fd1906;
	fma.rn.f64 	%fd33, %fd1887, %fd1887, %fd1907;
	mul.f64 	%fd1908, %fd1886, %fd1902;
	fma.rn.f64 	%fd1909, %fd1885, %fd1899, %fd1908;
	fma.rn.f64 	%fd1910, %fd1887, %fd1905, %fd1909;
	mul.f64 	%fd1911, %fd1902, %fd1902;
	fma.rn.f64 	%fd1912, %fd1899, %fd1899, %fd1911;
	fma.rn.f64 	%fd1913, %fd1905, %fd1905, %fd1912;
	ld.global.f64 	%fd5842, [%rd163];
	sub.f64 	%fd35, %fd5842, %fd5845;
	ld.global.f64 	%fd5841, [%rd163+8];
	sub.f64 	%fd37, %fd5841, %fd5844;
	ld.global.f64 	%fd5840, [%rd163+16];
	sub.f64 	%fd39, %fd5840, %fd5843;
	mul.f64 	%fd1914, %fd37, %fd1886;
	fma.rn.f64 	%fd1915, %fd35, %fd1885, %fd1914;
	fma.rn.f64 	%fd1916, %fd39, %fd1887, %fd1915;
	mul.f64 	%fd1917, %fd37, %fd1902;
	fma.rn.f64 	%fd1918, %fd35, %fd1899, %fd1917;
	fma.rn.f64 	%fd1919, %fd39, %fd1905, %fd1918;
	div.rn.f64 	%fd1920, %fd1910, %fd33;
	mul.f64 	%fd1921, %fd1920, %fd1920;
	mul.f64 	%fd1922, %fd33, %fd1921;
	sub.f64 	%fd1923, %fd1913, %fd1922;
	mul.f64 	%fd1924, %fd1916, %fd1920;
	sub.f64 	%fd1925, %fd1919, %fd1924;
	div.rn.f64 	%fd1926, %fd1925, %fd1923;
	mul.f64 	%fd1927, %fd33, %fd1920;
	mul.f64 	%fd1928, %fd1927, %fd1926;
	sub.f64 	%fd1929, %fd1916, %fd1928;
	div.rn.f64 	%fd40, %fd1929, %fd33;
	setp.gt.f64 	%p34, %fd40, 0d0000000000000000;
	setp.lt.f64 	%p35, %fd40, 0d3FF0000000000000;
	setp.ge.f64 	%p36, %fd1926, 0d0000000000000000;
	and.pred  	%p37, %p34, %p35;
	and.pred  	%p38, %p36, %p37;
	mov.u16 	%rs338, 3;
	@%p38 bra 	$L__BB7_21;

	sub.f64 	%fd1930, %fd5851, %fd5848;
	sub.f64 	%fd1931, %fd5850, %fd5847;
	mul.f64 	%fd1932, %fd1931, %fd32;
	sub.f64 	%fd1933, %fd5849, %fd5846;
	mul.f64 	%fd1934, %fd1933, %fd31;
	sub.f64 	%fd1935, %fd1932, %fd1934;
	mul.f64 	%fd1936, %fd1933, %fd30;
	mul.f64 	%fd1937, %fd1930, %fd32;
	sub.f64 	%fd1938, %fd1936, %fd1937;
	mul.f64 	%fd1939, %fd1930, %fd31;
	mul.f64 	%fd1940, %fd1931, %fd30;
	sub.f64 	%fd1941, %fd1939, %fd1940;
	mul.f64 	%fd1942, %fd1931, %fd1931;
	fma.rn.f64 	%fd1943, %fd1930, %fd1930, %fd1942;
	fma.rn.f64 	%fd1944, %fd1933, %fd1933, %fd1943;
	mul.f64 	%fd1945, %fd1931, %fd1938;
	fma.rn.f64 	%fd1946, %fd1930, %fd1935, %fd1945;
	fma.rn.f64 	%fd1947, %fd1933, %fd1941, %fd1946;
	mul.f64 	%fd1948, %fd1938, %fd1938;
	fma.rn.f64 	%fd1949, %fd1935, %fd1935, %fd1948;
	fma.rn.f64 	%fd1950, %fd1941, %fd1941, %fd1949;
	sub.f64 	%fd1951, %fd5842, %fd5848;
	sub.f64 	%fd1952, %fd5841, %fd5847;
	mul.f64 	%fd1953, %fd1952, %fd1931;
	fma.rn.f64 	%fd1954, %fd1951, %fd1930, %fd1953;
	sub.f64 	%fd1955, %fd5840, %fd5846;
	fma.rn.f64 	%fd1956, %fd1955, %fd1933, %fd1954;
	mul.f64 	%fd1957, %fd1952, %fd1938;
	fma.rn.f64 	%fd1958, %fd1951, %fd1935, %fd1957;
	fma.rn.f64 	%fd1959, %fd1955, %fd1941, %fd1958;
	div.rn.f64 	%fd1960, %fd1947, %fd1944;
	mul.f64 	%fd1961, %fd1960, %fd1960;
	mul.f64 	%fd1962, %fd1944, %fd1961;
	sub.f64 	%fd1963, %fd1950, %fd1962;
	mul.f64 	%fd1964, %fd1956, %fd1960;
	sub.f64 	%fd1965, %fd1959, %fd1964;
	div.rn.f64 	%fd1966, %fd1965, %fd1963;
	mul.f64 	%fd1967, %fd1944, %fd1960;
	mul.f64 	%fd1968, %fd1967, %fd1966;
	sub.f64 	%fd1969, %fd1956, %fd1968;
	div.rn.f64 	%fd41, %fd1969, %fd1944;
	setp.gt.f64 	%p39, %fd41, 0d0000000000000000;
	setp.lt.f64 	%p40, %fd41, 0d3FF0000000000000;
	setp.ge.f64 	%p41, %fd1966, 0d0000000000000000;
	and.pred  	%p42, %p39, %p40;
	and.pred  	%p43, %p41, %p42;
	mov.u16 	%rs338, 4;
	@%p43 bra 	$L__BB7_21;

	sub.f64 	%fd1970, %fd5845, %fd5851;
	sub.f64 	%fd1971, %fd5844, %fd5850;
	mul.f64 	%fd1972, %fd1971, %fd32;
	sub.f64 	%fd1973, %fd5843, %fd5849;
	mul.f64 	%fd1974, %fd1973, %fd31;
	sub.f64 	%fd1975, %fd1972, %fd1974;
	mul.f64 	%fd1976, %fd1973, %fd30;
	mul.f64 	%fd1977, %fd1970, %fd32;
	sub.f64 	%fd1978, %fd1976, %fd1977;
	mul.f64 	%fd1979, %fd1970, %fd31;
	mul.f64 	%fd1980, %fd1971, %fd30;
	sub.f64 	%fd1981, %fd1979, %fd1980;
	mul.f64 	%fd1982, %fd1971, %fd1971;
	fma.rn.f64 	%fd1983, %fd1970, %fd1970, %fd1982;
	fma.rn.f64 	%fd1984, %fd1973, %fd1973, %fd1983;
	mul.f64 	%fd1985, %fd1971, %fd1978;
	fma.rn.f64 	%fd1986, %fd1970, %fd1975, %fd1985;
	fma.rn.f64 	%fd1987, %fd1973, %fd1981, %fd1986;
	mul.f64 	%fd1988, %fd1978, %fd1978;
	fma.rn.f64 	%fd1989, %fd1975, %fd1975, %fd1988;
	fma.rn.f64 	%fd1990, %fd1981, %fd1981, %fd1989;
	sub.f64 	%fd1991, %fd5842, %fd5851;
	sub.f64 	%fd1992, %fd5841, %fd5850;
	mul.f64 	%fd1993, %fd1971, %fd1992;
	fma.rn.f64 	%fd1994, %fd1970, %fd1991, %fd1993;
	sub.f64 	%fd1995, %fd5840, %fd5849;
	fma.rn.f64 	%fd1996, %fd1973, %fd1995, %fd1994;
	mul.f64 	%fd1997, %fd1992, %fd1978;
	fma.rn.f64 	%fd1998, %fd1991, %fd1975, %fd1997;
	fma.rn.f64 	%fd1999, %fd1995, %fd1981, %fd1998;
	div.rn.f64 	%fd2000, %fd1987, %fd1984;
	mul.f64 	%fd2001, %fd2000, %fd2000;
	mul.f64 	%fd2002, %fd1984, %fd2001;
	sub.f64 	%fd2003, %fd1990, %fd2002;
	mul.f64 	%fd2004, %fd1996, %fd2000;
	sub.f64 	%fd2005, %fd1999, %fd2004;
	div.rn.f64 	%fd2006, %fd2005, %fd2003;
	mul.f64 	%fd2007, %fd1984, %fd2000;
	mul.f64 	%fd2008, %fd2007, %fd2006;
	sub.f64 	%fd2009, %fd1996, %fd2008;
	div.rn.f64 	%fd42, %fd2009, %fd1984;
	setp.gt.f64 	%p44, %fd42, 0d0000000000000000;
	setp.lt.f64 	%p45, %fd42, 0d3FF0000000000000;
	setp.ge.f64 	%p46, %fd2006, 0d0000000000000000;
	and.pred  	%p47, %p44, %p45;
	and.pred  	%p48, %p46, %p47;
	mov.u16 	%rs338, 5;
	@%p48 bra 	$L__BB7_21;

	setp.le.f64 	%p49, %fd40, 0d0000000000000000;
	setp.ge.f64 	%p50, %fd42, 0d3FF0000000000000;
	and.pred  	%p51, %p49, %p50;
	mov.u16 	%rs338, 0;
	@%p51 bra 	$L__BB7_21;

	setp.le.f64 	%p52, %fd41, 0d0000000000000000;
	setp.ge.f64 	%p53, %fd40, 0d3FF0000000000000;
	and.pred  	%p54, %p52, %p53;
	mov.u16 	%rs338, 1;
	@%p54 bra 	$L__BB7_21;

	setp.le.f64 	%p55, %fd42, 0d0000000000000000;
	setp.ge.f64 	%p56, %fd41, 0d3FF0000000000000;
	and.pred  	%p57, %p55, %p56;
	selp.b16 	%rs338, 2, 6, %p57;

$L__BB7_21:
	mov.f64 	%fd5835, 0d4415AF1D80000000;
	setp.gt.s16 	%p58, %rs338, 2;
	@%p58 bra 	$L__BB7_26;
	bra.uni 	$L__BB7_22;

$L__BB7_26:
	setp.gt.s16 	%p59, %rs338, 4;
	@%p59 bra 	$L__BB7_30;

	setp.eq.s16 	%p62, %rs338, 3;
	@%p62 bra 	$L__BB7_34;

	setp.eq.s16 	%p63, %rs338, 4;
	@%p63 bra 	$L__BB7_29;
	bra.uni 	$L__BB7_37;

$L__BB7_29:
	sub.f64 	%fd2042, %fd5848, %fd5842;
	sub.f64 	%fd2043, %fd5849, %fd5840;
	sub.f64 	%fd2044, %fd5847, %fd5841;
	mul.f64 	%fd2045, %fd2044, %fd2043;
	sub.f64 	%fd2046, %fd5850, %fd5841;
	sub.f64 	%fd2047, %fd5846, %fd5840;
	mul.f64 	%fd2048, %fd2047, %fd2046;
	sub.f64 	%fd2049, %fd2045, %fd2048;
	sub.f64 	%fd2050, %fd5851, %fd5842;
	mul.f64 	%fd2051, %fd2047, %fd2050;
	mul.f64 	%fd2052, %fd2042, %fd2043;
	sub.f64 	%fd2053, %fd2051, %fd2052;
	mul.f64 	%fd2054, %fd2042, %fd2046;
	mul.f64 	%fd2055, %fd2044, %fd2050;
	sub.f64 	%fd2056, %fd2054, %fd2055;
	mul.f64 	%fd2057, %fd2053, %fd2053;
	fma.rn.f64 	%fd2058, %fd2049, %fd2049, %fd2057;
	fma.rn.f64 	%fd2059, %fd2056, %fd2056, %fd2058;
	sub.f64 	%fd2060, %fd5851, %fd5848;
	sub.f64 	%fd2061, %fd5850, %fd5847;
	mul.f64 	%fd2062, %fd2061, %fd2061;
	fma.rn.f64 	%fd2063, %fd2060, %fd2060, %fd2062;
	sub.f64 	%fd2064, %fd5849, %fd5846;
	fma.rn.f64 	%fd2065, %fd2064, %fd2064, %fd2063;
	div.rn.f64 	%fd5835, %fd2059, %fd2065;
	bra.uni 	$L__BB7_37;

$L__BB7_22:
	setp.eq.s16 	%p64, %rs338, 0;
	@%p64 bra 	$L__BB7_36;

	setp.eq.s16 	%p65, %rs338, 1;
	@%p65 bra 	$L__BB7_35;

	setp.eq.s16 	%p66, %rs338, 2;
	@%p66 bra 	$L__BB7_25;
	bra.uni 	$L__BB7_37;

$L__BB7_25:
	sub.f64 	%fd2084, %fd5842, %fd5851;
	sub.f64 	%fd2085, %fd5841, %fd5850;
	mul.f64 	%fd2086, %fd2085, %fd2085;
	fma.rn.f64 	%fd2087, %fd2084, %fd2084, %fd2086;
	sub.f64 	%fd2088, %fd5840, %fd5849;
	fma.rn.f64 	%fd5835, %fd2088, %fd2088, %fd2087;
	bra.uni 	$L__BB7_37;

$L__BB7_30:
	setp.eq.s16 	%p60, %rs338, 5;
	@%p60 bra 	$L__BB7_33;

	setp.ne.s16 	%p61, %rs338, 6;
	@%p61 bra 	$L__BB7_37;

	mul.f64 	%fd2011, %fd37, %fd31;
	fma.rn.f64 	%fd2012, %fd35, %fd30, %fd2011;
	fma.rn.f64 	%fd2013, %fd39, %fd32, %fd2012;
	mul.f64 	%fd2014, %fd2013, %fd2013;
	mul.f64 	%fd2015, %fd31, %fd31;
	fma.rn.f64 	%fd2016, %fd30, %fd30, %fd2015;
	fma.rn.f64 	%fd2017, %fd32, %fd32, %fd2016;
	div.rn.f64 	%fd5835, %fd2014, %fd2017;
	bra.uni 	$L__BB7_37;

$L__BB7_34:
	sub.f64 	%fd2066, %fd5845, %fd5842;
	sub.f64 	%fd2067, %fd5846, %fd5840;
	sub.f64 	%fd2068, %fd5844, %fd5841;
	mul.f64 	%fd2069, %fd2068, %fd2067;
	sub.f64 	%fd2070, %fd5847, %fd5841;
	sub.f64 	%fd2071, %fd5843, %fd5840;
	mul.f64 	%fd2072, %fd2071, %fd2070;
	sub.f64 	%fd2073, %fd2069, %fd2072;
	sub.f64 	%fd2074, %fd5848, %fd5842;
	mul.f64 	%fd2075, %fd2071, %fd2074;
	mul.f64 	%fd2076, %fd2066, %fd2067;
	sub.f64 	%fd2077, %fd2075, %fd2076;
	mul.f64 	%fd2078, %fd2066, %fd2070;
	mul.f64 	%fd2079, %fd2068, %fd2074;
	sub.f64 	%fd2080, %fd2078, %fd2079;
	mul.f64 	%fd2081, %fd2077, %fd2077;
	fma.rn.f64 	%fd2082, %fd2073, %fd2073, %fd2081;
	fma.rn.f64 	%fd2083, %fd2080, %fd2080, %fd2082;
	div.rn.f64 	%fd5835, %fd2083, %fd33;
	bra.uni 	$L__BB7_37;

$L__BB7_36:
	mul.f64 	%fd2094, %fd37, %fd37;
	fma.rn.f64 	%fd2095, %fd35, %fd35, %fd2094;
	fma.rn.f64 	%fd5835, %fd39, %fd39, %fd2095;
	bra.uni 	$L__BB7_37;

$L__BB7_35:
	sub.f64 	%fd2089, %fd5842, %fd5848;
	sub.f64 	%fd2090, %fd5841, %fd5847;
	mul.f64 	%fd2091, %fd2090, %fd2090;
	fma.rn.f64 	%fd2092, %fd2089, %fd2089, %fd2091;
	sub.f64 	%fd2093, %fd5840, %fd5846;
	fma.rn.f64 	%fd5835, %fd2093, %fd2093, %fd2092;
	bra.uni 	$L__BB7_37;

$L__BB7_33:
	sub.f64 	%fd2018, %fd5851, %fd5842;
	sub.f64 	%fd2019, %fd5843, %fd5840;
	sub.f64 	%fd2020, %fd5850, %fd5841;
	mul.f64 	%fd2021, %fd2019, %fd2020;
	sub.f64 	%fd2022, %fd5844, %fd5841;
	sub.f64 	%fd2023, %fd5849, %fd5840;
	mul.f64 	%fd2024, %fd2022, %fd2023;
	sub.f64 	%fd2025, %fd2021, %fd2024;
	sub.f64 	%fd2026, %fd5845, %fd5842;
	mul.f64 	%fd2027, %fd2026, %fd2023;
	mul.f64 	%fd2028, %fd2019, %fd2018;
	sub.f64 	%fd2029, %fd2027, %fd2028;
	mul.f64 	%fd2030, %fd2022, %fd2018;
	mul.f64 	%fd2031, %fd2026, %fd2020;
	sub.f64 	%fd2032, %fd2030, %fd2031;
	mul.f64 	%fd2033, %fd2029, %fd2029;
	fma.rn.f64 	%fd2034, %fd2025, %fd2025, %fd2033;
	fma.rn.f64 	%fd2035, %fd2032, %fd2032, %fd2034;
	sub.f64 	%fd2036, %fd5845, %fd5851;
	sub.f64 	%fd2037, %fd5844, %fd5850;
	mul.f64 	%fd2038, %fd2037, %fd2037;
	fma.rn.f64 	%fd2039, %fd2036, %fd2036, %fd2038;
	sub.f64 	%fd2040, %fd5843, %fd5849;
	fma.rn.f64 	%fd2041, %fd2040, %fd2040, %fd2039;
	div.rn.f64 	%fd5835, %fd2035, %fd2041;

$L__BB7_37:
	mul.f64 	%fd2096, %fd5834, %fd5834;
	sub.f64 	%fd5833, %fd5835, %fd2096;
	fma.rn.f64 	%fd5832, %fd5854, %fd5834, %fd1;
	setp.geu.f64 	%p67, %fd5833, %fd5832;
	mov.u16 	%rs339, 0;
	@%p67 bra 	$L__BB7_46;

	mul.lo.s64 	%rd173, %rd78, %rd49;
	add.s64 	%rd174, %rd29, %rd173;
	ld.global.f64 	%fd2097, [%rd174];
	mul.f64 	%fd2098, %fd2097, %fd1869;
	mul.f64 	%fd5827, %fd2098, %fd1867;
	div.rn.f64 	%fd5836, %fd5833, %fd5832;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r683}, %fd5836;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r684, %temp}, %fd5836;
	}
	setp.gt.s32 	%p68, %r683, 1048575;
	mov.u32 	%r685, -1023;
	@%p68 bra 	$L__BB7_40;

	mul.f64 	%fd5836, %fd5836, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r683}, %fd5836;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r684, %temp}, %fd5836;
	}
	mov.u32 	%r685, -1077;

$L__BB7_40:
	add.s32 	%r577, %r683, -1;
	setp.lt.u32 	%p69, %r577, 2146435071;
	@%p69 bra 	$L__BB7_42;
	bra.uni 	$L__BB7_41;

$L__BB7_42:
	shr.u32 	%r579, %r683, 20;
	add.s32 	%r686, %r685, %r579;
	and.b32  	%r580, %r683, -2146435073;
	or.b32  	%r581, %r580, 1072693248;
	mov.b64 	%fd5837, {%r684, %r581};
	setp.lt.s32 	%p71, %r581, 1073127583;
	@%p71 bra 	$L__BB7_44;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r582, %temp}, %fd5837;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r583}, %fd5837;
	}
	add.s32 	%r584, %r583, -1048576;
	mov.b64 	%fd5837, {%r582, %r584};
	add.s32 	%r686, %r686, 1;

$L__BB7_44:
	add.f64 	%fd2101, %fd5837, 0d3FF0000000000000;
	mov.f64 	%fd2102, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2103, %fd2101;
	neg.f64 	%fd2104, %fd2101;
	fma.rn.f64 	%fd2105, %fd2104, %fd2103, %fd2102;
	fma.rn.f64 	%fd2106, %fd2105, %fd2105, %fd2105;
	fma.rn.f64 	%fd2107, %fd2106, %fd2103, %fd2103;
	add.f64 	%fd2108, %fd5837, 0dBFF0000000000000;
	mul.f64 	%fd2109, %fd2108, %fd2107;
	fma.rn.f64 	%fd2110, %fd2108, %fd2107, %fd2109;
	mul.f64 	%fd2111, %fd2110, %fd2110;
	mov.f64 	%fd2112, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2113, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2114, %fd2113, %fd2111, %fd2112;
	mov.f64 	%fd2115, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2116, %fd2114, %fd2111, %fd2115;
	mov.f64 	%fd2117, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2118, %fd2116, %fd2111, %fd2117;
	mov.f64 	%fd2119, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2120, %fd2118, %fd2111, %fd2119;
	mov.f64 	%fd2121, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2122, %fd2120, %fd2111, %fd2121;
	mov.f64 	%fd2123, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2124, %fd2122, %fd2111, %fd2123;
	mov.f64 	%fd2125, 0d3FB5555555555554;
	fma.rn.f64 	%fd2126, %fd2124, %fd2111, %fd2125;
	sub.f64 	%fd2127, %fd2108, %fd2110;
	add.f64 	%fd2128, %fd2127, %fd2127;
	neg.f64 	%fd2129, %fd2110;
	fma.rn.f64 	%fd2130, %fd2129, %fd2108, %fd2128;
	mul.f64 	%fd2131, %fd2107, %fd2130;
	mul.f64 	%fd2132, %fd2111, %fd2126;
	fma.rn.f64 	%fd2133, %fd2132, %fd2110, %fd2131;
	xor.b32  	%r585, %r686, -2147483648;
	mov.u32 	%r586, -2147483648;
	mov.u32 	%r587, 1127219200;
	mov.b64 	%fd2134, {%r585, %r587};
	mov.b64 	%fd2135, {%r586, %r587};
	sub.f64 	%fd2136, %fd2134, %fd2135;
	mov.f64 	%fd2137, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2138, %fd2136, %fd2137, %fd2110;
	neg.f64 	%fd2139, %fd2136;
	fma.rn.f64 	%fd2140, %fd2139, %fd2137, %fd2138;
	sub.f64 	%fd2141, %fd2140, %fd2110;
	sub.f64 	%fd2142, %fd2133, %fd2141;
	mov.f64 	%fd2143, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2144, %fd2136, %fd2143, %fd2142;
	add.f64 	%fd5838, %fd2138, %fd2144;
	bra.uni 	$L__BB7_45;

$L__BB7_41:
	mov.f64 	%fd2099, 0d7FF0000000000000;
	fma.rn.f64 	%fd2100, %fd5836, %fd2099, %fd2099;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r578}, %fd5836;
	}
	mov.b32 	%f1, %r578;
	setp.eq.f32 	%p70, %f1, 0f00000000;
	selp.f64 	%fd5838, 0dFFF0000000000000, %fd2100, %p70;

$L__BB7_45:
	sub.f64 	%fd2145, %fd5833, %fd5832;
	div.rn.f64 	%fd2146, %fd2145, %fd5832;
	mul.f64 	%fd2147, %fd2146, %fd5838;
	mul.f64 	%fd2148, %fd2147, 0dC000000000000000;
	div.rn.f64 	%fd2149, %fd2148, %fd5832;
	mul.f64 	%fd2150, %fd2146, %fd2146;
	div.rn.f64 	%fd2151, %fd2150, %fd5833;
	sub.f64 	%fd2152, %fd2149, %fd2151;
	mul.f64 	%fd5826, %fd2152, %fd1868;
	mul.f64 	%fd2153, %fd5827, %fd5826;
	fma.rn.f64 	%fd5825, %fd5827, %fd5826, %fd2153;
	sqrt.rn.f64 	%fd5824, %fd5833;
	mov.u16 	%rs339, 1;

$L__BB7_46:
	setp.lt.u32 	%p72, %r18, 2;
	mov.f64 	%fd170, %fd5883;
	mov.f64 	%fd171, %fd5883;
	mov.f64 	%fd172, %fd5883;
	mov.f64 	%fd173, %fd5883;
	mov.f64 	%fd174, %fd5883;
	mov.f64 	%fd175, %fd5883;
	mov.f64 	%fd176, %fd5883;
	mov.f64 	%fd177, %fd5883;
	mov.f64 	%fd178, %fd5883;
	mov.f64 	%fd179, %fd5883;
	mov.f64 	%fd180, %fd5883;
	mov.f64 	%fd181, %fd5883;
	mov.f64 	%fd5884, %fd5883;
	mov.f64 	%fd5885, %fd5883;
	mov.f64 	%fd5886, %fd5883;
	mov.f64 	%fd5887, %fd5883;
	mov.f64 	%fd5888, %fd5883;
	mov.f64 	%fd5889, %fd5883;
	mov.f64 	%fd5890, %fd5883;
	mov.f64 	%fd5891, %fd5883;
	mov.f64 	%fd5892, %fd5883;
	mov.f64 	%fd5893, %fd5883;
	mov.f64 	%fd5894, %fd5883;
	@%p72 bra 	$L__BB7_88;

	ld.global.u32 	%r676, [%rd77];
	cvt.s64.s32 	%rd79, %r676;
	mul.lo.s64 	%rd175, %rd79, %rd47;
	add.s64 	%rd176, %rd27, %rd175;
	ld.global.u32 	%r675, [%rd76];
	cvt.s64.s32 	%rd80, %r675;
	mul.lo.s64 	%rd177, %rd80, %rd47;
	add.s64 	%rd178, %rd27, %rd177;
	mul.lo.s64 	%rd179, %rd79, %rd48;
	add.s64 	%rd180, %rd31, %rd179;
	mul.lo.s64 	%rd181, %rd80, %rd48;
	add.s64 	%rd182, %rd31, %rd181;
	ld.global.f64 	%fd2179, [%rd182];
	ld.global.f64 	%fd2180, [%rd180];
	add.f64 	%fd5831, %fd2180, %fd2179;
	ld.global.u32 	%r674, [%rd176];
	cvt.s64.s32 	%rd81, %r674;
	mul.lo.s64 	%rd183, %rd81, %rd46;
	add.s64 	%rd184, %rd23, %rd183;
	ld.global.u32 	%r673, [%rd176+4];
	cvt.s64.s32 	%rd82, %r673;
	mul.lo.s64 	%rd185, %rd82, %rd46;
	add.s64 	%rd186, %rd23, %rd185;
	ld.global.u32 	%r672, [%rd178];
	cvt.s64.s32 	%rd83, %r672;
	mul.lo.s64 	%rd187, %rd83, %rd46;
	add.s64 	%rd188, %rd23, %rd187;
	ld.global.u32 	%r671, [%rd178+4];
	cvt.s64.s32 	%rd84, %r671;
	mul.lo.s64 	%rd189, %rd84, %rd46;
	add.s64 	%rd190, %rd23, %rd189;
	ld.global.f64 	%fd175, [%rd186];
	ld.global.f64 	%fd172, [%rd184];
	sub.f64 	%fd89, %fd175, %fd172;
	ld.global.f64 	%fd174, [%rd186+8];
	ld.global.f64 	%fd171, [%rd184+8];
	sub.f64 	%fd92, %fd174, %fd171;
	ld.global.f64 	%fd173, [%rd186+16];
	ld.global.f64 	%fd170, [%rd184+16];
	sub.f64 	%fd95, %fd173, %fd170;
	ld.global.f64 	%fd181, [%rd190];
	ld.global.f64 	%fd178, [%rd188];
	sub.f64 	%fd98, %fd181, %fd178;
	ld.global.f64 	%fd180, [%rd190+8];
	ld.global.f64 	%fd177, [%rd188+8];
	sub.f64 	%fd101, %fd180, %fd177;
	ld.global.f64 	%fd179, [%rd190+16];
	ld.global.f64 	%fd176, [%rd188+16];
	sub.f64 	%fd104, %fd179, %fd176;
	sub.f64 	%fd105, %fd172, %fd178;
	sub.f64 	%fd106, %fd171, %fd177;
	sub.f64 	%fd107, %fd170, %fd176;
	mul.f64 	%fd2181, %fd92, %fd92;
	fma.rn.f64 	%fd2182, %fd89, %fd89, %fd2181;
	fma.rn.f64 	%fd108, %fd95, %fd95, %fd2182;
	mul.f64 	%fd2183, %fd92, %fd101;
	fma.rn.f64 	%fd2184, %fd89, %fd98, %fd2183;
	fma.rn.f64 	%fd109, %fd95, %fd104, %fd2184;
	mul.f64 	%fd2185, %fd101, %fd101;
	fma.rn.f64 	%fd2186, %fd98, %fd98, %fd2185;
	fma.rn.f64 	%fd110, %fd104, %fd104, %fd2186;
	mul.f64 	%fd2187, %fd92, %fd106;
	fma.rn.f64 	%fd2188, %fd89, %fd105, %fd2187;
	fma.rn.f64 	%fd111, %fd95, %fd107, %fd2188;
	mul.f64 	%fd2189, %fd106, %fd101;
	fma.rn.f64 	%fd2190, %fd105, %fd98, %fd2189;
	fma.rn.f64 	%fd112, %fd107, %fd104, %fd2190;
	mul.f64 	%fd2191, %fd108, %fd110;
	mul.f64 	%fd2192, %fd109, %fd109;
	sub.f64 	%fd113, %fd2191, %fd2192;
	mul.f64 	%fd2193, %fd109, %fd112;
	mul.f64 	%fd2194, %fd111, %fd110;
	sub.f64 	%fd114, %fd2193, %fd2194;
	setp.le.f64 	%p73, %fd114, 0d0000000000000000;
	@%p73 bra 	$L__BB7_51;

	setp.ge.f64 	%p1, %fd114, %fd113;
	add.f64 	%fd115, %fd112, %fd109;
	@%p1 bra 	$L__BB7_50;

	selp.f64 	%fd2196, %fd110, %fd113, %p1;
	mul.f64 	%fd2197, %fd108, %fd112;
	mul.f64 	%fd2198, %fd111, %fd109;
	sub.f64 	%fd2199, %fd2197, %fd2198;
	mul.f64 	%fd2200, %fd95, %fd101;
	mul.f64 	%fd2201, %fd92, %fd104;
	sub.f64 	%fd2202, %fd2201, %fd2200;
	mul.f64 	%fd2203, %fd89, %fd104;
	mul.f64 	%fd2204, %fd95, %fd98;
	sub.f64 	%fd2205, %fd2204, %fd2203;
	mul.f64 	%fd2206, %fd92, %fd98;
	mul.f64 	%fd2207, %fd89, %fd101;
	sub.f64 	%fd2208, %fd2207, %fd2206;
	setp.gt.f64 	%p74, %fd2199, 0d0000000000000000;
	setp.lt.f64 	%p75, %fd2199, %fd2196;
	mul.f64 	%fd2209, %fd106, %fd2205;
	fma.rn.f64 	%fd2210, %fd105, %fd2202, %fd2209;
	fma.rn.f64 	%fd2211, %fd107, %fd2208, %fd2210;
	setp.eq.f64 	%p76, %fd2211, 0d0000000000000000;
	mul.f64 	%fd2212, %fd2205, %fd2205;
	fma.rn.f64 	%fd2213, %fd2202, %fd2202, %fd2212;
	fma.rn.f64 	%fd2214, %fd2208, %fd2208, %fd2213;
	mul.f64 	%fd2215, %fd108, 0d3BC79CA100000000;
	mul.f64 	%fd2216, %fd2215, %fd110;
	setp.lt.f64 	%p77, %fd2214, %fd2216;
	or.pred  	%p78, %p76, %p77;
	and.pred  	%p79, %p74, %p75;
	and.pred  	%p80, %p78, %p79;
	mul.f64 	%fd2217, %fd113, 0d3FE0000000000000;
	setp.lt.f64 	%p81, %fd114, %fd2217;
	selp.b32 	%r590, 2, 5, %p81;
	selp.f64 	%fd2218, %fd112, %fd115, %p81;
	selp.f64 	%fd5859, %fd110, %fd2196, %p80;
	selp.b32 	%r693, %r590, 8, %p80;
	selp.f64 	%fd5860, %fd2218, %fd2199, %p80;

$L__BB7_50:
	selp.f64 	%fd5861, %fd110, %fd5859, %p1;
	selp.b32 	%r694, 5, %r693, %p1;
	selp.f64 	%fd5862, %fd115, %fd5860, %p1;

$L__BB7_51:
	selp.f64 	%fd124, %fd110, %fd5861, %p73;
	selp.b32 	%r695, 2, %r694, %p73;
	selp.f64 	%fd125, %fd112, %fd5862, %p73;
	setp.gtu.f64 	%p84, %fd125, 0d0000000000000000;
	@%p84 bra 	$L__BB7_55;
	bra.uni 	$L__BB7_52;

$L__BB7_55:
	setp.ltu.f64 	%p87, %fd125, %fd124;
	@%p87 bra 	$L__BB7_59;

	mov.f64 	%fd2220, 0d0000000000000000;
	sub.f64 	%fd2221, %fd2220, %fd111;
	add.f64 	%fd127, %fd2221, %fd109;
	setp.le.f64 	%p88, %fd127, 0d0000000000000000;
	mov.u32 	%r695, 1;
	@%p88 bra 	$L__BB7_59;

	setp.ge.f64 	%p89, %fd127, %fd108;
	mov.u32 	%r695, 4;
	@%p89 bra 	$L__BB7_59;

	mov.u32 	%r695, 7;
	bra.uni 	$L__BB7_59;

$L__BB7_52:
	mov.f64 	%fd2219, 0d0000000000000000;
	sub.f64 	%fd126, %fd2219, %fd111;
	setp.le.f64 	%p85, %fd126, 0d0000000000000000;
	mov.u32 	%r695, 0;
	@%p85 bra 	$L__BB7_59;

	setp.ge.f64 	%p86, %fd126, %fd108;
	mov.u32 	%r695, 3;
	@%p86 bra 	$L__BB7_59;

	mov.u32 	%r695, 6;

$L__BB7_59:
	setp.eq.s32 	%p90, %r695, 0;
	@%p90 bra 	$L__BB7_75;

	setp.eq.s32 	%p91, %r695, 1;
	@%p91 bra 	$L__BB7_74;
	bra.uni 	$L__BB7_61;

$L__BB7_74:
	sub.f64 	%fd2320, %fd172, %fd181;
	sub.f64 	%fd2321, %fd171, %fd180;
	mul.f64 	%fd2322, %fd2321, %fd2321;
	fma.rn.f64 	%fd2323, %fd2320, %fd2320, %fd2322;
	sub.f64 	%fd2324, %fd170, %fd179;
	fma.rn.f64 	%fd5863, %fd2324, %fd2324, %fd2323;
	bra.uni 	$L__BB7_76;

$L__BB7_75:
	mul.f64 	%fd2325, %fd106, %fd106;
	fma.rn.f64 	%fd2326, %fd105, %fd105, %fd2325;
	fma.rn.f64 	%fd5863, %fd107, %fd107, %fd2326;
	bra.uni 	$L__BB7_76;

$L__BB7_61:
	setp.eq.s32 	%p92, %r695, 2;
	@%p92 bra 	$L__BB7_73;
	bra.uni 	$L__BB7_62;

$L__BB7_73:
	sub.f64 	%fd2302, %fd178, %fd172;
	sub.f64 	%fd2303, %fd179, %fd170;
	sub.f64 	%fd2304, %fd177, %fd171;
	mul.f64 	%fd2305, %fd2304, %fd2303;
	sub.f64 	%fd2306, %fd180, %fd171;
	sub.f64 	%fd2307, %fd176, %fd170;
	mul.f64 	%fd2308, %fd2307, %fd2306;
	sub.f64 	%fd2309, %fd2305, %fd2308;
	sub.f64 	%fd2310, %fd181, %fd172;
	mul.f64 	%fd2311, %fd2307, %fd2310;
	mul.f64 	%fd2312, %fd2302, %fd2303;
	sub.f64 	%fd2313, %fd2311, %fd2312;
	mul.f64 	%fd2314, %fd2302, %fd2306;
	mul.f64 	%fd2315, %fd2304, %fd2310;
	sub.f64 	%fd2316, %fd2314, %fd2315;
	mul.f64 	%fd2317, %fd2313, %fd2313;
	fma.rn.f64 	%fd2318, %fd2309, %fd2309, %fd2317;
	fma.rn.f64 	%fd2319, %fd2316, %fd2316, %fd2318;
	div.rn.f64 	%fd5863, %fd2319, %fd110;
	bra.uni 	$L__BB7_76;

$L__BB7_62:
	setp.eq.s32 	%p93, %r695, 3;
	@%p93 bra 	$L__BB7_72;
	bra.uni 	$L__BB7_63;

$L__BB7_72:
	sub.f64 	%fd2297, %fd175, %fd178;
	sub.f64 	%fd2298, %fd174, %fd177;
	mul.f64 	%fd2299, %fd2298, %fd2298;
	fma.rn.f64 	%fd2300, %fd2297, %fd2297, %fd2299;
	sub.f64 	%fd2301, %fd173, %fd176;
	fma.rn.f64 	%fd5863, %fd2301, %fd2301, %fd2300;
	bra.uni 	$L__BB7_76;

$L__BB7_63:
	setp.eq.s32 	%p94, %r695, 4;
	@%p94 bra 	$L__BB7_71;
	bra.uni 	$L__BB7_64;

$L__BB7_71:
	sub.f64 	%fd2292, %fd175, %fd181;
	sub.f64 	%fd2293, %fd174, %fd180;
	mul.f64 	%fd2294, %fd2293, %fd2293;
	fma.rn.f64 	%fd2295, %fd2292, %fd2292, %fd2294;
	sub.f64 	%fd2296, %fd173, %fd179;
	fma.rn.f64 	%fd5863, %fd2296, %fd2296, %fd2295;
	bra.uni 	$L__BB7_76;

$L__BB7_64:
	setp.eq.s32 	%p95, %r695, 5;
	@%p95 bra 	$L__BB7_70;
	bra.uni 	$L__BB7_65;

$L__BB7_70:
	sub.f64 	%fd2274, %fd178, %fd175;
	sub.f64 	%fd2275, %fd179, %fd173;
	sub.f64 	%fd2276, %fd177, %fd174;
	mul.f64 	%fd2277, %fd2276, %fd2275;
	sub.f64 	%fd2278, %fd180, %fd174;
	sub.f64 	%fd2279, %fd176, %fd173;
	mul.f64 	%fd2280, %fd2279, %fd2278;
	sub.f64 	%fd2281, %fd2277, %fd2280;
	sub.f64 	%fd2282, %fd181, %fd175;
	mul.f64 	%fd2283, %fd2279, %fd2282;
	mul.f64 	%fd2284, %fd2274, %fd2275;
	sub.f64 	%fd2285, %fd2283, %fd2284;
	mul.f64 	%fd2286, %fd2274, %fd2278;
	mul.f64 	%fd2287, %fd2276, %fd2282;
	sub.f64 	%fd2288, %fd2286, %fd2287;
	mul.f64 	%fd2289, %fd2285, %fd2285;
	fma.rn.f64 	%fd2290, %fd2281, %fd2281, %fd2289;
	fma.rn.f64 	%fd2291, %fd2288, %fd2288, %fd2290;
	div.rn.f64 	%fd5863, %fd2291, %fd110;
	bra.uni 	$L__BB7_76;

$L__BB7_65:
	setp.eq.s32 	%p96, %r695, 6;
	@%p96 bra 	$L__BB7_69;
	bra.uni 	$L__BB7_66;

$L__BB7_69:
	sub.f64 	%fd2259, %fd175, %fd178;
	sub.f64 	%fd2260, %fd173, %fd176;
	mul.f64 	%fd2261, %fd106, %fd2260;
	sub.f64 	%fd2262, %fd174, %fd177;
	mul.f64 	%fd2263, %fd2262, %fd107;
	sub.f64 	%fd2264, %fd2261, %fd2263;
	mul.f64 	%fd2265, %fd2259, %fd107;
	mul.f64 	%fd2266, %fd105, %fd2260;
	sub.f64 	%fd2267, %fd2265, %fd2266;
	mul.f64 	%fd2268, %fd105, %fd2262;
	mul.f64 	%fd2269, %fd2259, %fd106;
	sub.f64 	%fd2270, %fd2268, %fd2269;
	mul.f64 	%fd2271, %fd2267, %fd2267;
	fma.rn.f64 	%fd2272, %fd2264, %fd2264, %fd2271;
	fma.rn.f64 	%fd2273, %fd2270, %fd2270, %fd2272;
	div.rn.f64 	%fd5863, %fd2273, %fd108;
	bra.uni 	$L__BB7_76;

$L__BB7_66:
	setp.eq.s32 	%p97, %r695, 7;
	@%p97 bra 	$L__BB7_68;
	bra.uni 	$L__BB7_67;

$L__BB7_68:
	sub.f64 	%fd2241, %fd172, %fd181;
	sub.f64 	%fd2242, %fd173, %fd179;
	sub.f64 	%fd2243, %fd171, %fd180;
	mul.f64 	%fd2244, %fd2243, %fd2242;
	sub.f64 	%fd2245, %fd174, %fd180;
	sub.f64 	%fd2246, %fd170, %fd179;
	mul.f64 	%fd2247, %fd2245, %fd2246;
	sub.f64 	%fd2248, %fd2244, %fd2247;
	sub.f64 	%fd2249, %fd175, %fd181;
	mul.f64 	%fd2250, %fd2249, %fd2246;
	mul.f64 	%fd2251, %fd2241, %fd2242;
	sub.f64 	%fd2252, %fd2250, %fd2251;
	mul.f64 	%fd2253, %fd2241, %fd2245;
	mul.f64 	%fd2254, %fd2249, %fd2243;
	sub.f64 	%fd2255, %fd2253, %fd2254;
	mul.f64 	%fd2256, %fd2252, %fd2252;
	fma.rn.f64 	%fd2257, %fd2248, %fd2248, %fd2256;
	fma.rn.f64 	%fd2258, %fd2255, %fd2255, %fd2257;
	div.rn.f64 	%fd5863, %fd2258, %fd108;
	bra.uni 	$L__BB7_76;

$L__BB7_67:
	sub.f64 	%fd2222, %fd178, %fd172;
	mul.f64 	%fd2223, %fd95, %fd101;
	mul.f64 	%fd2224, %fd92, %fd104;
	sub.f64 	%fd2225, %fd2224, %fd2223;
	mul.f64 	%fd2226, %fd89, %fd104;
	mul.f64 	%fd2227, %fd95, %fd98;
	sub.f64 	%fd2228, %fd2227, %fd2226;
	mul.f64 	%fd2229, %fd92, %fd98;
	mul.f64 	%fd2230, %fd89, %fd101;
	sub.f64 	%fd2231, %fd2230, %fd2229;
	sub.f64 	%fd2232, %fd177, %fd171;
	mul.f64 	%fd2233, %fd2232, %fd2228;
	fma.rn.f64 	%fd2234, %fd2222, %fd2225, %fd2233;
	sub.f64 	%fd2235, %fd176, %fd170;
	fma.rn.f64 	%fd2236, %fd2235, %fd2231, %fd2234;
	mul.f64 	%fd2237, %fd2236, %fd2236;
	mul.f64 	%fd2238, %fd2228, %fd2228;
	fma.rn.f64 	%fd2239, %fd2225, %fd2225, %fd2238;
	fma.rn.f64 	%fd2240, %fd2231, %fd2231, %fd2239;
	div.rn.f64 	%fd5863, %fd2237, %fd2240;

$L__BB7_76:
	mul.f64 	%fd2339, %fd5831, %fd5831;
	sub.f64 	%fd5830, %fd5863, %fd2339;
	fma.rn.f64 	%fd5829, %fd5854, %fd5831, %fd1;
	setp.geu.f64 	%p98, %fd5830, %fd5829;
	mov.u16 	%rs340, 0;
	mov.f64 	%fd5884, %fd5883;
	mov.f64 	%fd5885, %fd5883;
	mov.f64 	%fd5886, %fd5883;
	mov.f64 	%fd5887, %fd5883;
	mov.f64 	%fd5888, %fd5883;
	mov.f64 	%fd5889, %fd5883;
	mov.f64 	%fd5890, %fd5883;
	mov.f64 	%fd5891, %fd5883;
	mov.f64 	%fd5892, %fd5883;
	mov.f64 	%fd5893, %fd5883;
	mov.f64 	%fd5894, %fd5883;
	@%p98 bra 	$L__BB7_88;

	mul.lo.s64 	%rd191, %rd81, %rd54;
	add.s64 	%rd192, %rd33, %rd191;
	mul.lo.s64 	%rd193, %rd82, %rd54;
	add.s64 	%rd194, %rd33, %rd193;
	mul.lo.s64 	%rd195, %rd83, %rd54;
	add.s64 	%rd196, %rd33, %rd195;
	mul.lo.s64 	%rd197, %rd84, %rd54;
	add.s64 	%rd198, %rd33, %rd197;
	ld.global.f64 	%fd5888, [%rd194];
	ld.global.f64 	%fd5885, [%rd192];
	sub.f64 	%fd2341, %fd5888, %fd5885;
	ld.global.f64 	%fd5887, [%rd194+8];
	ld.global.f64 	%fd5884, [%rd192+8];
	sub.f64 	%fd2342, %fd5887, %fd5884;
	ld.global.f64 	%fd5886, [%rd194+16];
	ld.global.f64 	%fd5883, [%rd192+16];
	sub.f64 	%fd2343, %fd5886, %fd5883;
	ld.global.f64 	%fd5894, [%rd198];
	ld.global.f64 	%fd5891, [%rd196];
	sub.f64 	%fd2344, %fd5894, %fd5891;
	ld.global.f64 	%fd5893, [%rd198+8];
	ld.global.f64 	%fd5890, [%rd196+8];
	sub.f64 	%fd2345, %fd5893, %fd5890;
	ld.global.f64 	%fd5892, [%rd198+16];
	ld.global.f64 	%fd5889, [%rd196+16];
	sub.f64 	%fd2346, %fd5892, %fd5889;
	mul.f64 	%fd2347, %fd2342, %fd2342;
	fma.rn.f64 	%fd2348, %fd2341, %fd2341, %fd2347;
	fma.rn.f64 	%fd2349, %fd2343, %fd2343, %fd2348;
	mul.f64 	%fd2350, %fd2349, 0d3F50624DE0000000;
	mul.f64 	%fd2351, %fd2345, %fd2345;
	fma.rn.f64 	%fd2352, %fd2344, %fd2344, %fd2351;
	fma.rn.f64 	%fd2353, %fd2346, %fd2346, %fd2352;
	mul.f64 	%fd5896, %fd2350, %fd2353;
	mul.f64 	%fd2354, %fd95, %fd101;
	mul.f64 	%fd2355, %fd92, %fd104;
	sub.f64 	%fd2356, %fd2355, %fd2354;
	mul.f64 	%fd2357, %fd89, %fd104;
	mul.f64 	%fd2358, %fd95, %fd98;
	sub.f64 	%fd2359, %fd2358, %fd2357;
	mul.f64 	%fd2360, %fd92, %fd98;
	mul.f64 	%fd2361, %fd89, %fd101;
	sub.f64 	%fd2362, %fd2361, %fd2360;
	mul.f64 	%fd2363, %fd2359, %fd2359;
	fma.rn.f64 	%fd2364, %fd2356, %fd2356, %fd2363;
	fma.rn.f64 	%fd153, %fd2362, %fd2362, %fd2364;
	setp.geu.f64 	%p99, %fd153, %fd5896;
	mov.f64 	%fd5864, 0d3FF0000000000000;
	@%p99 bra 	$L__BB7_79;

	div.rn.f64 	%fd2365, %fd153, %fd5896;
	mov.f64 	%fd2366, 0d0000000000000000;
	sub.f64 	%fd2367, %fd2366, %fd2365;
	add.f64 	%fd2368, %fd2367, 0d4000000000000000;
	mul.f64 	%fd5864, %fd2365, %fd2368;

$L__BB7_79:
	setp.neu.f64 	%p100, %fd5864, 0d3FF0000000000000;
	setp.eq.f64 	%p101, %fd5864, 0d3FF0000000000000;
	selp.u16 	%rs341, 1, 0, %p101;
	mov.u16 	%rs340, 1;
	mov.u32 	%r706, %r674;
	mov.u32 	%r707, %r673;
	mov.u32 	%r708, %r672;
	mov.u32 	%r709, %r671;
	@%p100 bra 	$L__BB7_88;

	mul.lo.s64 	%rd199, %rd79, %rd55;
	add.s64 	%rd200, %rd28, %rd199;
	mul.lo.s64 	%rd201, %rd80, %rd55;
	add.s64 	%rd202, %rd28, %rd201;
	ld.global.f64 	%fd2369, [%rd202];
	ld.global.f64 	%fd2370, [%rd200];
	add.f64 	%fd2371, %fd2370, %fd2369;
	mul.f64 	%fd2372, %fd2371, %fd1869;
	mul.f64 	%fd5821, %fd2372, %fd1867;
	div.rn.f64 	%fd5865, %fd5830, %fd5829;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r696}, %fd5865;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r697, %temp}, %fd5865;
	}
	setp.gt.s32 	%p102, %r696, 1048575;
	mov.u32 	%r698, -1023;
	@%p102 bra 	$L__BB7_82;

	mul.f64 	%fd5865, %fd5865, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r696}, %fd5865;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r697, %temp}, %fd5865;
	}
	mov.u32 	%r698, -1077;

$L__BB7_82:
	add.s32 	%r599, %r696, -1;
	setp.lt.u32 	%p103, %r599, 2146435071;
	@%p103 bra 	$L__BB7_84;
	bra.uni 	$L__BB7_83;

$L__BB7_84:
	shr.u32 	%r601, %r696, 20;
	add.s32 	%r699, %r698, %r601;
	and.b32  	%r602, %r696, -2146435073;
	or.b32  	%r603, %r602, 1072693248;
	mov.b64 	%fd5866, {%r697, %r603};
	setp.lt.s32 	%p105, %r603, 1073127583;
	@%p105 bra 	$L__BB7_86;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r604, %temp}, %fd5866;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r605}, %fd5866;
	}
	add.s32 	%r606, %r605, -1048576;
	mov.b64 	%fd5866, {%r604, %r606};
	add.s32 	%r699, %r699, 1;

$L__BB7_86:
	add.f64 	%fd2375, %fd5866, 0d3FF0000000000000;
	mov.f64 	%fd2376, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2377, %fd2375;
	neg.f64 	%fd2378, %fd2375;
	fma.rn.f64 	%fd2379, %fd2378, %fd2377, %fd2376;
	fma.rn.f64 	%fd2380, %fd2379, %fd2379, %fd2379;
	fma.rn.f64 	%fd2381, %fd2380, %fd2377, %fd2377;
	add.f64 	%fd2382, %fd5866, 0dBFF0000000000000;
	mul.f64 	%fd2383, %fd2382, %fd2381;
	fma.rn.f64 	%fd2384, %fd2382, %fd2381, %fd2383;
	mul.f64 	%fd2385, %fd2384, %fd2384;
	mov.f64 	%fd2386, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2387, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2388, %fd2387, %fd2385, %fd2386;
	mov.f64 	%fd2389, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2390, %fd2388, %fd2385, %fd2389;
	mov.f64 	%fd2391, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2392, %fd2390, %fd2385, %fd2391;
	mov.f64 	%fd2393, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2394, %fd2392, %fd2385, %fd2393;
	mov.f64 	%fd2395, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2396, %fd2394, %fd2385, %fd2395;
	mov.f64 	%fd2397, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2398, %fd2396, %fd2385, %fd2397;
	mov.f64 	%fd2399, 0d3FB5555555555554;
	fma.rn.f64 	%fd2400, %fd2398, %fd2385, %fd2399;
	sub.f64 	%fd2401, %fd2382, %fd2384;
	add.f64 	%fd2402, %fd2401, %fd2401;
	neg.f64 	%fd2403, %fd2384;
	fma.rn.f64 	%fd2404, %fd2403, %fd2382, %fd2402;
	mul.f64 	%fd2405, %fd2381, %fd2404;
	mul.f64 	%fd2406, %fd2385, %fd2400;
	fma.rn.f64 	%fd2407, %fd2406, %fd2384, %fd2405;
	xor.b32  	%r607, %r699, -2147483648;
	mov.u32 	%r608, -2147483648;
	mov.u32 	%r609, 1127219200;
	mov.b64 	%fd2408, {%r607, %r609};
	mov.b64 	%fd2409, {%r608, %r609};
	sub.f64 	%fd2410, %fd2408, %fd2409;
	mov.f64 	%fd2411, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2412, %fd2410, %fd2411, %fd2384;
	neg.f64 	%fd2413, %fd2410;
	fma.rn.f64 	%fd2414, %fd2413, %fd2411, %fd2412;
	sub.f64 	%fd2415, %fd2414, %fd2384;
	sub.f64 	%fd2416, %fd2407, %fd2415;
	mov.f64 	%fd2417, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2418, %fd2410, %fd2417, %fd2416;
	add.f64 	%fd5867, %fd2412, %fd2418;
	bra.uni 	$L__BB7_87;

$L__BB7_83:
	mov.f64 	%fd2373, 0d7FF0000000000000;
	fma.rn.f64 	%fd2374, %fd5865, %fd2373, %fd2373;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r600}, %fd5865;
	}
	mov.b32 	%f2, %r600;
	setp.eq.f32 	%p104, %f2, 0f00000000;
	selp.f64 	%fd5867, 0dFFF0000000000000, %fd2374, %p104;

$L__BB7_87:
	sub.f64 	%fd2419, %fd5830, %fd5829;
	div.rn.f64 	%fd2420, %fd2419, %fd5829;
	mul.f64 	%fd2421, %fd2420, %fd5867;
	mul.f64 	%fd2422, %fd2421, 0dC000000000000000;
	div.rn.f64 	%fd2423, %fd2422, %fd5829;
	mul.f64 	%fd2424, %fd2420, %fd2420;
	div.rn.f64 	%fd2425, %fd2424, %fd5830;
	sub.f64 	%fd2426, %fd2423, %fd2425;
	mul.f64 	%fd5820, %fd2426, %fd1868;
	mul.f64 	%fd2427, %fd5821, %fd5820;
	fma.rn.f64 	%fd5819, %fd5821, %fd5820, %fd2427;
	sqrt.rn.f64 	%fd5818, %fd5830;
	mov.u32 	%r706, %r674;
	mov.u32 	%r707, %r673;
	mov.u32 	%r708, %r672;
	mov.u32 	%r709, %r671;

$L__BB7_88:
	@%p72 bra 	$L__BB7_271;

	setp.eq.s64 	%p107, %rd123, 0;
	@%p107 bra 	$L__BB7_91;

	cvta.to.global.u64 	%rd203, %rd123;
	mul.lo.s64 	%rd204, %rd75, %rd50;
	add.s64 	%rd205, %rd203, %rd204;
	ld.global.f64 	%fd2428, [%rd205];
	add.f64 	%fd5903, %fd2428, 0d0000000000000000;
	ld.global.f64 	%fd2429, [%rd205+8];
	add.f64 	%fd5902, %fd2429, 0d0000000000000000;
	ld.global.f64 	%fd2430, [%rd205+16];
	add.f64 	%fd5901, %fd2430, 0d0000000000000000;
	bra.uni 	$L__BB7_93;

$L__BB7_91:
	setp.eq.s64 	%p108, %rd92, 0;
	mov.f64 	%fd5901, 0d0000000000000000;
	mov.f64 	%fd5902, %fd5901;
	mov.f64 	%fd5903, %fd5901;
	@%p108 bra 	$L__BB7_93;

	cvta.to.global.u64 	%rd206, %rd92;
	mul.lo.s64 	%rd207, %rd75, %rd51;
	add.s64 	%rd208, %rd206, %rd207;
	ld.global.f64 	%fd2434, [%rd208];
	add.f64 	%fd5903, %fd2434, 0d0000000000000000;
	ld.global.f64 	%fd2435, [%rd208+8];
	add.f64 	%fd5902, %fd2435, 0d0000000000000000;
	ld.global.f64 	%fd2436, [%rd208+16];
	add.f64 	%fd5901, %fd2436, 0d0000000000000000;

$L__BB7_93:
	setp.eq.s64 	%p109, %rd121, 0;
	@%p109 bra 	$L__BB7_95;

	cvta.to.global.u64 	%rd209, %rd121;
	mul.lo.s64 	%rd210, %rd75, %rd52;
	add.s64 	%rd211, %rd209, %rd210;
	ld.global.f64 	%fd2437, [%rd211];
	add.f64 	%fd5905, %fd2437, 0d0000000000000000;
	ld.global.f64 	%fd2438, [%rd211+8];
	add.f64 	%fd5904, %fd2438, 0d0000000000000000;
	bra.uni 	$L__BB7_97;

$L__BB7_95:
	setp.eq.s64 	%p110, %rd90, 0;
	mov.f64 	%fd5904, 0d0000000000000000;
	mov.f64 	%fd5905, %fd5904;
	@%p110 bra 	$L__BB7_97;

	cvta.to.global.u64 	%rd212, %rd90;
	mul.lo.s64 	%rd213, %rd75, %rd53;
	add.s64 	%rd214, %rd212, %rd213;
	ld.global.f64 	%fd2441, [%rd214];
	add.f64 	%fd5905, %fd2441, 0d0000000000000000;
	ld.global.f64 	%fd2442, [%rd214+8];
	add.f64 	%fd5904, %fd2442, 0d0000000000000000;

$L__BB7_97:
	sub.f64 	%fd217, %fd175, %fd172;
	mul.f64 	%fd2444, %fd217, %fd217;
	sub.f64 	%fd218, %fd174, %fd171;
	fma.rn.f64 	%fd2445, %fd218, %fd218, %fd2444;
	sub.f64 	%fd219, %fd173, %fd170;
	fma.rn.f64 	%fd220, %fd219, %fd219, %fd2445;
	sub.f64 	%fd221, %fd181, %fd178;
	mul.f64 	%fd2446, %fd217, %fd221;
	sub.f64 	%fd222, %fd180, %fd177;
	fma.rn.f64 	%fd2447, %fd218, %fd222, %fd2446;
	sub.f64 	%fd223, %fd179, %fd176;
	fma.rn.f64 	%fd224, %fd219, %fd223, %fd2447;
	mul.f64 	%fd2448, %fd221, %fd221;
	fma.rn.f64 	%fd2449, %fd222, %fd222, %fd2448;
	fma.rn.f64 	%fd225, %fd223, %fd223, %fd2449;
	sub.f64 	%fd226, %fd172, %fd178;
	mul.f64 	%fd2450, %fd217, %fd226;
	sub.f64 	%fd227, %fd171, %fd177;
	fma.rn.f64 	%fd2451, %fd218, %fd227, %fd2450;
	sub.f64 	%fd228, %fd170, %fd176;
	fma.rn.f64 	%fd229, %fd219, %fd228, %fd2451;
	mul.f64 	%fd2452, %fd226, %fd221;
	fma.rn.f64 	%fd2453, %fd227, %fd222, %fd2452;
	fma.rn.f64 	%fd230, %fd228, %fd223, %fd2453;
	mul.f64 	%fd2454, %fd220, %fd225;
	mul.f64 	%fd2455, %fd224, %fd224;
	sub.f64 	%fd231, %fd2454, %fd2455;
	mul.f64 	%fd2456, %fd224, %fd230;
	mul.f64 	%fd2457, %fd229, %fd225;
	sub.f64 	%fd232, %fd2456, %fd2457;
	setp.le.f64 	%p111, %fd232, 0d0000000000000000;
	@%p111 bra 	$L__BB7_101;

	setp.ge.f64 	%p2, %fd232, %fd231;
	add.f64 	%fd233, %fd230, %fd224;
	@%p2 bra 	$L__BB7_100;

	selp.f64 	%fd2459, %fd225, %fd231, %p2;
	mul.f64 	%fd2460, %fd220, %fd230;
	mul.f64 	%fd2461, %fd229, %fd224;
	sub.f64 	%fd2462, %fd2460, %fd2461;
	mul.f64 	%fd2463, %fd219, %fd222;
	mul.f64 	%fd2464, %fd218, %fd223;
	sub.f64 	%fd2465, %fd2464, %fd2463;
	mul.f64 	%fd2466, %fd217, %fd223;
	mul.f64 	%fd2467, %fd219, %fd221;
	sub.f64 	%fd2468, %fd2467, %fd2466;
	mul.f64 	%fd2469, %fd218, %fd221;
	mul.f64 	%fd2470, %fd217, %fd222;
	sub.f64 	%fd2471, %fd2470, %fd2469;
	setp.gt.f64 	%p112, %fd2462, 0d0000000000000000;
	setp.lt.f64 	%p113, %fd2462, %fd2459;
	mul.f64 	%fd2472, %fd227, %fd2468;
	fma.rn.f64 	%fd2473, %fd226, %fd2465, %fd2472;
	fma.rn.f64 	%fd2474, %fd228, %fd2471, %fd2473;
	setp.eq.f64 	%p114, %fd2474, 0d0000000000000000;
	mul.f64 	%fd2475, %fd2468, %fd2468;
	fma.rn.f64 	%fd2476, %fd2465, %fd2465, %fd2475;
	fma.rn.f64 	%fd2477, %fd2471, %fd2471, %fd2476;
	mul.f64 	%fd2478, %fd220, 0d3BC79CA100000000;
	mul.f64 	%fd2479, %fd2478, %fd225;
	setp.lt.f64 	%p115, %fd2477, %fd2479;
	or.pred  	%p116, %p114, %p115;
	and.pred  	%p117, %p112, %p113;
	and.pred  	%p118, %p116, %p117;
	mul.f64 	%fd2480, %fd231, 0d3FE0000000000000;
	setp.lt.f64 	%p119, %fd232, %fd2480;
	selp.b32 	%r612, 2, 5, %p119;
	selp.f64 	%fd2481, %fd230, %fd233, %p119;
	selp.f64 	%fd5907, %fd225, %fd2459, %p118;
	selp.b32 	%r710, %r612, 8, %p118;
	selp.f64 	%fd5906, %fd2481, %fd2462, %p118;

$L__BB7_100:
	selp.f64 	%fd5909, %fd225, %fd5907, %p2;
	selp.b32 	%r711, 5, %r710, %p2;
	selp.f64 	%fd5908, %fd233, %fd5906, %p2;

$L__BB7_101:
	selp.f64 	%fd242, %fd225, %fd5909, %p111;
	selp.b32 	%r712, 2, %r711, %p111;
	selp.f64 	%fd243, %fd230, %fd5908, %p111;
	setp.gtu.f64 	%p122, %fd243, 0d0000000000000000;
	@%p122 bra 	$L__BB7_105;
	bra.uni 	$L__BB7_102;

$L__BB7_105:
	setp.ltu.f64 	%p125, %fd243, %fd242;
	@%p125 bra 	$L__BB7_109;

	mov.f64 	%fd2483, 0d0000000000000000;
	sub.f64 	%fd2484, %fd2483, %fd229;
	add.f64 	%fd245, %fd2484, %fd224;
	setp.le.f64 	%p126, %fd245, 0d0000000000000000;
	mov.u32 	%r712, 1;
	@%p126 bra 	$L__BB7_109;

	setp.ge.f64 	%p127, %fd245, %fd220;
	mov.u32 	%r712, 4;
	@%p127 bra 	$L__BB7_109;

	mov.u32 	%r712, 7;
	bra.uni 	$L__BB7_109;

$L__BB7_102:
	mov.f64 	%fd2482, 0d0000000000000000;
	sub.f64 	%fd244, %fd2482, %fd229;
	setp.le.f64 	%p123, %fd244, 0d0000000000000000;
	mov.u32 	%r712, 0;
	@%p123 bra 	$L__BB7_109;

	setp.ge.f64 	%p124, %fd244, %fd220;
	mov.u32 	%r712, 3;
	@%p124 bra 	$L__BB7_109;

	mov.u32 	%r712, 6;

$L__BB7_109:
	setp.eq.s32 	%p128, %r712, 0;
	mov.f64 	%fd6118, 0d0000000000000000;
	mov.f64 	%fd6119, %fd6118;
	mov.f64 	%fd6120, %fd6118;
	mov.f64 	%fd6121, %fd6118;
	mov.f64 	%fd6122, %fd6118;
	mov.f64 	%fd6123, %fd6118;
	mov.f64 	%fd6010, %fd6118;
	mov.f64 	%fd6011, %fd6118;
	mov.f64 	%fd6012, %fd6118;
	mov.f64 	%fd6013, %fd6118;
	mov.f64 	%fd6014, %fd6118;
	mov.f64 	%fd6015, %fd6118;
	mov.f64 	%fd5981, %fd6118;
	mov.f64 	%fd5982, %fd6118;
	mov.f64 	%fd5983, %fd6118;
	mov.f64 	%fd5984, %fd6118;
	mov.f64 	%fd5985, %fd6118;
	mov.f64 	%fd5986, %fd6118;
	mov.f64 	%fd5959, %fd6118;
	mov.f64 	%fd5960, %fd6118;
	mov.f64 	%fd5961, %fd6118;
	mov.f64 	%fd5962, %fd6118;
	mov.f64 	%fd5963, %fd6118;
	mov.f64 	%fd5964, %fd6118;
	mov.f64 	%fd5965, %fd6118;
	mov.f64 	%fd5966, %fd6118;
	mov.f64 	%fd5967, %fd6118;
	mov.f64 	%fd5968, %fd6118;
	mov.f64 	%fd5969, %fd6118;
	mov.f64 	%fd5970, %fd6118;
	mov.f64 	%fd5971, %fd6118;
	mov.f64 	%fd5972, %fd6118;
	mov.f64 	%fd5973, %fd6118;
	mov.f64 	%fd6192, %fd6118;
	mov.f64 	%fd6193, %fd6118;
	@%p128 bra 	$L__BB7_139;

	setp.eq.s32 	%p3, %r712, 1;
	selp.f64 	%fd6117, 0d3FF0000000000000, 0d0000000000000000, %p3;
	mov.f64 	%fd6116, 0d0000000000000000;
	@%p3 bra 	$L__BB7_137;
	bra.uni 	$L__BB7_111;

$L__BB7_137:
	mov.f64 	%fd6118, 0d0000000000000000;
	mov.f64 	%fd6119, %fd6118;
	mov.f64 	%fd6120, %fd6118;
	mov.f64 	%fd6121, %fd6118;
	mov.f64 	%fd6122, %fd6118;
	mov.f64 	%fd6123, %fd6118;
	mov.f64 	%fd6010, %fd6118;
	mov.f64 	%fd6011, %fd6118;
	mov.f64 	%fd6012, %fd6118;
	mov.f64 	%fd6013, %fd6118;
	mov.f64 	%fd6014, %fd6118;
	mov.f64 	%fd6015, %fd6118;
	mov.f64 	%fd5981, %fd6118;
	mov.f64 	%fd5982, %fd6118;
	mov.f64 	%fd5983, %fd6118;
	mov.f64 	%fd5984, %fd6118;
	mov.f64 	%fd5985, %fd6118;
	mov.f64 	%fd5986, %fd6118;
	mov.f64 	%fd5959, %fd6118;
	mov.f64 	%fd5960, %fd6118;
	mov.f64 	%fd5961, %fd6118;
	mov.f64 	%fd5962, %fd6118;
	mov.f64 	%fd5963, %fd6118;
	mov.f64 	%fd5964, %fd6118;
	mov.f64 	%fd5965, %fd6118;
	mov.f64 	%fd5966, %fd6118;
	mov.f64 	%fd5967, %fd6118;
	mov.f64 	%fd5968, %fd6118;
	mov.f64 	%fd5969, %fd6118;
	mov.f64 	%fd5970, %fd6118;
	mov.f64 	%fd5971, %fd6118;
	mov.f64 	%fd5972, %fd6118;
	mov.f64 	%fd5973, %fd6118;
	mov.f64 	%fd6151, %fd6118;
	mov.f64 	%fd6152, %fd6118;
	bra.uni 	$L__BB7_138;

$L__BB7_111:
	setp.eq.s32 	%p4, %r712, 2;
	setp.ne.s32 	%p129, %r712, 2;
	mov.f64 	%fd6118, 0d0000000000000000;
	mov.f64 	%fd6119, %fd6118;
	mov.f64 	%fd6120, %fd6118;
	mov.f64 	%fd6121, %fd6118;
	mov.f64 	%fd6122, %fd6118;
	mov.f64 	%fd6123, %fd6118;
	mov.f64 	%fd6153, %fd6118;
	@%p129 bra 	$L__BB7_113;

	div.rn.f64 	%fd6153, %fd230, %fd225;
	mov.f64 	%fd6118, %fd228;
	mov.f64 	%fd6119, %fd227;
	mov.f64 	%fd6120, %fd226;
	mov.f64 	%fd6121, %fd223;
	mov.f64 	%fd6122, %fd222;
	mov.f64 	%fd6123, %fd221;

$L__BB7_113:
	selp.f64 	%fd6081, %fd6153, %fd6117, %p4;
	selp.f64 	%fd6080, 0d0000000000000000, %fd6116, %p4;
	@%p4 bra 	$L__BB7_135;
	bra.uni 	$L__BB7_114;

$L__BB7_135:
	mov.f64 	%fd6010, 0d0000000000000000;
	mov.f64 	%fd6011, %fd6010;
	mov.f64 	%fd6012, %fd6010;
	mov.f64 	%fd6013, %fd6010;
	mov.f64 	%fd6014, %fd6010;
	mov.f64 	%fd6015, %fd6010;
	mov.f64 	%fd5981, %fd6010;
	mov.f64 	%fd5982, %fd6010;
	mov.f64 	%fd5983, %fd6010;
	mov.f64 	%fd5984, %fd6010;
	mov.f64 	%fd5985, %fd6010;
	mov.f64 	%fd5986, %fd6010;
	mov.f64 	%fd5959, %fd6010;
	mov.f64 	%fd5960, %fd6010;
	mov.f64 	%fd5961, %fd6010;
	mov.f64 	%fd5962, %fd6010;
	mov.f64 	%fd5963, %fd6010;
	mov.f64 	%fd5964, %fd6010;
	mov.f64 	%fd5965, %fd6010;
	mov.f64 	%fd5966, %fd6010;
	mov.f64 	%fd5967, %fd6010;
	mov.f64 	%fd5968, %fd6010;
	mov.f64 	%fd5969, %fd6010;
	mov.f64 	%fd5970, %fd6010;
	mov.f64 	%fd5971, %fd6010;
	mov.f64 	%fd5972, %fd6010;
	mov.f64 	%fd5973, %fd6010;
	mov.f64 	%fd6109, %fd6010;
	mov.f64 	%fd6110, %fd6010;
	bra.uni 	$L__BB7_136;

$L__BB7_114:
	setp.eq.s32 	%p5, %r712, 3;
	selp.f64 	%fd2529, 0d3FF0000000000000, 0d0000000000000000, %p5;
	selp.f64 	%fd6044, %fd2529, %fd6080, %p5;
	selp.f64 	%fd6045, 0d0000000000000000, %fd6081, %p5;
	@%p5 bra 	$L__BB7_133;
	bra.uni 	$L__BB7_115;

$L__BB7_133:
	mov.f64 	%fd6010, 0d0000000000000000;
	mov.f64 	%fd6011, %fd6010;
	mov.f64 	%fd6012, %fd6010;
	mov.f64 	%fd6013, %fd6010;
	mov.f64 	%fd6014, %fd6010;
	mov.f64 	%fd6015, %fd6010;
	mov.f64 	%fd5981, %fd6010;
	mov.f64 	%fd5982, %fd6010;
	mov.f64 	%fd5983, %fd6010;
	mov.f64 	%fd5984, %fd6010;
	mov.f64 	%fd5985, %fd6010;
	mov.f64 	%fd5986, %fd6010;
	mov.f64 	%fd5959, %fd6010;
	mov.f64 	%fd5960, %fd6010;
	mov.f64 	%fd5961, %fd6010;
	mov.f64 	%fd5962, %fd6010;
	mov.f64 	%fd5963, %fd6010;
	mov.f64 	%fd5964, %fd6010;
	mov.f64 	%fd5965, %fd6010;
	mov.f64 	%fd5966, %fd6010;
	mov.f64 	%fd5967, %fd6010;
	mov.f64 	%fd5968, %fd6010;
	mov.f64 	%fd5969, %fd6010;
	mov.f64 	%fd5970, %fd6010;
	mov.f64 	%fd5971, %fd6010;
	mov.f64 	%fd5972, %fd6010;
	mov.f64 	%fd5973, %fd6010;
	mov.f64 	%fd6073, %fd6010;
	mov.f64 	%fd6074, %fd6010;
	bra.uni 	$L__BB7_134;

$L__BB7_115:
	setp.eq.s32 	%p6, %r712, 4;
	selp.f64 	%fd2530, 0d3FF0000000000000, 0d0000000000000000, %p6;
	selp.f64 	%fd6008, %fd2530, %fd6044, %p6;
	selp.f64 	%fd6009, %fd2530, %fd6045, %p6;
	@%p6 bra 	$L__BB7_131;
	bra.uni 	$L__BB7_116;

$L__BB7_131:
	mov.f64 	%fd6010, 0d0000000000000000;
	mov.f64 	%fd6011, %fd6010;
	mov.f64 	%fd6012, %fd6010;
	mov.f64 	%fd6013, %fd6010;
	mov.f64 	%fd6014, %fd6010;
	mov.f64 	%fd6015, %fd6010;
	mov.f64 	%fd5981, %fd6010;
	mov.f64 	%fd5982, %fd6010;
	mov.f64 	%fd5983, %fd6010;
	mov.f64 	%fd5984, %fd6010;
	mov.f64 	%fd5985, %fd6010;
	mov.f64 	%fd5986, %fd6010;
	mov.f64 	%fd5959, %fd6010;
	mov.f64 	%fd5960, %fd6010;
	mov.f64 	%fd5961, %fd6010;
	mov.f64 	%fd5962, %fd6010;
	mov.f64 	%fd5963, %fd6010;
	mov.f64 	%fd5964, %fd6010;
	mov.f64 	%fd5965, %fd6010;
	mov.f64 	%fd5966, %fd6010;
	mov.f64 	%fd5967, %fd6010;
	mov.f64 	%fd5968, %fd6010;
	mov.f64 	%fd5969, %fd6010;
	mov.f64 	%fd5970, %fd6010;
	mov.f64 	%fd5971, %fd6010;
	mov.f64 	%fd5972, %fd6010;
	mov.f64 	%fd5973, %fd6010;
	mov.f64 	%fd6037, %fd6010;
	mov.f64 	%fd6038, %fd6010;
	bra.uni 	$L__BB7_132;

$L__BB7_116:
	setp.eq.s32 	%p7, %r712, 5;
	setp.ne.s32 	%p131, %r712, 5;
	mov.f64 	%fd6010, 0d0000000000000000;
	mov.f64 	%fd6011, %fd6010;
	mov.f64 	%fd6012, %fd6010;
	mov.f64 	%fd6013, %fd6010;
	mov.f64 	%fd6014, %fd6010;
	mov.f64 	%fd6015, %fd6010;
	mov.f64 	%fd6039, %fd6010;
	mov.f64 	%fd5925, %fd6010;
	@%p131 bra 	$L__BB7_118;

	sub.f64 	%fd6012, %fd175, %fd178;
	mul.f64 	%fd2541, %fd6012, %fd221;
	sub.f64 	%fd6011, %fd174, %fd177;
	fma.rn.f64 	%fd2542, %fd6011, %fd222, %fd2541;
	sub.f64 	%fd6010, %fd173, %fd176;
	fma.rn.f64 	%fd2543, %fd6010, %fd223, %fd2542;
	div.rn.f64 	%fd6039, %fd2543, %fd225;
	mov.f64 	%fd5925, 0d3FF0000000000000;
	mov.f64 	%fd6013, %fd223;
	mov.f64 	%fd6014, %fd222;
	mov.f64 	%fd6015, %fd221;

$L__BB7_118:
	selp.f64 	%fd5979, %fd5925, %fd6008, %p7;
	selp.f64 	%fd5980, %fd6039, %fd6009, %p7;
	@%p7 bra 	$L__BB7_129;
	bra.uni 	$L__BB7_119;

$L__BB7_129:
	mov.f64 	%fd5981, 0d0000000000000000;
	mov.f64 	%fd5982, %fd5981;
	mov.f64 	%fd5983, %fd5981;
	mov.f64 	%fd5984, %fd5981;
	mov.f64 	%fd5985, %fd5981;
	mov.f64 	%fd5986, %fd5981;
	mov.f64 	%fd5959, %fd5981;
	mov.f64 	%fd5960, %fd5981;
	mov.f64 	%fd5961, %fd5981;
	mov.f64 	%fd5962, %fd5981;
	mov.f64 	%fd5963, %fd5981;
	mov.f64 	%fd5964, %fd5981;
	mov.f64 	%fd5965, %fd5981;
	mov.f64 	%fd5966, %fd5981;
	mov.f64 	%fd5967, %fd5981;
	mov.f64 	%fd5968, %fd5981;
	mov.f64 	%fd5969, %fd5981;
	mov.f64 	%fd5970, %fd5981;
	mov.f64 	%fd5971, %fd5981;
	mov.f64 	%fd5972, %fd5981;
	mov.f64 	%fd5973, %fd5981;
	mov.f64 	%fd6002, %fd5981;
	mov.f64 	%fd6003, %fd5981;
	bra.uni 	$L__BB7_130;

$L__BB7_119:
	setp.eq.s32 	%p8, %r712, 6;
	setp.ne.s32 	%p133, %r712, 6;
	mov.f64 	%fd5981, 0d0000000000000000;
	mov.f64 	%fd5982, %fd5981;
	mov.f64 	%fd5983, %fd5981;
	mov.f64 	%fd5984, %fd5981;
	mov.f64 	%fd5985, %fd5981;
	mov.f64 	%fd5986, %fd5981;
	mov.f64 	%fd6004, %fd5981;
	@%p133 bra 	$L__BB7_121;

	sub.f64 	%fd5983, %fd178, %fd172;
	mul.f64 	%fd2552, %fd217, %fd5983;
	sub.f64 	%fd5982, %fd177, %fd171;
	fma.rn.f64 	%fd2553, %fd218, %fd5982, %fd2552;
	sub.f64 	%fd5981, %fd176, %fd170;
	fma.rn.f64 	%fd2554, %fd219, %fd5981, %fd2553;
	div.rn.f64 	%fd6004, %fd2554, %fd220;
	mov.f64 	%fd5984, %fd219;
	mov.f64 	%fd5985, %fd218;
	mov.f64 	%fd5986, %fd217;

$L__BB7_121:
	selp.f64 	%fd5957, %fd6004, %fd5979, %p8;
	selp.f64 	%fd5958, 0d0000000000000000, %fd5980, %p8;
	@%p8 bra 	$L__BB7_127;
	bra.uni 	$L__BB7_122;

$L__BB7_127:
	mov.f64 	%fd5959, 0d0000000000000000;
	mov.f64 	%fd5960, %fd5959;
	mov.f64 	%fd5961, %fd5959;
	mov.f64 	%fd5962, %fd5959;
	mov.f64 	%fd5963, %fd5959;
	mov.f64 	%fd5964, %fd5959;
	mov.f64 	%fd5965, %fd5959;
	mov.f64 	%fd5966, %fd5959;
	mov.f64 	%fd5967, %fd5959;
	mov.f64 	%fd5968, %fd5959;
	mov.f64 	%fd5969, %fd5959;
	mov.f64 	%fd5970, %fd5959;
	mov.f64 	%fd5971, %fd5959;
	mov.f64 	%fd5972, %fd5959;
	mov.f64 	%fd5973, %fd5959;
	mov.f64 	%fd5974, %fd5959;
	mov.f64 	%fd5975, %fd5959;
	bra.uni 	$L__BB7_128;

$L__BB7_122:
	setp.eq.s32 	%p9, %r712, 7;
	setp.ne.s32 	%p135, %r712, 7;
	mov.f64 	%fd5965, 0d0000000000000000;
	mov.f64 	%fd5959, %fd5965;
	mov.f64 	%fd5960, %fd5965;
	mov.f64 	%fd5961, %fd5965;
	mov.f64 	%fd5962, %fd5965;
	mov.f64 	%fd5963, %fd5965;
	mov.f64 	%fd5964, %fd5965;
	mov.f64 	%fd5941, %fd5965;
	mov.f64 	%fd5976, %fd5965;
	@%p135 bra 	$L__BB7_124;

	sub.f64 	%fd5961, %fd181, %fd172;
	mul.f64 	%fd2565, %fd217, %fd5961;
	sub.f64 	%fd5960, %fd180, %fd171;
	fma.rn.f64 	%fd2566, %fd218, %fd5960, %fd2565;
	sub.f64 	%fd5959, %fd179, %fd170;
	fma.rn.f64 	%fd2567, %fd219, %fd5959, %fd2566;
	div.rn.f64 	%fd5976, %fd2567, %fd220;
	mov.f64 	%fd5941, 0d3FF0000000000000;
	mov.f64 	%fd5962, %fd219;
	mov.f64 	%fd5963, %fd218;
	mov.f64 	%fd5964, %fd217;

$L__BB7_124:
	selp.f64 	%fd318, %fd5941, %fd5958, %p9;
	selp.f64 	%fd317, %fd5976, %fd5957, %p9;
	mov.f64 	%fd5966, %fd5965;
	mov.f64 	%fd5967, %fd5965;
	mov.f64 	%fd5968, %fd5965;
	mov.f64 	%fd5969, %fd5965;
	mov.f64 	%fd5970, %fd5965;
	mov.f64 	%fd5971, %fd5965;
	mov.f64 	%fd5972, %fd5965;
	mov.f64 	%fd5973, %fd5965;
	mov.f64 	%fd5953, %fd5965;
	mov.f64 	%fd5954, %fd5965;
	@%p9 bra 	$L__BB7_126;

	mov.f64 	%fd2580, 0d0000000000000000;
	sub.f64 	%fd5977, %fd2580, %fd224;
	div.rn.f64 	%fd2581, %fd5977, %fd220;
	mul.f64 	%fd2582, %fd2581, %fd2581;
	mul.f64 	%fd2583, %fd220, %fd2582;
	sub.f64 	%fd2584, %fd225, %fd2583;
	sub.f64 	%fd5978, %fd2580, %fd229;
	mul.f64 	%fd2585, %fd5978, %fd2581;
	sub.f64 	%fd2586, %fd230, %fd2585;
	div.rn.f64 	%fd5953, %fd2586, %fd2584;
	mul.f64 	%fd2587, %fd220, %fd2581;
	mul.f64 	%fd2588, %fd2587, %fd5953;
	sub.f64 	%fd2589, %fd5978, %fd2588;
	div.rn.f64 	%fd5954, %fd2589, %fd220;
	mov.f64 	%fd5965, %fd228;
	mov.f64 	%fd5966, %fd227;
	mov.f64 	%fd5967, %fd226;
	mov.f64 	%fd5968, %fd219;
	mov.f64 	%fd5969, %fd218;
	mov.f64 	%fd5970, %fd217;
	mov.f64 	%fd5971, %fd223;
	mov.f64 	%fd5972, %fd222;
	mov.f64 	%fd5973, %fd221;

$L__BB7_126:
	selp.f64 	%fd5975, %fd317, %fd5954, %p9;
	selp.f64 	%fd5974, %fd318, %fd5953, %p9;
	selp.u16 	%rs344, 1, 0, %p9;

$L__BB7_128:
	selp.f64 	%fd6003, %fd5957, %fd5975, %p8;
	selp.f64 	%fd6002, %fd5958, %fd5974, %p8;
	selp.u16 	%rs346, 1, 0, %p8;

$L__BB7_130:
	selp.f64 	%fd6038, %fd5979, %fd6003, %p7;
	selp.f64 	%fd6037, %fd5980, %fd6002, %p7;
	selp.u16 	%rs349, 1, 0, %p7;

$L__BB7_132:
	selp.f64 	%fd6074, %fd6008, %fd6038, %p6;
	selp.f64 	%fd6073, %fd6009, %fd6037, %p6;
	selp.u16 	%rs353, 1, 0, %p6;

$L__BB7_134:
	selp.f64 	%fd6110, %fd6044, %fd6074, %p5;
	selp.f64 	%fd6109, %fd6045, %fd6073, %p5;
	selp.u16 	%rs358, 1, 0, %p5;

$L__BB7_136:
	selp.f64 	%fd6152, %fd6080, %fd6110, %p4;
	selp.f64 	%fd6151, %fd6081, %fd6109, %p4;
	selp.u16 	%rs364, 1, 0, %p4;

$L__BB7_138:
	selp.f64 	%fd6193, %fd6116, %fd6152, %p3;
	selp.f64 	%fd6192, %fd6117, %fd6151, %p3;
	selp.u16 	%rs363, 1, 0, %p3;

$L__BB7_139:
	selp.f64 	%fd607, 0d0000000000000000, %fd6192, %p128;
	selp.f64 	%fd605, 0d0000000000000000, %fd6193, %p128;
	mov.f64 	%fd2758, 0d3FF0000000000000;
	sub.f64 	%fd606, %fd2758, %fd605;
	mul.f64 	%fd2759, %fd172, %fd606;
	mul.f64 	%fd2760, %fd171, %fd606;
	mul.f64 	%fd2761, %fd170, %fd606;
	fma.rn.f64 	%fd2762, %fd175, %fd605, %fd2759;
	fma.rn.f64 	%fd2763, %fd174, %fd605, %fd2760;
	fma.rn.f64 	%fd2764, %fd173, %fd605, %fd2761;
	sub.f64 	%fd608, %fd2758, %fd607;
	mul.f64 	%fd2765, %fd178, %fd608;
	mul.f64 	%fd2766, %fd177, %fd608;
	mul.f64 	%fd2767, %fd176, %fd608;
	fma.rn.f64 	%fd2768, %fd181, %fd607, %fd2765;
	fma.rn.f64 	%fd2769, %fd180, %fd607, %fd2766;
	fma.rn.f64 	%fd2770, %fd179, %fd607, %fd2767;
	sub.f64 	%fd2771, %fd2762, %fd2768;
	sub.f64 	%fd2772, %fd2763, %fd2769;
	sub.f64 	%fd2773, %fd2764, %fd2770;
	mul.f64 	%fd2774, %fd2772, %fd2772;
	fma.rn.f64 	%fd2775, %fd2771, %fd2771, %fd2774;
	fma.rn.f64 	%fd2776, %fd2773, %fd2773, %fd2775;
	sqrt.rn.f64 	%fd2777, %fd2776;
	div.rn.f64 	%fd609, %fd2771, %fd2777;
	div.rn.f64 	%fd610, %fd2772, %fd2777;
	div.rn.f64 	%fd611, %fd2773, %fd2777;
	add.f64 	%fd612, %fd5905, 0d0000000000000000;
	add.f64 	%fd2778, %fd5903, 0d0000000000000000;
	add.f64 	%fd2779, %fd5902, 0d0000000000000000;
	mul.f64 	%fd2780, %fd2779, %fd2772;
	fma.rn.f64 	%fd2781, %fd2778, %fd2771, %fd2780;
	add.f64 	%fd2782, %fd5901, 0d0000000000000000;
	fma.rn.f64 	%fd2783, %fd2782, %fd2773, %fd2781;
	mul.f64 	%fd2784, %fd2777, %fd2777;
	div.rn.f64 	%fd613, %fd2783, %fd2784;
	div.rn.f64 	%fd2785, %fd2778, %fd2777;
	add.f64 	%fd6202, %fd2785, 0d0000000000000000;
	div.rn.f64 	%fd2786, %fd2779, %fd2777;
	add.f64 	%fd6201, %fd2786, 0d0000000000000000;
	div.rn.f64 	%fd2787, %fd2782, %fd2777;
	add.f64 	%fd6200, %fd2787, 0d0000000000000000;
	setp.leu.f64 	%p145, %fd2777, 0d0000000000000000;
	@%p145 bra 	$L__BB7_141;

	mov.f64 	%fd2788, 0d0000000000000000;
	sub.f64 	%fd2789, %fd2788, %fd613;
	fma.rn.f64 	%fd6202, %fd609, %fd2789, %fd6202;
	fma.rn.f64 	%fd6201, %fd610, %fd2789, %fd6201;
	fma.rn.f64 	%fd6200, %fd611, %fd2789, %fd6200;

$L__BB7_141:
	mov.f64 	%fd2790, 0d0000000000000000;
	sub.f64 	%fd2791, %fd2790, %fd6202;
	add.f64 	%fd2792, %fd2791, 0d0000000000000000;
	sub.f64 	%fd2793, %fd2790, %fd6201;
	add.f64 	%fd2794, %fd2793, 0d0000000000000000;
	sub.f64 	%fd2795, %fd2790, %fd6200;
	add.f64 	%fd2796, %fd2795, 0d0000000000000000;
	fma.rn.f64 	%fd6242, %fd607, %fd2792, 0d0000000000000000;
	fma.rn.f64 	%fd6241, %fd607, %fd2794, 0d0000000000000000;
	fma.rn.f64 	%fd6240, %fd607, %fd2796, 0d0000000000000000;
	mul.f64 	%fd2797, %fd181, %fd2792;
	fma.rn.f64 	%fd2798, %fd180, %fd2794, %fd2797;
	fma.rn.f64 	%fd2799, %fd179, %fd2796, %fd2798;
	add.f64 	%fd2800, %fd2799, 0d0000000000000000;
	fma.rn.f64 	%fd6239, %fd608, %fd2792, 0d0000000000000000;
	fma.rn.f64 	%fd6238, %fd608, %fd2794, 0d0000000000000000;
	fma.rn.f64 	%fd6237, %fd608, %fd2796, 0d0000000000000000;
	add.f64 	%fd2801, %fd5904, 0d0000000000000000;
	add.f64 	%fd2802, %fd2801, %fd2800;
	mul.f64 	%fd2803, %fd178, %fd2792;
	fma.rn.f64 	%fd2804, %fd177, %fd2794, %fd2803;
	fma.rn.f64 	%fd2805, %fd176, %fd2796, %fd2804;
	add.f64 	%fd2806, %fd2805, 0d0000000000000000;
	sub.f64 	%fd2807, %fd2790, %fd2806;
	add.f64 	%fd2808, %fd6202, 0d0000000000000000;
	fma.rn.f64 	%fd6236, %fd605, %fd2808, 0d0000000000000000;
	add.f64 	%fd2809, %fd6201, 0d0000000000000000;
	fma.rn.f64 	%fd6235, %fd605, %fd2809, 0d0000000000000000;
	add.f64 	%fd2810, %fd6200, 0d0000000000000000;
	fma.rn.f64 	%fd6234, %fd605, %fd2810, 0d0000000000000000;
	add.f64 	%fd632, %fd2802, %fd2807;
	mul.f64 	%fd2811, %fd175, %fd2808;
	fma.rn.f64 	%fd2812, %fd174, %fd2809, %fd2811;
	fma.rn.f64 	%fd2813, %fd173, %fd2810, %fd2812;
	add.f64 	%fd2814, %fd2813, 0d0000000000000000;
	fma.rn.f64 	%fd6233, %fd606, %fd2808, 0d0000000000000000;
	fma.rn.f64 	%fd6232, %fd606, %fd2809, 0d0000000000000000;
	fma.rn.f64 	%fd6231, %fd606, %fd2810, 0d0000000000000000;
	add.f64 	%fd2815, %fd612, %fd2814;
	mul.f64 	%fd2816, %fd172, %fd2808;
	fma.rn.f64 	%fd2817, %fd171, %fd2809, %fd2816;
	fma.rn.f64 	%fd2818, %fd170, %fd2810, %fd2817;
	add.f64 	%fd2819, %fd2818, 0d0000000000000000;
	sub.f64 	%fd2820, %fd2790, %fd2819;
	add.f64 	%fd636, %fd2815, %fd2820;
	and.b16  	%rs296, %rs363, 255;
	setp.ne.s16 	%p147, %rs296, 0;
	or.pred  	%p148, %p147, %p128;
	@%p148 bra 	$L__BB7_159;

	add.f64 	%fd6282, %fd632, 0d0000000000000000;
	and.b16  	%rs297, %rs364, 255;
	setp.eq.s16 	%p10, %rs297, 0;
	setp.ne.s16 	%p149, %rs297, 0;
	@%p149 bra 	$L__BB7_157;

	and.b16  	%rs298, %rs358, 255;
	setp.eq.s16 	%p11, %rs298, 0;
	setp.ne.s16 	%p150, %rs298, 0;
	mov.f64 	%fd6269, %fd6282;
	@%p150 bra 	$L__BB7_156;

	and.b16  	%rs299, %rs353, 255;
	setp.eq.s16 	%p12, %rs299, 0;
	setp.ne.s16 	%p151, %rs299, 0;
	mov.f64 	%fd6256, %fd6282;
	@%p151 bra 	$L__BB7_155;

	and.b16  	%rs300, %rs349, 255;
	setp.eq.s16 	%p13, %rs300, 0;
	setp.ne.s16 	%p152, %rs300, 0;
	mov.f64 	%fd6243, %fd6282;
	@%p152 bra 	$L__BB7_153;

	add.f64 	%fd6230, %fd636, 0d0000000000000000;
	and.b16  	%rs301, %rs346, 255;
	setp.eq.s16 	%p14, %rs301, 0;
	setp.ne.s16 	%p153, %rs301, 0;
	mov.f64 	%fd6229, %fd6282;
	@%p153 bra 	$L__BB7_151;

	and.b16  	%rs302, %rs344, 255;
	setp.ne.s16 	%p154, %rs302, 0;
	mov.f64 	%fd6215, %fd6282;
	mov.f64 	%fd6216, %fd6230;
	@%p154 bra 	$L__BB7_149;

	div.rn.f64 	%fd2823, %fd5977, %fd220;
	mul.f64 	%fd2824, %fd2823, %fd2823;
	mul.f64 	%fd2825, %fd220, %fd2824;
	sub.f64 	%fd2826, %fd225, %fd2825;
	mul.f64 	%fd2827, %fd2823, %fd5978;
	sub.f64 	%fd2828, %fd230, %fd2827;
	div.rn.f64 	%fd2829, %fd2828, %fd2826;
	mul.f64 	%fd2830, %fd220, %fd2823;
	mul.f64 	%fd2831, %fd2830, %fd2829;
	sub.f64 	%fd2832, %fd5978, %fd2831;
	div.rn.f64 	%fd2833, %fd2832, %fd220;
	div.rn.f64 	%fd2834, %fd6230, %fd220;
	add.f64 	%fd2835, %fd2834, 0d0000000000000000;
	mov.f64 	%fd6215, 0d0000000000000000;
	mul.f64 	%fd2836, %fd2833, %fd6230;
	div.rn.f64 	%fd2837, %fd2836, %fd220;
	sub.f64 	%fd2838, %fd6215, %fd2837;
	sub.f64 	%fd2839, %fd6215, %fd2835;
	fma.rn.f64 	%fd2840, %fd2829, %fd2839, 0d0000000000000000;
	fma.rn.f64 	%fd2841, %fd2830, %fd2839, %fd6282;
	fma.rn.f64 	%fd2842, %fd2823, %fd2840, %fd2838;
	fma.rn.f64 	%fd2843, %fd220, %fd2840, 0d0000000000000000;
	div.rn.f64 	%fd2844, %fd2841, %fd2826;
	add.f64 	%fd2845, %fd2844, 0d0000000000000000;
	mul.f64 	%fd2846, %fd2829, %fd2841;
	div.rn.f64 	%fd2847, %fd2846, %fd2826;
	sub.f64 	%fd2848, %fd6215, %fd2847;
	sub.f64 	%fd2849, %fd6215, %fd2845;
	fma.rn.f64 	%fd2850, %fd5978, %fd2849, %fd2843;
	fma.rn.f64 	%fd2851, %fd2823, %fd2849, %fd2835;
	add.f64 	%fd2852, %fd2851, 0d0000000000000000;
	add.f64 	%fd2853, %fd2848, 0d0000000000000000;
	sub.f64 	%fd2854, %fd6215, %fd2848;
	fma.rn.f64 	%fd2855, %fd220, %fd2854, 0d0000000000000000;
	fma.rn.f64 	%fd2856, %fd2824, %fd2854, %fd2842;
	fma.rn.f64 	%fd2857, %fd2823, %fd2855, %fd2850;
	fma.rn.f64 	%fd2858, %fd2823, %fd2855, %fd2857;
	div.rn.f64 	%fd2859, %fd2858, %fd220;
	add.f64 	%fd2860, %fd2859, 0d0000000000000000;
	mul.f64 	%fd2861, %fd2823, %fd2858;
	div.rn.f64 	%fd2862, %fd2861, %fd220;
	sub.f64 	%fd2863, %fd2856, %fd2862;
	add.f64 	%fd2864, %fd2863, 0d0000000000000000;
	fma.rn.f64 	%fd2865, %fd5973, %fd2845, 0d0000000000000000;
	fma.rn.f64 	%fd2866, %fd5972, %fd2845, 0d0000000000000000;
	fma.rn.f64 	%fd2867, %fd5971, %fd2845, 0d0000000000000000;
	fma.rn.f64 	%fd2868, %fd5967, %fd2845, 0d0000000000000000;
	fma.rn.f64 	%fd2869, %fd5966, %fd2845, 0d0000000000000000;
	fma.rn.f64 	%fd2870, %fd5965, %fd2845, 0d0000000000000000;
	sub.f64 	%fd2871, %fd6215, %fd2852;
	fma.rn.f64 	%fd2872, %fd5970, %fd2871, %fd2865;
	fma.rn.f64 	%fd2873, %fd5969, %fd2871, %fd2866;
	fma.rn.f64 	%fd2874, %fd5968, %fd2871, %fd2867;
	fma.rn.f64 	%fd2875, %fd5967, %fd2871, 0d0000000000000000;
	fma.rn.f64 	%fd2876, %fd5966, %fd2871, 0d0000000000000000;
	fma.rn.f64 	%fd2877, %fd5965, %fd2871, 0d0000000000000000;
	add.f64 	%fd2878, %fd5973, %fd5973;
	add.f64 	%fd2879, %fd5972, %fd5972;
	add.f64 	%fd2880, %fd5971, %fd5971;
	fma.rn.f64 	%fd2881, %fd2878, %fd2853, %fd2868;
	fma.rn.f64 	%fd2882, %fd2879, %fd2853, %fd2869;
	fma.rn.f64 	%fd2883, %fd2880, %fd2853, %fd2870;
	sub.f64 	%fd2884, %fd6215, %fd2860;
	fma.rn.f64 	%fd2885, %fd5973, %fd2884, %fd2875;
	fma.rn.f64 	%fd2886, %fd5972, %fd2884, %fd2876;
	fma.rn.f64 	%fd2887, %fd5971, %fd2884, %fd2877;
	fma.rn.f64 	%fd2888, %fd5970, %fd2884, %fd2881;
	fma.rn.f64 	%fd2889, %fd5969, %fd2884, %fd2882;
	fma.rn.f64 	%fd2890, %fd5968, %fd2884, %fd2883;
	add.f64 	%fd2891, %fd5970, %fd5970;
	add.f64 	%fd2892, %fd5969, %fd5969;
	add.f64 	%fd2893, %fd5968, %fd5968;
	fma.rn.f64 	%fd2894, %fd2891, %fd2864, %fd2885;
	fma.rn.f64 	%fd2895, %fd2892, %fd2864, %fd2886;
	fma.rn.f64 	%fd2896, %fd2893, %fd2864, %fd2887;
	add.f64 	%fd6242, %fd6242, %fd2888;
	add.f64 	%fd6241, %fd6241, %fd2889;
	add.f64 	%fd6240, %fd6240, %fd2890;
	sub.f64 	%fd2897, %fd6239, %fd2888;
	sub.f64 	%fd2898, %fd6238, %fd2889;
	sub.f64 	%fd2899, %fd6237, %fd2890;
	add.f64 	%fd6236, %fd6236, %fd2894;
	add.f64 	%fd6235, %fd6235, %fd2895;
	add.f64 	%fd6234, %fd6234, %fd2896;
	sub.f64 	%fd2900, %fd6233, %fd2894;
	sub.f64 	%fd2901, %fd6232, %fd2895;
	sub.f64 	%fd2902, %fd6231, %fd2896;
	add.f64 	%fd6233, %fd2872, %fd2900;
	add.f64 	%fd6232, %fd2873, %fd2901;
	add.f64 	%fd6231, %fd2874, %fd2902;
	sub.f64 	%fd6239, %fd2897, %fd2872;
	sub.f64 	%fd6238, %fd2898, %fd2873;
	sub.f64 	%fd6237, %fd2899, %fd2874;
	mov.f64 	%fd6216, %fd6215;

$L__BB7_149:
	selp.f64 	%fd2903, 0d0000000000000000, %fd6230, %p14;
	add.f64 	%fd6230, %fd2903, %fd6216;
	selp.f64 	%fd2904, 0d0000000000000000, %fd6282, %p14;
	add.f64 	%fd6229, %fd2904, %fd6215;
	setp.eq.s16 	%p155, %rs302, 0;
	@%p155 bra 	$L__BB7_151;

	add.f64 	%fd2907, %fd6216, 0d0000000000000000;
	mov.f64 	%fd6229, 0d0000000000000000;
	div.rn.f64 	%fd2908, %fd2907, %fd220;
	add.f64 	%fd2909, %fd2908, 0d0000000000000000;
	mul.f64 	%fd2910, %fd5976, %fd2907;
	div.rn.f64 	%fd2911, %fd2910, %fd220;
	sub.f64 	%fd2912, %fd6229, %fd2911;
	add.f64 	%fd2913, %fd5964, %fd5964;
	add.f64 	%fd2914, %fd5963, %fd5963;
	add.f64 	%fd2915, %fd5962, %fd5962;
	fma.rn.f64 	%fd2916, %fd2913, %fd2912, 0d0000000000000000;
	fma.rn.f64 	%fd2917, %fd2914, %fd2912, 0d0000000000000000;
	fma.rn.f64 	%fd2918, %fd2915, %fd2912, 0d0000000000000000;
	fma.rn.f64 	%fd2919, %fd5964, %fd2909, 0d0000000000000000;
	fma.rn.f64 	%fd2920, %fd5963, %fd2909, 0d0000000000000000;
	fma.rn.f64 	%fd2921, %fd5962, %fd2909, 0d0000000000000000;
	fma.rn.f64 	%fd2922, %fd5961, %fd2909, %fd2916;
	fma.rn.f64 	%fd2923, %fd5960, %fd2909, %fd2917;
	fma.rn.f64 	%fd2924, %fd5959, %fd2909, %fd2918;
	add.f64 	%fd6236, %fd6236, %fd2922;
	add.f64 	%fd6235, %fd6235, %fd2923;
	add.f64 	%fd6234, %fd6234, %fd2924;
	sub.f64 	%fd2925, %fd6233, %fd2922;
	sub.f64 	%fd2926, %fd6232, %fd2923;
	sub.f64 	%fd2927, %fd6231, %fd2924;
	add.f64 	%fd6242, %fd6242, %fd2919;
	add.f64 	%fd6241, %fd6241, %fd2920;
	add.f64 	%fd6240, %fd6240, %fd2921;
	sub.f64 	%fd6233, %fd2925, %fd2919;
	sub.f64 	%fd6232, %fd2926, %fd2920;
	sub.f64 	%fd6231, %fd2927, %fd2921;
	mov.f64 	%fd6230, %fd6229;

$L__BB7_151:
	selp.f64 	%fd2928, 0d0000000000000000, %fd6282, %p13;
	add.f64 	%fd6243, %fd2928, %fd6229;
	@%p14 bra 	$L__BB7_153;

	add.f64 	%fd2930, %fd6230, 0d0000000000000000;
	mov.f64 	%fd6243, 0d0000000000000000;
	div.rn.f64 	%fd2931, %fd2930, %fd220;
	add.f64 	%fd2932, %fd2931, 0d0000000000000000;
	mul.f64 	%fd2933, %fd6004, %fd2930;
	div.rn.f64 	%fd2934, %fd2933, %fd220;
	sub.f64 	%fd2935, %fd6243, %fd2934;
	add.f64 	%fd2936, %fd5986, %fd5986;
	add.f64 	%fd2937, %fd5985, %fd5985;
	add.f64 	%fd2938, %fd5984, %fd5984;
	fma.rn.f64 	%fd2939, %fd2936, %fd2935, 0d0000000000000000;
	fma.rn.f64 	%fd2940, %fd2937, %fd2935, 0d0000000000000000;
	fma.rn.f64 	%fd2941, %fd2938, %fd2935, 0d0000000000000000;
	fma.rn.f64 	%fd2942, %fd5986, %fd2932, 0d0000000000000000;
	fma.rn.f64 	%fd2943, %fd5985, %fd2932, 0d0000000000000000;
	fma.rn.f64 	%fd2944, %fd5984, %fd2932, 0d0000000000000000;
	fma.rn.f64 	%fd2945, %fd5983, %fd2932, %fd2939;
	fma.rn.f64 	%fd2946, %fd5982, %fd2932, %fd2940;
	fma.rn.f64 	%fd2947, %fd5981, %fd2932, %fd2941;
	add.f64 	%fd6236, %fd6236, %fd2945;
	add.f64 	%fd6235, %fd6235, %fd2946;
	add.f64 	%fd6234, %fd6234, %fd2947;
	sub.f64 	%fd2948, %fd6233, %fd2945;
	sub.f64 	%fd2949, %fd6232, %fd2946;
	sub.f64 	%fd2950, %fd6231, %fd2947;
	add.f64 	%fd6239, %fd6239, %fd2942;
	add.f64 	%fd6238, %fd6238, %fd2943;
	add.f64 	%fd6237, %fd6237, %fd2944;
	sub.f64 	%fd6233, %fd2948, %fd2942;
	sub.f64 	%fd6232, %fd2949, %fd2943;
	sub.f64 	%fd6231, %fd2950, %fd2944;

$L__BB7_153:
	selp.f64 	%fd2951, 0d0000000000000000, %fd6282, %p12;
	add.f64 	%fd6256, %fd2951, %fd6243;
	@%p13 bra 	$L__BB7_155;

	add.f64 	%fd2953, %fd6243, 0d0000000000000000;
	mov.f64 	%fd6256, 0d0000000000000000;
	div.rn.f64 	%fd2954, %fd2953, %fd225;
	add.f64 	%fd2955, %fd2954, 0d0000000000000000;
	mul.f64 	%fd2956, %fd6039, %fd2953;
	div.rn.f64 	%fd2957, %fd2956, %fd225;
	sub.f64 	%fd2958, %fd6256, %fd2957;
	add.f64 	%fd2959, %fd6015, %fd6015;
	add.f64 	%fd2960, %fd6014, %fd6014;
	add.f64 	%fd2961, %fd6013, %fd6013;
	fma.rn.f64 	%fd2962, %fd2959, %fd2958, 0d0000000000000000;
	fma.rn.f64 	%fd2963, %fd2960, %fd2958, 0d0000000000000000;
	fma.rn.f64 	%fd2964, %fd2961, %fd2958, 0d0000000000000000;
	fma.rn.f64 	%fd2965, %fd6015, %fd2955, 0d0000000000000000;
	fma.rn.f64 	%fd2966, %fd6014, %fd2955, 0d0000000000000000;
	fma.rn.f64 	%fd2967, %fd6013, %fd2955, 0d0000000000000000;
	fma.rn.f64 	%fd2968, %fd6012, %fd2955, %fd2962;
	fma.rn.f64 	%fd2969, %fd6011, %fd2955, %fd2963;
	fma.rn.f64 	%fd2970, %fd6010, %fd2955, %fd2964;
	add.f64 	%fd6242, %fd6242, %fd2968;
	add.f64 	%fd6241, %fd6241, %fd2969;
	add.f64 	%fd6240, %fd6240, %fd2970;
	sub.f64 	%fd2971, %fd6239, %fd2968;
	sub.f64 	%fd2972, %fd6238, %fd2969;
	sub.f64 	%fd2973, %fd6237, %fd2970;
	add.f64 	%fd6236, %fd6236, %fd2965;
	add.f64 	%fd6235, %fd6235, %fd2966;
	add.f64 	%fd6234, %fd6234, %fd2967;
	sub.f64 	%fd6239, %fd2971, %fd2965;
	sub.f64 	%fd6238, %fd2972, %fd2966;
	sub.f64 	%fd6237, %fd2973, %fd2967;

$L__BB7_155:
	selp.f64 	%fd2974, 0d0000000000000000, %fd6282, %p11;
	add.f64 	%fd2975, %fd2974, %fd6256;
	selp.f64 	%fd6269, %fd2975, %fd2974, %p12;

$L__BB7_156:
	selp.f64 	%fd2976, 0d0000000000000000, %fd6282, %p10;
	add.f64 	%fd2977, %fd2976, %fd6269;
	selp.f64 	%fd6282, %fd2977, %fd2976, %p11;

$L__BB7_157:
	@%p10 bra 	$L__BB7_159;

	add.f64 	%fd2978, %fd6282, 0d0000000000000000;
	mov.f64 	%fd2979, 0d0000000000000000;
	div.rn.f64 	%fd2980, %fd2978, %fd225;
	add.f64 	%fd2981, %fd2980, 0d0000000000000000;
	mul.f64 	%fd2982, %fd6153, %fd2978;
	div.rn.f64 	%fd2983, %fd2982, %fd225;
	sub.f64 	%fd2984, %fd2979, %fd2983;
	add.f64 	%fd2985, %fd6123, %fd6123;
	add.f64 	%fd2986, %fd6122, %fd6122;
	add.f64 	%fd2987, %fd6121, %fd6121;
	fma.rn.f64 	%fd2988, %fd2985, %fd2984, 0d0000000000000000;
	fma.rn.f64 	%fd2989, %fd2986, %fd2984, 0d0000000000000000;
	fma.rn.f64 	%fd2990, %fd2987, %fd2984, 0d0000000000000000;
	fma.rn.f64 	%fd2991, %fd6123, %fd2981, 0d0000000000000000;
	fma.rn.f64 	%fd2992, %fd6122, %fd2981, 0d0000000000000000;
	fma.rn.f64 	%fd2993, %fd6121, %fd2981, 0d0000000000000000;
	fma.rn.f64 	%fd2994, %fd6120, %fd2981, %fd2988;
	fma.rn.f64 	%fd2995, %fd6119, %fd2981, %fd2989;
	fma.rn.f64 	%fd2996, %fd6118, %fd2981, %fd2990;
	add.f64 	%fd6242, %fd6242, %fd2994;
	add.f64 	%fd6241, %fd6241, %fd2995;
	add.f64 	%fd6240, %fd6240, %fd2996;
	sub.f64 	%fd2997, %fd6239, %fd2994;
	sub.f64 	%fd2998, %fd6238, %fd2995;
	sub.f64 	%fd2999, %fd6237, %fd2996;
	add.f64 	%fd6233, %fd6233, %fd2991;
	add.f64 	%fd6232, %fd6232, %fd2992;
	add.f64 	%fd6231, %fd6231, %fd2993;
	sub.f64 	%fd6239, %fd2997, %fd2991;
	sub.f64 	%fd6238, %fd2998, %fd2992;
	sub.f64 	%fd6237, %fd2999, %fd2993;

$L__BB7_159:
	mov.f64 	%fd6295, 0d0000000000000000;
	mov.f64 	%fd3004, 0d7FF8000000000000;
	add.f64 	%fd6298, %fd3004, 0d0000000000000000;
	mov.f64 	%fd6296, 0d0000000000000000;
	mov.f64 	%fd6313, 0d0000000000000000;
	mov.f64 	%fd6297, %fd6313;
	@%p111 bra 	$L__BB7_162;

	mov.f64 	%fd6295, 0d0000000000000000;
	setp.ge.f64 	%p162, %fd232, %fd231;
	mov.u16 	%rs370, 1;
	@%p162 bra 	$L__BB7_162;

	mul.f64 	%fd3010, %fd218, %fd223;
	mul.f64 	%fd3011, %fd219, %fd222;
	sub.f64 	%fd6295, %fd3010, %fd3011;
	mul.f64 	%fd3012, %fd217, %fd223;
	mul.f64 	%fd3013, %fd219, %fd221;
	sub.f64 	%fd6296, %fd3013, %fd3012;
	mul.f64 	%fd3014, %fd218, %fd221;
	mul.f64 	%fd3015, %fd217, %fd222;
	sub.f64 	%fd6297, %fd3015, %fd3014;
	mul.f64 	%fd3016, %fd220, 0d3BC79CA100000000;
	fma.rn.f64 	%fd6298, %fd3016, 0d0000000000000000, 0d0000000000000000;
	mov.u16 	%rs370, 0;

$L__BB7_162:
	mov.f64 	%fd6314, %fd6313;
	mov.f64 	%fd6315, %fd6313;
	mov.f64 	%fd6316, %fd6313;
	mov.f64 	%fd6317, %fd6313;
	mov.f64 	%fd6318, %fd6313;
	mov.f64 	%fd6319, %fd6313;
	mov.f64 	%fd6320, %fd6313;
	mov.f64 	%fd6321, %fd6313;
	mov.f64 	%fd6322, %fd6313;
	mov.f64 	%fd6323, %fd6313;
	mov.f64 	%fd6324, %fd6313;
	mov.f64 	%fd6325, %fd6313;
	mov.f64 	%fd6326, %fd6313;
	@%p111 bra 	$L__BB7_167;

	and.b16  	%rs312, %rs370, 255;
	setp.ne.s16 	%p164, %rs312, 0;
	mov.f64 	%fd6313, 0d0000000000000000;
	mov.f64 	%fd6314, %fd6313;
	mov.f64 	%fd6315, %fd6313;
	mov.f64 	%fd6316, %fd6313;
	mov.f64 	%fd6317, %fd6313;
	mov.f64 	%fd6318, %fd6313;
	mov.f64 	%fd6319, %fd6313;
	mov.f64 	%fd6320, %fd6313;
	mov.f64 	%fd6321, %fd6313;
	mov.f64 	%fd6322, %fd6313;
	mov.f64 	%fd6323, %fd6313;
	mov.f64 	%fd6324, %fd6313;
	mov.f64 	%fd6325, %fd6313;
	mov.f64 	%fd6326, %fd6313;
	@%p164 bra 	$L__BB7_165;

	fma.rn.f64 	%fd3045, %fd225, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3046, %fd3045, 0d3BC79CA100000000, 0d0000000000000000;
	add.f64 	%fd3047, %fd6295, %fd6295;
	add.f64 	%fd3048, %fd6296, %fd6296;
	add.f64 	%fd3049, %fd6297, %fd6297;
	fma.rn.f64 	%fd3050, %fd3047, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3051, %fd3048, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3052, %fd3049, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3053, %fd226, 0d0000000000000000, %fd3050;
	fma.rn.f64 	%fd3054, %fd227, 0d0000000000000000, %fd3051;
	fma.rn.f64 	%fd3055, %fd228, 0d0000000000000000, %fd3052;
	fma.rn.f64 	%fd6318, %fd6295, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6319, %fd6296, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6320, %fd6297, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd3056, %fd222, %fd3055;
	mul.f64 	%fd3057, %fd223, %fd3054;
	sub.f64 	%fd3058, %fd3056, %fd3057;
	mul.f64 	%fd3059, %fd223, %fd3053;
	mul.f64 	%fd3060, %fd221, %fd3055;
	sub.f64 	%fd3061, %fd3059, %fd3060;
	mul.f64 	%fd3062, %fd221, %fd3054;
	mul.f64 	%fd3063, %fd222, %fd3053;
	sub.f64 	%fd3064, %fd3062, %fd3063;
	add.f64 	%fd6324, %fd3058, 0d0000000000000000;
	add.f64 	%fd6325, %fd3061, 0d0000000000000000;
	add.f64 	%fd6326, %fd3064, 0d0000000000000000;
	mul.f64 	%fd3065, %fd218, %fd3055;
	mul.f64 	%fd3066, %fd219, %fd3054;
	mul.f64 	%fd3067, %fd219, %fd3053;
	mul.f64 	%fd3068, %fd217, %fd3055;
	mul.f64 	%fd3069, %fd217, %fd3054;
	mul.f64 	%fd3070, %fd218, %fd3053;
	sub.f64 	%fd3071, %fd3066, %fd3065;
	add.f64 	%fd6321, %fd3071, 0d0000000000000000;
	sub.f64 	%fd3072, %fd3068, %fd3067;
	add.f64 	%fd6322, %fd3072, 0d0000000000000000;
	sub.f64 	%fd3073, %fd3070, %fd3069;
	add.f64 	%fd6323, %fd3073, 0d0000000000000000;
	fma.rn.f64 	%fd6316, %fd229, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6314, %fd224, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6317, %fd230, 0d0000000000000000, %fd3046;
	fma.rn.f64 	%fd6313, %fd220, 0d0000000000000000, 0d0000000000000000;
	mov.f64 	%fd6315, %fd6298;

$L__BB7_165:
	setp.eq.s16 	%p165, %rs312, 0;
	@%p165 bra 	$L__BB7_167;

	add.f64 	%fd6315, %fd6315, 0d0000000000000000;
	add.f64 	%fd6313, %fd6313, 0d0000000000000000;
	add.f64 	%fd6316, %fd6316, 0d0000000000000000;

$L__BB7_167:
	add.f64 	%fd3076, %fd6313, 0d0000000000000000;
	mov.f64 	%fd3075, 0d0000000000000000;
	selp.f64 	%fd3077, %fd3076, %fd6313, %p111;
	add.f64 	%fd3078, %fd6315, 0d0000000000000000;
	selp.f64 	%fd3079, %fd3078, %fd6315, %p111;
	mul.f64 	%fd837, %fd229, 0d0000000000000000;
	add.f64 	%fd3080, %fd837, %fd3079;
	mul.f64 	%fd838, %fd225, 0d0000000000000000;
	add.f64 	%fd3081, %fd838, %fd6314;
	mul.f64 	%fd839, %fd230, 0d0000000000000000;
	add.f64 	%fd3082, %fd839, %fd6316;
	mul.f64 	%fd840, %fd224, 0d0000000000000000;
	add.f64 	%fd3083, %fd840, %fd3077;
	add.f64 	%fd3084, %fd840, %fd3082;
	add.f64 	%fd3085, %fd840, %fd3084;
	add.f64 	%fd3086, %fd838, %fd6317;
	mul.f64 	%fd841, %fd220, 0d0000000000000000;
	add.f64 	%fd3087, %fd841, %fd3080;
	fma.rn.f64 	%fd3088, %fd226, %fd3083, %fd6321;
	fma.rn.f64 	%fd3089, %fd227, %fd3083, %fd6322;
	fma.rn.f64 	%fd3090, %fd228, %fd3083, %fd6323;
	fma.rn.f64 	%fd3091, %fd221, %fd3083, %fd6318;
	fma.rn.f64 	%fd3092, %fd222, %fd3083, %fd6319;
	fma.rn.f64 	%fd3093, %fd223, %fd3083, %fd6320;
	fma.rn.f64 	%fd3094, %fd226, %fd3081, %fd6324;
	fma.rn.f64 	%fd3095, %fd227, %fd3081, %fd6325;
	fma.rn.f64 	%fd3096, %fd228, %fd3081, %fd6326;
	fma.rn.f64 	%fd3097, %fd217, %fd3081, %fd3091;
	fma.rn.f64 	%fd3098, %fd218, %fd3081, %fd3092;
	fma.rn.f64 	%fd3099, %fd219, %fd3081, %fd3093;
	add.f64 	%fd842, %fd221, %fd221;
	add.f64 	%fd843, %fd222, %fd222;
	add.f64 	%fd844, %fd223, %fd223;
	fma.rn.f64 	%fd3100, %fd842, %fd3087, %fd3088;
	fma.rn.f64 	%fd3101, %fd843, %fd3087, %fd3089;
	fma.rn.f64 	%fd3102, %fd844, %fd3087, %fd3090;
	fma.rn.f64 	%fd3103, %fd221, %fd3085, %fd3094;
	fma.rn.f64 	%fd3104, %fd222, %fd3085, %fd3095;
	fma.rn.f64 	%fd3105, %fd223, %fd3085, %fd3096;
	fma.rn.f64 	%fd3106, %fd217, %fd3085, %fd3100;
	fma.rn.f64 	%fd3107, %fd218, %fd3085, %fd3101;
	fma.rn.f64 	%fd3108, %fd219, %fd3085, %fd3102;
	add.f64 	%fd845, %fd217, %fd217;
	add.f64 	%fd846, %fd218, %fd218;
	add.f64 	%fd847, %fd219, %fd219;
	fma.rn.f64 	%fd3109, %fd845, %fd3086, %fd3103;
	fma.rn.f64 	%fd3110, %fd846, %fd3086, %fd3104;
	fma.rn.f64 	%fd3111, %fd847, %fd3086, %fd3105;
	add.f64 	%fd3112, %fd6233, %fd3097;
	add.f64 	%fd3113, %fd6232, %fd3098;
	add.f64 	%fd3114, %fd6231, %fd3099;
	sub.f64 	%fd3115, %fd6239, %fd3097;
	sub.f64 	%fd3116, %fd6238, %fd3098;
	sub.f64 	%fd3117, %fd6237, %fd3099;
	add.f64 	%fd934, %fd6242, %fd3106;
	add.f64 	%fd933, %fd6241, %fd3107;
	add.f64 	%fd932, %fd6240, %fd3108;
	sub.f64 	%fd931, %fd3115, %fd3106;
	sub.f64 	%fd930, %fd3116, %fd3107;
	sub.f64 	%fd929, %fd3117, %fd3108;
	add.f64 	%fd928, %fd6236, %fd3109;
	add.f64 	%fd927, %fd6235, %fd3110;
	add.f64 	%fd926, %fd6234, %fd3111;
	sub.f64 	%fd925, %fd3112, %fd3109;
	sub.f64 	%fd924, %fd3113, %fd3110;
	sub.f64 	%fd923, %fd3114, %fd3111;
	and.b16  	%rs314, %rs340, 255;
	setp.eq.s16 	%p167, %rs314, 0;
	mov.f64 	%fd6338, %fd3075;
	mov.f64 	%fd6339, %fd3075;
	@%p167 bra 	$L__BB7_210;

	and.b16  	%rs315, %rs341, 255;
	setp.eq.s16 	%p168, %rs315, 0;
	mov.f64 	%fd6340, 0d0000000000000000;
	mov.f64 	%fd6338, %fd6340;
	mov.f64 	%fd6339, %fd6340;
	@%p168 bra 	$L__BB7_192;

	setp.eq.s64 	%p169, %rd119, 0;
	@%p169 bra 	$L__BB7_171;

	cvta.to.global.u64 	%rd215, %rd119;
	mul.lo.s64 	%rd216, %rd75, %rd56;
	add.s64 	%rd217, %rd215, %rd216;
	ld.global.f64 	%fd3120, [%rd217];
	add.f64 	%fd6327, %fd3120, 0d0000000000000000;
	bra.uni 	$L__BB7_173;

$L__BB7_171:
	setp.eq.s64 	%p170, %rd88, 0;
	mov.f64 	%fd6327, 0d0000000000000000;
	@%p170 bra 	$L__BB7_173;

	cvta.to.global.u64 	%rd218, %rd88;
	mul.lo.s64 	%rd219, %rd75, %rd57;
	add.s64 	%rd220, %rd218, %rd219;
	ld.global.f64 	%fd3122, [%rd220];
	add.f64 	%fd6327, %fd3122, 0d0000000000000000;

$L__BB7_173:
	mov.f64 	%fd6339, 0d0000000000000000;
	sub.f64 	%fd3125, %fd6339, %fd6327;
	fma.rn.f64 	%fd3126, %fd5818, %fd3125, 0d0000000000000000;
	fma.rn.f64 	%fd3127, %fd5819, %fd3125, 0d0000000000000000;
	rcp.rn.f64 	%fd3128, %fd5818;
	mul.f64 	%fd3129, %fd3128, 0d3FE0000000000000;
	fma.rn.f64 	%fd6338, %fd3129, %fd3127, 0d0000000000000000;
	fma.rn.f64 	%fd3130, %fd3126, 0d4000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd864, %fd5820, %fd3130, 0d0000000000000000;
	fma.rn.f64 	%fd865, %fd5821, %fd3130, 0d0000000000000000;
	setp.geu.f64 	%p171, %fd5830, %fd5829;
	@%p171 bra 	$L__BB7_182;

	sub.f64 	%fd3131, %fd5830, %fd5829;
	div.rn.f64 	%fd6331, %fd3131, %fd5829;
	div.rn.f64 	%fd6332, %fd5830, %fd5829;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r713}, %fd6332;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r714, %temp}, %fd6332;
	}
	setp.gt.s32 	%p172, %r713, 1048575;
	mov.u32 	%r715, -1023;
	mov.f64 	%fd6328, %fd6332;
	@%p172 bra 	$L__BB7_176;

	mul.f64 	%fd6328, %fd6332, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r713}, %fd6328;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r714, %temp}, %fd6328;
	}
	mov.u32 	%r715, -1077;

$L__BB7_176:
	add.s32 	%r621, %r713, -1;
	setp.lt.u32 	%p173, %r621, 2146435071;
	@%p173 bra 	$L__BB7_178;
	bra.uni 	$L__BB7_177;

$L__BB7_178:
	shr.u32 	%r623, %r713, 20;
	add.s32 	%r716, %r715, %r623;
	and.b32  	%r624, %r713, -2146435073;
	or.b32  	%r625, %r624, 1072693248;
	mov.b64 	%fd6329, {%r714, %r625};
	setp.lt.s32 	%p175, %r625, 1073127583;
	@%p175 bra 	$L__BB7_180;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r626, %temp}, %fd6329;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r627}, %fd6329;
	}
	add.s32 	%r628, %r627, -1048576;
	mov.b64 	%fd6329, {%r626, %r628};
	add.s32 	%r716, %r716, 1;

$L__BB7_180:
	add.f64 	%fd3134, %fd6329, 0d3FF0000000000000;
	mov.f64 	%fd3135, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd3136, %fd3134;
	neg.f64 	%fd3137, %fd3134;
	fma.rn.f64 	%fd3138, %fd3137, %fd3136, %fd3135;
	fma.rn.f64 	%fd3139, %fd3138, %fd3138, %fd3138;
	fma.rn.f64 	%fd3140, %fd3139, %fd3136, %fd3136;
	add.f64 	%fd3141, %fd6329, 0dBFF0000000000000;
	mul.f64 	%fd3142, %fd3141, %fd3140;
	fma.rn.f64 	%fd3143, %fd3141, %fd3140, %fd3142;
	mul.f64 	%fd3144, %fd3143, %fd3143;
	mov.f64 	%fd3145, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd3146, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd3147, %fd3146, %fd3144, %fd3145;
	mov.f64 	%fd3148, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd3149, %fd3147, %fd3144, %fd3148;
	mov.f64 	%fd3150, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd3151, %fd3149, %fd3144, %fd3150;
	mov.f64 	%fd3152, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd3153, %fd3151, %fd3144, %fd3152;
	mov.f64 	%fd3154, 0d3F624924923BE72D;
	fma.rn.f64 	%fd3155, %fd3153, %fd3144, %fd3154;
	mov.f64 	%fd3156, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd3157, %fd3155, %fd3144, %fd3156;
	mov.f64 	%fd3158, 0d3FB5555555555554;
	fma.rn.f64 	%fd3159, %fd3157, %fd3144, %fd3158;
	sub.f64 	%fd3160, %fd3141, %fd3143;
	add.f64 	%fd3161, %fd3160, %fd3160;
	neg.f64 	%fd3162, %fd3143;
	fma.rn.f64 	%fd3163, %fd3162, %fd3141, %fd3161;
	mul.f64 	%fd3164, %fd3140, %fd3163;
	mul.f64 	%fd3165, %fd3144, %fd3159;
	fma.rn.f64 	%fd3166, %fd3165, %fd3143, %fd3164;
	xor.b32  	%r629, %r716, -2147483648;
	mov.u32 	%r630, -2147483648;
	mov.u32 	%r631, 1127219200;
	mov.b64 	%fd3167, {%r629, %r631};
	mov.b64 	%fd3168, {%r630, %r631};
	sub.f64 	%fd3169, %fd3167, %fd3168;
	mov.f64 	%fd3170, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd3171, %fd3169, %fd3170, %fd3143;
	neg.f64 	%fd3172, %fd3169;
	fma.rn.f64 	%fd3173, %fd3172, %fd3170, %fd3171;
	sub.f64 	%fd3174, %fd3173, %fd3143;
	sub.f64 	%fd3175, %fd3166, %fd3174;
	mov.f64 	%fd3176, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd3177, %fd3169, %fd3176, %fd3175;
	add.f64 	%fd6333, %fd3171, %fd3177;
	bra.uni 	$L__BB7_181;

$L__BB7_177:
	mov.f64 	%fd3132, 0d7FF0000000000000;
	fma.rn.f64 	%fd3133, %fd6328, %fd3132, %fd3132;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r622}, %fd6328;
	}
	mov.b32 	%f3, %r622;
	setp.eq.f32 	%p174, %f3, 0f00000000;
	selp.f64 	%fd6333, 0dFFF0000000000000, %fd3133, %p174;

$L__BB7_181:
	mul.f64 	%fd3178, %fd6331, %fd6333;
	mul.f64 	%fd3179, %fd3178, 0dC000000000000000;
	div.rn.f64 	%fd6334, %fd3179, %fd5829;
	mul.f64 	%fd3180, %fd6331, %fd6331;
	div.rn.f64 	%fd6335, %fd3180, %fd5830;

$L__BB7_182:
	setp.lt.f64 	%p177, %fd5830, %fd5829;
	selp.f64 	%fd883, %fd865, 0d0000000000000000, %p177;
	@%p171 bra 	$L__BB7_184;

	fma.rn.f64 	%fd3182, %fd883, %fd1868, 0d0000000000000000;
	mov.f64 	%fd3183, 0d0000000000000000;
	sub.f64 	%fd3184, %fd3183, %fd3182;
	div.rn.f64 	%fd3185, %fd3184, %fd5830;
	add.f64 	%fd3186, %fd3185, 0d0000000000000000;
	mul.f64 	%fd3187, %fd3184, %fd6335;
	div.rn.f64 	%fd3188, %fd3187, %fd5830;
	sub.f64 	%fd3189, %fd6338, %fd3188;
	fma.rn.f64 	%fd3190, %fd3186, %fd6331, 0d0000000000000000;
	div.rn.f64 	%fd3191, %fd3190, %fd5829;
	add.f64 	%fd3192, %fd3191, 0d0000000000000000;
	mul.f64 	%fd3193, %fd6331, %fd3190;
	div.rn.f64 	%fd3194, %fd3193, %fd5829;
	sub.f64 	%fd3195, %fd3183, %fd3194;
	add.f64 	%fd3196, %fd3191, %fd3192;
	sub.f64 	%fd3197, %fd3195, %fd3194;
	div.rn.f64 	%fd3198, %fd3182, %fd5829;
	add.f64 	%fd3199, %fd3198, 0d0000000000000000;
	mul.f64 	%fd3200, %fd3182, %fd6334;
	div.rn.f64 	%fd3201, %fd3200, %fd5829;
	sub.f64 	%fd3202, %fd3197, %fd3201;
	add.f64 	%fd3203, %fd3199, %fd3199;
	sub.f64 	%fd3204, %fd3183, %fd3203;
	fma.rn.f64 	%fd3205, %fd3204, %fd6333, 0d0000000000000000;
	fma.rn.f64 	%fd3206, %fd3204, %fd6331, 0d0000000000000000;
	rcp.rn.f64 	%fd3207, %fd6332;
	fma.rn.f64 	%fd3208, %fd3207, %fd3206, 0d0000000000000000;
	div.rn.f64 	%fd3209, %fd3208, %fd5829;
	add.f64 	%fd3210, %fd3209, %fd3189;
	mul.f64 	%fd3211, %fd6332, %fd3208;
	div.rn.f64 	%fd3212, %fd3211, %fd5829;
	sub.f64 	%fd3213, %fd3202, %fd3212;
	div.rn.f64 	%fd3214, %fd3205, %fd5829;
	add.f64 	%fd3215, %fd3196, %fd3214;
	mul.f64 	%fd3216, %fd6331, %fd3205;
	div.rn.f64 	%fd3217, %fd3216, %fd5829;
	sub.f64 	%fd3218, %fd3213, %fd3217;
	add.f64 	%fd6338, %fd3215, %fd3210;
	sub.f64 	%fd6339, %fd3218, %fd3215;

$L__BB7_184:
	fma.rn.f64 	%fd3219, %fd864, %fd1867, 0d0000000000000000;
	fma.rn.f64 	%fd888, %fd3219, %fd1869, 0d0000000000000000;
	setp.eq.s64 	%p178, %rd137, 0;
	@%p178 bra 	$L__BB7_186;

	cvt.s64.s32 	%rd222, %r675;
	mul.lo.s64 	%rd223, %rd222, %rd61;
	add.s64 	%rd221, %rd137, %rd223;
	// begin inline asm
	{ atom.add.f64 %fd3220,[%rd221],%fd888; }

	// end inline asm
	bra.uni 	$L__BB7_188;

$L__BB7_186:
	setp.eq.s64 	%p179, %rd114, 0;
	@%p179 bra 	$L__BB7_188;

	cvt.s64.s32 	%rd225, %r675;
	mul.lo.s64 	%rd226, %rd225, %rd55;
	add.s64 	%rd224, %rd114, %rd226;
	// begin inline asm
	{ atom.add.f64 %fd3222,[%rd224],%fd888; }

	// end inline asm

$L__BB7_188:
	@%p178 bra 	$L__BB7_190;

	cvt.s64.s32 	%rd228, %r676;
	mul.lo.s64 	%rd229, %rd228, %rd61;
	add.s64 	%rd227, %rd137, %rd229;
	// begin inline asm
	{ atom.add.f64 %fd3224,[%rd227],%fd888; }

	// end inline asm
	bra.uni 	$L__BB7_192;

$L__BB7_190:
	setp.eq.s64 	%p181, %rd114, 0;
	@%p181 bra 	$L__BB7_192;

	cvt.s64.s32 	%rd231, %r676;
	mul.lo.s64 	%rd232, %rd231, %rd55;
	add.s64 	%rd230, %rd114, %rd232;
	// begin inline asm
	{ atom.add.f64 %fd3226,[%rd230],%fd888; }

	// end inline asm

$L__BB7_192:
	mul.f64 	%fd3230, %fd219, %fd222;
	mul.f64 	%fd3231, %fd218, %fd223;
	sub.f64 	%fd891, %fd3231, %fd3230;
	mul.f64 	%fd3232, %fd217, %fd223;
	mul.f64 	%fd3233, %fd219, %fd221;
	sub.f64 	%fd892, %fd3233, %fd3232;
	mul.f64 	%fd3234, %fd218, %fd221;
	mul.f64 	%fd3235, %fd217, %fd222;
	sub.f64 	%fd893, %fd3235, %fd3234;
	mul.f64 	%fd3236, %fd892, %fd892;
	fma.rn.f64 	%fd3237, %fd891, %fd891, %fd3236;
	fma.rn.f64 	%fd894, %fd893, %fd893, %fd3237;
	setp.geu.f64 	%p182, %fd894, %fd5896;
	mov.f64 	%fd6341, %fd6340;
	@%p182 bra 	$L__BB7_194;

	div.rn.f64 	%fd3238, %fd894, %fd5896;
	mov.f64 	%fd3239, 0d0000000000000000;
	sub.f64 	%fd3240, %fd3239, %fd3238;
	add.f64 	%fd3241, %fd3240, 0d4000000000000000;
	fma.rn.f64 	%fd3242, %fd3238, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3243, %fd3241, 0d0000000000000000, 0d0000000000000000;
	sub.f64 	%fd3244, %fd3243, %fd3242;
	div.rn.f64 	%fd3245, %fd3244, %fd5896;
	add.f64 	%fd6341, %fd3245, 0d0000000000000000;
	mul.f64 	%fd3246, %fd3238, %fd3244;
	div.rn.f64 	%fd3247, %fd3246, %fd5896;
	sub.f64 	%fd6340, %fd3239, %fd3247;

$L__BB7_194:
	add.f64 	%fd3248, %fd891, %fd891;
	add.f64 	%fd3249, %fd892, %fd892;
	add.f64 	%fd3250, %fd893, %fd893;
	fma.rn.f64 	%fd3251, %fd3248, %fd6341, 0d0000000000000000;
	fma.rn.f64 	%fd3252, %fd3249, %fd6341, 0d0000000000000000;
	fma.rn.f64 	%fd3253, %fd3250, %fd6341, 0d0000000000000000;
	mul.f64 	%fd3254, %fd222, %fd3253;
	mul.f64 	%fd3255, %fd223, %fd3252;
	sub.f64 	%fd3256, %fd3254, %fd3255;
	mul.f64 	%fd3257, %fd223, %fd3251;
	mul.f64 	%fd3258, %fd221, %fd3253;
	sub.f64 	%fd3259, %fd3257, %fd3258;
	mul.f64 	%fd3260, %fd221, %fd3252;
	mul.f64 	%fd3261, %fd222, %fd3251;
	sub.f64 	%fd3262, %fd3260, %fd3261;
	add.f64 	%fd3263, %fd3256, 0d0000000000000000;
	add.f64 	%fd3264, %fd3259, 0d0000000000000000;
	add.f64 	%fd3265, %fd3262, 0d0000000000000000;
	mul.f64 	%fd3266, %fd218, %fd3253;
	mul.f64 	%fd3267, %fd219, %fd3252;
	mul.f64 	%fd3268, %fd219, %fd3251;
	mul.f64 	%fd3269, %fd217, %fd3253;
	mul.f64 	%fd3270, %fd217, %fd3252;
	mul.f64 	%fd3271, %fd218, %fd3251;
	sub.f64 	%fd3272, %fd3267, %fd3266;
	add.f64 	%fd3273, %fd3272, 0d0000000000000000;
	sub.f64 	%fd3274, %fd3269, %fd3268;
	add.f64 	%fd3275, %fd3274, 0d0000000000000000;
	sub.f64 	%fd3276, %fd3271, %fd3270;
	add.f64 	%fd3277, %fd3276, 0d0000000000000000;
	add.f64 	%fd934, %fd934, %fd3273;
	add.f64 	%fd933, %fd933, %fd3275;
	add.f64 	%fd932, %fd932, %fd3277;
	sub.f64 	%fd931, %fd931, %fd3273;
	sub.f64 	%fd930, %fd930, %fd3275;
	sub.f64 	%fd929, %fd929, %fd3277;
	add.f64 	%fd928, %fd928, %fd3263;
	add.f64 	%fd927, %fd927, %fd3264;
	add.f64 	%fd926, %fd926, %fd3265;
	sub.f64 	%fd925, %fd925, %fd3263;
	sub.f64 	%fd924, %fd924, %fd3264;
	sub.f64 	%fd923, %fd923, %fd3265;
	sub.f64 	%fd3278, %fd5888, %fd5885;
	mul.f64 	%fd3279, %fd3278, %fd3278;
	sub.f64 	%fd3280, %fd5887, %fd5884;
	fma.rn.f64 	%fd3281, %fd3280, %fd3280, %fd3279;
	sub.f64 	%fd3282, %fd5886, %fd5883;
	fma.rn.f64 	%fd3283, %fd3282, %fd3282, %fd3281;
	mul.f64 	%fd3284, %fd3283, 0d3F50624DE0000000;
	sub.f64 	%fd3285, %fd5894, %fd5891;
	mul.f64 	%fd3286, %fd3285, %fd3285;
	sub.f64 	%fd3287, %fd5893, %fd5890;
	fma.rn.f64 	%fd3288, %fd3287, %fd3287, %fd3286;
	sub.f64 	%fd3289, %fd5892, %fd5889;
	fma.rn.f64 	%fd3290, %fd3289, %fd3289, %fd3288;
	add.f64 	%fd3291, %fd6340, 0d0000000000000000;
	fma.rn.f64 	%fd3292, %fd3290, %fd3291, 0d0000000000000000;
	fma.rn.f64 	%fd3293, %fd3284, %fd3291, 0d0000000000000000;
	add.f64 	%fd3294, %fd3285, %fd3285;
	add.f64 	%fd3295, %fd3287, %fd3287;
	add.f64 	%fd3296, %fd3289, %fd3289;
	fma.rn.f64 	%fd911, %fd3294, %fd3293, 0d0000000000000000;
	fma.rn.f64 	%fd912, %fd3295, %fd3293, 0d0000000000000000;
	fma.rn.f64 	%fd913, %fd3296, %fd3293, 0d0000000000000000;
	fma.rn.f64 	%fd3297, %fd3292, 0d3F50624DE0000000, 0d0000000000000000;
	add.f64 	%fd3298, %fd3278, %fd3278;
	add.f64 	%fd3299, %fd3280, %fd3280;
	add.f64 	%fd3300, %fd3282, %fd3282;
	fma.rn.f64 	%fd914, %fd3298, %fd3297, 0d0000000000000000;
	fma.rn.f64 	%fd915, %fd3299, %fd3297, 0d0000000000000000;
	fma.rn.f64 	%fd916, %fd3300, %fd3297, 0d0000000000000000;
	setp.eq.s64 	%p183, %rd127, 0;
	@%p183 bra 	$L__BB7_196;

	cvt.s64.s32 	%rd236, %r709;
	mul.lo.s64 	%rd237, %rd236, %rd60;
	add.s64 	%rd233, %rd127, %rd237;
	// begin inline asm
	{ atom.add.f64 %fd3301,[%rd233],%fd911; }

	// end inline asm
	add.s64 	%rd234, %rd233, 8;
	// begin inline asm
	{ atom.add.f64 %fd3303,[%rd234],%fd912; }

	// end inline asm
	add.s64 	%rd235, %rd233, 16;
	// begin inline asm
	{ atom.add.f64 %fd3305,[%rd235],%fd913; }

	// end inline asm
	bra.uni 	$L__BB7_198;

$L__BB7_196:
	setp.eq.s64 	%p184, %rd104, 0;
	@%p184 bra 	$L__BB7_198;

	cvt.s64.s32 	%rd241, %r709;
	mul.lo.s64 	%rd242, %rd241, %rd54;
	add.s64 	%rd238, %rd104, %rd242;
	// begin inline asm
	{ atom.add.f64 %fd3307,[%rd238],%fd911; }

	// end inline asm
	add.s64 	%rd239, %rd238, 8;
	// begin inline asm
	{ atom.add.f64 %fd3309,[%rd239],%fd912; }

	// end inline asm
	add.s64 	%rd240, %rd238, 16;
	// begin inline asm
	{ atom.add.f64 %fd3311,[%rd240],%fd913; }

	// end inline asm

$L__BB7_198:
	mov.f64 	%fd3313, 0d0000000000000000;
	sub.f64 	%fd3314, %fd3313, %fd911;
	add.f64 	%fd917, %fd3314, 0d0000000000000000;
	sub.f64 	%fd3315, %fd3313, %fd912;
	add.f64 	%fd918, %fd3315, 0d0000000000000000;
	sub.f64 	%fd3316, %fd3313, %fd913;
	add.f64 	%fd919, %fd3316, 0d0000000000000000;
	@%p183 bra 	$L__BB7_200;

	cvt.s64.s32 	%rd246, %r708;
	mul.lo.s64 	%rd247, %rd246, %rd60;
	add.s64 	%rd243, %rd127, %rd247;
	// begin inline asm
	{ atom.add.f64 %fd3317,[%rd243],%fd917; }

	// end inline asm
	add.s64 	%rd244, %rd243, 8;
	// begin inline asm
	{ atom.add.f64 %fd3319,[%rd244],%fd918; }

	// end inline asm
	add.s64 	%rd245, %rd243, 16;
	// begin inline asm
	{ atom.add.f64 %fd3321,[%rd245],%fd919; }

	// end inline asm
	bra.uni 	$L__BB7_202;

$L__BB7_200:
	setp.eq.s64 	%p186, %rd104, 0;
	@%p186 bra 	$L__BB7_202;

	cvt.s64.s32 	%rd251, %r708;
	mul.lo.s64 	%rd252, %rd251, %rd54;
	add.s64 	%rd248, %rd104, %rd252;
	// begin inline asm
	{ atom.add.f64 %fd3323,[%rd248],%fd917; }

	// end inline asm
	add.s64 	%rd249, %rd248, 8;
	// begin inline asm
	{ atom.add.f64 %fd3325,[%rd249],%fd918; }

	// end inline asm
	add.s64 	%rd250, %rd248, 16;
	// begin inline asm
	{ atom.add.f64 %fd3327,[%rd250],%fd919; }

	// end inline asm

$L__BB7_202:
	@%p183 bra 	$L__BB7_204;

	cvt.s64.s32 	%rd256, %r707;
	mul.lo.s64 	%rd257, %rd256, %rd60;
	add.s64 	%rd253, %rd127, %rd257;
	// begin inline asm
	{ atom.add.f64 %fd3329,[%rd253],%fd914; }

	// end inline asm
	add.s64 	%rd254, %rd253, 8;
	// begin inline asm
	{ atom.add.f64 %fd3331,[%rd254],%fd915; }

	// end inline asm
	add.s64 	%rd255, %rd253, 16;
	// begin inline asm
	{ atom.add.f64 %fd3333,[%rd255],%fd916; }

	// end inline asm
	bra.uni 	$L__BB7_206;

$L__BB7_204:
	setp.eq.s64 	%p188, %rd104, 0;
	@%p188 bra 	$L__BB7_206;

	cvt.s64.s32 	%rd261, %r707;
	mul.lo.s64 	%rd262, %rd261, %rd54;
	add.s64 	%rd258, %rd104, %rd262;
	// begin inline asm
	{ atom.add.f64 %fd3335,[%rd258],%fd914; }

	// end inline asm
	add.s64 	%rd259, %rd258, 8;
	// begin inline asm
	{ atom.add.f64 %fd3337,[%rd259],%fd915; }

	// end inline asm
	add.s64 	%rd260, %rd258, 16;
	// begin inline asm
	{ atom.add.f64 %fd3339,[%rd260],%fd916; }

	// end inline asm

$L__BB7_206:
	mov.f64 	%fd3341, 0d0000000000000000;
	sub.f64 	%fd3342, %fd3341, %fd914;
	add.f64 	%fd920, %fd3342, 0d0000000000000000;
	sub.f64 	%fd3343, %fd3341, %fd915;
	add.f64 	%fd921, %fd3343, 0d0000000000000000;
	sub.f64 	%fd3344, %fd3341, %fd916;
	add.f64 	%fd922, %fd3344, 0d0000000000000000;
	@%p183 bra 	$L__BB7_208;

	cvt.s64.s32 	%rd266, %r706;
	mul.lo.s64 	%rd267, %rd266, %rd60;
	add.s64 	%rd263, %rd127, %rd267;
	// begin inline asm
	{ atom.add.f64 %fd3345,[%rd263],%fd920; }

	// end inline asm
	add.s64 	%rd264, %rd263, 8;
	// begin inline asm
	{ atom.add.f64 %fd3347,[%rd264],%fd921; }

	// end inline asm
	add.s64 	%rd265, %rd263, 16;
	// begin inline asm
	{ atom.add.f64 %fd3349,[%rd265],%fd922; }

	// end inline asm
	bra.uni 	$L__BB7_210;

$L__BB7_208:
	setp.eq.s64 	%p190, %rd104, 0;
	@%p190 bra 	$L__BB7_210;

	cvt.s64.s32 	%rd271, %r706;
	mul.lo.s64 	%rd272, %rd271, %rd54;
	add.s64 	%rd268, %rd104, %rd272;
	// begin inline asm
	{ atom.add.f64 %fd3351,[%rd268],%fd920; }

	// end inline asm
	add.s64 	%rd269, %rd268, 8;
	// begin inline asm
	{ atom.add.f64 %fd3353,[%rd269],%fd921; }

	// end inline asm
	add.s64 	%rd270, %rd268, 16;
	// begin inline asm
	{ atom.add.f64 %fd3355,[%rd270],%fd922; }

	// end inline asm

$L__BB7_210:
	add.f64 	%fd3358, %fd6339, 0d0000000000000000;
	fma.rn.f64 	%fd3360, %fd5854, %fd3358, 0d0000000000000000;
	add.f64 	%fd937, %fd6338, 0d0000000000000000;
	sub.f64 	%fd3361, %fd3075, %fd6338;
	fma.rn.f64 	%fd3362, %fd5831, %fd3361, %fd3360;
	fma.rn.f64 	%fd938, %fd5831, %fd3361, %fd3362;
	@%p111 bra 	$L__BB7_214;

	setp.ge.f64 	%p15, %fd232, %fd231;
	add.f64 	%fd939, %fd230, %fd224;
	@%p15 bra 	$L__BB7_213;

	selp.f64 	%fd3364, %fd225, %fd231, %p15;
	mul.f64 	%fd3365, %fd220, %fd230;
	mul.f64 	%fd3366, %fd229, %fd224;
	sub.f64 	%fd3367, %fd3365, %fd3366;
	mul.f64 	%fd3368, %fd219, %fd222;
	mul.f64 	%fd3369, %fd218, %fd223;
	sub.f64 	%fd3370, %fd3369, %fd3368;
	mul.f64 	%fd3371, %fd217, %fd223;
	mul.f64 	%fd3372, %fd219, %fd221;
	sub.f64 	%fd3373, %fd3372, %fd3371;
	mul.f64 	%fd3374, %fd218, %fd221;
	mul.f64 	%fd3375, %fd217, %fd222;
	sub.f64 	%fd3376, %fd3375, %fd3374;
	setp.gt.f64 	%p192, %fd3367, 0d0000000000000000;
	setp.lt.f64 	%p193, %fd3367, %fd3364;
	mul.f64 	%fd3377, %fd227, %fd3373;
	fma.rn.f64 	%fd3378, %fd226, %fd3370, %fd3377;
	fma.rn.f64 	%fd3379, %fd228, %fd3376, %fd3378;
	setp.eq.f64 	%p194, %fd3379, 0d0000000000000000;
	mul.f64 	%fd3380, %fd3373, %fd3373;
	fma.rn.f64 	%fd3381, %fd3370, %fd3370, %fd3380;
	fma.rn.f64 	%fd3382, %fd3376, %fd3376, %fd3381;
	mul.f64 	%fd3383, %fd220, 0d3BC79CA100000000;
	mul.f64 	%fd3384, %fd3383, %fd225;
	setp.lt.f64 	%p195, %fd3382, %fd3384;
	or.pred  	%p196, %p194, %p195;
	and.pred  	%p197, %p192, %p193;
	and.pred  	%p198, %p196, %p197;
	mul.f64 	%fd3385, %fd231, 0d3FE0000000000000;
	setp.lt.f64 	%p199, %fd232, %fd3385;
	selp.b32 	%r634, 2, 5, %p199;
	selp.f64 	%fd3386, %fd230, %fd939, %p199;
	selp.f64 	%fd6357, %fd225, %fd3364, %p198;
	selp.b32 	%r717, %r634, 8, %p198;
	selp.f64 	%fd6356, %fd3386, %fd3367, %p198;

$L__BB7_213:
	selp.f64 	%fd6359, %fd225, %fd6357, %p15;
	selp.b32 	%r718, 5, %r717, %p15;
	selp.f64 	%fd6358, %fd939, %fd6356, %p15;

$L__BB7_214:
	selp.f64 	%fd948, %fd225, %fd6359, %p111;
	selp.b32 	%r719, 2, %r718, %p111;
	selp.f64 	%fd949, %fd230, %fd6358, %p111;
	setp.gtu.f64 	%p202, %fd949, 0d0000000000000000;
	@%p202 bra 	$L__BB7_218;
	bra.uni 	$L__BB7_215;

$L__BB7_218:
	setp.ltu.f64 	%p205, %fd949, %fd948;
	@%p205 bra 	$L__BB7_222;

	mov.f64 	%fd3388, 0d0000000000000000;
	sub.f64 	%fd3389, %fd3388, %fd229;
	add.f64 	%fd951, %fd3389, %fd224;
	setp.le.f64 	%p206, %fd951, 0d0000000000000000;
	mov.u32 	%r719, 1;
	@%p206 bra 	$L__BB7_222;

	setp.ge.f64 	%p207, %fd951, %fd220;
	mov.u32 	%r719, 4;
	@%p207 bra 	$L__BB7_222;

	mov.u32 	%r719, 7;
	bra.uni 	$L__BB7_222;

$L__BB7_215:
	mov.f64 	%fd3387, 0d0000000000000000;
	sub.f64 	%fd950, %fd3387, %fd229;
	setp.le.f64 	%p203, %fd950, 0d0000000000000000;
	mov.u32 	%r719, 0;
	@%p203 bra 	$L__BB7_222;

	setp.ge.f64 	%p204, %fd950, %fd220;
	mov.u32 	%r719, 3;
	@%p204 bra 	$L__BB7_222;

	mov.u32 	%r719, 6;

$L__BB7_222:
	setp.eq.s32 	%p208, %r719, 0;
	@%p208 bra 	$L__BB7_238;

	setp.eq.s32 	%p209, %r719, 1;
	@%p209 bra 	$L__BB7_237;
	bra.uni 	$L__BB7_224;

$L__BB7_237:
	sub.f64 	%fd3743, %fd172, %fd181;
	add.f64 	%fd3744, %fd3743, %fd3743;
	sub.f64 	%fd3745, %fd171, %fd180;
	add.f64 	%fd3746, %fd3745, %fd3745;
	sub.f64 	%fd3747, %fd170, %fd179;
	add.f64 	%fd3748, %fd3747, %fd3747;
	fma.rn.f64 	%fd3749, %fd3744, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3750, %fd3746, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3751, %fd3748, %fd937, 0d0000000000000000;
	add.f64 	%fd925, %fd925, %fd3749;
	add.f64 	%fd924, %fd924, %fd3750;
	add.f64 	%fd923, %fd923, %fd3751;
	sub.f64 	%fd934, %fd934, %fd3749;
	sub.f64 	%fd933, %fd933, %fd3750;
	sub.f64 	%fd932, %fd932, %fd3751;
	bra.uni 	$L__BB7_239;

$L__BB7_238:
	add.f64 	%fd3752, %fd226, %fd226;
	add.f64 	%fd3753, %fd227, %fd227;
	add.f64 	%fd3754, %fd228, %fd228;
	fma.rn.f64 	%fd3755, %fd3752, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3756, %fd3753, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3757, %fd3754, %fd937, 0d0000000000000000;
	add.f64 	%fd925, %fd925, %fd3755;
	add.f64 	%fd924, %fd924, %fd3756;
	add.f64 	%fd923, %fd923, %fd3757;
	sub.f64 	%fd931, %fd931, %fd3755;
	sub.f64 	%fd930, %fd930, %fd3756;
	sub.f64 	%fd929, %fd929, %fd3757;
	bra.uni 	$L__BB7_239;

$L__BB7_224:
	setp.eq.s32 	%p210, %r719, 2;
	@%p210 bra 	$L__BB7_236;
	bra.uni 	$L__BB7_225;

$L__BB7_236:
	sub.f64 	%fd3676, %fd178, %fd172;
	sub.f64 	%fd3677, %fd179, %fd170;
	sub.f64 	%fd3678, %fd177, %fd171;
	mul.f64 	%fd3679, %fd3678, %fd3677;
	sub.f64 	%fd3680, %fd180, %fd171;
	sub.f64 	%fd3681, %fd176, %fd170;
	mul.f64 	%fd3682, %fd3681, %fd3680;
	sub.f64 	%fd3683, %fd3679, %fd3682;
	sub.f64 	%fd3684, %fd181, %fd172;
	mul.f64 	%fd3685, %fd3681, %fd3684;
	mul.f64 	%fd3686, %fd3676, %fd3677;
	sub.f64 	%fd3687, %fd3685, %fd3686;
	mul.f64 	%fd3688, %fd3676, %fd3680;
	mul.f64 	%fd3689, %fd3678, %fd3684;
	sub.f64 	%fd3690, %fd3688, %fd3689;
	mul.f64 	%fd3691, %fd3687, %fd3687;
	fma.rn.f64 	%fd3692, %fd3683, %fd3683, %fd3691;
	fma.rn.f64 	%fd3693, %fd3690, %fd3690, %fd3692;
	div.rn.f64 	%fd3694, %fd3693, %fd225;
	div.rn.f64 	%fd3695, %fd937, %fd225;
	add.f64 	%fd3696, %fd3695, 0d0000000000000000;
	mov.f64 	%fd3697, 0d0000000000000000;
	mul.f64 	%fd3698, %fd3694, %fd937;
	div.rn.f64 	%fd3699, %fd3698, %fd225;
	sub.f64 	%fd3700, %fd3697, %fd3699;
	fma.rn.f64 	%fd3701, %fd842, %fd3700, 0d0000000000000000;
	fma.rn.f64 	%fd3702, %fd843, %fd3700, 0d0000000000000000;
	fma.rn.f64 	%fd3703, %fd844, %fd3700, 0d0000000000000000;
	add.f64 	%fd3704, %fd3683, %fd3683;
	add.f64 	%fd3705, %fd3687, %fd3687;
	add.f64 	%fd3706, %fd3690, %fd3690;
	fma.rn.f64 	%fd3707, %fd3704, %fd3696, 0d0000000000000000;
	fma.rn.f64 	%fd3708, %fd3705, %fd3696, 0d0000000000000000;
	fma.rn.f64 	%fd3709, %fd3706, %fd3696, 0d0000000000000000;
	mul.f64 	%fd3710, %fd3680, %fd3709;
	mul.f64 	%fd3711, %fd3677, %fd3708;
	sub.f64 	%fd3712, %fd3710, %fd3711;
	mul.f64 	%fd3713, %fd3677, %fd3707;
	mul.f64 	%fd3714, %fd3684, %fd3709;
	sub.f64 	%fd3715, %fd3713, %fd3714;
	mul.f64 	%fd3716, %fd3684, %fd3708;
	mul.f64 	%fd3717, %fd3680, %fd3707;
	sub.f64 	%fd3718, %fd3716, %fd3717;
	add.f64 	%fd3719, %fd3712, 0d0000000000000000;
	add.f64 	%fd3720, %fd3715, 0d0000000000000000;
	add.f64 	%fd3721, %fd3718, 0d0000000000000000;
	mul.f64 	%fd3722, %fd3678, %fd3709;
	mul.f64 	%fd3723, %fd3681, %fd3708;
	mul.f64 	%fd3724, %fd3681, %fd3707;
	mul.f64 	%fd3725, %fd3676, %fd3709;
	mul.f64 	%fd3726, %fd3676, %fd3708;
	mul.f64 	%fd3727, %fd3678, %fd3707;
	sub.f64 	%fd3728, %fd3723, %fd3722;
	add.f64 	%fd3729, %fd3728, 0d0000000000000000;
	sub.f64 	%fd3730, %fd3725, %fd3724;
	add.f64 	%fd3731, %fd3730, 0d0000000000000000;
	sub.f64 	%fd3732, %fd3727, %fd3726;
	add.f64 	%fd3733, %fd3732, 0d0000000000000000;
	add.f64 	%fd3734, %fd934, %fd3701;
	add.f64 	%fd3735, %fd933, %fd3702;
	add.f64 	%fd3736, %fd932, %fd3703;
	sub.f64 	%fd3737, %fd931, %fd3701;
	sub.f64 	%fd3738, %fd930, %fd3702;
	sub.f64 	%fd3739, %fd929, %fd3703;
	add.f64 	%fd934, %fd3734, %fd3729;
	add.f64 	%fd933, %fd3735, %fd3731;
	add.f64 	%fd932, %fd3736, %fd3733;
	sub.f64 	%fd3740, %fd925, %fd3729;
	sub.f64 	%fd3741, %fd924, %fd3731;
	sub.f64 	%fd3742, %fd923, %fd3733;
	add.f64 	%fd931, %fd3737, %fd3719;
	add.f64 	%fd930, %fd3738, %fd3720;
	add.f64 	%fd929, %fd3739, %fd3721;
	sub.f64 	%fd925, %fd3740, %fd3719;
	sub.f64 	%fd924, %fd3741, %fd3720;
	sub.f64 	%fd923, %fd3742, %fd3721;
	bra.uni 	$L__BB7_239;

$L__BB7_225:
	setp.eq.s32 	%p211, %r719, 3;
	@%p211 bra 	$L__BB7_235;
	bra.uni 	$L__BB7_226;

$L__BB7_235:
	sub.f64 	%fd3667, %fd175, %fd178;
	add.f64 	%fd3668, %fd3667, %fd3667;
	sub.f64 	%fd3669, %fd174, %fd177;
	add.f64 	%fd3670, %fd3669, %fd3669;
	sub.f64 	%fd3671, %fd173, %fd176;
	add.f64 	%fd3672, %fd3671, %fd3671;
	fma.rn.f64 	%fd3673, %fd3668, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3674, %fd3670, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3675, %fd3672, %fd937, 0d0000000000000000;
	add.f64 	%fd928, %fd928, %fd3673;
	add.f64 	%fd927, %fd927, %fd3674;
	add.f64 	%fd926, %fd926, %fd3675;
	sub.f64 	%fd931, %fd931, %fd3673;
	sub.f64 	%fd930, %fd930, %fd3674;
	sub.f64 	%fd929, %fd929, %fd3675;
	bra.uni 	$L__BB7_239;

$L__BB7_226:
	setp.eq.s32 	%p212, %r719, 4;
	@%p212 bra 	$L__BB7_234;
	bra.uni 	$L__BB7_227;

$L__BB7_234:
	sub.f64 	%fd3658, %fd175, %fd181;
	add.f64 	%fd3659, %fd3658, %fd3658;
	sub.f64 	%fd3660, %fd174, %fd180;
	add.f64 	%fd3661, %fd3660, %fd3660;
	sub.f64 	%fd3662, %fd173, %fd179;
	add.f64 	%fd3663, %fd3662, %fd3662;
	fma.rn.f64 	%fd3664, %fd3659, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3665, %fd3661, %fd937, 0d0000000000000000;
	fma.rn.f64 	%fd3666, %fd3663, %fd937, 0d0000000000000000;
	add.f64 	%fd928, %fd928, %fd3664;
	add.f64 	%fd927, %fd927, %fd3665;
	add.f64 	%fd926, %fd926, %fd3666;
	sub.f64 	%fd934, %fd934, %fd3664;
	sub.f64 	%fd933, %fd933, %fd3665;
	sub.f64 	%fd932, %fd932, %fd3666;
	bra.uni 	$L__BB7_239;

$L__BB7_227:
	setp.eq.s32 	%p213, %r719, 5;
	@%p213 bra 	$L__BB7_233;
	bra.uni 	$L__BB7_228;

$L__BB7_233:
	sub.f64 	%fd3591, %fd178, %fd175;
	sub.f64 	%fd3592, %fd179, %fd173;
	sub.f64 	%fd3593, %fd177, %fd174;
	mul.f64 	%fd3594, %fd3593, %fd3592;
	sub.f64 	%fd3595, %fd180, %fd174;
	sub.f64 	%fd3596, %fd176, %fd173;
	mul.f64 	%fd3597, %fd3596, %fd3595;
	sub.f64 	%fd3598, %fd3594, %fd3597;
	sub.f64 	%fd3599, %fd181, %fd175;
	mul.f64 	%fd3600, %fd3596, %fd3599;
	mul.f64 	%fd3601, %fd3591, %fd3592;
	sub.f64 	%fd3602, %fd3600, %fd3601;
	mul.f64 	%fd3603, %fd3591, %fd3595;
	mul.f64 	%fd3604, %fd3593, %fd3599;
	sub.f64 	%fd3605, %fd3603, %fd3604;
	mul.f64 	%fd3606, %fd3602, %fd3602;
	fma.rn.f64 	%fd3607, %fd3598, %fd3598, %fd3606;
	fma.rn.f64 	%fd3608, %fd3605, %fd3605, %fd3607;
	div.rn.f64 	%fd3609, %fd3608, %fd225;
	div.rn.f64 	%fd3610, %fd937, %fd225;
	add.f64 	%fd3611, %fd3610, 0d0000000000000000;
	mov.f64 	%fd3612, 0d0000000000000000;
	mul.f64 	%fd3613, %fd3609, %fd937;
	div.rn.f64 	%fd3614, %fd3613, %fd225;
	sub.f64 	%fd3615, %fd3612, %fd3614;
	fma.rn.f64 	%fd3616, %fd842, %fd3615, 0d0000000000000000;
	fma.rn.f64 	%fd3617, %fd843, %fd3615, 0d0000000000000000;
	fma.rn.f64 	%fd3618, %fd844, %fd3615, 0d0000000000000000;
	add.f64 	%fd3619, %fd3598, %fd3598;
	add.f64 	%fd3620, %fd3602, %fd3602;
	add.f64 	%fd3621, %fd3605, %fd3605;
	fma.rn.f64 	%fd3622, %fd3619, %fd3611, 0d0000000000000000;
	fma.rn.f64 	%fd3623, %fd3620, %fd3611, 0d0000000000000000;
	fma.rn.f64 	%fd3624, %fd3621, %fd3611, 0d0000000000000000;
	mul.f64 	%fd3625, %fd3595, %fd3624;
	mul.f64 	%fd3626, %fd3592, %fd3623;
	sub.f64 	%fd3627, %fd3625, %fd3626;
	mul.f64 	%fd3628, %fd3592, %fd3622;
	mul.f64 	%fd3629, %fd3599, %fd3624;
	sub.f64 	%fd3630, %fd3628, %fd3629;
	mul.f64 	%fd3631, %fd3599, %fd3623;
	mul.f64 	%fd3632, %fd3595, %fd3622;
	sub.f64 	%fd3633, %fd3631, %fd3632;
	add.f64 	%fd3634, %fd3627, 0d0000000000000000;
	add.f64 	%fd3635, %fd3630, 0d0000000000000000;
	add.f64 	%fd3636, %fd3633, 0d0000000000000000;
	mul.f64 	%fd3637, %fd3593, %fd3624;
	mul.f64 	%fd3638, %fd3596, %fd3623;
	mul.f64 	%fd3639, %fd3596, %fd3622;
	mul.f64 	%fd3640, %fd3591, %fd3624;
	mul.f64 	%fd3641, %fd3591, %fd3623;
	mul.f64 	%fd3642, %fd3593, %fd3622;
	sub.f64 	%fd3643, %fd3638, %fd3637;
	add.f64 	%fd3644, %fd3643, 0d0000000000000000;
	sub.f64 	%fd3645, %fd3640, %fd3639;
	add.f64 	%fd3646, %fd3645, 0d0000000000000000;
	sub.f64 	%fd3647, %fd3642, %fd3641;
	add.f64 	%fd3648, %fd3647, 0d0000000000000000;
	add.f64 	%fd3649, %fd934, %fd3616;
	add.f64 	%fd3650, %fd933, %fd3617;
	add.f64 	%fd3651, %fd932, %fd3618;
	sub.f64 	%fd3652, %fd931, %fd3616;
	sub.f64 	%fd3653, %fd930, %fd3617;
	sub.f64 	%fd3654, %fd929, %fd3618;
	add.f64 	%fd934, %fd3649, %fd3644;
	add.f64 	%fd933, %fd3650, %fd3646;
	add.f64 	%fd932, %fd3651, %fd3648;
	sub.f64 	%fd3655, %fd928, %fd3644;
	sub.f64 	%fd3656, %fd927, %fd3646;
	sub.f64 	%fd3657, %fd926, %fd3648;
	add.f64 	%fd931, %fd3652, %fd3634;
	add.f64 	%fd930, %fd3653, %fd3635;
	add.f64 	%fd929, %fd3654, %fd3636;
	sub.f64 	%fd928, %fd3655, %fd3634;
	sub.f64 	%fd927, %fd3656, %fd3635;
	sub.f64 	%fd926, %fd3657, %fd3636;
	bra.uni 	$L__BB7_239;

$L__BB7_228:
	setp.eq.s32 	%p214, %r719, 6;
	@%p214 bra 	$L__BB7_232;
	bra.uni 	$L__BB7_229;

$L__BB7_232:
	sub.f64 	%fd3527, %fd175, %fd178;
	sub.f64 	%fd3528, %fd173, %fd176;
	mul.f64 	%fd3529, %fd3528, %fd227;
	sub.f64 	%fd3530, %fd174, %fd177;
	mul.f64 	%fd3531, %fd228, %fd3530;
	sub.f64 	%fd3532, %fd3529, %fd3531;
	mul.f64 	%fd3533, %fd228, %fd3527;
	mul.f64 	%fd3534, %fd3528, %fd226;
	sub.f64 	%fd3535, %fd3533, %fd3534;
	mul.f64 	%fd3536, %fd3530, %fd226;
	mul.f64 	%fd3537, %fd227, %fd3527;
	sub.f64 	%fd3538, %fd3536, %fd3537;
	mul.f64 	%fd3539, %fd3535, %fd3535;
	fma.rn.f64 	%fd3540, %fd3532, %fd3532, %fd3539;
	fma.rn.f64 	%fd3541, %fd3538, %fd3538, %fd3540;
	div.rn.f64 	%fd3542, %fd3541, %fd220;
	div.rn.f64 	%fd3543, %fd937, %fd220;
	add.f64 	%fd3544, %fd3543, 0d0000000000000000;
	mov.f64 	%fd3545, 0d0000000000000000;
	mul.f64 	%fd3546, %fd3542, %fd937;
	div.rn.f64 	%fd3547, %fd3546, %fd220;
	sub.f64 	%fd3548, %fd3545, %fd3547;
	fma.rn.f64 	%fd3549, %fd845, %fd3548, 0d0000000000000000;
	fma.rn.f64 	%fd3550, %fd846, %fd3548, 0d0000000000000000;
	fma.rn.f64 	%fd3551, %fd847, %fd3548, 0d0000000000000000;
	add.f64 	%fd3552, %fd3532, %fd3532;
	add.f64 	%fd3553, %fd3535, %fd3535;
	add.f64 	%fd3554, %fd3538, %fd3538;
	fma.rn.f64 	%fd3555, %fd3552, %fd3544, 0d0000000000000000;
	fma.rn.f64 	%fd3556, %fd3553, %fd3544, 0d0000000000000000;
	fma.rn.f64 	%fd3557, %fd3554, %fd3544, 0d0000000000000000;
	mul.f64 	%fd3558, %fd3530, %fd3557;
	mul.f64 	%fd3559, %fd3528, %fd3556;
	sub.f64 	%fd3560, %fd3558, %fd3559;
	mul.f64 	%fd3561, %fd3528, %fd3555;
	mul.f64 	%fd3562, %fd3527, %fd3557;
	sub.f64 	%fd3563, %fd3561, %fd3562;
	mul.f64 	%fd3564, %fd3527, %fd3556;
	mul.f64 	%fd3565, %fd3530, %fd3555;
	sub.f64 	%fd3566, %fd3564, %fd3565;
	add.f64 	%fd3567, %fd3560, 0d0000000000000000;
	add.f64 	%fd3568, %fd3563, 0d0000000000000000;
	add.f64 	%fd3569, %fd3566, 0d0000000000000000;
	mul.f64 	%fd3570, %fd227, %fd3557;
	mul.f64 	%fd3571, %fd228, %fd3556;
	mul.f64 	%fd3572, %fd228, %fd3555;
	mul.f64 	%fd3573, %fd226, %fd3557;
	mul.f64 	%fd3574, %fd226, %fd3556;
	mul.f64 	%fd3575, %fd227, %fd3555;
	sub.f64 	%fd3576, %fd3571, %fd3570;
	add.f64 	%fd3577, %fd3576, 0d0000000000000000;
	sub.f64 	%fd3578, %fd3573, %fd3572;
	add.f64 	%fd3579, %fd3578, 0d0000000000000000;
	sub.f64 	%fd3580, %fd3575, %fd3574;
	add.f64 	%fd3581, %fd3580, 0d0000000000000000;
	add.f64 	%fd3582, %fd928, %fd3549;
	add.f64 	%fd3583, %fd927, %fd3550;
	add.f64 	%fd3584, %fd926, %fd3551;
	sub.f64 	%fd3585, %fd925, %fd3549;
	sub.f64 	%fd3586, %fd924, %fd3550;
	sub.f64 	%fd3587, %fd923, %fd3551;
	add.f64 	%fd928, %fd3582, %fd3577;
	add.f64 	%fd927, %fd3583, %fd3579;
	add.f64 	%fd926, %fd3584, %fd3581;
	sub.f64 	%fd3588, %fd931, %fd3577;
	sub.f64 	%fd3589, %fd930, %fd3579;
	sub.f64 	%fd3590, %fd929, %fd3581;
	add.f64 	%fd925, %fd3585, %fd3567;
	add.f64 	%fd924, %fd3586, %fd3568;
	add.f64 	%fd923, %fd3587, %fd3569;
	sub.f64 	%fd931, %fd3588, %fd3567;
	sub.f64 	%fd930, %fd3589, %fd3568;
	sub.f64 	%fd929, %fd3590, %fd3569;
	bra.uni 	$L__BB7_239;

$L__BB7_229:
	setp.eq.s32 	%p215, %r719, 7;
	@%p215 bra 	$L__BB7_231;
	bra.uni 	$L__BB7_230;

$L__BB7_231:
	sub.f64 	%fd3460, %fd172, %fd181;
	sub.f64 	%fd3461, %fd173, %fd179;
	sub.f64 	%fd3462, %fd171, %fd180;
	mul.f64 	%fd3463, %fd3461, %fd3462;
	sub.f64 	%fd3464, %fd174, %fd180;
	sub.f64 	%fd3465, %fd170, %fd179;
	mul.f64 	%fd3466, %fd3465, %fd3464;
	sub.f64 	%fd3467, %fd3463, %fd3466;
	sub.f64 	%fd3468, %fd175, %fd181;
	mul.f64 	%fd3469, %fd3465, %fd3468;
	mul.f64 	%fd3470, %fd3461, %fd3460;
	sub.f64 	%fd3471, %fd3469, %fd3470;
	mul.f64 	%fd3472, %fd3464, %fd3460;
	mul.f64 	%fd3473, %fd3462, %fd3468;
	sub.f64 	%fd3474, %fd3472, %fd3473;
	mul.f64 	%fd3475, %fd3471, %fd3471;
	fma.rn.f64 	%fd3476, %fd3467, %fd3467, %fd3475;
	fma.rn.f64 	%fd3477, %fd3474, %fd3474, %fd3476;
	div.rn.f64 	%fd3478, %fd3477, %fd220;
	div.rn.f64 	%fd3479, %fd937, %fd220;
	add.f64 	%fd3480, %fd3479, 0d0000000000000000;
	mov.f64 	%fd3481, 0d0000000000000000;
	mul.f64 	%fd3482, %fd3478, %fd937;
	div.rn.f64 	%fd3483, %fd3482, %fd220;
	sub.f64 	%fd3484, %fd3481, %fd3483;
	fma.rn.f64 	%fd3485, %fd845, %fd3484, 0d0000000000000000;
	fma.rn.f64 	%fd3486, %fd846, %fd3484, 0d0000000000000000;
	fma.rn.f64 	%fd3487, %fd847, %fd3484, 0d0000000000000000;
	add.f64 	%fd3488, %fd3467, %fd3467;
	add.f64 	%fd3489, %fd3471, %fd3471;
	add.f64 	%fd3490, %fd3474, %fd3474;
	fma.rn.f64 	%fd3491, %fd3488, %fd3480, 0d0000000000000000;
	fma.rn.f64 	%fd3492, %fd3489, %fd3480, 0d0000000000000000;
	fma.rn.f64 	%fd3493, %fd3490, %fd3480, 0d0000000000000000;
	mul.f64 	%fd3494, %fd3464, %fd3493;
	mul.f64 	%fd3495, %fd3461, %fd3492;
	sub.f64 	%fd3496, %fd3494, %fd3495;
	mul.f64 	%fd3497, %fd3461, %fd3491;
	mul.f64 	%fd3498, %fd3468, %fd3493;
	sub.f64 	%fd3499, %fd3497, %fd3498;
	mul.f64 	%fd3500, %fd3468, %fd3492;
	mul.f64 	%fd3501, %fd3464, %fd3491;
	sub.f64 	%fd3502, %fd3500, %fd3501;
	add.f64 	%fd3503, %fd3496, 0d0000000000000000;
	add.f64 	%fd3504, %fd3499, 0d0000000000000000;
	add.f64 	%fd3505, %fd3502, 0d0000000000000000;
	mul.f64 	%fd3506, %fd3462, %fd3493;
	mul.f64 	%fd3507, %fd3465, %fd3492;
	mul.f64 	%fd3508, %fd3465, %fd3491;
	mul.f64 	%fd3509, %fd3460, %fd3493;
	mul.f64 	%fd3510, %fd3460, %fd3492;
	mul.f64 	%fd3511, %fd3462, %fd3491;
	sub.f64 	%fd3512, %fd3507, %fd3506;
	add.f64 	%fd3513, %fd3512, 0d0000000000000000;
	sub.f64 	%fd3514, %fd3509, %fd3508;
	add.f64 	%fd3515, %fd3514, 0d0000000000000000;
	sub.f64 	%fd3516, %fd3511, %fd3510;
	add.f64 	%fd3517, %fd3516, 0d0000000000000000;
	add.f64 	%fd3518, %fd928, %fd3485;
	add.f64 	%fd3519, %fd927, %fd3486;
	add.f64 	%fd3520, %fd926, %fd3487;
	sub.f64 	%fd3521, %fd925, %fd3485;
	sub.f64 	%fd3522, %fd924, %fd3486;
	sub.f64 	%fd3523, %fd923, %fd3487;
	add.f64 	%fd928, %fd3518, %fd3513;
	add.f64 	%fd927, %fd3519, %fd3515;
	add.f64 	%fd926, %fd3520, %fd3517;
	sub.f64 	%fd3524, %fd934, %fd3513;
	sub.f64 	%fd3525, %fd933, %fd3515;
	sub.f64 	%fd3526, %fd932, %fd3517;
	add.f64 	%fd925, %fd3521, %fd3503;
	add.f64 	%fd924, %fd3522, %fd3504;
	add.f64 	%fd923, %fd3523, %fd3505;
	sub.f64 	%fd934, %fd3524, %fd3503;
	sub.f64 	%fd933, %fd3525, %fd3504;
	sub.f64 	%fd932, %fd3526, %fd3505;
	bra.uni 	$L__BB7_239;

$L__BB7_230:
	sub.f64 	%fd3390, %fd178, %fd172;
	mul.f64 	%fd3391, %fd219, %fd222;
	mul.f64 	%fd3392, %fd218, %fd223;
	sub.f64 	%fd3393, %fd3392, %fd3391;
	mul.f64 	%fd3394, %fd217, %fd223;
	mul.f64 	%fd3395, %fd219, %fd221;
	sub.f64 	%fd3396, %fd3395, %fd3394;
	mul.f64 	%fd3397, %fd218, %fd221;
	mul.f64 	%fd3398, %fd217, %fd222;
	sub.f64 	%fd3399, %fd3398, %fd3397;
	sub.f64 	%fd3400, %fd177, %fd171;
	mul.f64 	%fd3401, %fd3400, %fd3396;
	fma.rn.f64 	%fd3402, %fd3390, %fd3393, %fd3401;
	sub.f64 	%fd3403, %fd176, %fd170;
	fma.rn.f64 	%fd3404, %fd3403, %fd3399, %fd3402;
	mul.f64 	%fd3405, %fd3404, %fd3404;
	mul.f64 	%fd3406, %fd3396, %fd3396;
	fma.rn.f64 	%fd3407, %fd3393, %fd3393, %fd3406;
	fma.rn.f64 	%fd3408, %fd3399, %fd3399, %fd3407;
	div.rn.f64 	%fd3409, %fd3405, %fd3408;
	div.rn.f64 	%fd3410, %fd937, %fd3408;
	add.f64 	%fd3411, %fd3410, 0d0000000000000000;
	mov.f64 	%fd3412, 0d0000000000000000;
	mul.f64 	%fd3413, %fd3409, %fd937;
	div.rn.f64 	%fd3414, %fd3413, %fd3408;
	sub.f64 	%fd3415, %fd3412, %fd3414;
	add.f64 	%fd3416, %fd3393, %fd3393;
	add.f64 	%fd3417, %fd3396, %fd3396;
	add.f64 	%fd3418, %fd3399, %fd3399;
	fma.rn.f64 	%fd3419, %fd3416, %fd3415, 0d0000000000000000;
	fma.rn.f64 	%fd3420, %fd3417, %fd3415, 0d0000000000000000;
	fma.rn.f64 	%fd3421, %fd3418, %fd3415, 0d0000000000000000;
	fma.rn.f64 	%fd3422, %fd3404, %fd3411, 0d0000000000000000;
	fma.rn.f64 	%fd3423, %fd3404, %fd3411, %fd3422;
	fma.rn.f64 	%fd3424, %fd3393, %fd3423, 0d0000000000000000;
	fma.rn.f64 	%fd3425, %fd3396, %fd3423, 0d0000000000000000;
	fma.rn.f64 	%fd3426, %fd3399, %fd3423, 0d0000000000000000;
	fma.rn.f64 	%fd3427, %fd3390, %fd3423, %fd3419;
	fma.rn.f64 	%fd3428, %fd3400, %fd3423, %fd3420;
	fma.rn.f64 	%fd3429, %fd3403, %fd3423, %fd3421;
	mul.f64 	%fd3430, %fd222, %fd3429;
	mul.f64 	%fd3431, %fd223, %fd3428;
	sub.f64 	%fd3432, %fd3430, %fd3431;
	mul.f64 	%fd3433, %fd223, %fd3427;
	mul.f64 	%fd3434, %fd221, %fd3429;
	sub.f64 	%fd3435, %fd3433, %fd3434;
	mul.f64 	%fd3436, %fd221, %fd3428;
	mul.f64 	%fd3437, %fd222, %fd3427;
	sub.f64 	%fd3438, %fd3436, %fd3437;
	add.f64 	%fd3439, %fd3432, 0d0000000000000000;
	add.f64 	%fd3440, %fd3435, 0d0000000000000000;
	add.f64 	%fd3441, %fd3438, 0d0000000000000000;
	mul.f64 	%fd3442, %fd218, %fd3429;
	mul.f64 	%fd3443, %fd219, %fd3428;
	mul.f64 	%fd3444, %fd219, %fd3427;
	mul.f64 	%fd3445, %fd217, %fd3429;
	mul.f64 	%fd3446, %fd217, %fd3428;
	mul.f64 	%fd3447, %fd218, %fd3427;
	sub.f64 	%fd3448, %fd3443, %fd3442;
	add.f64 	%fd3449, %fd3448, 0d0000000000000000;
	sub.f64 	%fd3450, %fd3445, %fd3444;
	add.f64 	%fd3451, %fd3450, 0d0000000000000000;
	sub.f64 	%fd3452, %fd3447, %fd3446;
	add.f64 	%fd3453, %fd3452, 0d0000000000000000;
	add.f64 	%fd3454, %fd931, %fd3424;
	add.f64 	%fd3455, %fd930, %fd3425;
	add.f64 	%fd3456, %fd929, %fd3426;
	sub.f64 	%fd3457, %fd925, %fd3424;
	sub.f64 	%fd3458, %fd924, %fd3425;
	sub.f64 	%fd3459, %fd923, %fd3426;
	add.f64 	%fd934, %fd934, %fd3449;
	add.f64 	%fd933, %fd933, %fd3451;
	add.f64 	%fd932, %fd932, %fd3453;
	sub.f64 	%fd931, %fd3454, %fd3449;
	sub.f64 	%fd930, %fd3455, %fd3451;
	sub.f64 	%fd929, %fd3456, %fd3453;
	add.f64 	%fd928, %fd928, %fd3439;
	add.f64 	%fd927, %fd927, %fd3440;
	add.f64 	%fd926, %fd926, %fd3441;
	sub.f64 	%fd925, %fd3457, %fd3439;
	sub.f64 	%fd924, %fd3458, %fd3440;
	sub.f64 	%fd923, %fd3459, %fd3441;

$L__BB7_239:
	mov.f64 	%fd6372, 0d0000000000000000;
	mov.f64 	%fd6373, 0d0000000000000000;
	mov.f64 	%fd6390, 0d0000000000000000;
	mov.f64 	%fd6374, %fd6390;
	@%p111 bra 	$L__BB7_242;

	setp.ge.f64 	%p217, %fd232, %fd231;
	mov.u16 	%rs372, 1;
	@%p217 bra 	$L__BB7_242;

	mul.f64 	%fd3766, %fd218, %fd223;
	mul.f64 	%fd3767, %fd219, %fd222;
	sub.f64 	%fd6372, %fd3766, %fd3767;
	mul.f64 	%fd3768, %fd217, %fd223;
	mul.f64 	%fd3769, %fd219, %fd221;
	sub.f64 	%fd6373, %fd3769, %fd3768;
	mul.f64 	%fd3770, %fd218, %fd221;
	mul.f64 	%fd3771, %fd217, %fd222;
	sub.f64 	%fd6374, %fd3771, %fd3770;
	mul.f64 	%fd6375, %fd220, 0d3BC79CA100000000;
	mov.u16 	%rs372, 0;

$L__BB7_242:
	mov.f64 	%fd6391, %fd6390;
	mov.f64 	%fd6392, %fd6390;
	mov.f64 	%fd6393, %fd6390;
	mov.f64 	%fd6394, %fd6390;
	mov.f64 	%fd6395, %fd6390;
	mov.f64 	%fd6396, %fd6390;
	mov.f64 	%fd6397, %fd6390;
	mov.f64 	%fd6398, %fd6390;
	mov.f64 	%fd6399, %fd6390;
	mov.f64 	%fd6400, %fd6390;
	mov.f64 	%fd6401, %fd6390;
	mov.f64 	%fd6402, %fd6390;
	mov.f64 	%fd6403, %fd6390;
	@%p111 bra 	$L__BB7_247;

	and.b16  	%rs318, %rs372, 255;
	setp.ne.s16 	%p219, %rs318, 0;
	mov.f64 	%fd6390, 0d0000000000000000;
	mov.f64 	%fd6391, %fd6390;
	mov.f64 	%fd6392, %fd6390;
	mov.f64 	%fd6393, %fd6390;
	mov.f64 	%fd6394, %fd6390;
	mov.f64 	%fd6395, %fd6390;
	mov.f64 	%fd6396, %fd6390;
	mov.f64 	%fd6397, %fd6390;
	mov.f64 	%fd6398, %fd6390;
	mov.f64 	%fd6399, %fd6390;
	mov.f64 	%fd6400, %fd6390;
	mov.f64 	%fd6401, %fd6390;
	mov.f64 	%fd6402, %fd6390;
	mov.f64 	%fd6403, %fd6390;
	@%p219 bra 	$L__BB7_245;

	add.f64 	%fd3800, %fd838, 0d0000000000000000;
	fma.rn.f64 	%fd6392, %fd6375, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3801, %fd3800, 0d3BC79CA100000000, 0d0000000000000000;
	add.f64 	%fd3802, %fd6372, %fd6372;
	add.f64 	%fd3803, %fd6373, %fd6373;
	add.f64 	%fd3804, %fd6374, %fd6374;
	fma.rn.f64 	%fd3805, %fd3802, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3806, %fd3803, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3807, %fd3804, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3808, %fd226, 0d0000000000000000, %fd3805;
	fma.rn.f64 	%fd3809, %fd227, 0d0000000000000000, %fd3806;
	fma.rn.f64 	%fd3810, %fd228, 0d0000000000000000, %fd3807;
	fma.rn.f64 	%fd6395, %fd6372, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6396, %fd6373, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6397, %fd6374, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd3811, %fd222, %fd3810;
	mul.f64 	%fd3812, %fd223, %fd3809;
	sub.f64 	%fd3813, %fd3811, %fd3812;
	mul.f64 	%fd3814, %fd223, %fd3808;
	mul.f64 	%fd3815, %fd221, %fd3810;
	sub.f64 	%fd3816, %fd3814, %fd3815;
	mul.f64 	%fd3817, %fd221, %fd3809;
	mul.f64 	%fd3818, %fd222, %fd3808;
	sub.f64 	%fd3819, %fd3817, %fd3818;
	add.f64 	%fd6401, %fd3813, 0d0000000000000000;
	add.f64 	%fd6402, %fd3816, 0d0000000000000000;
	add.f64 	%fd6403, %fd3819, 0d0000000000000000;
	mul.f64 	%fd3820, %fd218, %fd3810;
	mul.f64 	%fd3821, %fd219, %fd3809;
	mul.f64 	%fd3822, %fd219, %fd3808;
	mul.f64 	%fd3823, %fd217, %fd3810;
	mul.f64 	%fd3824, %fd217, %fd3809;
	mul.f64 	%fd3825, %fd218, %fd3808;
	sub.f64 	%fd3826, %fd3821, %fd3820;
	add.f64 	%fd6398, %fd3826, 0d0000000000000000;
	sub.f64 	%fd3827, %fd3823, %fd3822;
	add.f64 	%fd6399, %fd3827, 0d0000000000000000;
	sub.f64 	%fd3828, %fd3825, %fd3824;
	add.f64 	%fd6400, %fd3828, 0d0000000000000000;
	add.f64 	%fd6393, %fd837, 0d0000000000000000;
	add.f64 	%fd6391, %fd840, 0d0000000000000000;
	add.f64 	%fd6394, %fd839, %fd3801;
	add.f64 	%fd6390, %fd841, 0d0000000000000000;

$L__BB7_245:
	setp.eq.s16 	%p220, %rs318, 0;
	@%p220 bra 	$L__BB7_247;

	add.f64 	%fd6392, %fd6392, 0d0000000000000000;
	add.f64 	%fd6390, %fd6390, 0d0000000000000000;
	add.f64 	%fd6393, %fd6393, 0d0000000000000000;

$L__BB7_247:
	add.f64 	%fd3829, %fd6390, 0d0000000000000000;
	selp.f64 	%fd3830, %fd3829, %fd6390, %p111;
	add.f64 	%fd3831, %fd6392, 0d0000000000000000;
	selp.f64 	%fd3832, %fd3831, %fd6392, %p111;
	add.f64 	%fd3833, %fd837, %fd3832;
	add.f64 	%fd3834, %fd840, %fd3830;
	add.f64 	%fd3835, %fd839, %fd6393;
	add.f64 	%fd3836, %fd840, %fd3835;
	add.f64 	%fd3837, %fd840, %fd3836;
	add.f64 	%fd3838, %fd841, %fd3833;
	fma.rn.f64 	%fd3839, %fd226, %fd3834, %fd6398;
	fma.rn.f64 	%fd3840, %fd227, %fd3834, %fd6399;
	fma.rn.f64 	%fd3841, %fd228, %fd3834, %fd6400;
	fma.rn.f64 	%fd3842, %fd221, %fd3834, %fd6395;
	fma.rn.f64 	%fd3843, %fd222, %fd3834, %fd6396;
	fma.rn.f64 	%fd3844, %fd223, %fd3834, %fd6397;
	add.f64 	%fd3845, %fd838, %fd6391;
	fma.rn.f64 	%fd3846, %fd226, %fd3845, %fd6401;
	fma.rn.f64 	%fd3847, %fd227, %fd3845, %fd6402;
	fma.rn.f64 	%fd3848, %fd228, %fd3845, %fd6403;
	fma.rn.f64 	%fd3849, %fd217, %fd3845, %fd3842;
	fma.rn.f64 	%fd3850, %fd218, %fd3845, %fd3843;
	fma.rn.f64 	%fd3851, %fd219, %fd3845, %fd3844;
	fma.rn.f64 	%fd3852, %fd842, %fd3838, %fd3839;
	fma.rn.f64 	%fd3853, %fd843, %fd3838, %fd3840;
	fma.rn.f64 	%fd3854, %fd844, %fd3838, %fd3841;
	fma.rn.f64 	%fd3855, %fd221, %fd3837, %fd3846;
	fma.rn.f64 	%fd3856, %fd222, %fd3837, %fd3847;
	fma.rn.f64 	%fd3857, %fd223, %fd3837, %fd3848;
	fma.rn.f64 	%fd3858, %fd217, %fd3837, %fd3852;
	fma.rn.f64 	%fd3859, %fd218, %fd3837, %fd3853;
	fma.rn.f64 	%fd3860, %fd219, %fd3837, %fd3854;
	add.f64 	%fd3861, %fd838, %fd6394;
	fma.rn.f64 	%fd3862, %fd845, %fd3861, %fd3855;
	fma.rn.f64 	%fd3863, %fd846, %fd3861, %fd3856;
	fma.rn.f64 	%fd3864, %fd847, %fd3861, %fd3857;
	add.f64 	%fd3865, %fd925, %fd3849;
	add.f64 	%fd3866, %fd924, %fd3850;
	add.f64 	%fd3867, %fd923, %fd3851;
	sub.f64 	%fd3868, %fd931, %fd3849;
	sub.f64 	%fd3869, %fd930, %fd3850;
	sub.f64 	%fd3870, %fd929, %fd3851;
	add.f64 	%fd3871, %fd934, %fd3858;
	add.f64 	%fd3872, %fd933, %fd3859;
	add.f64 	%fd3873, %fd932, %fd3860;
	sub.f64 	%fd1089, %fd3868, %fd3858;
	sub.f64 	%fd1090, %fd3869, %fd3859;
	sub.f64 	%fd1091, %fd3870, %fd3860;
	add.f64 	%fd1092, %fd928, %fd3862;
	add.f64 	%fd1093, %fd927, %fd3863;
	add.f64 	%fd1094, %fd926, %fd3864;
	sub.f64 	%fd1095, %fd3865, %fd3862;
	sub.f64 	%fd1096, %fd3866, %fd3863;
	sub.f64 	%fd1097, %fd3867, %fd3864;
	add.f64 	%fd1098, %fd3871, 0d0000000000000000;
	add.f64 	%fd1099, %fd3872, 0d0000000000000000;
	add.f64 	%fd1100, %fd3873, 0d0000000000000000;
	setp.eq.s64 	%p222, %rd125, 0;
	@%p222 bra 	$L__BB7_249;

	cvt.s64.s32 	%rd276, %r671;
	mul.lo.s64 	%rd277, %rd276, %rd58;
	add.s64 	%rd273, %rd125, %rd277;
	// begin inline asm
	{ atom.add.f64 %fd3874,[%rd273],%fd1098; }

	// end inline asm
	add.s64 	%rd274, %rd273, 8;
	// begin inline asm
	{ atom.add.f64 %fd3876,[%rd274],%fd1099; }

	// end inline asm
	add.s64 	%rd275, %rd273, 16;
	// begin inline asm
	{ atom.add.f64 %fd3878,[%rd275],%fd1100; }

	// end inline asm
	bra.uni 	$L__BB7_251;

$L__BB7_249:
	setp.eq.s64 	%p223, %rd100, 0;
	@%p223 bra 	$L__BB7_251;

	cvt.s64.s32 	%rd281, %r671;
	mul.lo.s64 	%rd282, %rd281, %rd46;
	add.s64 	%rd278, %rd100, %rd282;
	// begin inline asm
	{ atom.add.f64 %fd3880,[%rd278],%fd1098; }

	// end inline asm
	add.s64 	%rd279, %rd278, 8;
	// begin inline asm
	{ atom.add.f64 %fd3882,[%rd279],%fd1099; }

	// end inline asm
	add.s64 	%rd280, %rd278, 16;
	// begin inline asm
	{ atom.add.f64 %fd3884,[%rd280],%fd1100; }

	// end inline asm

$L__BB7_251:
	add.f64 	%fd1101, %fd1089, 0d0000000000000000;
	add.f64 	%fd1102, %fd1090, 0d0000000000000000;
	add.f64 	%fd1103, %fd1091, 0d0000000000000000;
	@%p222 bra 	$L__BB7_253;

	cvt.s64.s32 	%rd286, %r672;
	mul.lo.s64 	%rd287, %rd286, %rd58;
	add.s64 	%rd283, %rd125, %rd287;
	// begin inline asm
	{ atom.add.f64 %fd3886,[%rd283],%fd1101; }

	// end inline asm
	add.s64 	%rd284, %rd283, 8;
	// begin inline asm
	{ atom.add.f64 %fd3888,[%rd284],%fd1102; }

	// end inline asm
	add.s64 	%rd285, %rd283, 16;
	// begin inline asm
	{ atom.add.f64 %fd3890,[%rd285],%fd1103; }

	// end inline asm
	bra.uni 	$L__BB7_255;

$L__BB7_253:
	setp.eq.s64 	%p225, %rd100, 0;
	@%p225 bra 	$L__BB7_255;

	cvt.s64.s32 	%rd291, %r672;
	mul.lo.s64 	%rd292, %rd291, %rd46;
	add.s64 	%rd288, %rd100, %rd292;
	// begin inline asm
	{ atom.add.f64 %fd3892,[%rd288],%fd1101; }

	// end inline asm
	add.s64 	%rd289, %rd288, 8;
	// begin inline asm
	{ atom.add.f64 %fd3894,[%rd289],%fd1102; }

	// end inline asm
	add.s64 	%rd290, %rd288, 16;
	// begin inline asm
	{ atom.add.f64 %fd3896,[%rd290],%fd1103; }

	// end inline asm

$L__BB7_255:
	add.f64 	%fd1104, %fd1092, 0d0000000000000000;
	add.f64 	%fd1105, %fd1093, 0d0000000000000000;
	add.f64 	%fd1106, %fd1094, 0d0000000000000000;
	@%p222 bra 	$L__BB7_257;

	cvt.s64.s32 	%rd296, %r673;
	mul.lo.s64 	%rd297, %rd296, %rd58;
	add.s64 	%rd293, %rd125, %rd297;
	// begin inline asm
	{ atom.add.f64 %fd3898,[%rd293],%fd1104; }

	// end inline asm
	add.s64 	%rd294, %rd293, 8;
	// begin inline asm
	{ atom.add.f64 %fd3900,[%rd294],%fd1105; }

	// end inline asm
	add.s64 	%rd295, %rd293, 16;
	// begin inline asm
	{ atom.add.f64 %fd3902,[%rd295],%fd1106; }

	// end inline asm
	bra.uni 	$L__BB7_259;

$L__BB7_257:
	setp.eq.s64 	%p227, %rd100, 0;
	@%p227 bra 	$L__BB7_259;

	cvt.s64.s32 	%rd301, %r673;
	mul.lo.s64 	%rd302, %rd301, %rd46;
	add.s64 	%rd298, %rd100, %rd302;
	// begin inline asm
	{ atom.add.f64 %fd3904,[%rd298],%fd1104; }

	// end inline asm
	add.s64 	%rd299, %rd298, 8;
	// begin inline asm
	{ atom.add.f64 %fd3906,[%rd299],%fd1105; }

	// end inline asm
	add.s64 	%rd300, %rd298, 16;
	// begin inline asm
	{ atom.add.f64 %fd3908,[%rd300],%fd1106; }

	// end inline asm

$L__BB7_259:
	add.f64 	%fd1107, %fd1095, 0d0000000000000000;
	add.f64 	%fd1108, %fd1096, 0d0000000000000000;
	add.f64 	%fd1109, %fd1097, 0d0000000000000000;
	@%p222 bra 	$L__BB7_261;

	cvt.s64.s32 	%rd306, %r674;
	mul.lo.s64 	%rd307, %rd306, %rd58;
	add.s64 	%rd303, %rd125, %rd307;
	// begin inline asm
	{ atom.add.f64 %fd3910,[%rd303],%fd1107; }

	// end inline asm
	add.s64 	%rd304, %rd303, 8;
	// begin inline asm
	{ atom.add.f64 %fd3912,[%rd304],%fd1108; }

	// end inline asm
	add.s64 	%rd305, %rd303, 16;
	// begin inline asm
	{ atom.add.f64 %fd3914,[%rd305],%fd1109; }

	// end inline asm
	bra.uni 	$L__BB7_263;

$L__BB7_261:
	setp.eq.s64 	%p229, %rd100, 0;
	@%p229 bra 	$L__BB7_263;

	cvt.s64.s32 	%rd311, %r674;
	mul.lo.s64 	%rd312, %rd311, %rd46;
	add.s64 	%rd308, %rd100, %rd312;
	// begin inline asm
	{ atom.add.f64 %fd3916,[%rd308],%fd1107; }

	// end inline asm
	add.s64 	%rd309, %rd308, 8;
	// begin inline asm
	{ atom.add.f64 %fd3918,[%rd309],%fd1108; }

	// end inline asm
	add.s64 	%rd310, %rd308, 16;
	// begin inline asm
	{ atom.add.f64 %fd3920,[%rd310],%fd1109; }

	// end inline asm

$L__BB7_263:
	setp.eq.s64 	%p230, %rd131, 0;
	add.f64 	%fd1110, %fd938, 0d0000000000000000;
	@%p230 bra 	$L__BB7_265;

	cvt.s64.s32 	%rd314, %r675;
	mul.lo.s64 	%rd315, %rd314, %rd64;
	add.s64 	%rd313, %rd131, %rd315;
	// begin inline asm
	{ atom.add.f64 %fd3922,[%rd313],%fd1110; }

	// end inline asm
	bra.uni 	$L__BB7_267;

$L__BB7_265:
	setp.eq.s64 	%p231, %rd108, 0;
	@%p231 bra 	$L__BB7_267;

	cvt.s64.s32 	%rd317, %r675;
	mul.lo.s64 	%rd318, %rd317, %rd48;
	add.s64 	%rd316, %rd108, %rd318;
	// begin inline asm
	{ atom.add.f64 %fd3924,[%rd316],%fd1110; }

	// end inline asm

$L__BB7_267:
	@%p230 bra 	$L__BB7_269;

	cvt.s64.s32 	%rd320, %r676;
	mul.lo.s64 	%rd321, %rd320, %rd64;
	add.s64 	%rd319, %rd131, %rd321;
	// begin inline asm
	{ atom.add.f64 %fd3926,[%rd319],%fd1110; }

	// end inline asm
	bra.uni 	$L__BB7_271;

$L__BB7_269:
	setp.eq.s64 	%p233, %rd108, 0;
	@%p233 bra 	$L__BB7_271;

	cvt.s64.s32 	%rd323, %r676;
	mul.lo.s64 	%rd324, %rd323, %rd48;
	add.s64 	%rd322, %rd108, %rd324;
	// begin inline asm
	{ atom.add.f64 %fd3928,[%rd322],%fd1110; }

	// end inline asm

$L__BB7_271:
	@%p32 bra 	$L__BB7_392;

	setp.eq.s64 	%p235, %rd123, 0;
	@%p235 bra 	$L__BB7_274;

	cvta.to.global.u64 	%rd325, %rd123;
	mul.lo.s64 	%rd326, %rd75, %rd50;
	add.s64 	%rd327, %rd325, %rd326;
	ld.global.f64 	%fd3930, [%rd327];
	add.f64 	%fd6406, %fd3930, 0d0000000000000000;
	ld.global.f64 	%fd3931, [%rd327+8];
	add.f64 	%fd6405, %fd3931, 0d0000000000000000;
	ld.global.f64 	%fd3932, [%rd327+16];
	add.f64 	%fd6404, %fd3932, 0d0000000000000000;
	bra.uni 	$L__BB7_276;

$L__BB7_274:
	setp.eq.s64 	%p236, %rd92, 0;
	mov.f64 	%fd6404, 0d0000000000000000;
	mov.f64 	%fd6405, %fd6404;
	mov.f64 	%fd6406, %fd6404;
	@%p236 bra 	$L__BB7_276;

	cvta.to.global.u64 	%rd328, %rd92;
	mul.lo.s64 	%rd329, %rd75, %rd51;
	add.s64 	%rd330, %rd328, %rd329;
	ld.global.f64 	%fd3936, [%rd330];
	add.f64 	%fd6406, %fd3936, 0d0000000000000000;
	ld.global.f64 	%fd3937, [%rd330+8];
	add.f64 	%fd6405, %fd3937, 0d0000000000000000;
	ld.global.f64 	%fd3938, [%rd330+16];
	add.f64 	%fd6404, %fd3938, 0d0000000000000000;

$L__BB7_276:
	setp.eq.s64 	%p237, %rd121, 0;
	@%p237 bra 	$L__BB7_278;

	cvta.to.global.u64 	%rd331, %rd121;
	mul.lo.s64 	%rd332, %rd75, %rd52;
	add.s64 	%rd333, %rd331, %rd332;
	ld.global.f64 	%fd3939, [%rd333];
	add.f64 	%fd6408, %fd3939, 0d0000000000000000;
	ld.global.f64 	%fd3940, [%rd333+8];
	add.f64 	%fd6407, %fd3940, 0d0000000000000000;
	bra.uni 	$L__BB7_280;

$L__BB7_278:
	setp.eq.s64 	%p238, %rd90, 0;
	mov.f64 	%fd6407, 0d0000000000000000;
	mov.f64 	%fd6408, %fd6407;
	@%p238 bra 	$L__BB7_280;

	cvta.to.global.u64 	%rd334, %rd90;
	mul.lo.s64 	%rd335, %rd75, %rd53;
	add.s64 	%rd336, %rd334, %rd335;
	ld.global.f64 	%fd3943, [%rd336];
	add.f64 	%fd6408, %fd3943, 0d0000000000000000;
	ld.global.f64 	%fd3944, [%rd336+8];
	add.f64 	%fd6407, %fd3944, 0d0000000000000000;

$L__BB7_280:
	sub.f64 	%fd1126, %fd5849, %fd5843;
	sub.f64 	%fd1127, %fd5847, %fd5844;
	mul.f64 	%fd3945, %fd1127, %fd1126;
	sub.f64 	%fd1128, %fd5850, %fd5844;
	sub.f64 	%fd1129, %fd5846, %fd5843;
	mul.f64 	%fd3946, %fd1129, %fd1128;
	sub.f64 	%fd1130, %fd3945, %fd3946;
	sub.f64 	%fd1131, %fd5851, %fd5845;
	mul.f64 	%fd3947, %fd1129, %fd1131;
	sub.f64 	%fd1132, %fd5848, %fd5845;
	mul.f64 	%fd3948, %fd1132, %fd1126;
	sub.f64 	%fd1133, %fd3947, %fd3948;
	mul.f64 	%fd3949, %fd1132, %fd1128;
	mul.f64 	%fd3950, %fd1127, %fd1131;
	sub.f64 	%fd1134, %fd3949, %fd3950;
	mul.f64 	%fd3951, %fd1127, %fd1134;
	mul.f64 	%fd3952, %fd1129, %fd1133;
	sub.f64 	%fd1135, %fd3951, %fd3952;
	mul.f64 	%fd3953, %fd1129, %fd1130;
	mul.f64 	%fd3954, %fd1132, %fd1134;
	sub.f64 	%fd1136, %fd3953, %fd3954;
	mul.f64 	%fd3955, %fd1132, %fd1133;
	mul.f64 	%fd3956, %fd1127, %fd1130;
	sub.f64 	%fd1137, %fd3955, %fd3956;
	mul.f64 	%fd3957, %fd1132, %fd1132;
	fma.rn.f64 	%fd3958, %fd1127, %fd1127, %fd3957;
	fma.rn.f64 	%fd1138, %fd1129, %fd1129, %fd3958;
	mul.f64 	%fd3959, %fd1127, %fd1136;
	fma.rn.f64 	%fd3960, %fd1132, %fd1135, %fd3959;
	fma.rn.f64 	%fd3961, %fd1129, %fd1137, %fd3960;
	mul.f64 	%fd3962, %fd1136, %fd1136;
	fma.rn.f64 	%fd3963, %fd1135, %fd1135, %fd3962;
	fma.rn.f64 	%fd3964, %fd1137, %fd1137, %fd3963;
	sub.f64 	%fd1139, %fd5842, %fd5845;
	mul.f64 	%fd3965, %fd1139, %fd1132;
	sub.f64 	%fd1140, %fd5841, %fd5844;
	fma.rn.f64 	%fd3966, %fd1140, %fd1127, %fd3965;
	sub.f64 	%fd1141, %fd5840, %fd5843;
	fma.rn.f64 	%fd1142, %fd1141, %fd1129, %fd3966;
	mul.f64 	%fd3967, %fd1140, %fd1136;
	fma.rn.f64 	%fd3968, %fd1139, %fd1135, %fd3967;
	fma.rn.f64 	%fd3969, %fd1141, %fd1137, %fd3968;
	div.rn.f64 	%fd1143, %fd3961, %fd1138;
	mul.f64 	%fd1144, %fd1143, %fd1143;
	mul.f64 	%fd3970, %fd1138, %fd1144;
	sub.f64 	%fd1145, %fd3964, %fd3970;
	mul.f64 	%fd3971, %fd1142, %fd1143;
	sub.f64 	%fd3972, %fd3969, %fd3971;
	div.rn.f64 	%fd1146, %fd3972, %fd1145;
	mul.f64 	%fd1147, %fd1138, %fd1143;
	mul.f64 	%fd3973, %fd1147, %fd1146;
	sub.f64 	%fd3974, %fd1142, %fd3973;
	div.rn.f64 	%fd1148, %fd3974, %fd1138;
	setp.gt.f64 	%p239, %fd1148, 0d0000000000000000;
	setp.lt.f64 	%p240, %fd1148, 0d3FF0000000000000;
	setp.ge.f64 	%p241, %fd1146, 0d0000000000000000;
	and.pred  	%p242, %p239, %p240;
	and.pred  	%p16, %p241, %p242;
	mov.u32 	%r720, 3;
	@%p16 bra 	$L__BB7_286;

	sub.f64 	%fd3975, %fd5851, %fd5848;
	sub.f64 	%fd3976, %fd5850, %fd5847;
	mul.f64 	%fd3977, %fd3976, %fd1134;
	sub.f64 	%fd3978, %fd5849, %fd5846;
	mul.f64 	%fd3979, %fd3978, %fd1133;
	sub.f64 	%fd3980, %fd3977, %fd3979;
	mul.f64 	%fd3981, %fd3978, %fd1130;
	mul.f64 	%fd3982, %fd3975, %fd1134;
	sub.f64 	%fd3983, %fd3981, %fd3982;
	mul.f64 	%fd3984, %fd3975, %fd1133;
	mul.f64 	%fd3985, %fd3976, %fd1130;
	sub.f64 	%fd3986, %fd3984, %fd3985;
	mul.f64 	%fd3987, %fd3975, %fd3975;
	fma.rn.f64 	%fd3988, %fd3976, %fd3976, %fd3987;
	fma.rn.f64 	%fd3989, %fd3978, %fd3978, %fd3988;
	mul.f64 	%fd3990, %fd3976, %fd3983;
	fma.rn.f64 	%fd3991, %fd3975, %fd3980, %fd3990;
	fma.rn.f64 	%fd3992, %fd3978, %fd3986, %fd3991;
	mul.f64 	%fd3993, %fd3983, %fd3983;
	fma.rn.f64 	%fd3994, %fd3980, %fd3980, %fd3993;
	fma.rn.f64 	%fd3995, %fd3986, %fd3986, %fd3994;
	sub.f64 	%fd3996, %fd5842, %fd5848;
	mul.f64 	%fd3997, %fd3996, %fd3975;
	sub.f64 	%fd3998, %fd5841, %fd5847;
	fma.rn.f64 	%fd3999, %fd3998, %fd3976, %fd3997;
	sub.f64 	%fd4000, %fd5840, %fd5846;
	fma.rn.f64 	%fd4001, %fd4000, %fd3978, %fd3999;
	mul.f64 	%fd4002, %fd3998, %fd3983;
	fma.rn.f64 	%fd4003, %fd3996, %fd3980, %fd4002;
	fma.rn.f64 	%fd4004, %fd4000, %fd3986, %fd4003;
	div.rn.f64 	%fd4005, %fd3992, %fd3989;
	mul.f64 	%fd4006, %fd4005, %fd4005;
	mul.f64 	%fd4007, %fd3989, %fd4006;
	sub.f64 	%fd4008, %fd3995, %fd4007;
	mul.f64 	%fd4009, %fd4001, %fd4005;
	sub.f64 	%fd4010, %fd4004, %fd4009;
	div.rn.f64 	%fd4011, %fd4010, %fd4008;
	mul.f64 	%fd4012, %fd3989, %fd4005;
	mul.f64 	%fd4013, %fd4012, %fd4011;
	sub.f64 	%fd4014, %fd4001, %fd4013;
	div.rn.f64 	%fd1149, %fd4014, %fd3989;
	setp.gt.f64 	%p243, %fd1149, 0d0000000000000000;
	setp.lt.f64 	%p244, %fd1149, 0d3FF0000000000000;
	setp.ge.f64 	%p245, %fd4011, 0d0000000000000000;
	and.pred  	%p246, %p243, %p244;
	and.pred  	%p247, %p245, %p246;
	mov.u32 	%r720, 4;
	@%p247 bra 	$L__BB7_286;

	sub.f64 	%fd4015, %fd5845, %fd5851;
	sub.f64 	%fd4016, %fd5844, %fd5850;
	mul.f64 	%fd4017, %fd4016, %fd1134;
	sub.f64 	%fd4018, %fd5843, %fd5849;
	mul.f64 	%fd4019, %fd4018, %fd1133;
	sub.f64 	%fd4020, %fd4017, %fd4019;
	mul.f64 	%fd4021, %fd4018, %fd1130;
	mul.f64 	%fd4022, %fd4015, %fd1134;
	sub.f64 	%fd4023, %fd4021, %fd4022;
	mul.f64 	%fd4024, %fd4015, %fd1133;
	mul.f64 	%fd4025, %fd4016, %fd1130;
	sub.f64 	%fd4026, %fd4024, %fd4025;
	mul.f64 	%fd4027, %fd4015, %fd4015;
	fma.rn.f64 	%fd4028, %fd4016, %fd4016, %fd4027;
	fma.rn.f64 	%fd4029, %fd4018, %fd4018, %fd4028;
	mul.f64 	%fd4030, %fd4016, %fd4023;
	fma.rn.f64 	%fd4031, %fd4015, %fd4020, %fd4030;
	fma.rn.f64 	%fd4032, %fd4018, %fd4026, %fd4031;
	mul.f64 	%fd4033, %fd4023, %fd4023;
	fma.rn.f64 	%fd4034, %fd4020, %fd4020, %fd4033;
	fma.rn.f64 	%fd4035, %fd4026, %fd4026, %fd4034;
	sub.f64 	%fd4036, %fd5842, %fd5851;
	mul.f64 	%fd4037, %fd4015, %fd4036;
	sub.f64 	%fd4038, %fd5841, %fd5850;
	fma.rn.f64 	%fd4039, %fd4016, %fd4038, %fd4037;
	sub.f64 	%fd4040, %fd5840, %fd5849;
	fma.rn.f64 	%fd4041, %fd4018, %fd4040, %fd4039;
	mul.f64 	%fd4042, %fd4038, %fd4023;
	fma.rn.f64 	%fd4043, %fd4036, %fd4020, %fd4042;
	fma.rn.f64 	%fd4044, %fd4040, %fd4026, %fd4043;
	div.rn.f64 	%fd4045, %fd4032, %fd4029;
	mul.f64 	%fd4046, %fd4045, %fd4045;
	mul.f64 	%fd4047, %fd4029, %fd4046;
	sub.f64 	%fd4048, %fd4035, %fd4047;
	mul.f64 	%fd4049, %fd4041, %fd4045;
	sub.f64 	%fd4050, %fd4044, %fd4049;
	div.rn.f64 	%fd4051, %fd4050, %fd4048;
	mul.f64 	%fd4052, %fd4029, %fd4045;
	mul.f64 	%fd4053, %fd4052, %fd4051;
	sub.f64 	%fd4054, %fd4041, %fd4053;
	div.rn.f64 	%fd1150, %fd4054, %fd4029;
	setp.gt.f64 	%p248, %fd1150, 0d0000000000000000;
	setp.lt.f64 	%p249, %fd1150, 0d3FF0000000000000;
	setp.ge.f64 	%p250, %fd4051, 0d0000000000000000;
	and.pred  	%p251, %p248, %p249;
	and.pred  	%p252, %p250, %p251;
	mov.u32 	%r720, 5;
	@%p252 bra 	$L__BB7_286;

	setp.le.f64 	%p253, %fd1148, 0d0000000000000000;
	setp.ge.f64 	%p254, %fd1150, 0d3FF0000000000000;
	and.pred  	%p255, %p253, %p254;
	mov.u32 	%r720, 0;
	@%p255 bra 	$L__BB7_286;

	setp.le.f64 	%p256, %fd1149, 0d0000000000000000;
	setp.ge.f64 	%p257, %fd1148, 0d3FF0000000000000;
	and.pred  	%p258, %p256, %p257;
	mov.u32 	%r720, 1;
	@%p258 bra 	$L__BB7_286;

	setp.le.f64 	%p259, %fd1150, 0d0000000000000000;
	setp.ge.f64 	%p260, %fd1149, 0d3FF0000000000000;
	and.pred  	%p261, %p259, %p260;
	selp.b32 	%r720, 2, 6, %p261;

$L__BB7_286:
	setp.eq.s32 	%p262, %r720, 0;
	mov.f64 	%fd6508, 0d0000000000000000;
	mov.f64 	%fd6509, %fd6508;
	mov.f64 	%fd6510, %fd6508;
	mov.f64 	%fd6511, %fd6508;
	mov.f64 	%fd6512, %fd6508;
	mov.f64 	%fd6513, %fd6508;
	mov.f64 	%fd6476, %fd6508;
	mov.f64 	%fd6477, %fd6508;
	mov.f64 	%fd6478, %fd6508;
	mov.f64 	%fd6479, %fd6508;
	mov.f64 	%fd6480, %fd6508;
	mov.f64 	%fd6481, %fd6508;
	mov.f64 	%fd6452, %fd6508;
	mov.f64 	%fd6453, %fd6508;
	mov.f64 	%fd6454, %fd6508;
	mov.f64 	%fd6455, %fd6508;
	mov.f64 	%fd6456, %fd6508;
	mov.f64 	%fd6457, %fd6508;
	mov.f64 	%fd6458, %fd6508;
	mov.f64 	%fd6459, %fd6508;
	mov.f64 	%fd6460, %fd6508;
	mov.f64 	%fd6461, %fd6508;
	mov.f64 	%fd6462, %fd6508;
	mov.f64 	%fd6463, %fd6508;
	mov.f64 	%fd6464, %fd6508;
	mov.f64 	%fd6465, %fd6508;
	mov.f64 	%fd6466, %fd6508;
	mov.f64 	%fd6611, %fd6508;
	mov.f64 	%fd6612, %fd6508;
	@%p262 bra 	$L__BB7_308;

	setp.eq.s32 	%p17, %r720, 1;
	selp.f64 	%fd6545, 0d3FF0000000000000, 0d0000000000000000, %p17;
	mov.f64 	%fd6546, 0d0000000000000000;
	@%p17 bra 	$L__BB7_306;
	bra.uni 	$L__BB7_288;

$L__BB7_306:
	mov.f64 	%fd6508, 0d0000000000000000;
	mov.f64 	%fd6509, %fd6508;
	mov.f64 	%fd6510, %fd6508;
	mov.f64 	%fd6511, %fd6508;
	mov.f64 	%fd6512, %fd6508;
	mov.f64 	%fd6513, %fd6508;
	mov.f64 	%fd6476, %fd6508;
	mov.f64 	%fd6477, %fd6508;
	mov.f64 	%fd6478, %fd6508;
	mov.f64 	%fd6479, %fd6508;
	mov.f64 	%fd6480, %fd6508;
	mov.f64 	%fd6481, %fd6508;
	mov.f64 	%fd6452, %fd6508;
	mov.f64 	%fd6453, %fd6508;
	mov.f64 	%fd6454, %fd6508;
	mov.f64 	%fd6455, %fd6508;
	mov.f64 	%fd6456, %fd6508;
	mov.f64 	%fd6457, %fd6508;
	mov.f64 	%fd6458, %fd6508;
	mov.f64 	%fd6459, %fd6508;
	mov.f64 	%fd6460, %fd6508;
	mov.f64 	%fd6461, %fd6508;
	mov.f64 	%fd6462, %fd6508;
	mov.f64 	%fd6463, %fd6508;
	mov.f64 	%fd6464, %fd6508;
	mov.f64 	%fd6465, %fd6508;
	mov.f64 	%fd6466, %fd6508;
	mov.f64 	%fd6574, %fd6508;
	mov.f64 	%fd6575, %fd6508;
	bra.uni 	$L__BB7_307;

$L__BB7_288:
	setp.eq.s32 	%p18, %r720, 2;
	selp.f64 	%fd4085, 0d3FF0000000000000, 0d0000000000000000, %p18;
	selp.f64 	%fd6507, %fd4085, %fd6546, %p18;
	selp.f64 	%fd6506, 0d0000000000000000, %fd6545, %p18;
	@%p18 bra 	$L__BB7_304;
	bra.uni 	$L__BB7_289;

$L__BB7_304:
	mov.f64 	%fd6508, 0d0000000000000000;
	mov.f64 	%fd6509, %fd6508;
	mov.f64 	%fd6510, %fd6508;
	mov.f64 	%fd6511, %fd6508;
	mov.f64 	%fd6512, %fd6508;
	mov.f64 	%fd6513, %fd6508;
	mov.f64 	%fd6476, %fd6508;
	mov.f64 	%fd6477, %fd6508;
	mov.f64 	%fd6478, %fd6508;
	mov.f64 	%fd6479, %fd6508;
	mov.f64 	%fd6480, %fd6508;
	mov.f64 	%fd6481, %fd6508;
	mov.f64 	%fd6452, %fd6508;
	mov.f64 	%fd6453, %fd6508;
	mov.f64 	%fd6454, %fd6508;
	mov.f64 	%fd6455, %fd6508;
	mov.f64 	%fd6456, %fd6508;
	mov.f64 	%fd6457, %fd6508;
	mov.f64 	%fd6458, %fd6508;
	mov.f64 	%fd6459, %fd6508;
	mov.f64 	%fd6460, %fd6508;
	mov.f64 	%fd6461, %fd6508;
	mov.f64 	%fd6462, %fd6508;
	mov.f64 	%fd6463, %fd6508;
	mov.f64 	%fd6464, %fd6508;
	mov.f64 	%fd6465, %fd6508;
	mov.f64 	%fd6466, %fd6508;
	mov.f64 	%fd6535, %fd6508;
	mov.f64 	%fd6536, %fd6508;
	bra.uni 	$L__BB7_305;

$L__BB7_289:
	setp.eq.s32 	%p19, %r720, 3;
	setp.ne.s32 	%p263, %r720, 3;
	mov.f64 	%fd6508, 0d0000000000000000;
	mov.f64 	%fd6509, %fd6508;
	mov.f64 	%fd6510, %fd6508;
	mov.f64 	%fd6511, %fd6508;
	mov.f64 	%fd6512, %fd6508;
	mov.f64 	%fd6513, %fd6508;
	mov.f64 	%fd6537, %fd6508;
	@%p263 bra 	$L__BB7_291;

	div.rn.f64 	%fd6537, %fd1142, %fd1138;
	mov.f64 	%fd6508, %fd1141;
	mov.f64 	%fd6509, %fd1140;
	mov.f64 	%fd6510, %fd1139;
	mov.f64 	%fd6511, %fd1129;
	mov.f64 	%fd6512, %fd1127;
	mov.f64 	%fd6513, %fd1132;

$L__BB7_291:
	selp.f64 	%fd6474, %fd6537, %fd6506, %p19;
	selp.f64 	%fd6475, 0d0000000000000000, %fd6507, %p19;
	@%p19 bra 	$L__BB7_302;
	bra.uni 	$L__BB7_292;

$L__BB7_302:
	mov.f64 	%fd6476, 0d0000000000000000;
	mov.f64 	%fd6477, %fd6476;
	mov.f64 	%fd6478, %fd6476;
	mov.f64 	%fd6479, %fd6476;
	mov.f64 	%fd6480, %fd6476;
	mov.f64 	%fd6481, %fd6476;
	mov.f64 	%fd6452, %fd6476;
	mov.f64 	%fd6453, %fd6476;
	mov.f64 	%fd6454, %fd6476;
	mov.f64 	%fd6455, %fd6476;
	mov.f64 	%fd6456, %fd6476;
	mov.f64 	%fd6457, %fd6476;
	mov.f64 	%fd6458, %fd6476;
	mov.f64 	%fd6459, %fd6476;
	mov.f64 	%fd6460, %fd6476;
	mov.f64 	%fd6461, %fd6476;
	mov.f64 	%fd6462, %fd6476;
	mov.f64 	%fd6463, %fd6476;
	mov.f64 	%fd6464, %fd6476;
	mov.f64 	%fd6465, %fd6476;
	mov.f64 	%fd6466, %fd6476;
	mov.f64 	%fd6497, %fd6476;
	mov.f64 	%fd6498, %fd6476;
	bra.uni 	$L__BB7_303;

$L__BB7_292:
	setp.eq.s32 	%p20, %r720, 4;
	setp.ne.s32 	%p265, %r720, 4;
	mov.f64 	%fd6476, 0d0000000000000000;
	mov.f64 	%fd6477, %fd6476;
	mov.f64 	%fd6478, %fd6476;
	mov.f64 	%fd6479, %fd6476;
	mov.f64 	%fd6480, %fd6476;
	mov.f64 	%fd6481, %fd6476;
	mov.f64 	%fd6500, %fd6476;
	mov.f64 	%fd6424, %fd6476;
	@%p265 bra 	$L__BB7_294;

	sub.f64 	%fd6478, %fd5842, %fd5848;
	sub.f64 	%fd6481, %fd5851, %fd5848;
	mul.f64 	%fd4103, %fd6478, %fd6481;
	sub.f64 	%fd6480, %fd5850, %fd5847;
	sub.f64 	%fd6477, %fd5841, %fd5847;
	fma.rn.f64 	%fd4104, %fd6477, %fd6480, %fd4103;
	sub.f64 	%fd6479, %fd5849, %fd5846;
	sub.f64 	%fd6476, %fd5840, %fd5846;
	fma.rn.f64 	%fd4105, %fd6476, %fd6479, %fd4104;
	mul.f64 	%fd4106, %fd6481, %fd6481;
	fma.rn.f64 	%fd4107, %fd6480, %fd6480, %fd4106;
	fma.rn.f64 	%fd6499, %fd6479, %fd6479, %fd4107;
	div.rn.f64 	%fd6500, %fd4105, %fd6499;
	mov.f64 	%fd4108, 0d3FF0000000000000;
	sub.f64 	%fd6424, %fd4108, %fd6500;

$L__BB7_294:
	selp.f64 	%fd6450, %fd6424, %fd6474, %p20;
	selp.f64 	%fd6451, %fd6500, %fd6475, %p20;
	@%p20 bra 	$L__BB7_300;
	bra.uni 	$L__BB7_295;

$L__BB7_300:
	mov.f64 	%fd6452, 0d0000000000000000;
	mov.f64 	%fd6453, %fd6452;
	mov.f64 	%fd6454, %fd6452;
	mov.f64 	%fd6455, %fd6452;
	mov.f64 	%fd6456, %fd6452;
	mov.f64 	%fd6457, %fd6452;
	mov.f64 	%fd6458, %fd6452;
	mov.f64 	%fd6459, %fd6452;
	mov.f64 	%fd6460, %fd6452;
	mov.f64 	%fd6461, %fd6452;
	mov.f64 	%fd6462, %fd6452;
	mov.f64 	%fd6463, %fd6452;
	mov.f64 	%fd6464, %fd6452;
	mov.f64 	%fd6465, %fd6452;
	mov.f64 	%fd6466, %fd6452;
	mov.f64 	%fd6467, %fd6452;
	mov.f64 	%fd6468, %fd6452;
	bra.uni 	$L__BB7_301;

$L__BB7_295:
	setp.eq.s32 	%p21, %r720, 5;
	setp.ne.s32 	%p267, %r720, 5;
	mov.f64 	%fd6458, 0d0000000000000000;
	mov.f64 	%fd6452, %fd6458;
	mov.f64 	%fd6453, %fd6458;
	mov.f64 	%fd6454, %fd6458;
	mov.f64 	%fd6455, %fd6458;
	mov.f64 	%fd6456, %fd6458;
	mov.f64 	%fd6457, %fd6458;
	mov.f64 	%fd6470, %fd6458;
	@%p267 bra 	$L__BB7_297;

	mul.f64 	%fd4117, %fd1139, %fd1131;
	fma.rn.f64 	%fd4118, %fd1140, %fd1128, %fd4117;
	fma.rn.f64 	%fd4119, %fd1141, %fd1126, %fd4118;
	mul.f64 	%fd4120, %fd1131, %fd1131;
	fma.rn.f64 	%fd4121, %fd1128, %fd1128, %fd4120;
	fma.rn.f64 	%fd6469, %fd1126, %fd1126, %fd4121;
	div.rn.f64 	%fd6470, %fd4119, %fd6469;
	mov.f64 	%fd6452, %fd1141;
	mov.f64 	%fd6453, %fd1140;
	mov.f64 	%fd6454, %fd1139;
	mov.f64 	%fd6455, %fd1126;
	mov.f64 	%fd6456, %fd1128;
	mov.f64 	%fd6457, %fd1131;

$L__BB7_297:
	selp.f64 	%fd1207, %fd6470, %fd6451, %p21;
	selp.f64 	%fd1206, 0d0000000000000000, %fd6450, %p21;
	mov.f64 	%fd6459, %fd6458;
	mov.f64 	%fd6460, %fd6458;
	mov.f64 	%fd6461, %fd6458;
	mov.f64 	%fd6462, %fd6458;
	mov.f64 	%fd6463, %fd6458;
	mov.f64 	%fd6464, %fd6458;
	mov.f64 	%fd6465, %fd6458;
	mov.f64 	%fd6466, %fd6458;
	mov.f64 	%fd6445, %fd6458;
	mov.f64 	%fd6446, %fd6458;
	@%p21 bra 	$L__BB7_299;

	mul.f64 	%fd4134, %fd1132, %fd1131;
	fma.rn.f64 	%fd4135, %fd1127, %fd1128, %fd4134;
	fma.rn.f64 	%fd6471, %fd1129, %fd1126, %fd4135;
	mul.f64 	%fd4136, %fd1131, %fd1131;
	fma.rn.f64 	%fd4137, %fd1128, %fd1128, %fd4136;
	fma.rn.f64 	%fd6472, %fd1126, %fd1126, %fd4137;
	mul.f64 	%fd4138, %fd1139, %fd1131;
	fma.rn.f64 	%fd4139, %fd1140, %fd1128, %fd4138;
	fma.rn.f64 	%fd6473, %fd1141, %fd1126, %fd4139;
	div.rn.f64 	%fd4140, %fd6471, %fd1138;
	mul.f64 	%fd4141, %fd4140, %fd4140;
	mul.f64 	%fd4142, %fd1138, %fd4141;
	sub.f64 	%fd4143, %fd6472, %fd4142;
	mul.f64 	%fd4144, %fd1142, %fd4140;
	sub.f64 	%fd4145, %fd6473, %fd4144;
	div.rn.f64 	%fd6445, %fd4145, %fd4143;
	mul.f64 	%fd4146, %fd1138, %fd4140;
	mul.f64 	%fd4147, %fd4146, %fd6445;
	sub.f64 	%fd4148, %fd1142, %fd4147;
	div.rn.f64 	%fd6446, %fd4148, %fd1138;
	mov.f64 	%fd6458, %fd1129;
	mov.f64 	%fd6459, %fd1127;
	mov.f64 	%fd6460, %fd1132;
	mov.f64 	%fd6461, %fd1126;
	mov.f64 	%fd6462, %fd1128;
	mov.f64 	%fd6463, %fd1131;
	mov.f64 	%fd6464, %fd1141;
	mov.f64 	%fd6465, %fd1140;
	mov.f64 	%fd6466, %fd1139;

$L__BB7_299:
	selp.f64 	%fd6468, %fd1206, %fd6446, %p21;
	selp.f64 	%fd6467, %fd1207, %fd6445, %p21;
	selp.u16 	%rs375, 1, 0, %p21;

$L__BB7_301:
	selp.f64 	%fd6498, %fd6450, %fd6468, %p20;
	selp.f64 	%fd6497, %fd6451, %fd6467, %p20;
	selp.u16 	%rs377, 1, 0, %p20;

$L__BB7_303:
	selp.f64 	%fd6536, %fd6474, %fd6498, %p19;
	selp.f64 	%fd6535, %fd6475, %fd6497, %p19;
	selp.u16 	%rs380, 1, 0, %p19;

$L__BB7_305:
	selp.f64 	%fd6575, %fd6506, %fd6536, %p18;
	selp.f64 	%fd6574, %fd6507, %fd6535, %p18;
	selp.u16 	%rs384, 1, 0, %p18;

$L__BB7_307:
	selp.f64 	%fd6612, %fd6545, %fd6575, %p17;
	selp.f64 	%fd6611, %fd6546, %fd6574, %p17;
	selp.u16 	%rs383, 1, 0, %p17;

$L__BB7_308:
	selp.f64 	%fd1417, 0d0000000000000000, %fd6611, %p262;
	selp.f64 	%fd1416, 0d0000000000000000, %fd6612, %p262;
	mov.f64 	%fd4251, 0d3FF0000000000000;
	sub.f64 	%fd4252, %fd4251, %fd1416;
	sub.f64 	%fd1418, %fd4252, %fd1417;
	mul.f64 	%fd4253, %fd5845, %fd1418;
	mul.f64 	%fd4254, %fd5844, %fd1418;
	mul.f64 	%fd4255, %fd5843, %fd1418;
	fma.rn.f64 	%fd4256, %fd5848, %fd1416, %fd4253;
	fma.rn.f64 	%fd4257, %fd5847, %fd1416, %fd4254;
	fma.rn.f64 	%fd4258, %fd5846, %fd1416, %fd4255;
	fma.rn.f64 	%fd4259, %fd5851, %fd1417, %fd4256;
	fma.rn.f64 	%fd4260, %fd5850, %fd1417, %fd4257;
	fma.rn.f64 	%fd4261, %fd5849, %fd1417, %fd4258;
	sub.f64 	%fd4262, %fd5842, %fd4259;
	sub.f64 	%fd4263, %fd5841, %fd4260;
	sub.f64 	%fd4264, %fd5840, %fd4261;
	mul.f64 	%fd4265, %fd4263, %fd4263;
	fma.rn.f64 	%fd4266, %fd4262, %fd4262, %fd4265;
	fma.rn.f64 	%fd4267, %fd4264, %fd4264, %fd4266;
	sqrt.rn.f64 	%fd4268, %fd4267;
	div.rn.f64 	%fd1419, %fd4262, %fd4268;
	div.rn.f64 	%fd1420, %fd4263, %fd4268;
	div.rn.f64 	%fd1421, %fd4264, %fd4268;
	add.f64 	%fd1422, %fd6408, 0d0000000000000000;
	add.f64 	%fd4269, %fd6406, 0d0000000000000000;
	add.f64 	%fd4270, %fd6405, 0d0000000000000000;
	mul.f64 	%fd4271, %fd4270, %fd4263;
	fma.rn.f64 	%fd4272, %fd4269, %fd4262, %fd4271;
	add.f64 	%fd4273, %fd6404, 0d0000000000000000;
	fma.rn.f64 	%fd4274, %fd4273, %fd4264, %fd4272;
	mul.f64 	%fd4275, %fd4268, %fd4268;
	div.rn.f64 	%fd1423, %fd4274, %fd4275;
	div.rn.f64 	%fd4276, %fd4269, %fd4268;
	add.f64 	%fd6623, %fd4276, 0d0000000000000000;
	div.rn.f64 	%fd4277, %fd4270, %fd4268;
	add.f64 	%fd6622, %fd4277, 0d0000000000000000;
	div.rn.f64 	%fd4278, %fd4273, %fd4268;
	add.f64 	%fd6621, %fd4278, 0d0000000000000000;
	setp.leu.f64 	%p275, %fd4268, 0d0000000000000000;
	@%p275 bra 	$L__BB7_310;

	mov.f64 	%fd4279, 0d0000000000000000;
	sub.f64 	%fd4280, %fd4279, %fd1423;
	fma.rn.f64 	%fd6623, %fd1419, %fd4280, %fd6623;
	fma.rn.f64 	%fd6622, %fd1420, %fd4280, %fd6622;
	fma.rn.f64 	%fd6621, %fd1421, %fd4280, %fd6621;

$L__BB7_310:
	add.f64 	%fd6654, %fd6623, 0d0000000000000000;
	mov.f64 	%fd6706, 0d0000000000000000;
	add.f64 	%fd6653, %fd6622, 0d0000000000000000;
	add.f64 	%fd6652, %fd6621, 0d0000000000000000;
	sub.f64 	%fd4282, %fd6706, %fd6622;
	add.f64 	%fd4283, %fd4282, 0d0000000000000000;
	sub.f64 	%fd4284, %fd6706, %fd6623;
	add.f64 	%fd4285, %fd4284, 0d0000000000000000;
	sub.f64 	%fd4286, %fd6706, %fd6621;
	add.f64 	%fd4287, %fd4286, 0d0000000000000000;
	fma.rn.f64 	%fd6663, %fd1417, %fd4285, 0d0000000000000000;
	fma.rn.f64 	%fd6662, %fd1417, %fd4283, 0d0000000000000000;
	fma.rn.f64 	%fd6661, %fd1417, %fd4287, 0d0000000000000000;
	mul.f64 	%fd4288, %fd5851, %fd4285;
	fma.rn.f64 	%fd4289, %fd5850, %fd4283, %fd4288;
	fma.rn.f64 	%fd4290, %fd5849, %fd4287, %fd4289;
	add.f64 	%fd4291, %fd4290, 0d0000000000000000;
	fma.rn.f64 	%fd6660, %fd1416, %fd4285, 0d0000000000000000;
	fma.rn.f64 	%fd6659, %fd1416, %fd4283, 0d0000000000000000;
	fma.rn.f64 	%fd6658, %fd1416, %fd4287, 0d0000000000000000;
	add.f64 	%fd4292, %fd6407, 0d0000000000000000;
	add.f64 	%fd4293, %fd4292, %fd4291;
	mul.f64 	%fd4294, %fd5848, %fd4285;
	fma.rn.f64 	%fd4295, %fd5847, %fd4283, %fd4294;
	fma.rn.f64 	%fd4296, %fd5846, %fd4287, %fd4295;
	add.f64 	%fd4297, %fd4296, 0d0000000000000000;
	fma.rn.f64 	%fd6657, %fd1418, %fd4285, 0d0000000000000000;
	fma.rn.f64 	%fd6656, %fd1418, %fd4283, 0d0000000000000000;
	fma.rn.f64 	%fd6655, %fd1418, %fd4287, 0d0000000000000000;
	add.f64 	%fd4298, %fd1422, %fd4297;
	mul.f64 	%fd4299, %fd5845, %fd4285;
	fma.rn.f64 	%fd4300, %fd5844, %fd4283, %fd4299;
	fma.rn.f64 	%fd4301, %fd5843, %fd4287, %fd4300;
	add.f64 	%fd4302, %fd4301, 0d0000000000000000;
	sub.f64 	%fd4303, %fd6706, %fd4302;
	add.f64 	%fd1445, %fd4293, %fd4303;
	add.f64 	%fd1446, %fd4298, %fd4303;
	or.b16  	%rs325, %rs384, %rs383;
	and.b16  	%rs326, %rs325, 255;
	setp.ne.s16 	%p277, %rs326, 0;
	or.pred  	%p278, %p277, %p262;
	@%p278 bra 	$L__BB7_321;

	add.f64 	%fd6664, %fd1446, 0d0000000000000000;
	and.b16  	%rs327, %rs380, 255;
	setp.eq.s16 	%p22, %rs327, 0;
	setp.ne.s16 	%p279, %rs327, 0;
	@%p279 bra 	$L__BB7_319;

	add.f64 	%fd6650, %fd1445, 0d0000000000000000;
	and.b16  	%rs328, %rs377, 255;
	setp.eq.s16 	%p23, %rs328, 0;
	setp.ne.s16 	%p280, %rs328, 0;
	mov.f64 	%fd6651, %fd6664;
	@%p280 bra 	$L__BB7_317;

	and.b16  	%rs329, %rs375, 255;
	setp.ne.s16 	%p281, %rs329, 0;
	mov.f64 	%fd6636, %fd6650;
	mov.f64 	%fd6637, %fd6664;
	@%p281 bra 	$L__BB7_315;

	div.rn.f64 	%fd4306, %fd6471, %fd1138;
	mul.f64 	%fd4307, %fd4306, %fd4306;
	mul.f64 	%fd4308, %fd1138, %fd4307;
	sub.f64 	%fd4309, %fd6472, %fd4308;
	mul.f64 	%fd4310, %fd1142, %fd4306;
	sub.f64 	%fd4311, %fd6473, %fd4310;
	div.rn.f64 	%fd4312, %fd4311, %fd4309;
	mul.f64 	%fd4313, %fd1138, %fd4306;
	mul.f64 	%fd4314, %fd4313, %fd4312;
	sub.f64 	%fd4315, %fd1142, %fd4314;
	div.rn.f64 	%fd4316, %fd4315, %fd1138;
	div.rn.f64 	%fd4317, %fd6664, %fd1138;
	add.f64 	%fd4318, %fd4317, 0d0000000000000000;
	mov.f64 	%fd6636, 0d0000000000000000;
	mul.f64 	%fd4319, %fd4316, %fd6664;
	div.rn.f64 	%fd4320, %fd4319, %fd1138;
	sub.f64 	%fd4321, %fd6636, %fd4320;
	sub.f64 	%fd4322, %fd6636, %fd4318;
	fma.rn.f64 	%fd4323, %fd4312, %fd4322, 0d0000000000000000;
	fma.rn.f64 	%fd4324, %fd4313, %fd4322, %fd6650;
	fma.rn.f64 	%fd4325, %fd4306, %fd4323, %fd4321;
	fma.rn.f64 	%fd4326, %fd1138, %fd4323, 0d0000000000000000;
	div.rn.f64 	%fd4327, %fd4324, %fd4309;
	add.f64 	%fd4328, %fd4327, 0d0000000000000000;
	mul.f64 	%fd4329, %fd4312, %fd4324;
	div.rn.f64 	%fd4330, %fd4329, %fd4309;
	sub.f64 	%fd4331, %fd6636, %fd4330;
	sub.f64 	%fd4332, %fd6636, %fd4328;
	fma.rn.f64 	%fd4333, %fd1142, %fd4332, %fd4326;
	fma.rn.f64 	%fd4334, %fd4306, %fd4332, %fd4318;
	add.f64 	%fd4335, %fd4334, 0d0000000000000000;
	add.f64 	%fd4336, %fd4331, 0d0000000000000000;
	sub.f64 	%fd4337, %fd6636, %fd4331;
	fma.rn.f64 	%fd4338, %fd1138, %fd4337, 0d0000000000000000;
	fma.rn.f64 	%fd4339, %fd4307, %fd4337, %fd4325;
	fma.rn.f64 	%fd4340, %fd4306, %fd4338, %fd4333;
	fma.rn.f64 	%fd4341, %fd4306, %fd4338, %fd4340;
	div.rn.f64 	%fd4342, %fd4341, %fd1138;
	add.f64 	%fd4343, %fd4342, 0d0000000000000000;
	mul.f64 	%fd4344, %fd4306, %fd4341;
	div.rn.f64 	%fd4345, %fd4344, %fd1138;
	sub.f64 	%fd4346, %fd4339, %fd4345;
	add.f64 	%fd4347, %fd4346, 0d0000000000000000;
	fma.rn.f64 	%fd4348, %fd6466, %fd4328, 0d0000000000000000;
	fma.rn.f64 	%fd4349, %fd6465, %fd4328, 0d0000000000000000;
	fma.rn.f64 	%fd4350, %fd6464, %fd4328, 0d0000000000000000;
	fma.rn.f64 	%fd4351, %fd6463, %fd4328, 0d0000000000000000;
	fma.rn.f64 	%fd4352, %fd6462, %fd4328, 0d0000000000000000;
	fma.rn.f64 	%fd4353, %fd6461, %fd4328, 0d0000000000000000;
	fma.rn.f64 	%fd4354, %fd6466, %fd4335, 0d0000000000000000;
	fma.rn.f64 	%fd4355, %fd6465, %fd4335, 0d0000000000000000;
	fma.rn.f64 	%fd4356, %fd6464, %fd4335, 0d0000000000000000;
	fma.rn.f64 	%fd4357, %fd6460, %fd4335, %fd4351;
	fma.rn.f64 	%fd4358, %fd6459, %fd4335, %fd4352;
	fma.rn.f64 	%fd4359, %fd6458, %fd4335, %fd4353;
	add.f64 	%fd4360, %fd6463, %fd6463;
	add.f64 	%fd4361, %fd6462, %fd6462;
	add.f64 	%fd4362, %fd6461, %fd6461;
	fma.rn.f64 	%fd4363, %fd4360, %fd4336, %fd4348;
	fma.rn.f64 	%fd4364, %fd4361, %fd4336, %fd4349;
	fma.rn.f64 	%fd4365, %fd4362, %fd4336, %fd4350;
	fma.rn.f64 	%fd4366, %fd6463, %fd4343, %fd4354;
	fma.rn.f64 	%fd4367, %fd6462, %fd4343, %fd4355;
	fma.rn.f64 	%fd4368, %fd6461, %fd4343, %fd4356;
	fma.rn.f64 	%fd4369, %fd6460, %fd4343, %fd4363;
	fma.rn.f64 	%fd4370, %fd6459, %fd4343, %fd4364;
	fma.rn.f64 	%fd4371, %fd6458, %fd4343, %fd4365;
	add.f64 	%fd4372, %fd6460, %fd6460;
	add.f64 	%fd4373, %fd6459, %fd6459;
	add.f64 	%fd4374, %fd6458, %fd6458;
	fma.rn.f64 	%fd4375, %fd4372, %fd4347, %fd4366;
	fma.rn.f64 	%fd4376, %fd4373, %fd4347, %fd4367;
	fma.rn.f64 	%fd4377, %fd4374, %fd4347, %fd4368;
	add.f64 	%fd6654, %fd6654, %fd4357;
	add.f64 	%fd6653, %fd6653, %fd4358;
	add.f64 	%fd6652, %fd6652, %fd4359;
	sub.f64 	%fd4378, %fd6657, %fd4357;
	sub.f64 	%fd4379, %fd6656, %fd4358;
	sub.f64 	%fd4380, %fd6655, %fd4359;
	add.f64 	%fd6663, %fd6663, %fd4369;
	add.f64 	%fd6662, %fd6662, %fd4370;
	add.f64 	%fd6661, %fd6661, %fd4371;
	sub.f64 	%fd4381, %fd4378, %fd4369;
	sub.f64 	%fd4382, %fd4379, %fd4370;
	sub.f64 	%fd4383, %fd4380, %fd4371;
	add.f64 	%fd6660, %fd6660, %fd4375;
	add.f64 	%fd6659, %fd6659, %fd4376;
	add.f64 	%fd6658, %fd6658, %fd4377;
	sub.f64 	%fd6657, %fd4381, %fd4375;
	sub.f64 	%fd6656, %fd4382, %fd4376;
	sub.f64 	%fd6655, %fd4383, %fd4377;
	mov.f64 	%fd6637, %fd6636;

$L__BB7_315:
	selp.f64 	%fd4384, 0d0000000000000000, %fd6664, %p23;
	add.f64 	%fd6651, %fd4384, %fd6637;
	selp.f64 	%fd4385, 0d0000000000000000, %fd6650, %p23;
	add.f64 	%fd6650, %fd4385, %fd6636;
	setp.eq.s16 	%p282, %rs329, 0;
	@%p282 bra 	$L__BB7_317;

	add.f64 	%fd4388, %fd6636, 0d0000000000000000;
	mov.f64 	%fd6650, 0d0000000000000000;
	div.rn.f64 	%fd4389, %fd4388, %fd6469;
	add.f64 	%fd4390, %fd4389, 0d0000000000000000;
	mul.f64 	%fd4391, %fd6470, %fd4388;
	div.rn.f64 	%fd4392, %fd4391, %fd6469;
	sub.f64 	%fd4393, %fd6650, %fd4392;
	add.f64 	%fd4394, %fd6457, %fd6457;
	add.f64 	%fd4395, %fd6456, %fd6456;
	add.f64 	%fd4396, %fd6455, %fd6455;
	fma.rn.f64 	%fd4397, %fd4394, %fd4393, 0d0000000000000000;
	fma.rn.f64 	%fd4398, %fd4395, %fd4393, 0d0000000000000000;
	fma.rn.f64 	%fd4399, %fd4396, %fd4393, 0d0000000000000000;
	fma.rn.f64 	%fd4400, %fd6457, %fd4390, 0d0000000000000000;
	fma.rn.f64 	%fd4401, %fd6456, %fd4390, 0d0000000000000000;
	fma.rn.f64 	%fd4402, %fd6455, %fd4390, 0d0000000000000000;
	fma.rn.f64 	%fd4403, %fd6454, %fd4390, %fd4397;
	fma.rn.f64 	%fd4404, %fd6453, %fd4390, %fd4398;
	fma.rn.f64 	%fd4405, %fd6452, %fd4390, %fd4399;
	add.f64 	%fd6663, %fd6663, %fd4403;
	add.f64 	%fd6662, %fd6662, %fd4404;
	add.f64 	%fd6661, %fd6661, %fd4405;
	sub.f64 	%fd4406, %fd6657, %fd4403;
	sub.f64 	%fd4407, %fd6656, %fd4404;
	sub.f64 	%fd4408, %fd6655, %fd4405;
	add.f64 	%fd6654, %fd6654, %fd4400;
	add.f64 	%fd6653, %fd6653, %fd4401;
	add.f64 	%fd6652, %fd6652, %fd4402;
	sub.f64 	%fd6657, %fd4406, %fd4400;
	sub.f64 	%fd6656, %fd4407, %fd4401;
	sub.f64 	%fd6655, %fd4408, %fd4402;
	mov.f64 	%fd6651, %fd6650;

$L__BB7_317:
	selp.f64 	%fd4409, 0d0000000000000000, %fd6664, %p22;
	add.f64 	%fd6664, %fd4409, %fd6651;
	@%p23 bra 	$L__BB7_319;

	add.f64 	%fd4411, %fd6650, 0d0000000000000000;
	mov.f64 	%fd6664, 0d0000000000000000;
	add.f64 	%fd4412, %fd6651, 0d0000000000000000;
	sub.f64 	%fd4413, %fd4411, %fd4412;
	div.rn.f64 	%fd4414, %fd4413, %fd6499;
	add.f64 	%fd4415, %fd4414, 0d0000000000000000;
	mul.f64 	%fd4416, %fd6500, %fd4413;
	div.rn.f64 	%fd4417, %fd4416, %fd6499;
	sub.f64 	%fd4418, %fd6664, %fd4417;
	add.f64 	%fd4419, %fd6481, %fd6481;
	add.f64 	%fd4420, %fd6480, %fd6480;
	add.f64 	%fd4421, %fd6479, %fd6479;
	fma.rn.f64 	%fd4422, %fd4419, %fd4418, 0d0000000000000000;
	fma.rn.f64 	%fd4423, %fd4420, %fd4418, 0d0000000000000000;
	fma.rn.f64 	%fd4424, %fd4421, %fd4418, 0d0000000000000000;
	fma.rn.f64 	%fd4425, %fd6481, %fd4415, 0d0000000000000000;
	fma.rn.f64 	%fd4426, %fd6480, %fd4415, 0d0000000000000000;
	fma.rn.f64 	%fd4427, %fd6479, %fd4415, 0d0000000000000000;
	fma.rn.f64 	%fd4428, %fd6478, %fd4415, %fd4422;
	fma.rn.f64 	%fd4429, %fd6477, %fd4415, %fd4423;
	fma.rn.f64 	%fd4430, %fd6476, %fd4415, %fd4424;
	add.f64 	%fd6663, %fd6663, %fd4428;
	add.f64 	%fd6662, %fd6662, %fd4429;
	add.f64 	%fd6661, %fd6661, %fd4430;
	sub.f64 	%fd4431, %fd6660, %fd4428;
	sub.f64 	%fd4432, %fd6659, %fd4429;
	sub.f64 	%fd4433, %fd6658, %fd4430;
	add.f64 	%fd6654, %fd6654, %fd4425;
	add.f64 	%fd6653, %fd6653, %fd4426;
	add.f64 	%fd6652, %fd6652, %fd4427;
	sub.f64 	%fd6660, %fd4431, %fd4425;
	sub.f64 	%fd6659, %fd4432, %fd4426;
	sub.f64 	%fd6658, %fd4433, %fd4427;

$L__BB7_319:
	@%p22 bra 	$L__BB7_321;

	add.f64 	%fd4434, %fd6664, 0d0000000000000000;
	mov.f64 	%fd4435, 0d0000000000000000;
	div.rn.f64 	%fd4436, %fd4434, %fd1138;
	add.f64 	%fd4437, %fd4436, 0d0000000000000000;
	mul.f64 	%fd4438, %fd6537, %fd4434;
	div.rn.f64 	%fd4439, %fd4438, %fd1138;
	sub.f64 	%fd4440, %fd4435, %fd4439;
	add.f64 	%fd4441, %fd6513, %fd6513;
	add.f64 	%fd4442, %fd6512, %fd6512;
	add.f64 	%fd4443, %fd6511, %fd6511;
	fma.rn.f64 	%fd4444, %fd4441, %fd4440, 0d0000000000000000;
	fma.rn.f64 	%fd4445, %fd4442, %fd4440, 0d0000000000000000;
	fma.rn.f64 	%fd4446, %fd4443, %fd4440, 0d0000000000000000;
	fma.rn.f64 	%fd4447, %fd6513, %fd4437, 0d0000000000000000;
	fma.rn.f64 	%fd4448, %fd6512, %fd4437, 0d0000000000000000;
	fma.rn.f64 	%fd4449, %fd6511, %fd4437, 0d0000000000000000;
	fma.rn.f64 	%fd4450, %fd6510, %fd4437, %fd4444;
	fma.rn.f64 	%fd4451, %fd6509, %fd4437, %fd4445;
	fma.rn.f64 	%fd4452, %fd6508, %fd4437, %fd4446;
	add.f64 	%fd6660, %fd6660, %fd4450;
	add.f64 	%fd6659, %fd6659, %fd4451;
	add.f64 	%fd6658, %fd6658, %fd4452;
	sub.f64 	%fd4453, %fd6657, %fd4450;
	sub.f64 	%fd4454, %fd6656, %fd4451;
	sub.f64 	%fd4455, %fd6655, %fd4452;
	add.f64 	%fd6654, %fd6654, %fd4447;
	add.f64 	%fd6653, %fd6653, %fd4448;
	add.f64 	%fd6652, %fd6652, %fd4449;
	sub.f64 	%fd6657, %fd4453, %fd4447;
	sub.f64 	%fd6656, %fd4454, %fd4448;
	sub.f64 	%fd6655, %fd4455, %fd4449;

$L__BB7_321:
	mov.f64 	%fd6707, %fd6706;
	mov.f64 	%fd6708, %fd6706;
	mov.f64 	%fd6709, %fd6706;
	mov.f64 	%fd6710, %fd6706;
	mov.f64 	%fd6711, %fd6706;
	mov.f64 	%fd6712, %fd6706;
	mov.f64 	%fd6713, %fd6706;
	mov.f64 	%fd6714, %fd6706;
	@%p16 bra 	$L__BB7_325;

	sub.f64 	%fd1544, %fd5851, %fd5848;
	sub.f64 	%fd1545, %fd5850, %fd5847;
	mul.f64 	%fd4473, %fd1545, %fd1134;
	sub.f64 	%fd1546, %fd5849, %fd5846;
	mul.f64 	%fd4474, %fd1546, %fd1133;
	sub.f64 	%fd1547, %fd4473, %fd4474;
	mul.f64 	%fd4475, %fd1546, %fd1130;
	mul.f64 	%fd4476, %fd1544, %fd1134;
	sub.f64 	%fd1548, %fd4475, %fd4476;
	mul.f64 	%fd4477, %fd1544, %fd1133;
	mul.f64 	%fd4478, %fd1545, %fd1130;
	sub.f64 	%fd1549, %fd4477, %fd4478;
	mul.f64 	%fd4479, %fd1544, %fd1544;
	fma.rn.f64 	%fd4480, %fd1545, %fd1545, %fd4479;
	fma.rn.f64 	%fd1550, %fd1546, %fd1546, %fd4480;
	mul.f64 	%fd4481, %fd1545, %fd1548;
	fma.rn.f64 	%fd4482, %fd1544, %fd1547, %fd4481;
	fma.rn.f64 	%fd4483, %fd1546, %fd1549, %fd4482;
	mul.f64 	%fd4484, %fd1548, %fd1548;
	fma.rn.f64 	%fd4485, %fd1547, %fd1547, %fd4484;
	fma.rn.f64 	%fd4486, %fd1549, %fd1549, %fd4485;
	sub.f64 	%fd1551, %fd5842, %fd5848;
	mul.f64 	%fd4487, %fd1551, %fd1544;
	sub.f64 	%fd1552, %fd5841, %fd5847;
	fma.rn.f64 	%fd4488, %fd1552, %fd1545, %fd4487;
	sub.f64 	%fd1553, %fd5840, %fd5846;
	fma.rn.f64 	%fd1554, %fd1553, %fd1546, %fd4488;
	mul.f64 	%fd4489, %fd1552, %fd1548;
	fma.rn.f64 	%fd4490, %fd1551, %fd1547, %fd4489;
	fma.rn.f64 	%fd4491, %fd1553, %fd1549, %fd4490;
	div.rn.f64 	%fd1555, %fd4483, %fd1550;
	mul.f64 	%fd1556, %fd1555, %fd1555;
	mul.f64 	%fd4492, %fd1550, %fd1556;
	sub.f64 	%fd1557, %fd4486, %fd4492;
	mul.f64 	%fd4493, %fd1554, %fd1555;
	sub.f64 	%fd4494, %fd4491, %fd4493;
	div.rn.f64 	%fd1558, %fd4494, %fd1557;
	mul.f64 	%fd1559, %fd1550, %fd1555;
	mul.f64 	%fd4495, %fd1559, %fd1558;
	sub.f64 	%fd4496, %fd1554, %fd4495;
	div.rn.f64 	%fd1560, %fd4496, %fd1550;
	setp.gt.f64 	%p285, %fd1560, 0d0000000000000000;
	mov.f64 	%fd4472, 0d0000000000000000;
	setp.lt.f64 	%p286, %fd1560, 0d3FF0000000000000;
	setp.ge.f64 	%p287, %fd1558, 0d0000000000000000;
	and.pred  	%p288, %p285, %p286;
	and.pred  	%p289, %p287, %p288;
	mov.f64 	%fd6686, %fd4472;
	mov.f64 	%fd6687, %fd4472;
	mov.f64 	%fd6688, %fd4472;
	mov.f64 	%fd6689, %fd4472;
	mov.f64 	%fd6690, %fd4472;
	mov.f64 	%fd6691, %fd4472;
	mov.f64 	%fd6692, %fd4472;
	mov.f64 	%fd6693, %fd4472;
	@%p289 bra 	$L__BB7_324;

	sub.f64 	%fd4497, %fd5845, %fd5851;
	sub.f64 	%fd4498, %fd5844, %fd5850;
	mul.f64 	%fd4499, %fd4498, %fd1134;
	sub.f64 	%fd4500, %fd5843, %fd5849;
	mul.f64 	%fd4501, %fd4500, %fd1133;
	sub.f64 	%fd4502, %fd4499, %fd4501;
	mul.f64 	%fd4503, %fd4500, %fd1130;
	mul.f64 	%fd4504, %fd4497, %fd1134;
	sub.f64 	%fd4505, %fd4503, %fd4504;
	mul.f64 	%fd4506, %fd4497, %fd1133;
	mul.f64 	%fd4507, %fd4498, %fd1130;
	sub.f64 	%fd4508, %fd4506, %fd4507;
	mul.f64 	%fd4509, %fd4497, %fd4497;
	fma.rn.f64 	%fd4510, %fd4498, %fd4498, %fd4509;
	fma.rn.f64 	%fd4511, %fd4500, %fd4500, %fd4510;
	mul.f64 	%fd4512, %fd4498, %fd4505;
	fma.rn.f64 	%fd4513, %fd4497, %fd4502, %fd4512;
	fma.rn.f64 	%fd4514, %fd4500, %fd4508, %fd4513;
	mul.f64 	%fd4515, %fd4505, %fd4505;
	fma.rn.f64 	%fd4516, %fd4502, %fd4502, %fd4515;
	fma.rn.f64 	%fd4517, %fd4508, %fd4508, %fd4516;
	sub.f64 	%fd4518, %fd5842, %fd5851;
	mul.f64 	%fd4519, %fd4497, %fd4518;
	sub.f64 	%fd4520, %fd5841, %fd5850;
	fma.rn.f64 	%fd4521, %fd4498, %fd4520, %fd4519;
	sub.f64 	%fd4522, %fd5840, %fd5849;
	fma.rn.f64 	%fd4523, %fd4500, %fd4522, %fd4521;
	mul.f64 	%fd4524, %fd4520, %fd4505;
	fma.rn.f64 	%fd4525, %fd4518, %fd4502, %fd4524;
	fma.rn.f64 	%fd4526, %fd4522, %fd4508, %fd4525;
	div.rn.f64 	%fd4527, %fd4514, %fd4511;
	mul.f64 	%fd4528, %fd4527, %fd4527;
	mul.f64 	%fd4529, %fd4511, %fd4528;
	sub.f64 	%fd4530, %fd4517, %fd4529;
	mul.f64 	%fd4531, %fd4523, %fd4527;
	sub.f64 	%fd4532, %fd4526, %fd4531;
	div.rn.f64 	%fd4533, %fd4532, %fd4530;
	mul.f64 	%fd4534, %fd4511, %fd4527;
	mul.f64 	%fd4535, %fd4534, %fd4533;
	sub.f64 	%fd4536, %fd4523, %fd4535;
	div.rn.f64 	%fd4537, %fd4536, %fd4511;
	mov.f64 	%fd4538, 0d0000000000000000;
	div.rn.f64 	%fd4539, %fd4538, %fd4511;
	add.f64 	%fd4540, %fd4539, 0d0000000000000000;
	mul.f64 	%fd4541, %fd4537, 0d0000000000000000;
	div.rn.f64 	%fd4542, %fd4541, %fd4511;
	sub.f64 	%fd4543, %fd4538, %fd4542;
	sub.f64 	%fd4544, %fd4538, %fd4540;
	fma.rn.f64 	%fd4545, %fd4544, %fd4533, 0d0000000000000000;
	fma.rn.f64 	%fd4546, %fd4544, %fd4534, 0d0000000000000000;
	fma.rn.f64 	%fd4547, %fd4527, %fd4545, %fd4543;
	fma.rn.f64 	%fd4548, %fd4511, %fd4545, 0d0000000000000000;
	div.rn.f64 	%fd4549, %fd4546, %fd4530;
	add.f64 	%fd6689, %fd4549, 0d0000000000000000;
	mul.f64 	%fd4550, %fd4546, %fd4533;
	div.rn.f64 	%fd4551, %fd4550, %fd4530;
	sub.f64 	%fd4552, %fd4538, %fd4551;
	sub.f64 	%fd4553, %fd4538, %fd6689;
	fma.rn.f64 	%fd4554, %fd4523, %fd4553, %fd4548;
	fma.rn.f64 	%fd4555, %fd4527, %fd4553, %fd4540;
	add.f64 	%fd6690, %fd4555, 0d0000000000000000;
	add.f64 	%fd6686, %fd4552, 0d0000000000000000;
	sub.f64 	%fd4556, %fd4538, %fd4552;
	fma.rn.f64 	%fd4557, %fd4511, %fd4556, 0d0000000000000000;
	fma.rn.f64 	%fd4558, %fd4528, %fd4556, %fd4547;
	fma.rn.f64 	%fd4559, %fd4527, %fd4557, %fd4554;
	fma.rn.f64 	%fd4560, %fd4527, %fd4557, %fd4559;
	div.rn.f64 	%fd4561, %fd4560, %fd4511;
	add.f64 	%fd6687, %fd4561, 0d0000000000000000;
	mul.f64 	%fd4562, %fd4527, %fd4560;
	div.rn.f64 	%fd4563, %fd4562, %fd4511;
	sub.f64 	%fd4564, %fd4558, %fd4563;
	add.f64 	%fd6688, %fd4564, 0d0000000000000000;
	fma.rn.f64 	%fd4565, %fd4518, %fd6689, 0d0000000000000000;
	fma.rn.f64 	%fd4566, %fd4520, %fd6689, 0d0000000000000000;
	fma.rn.f64 	%fd4567, %fd4522, %fd6689, 0d0000000000000000;
	fma.rn.f64 	%fd4568, %fd4502, %fd6689, 0d0000000000000000;
	fma.rn.f64 	%fd4569, %fd4505, %fd6689, 0d0000000000000000;
	fma.rn.f64 	%fd4570, %fd4508, %fd6689, 0d0000000000000000;
	fma.rn.f64 	%fd4571, %fd4518, %fd6690, 0d0000000000000000;
	fma.rn.f64 	%fd4572, %fd4520, %fd6690, 0d0000000000000000;
	fma.rn.f64 	%fd4573, %fd4522, %fd6690, 0d0000000000000000;
	fma.rn.f64 	%fd4574, %fd4497, %fd6690, %fd4568;
	fma.rn.f64 	%fd4575, %fd4498, %fd6690, %fd4569;
	fma.rn.f64 	%fd4576, %fd4500, %fd6690, %fd4570;
	add.f64 	%fd6654, %fd4574, %fd6654;
	add.f64 	%fd6653, %fd4575, %fd6653;
	add.f64 	%fd6652, %fd4576, %fd6652;
	sub.f64 	%fd4577, %fd6663, %fd4574;
	sub.f64 	%fd4578, %fd6662, %fd4575;
	sub.f64 	%fd4579, %fd6661, %fd4576;
	fma.rn.f64 	%fd4580, %fd4508, %fd6686, 0d0000000000000000;
	add.f64 	%fd4581, %fd4567, %fd4580;
	add.f64 	%fd4582, %fd4580, %fd4581;
	fma.rn.f64 	%fd4583, %fd4505, %fd6686, 0d0000000000000000;
	add.f64 	%fd4584, %fd4566, %fd4583;
	add.f64 	%fd4585, %fd4583, %fd4584;
	fma.rn.f64 	%fd4586, %fd4502, %fd6686, 0d0000000000000000;
	add.f64 	%fd4587, %fd4565, %fd4586;
	add.f64 	%fd4588, %fd4586, %fd4587;
	fma.rn.f64 	%fd4589, %fd4508, %fd6687, 0d0000000000000000;
	fma.rn.f64 	%fd4590, %fd4500, %fd6687, 0d0000000000000000;
	add.f64 	%fd4591, %fd4582, %fd4590;
	add.f64 	%fd4592, %fd4573, %fd4589;
	fma.rn.f64 	%fd4593, %fd4505, %fd6687, 0d0000000000000000;
	fma.rn.f64 	%fd4594, %fd4498, %fd6687, 0d0000000000000000;
	add.f64 	%fd4595, %fd4585, %fd4594;
	add.f64 	%fd4596, %fd4572, %fd4593;
	fma.rn.f64 	%fd4597, %fd4502, %fd6687, 0d0000000000000000;
	fma.rn.f64 	%fd4598, %fd4497, %fd6687, 0d0000000000000000;
	add.f64 	%fd4599, %fd4588, %fd4598;
	add.f64 	%fd4600, %fd4571, %fd4597;
	fma.rn.f64 	%fd4601, %fd4500, %fd6688, 0d0000000000000000;
	add.f64 	%fd4602, %fd4592, %fd4601;
	add.f64 	%fd4603, %fd4601, %fd4602;
	fma.rn.f64 	%fd4604, %fd4498, %fd6688, 0d0000000000000000;
	add.f64 	%fd4605, %fd4596, %fd4604;
	add.f64 	%fd4606, %fd4604, %fd4605;
	fma.rn.f64 	%fd4607, %fd4497, %fd6688, 0d0000000000000000;
	add.f64 	%fd4608, %fd4600, %fd4607;
	add.f64 	%fd4609, %fd4607, %fd4608;
	mul.f64 	%fd4610, %fd1133, %fd4591;
	mul.f64 	%fd4611, %fd1134, %fd4595;
	sub.f64 	%fd4612, %fd4610, %fd4611;
	mul.f64 	%fd4613, %fd1134, %fd4599;
	mul.f64 	%fd4614, %fd1130, %fd4591;
	sub.f64 	%fd4615, %fd4613, %fd4614;
	mul.f64 	%fd4616, %fd1130, %fd4595;
	mul.f64 	%fd4617, %fd1133, %fd4599;
	sub.f64 	%fd4618, %fd4616, %fd4617;
	add.f64 	%fd4619, %fd4612, %fd4609;
	add.f64 	%fd4620, %fd4615, %fd4606;
	add.f64 	%fd4621, %fd4618, %fd4603;
	mul.f64 	%fd4622, %fd4498, %fd4591;
	mul.f64 	%fd4623, %fd4500, %fd4595;
	mul.f64 	%fd4624, %fd4500, %fd4599;
	mul.f64 	%fd4625, %fd4497, %fd4591;
	mul.f64 	%fd4626, %fd4497, %fd4595;
	mul.f64 	%fd4627, %fd4498, %fd4599;
	sub.f64 	%fd4628, %fd4623, %fd4622;
	add.f64 	%fd6691, %fd4628, 0d0000000000000000;
	sub.f64 	%fd4629, %fd4625, %fd4624;
	add.f64 	%fd6692, %fd4629, 0d0000000000000000;
	sub.f64 	%fd4630, %fd4627, %fd4626;
	add.f64 	%fd6693, %fd4630, 0d0000000000000000;
	add.f64 	%fd6657, %fd4619, %fd6657;
	add.f64 	%fd6656, %fd4620, %fd6656;
	add.f64 	%fd6655, %fd4621, %fd6655;
	sub.f64 	%fd6663, %fd4577, %fd4619;
	sub.f64 	%fd6662, %fd4578, %fd4620;
	sub.f64 	%fd6661, %fd4579, %fd4621;

$L__BB7_324:
	div.rn.f64 	%fd4632, %fd4472, %fd1550;
	add.f64 	%fd4633, %fd4632, 0d0000000000000000;
	mul.f64 	%fd4634, %fd1560, 0d0000000000000000;
	div.rn.f64 	%fd4635, %fd4634, %fd1550;
	sub.f64 	%fd4636, %fd4472, %fd4635;
	sub.f64 	%fd4637, %fd4472, %fd4633;
	fma.rn.f64 	%fd4638, %fd4637, %fd1558, 0d0000000000000000;
	fma.rn.f64 	%fd4639, %fd4637, %fd1559, 0d0000000000000000;
	fma.rn.f64 	%fd4640, %fd1555, %fd4638, %fd4636;
	fma.rn.f64 	%fd4641, %fd1550, %fd4638, 0d0000000000000000;
	div.rn.f64 	%fd4642, %fd4639, %fd1557;
	add.f64 	%fd4643, %fd4642, 0d0000000000000000;
	mul.f64 	%fd4644, %fd4639, %fd1558;
	div.rn.f64 	%fd4645, %fd4644, %fd1557;
	sub.f64 	%fd4646, %fd4472, %fd4645;
	sub.f64 	%fd4647, %fd4472, %fd4643;
	fma.rn.f64 	%fd4648, %fd1554, %fd4647, %fd4641;
	fma.rn.f64 	%fd4649, %fd1555, %fd4647, %fd4633;
	add.f64 	%fd4650, %fd4649, 0d0000000000000000;
	add.f64 	%fd4651, %fd4646, 0d0000000000000000;
	sub.f64 	%fd4652, %fd4472, %fd4646;
	fma.rn.f64 	%fd4653, %fd1550, %fd4652, 0d0000000000000000;
	fma.rn.f64 	%fd4654, %fd1556, %fd4652, %fd4640;
	fma.rn.f64 	%fd4655, %fd1555, %fd4653, %fd4648;
	fma.rn.f64 	%fd4656, %fd1555, %fd4653, %fd4655;
	div.rn.f64 	%fd4657, %fd4656, %fd1550;
	add.f64 	%fd4658, %fd4657, 0d0000000000000000;
	mul.f64 	%fd4659, %fd1555, %fd4656;
	div.rn.f64 	%fd4660, %fd4659, %fd1550;
	sub.f64 	%fd4661, %fd4654, %fd4660;
	add.f64 	%fd4662, %fd4661, 0d0000000000000000;
	add.f64 	%fd6710, %fd4643, %fd6689;
	add.f64 	%fd6711, %fd4650, %fd6690;
	add.f64 	%fd6706, %fd4651, %fd6686;
	add.f64 	%fd6708, %fd4658, %fd6687;
	add.f64 	%fd6709, %fd4662, %fd6688;
	add.f64 	%fd4663, %fd6710, 0d0000000000000000;
	fma.rn.f64 	%fd4664, %fd1551, %fd4663, 0d0000000000000000;
	fma.rn.f64 	%fd4665, %fd1552, %fd4663, 0d0000000000000000;
	fma.rn.f64 	%fd4666, %fd1553, %fd4663, 0d0000000000000000;
	fma.rn.f64 	%fd4667, %fd1547, %fd4663, 0d0000000000000000;
	fma.rn.f64 	%fd4668, %fd1548, %fd4663, 0d0000000000000000;
	fma.rn.f64 	%fd4669, %fd1549, %fd4663, 0d0000000000000000;
	add.f64 	%fd4670, %fd6711, 0d0000000000000000;
	fma.rn.f64 	%fd4671, %fd1551, %fd4670, 0d0000000000000000;
	fma.rn.f64 	%fd4672, %fd1552, %fd4670, 0d0000000000000000;
	fma.rn.f64 	%fd4673, %fd1553, %fd4670, 0d0000000000000000;
	fma.rn.f64 	%fd4674, %fd1544, %fd4670, %fd4667;
	fma.rn.f64 	%fd4675, %fd1545, %fd4670, %fd4668;
	fma.rn.f64 	%fd4676, %fd1546, %fd4670, %fd4669;
	add.f64 	%fd6654, %fd6654, %fd4674;
	add.f64 	%fd6653, %fd6653, %fd4675;
	add.f64 	%fd6652, %fd6652, %fd4676;
	sub.f64 	%fd4677, %fd6660, %fd4674;
	sub.f64 	%fd4678, %fd6659, %fd4675;
	sub.f64 	%fd4679, %fd6658, %fd4676;
	add.f64 	%fd4680, %fd6706, 0d0000000000000000;
	fma.rn.f64 	%fd4681, %fd1549, %fd4680, 0d0000000000000000;
	add.f64 	%fd4682, %fd4681, %fd4666;
	add.f64 	%fd4683, %fd4681, %fd4682;
	fma.rn.f64 	%fd4684, %fd1548, %fd4680, 0d0000000000000000;
	add.f64 	%fd4685, %fd4684, %fd4665;
	add.f64 	%fd4686, %fd4684, %fd4685;
	fma.rn.f64 	%fd4687, %fd1547, %fd4680, 0d0000000000000000;
	add.f64 	%fd4688, %fd4687, %fd4664;
	add.f64 	%fd4689, %fd4687, %fd4688;
	add.f64 	%fd4690, %fd6708, 0d0000000000000000;
	add.f64 	%fd6707, %fd6687, %fd4690;
	add.f64 	%fd4691, %fd6707, 0d0000000000000000;
	fma.rn.f64 	%fd4692, %fd1549, %fd4691, 0d0000000000000000;
	fma.rn.f64 	%fd4693, %fd1546, %fd4691, 0d0000000000000000;
	add.f64 	%fd4694, %fd4693, %fd4683;
	add.f64 	%fd4695, %fd4692, %fd4673;
	fma.rn.f64 	%fd4696, %fd1548, %fd4691, 0d0000000000000000;
	fma.rn.f64 	%fd4697, %fd1545, %fd4691, 0d0000000000000000;
	add.f64 	%fd4698, %fd4697, %fd4686;
	add.f64 	%fd4699, %fd4696, %fd4672;
	fma.rn.f64 	%fd4700, %fd1547, %fd4691, 0d0000000000000000;
	fma.rn.f64 	%fd4701, %fd1544, %fd4691, 0d0000000000000000;
	add.f64 	%fd4702, %fd4701, %fd4689;
	add.f64 	%fd4703, %fd4700, %fd4671;
	add.f64 	%fd4704, %fd6709, 0d0000000000000000;
	fma.rn.f64 	%fd4705, %fd1546, %fd4704, 0d0000000000000000;
	add.f64 	%fd4706, %fd4705, %fd4695;
	add.f64 	%fd4707, %fd4705, %fd4706;
	fma.rn.f64 	%fd4708, %fd1545, %fd4704, 0d0000000000000000;
	add.f64 	%fd4709, %fd4708, %fd4699;
	add.f64 	%fd4710, %fd4708, %fd4709;
	fma.rn.f64 	%fd4711, %fd1544, %fd4704, 0d0000000000000000;
	add.f64 	%fd4712, %fd4711, %fd4703;
	add.f64 	%fd4713, %fd4711, %fd4712;
	mul.f64 	%fd4714, %fd1133, %fd4694;
	mul.f64 	%fd4715, %fd1134, %fd4698;
	sub.f64 	%fd4716, %fd4714, %fd4715;
	mul.f64 	%fd4717, %fd1134, %fd4702;
	mul.f64 	%fd4718, %fd1130, %fd4694;
	sub.f64 	%fd4719, %fd4717, %fd4718;
	mul.f64 	%fd4720, %fd1130, %fd4698;
	mul.f64 	%fd4721, %fd1133, %fd4702;
	sub.f64 	%fd4722, %fd4720, %fd4721;
	add.f64 	%fd4723, %fd4713, %fd4716;
	add.f64 	%fd4724, %fd4710, %fd4719;
	add.f64 	%fd4725, %fd4707, %fd4722;
	mul.f64 	%fd4726, %fd1545, %fd4694;
	mul.f64 	%fd4727, %fd1546, %fd4698;
	sub.f64 	%fd4728, %fd4726, %fd4727;
	mul.f64 	%fd4729, %fd1546, %fd4702;
	mul.f64 	%fd4730, %fd1544, %fd4694;
	sub.f64 	%fd4731, %fd4729, %fd4730;
	mul.f64 	%fd4732, %fd1544, %fd4698;
	mul.f64 	%fd4733, %fd1545, %fd4702;
	sub.f64 	%fd4734, %fd4732, %fd4733;
	sub.f64 	%fd6712, %fd6691, %fd4728;
	sub.f64 	%fd6713, %fd6692, %fd4731;
	sub.f64 	%fd6714, %fd6693, %fd4734;
	add.f64 	%fd6663, %fd6663, %fd4723;
	add.f64 	%fd6662, %fd6662, %fd4724;
	add.f64 	%fd6661, %fd6661, %fd4725;
	sub.f64 	%fd6660, %fd4677, %fd4723;
	sub.f64 	%fd6659, %fd4678, %fd4724;
	sub.f64 	%fd6658, %fd4679, %fd4725;

$L__BB7_325:
	mov.f64 	%fd4736, 0d0000000000000000;
	div.rn.f64 	%fd4737, %fd4736, %fd1138;
	add.f64 	%fd4738, %fd4737, 0d0000000000000000;
	mul.f64 	%fd4739, %fd1148, 0d0000000000000000;
	div.rn.f64 	%fd4740, %fd4739, %fd1138;
	sub.f64 	%fd4741, %fd4736, %fd4740;
	sub.f64 	%fd4742, %fd4736, %fd4738;
	fma.rn.f64 	%fd4743, %fd4742, %fd1146, 0d0000000000000000;
	fma.rn.f64 	%fd4744, %fd4742, %fd1147, 0d0000000000000000;
	fma.rn.f64 	%fd4745, %fd1143, %fd4743, %fd4741;
	fma.rn.f64 	%fd4746, %fd1138, %fd4743, 0d0000000000000000;
	div.rn.f64 	%fd4747, %fd4744, %fd1145;
	add.f64 	%fd1634, %fd4747, 0d0000000000000000;
	mul.f64 	%fd4748, %fd4744, %fd1146;
	div.rn.f64 	%fd4749, %fd4748, %fd1145;
	sub.f64 	%fd4750, %fd4736, %fd4749;
	sub.f64 	%fd4751, %fd4736, %fd1634;
	fma.rn.f64 	%fd4752, %fd1142, %fd4751, %fd4746;
	fma.rn.f64 	%fd4753, %fd1143, %fd4751, %fd4738;
	add.f64 	%fd1635, %fd4753, 0d0000000000000000;
	add.f64 	%fd1636, %fd4750, 0d0000000000000000;
	sub.f64 	%fd4754, %fd4736, %fd4750;
	fma.rn.f64 	%fd4755, %fd1138, %fd4754, 0d0000000000000000;
	fma.rn.f64 	%fd4756, %fd1144, %fd4754, %fd4745;
	fma.rn.f64 	%fd4757, %fd1143, %fd4755, %fd4752;
	fma.rn.f64 	%fd4758, %fd1143, %fd4755, %fd4757;
	div.rn.f64 	%fd4759, %fd4758, %fd1138;
	add.f64 	%fd1637, %fd4759, 0d0000000000000000;
	mul.f64 	%fd4760, %fd1143, %fd4758;
	div.rn.f64 	%fd4761, %fd4760, %fd1138;
	sub.f64 	%fd4762, %fd4756, %fd4761;
	add.f64 	%fd1638, %fd4762, 0d0000000000000000;
	add.f64 	%fd4763, %fd1634, %fd6710;
	add.f64 	%fd4764, %fd1635, %fd6711;
	add.f64 	%fd4765, %fd1636, %fd6706;
	add.f64 	%fd4766, %fd1637, %fd6708;
	add.f64 	%fd4767, %fd1638, %fd6709;
	add.f64 	%fd4768, %fd4763, 0d0000000000000000;
	fma.rn.f64 	%fd4769, %fd1139, %fd4768, 0d0000000000000000;
	fma.rn.f64 	%fd4770, %fd1140, %fd4768, 0d0000000000000000;
	fma.rn.f64 	%fd4771, %fd1141, %fd4768, 0d0000000000000000;
	fma.rn.f64 	%fd4772, %fd1135, %fd4768, 0d0000000000000000;
	fma.rn.f64 	%fd4773, %fd1136, %fd4768, 0d0000000000000000;
	fma.rn.f64 	%fd4774, %fd1137, %fd4768, 0d0000000000000000;
	add.f64 	%fd4775, %fd4764, 0d0000000000000000;
	fma.rn.f64 	%fd4776, %fd1139, %fd4775, 0d0000000000000000;
	fma.rn.f64 	%fd4777, %fd1140, %fd4775, 0d0000000000000000;
	fma.rn.f64 	%fd4778, %fd1141, %fd4775, 0d0000000000000000;
	fma.rn.f64 	%fd4779, %fd1132, %fd4775, %fd4772;
	fma.rn.f64 	%fd4780, %fd1127, %fd4775, %fd4773;
	fma.rn.f64 	%fd4781, %fd1129, %fd4775, %fd4774;
	add.f64 	%fd1639, %fd6654, %fd4779;
	add.f64 	%fd1640, %fd6653, %fd4780;
	add.f64 	%fd1641, %fd6652, %fd4781;
	sub.f64 	%fd4782, %fd6657, %fd4779;
	sub.f64 	%fd4783, %fd6656, %fd4780;
	sub.f64 	%fd4784, %fd6655, %fd4781;
	add.f64 	%fd4785, %fd4765, 0d0000000000000000;
	fma.rn.f64 	%fd4786, %fd1137, %fd4785, 0d0000000000000000;
	add.f64 	%fd4787, %fd4786, %fd4771;
	add.f64 	%fd4788, %fd4786, %fd4787;
	fma.rn.f64 	%fd4789, %fd1136, %fd4785, 0d0000000000000000;
	add.f64 	%fd4790, %fd4789, %fd4770;
	add.f64 	%fd4791, %fd4789, %fd4790;
	fma.rn.f64 	%fd4792, %fd1135, %fd4785, 0d0000000000000000;
	add.f64 	%fd4793, %fd4792, %fd4769;
	add.f64 	%fd4794, %fd4792, %fd4793;
	add.f64 	%fd4795, %fd4766, 0d0000000000000000;
	add.f64 	%fd4796, %fd6707, %fd4795;
	add.f64 	%fd4797, %fd4796, 0d0000000000000000;
	fma.rn.f64 	%fd4798, %fd1137, %fd4797, 0d0000000000000000;
	fma.rn.f64 	%fd4799, %fd1129, %fd4797, 0d0000000000000000;
	add.f64 	%fd4800, %fd4799, %fd4788;
	add.f64 	%fd4801, %fd4798, %fd4778;
	fma.rn.f64 	%fd4802, %fd1136, %fd4797, 0d0000000000000000;
	fma.rn.f64 	%fd4803, %fd1127, %fd4797, 0d0000000000000000;
	add.f64 	%fd4804, %fd4803, %fd4791;
	add.f64 	%fd4805, %fd4802, %fd4777;
	fma.rn.f64 	%fd4806, %fd1135, %fd4797, 0d0000000000000000;
	fma.rn.f64 	%fd4807, %fd1132, %fd4797, 0d0000000000000000;
	add.f64 	%fd4808, %fd4807, %fd4794;
	add.f64 	%fd4809, %fd4806, %fd4776;
	add.f64 	%fd4810, %fd4767, 0d0000000000000000;
	fma.rn.f64 	%fd4811, %fd1129, %fd4810, 0d0000000000000000;
	add.f64 	%fd4812, %fd4811, %fd4801;
	add.f64 	%fd4813, %fd4811, %fd4812;
	fma.rn.f64 	%fd4814, %fd1127, %fd4810, 0d0000000000000000;
	add.f64 	%fd4815, %fd4814, %fd4805;
	add.f64 	%fd4816, %fd4814, %fd4815;
	fma.rn.f64 	%fd4817, %fd1132, %fd4810, 0d0000000000000000;
	add.f64 	%fd4818, %fd4817, %fd4809;
	add.f64 	%fd4819, %fd4817, %fd4818;
	mul.f64 	%fd4820, %fd1133, %fd4800;
	mul.f64 	%fd4821, %fd1134, %fd4804;
	sub.f64 	%fd4822, %fd4820, %fd4821;
	mul.f64 	%fd4823, %fd1134, %fd4808;
	mul.f64 	%fd4824, %fd1130, %fd4800;
	sub.f64 	%fd4825, %fd4823, %fd4824;
	mul.f64 	%fd4826, %fd1130, %fd4804;
	mul.f64 	%fd4827, %fd1133, %fd4808;
	sub.f64 	%fd4828, %fd4826, %fd4827;
	add.f64 	%fd4829, %fd4819, %fd4822;
	add.f64 	%fd4830, %fd4816, %fd4825;
	add.f64 	%fd4831, %fd4813, %fd4828;
	mul.f64 	%fd4832, %fd1127, %fd4800;
	mul.f64 	%fd4833, %fd1129, %fd4804;
	sub.f64 	%fd4834, %fd4832, %fd4833;
	mul.f64 	%fd4835, %fd1129, %fd4808;
	mul.f64 	%fd4836, %fd1132, %fd4800;
	sub.f64 	%fd4837, %fd4835, %fd4836;
	mul.f64 	%fd4838, %fd1132, %fd4804;
	mul.f64 	%fd4839, %fd1127, %fd4808;
	sub.f64 	%fd4840, %fd4838, %fd4839;
	sub.f64 	%fd4841, %fd6712, %fd4834;
	sub.f64 	%fd4842, %fd6713, %fd4837;
	sub.f64 	%fd4843, %fd6714, %fd4840;
	mul.f64 	%fd4844, %fd1128, %fd4843;
	mul.f64 	%fd4845, %fd1126, %fd4842;
	sub.f64 	%fd4846, %fd4844, %fd4845;
	mul.f64 	%fd4847, %fd1126, %fd4841;
	mul.f64 	%fd4848, %fd1131, %fd4843;
	sub.f64 	%fd4849, %fd4847, %fd4848;
	mul.f64 	%fd4850, %fd1131, %fd4842;
	mul.f64 	%fd4851, %fd1128, %fd4841;
	sub.f64 	%fd4852, %fd4850, %fd4851;
	add.f64 	%fd4853, %fd4829, %fd4846;
	add.f64 	%fd4854, %fd4830, %fd4849;
	add.f64 	%fd4855, %fd4831, %fd4852;
	mul.f64 	%fd4856, %fd1127, %fd4843;
	mul.f64 	%fd4857, %fd1129, %fd4842;
	mul.f64 	%fd4858, %fd1129, %fd4841;
	mul.f64 	%fd4859, %fd1132, %fd4843;
	mul.f64 	%fd4860, %fd1132, %fd4842;
	mul.f64 	%fd4861, %fd1127, %fd4841;
	sub.f64 	%fd4862, %fd4857, %fd4856;
	add.f64 	%fd4863, %fd4862, 0d0000000000000000;
	sub.f64 	%fd4864, %fd4859, %fd4858;
	add.f64 	%fd4865, %fd4864, 0d0000000000000000;
	sub.f64 	%fd4866, %fd4861, %fd4860;
	add.f64 	%fd4867, %fd4866, 0d0000000000000000;
	add.f64 	%fd6739, %fd6663, %fd4863;
	add.f64 	%fd6738, %fd6662, %fd4865;
	add.f64 	%fd6737, %fd6661, %fd4867;
	sub.f64 	%fd4868, %fd4782, %fd4863;
	sub.f64 	%fd4869, %fd4783, %fd4865;
	sub.f64 	%fd4870, %fd4784, %fd4867;
	add.f64 	%fd6736, %fd6660, %fd4853;
	add.f64 	%fd6735, %fd6659, %fd4854;
	add.f64 	%fd6734, %fd6658, %fd4855;
	sub.f64 	%fd6733, %fd4868, %fd4853;
	sub.f64 	%fd6732, %fd4869, %fd4854;
	sub.f64 	%fd6731, %fd4870, %fd4855;
	and.b16  	%rs333, %rs339, 255;
	setp.eq.s16 	%p290, %rs333, 0;
	mov.f64 	%fd6726, %fd4736;
	mov.f64 	%fd6727, %fd4736;
	@%p290 bra 	$L__BB7_345;

	setp.eq.s64 	%p291, %rd119, 0;
	@%p291 bra 	$L__BB7_328;

	cvta.to.global.u64 	%rd337, %rd119;
	mul.lo.s64 	%rd338, %rd75, %rd56;
	add.s64 	%rd339, %rd337, %rd338;
	ld.global.f64 	%fd4871, [%rd339];
	add.f64 	%fd6715, %fd4871, 0d0000000000000000;
	bra.uni 	$L__BB7_330;

$L__BB7_328:
	setp.eq.s64 	%p292, %rd88, 0;
	mov.f64 	%fd6715, 0d0000000000000000;
	@%p292 bra 	$L__BB7_330;

	cvta.to.global.u64 	%rd340, %rd88;
	mul.lo.s64 	%rd341, %rd75, %rd57;
	add.s64 	%rd342, %rd340, %rd341;
	ld.global.f64 	%fd4873, [%rd342];
	add.f64 	%fd6715, %fd4873, 0d0000000000000000;

$L__BB7_330:
	mov.f64 	%fd6727, 0d0000000000000000;
	sub.f64 	%fd4876, %fd6727, %fd6715;
	fma.rn.f64 	%fd4877, %fd5824, %fd4876, 0d0000000000000000;
	fma.rn.f64 	%fd4878, %fd5825, %fd4876, 0d0000000000000000;
	rcp.rn.f64 	%fd4879, %fd5824;
	mul.f64 	%fd4880, %fd4879, 0d3FE0000000000000;
	fma.rn.f64 	%fd6726, %fd4880, %fd4878, 0d0000000000000000;
	fma.rn.f64 	%fd4881, %fd4877, 0d4000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd1655, %fd5826, %fd4881, 0d0000000000000000;
	fma.rn.f64 	%fd1656, %fd5827, %fd4881, 0d0000000000000000;
	setp.geu.f64 	%p293, %fd5833, %fd5832;
	@%p293 bra 	$L__BB7_339;

	sub.f64 	%fd4882, %fd5833, %fd5832;
	div.rn.f64 	%fd6719, %fd4882, %fd5832;
	div.rn.f64 	%fd6720, %fd5833, %fd5832;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r721}, %fd6720;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r722, %temp}, %fd6720;
	}
	setp.gt.s32 	%p294, %r721, 1048575;
	mov.u32 	%r723, -1023;
	mov.f64 	%fd6716, %fd6720;
	@%p294 bra 	$L__BB7_333;

	mul.f64 	%fd6716, %fd6720, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r721}, %fd6716;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r722, %temp}, %fd6716;
	}
	mov.u32 	%r723, -1077;

$L__BB7_333:
	add.s32 	%r648, %r721, -1;
	setp.lt.u32 	%p295, %r648, 2146435071;
	@%p295 bra 	$L__BB7_335;
	bra.uni 	$L__BB7_334;

$L__BB7_335:
	shr.u32 	%r650, %r721, 20;
	add.s32 	%r724, %r723, %r650;
	and.b32  	%r651, %r721, -2146435073;
	or.b32  	%r652, %r651, 1072693248;
	mov.b64 	%fd6717, {%r722, %r652};
	setp.lt.s32 	%p297, %r652, 1073127583;
	@%p297 bra 	$L__BB7_337;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r653, %temp}, %fd6717;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r654}, %fd6717;
	}
	add.s32 	%r655, %r654, -1048576;
	mov.b64 	%fd6717, {%r653, %r655};
	add.s32 	%r724, %r724, 1;

$L__BB7_337:
	add.f64 	%fd4885, %fd6717, 0d3FF0000000000000;
	mov.f64 	%fd4886, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd4887, %fd4885;
	neg.f64 	%fd4888, %fd4885;
	fma.rn.f64 	%fd4889, %fd4888, %fd4887, %fd4886;
	fma.rn.f64 	%fd4890, %fd4889, %fd4889, %fd4889;
	fma.rn.f64 	%fd4891, %fd4890, %fd4887, %fd4887;
	add.f64 	%fd4892, %fd6717, 0dBFF0000000000000;
	mul.f64 	%fd4893, %fd4892, %fd4891;
	fma.rn.f64 	%fd4894, %fd4892, %fd4891, %fd4893;
	mul.f64 	%fd4895, %fd4894, %fd4894;
	mov.f64 	%fd4896, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd4897, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd4898, %fd4897, %fd4895, %fd4896;
	mov.f64 	%fd4899, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd4900, %fd4898, %fd4895, %fd4899;
	mov.f64 	%fd4901, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd4902, %fd4900, %fd4895, %fd4901;
	mov.f64 	%fd4903, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd4904, %fd4902, %fd4895, %fd4903;
	mov.f64 	%fd4905, 0d3F624924923BE72D;
	fma.rn.f64 	%fd4906, %fd4904, %fd4895, %fd4905;
	mov.f64 	%fd4907, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd4908, %fd4906, %fd4895, %fd4907;
	mov.f64 	%fd4909, 0d3FB5555555555554;
	fma.rn.f64 	%fd4910, %fd4908, %fd4895, %fd4909;
	sub.f64 	%fd4911, %fd4892, %fd4894;
	add.f64 	%fd4912, %fd4911, %fd4911;
	neg.f64 	%fd4913, %fd4894;
	fma.rn.f64 	%fd4914, %fd4913, %fd4892, %fd4912;
	mul.f64 	%fd4915, %fd4891, %fd4914;
	mul.f64 	%fd4916, %fd4895, %fd4910;
	fma.rn.f64 	%fd4917, %fd4916, %fd4894, %fd4915;
	xor.b32  	%r656, %r724, -2147483648;
	mov.u32 	%r657, -2147483648;
	mov.u32 	%r658, 1127219200;
	mov.b64 	%fd4918, {%r656, %r658};
	mov.b64 	%fd4919, {%r657, %r658};
	sub.f64 	%fd4920, %fd4918, %fd4919;
	mov.f64 	%fd4921, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd4922, %fd4920, %fd4921, %fd4894;
	neg.f64 	%fd4923, %fd4920;
	fma.rn.f64 	%fd4924, %fd4923, %fd4921, %fd4922;
	sub.f64 	%fd4925, %fd4924, %fd4894;
	sub.f64 	%fd4926, %fd4917, %fd4925;
	mov.f64 	%fd4927, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd4928, %fd4920, %fd4927, %fd4926;
	add.f64 	%fd6721, %fd4922, %fd4928;
	bra.uni 	$L__BB7_338;

$L__BB7_334:
	mov.f64 	%fd4883, 0d7FF0000000000000;
	fma.rn.f64 	%fd4884, %fd6716, %fd4883, %fd4883;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r649}, %fd6716;
	}
	mov.b32 	%f4, %r649;
	setp.eq.f32 	%p296, %f4, 0f00000000;
	selp.f64 	%fd6721, 0dFFF0000000000000, %fd4884, %p296;

$L__BB7_338:
	mul.f64 	%fd4929, %fd6719, %fd6721;
	mul.f64 	%fd4930, %fd4929, 0dC000000000000000;
	div.rn.f64 	%fd6722, %fd4930, %fd5832;
	mul.f64 	%fd4931, %fd6719, %fd6719;
	div.rn.f64 	%fd6723, %fd4931, %fd5833;

$L__BB7_339:
	setp.lt.f64 	%p299, %fd5833, %fd5832;
	selp.f64 	%fd1674, %fd1656, 0d0000000000000000, %p299;
	@%p293 bra 	$L__BB7_341;

	fma.rn.f64 	%fd4933, %fd1674, %fd1868, 0d0000000000000000;
	mov.f64 	%fd4934, 0d0000000000000000;
	sub.f64 	%fd4935, %fd4934, %fd4933;
	div.rn.f64 	%fd4936, %fd4935, %fd5833;
	add.f64 	%fd4937, %fd4936, 0d0000000000000000;
	mul.f64 	%fd4938, %fd4935, %fd6723;
	div.rn.f64 	%fd4939, %fd4938, %fd5833;
	sub.f64 	%fd4940, %fd6726, %fd4939;
	fma.rn.f64 	%fd4941, %fd4937, %fd6719, 0d0000000000000000;
	div.rn.f64 	%fd4942, %fd4941, %fd5832;
	add.f64 	%fd4943, %fd4942, 0d0000000000000000;
	mul.f64 	%fd4944, %fd6719, %fd4941;
	div.rn.f64 	%fd4945, %fd4944, %fd5832;
	sub.f64 	%fd4946, %fd4934, %fd4945;
	add.f64 	%fd4947, %fd4942, %fd4943;
	sub.f64 	%fd4948, %fd4946, %fd4945;
	div.rn.f64 	%fd4949, %fd4933, %fd5832;
	add.f64 	%fd4950, %fd4949, 0d0000000000000000;
	mul.f64 	%fd4951, %fd4933, %fd6722;
	div.rn.f64 	%fd4952, %fd4951, %fd5832;
	sub.f64 	%fd4953, %fd4948, %fd4952;
	add.f64 	%fd4954, %fd4950, %fd4950;
	sub.f64 	%fd4955, %fd4934, %fd4954;
	fma.rn.f64 	%fd4956, %fd4955, %fd6721, 0d0000000000000000;
	fma.rn.f64 	%fd4957, %fd4955, %fd6719, 0d0000000000000000;
	rcp.rn.f64 	%fd4958, %fd6720;
	fma.rn.f64 	%fd4959, %fd4958, %fd4957, 0d0000000000000000;
	div.rn.f64 	%fd4960, %fd4959, %fd5832;
	add.f64 	%fd4961, %fd4960, %fd4940;
	mul.f64 	%fd4962, %fd6720, %fd4959;
	div.rn.f64 	%fd4963, %fd4962, %fd5832;
	sub.f64 	%fd4964, %fd4953, %fd4963;
	div.rn.f64 	%fd4965, %fd4956, %fd5832;
	add.f64 	%fd4966, %fd4947, %fd4965;
	mul.f64 	%fd4967, %fd6719, %fd4956;
	div.rn.f64 	%fd4968, %fd4967, %fd5832;
	sub.f64 	%fd4969, %fd4964, %fd4968;
	add.f64 	%fd6726, %fd4966, %fd4961;
	sub.f64 	%fd6727, %fd4969, %fd4966;

$L__BB7_341:
	fma.rn.f64 	%fd4970, %fd1655, %fd1867, 0d0000000000000000;
	fma.rn.f64 	%fd1679, %fd4970, %fd1869, 0d0000000000000000;
	setp.eq.s64 	%p300, %rd135, 0;
	@%p300 bra 	$L__BB7_343;

	cvt.s64.s32 	%rd344, %r682;
	mul.lo.s64 	%rd345, %rd344, %rd59;
	add.s64 	%rd343, %rd135, %rd345;
	// begin inline asm
	{ atom.add.f64 %fd4971,[%rd343],%fd1679; }

	// end inline asm
	bra.uni 	$L__BB7_345;

$L__BB7_343:
	setp.eq.s64 	%p301, %rd112, 0;
	@%p301 bra 	$L__BB7_345;

	cvt.s64.s32 	%rd347, %r682;
	mul.lo.s64 	%rd348, %rd347, %rd49;
	add.s64 	%rd346, %rd112, %rd348;
	// begin inline asm
	{ atom.add.f64 %fd4973,[%rd346],%fd1679; }

	// end inline asm

$L__BB7_345:
	add.f64 	%fd4975, %fd6727, 0d0000000000000000;
	fma.rn.f64 	%fd4977, %fd5854, %fd4975, 0d0000000000000000;
	add.f64 	%fd1682, %fd6726, 0d0000000000000000;
	sub.f64 	%fd4978, %fd4736, %fd6726;
	fma.rn.f64 	%fd4979, %fd5834, %fd4978, %fd4977;
	fma.rn.f64 	%fd1683, %fd5834, %fd4978, %fd4979;
	mov.u32 	%r725, 3;
	@%p16 bra 	$L__BB7_351;

	sub.f64 	%fd4980, %fd5851, %fd5848;
	sub.f64 	%fd4981, %fd5850, %fd5847;
	mul.f64 	%fd4982, %fd4981, %fd1134;
	sub.f64 	%fd4983, %fd5849, %fd5846;
	mul.f64 	%fd4984, %fd4983, %fd1133;
	sub.f64 	%fd4985, %fd4982, %fd4984;
	mul.f64 	%fd4986, %fd4983, %fd1130;
	mul.f64 	%fd4987, %fd4980, %fd1134;
	sub.f64 	%fd4988, %fd4986, %fd4987;
	mul.f64 	%fd4989, %fd4980, %fd1133;
	mul.f64 	%fd4990, %fd4981, %fd1130;
	sub.f64 	%fd4991, %fd4989, %fd4990;
	mul.f64 	%fd4992, %fd4980, %fd4980;
	fma.rn.f64 	%fd4993, %fd4981, %fd4981, %fd4992;
	fma.rn.f64 	%fd4994, %fd4983, %fd4983, %fd4993;
	mul.f64 	%fd4995, %fd4981, %fd4988;
	fma.rn.f64 	%fd4996, %fd4980, %fd4985, %fd4995;
	fma.rn.f64 	%fd4997, %fd4983, %fd4991, %fd4996;
	mul.f64 	%fd4998, %fd4988, %fd4988;
	fma.rn.f64 	%fd4999, %fd4985, %fd4985, %fd4998;
	fma.rn.f64 	%fd5000, %fd4991, %fd4991, %fd4999;
	sub.f64 	%fd5001, %fd5842, %fd5848;
	mul.f64 	%fd5002, %fd5001, %fd4980;
	sub.f64 	%fd5003, %fd5841, %fd5847;
	fma.rn.f64 	%fd5004, %fd5003, %fd4981, %fd5002;
	sub.f64 	%fd5005, %fd5840, %fd5846;
	fma.rn.f64 	%fd5006, %fd5005, %fd4983, %fd5004;
	mul.f64 	%fd5007, %fd5003, %fd4988;
	fma.rn.f64 	%fd5008, %fd5001, %fd4985, %fd5007;
	fma.rn.f64 	%fd5009, %fd5005, %fd4991, %fd5008;
	div.rn.f64 	%fd5010, %fd4997, %fd4994;
	mul.f64 	%fd5011, %fd5010, %fd5010;
	mul.f64 	%fd5012, %fd4994, %fd5011;
	sub.f64 	%fd5013, %fd5000, %fd5012;
	mul.f64 	%fd5014, %fd5006, %fd5010;
	sub.f64 	%fd5015, %fd5009, %fd5014;
	div.rn.f64 	%fd5016, %fd5015, %fd5013;
	mul.f64 	%fd5017, %fd4994, %fd5010;
	mul.f64 	%fd5018, %fd5017, %fd5016;
	sub.f64 	%fd5019, %fd5006, %fd5018;
	div.rn.f64 	%fd1684, %fd5019, %fd4994;
	setp.gt.f64 	%p302, %fd1684, 0d0000000000000000;
	setp.lt.f64 	%p303, %fd1684, 0d3FF0000000000000;
	setp.ge.f64 	%p304, %fd5016, 0d0000000000000000;
	and.pred  	%p305, %p302, %p303;
	and.pred  	%p306, %p304, %p305;
	mov.u32 	%r725, 4;
	@%p306 bra 	$L__BB7_351;

	sub.f64 	%fd5020, %fd5845, %fd5851;
	sub.f64 	%fd5021, %fd5844, %fd5850;
	mul.f64 	%fd5022, %fd5021, %fd1134;
	sub.f64 	%fd5023, %fd5843, %fd5849;
	mul.f64 	%fd5024, %fd5023, %fd1133;
	sub.f64 	%fd5025, %fd5022, %fd5024;
	mul.f64 	%fd5026, %fd5023, %fd1130;
	mul.f64 	%fd5027, %fd5020, %fd1134;
	sub.f64 	%fd5028, %fd5026, %fd5027;
	mul.f64 	%fd5029, %fd5020, %fd1133;
	mul.f64 	%fd5030, %fd5021, %fd1130;
	sub.f64 	%fd5031, %fd5029, %fd5030;
	mul.f64 	%fd5032, %fd5020, %fd5020;
	fma.rn.f64 	%fd5033, %fd5021, %fd5021, %fd5032;
	fma.rn.f64 	%fd5034, %fd5023, %fd5023, %fd5033;
	mul.f64 	%fd5035, %fd5021, %fd5028;
	fma.rn.f64 	%fd5036, %fd5020, %fd5025, %fd5035;
	fma.rn.f64 	%fd5037, %fd5023, %fd5031, %fd5036;
	mul.f64 	%fd5038, %fd5028, %fd5028;
	fma.rn.f64 	%fd5039, %fd5025, %fd5025, %fd5038;
	fma.rn.f64 	%fd5040, %fd5031, %fd5031, %fd5039;
	sub.f64 	%fd5041, %fd5842, %fd5851;
	mul.f64 	%fd5042, %fd5020, %fd5041;
	sub.f64 	%fd5043, %fd5841, %fd5850;
	fma.rn.f64 	%fd5044, %fd5021, %fd5043, %fd5042;
	sub.f64 	%fd5045, %fd5840, %fd5849;
	fma.rn.f64 	%fd5046, %fd5023, %fd5045, %fd5044;
	mul.f64 	%fd5047, %fd5043, %fd5028;
	fma.rn.f64 	%fd5048, %fd5041, %fd5025, %fd5047;
	fma.rn.f64 	%fd5049, %fd5045, %fd5031, %fd5048;
	div.rn.f64 	%fd5050, %fd5037, %fd5034;
	mul.f64 	%fd5051, %fd5050, %fd5050;
	mul.f64 	%fd5052, %fd5034, %fd5051;
	sub.f64 	%fd5053, %fd5040, %fd5052;
	mul.f64 	%fd5054, %fd5046, %fd5050;
	sub.f64 	%fd5055, %fd5049, %fd5054;
	div.rn.f64 	%fd5056, %fd5055, %fd5053;
	mul.f64 	%fd5057, %fd5034, %fd5050;
	mul.f64 	%fd5058, %fd5057, %fd5056;
	sub.f64 	%fd5059, %fd5046, %fd5058;
	div.rn.f64 	%fd1685, %fd5059, %fd5034;
	setp.gt.f64 	%p307, %fd1685, 0d0000000000000000;
	setp.lt.f64 	%p308, %fd1685, 0d3FF0000000000000;
	setp.ge.f64 	%p309, %fd5056, 0d0000000000000000;
	and.pred  	%p310, %p307, %p308;
	and.pred  	%p311, %p309, %p310;
	mov.u32 	%r725, 5;
	@%p311 bra 	$L__BB7_351;

	setp.le.f64 	%p312, %fd1148, 0d0000000000000000;
	setp.ge.f64 	%p313, %fd1685, 0d3FF0000000000000;
	and.pred  	%p314, %p312, %p313;
	mov.u32 	%r725, 0;
	@%p314 bra 	$L__BB7_351;

	setp.le.f64 	%p315, %fd1684, 0d0000000000000000;
	setp.ge.f64 	%p316, %fd1148, 0d3FF0000000000000;
	and.pred  	%p317, %p315, %p316;
	mov.u32 	%r725, 1;
	@%p317 bra 	$L__BB7_351;

	setp.le.f64 	%p318, %fd1685, 0d0000000000000000;
	setp.ge.f64 	%p319, %fd1684, 0d3FF0000000000000;
	and.pred  	%p320, %p318, %p319;
	selp.b32 	%r725, 2, 6, %p320;

$L__BB7_351:
	setp.eq.s32 	%p321, %r725, 0;
	@%p321 bra 	$L__BB7_363;

	setp.eq.s32 	%p322, %r725, 1;
	@%p322 bra 	$L__BB7_362;
	bra.uni 	$L__BB7_353;

$L__BB7_362:
	sub.f64 	%fd5349, %fd5842, %fd5848;
	add.f64 	%fd5350, %fd5349, %fd5349;
	sub.f64 	%fd5351, %fd5841, %fd5847;
	add.f64 	%fd5352, %fd5351, %fd5351;
	sub.f64 	%fd5353, %fd5840, %fd5846;
	add.f64 	%fd5354, %fd5353, %fd5353;
	fma.rn.f64 	%fd5355, %fd5350, %fd1682, 0d0000000000000000;
	fma.rn.f64 	%fd5356, %fd5352, %fd1682, 0d0000000000000000;
	fma.rn.f64 	%fd5357, %fd5354, %fd1682, 0d0000000000000000;
	add.f64 	%fd6730, %fd1639, %fd5355;
	add.f64 	%fd6729, %fd1640, %fd5356;
	add.f64 	%fd6728, %fd1641, %fd5357;
	sub.f64 	%fd6736, %fd6736, %fd5355;
	sub.f64 	%fd6735, %fd6735, %fd5356;
	sub.f64 	%fd6734, %fd6734, %fd5357;
	bra.uni 	$L__BB7_364;

$L__BB7_363:
	add.f64 	%fd5358, %fd1139, %fd1139;
	add.f64 	%fd5359, %fd1140, %fd1140;
	add.f64 	%fd5360, %fd1141, %fd1141;
	fma.rn.f64 	%fd5361, %fd5358, %fd1682, 0d0000000000000000;
	fma.rn.f64 	%fd5362, %fd5359, %fd1682, 0d0000000000000000;
	fma.rn.f64 	%fd5363, %fd5360, %fd1682, 0d0000000000000000;
	add.f64 	%fd6730, %fd1639, %fd5361;
	add.f64 	%fd6729, %fd1640, %fd5362;
	add.f64 	%fd6728, %fd1641, %fd5363;
	sub.f64 	%fd6733, %fd6733, %fd5361;
	sub.f64 	%fd6732, %fd6732, %fd5362;
	sub.f64 	%fd6731, %fd6731, %fd5363;
	bra.uni 	$L__BB7_364;

$L__BB7_353:
	setp.eq.s32 	%p323, %r725, 2;
	@%p323 bra 	$L__BB7_361;
	bra.uni 	$L__BB7_354;

$L__BB7_361:
	sub.f64 	%fd5340, %fd5842, %fd5851;
	add.f64 	%fd5341, %fd5340, %fd5340;
	sub.f64 	%fd5342, %fd5841, %fd5850;
	add.f64 	%fd5343, %fd5342, %fd5342;
	sub.f64 	%fd5344, %fd5840, %fd5849;
	add.f64 	%fd5345, %fd5344, %fd5344;
	fma.rn.f64 	%fd5346, %fd5341, %fd1682, 0d0000000000000000;
	fma.rn.f64 	%fd5347, %fd5343, %fd1682, 0d0000000000000000;
	fma.rn.f64 	%fd5348, %fd5345, %fd1682, 0d0000000000000000;
	add.f64 	%fd6730, %fd1639, %fd5346;
	add.f64 	%fd6729, %fd1640, %fd5347;
	add.f64 	%fd6728, %fd1641, %fd5348;
	sub.f64 	%fd6739, %fd6739, %fd5346;
	sub.f64 	%fd6738, %fd6738, %fd5347;
	sub.f64 	%fd6737, %fd6737, %fd5348;
	bra.uni 	$L__BB7_364;

$L__BB7_354:
	setp.eq.s32 	%p324, %r725, 3;
	@%p324 bra 	$L__BB7_360;
	bra.uni 	$L__BB7_355;

$L__BB7_360:
	sub.f64 	%fd5270, %fd5845, %fd5842;
	sub.f64 	%fd5271, %fd5846, %fd5840;
	sub.f64 	%fd5272, %fd5844, %fd5841;
	mul.f64 	%fd5273, %fd5272, %fd5271;
	sub.f64 	%fd5274, %fd5847, %fd5841;
	sub.f64 	%fd5275, %fd5843, %fd5840;
	mul.f64 	%fd5276, %fd5275, %fd5274;
	sub.f64 	%fd5277, %fd5273, %fd5276;
	sub.f64 	%fd5278, %fd5848, %fd5842;
	mul.f64 	%fd5279, %fd5275, %fd5278;
	mul.f64 	%fd5280, %fd5270, %fd5271;
	sub.f64 	%fd5281, %fd5279, %fd5280;
	mul.f64 	%fd5282, %fd5270, %fd5274;
	mul.f64 	%fd5283, %fd5272, %fd5278;
	sub.f64 	%fd5284, %fd5282, %fd5283;
	mul.f64 	%fd5285, %fd5281, %fd5281;
	fma.rn.f64 	%fd5286, %fd5277, %fd5277, %fd5285;
	fma.rn.f64 	%fd5287, %fd5284, %fd5284, %fd5286;
	div.rn.f64 	%fd5288, %fd5287, %fd1138;
	div.rn.f64 	%fd5289, %fd1682, %fd1138;
	add.f64 	%fd5290, %fd5289, 0d0000000000000000;
	mov.f64 	%fd5291, 0d0000000000000000;
	mul.f64 	%fd5292, %fd5288, %fd1682;
	div.rn.f64 	%fd5293, %fd5292, %fd1138;
	sub.f64 	%fd5294, %fd5291, %fd5293;
	add.f64 	%fd5295, %fd1132, %fd1132;
	add.f64 	%fd5296, %fd1127, %fd1127;
	add.f64 	%fd5297, %fd1129, %fd1129;
	fma.rn.f64 	%fd5298, %fd5295, %fd5294, 0d0000000000000000;
	fma.rn.f64 	%fd5299, %fd5296, %fd5294, 0d0000000000000000;
	fma.rn.f64 	%fd5300, %fd5297, %fd5294, 0d0000000000000000;
	add.f64 	%fd5301, %fd5277, %fd5277;
	add.f64 	%fd5302, %fd5281, %fd5281;
	add.f64 	%fd5303, %fd5284, %fd5284;
	fma.rn.f64 	%fd5304, %fd5301, %fd5290, 0d0000000000000000;
	fma.rn.f64 	%fd5305, %fd5302, %fd5290, 0d0000000000000000;
	fma.rn.f64 	%fd5306, %fd5303, %fd5290, 0d0000000000000000;
	mul.f64 	%fd5307, %fd5274, %fd5306;
	mul.f64 	%fd5308, %fd5271, %fd5305;
	sub.f64 	%fd5309, %fd5307, %fd5308;
	mul.f64 	%fd5310, %fd5271, %fd5304;
	mul.f64 	%fd5311, %fd5278, %fd5306;
	sub.f64 	%fd5312, %fd5310, %fd5311;
	mul.f64 	%fd5313, %fd5278, %fd5305;
	mul.f64 	%fd5314, %fd5274, %fd5304;
	sub.f64 	%fd5315, %fd5313, %fd5314;
	add.f64 	%fd5316, %fd5309, 0d0000000000000000;
	add.f64 	%fd5317, %fd5312, 0d0000000000000000;
	add.f64 	%fd5318, %fd5315, 0d0000000000000000;
	mul.f64 	%fd5319, %fd5272, %fd5306;
	mul.f64 	%fd5320, %fd5275, %fd5305;
	mul.f64 	%fd5321, %fd5275, %fd5304;
	mul.f64 	%fd5322, %fd5270, %fd5306;
	mul.f64 	%fd5323, %fd5270, %fd5305;
	mul.f64 	%fd5324, %fd5272, %fd5304;
	sub.f64 	%fd5325, %fd5320, %fd5319;
	add.f64 	%fd5326, %fd5325, 0d0000000000000000;
	sub.f64 	%fd5327, %fd5322, %fd5321;
	add.f64 	%fd5328, %fd5327, 0d0000000000000000;
	sub.f64 	%fd5329, %fd5324, %fd5323;
	add.f64 	%fd5330, %fd5329, 0d0000000000000000;
	add.f64 	%fd5331, %fd6736, %fd5298;
	add.f64 	%fd5332, %fd6735, %fd5299;
	add.f64 	%fd5333, %fd6734, %fd5300;
	sub.f64 	%fd5334, %fd6733, %fd5298;
	sub.f64 	%fd5335, %fd6732, %fd5299;
	sub.f64 	%fd5336, %fd6731, %fd5300;
	add.f64 	%fd6736, %fd5331, %fd5326;
	add.f64 	%fd6735, %fd5332, %fd5328;
	add.f64 	%fd6734, %fd5333, %fd5330;
	sub.f64 	%fd5337, %fd1639, %fd5326;
	sub.f64 	%fd5338, %fd1640, %fd5328;
	sub.f64 	%fd5339, %fd1641, %fd5330;
	add.f64 	%fd6733, %fd5334, %fd5316;
	add.f64 	%fd6732, %fd5335, %fd5317;
	add.f64 	%fd6731, %fd5336, %fd5318;
	sub.f64 	%fd6730, %fd5337, %fd5316;
	sub.f64 	%fd6729, %fd5338, %fd5317;
	sub.f64 	%fd6728, %fd5339, %fd5318;
	bra.uni 	$L__BB7_364;

$L__BB7_355:
	setp.eq.s32 	%p325, %r725, 4;
	@%p325 bra 	$L__BB7_359;
	bra.uni 	$L__BB7_356;

$L__BB7_359:
	sub.f64 	%fd5194, %fd5848, %fd5842;
	sub.f64 	%fd5195, %fd5849, %fd5840;
	sub.f64 	%fd5196, %fd5847, %fd5841;
	mul.f64 	%fd5197, %fd5196, %fd5195;
	sub.f64 	%fd5198, %fd5850, %fd5841;
	sub.f64 	%fd5199, %fd5846, %fd5840;
	mul.f64 	%fd5200, %fd5199, %fd5198;
	sub.f64 	%fd5201, %fd5197, %fd5200;
	sub.f64 	%fd5202, %fd5851, %fd5842;
	mul.f64 	%fd5203, %fd5199, %fd5202;
	mul.f64 	%fd5204, %fd5194, %fd5195;
	sub.f64 	%fd5205, %fd5203, %fd5204;
	mul.f64 	%fd5206, %fd5194, %fd5198;
	mul.f64 	%fd5207, %fd5196, %fd5202;
	sub.f64 	%fd5208, %fd5206, %fd5207;
	mul.f64 	%fd5209, %fd5205, %fd5205;
	fma.rn.f64 	%fd5210, %fd5201, %fd5201, %fd5209;
	fma.rn.f64 	%fd5211, %fd5208, %fd5208, %fd5210;
	sub.f64 	%fd5212, %fd5851, %fd5848;
	mul.f64 	%fd5213, %fd5212, %fd5212;
	sub.f64 	%fd5214, %fd5850, %fd5847;
	fma.rn.f64 	%fd5215, %fd5214, %fd5214, %fd5213;
	sub.f64 	%fd5216, %fd5849, %fd5846;
	fma.rn.f64 	%fd5217, %fd5216, %fd5216, %fd5215;
	div.rn.f64 	%fd5218, %fd5211, %fd5217;
	div.rn.f64 	%fd5219, %fd1682, %fd5217;
	add.f64 	%fd5220, %fd5219, 0d0000000000000000;
	mov.f64 	%fd5221, 0d0000000000000000;
	mul.f64 	%fd5222, %fd5218, %fd1682;
	div.rn.f64 	%fd5223, %fd5222, %fd5217;
	sub.f64 	%fd5224, %fd5221, %fd5223;
	add.f64 	%fd5225, %fd5212, %fd5212;
	add.f64 	%fd5226, %fd5214, %fd5214;
	add.f64 	%fd5227, %fd5216, %fd5216;
	fma.rn.f64 	%fd5228, %fd5225, %fd5224, 0d0000000000000000;
	fma.rn.f64 	%fd5229, %fd5226, %fd5224, 0d0000000000000000;
	fma.rn.f64 	%fd5230, %fd5227, %fd5224, 0d0000000000000000;
	add.f64 	%fd5231, %fd5201, %fd5201;
	add.f64 	%fd5232, %fd5205, %fd5205;
	add.f64 	%fd5233, %fd5208, %fd5208;
	fma.rn.f64 	%fd5234, %fd5231, %fd5220, 0d0000000000000000;
	fma.rn.f64 	%fd5235, %fd5232, %fd5220, 0d0000000000000000;
	fma.rn.f64 	%fd5236, %fd5233, %fd5220, 0d0000000000000000;
	mul.f64 	%fd5237, %fd5198, %fd5236;
	mul.f64 	%fd5238, %fd5195, %fd5235;
	sub.f64 	%fd5239, %fd5237, %fd5238;
	mul.f64 	%fd5240, %fd5195, %fd5234;
	mul.f64 	%fd5241, %fd5202, %fd5236;
	sub.f64 	%fd5242, %fd5240, %fd5241;
	mul.f64 	%fd5243, %fd5202, %fd5235;
	mul.f64 	%fd5244, %fd5198, %fd5234;
	sub.f64 	%fd5245, %fd5243, %fd5244;
	add.f64 	%fd5246, %fd5239, 0d0000000000000000;
	add.f64 	%fd5247, %fd5242, 0d0000000000000000;
	add.f64 	%fd5248, %fd5245, 0d0000000000000000;
	mul.f64 	%fd5249, %fd5196, %fd5236;
	mul.f64 	%fd5250, %fd5199, %fd5235;
	mul.f64 	%fd5251, %fd5199, %fd5234;
	mul.f64 	%fd5252, %fd5194, %fd5236;
	mul.f64 	%fd5253, %fd5194, %fd5235;
	mul.f64 	%fd5254, %fd5196, %fd5234;
	sub.f64 	%fd5255, %fd5250, %fd5249;
	add.f64 	%fd5256, %fd5255, 0d0000000000000000;
	sub.f64 	%fd5257, %fd5252, %fd5251;
	add.f64 	%fd5258, %fd5257, 0d0000000000000000;
	sub.f64 	%fd5259, %fd5254, %fd5253;
	add.f64 	%fd5260, %fd5259, 0d0000000000000000;
	add.f64 	%fd5261, %fd6739, %fd5228;
	add.f64 	%fd5262, %fd6738, %fd5229;
	add.f64 	%fd5263, %fd6737, %fd5230;
	sub.f64 	%fd5264, %fd6736, %fd5228;
	sub.f64 	%fd5265, %fd6735, %fd5229;
	sub.f64 	%fd5266, %fd6734, %fd5230;
	add.f64 	%fd6739, %fd5261, %fd5256;
	add.f64 	%fd6738, %fd5262, %fd5258;
	add.f64 	%fd6737, %fd5263, %fd5260;
	sub.f64 	%fd5267, %fd1639, %fd5256;
	sub.f64 	%fd5268, %fd1640, %fd5258;
	sub.f64 	%fd5269, %fd1641, %fd5260;
	add.f64 	%fd6736, %fd5264, %fd5246;
	add.f64 	%fd6735, %fd5265, %fd5247;
	add.f64 	%fd6734, %fd5266, %fd5248;
	sub.f64 	%fd6730, %fd5267, %fd5246;
	sub.f64 	%fd6729, %fd5268, %fd5247;
	sub.f64 	%fd6728, %fd5269, %fd5248;
	bra.uni 	$L__BB7_364;

$L__BB7_356:
	setp.eq.s32 	%p326, %r725, 5;
	@%p326 bra 	$L__BB7_358;
	bra.uni 	$L__BB7_357;

$L__BB7_358:
	sub.f64 	%fd5118, %fd5851, %fd5842;
	sub.f64 	%fd5119, %fd5843, %fd5840;
	sub.f64 	%fd5120, %fd5850, %fd5841;
	mul.f64 	%fd5121, %fd5119, %fd5120;
	sub.f64 	%fd5122, %fd5844, %fd5841;
	sub.f64 	%fd5123, %fd5849, %fd5840;
	mul.f64 	%fd5124, %fd5122, %fd5123;
	sub.f64 	%fd5125, %fd5121, %fd5124;
	sub.f64 	%fd5126, %fd5845, %fd5842;
	mul.f64 	%fd5127, %fd5126, %fd5123;
	mul.f64 	%fd5128, %fd5119, %fd5118;
	sub.f64 	%fd5129, %fd5127, %fd5128;
	mul.f64 	%fd5130, %fd5122, %fd5118;
	mul.f64 	%fd5131, %fd5126, %fd5120;
	sub.f64 	%fd5132, %fd5130, %fd5131;
	mul.f64 	%fd5133, %fd5129, %fd5129;
	fma.rn.f64 	%fd5134, %fd5125, %fd5125, %fd5133;
	fma.rn.f64 	%fd5135, %fd5132, %fd5132, %fd5134;
	sub.f64 	%fd5136, %fd5845, %fd5851;
	mul.f64 	%fd5137, %fd5136, %fd5136;
	sub.f64 	%fd5138, %fd5844, %fd5850;
	fma.rn.f64 	%fd5139, %fd5138, %fd5138, %fd5137;
	sub.f64 	%fd5140, %fd5843, %fd5849;
	fma.rn.f64 	%fd5141, %fd5140, %fd5140, %fd5139;
	div.rn.f64 	%fd5142, %fd5135, %fd5141;
	div.rn.f64 	%fd5143, %fd1682, %fd5141;
	add.f64 	%fd5144, %fd5143, 0d0000000000000000;
	mov.f64 	%fd5145, 0d0000000000000000;
	mul.f64 	%fd5146, %fd5142, %fd1682;
	div.rn.f64 	%fd5147, %fd5146, %fd5141;
	sub.f64 	%fd5148, %fd5145, %fd5147;
	add.f64 	%fd5149, %fd5136, %fd5136;
	add.f64 	%fd5150, %fd5138, %fd5138;
	add.f64 	%fd5151, %fd5140, %fd5140;
	fma.rn.f64 	%fd5152, %fd5149, %fd5148, 0d0000000000000000;
	fma.rn.f64 	%fd5153, %fd5150, %fd5148, 0d0000000000000000;
	fma.rn.f64 	%fd5154, %fd5151, %fd5148, 0d0000000000000000;
	add.f64 	%fd5155, %fd5125, %fd5125;
	add.f64 	%fd5156, %fd5129, %fd5129;
	add.f64 	%fd5157, %fd5132, %fd5132;
	fma.rn.f64 	%fd5158, %fd5155, %fd5144, 0d0000000000000000;
	fma.rn.f64 	%fd5159, %fd5156, %fd5144, 0d0000000000000000;
	fma.rn.f64 	%fd5160, %fd5157, %fd5144, 0d0000000000000000;
	mul.f64 	%fd5161, %fd5122, %fd5160;
	mul.f64 	%fd5162, %fd5119, %fd5159;
	sub.f64 	%fd5163, %fd5161, %fd5162;
	mul.f64 	%fd5164, %fd5119, %fd5158;
	mul.f64 	%fd5165, %fd5126, %fd5160;
	sub.f64 	%fd5166, %fd5164, %fd5165;
	mul.f64 	%fd5167, %fd5126, %fd5159;
	mul.f64 	%fd5168, %fd5122, %fd5158;
	sub.f64 	%fd5169, %fd5167, %fd5168;
	add.f64 	%fd5170, %fd5163, 0d0000000000000000;
	add.f64 	%fd5171, %fd5166, 0d0000000000000000;
	add.f64 	%fd5172, %fd5169, 0d0000000000000000;
	mul.f64 	%fd5173, %fd5120, %fd5160;
	mul.f64 	%fd5174, %fd5123, %fd5159;
	mul.f64 	%fd5175, %fd5123, %fd5158;
	mul.f64 	%fd5176, %fd5118, %fd5160;
	mul.f64 	%fd5177, %fd5118, %fd5159;
	mul.f64 	%fd5178, %fd5120, %fd5158;
	sub.f64 	%fd5179, %fd5174, %fd5173;
	add.f64 	%fd5180, %fd5179, 0d0000000000000000;
	sub.f64 	%fd5181, %fd5176, %fd5175;
	add.f64 	%fd5182, %fd5181, 0d0000000000000000;
	sub.f64 	%fd5183, %fd5178, %fd5177;
	add.f64 	%fd5184, %fd5183, 0d0000000000000000;
	add.f64 	%fd5185, %fd6733, %fd5152;
	add.f64 	%fd5186, %fd6732, %fd5153;
	add.f64 	%fd5187, %fd6731, %fd5154;
	sub.f64 	%fd5188, %fd6739, %fd5152;
	sub.f64 	%fd5189, %fd6738, %fd5153;
	sub.f64 	%fd5190, %fd6737, %fd5154;
	add.f64 	%fd6733, %fd5185, %fd5180;
	add.f64 	%fd6732, %fd5186, %fd5182;
	add.f64 	%fd6731, %fd5187, %fd5184;
	sub.f64 	%fd5191, %fd1639, %fd5180;
	sub.f64 	%fd5192, %fd1640, %fd5182;
	sub.f64 	%fd5193, %fd1641, %fd5184;
	add.f64 	%fd6739, %fd5188, %fd5170;
	add.f64 	%fd6738, %fd5189, %fd5171;
	add.f64 	%fd6737, %fd5190, %fd5172;
	sub.f64 	%fd6730, %fd5191, %fd5170;
	sub.f64 	%fd6729, %fd5192, %fd5171;
	sub.f64 	%fd6728, %fd5193, %fd5172;
	bra.uni 	$L__BB7_364;

$L__BB7_357:
	mul.f64 	%fd5060, %fd1140, %fd1133;
	fma.rn.f64 	%fd5061, %fd1139, %fd1130, %fd5060;
	fma.rn.f64 	%fd5062, %fd1141, %fd1134, %fd5061;
	mul.f64 	%fd5063, %fd5062, %fd5062;
	mul.f64 	%fd5064, %fd1133, %fd1133;
	fma.rn.f64 	%fd5065, %fd1130, %fd1130, %fd5064;
	fma.rn.f64 	%fd5066, %fd1134, %fd1134, %fd5065;
	div.rn.f64 	%fd5067, %fd5063, %fd5066;
	div.rn.f64 	%fd5068, %fd1682, %fd5066;
	add.f64 	%fd5069, %fd5068, 0d0000000000000000;
	mov.f64 	%fd5070, 0d0000000000000000;
	mul.f64 	%fd5071, %fd5067, %fd1682;
	div.rn.f64 	%fd5072, %fd5071, %fd5066;
	sub.f64 	%fd5073, %fd5070, %fd5072;
	add.f64 	%fd5074, %fd1130, %fd1130;
	add.f64 	%fd5075, %fd1133, %fd1133;
	add.f64 	%fd5076, %fd1134, %fd1134;
	fma.rn.f64 	%fd5077, %fd5074, %fd5073, 0d0000000000000000;
	fma.rn.f64 	%fd5078, %fd5075, %fd5073, 0d0000000000000000;
	fma.rn.f64 	%fd5079, %fd5076, %fd5073, 0d0000000000000000;
	fma.rn.f64 	%fd5080, %fd5062, %fd5069, 0d0000000000000000;
	fma.rn.f64 	%fd5081, %fd5062, %fd5069, %fd5080;
	fma.rn.f64 	%fd5082, %fd1130, %fd5081, 0d0000000000000000;
	fma.rn.f64 	%fd5083, %fd1133, %fd5081, 0d0000000000000000;
	fma.rn.f64 	%fd5084, %fd1134, %fd5081, 0d0000000000000000;
	fma.rn.f64 	%fd5085, %fd1139, %fd5081, %fd5077;
	fma.rn.f64 	%fd5086, %fd1140, %fd5081, %fd5078;
	fma.rn.f64 	%fd5087, %fd1141, %fd5081, %fd5079;
	mul.f64 	%fd5088, %fd1128, %fd5087;
	mul.f64 	%fd5089, %fd1126, %fd5086;
	sub.f64 	%fd5090, %fd5088, %fd5089;
	mul.f64 	%fd5091, %fd1126, %fd5085;
	mul.f64 	%fd5092, %fd1131, %fd5087;
	sub.f64 	%fd5093, %fd5091, %fd5092;
	mul.f64 	%fd5094, %fd1131, %fd5086;
	mul.f64 	%fd5095, %fd1128, %fd5085;
	sub.f64 	%fd5096, %fd5094, %fd5095;
	add.f64 	%fd5097, %fd5090, 0d0000000000000000;
	add.f64 	%fd5098, %fd5093, 0d0000000000000000;
	add.f64 	%fd5099, %fd5096, 0d0000000000000000;
	mul.f64 	%fd5100, %fd1127, %fd5087;
	mul.f64 	%fd5101, %fd1129, %fd5086;
	mul.f64 	%fd5102, %fd1129, %fd5085;
	mul.f64 	%fd5103, %fd1132, %fd5087;
	mul.f64 	%fd5104, %fd1132, %fd5086;
	mul.f64 	%fd5105, %fd1127, %fd5085;
	sub.f64 	%fd5106, %fd5101, %fd5100;
	add.f64 	%fd5107, %fd5106, 0d0000000000000000;
	sub.f64 	%fd5108, %fd5103, %fd5102;
	add.f64 	%fd5109, %fd5108, 0d0000000000000000;
	sub.f64 	%fd5110, %fd5105, %fd5104;
	add.f64 	%fd5111, %fd5110, 0d0000000000000000;
	add.f64 	%fd6730, %fd1639, %fd5082;
	add.f64 	%fd6729, %fd1640, %fd5083;
	add.f64 	%fd6728, %fd1641, %fd5084;
	sub.f64 	%fd5112, %fd6733, %fd5082;
	sub.f64 	%fd5113, %fd6732, %fd5083;
	sub.f64 	%fd5114, %fd6731, %fd5084;
	add.f64 	%fd6739, %fd6739, %fd5107;
	add.f64 	%fd6738, %fd6738, %fd5109;
	add.f64 	%fd6737, %fd6737, %fd5111;
	sub.f64 	%fd5115, %fd5112, %fd5107;
	sub.f64 	%fd5116, %fd5113, %fd5109;
	sub.f64 	%fd5117, %fd5114, %fd5111;
	add.f64 	%fd6736, %fd6736, %fd5097;
	add.f64 	%fd6735, %fd6735, %fd5098;
	add.f64 	%fd6734, %fd6734, %fd5099;
	sub.f64 	%fd6733, %fd5115, %fd5097;
	sub.f64 	%fd6732, %fd5116, %fd5098;
	sub.f64 	%fd6731, %fd5117, %fd5099;

$L__BB7_364:
	mov.f64 	%fd6769, 0d0000000000000000;
	mov.f64 	%fd6770, %fd6769;
	mov.f64 	%fd6771, %fd6769;
	mov.f64 	%fd6772, %fd6769;
	mov.f64 	%fd6773, %fd6769;
	mov.f64 	%fd6774, %fd6769;
	mov.f64 	%fd6775, %fd6769;
	mov.f64 	%fd6776, %fd6769;
	mov.f64 	%fd6777, %fd6769;
	@%p16 bra 	$L__BB7_368;

	sub.f64 	%fd1755, %fd5851, %fd5848;
	sub.f64 	%fd1756, %fd5850, %fd5847;
	mul.f64 	%fd5381, %fd1756, %fd1134;
	sub.f64 	%fd1757, %fd5849, %fd5846;
	mul.f64 	%fd5382, %fd1757, %fd1133;
	sub.f64 	%fd1758, %fd5381, %fd5382;
	mul.f64 	%fd5383, %fd1757, %fd1130;
	mul.f64 	%fd5384, %fd1755, %fd1134;
	sub.f64 	%fd1759, %fd5383, %fd5384;
	mul.f64 	%fd5385, %fd1755, %fd1133;
	mul.f64 	%fd5386, %fd1756, %fd1130;
	sub.f64 	%fd1760, %fd5385, %fd5386;
	mul.f64 	%fd5387, %fd1755, %fd1755;
	fma.rn.f64 	%fd5388, %fd1756, %fd1756, %fd5387;
	fma.rn.f64 	%fd1761, %fd1757, %fd1757, %fd5388;
	mul.f64 	%fd5389, %fd1756, %fd1759;
	fma.rn.f64 	%fd5390, %fd1755, %fd1758, %fd5389;
	fma.rn.f64 	%fd5391, %fd1757, %fd1760, %fd5390;
	mul.f64 	%fd5392, %fd1759, %fd1759;
	fma.rn.f64 	%fd5393, %fd1758, %fd1758, %fd5392;
	fma.rn.f64 	%fd5394, %fd1760, %fd1760, %fd5393;
	sub.f64 	%fd1762, %fd5842, %fd5848;
	mul.f64 	%fd5395, %fd1762, %fd1755;
	sub.f64 	%fd1763, %fd5841, %fd5847;
	fma.rn.f64 	%fd5396, %fd1763, %fd1756, %fd5395;
	sub.f64 	%fd1764, %fd5840, %fd5846;
	fma.rn.f64 	%fd1765, %fd1764, %fd1757, %fd5396;
	mul.f64 	%fd5397, %fd1763, %fd1759;
	fma.rn.f64 	%fd5398, %fd1762, %fd1758, %fd5397;
	fma.rn.f64 	%fd5399, %fd1764, %fd1760, %fd5398;
	div.rn.f64 	%fd1766, %fd5391, %fd1761;
	mul.f64 	%fd1767, %fd1766, %fd1766;
	mul.f64 	%fd5400, %fd1761, %fd1767;
	sub.f64 	%fd1768, %fd5394, %fd5400;
	mul.f64 	%fd5401, %fd1765, %fd1766;
	sub.f64 	%fd5402, %fd5399, %fd5401;
	div.rn.f64 	%fd1769, %fd5402, %fd1768;
	mul.f64 	%fd1770, %fd1761, %fd1766;
	mul.f64 	%fd5403, %fd1770, %fd1769;
	sub.f64 	%fd5404, %fd1765, %fd5403;
	div.rn.f64 	%fd1771, %fd5404, %fd1761;
	setp.gt.f64 	%p327, %fd1771, 0d0000000000000000;
	mov.f64 	%fd5380, 0d0000000000000000;
	setp.lt.f64 	%p328, %fd1771, 0d3FF0000000000000;
	setp.ge.f64 	%p329, %fd1769, 0d0000000000000000;
	and.pred  	%p330, %p327, %p328;
	and.pred  	%p331, %p329, %p330;
	mov.f64 	%fd6749, %fd5380;
	mov.f64 	%fd6750, %fd5380;
	mov.f64 	%fd6751, %fd5380;
	mov.f64 	%fd6752, %fd5380;
	mov.f64 	%fd6753, %fd5380;
	mov.f64 	%fd6754, %fd5380;
	mov.f64 	%fd6755, %fd5380;
	mov.f64 	%fd6756, %fd5380;
	@%p331 bra 	$L__BB7_367;

	sub.f64 	%fd5405, %fd5845, %fd5851;
	sub.f64 	%fd5406, %fd5844, %fd5850;
	mul.f64 	%fd5407, %fd5406, %fd1134;
	sub.f64 	%fd5408, %fd5843, %fd5849;
	mul.f64 	%fd5409, %fd5408, %fd1133;
	sub.f64 	%fd5410, %fd5407, %fd5409;
	mul.f64 	%fd5411, %fd5408, %fd1130;
	mul.f64 	%fd5412, %fd5405, %fd1134;
	sub.f64 	%fd5413, %fd5411, %fd5412;
	mul.f64 	%fd5414, %fd5405, %fd1133;
	mul.f64 	%fd5415, %fd5406, %fd1130;
	sub.f64 	%fd5416, %fd5414, %fd5415;
	mul.f64 	%fd5417, %fd5405, %fd5405;
	fma.rn.f64 	%fd5418, %fd5406, %fd5406, %fd5417;
	fma.rn.f64 	%fd5419, %fd5408, %fd5408, %fd5418;
	mul.f64 	%fd5420, %fd5406, %fd5413;
	fma.rn.f64 	%fd5421, %fd5405, %fd5410, %fd5420;
	fma.rn.f64 	%fd5422, %fd5408, %fd5416, %fd5421;
	mul.f64 	%fd5423, %fd5413, %fd5413;
	fma.rn.f64 	%fd5424, %fd5410, %fd5410, %fd5423;
	fma.rn.f64 	%fd5425, %fd5416, %fd5416, %fd5424;
	sub.f64 	%fd5426, %fd5842, %fd5851;
	mul.f64 	%fd5427, %fd5405, %fd5426;
	sub.f64 	%fd5428, %fd5841, %fd5850;
	fma.rn.f64 	%fd5429, %fd5406, %fd5428, %fd5427;
	sub.f64 	%fd5430, %fd5840, %fd5849;
	fma.rn.f64 	%fd5431, %fd5408, %fd5430, %fd5429;
	mul.f64 	%fd5432, %fd5428, %fd5413;
	fma.rn.f64 	%fd5433, %fd5426, %fd5410, %fd5432;
	fma.rn.f64 	%fd5434, %fd5430, %fd5416, %fd5433;
	div.rn.f64 	%fd5435, %fd5422, %fd5419;
	mul.f64 	%fd5436, %fd5435, %fd5435;
	mul.f64 	%fd5437, %fd5419, %fd5436;
	sub.f64 	%fd5438, %fd5425, %fd5437;
	mul.f64 	%fd5439, %fd5431, %fd5435;
	sub.f64 	%fd5440, %fd5434, %fd5439;
	div.rn.f64 	%fd5441, %fd5440, %fd5438;
	mul.f64 	%fd5442, %fd5419, %fd5435;
	mul.f64 	%fd5443, %fd5442, %fd5441;
	sub.f64 	%fd5444, %fd5431, %fd5443;
	div.rn.f64 	%fd5445, %fd5444, %fd5419;
	mov.f64 	%fd5446, 0d0000000000000000;
	div.rn.f64 	%fd5447, %fd5446, %fd5419;
	add.f64 	%fd5448, %fd5447, 0d0000000000000000;
	mul.f64 	%fd5449, %fd5445, 0d0000000000000000;
	div.rn.f64 	%fd5450, %fd5449, %fd5419;
	sub.f64 	%fd5451, %fd5446, %fd5450;
	sub.f64 	%fd5452, %fd5446, %fd5448;
	fma.rn.f64 	%fd5453, %fd5452, %fd5441, 0d0000000000000000;
	fma.rn.f64 	%fd5454, %fd5452, %fd5442, 0d0000000000000000;
	fma.rn.f64 	%fd5455, %fd5435, %fd5453, %fd5451;
	fma.rn.f64 	%fd5456, %fd5419, %fd5453, 0d0000000000000000;
	div.rn.f64 	%fd5457, %fd5454, %fd5438;
	add.f64 	%fd6750, %fd5457, 0d0000000000000000;
	mul.f64 	%fd5458, %fd5454, %fd5441;
	div.rn.f64 	%fd5459, %fd5458, %fd5438;
	sub.f64 	%fd5460, %fd5446, %fd5459;
	sub.f64 	%fd5461, %fd5446, %fd6750;
	fma.rn.f64 	%fd5462, %fd5431, %fd5461, %fd5456;
	fma.rn.f64 	%fd5463, %fd5435, %fd5461, %fd5448;
	add.f64 	%fd6749, %fd5463, 0d0000000000000000;
	add.f64 	%fd6753, %fd5460, 0d0000000000000000;
	sub.f64 	%fd5464, %fd5446, %fd5460;
	fma.rn.f64 	%fd5465, %fd5419, %fd5464, 0d0000000000000000;
	fma.rn.f64 	%fd5466, %fd5436, %fd5464, %fd5455;
	fma.rn.f64 	%fd5467, %fd5435, %fd5465, %fd5462;
	fma.rn.f64 	%fd5468, %fd5435, %fd5465, %fd5467;
	div.rn.f64 	%fd5469, %fd5468, %fd5419;
	add.f64 	%fd6752, %fd5469, 0d0000000000000000;
	mul.f64 	%fd5470, %fd5435, %fd5468;
	div.rn.f64 	%fd5471, %fd5470, %fd5419;
	sub.f64 	%fd5472, %fd5466, %fd5471;
	add.f64 	%fd6751, %fd5472, 0d0000000000000000;
	fma.rn.f64 	%fd5473, %fd5426, %fd6750, 0d0000000000000000;
	fma.rn.f64 	%fd5474, %fd5428, %fd6750, 0d0000000000000000;
	fma.rn.f64 	%fd5475, %fd5430, %fd6750, 0d0000000000000000;
	fma.rn.f64 	%fd5476, %fd5410, %fd6750, 0d0000000000000000;
	fma.rn.f64 	%fd5477, %fd5413, %fd6750, 0d0000000000000000;
	fma.rn.f64 	%fd5478, %fd5416, %fd6750, 0d0000000000000000;
	fma.rn.f64 	%fd5479, %fd5426, %fd6749, 0d0000000000000000;
	fma.rn.f64 	%fd5480, %fd5428, %fd6749, 0d0000000000000000;
	fma.rn.f64 	%fd5481, %fd5430, %fd6749, 0d0000000000000000;
	fma.rn.f64 	%fd5482, %fd5405, %fd6749, %fd5476;
	fma.rn.f64 	%fd5483, %fd5406, %fd6749, %fd5477;
	fma.rn.f64 	%fd5484, %fd5408, %fd6749, %fd5478;
	add.f64 	%fd6730, %fd5482, %fd6730;
	add.f64 	%fd6729, %fd5483, %fd6729;
	add.f64 	%fd6728, %fd5484, %fd6728;
	sub.f64 	%fd5485, %fd6739, %fd5482;
	sub.f64 	%fd5486, %fd6738, %fd5483;
	sub.f64 	%fd5487, %fd6737, %fd5484;
	fma.rn.f64 	%fd5488, %fd5416, %fd6753, 0d0000000000000000;
	add.f64 	%fd5489, %fd5475, %fd5488;
	add.f64 	%fd5490, %fd5488, %fd5489;
	fma.rn.f64 	%fd5491, %fd5413, %fd6753, 0d0000000000000000;
	add.f64 	%fd5492, %fd5474, %fd5491;
	add.f64 	%fd5493, %fd5491, %fd5492;
	fma.rn.f64 	%fd5494, %fd5410, %fd6753, 0d0000000000000000;
	add.f64 	%fd5495, %fd5473, %fd5494;
	add.f64 	%fd5496, %fd5494, %fd5495;
	fma.rn.f64 	%fd5497, %fd5416, %fd6752, 0d0000000000000000;
	fma.rn.f64 	%fd5498, %fd5408, %fd6752, 0d0000000000000000;
	add.f64 	%fd5499, %fd5490, %fd5498;
	add.f64 	%fd5500, %fd5481, %fd5497;
	fma.rn.f64 	%fd5501, %fd5413, %fd6752, 0d0000000000000000;
	fma.rn.f64 	%fd5502, %fd5406, %fd6752, 0d0000000000000000;
	add.f64 	%fd5503, %fd5493, %fd5502;
	add.f64 	%fd5504, %fd5480, %fd5501;
	fma.rn.f64 	%fd5505, %fd5410, %fd6752, 0d0000000000000000;
	fma.rn.f64 	%fd5506, %fd5405, %fd6752, 0d0000000000000000;
	add.f64 	%fd5507, %fd5496, %fd5506;
	add.f64 	%fd5508, %fd5479, %fd5505;
	fma.rn.f64 	%fd5509, %fd5408, %fd6751, 0d0000000000000000;
	add.f64 	%fd5510, %fd5500, %fd5509;
	add.f64 	%fd5511, %fd5509, %fd5510;
	fma.rn.f64 	%fd5512, %fd5406, %fd6751, 0d0000000000000000;
	add.f64 	%fd5513, %fd5504, %fd5512;
	add.f64 	%fd5514, %fd5512, %fd5513;
	fma.rn.f64 	%fd5515, %fd5405, %fd6751, 0d0000000000000000;
	add.f64 	%fd5516, %fd5508, %fd5515;
	add.f64 	%fd5517, %fd5515, %fd5516;
	mul.f64 	%fd5518, %fd1133, %fd5499;
	mul.f64 	%fd5519, %fd1134, %fd5503;
	sub.f64 	%fd5520, %fd5518, %fd5519;
	mul.f64 	%fd5521, %fd1134, %fd5507;
	mul.f64 	%fd5522, %fd1130, %fd5499;
	sub.f64 	%fd5523, %fd5521, %fd5522;
	mul.f64 	%fd5524, %fd1130, %fd5503;
	mul.f64 	%fd5525, %fd1133, %fd5507;
	sub.f64 	%fd5526, %fd5524, %fd5525;
	add.f64 	%fd5527, %fd5520, %fd5517;
	add.f64 	%fd5528, %fd5523, %fd5514;
	add.f64 	%fd5529, %fd5526, %fd5511;
	mul.f64 	%fd5530, %fd5406, %fd5499;
	mul.f64 	%fd5531, %fd5408, %fd5503;
	mul.f64 	%fd5532, %fd5408, %fd5507;
	mul.f64 	%fd5533, %fd5405, %fd5499;
	mul.f64 	%fd5534, %fd5405, %fd5503;
	mul.f64 	%fd5535, %fd5406, %fd5507;
	sub.f64 	%fd5536, %fd5531, %fd5530;
	add.f64 	%fd6754, %fd5536, 0d0000000000000000;
	sub.f64 	%fd5537, %fd5533, %fd5532;
	add.f64 	%fd6755, %fd5537, 0d0000000000000000;
	sub.f64 	%fd5538, %fd5535, %fd5534;
	add.f64 	%fd6756, %fd5538, 0d0000000000000000;
	add.f64 	%fd6733, %fd5527, %fd6733;
	add.f64 	%fd6732, %fd5528, %fd6732;
	add.f64 	%fd6731, %fd5529, %fd6731;
	sub.f64 	%fd6739, %fd5485, %fd5527;
	sub.f64 	%fd6738, %fd5486, %fd5528;
	sub.f64 	%fd6737, %fd5487, %fd5529;

$L__BB7_367:
	div.rn.f64 	%fd5540, %fd5380, %fd1761;
	add.f64 	%fd5541, %fd5540, 0d0000000000000000;
	mul.f64 	%fd5542, %fd1771, 0d0000000000000000;
	div.rn.f64 	%fd5543, %fd5542, %fd1761;
	sub.f64 	%fd5544, %fd5380, %fd5543;
	sub.f64 	%fd5545, %fd5380, %fd5541;
	fma.rn.f64 	%fd5546, %fd5545, %fd1769, 0d0000000000000000;
	fma.rn.f64 	%fd5547, %fd5545, %fd1770, 0d0000000000000000;
	fma.rn.f64 	%fd5548, %fd1766, %fd5546, %fd5544;
	fma.rn.f64 	%fd5549, %fd1761, %fd5546, 0d0000000000000000;
	div.rn.f64 	%fd5550, %fd5547, %fd1768;
	add.f64 	%fd5551, %fd5550, 0d0000000000000000;
	mul.f64 	%fd5552, %fd5547, %fd1769;
	div.rn.f64 	%fd5553, %fd5552, %fd1768;
	sub.f64 	%fd5554, %fd5380, %fd5553;
	sub.f64 	%fd5555, %fd5380, %fd5551;
	fma.rn.f64 	%fd5556, %fd1765, %fd5555, %fd5549;
	fma.rn.f64 	%fd5557, %fd1766, %fd5555, %fd5541;
	add.f64 	%fd5558, %fd5557, 0d0000000000000000;
	add.f64 	%fd5559, %fd5554, 0d0000000000000000;
	sub.f64 	%fd5560, %fd5380, %fd5554;
	fma.rn.f64 	%fd5561, %fd1761, %fd5560, 0d0000000000000000;
	fma.rn.f64 	%fd5562, %fd1767, %fd5560, %fd5548;
	fma.rn.f64 	%fd5563, %fd1766, %fd5561, %fd5556;
	fma.rn.f64 	%fd5564, %fd1766, %fd5561, %fd5563;
	div.rn.f64 	%fd5565, %fd5564, %fd1761;
	add.f64 	%fd5566, %fd5565, 0d0000000000000000;
	mul.f64 	%fd5567, %fd1766, %fd5564;
	div.rn.f64 	%fd5568, %fd5567, %fd1761;
	sub.f64 	%fd5569, %fd5562, %fd5568;
	add.f64 	%fd5570, %fd5569, 0d0000000000000000;
	add.f64 	%fd6770, %fd5551, %fd6750;
	add.f64 	%fd6769, %fd5558, %fd6749;
	add.f64 	%fd6774, %fd5559, %fd6753;
	add.f64 	%fd6772, %fd5566, %fd6752;
	add.f64 	%fd6771, %fd5570, %fd6751;
	add.f64 	%fd5571, %fd6770, 0d0000000000000000;
	fma.rn.f64 	%fd5572, %fd1762, %fd5571, 0d0000000000000000;
	fma.rn.f64 	%fd5573, %fd1763, %fd5571, 0d0000000000000000;
	fma.rn.f64 	%fd5574, %fd1764, %fd5571, 0d0000000000000000;
	fma.rn.f64 	%fd5575, %fd1758, %fd5571, 0d0000000000000000;
	fma.rn.f64 	%fd5576, %fd1759, %fd5571, 0d0000000000000000;
	fma.rn.f64 	%fd5577, %fd1760, %fd5571, 0d0000000000000000;
	add.f64 	%fd5578, %fd6769, 0d0000000000000000;
	fma.rn.f64 	%fd5579, %fd1762, %fd5578, 0d0000000000000000;
	fma.rn.f64 	%fd5580, %fd1763, %fd5578, 0d0000000000000000;
	fma.rn.f64 	%fd5581, %fd1764, %fd5578, 0d0000000000000000;
	fma.rn.f64 	%fd5582, %fd1755, %fd5578, %fd5575;
	fma.rn.f64 	%fd5583, %fd1756, %fd5578, %fd5576;
	fma.rn.f64 	%fd5584, %fd1757, %fd5578, %fd5577;
	add.f64 	%fd6730, %fd6730, %fd5582;
	add.f64 	%fd6729, %fd6729, %fd5583;
	add.f64 	%fd6728, %fd6728, %fd5584;
	sub.f64 	%fd5585, %fd6736, %fd5582;
	sub.f64 	%fd5586, %fd6735, %fd5583;
	sub.f64 	%fd5587, %fd6734, %fd5584;
	add.f64 	%fd5588, %fd6774, 0d0000000000000000;
	fma.rn.f64 	%fd5589, %fd1760, %fd5588, 0d0000000000000000;
	add.f64 	%fd5590, %fd5574, %fd5589;
	add.f64 	%fd5591, %fd5589, %fd5590;
	fma.rn.f64 	%fd5592, %fd1759, %fd5588, 0d0000000000000000;
	add.f64 	%fd5593, %fd5573, %fd5592;
	add.f64 	%fd5594, %fd5592, %fd5593;
	fma.rn.f64 	%fd5595, %fd1758, %fd5588, 0d0000000000000000;
	add.f64 	%fd5596, %fd5572, %fd5595;
	add.f64 	%fd5597, %fd5595, %fd5596;
	add.f64 	%fd5598, %fd6772, 0d0000000000000000;
	add.f64 	%fd6773, %fd6752, %fd5598;
	add.f64 	%fd5599, %fd6773, 0d0000000000000000;
	fma.rn.f64 	%fd5600, %fd1760, %fd5599, 0d0000000000000000;
	fma.rn.f64 	%fd5601, %fd1757, %fd5599, 0d0000000000000000;
	add.f64 	%fd5602, %fd5601, %fd5591;
	add.f64 	%fd5603, %fd5581, %fd5600;
	fma.rn.f64 	%fd5604, %fd1759, %fd5599, 0d0000000000000000;
	fma.rn.f64 	%fd5605, %fd1756, %fd5599, 0d0000000000000000;
	add.f64 	%fd5606, %fd5605, %fd5594;
	add.f64 	%fd5607, %fd5580, %fd5604;
	fma.rn.f64 	%fd5608, %fd1758, %fd5599, 0d0000000000000000;
	fma.rn.f64 	%fd5609, %fd1755, %fd5599, 0d0000000000000000;
	add.f64 	%fd5610, %fd5609, %fd5597;
	add.f64 	%fd5611, %fd5579, %fd5608;
	add.f64 	%fd5612, %fd6771, 0d0000000000000000;
	fma.rn.f64 	%fd5613, %fd1757, %fd5612, 0d0000000000000000;
	add.f64 	%fd5614, %fd5613, %fd5603;
	add.f64 	%fd5615, %fd5613, %fd5614;
	fma.rn.f64 	%fd5616, %fd1756, %fd5612, 0d0000000000000000;
	add.f64 	%fd5617, %fd5616, %fd5607;
	add.f64 	%fd5618, %fd5616, %fd5617;
	fma.rn.f64 	%fd5619, %fd1755, %fd5612, 0d0000000000000000;
	add.f64 	%fd5620, %fd5619, %fd5611;
	add.f64 	%fd5621, %fd5619, %fd5620;
	mul.f64 	%fd5622, %fd1133, %fd5602;
	mul.f64 	%fd5623, %fd1134, %fd5606;
	sub.f64 	%fd5624, %fd5622, %fd5623;
	mul.f64 	%fd5625, %fd1134, %fd5610;
	mul.f64 	%fd5626, %fd1130, %fd5602;
	sub.f64 	%fd5627, %fd5625, %fd5626;
	mul.f64 	%fd5628, %fd1130, %fd5606;
	mul.f64 	%fd5629, %fd1133, %fd5610;
	sub.f64 	%fd5630, %fd5628, %fd5629;
	add.f64 	%fd5631, %fd5621, %fd5624;
	add.f64 	%fd5632, %fd5618, %fd5627;
	add.f64 	%fd5633, %fd5615, %fd5630;
	mul.f64 	%fd5634, %fd1756, %fd5602;
	mul.f64 	%fd5635, %fd1757, %fd5606;
	sub.f64 	%fd5636, %fd5634, %fd5635;
	mul.f64 	%fd5637, %fd1757, %fd5610;
	mul.f64 	%fd5638, %fd1755, %fd5602;
	sub.f64 	%fd5639, %fd5637, %fd5638;
	mul.f64 	%fd5640, %fd1755, %fd5606;
	mul.f64 	%fd5641, %fd1756, %fd5610;
	sub.f64 	%fd5642, %fd5640, %fd5641;
	sub.f64 	%fd6775, %fd6754, %fd5636;
	sub.f64 	%fd6776, %fd6755, %fd5639;
	sub.f64 	%fd6777, %fd6756, %fd5642;
	add.f64 	%fd6739, %fd6739, %fd5631;
	add.f64 	%fd6738, %fd6738, %fd5632;
	add.f64 	%fd6737, %fd6737, %fd5633;
	sub.f64 	%fd6736, %fd5585, %fd5631;
	sub.f64 	%fd6735, %fd5586, %fd5632;
	sub.f64 	%fd6734, %fd5587, %fd5633;

$L__BB7_368:
	add.f64 	%fd5643, %fd1634, %fd6770;
	add.f64 	%fd5644, %fd5643, 0d0000000000000000;
	fma.rn.f64 	%fd5645, %fd1139, %fd5644, 0d0000000000000000;
	fma.rn.f64 	%fd5646, %fd1140, %fd5644, 0d0000000000000000;
	fma.rn.f64 	%fd5647, %fd1141, %fd5644, 0d0000000000000000;
	fma.rn.f64 	%fd5648, %fd1135, %fd5644, 0d0000000000000000;
	fma.rn.f64 	%fd5649, %fd1136, %fd5644, 0d0000000000000000;
	fma.rn.f64 	%fd5650, %fd1137, %fd5644, 0d0000000000000000;
	add.f64 	%fd5651, %fd1635, %fd6769;
	add.f64 	%fd5652, %fd5651, 0d0000000000000000;
	fma.rn.f64 	%fd5653, %fd1139, %fd5652, 0d0000000000000000;
	fma.rn.f64 	%fd5654, %fd1140, %fd5652, 0d0000000000000000;
	fma.rn.f64 	%fd5655, %fd1141, %fd5652, 0d0000000000000000;
	fma.rn.f64 	%fd5656, %fd1132, %fd5652, %fd5648;
	fma.rn.f64 	%fd5657, %fd1127, %fd5652, %fd5649;
	fma.rn.f64 	%fd5658, %fd1129, %fd5652, %fd5650;
	add.f64 	%fd1845, %fd6730, %fd5656;
	add.f64 	%fd1846, %fd6729, %fd5657;
	add.f64 	%fd1847, %fd6728, %fd5658;
	sub.f64 	%fd5659, %fd6733, %fd5656;
	sub.f64 	%fd5660, %fd6732, %fd5657;
	sub.f64 	%fd5661, %fd6731, %fd5658;
	add.f64 	%fd5662, %fd1636, %fd6774;
	add.f64 	%fd5663, %fd5662, 0d0000000000000000;
	fma.rn.f64 	%fd5664, %fd1137, %fd5663, 0d0000000000000000;
	add.f64 	%fd5665, %fd5647, %fd5664;
	add.f64 	%fd5666, %fd5664, %fd5665;
	fma.rn.f64 	%fd5667, %fd1136, %fd5663, 0d0000000000000000;
	add.f64 	%fd5668, %fd5646, %fd5667;
	add.f64 	%fd5669, %fd5667, %fd5668;
	fma.rn.f64 	%fd5670, %fd1135, %fd5663, 0d0000000000000000;
	add.f64 	%fd5671, %fd5645, %fd5670;
	add.f64 	%fd5672, %fd5670, %fd5671;
	add.f64 	%fd5673, %fd1637, %fd6772;
	add.f64 	%fd5674, %fd5673, 0d0000000000000000;
	add.f64 	%fd5675, %fd6773, %fd5674;
	add.f64 	%fd5676, %fd5675, 0d0000000000000000;
	fma.rn.f64 	%fd5677, %fd1137, %fd5676, 0d0000000000000000;
	fma.rn.f64 	%fd5678, %fd1129, %fd5676, 0d0000000000000000;
	add.f64 	%fd5679, %fd5678, %fd5666;
	add.f64 	%fd5680, %fd5655, %fd5677;
	fma.rn.f64 	%fd5681, %fd1136, %fd5676, 0d0000000000000000;
	fma.rn.f64 	%fd5682, %fd1127, %fd5676, 0d0000000000000000;
	add.f64 	%fd5683, %fd5682, %fd5669;
	add.f64 	%fd5684, %fd5654, %fd5681;
	fma.rn.f64 	%fd5685, %fd1135, %fd5676, 0d0000000000000000;
	fma.rn.f64 	%fd5686, %fd1132, %fd5676, 0d0000000000000000;
	add.f64 	%fd5687, %fd5686, %fd5672;
	add.f64 	%fd5688, %fd5653, %fd5685;
	add.f64 	%fd5689, %fd1638, %fd6771;
	add.f64 	%fd5690, %fd5689, 0d0000000000000000;
	fma.rn.f64 	%fd5691, %fd1129, %fd5690, 0d0000000000000000;
	add.f64 	%fd5692, %fd5691, %fd5680;
	add.f64 	%fd5693, %fd5691, %fd5692;
	fma.rn.f64 	%fd5694, %fd1127, %fd5690, 0d0000000000000000;
	add.f64 	%fd5695, %fd5694, %fd5684;
	add.f64 	%fd5696, %fd5694, %fd5695;
	fma.rn.f64 	%fd5697, %fd1132, %fd5690, 0d0000000000000000;
	add.f64 	%fd5698, %fd5697, %fd5688;
	add.f64 	%fd5699, %fd5697, %fd5698;
	mul.f64 	%fd5700, %fd1133, %fd5679;
	mul.f64 	%fd5701, %fd1134, %fd5683;
	sub.f64 	%fd5702, %fd5700, %fd5701;
	mul.f64 	%fd5703, %fd1134, %fd5687;
	mul.f64 	%fd5704, %fd1130, %fd5679;
	sub.f64 	%fd5705, %fd5703, %fd5704;
	mul.f64 	%fd5706, %fd1130, %fd5683;
	mul.f64 	%fd5707, %fd1133, %fd5687;
	sub.f64 	%fd5708, %fd5706, %fd5707;
	add.f64 	%fd5709, %fd5699, %fd5702;
	add.f64 	%fd5710, %fd5696, %fd5705;
	add.f64 	%fd5711, %fd5693, %fd5708;
	mul.f64 	%fd5712, %fd1127, %fd5679;
	mul.f64 	%fd5713, %fd1129, %fd5683;
	sub.f64 	%fd5714, %fd5712, %fd5713;
	mul.f64 	%fd5715, %fd1129, %fd5687;
	mul.f64 	%fd5716, %fd1132, %fd5679;
	sub.f64 	%fd5717, %fd5715, %fd5716;
	mul.f64 	%fd5718, %fd1132, %fd5683;
	mul.f64 	%fd5719, %fd1127, %fd5687;
	sub.f64 	%fd5720, %fd5718, %fd5719;
	sub.f64 	%fd5721, %fd6775, %fd5714;
	sub.f64 	%fd5722, %fd6776, %fd5717;
	sub.f64 	%fd5723, %fd6777, %fd5720;
	mul.f64 	%fd5724, %fd1128, %fd5723;
	mul.f64 	%fd5725, %fd1126, %fd5722;
	sub.f64 	%fd5726, %fd5724, %fd5725;
	mul.f64 	%fd5727, %fd1126, %fd5721;
	mul.f64 	%fd5728, %fd1131, %fd5723;
	sub.f64 	%fd5729, %fd5727, %fd5728;
	mul.f64 	%fd5730, %fd1131, %fd5722;
	mul.f64 	%fd5731, %fd1128, %fd5721;
	sub.f64 	%fd5732, %fd5730, %fd5731;
	add.f64 	%fd5733, %fd5709, %fd5726;
	add.f64 	%fd5734, %fd5710, %fd5729;
	add.f64 	%fd5735, %fd5711, %fd5732;
	mul.f64 	%fd5736, %fd1127, %fd5723;
	mul.f64 	%fd5737, %fd1129, %fd5722;
	mul.f64 	%fd5738, %fd1129, %fd5721;
	mul.f64 	%fd5739, %fd1132, %fd5723;
	mul.f64 	%fd5740, %fd1132, %fd5722;
	mul.f64 	%fd5741, %fd1127, %fd5721;
	sub.f64 	%fd5742, %fd5737, %fd5736;
	add.f64 	%fd5743, %fd5742, 0d0000000000000000;
	sub.f64 	%fd5744, %fd5739, %fd5738;
	add.f64 	%fd5745, %fd5744, 0d0000000000000000;
	sub.f64 	%fd5746, %fd5741, %fd5740;
	add.f64 	%fd5747, %fd5746, 0d0000000000000000;
	add.f64 	%fd5748, %fd6739, %fd5743;
	add.f64 	%fd5749, %fd6738, %fd5745;
	add.f64 	%fd5750, %fd6737, %fd5747;
	sub.f64 	%fd5751, %fd5659, %fd5743;
	sub.f64 	%fd5752, %fd5660, %fd5745;
	sub.f64 	%fd5753, %fd5661, %fd5747;
	add.f64 	%fd1848, %fd6736, %fd5733;
	add.f64 	%fd1849, %fd6735, %fd5734;
	add.f64 	%fd1850, %fd6734, %fd5735;
	sub.f64 	%fd1851, %fd5751, %fd5733;
	sub.f64 	%fd1852, %fd5752, %fd5734;
	sub.f64 	%fd1853, %fd5753, %fd5735;
	add.f64 	%fd1854, %fd5748, 0d0000000000000000;
	add.f64 	%fd1855, %fd5749, 0d0000000000000000;
	add.f64 	%fd1856, %fd5750, 0d0000000000000000;
	setp.eq.s64 	%p332, %rd125, 0;
	@%p332 bra 	$L__BB7_370;

	cvt.s64.s32 	%rd352, %r677;
	mul.lo.s64 	%rd353, %rd352, %rd58;
	add.s64 	%rd349, %rd125, %rd353;
	// begin inline asm
	{ atom.add.f64 %fd5754,[%rd349],%fd1854; }

	// end inline asm
	add.s64 	%rd350, %rd349, 8;
	// begin inline asm
	{ atom.add.f64 %fd5756,[%rd350],%fd1855; }

	// end inline asm
	add.s64 	%rd351, %rd349, 16;
	// begin inline asm
	{ atom.add.f64 %fd5758,[%rd351],%fd1856; }

	// end inline asm
	bra.uni 	$L__BB7_372;

$L__BB7_370:
	setp.eq.s64 	%p333, %rd100, 0;
	@%p333 bra 	$L__BB7_372;

	cvt.s64.s32 	%rd357, %r677;
	mul.lo.s64 	%rd358, %rd357, %rd46;
	add.s64 	%rd354, %rd100, %rd358;
	// begin inline asm
	{ atom.add.f64 %fd5760,[%rd354],%fd1854; }

	// end inline asm
	add.s64 	%rd355, %rd354, 8;
	// begin inline asm
	{ atom.add.f64 %fd5762,[%rd355],%fd1855; }

	// end inline asm
	add.s64 	%rd356, %rd354, 16;
	// begin inline asm
	{ atom.add.f64 %fd5764,[%rd356],%fd1856; }

	// end inline asm

$L__BB7_372:
	add.f64 	%fd1857, %fd1848, 0d0000000000000000;
	add.f64 	%fd1858, %fd1849, 0d0000000000000000;
	add.f64 	%fd1859, %fd1850, 0d0000000000000000;
	@%p332 bra 	$L__BB7_374;

	cvt.s64.s32 	%rd362, %r678;
	mul.lo.s64 	%rd363, %rd362, %rd58;
	add.s64 	%rd359, %rd125, %rd363;
	// begin inline asm
	{ atom.add.f64 %fd5766,[%rd359],%fd1857; }

	// end inline asm
	add.s64 	%rd360, %rd359, 8;
	// begin inline asm
	{ atom.add.f64 %fd5768,[%rd360],%fd1858; }

	// end inline asm
	add.s64 	%rd361, %rd359, 16;
	// begin inline asm
	{ atom.add.f64 %fd5770,[%rd361],%fd1859; }

	// end inline asm
	bra.uni 	$L__BB7_376;

$L__BB7_374:
	setp.eq.s64 	%p335, %rd100, 0;
	@%p335 bra 	$L__BB7_376;

	cvt.s64.s32 	%rd367, %r678;
	mul.lo.s64 	%rd368, %rd367, %rd46;
	add.s64 	%rd364, %rd100, %rd368;
	// begin inline asm
	{ atom.add.f64 %fd5772,[%rd364],%fd1857; }

	// end inline asm
	add.s64 	%rd365, %rd364, 8;
	// begin inline asm
	{ atom.add.f64 %fd5774,[%rd365],%fd1858; }

	// end inline asm
	add.s64 	%rd366, %rd364, 16;
	// begin inline asm
	{ atom.add.f64 %fd5776,[%rd366],%fd1859; }

	// end inline asm

$L__BB7_376:
	add.f64 	%fd1860, %fd1851, 0d0000000000000000;
	add.f64 	%fd1861, %fd1852, 0d0000000000000000;
	add.f64 	%fd1862, %fd1853, 0d0000000000000000;
	@%p332 bra 	$L__BB7_378;

	cvt.s64.s32 	%rd372, %r679;
	mul.lo.s64 	%rd373, %rd372, %rd58;
	add.s64 	%rd369, %rd125, %rd373;
	// begin inline asm
	{ atom.add.f64 %fd5778,[%rd369],%fd1860; }

	// end inline asm
	add.s64 	%rd370, %rd369, 8;
	// begin inline asm
	{ atom.add.f64 %fd5780,[%rd370],%fd1861; }

	// end inline asm
	add.s64 	%rd371, %rd369, 16;
	// begin inline asm
	{ atom.add.f64 %fd5782,[%rd371],%fd1862; }

	// end inline asm
	bra.uni 	$L__BB7_380;

$L__BB7_378:
	setp.eq.s64 	%p337, %rd100, 0;
	@%p337 bra 	$L__BB7_380;

	cvt.s64.s32 	%rd377, %r679;
	mul.lo.s64 	%rd378, %rd377, %rd46;
	add.s64 	%rd374, %rd100, %rd378;
	// begin inline asm
	{ atom.add.f64 %fd5784,[%rd374],%fd1860; }

	// end inline asm
	add.s64 	%rd375, %rd374, 8;
	// begin inline asm
	{ atom.add.f64 %fd5786,[%rd375],%fd1861; }

	// end inline asm
	add.s64 	%rd376, %rd374, 16;
	// begin inline asm
	{ atom.add.f64 %fd5788,[%rd376],%fd1862; }

	// end inline asm

$L__BB7_380:
	add.f64 	%fd1863, %fd1845, 0d0000000000000000;
	add.f64 	%fd1864, %fd1846, 0d0000000000000000;
	add.f64 	%fd1865, %fd1847, 0d0000000000000000;
	@%p332 bra 	$L__BB7_382;

	cvt.s64.s32 	%rd382, %r680;
	mul.lo.s64 	%rd383, %rd382, %rd58;
	add.s64 	%rd379, %rd125, %rd383;
	// begin inline asm
	{ atom.add.f64 %fd5790,[%rd379],%fd1863; }

	// end inline asm
	add.s64 	%rd380, %rd379, 8;
	// begin inline asm
	{ atom.add.f64 %fd5792,[%rd380],%fd1864; }

	// end inline asm
	add.s64 	%rd381, %rd379, 16;
	// begin inline asm
	{ atom.add.f64 %fd5794,[%rd381],%fd1865; }

	// end inline asm
	bra.uni 	$L__BB7_384;

$L__BB7_382:
	setp.eq.s64 	%p339, %rd100, 0;
	@%p339 bra 	$L__BB7_384;

	cvt.s64.s32 	%rd387, %r680;
	mul.lo.s64 	%rd388, %rd387, %rd46;
	add.s64 	%rd384, %rd100, %rd388;
	// begin inline asm
	{ atom.add.f64 %fd5796,[%rd384],%fd1863; }

	// end inline asm
	add.s64 	%rd385, %rd384, 8;
	// begin inline asm
	{ atom.add.f64 %fd5798,[%rd385],%fd1864; }

	// end inline asm
	add.s64 	%rd386, %rd384, 16;
	// begin inline asm
	{ atom.add.f64 %fd5800,[%rd386],%fd1865; }

	// end inline asm

$L__BB7_384:
	setp.eq.s64 	%p340, %rd133, 0;
	add.f64 	%fd1866, %fd1683, 0d0000000000000000;
	@%p340 bra 	$L__BB7_386;

	cvt.s64.s32 	%rd390, %r681;
	mul.lo.s64 	%rd391, %rd390, %rd62;
	add.s64 	%rd389, %rd133, %rd391;
	// begin inline asm
	{ atom.add.f64 %fd5802,[%rd389],%fd1866; }

	// end inline asm
	bra.uni 	$L__BB7_388;

$L__BB7_386:
	setp.eq.s64 	%p341, %rd110, 0;
	@%p341 bra 	$L__BB7_388;

	cvt.s64.s32 	%rd393, %r681;
	mul.lo.s64 	%rd394, %rd393, %rd44;
	add.s64 	%rd392, %rd110, %rd394;
	// begin inline asm
	{ atom.add.f64 %fd5804,[%rd392],%fd1866; }

	// end inline asm

$L__BB7_388:
	setp.eq.s64 	%p342, %rd129, 0;
	@%p342 bra 	$L__BB7_390;

	cvt.s64.s32 	%rd396, %r682;
	mul.lo.s64 	%rd397, %rd396, %rd63;
	add.s64 	%rd395, %rd129, %rd397;
	// begin inline asm
	{ atom.add.f64 %fd5806,[%rd395],%fd1866; }

	// end inline asm
	bra.uni 	$L__BB7_392;

$L__BB7_390:
	setp.eq.s64 	%p343, %rd106, 0;
	@%p343 bra 	$L__BB7_392;

	cvt.s64.s32 	%rd399, %r682;
	mul.lo.s64 	%rd400, %rd399, %rd43;
	add.s64 	%rd398, %rd106, %rd400;
	// begin inline asm
	{ atom.add.f64 %fd5808,[%rd398],%fd1866; }

	// end inline asm

$L__BB7_392:
	ld.param.u64 	%rd402, [initialize_friction_collisions_cuda_kernel_backward_param_0+24];
	mov.u32 	%r666, %ntid.x;
	mov.u32 	%r665, %nctaid.x;
	mul.wide.u32 	%rd401, %r666, %r665;
	add.s64 	%rd403, %rd403, %rd401;
	setp.lt.u64 	%p344, %rd403, %rd402;
	@%p344 bra 	$L__BB7_2;

$L__BB7_393:
	ret;

}
	// .globl	val_IPC_collisions_cuda_kernel_forward
.visible .entry val_IPC_collisions_cuda_kernel_forward(
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_11[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_12[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_13[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_14[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_15[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_16[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_17[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_18[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_19[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_20[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_21[56],
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_22,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_23,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_24,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_25,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_26,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_27,
	.param .u32 val_IPC_collisions_cuda_kernel_forward_param_28
)
{
	.local .align 16 .b8 	__local_depot8[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<108>;
	.reg .b16 	%rs<169>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<550>;
	.reg .f64 	%fd<1136>;
	.reg .b64 	%rd<260>;


	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r256, %r257}, [val_IPC_collisions_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r258, %r259}, [val_IPC_collisions_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r264, %r265}, [val_IPC_collisions_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r272, %r273}, [val_IPC_collisions_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r280, %r281}, [val_IPC_collisions_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r288, %r289}, [val_IPC_collisions_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r296, %r297}, [val_IPC_collisions_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r304, %r305}, [val_IPC_collisions_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r312, %r313}, [val_IPC_collisions_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r320, %r321}, [val_IPC_collisions_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r328, %r329}, [val_IPC_collisions_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r336, %r337}, [val_IPC_collisions_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r344, %r345}, [val_IPC_collisions_cuda_kernel_forward_param_11+32];
	ld.param.v2.u32 	{%r352, %r353}, [val_IPC_collisions_cuda_kernel_forward_param_12+32];
	ld.param.v2.u32 	{%r360, %r361}, [val_IPC_collisions_cuda_kernel_forward_param_13+32];
	ld.param.v2.u32 	{%r368, %r369}, [val_IPC_collisions_cuda_kernel_forward_param_14+32];
	ld.param.v2.u32 	{%r376, %r377}, [val_IPC_collisions_cuda_kernel_forward_param_15+32];
	ld.param.v2.u32 	{%r384, %r385}, [val_IPC_collisions_cuda_kernel_forward_param_16+32];
	ld.param.v2.u32 	{%r392, %r393}, [val_IPC_collisions_cuda_kernel_forward_param_17+32];
	ld.param.v2.u32 	{%r400, %r401}, [val_IPC_collisions_cuda_kernel_forward_param_18+32];
	ld.param.v2.u32 	{%r408, %r409}, [val_IPC_collisions_cuda_kernel_forward_param_19+32];
	ld.param.v2.u32 	{%r416, %r417}, [val_IPC_collisions_cuda_kernel_forward_param_20+32];
	ld.param.v2.u32 	{%r424, %r425}, [val_IPC_collisions_cuda_kernel_forward_param_21+32];
	ld.param.f64 	%fd192, [val_IPC_collisions_cuda_kernel_forward_param_22];
	ld.param.f64 	%fd193, [val_IPC_collisions_cuda_kernel_forward_param_23];
	ld.param.f64 	%fd194, [val_IPC_collisions_cuda_kernel_forward_param_24];
	ld.param.f64 	%fd195, [val_IPC_collisions_cuda_kernel_forward_param_25];
	ld.param.f64 	%fd196, [val_IPC_collisions_cuda_kernel_forward_param_26];
	ld.param.f64 	%fd197, [val_IPC_collisions_cuda_kernel_forward_param_27];
	ld.param.u32 	%r255, [val_IPC_collisions_cuda_kernel_forward_param_28];
	ld.param.u64 	%rd117, [val_IPC_collisions_cuda_kernel_forward_param_21];
	ld.param.u64 	%rd115, [val_IPC_collisions_cuda_kernel_forward_param_20];
	ld.param.u64 	%rd113, [val_IPC_collisions_cuda_kernel_forward_param_19];
	ld.param.u64 	%rd111, [val_IPC_collisions_cuda_kernel_forward_param_18];
	ld.param.u64 	%rd109, [val_IPC_collisions_cuda_kernel_forward_param_17];
	ld.param.u64 	%rd107, [val_IPC_collisions_cuda_kernel_forward_param_16];
	ld.param.u64 	%rd105, [val_IPC_collisions_cuda_kernel_forward_param_15];
	ld.param.u64 	%rd103, [val_IPC_collisions_cuda_kernel_forward_param_14];
	ld.param.u64 	%rd101, [val_IPC_collisions_cuda_kernel_forward_param_13];
	ld.param.u64 	%rd99, [val_IPC_collisions_cuda_kernel_forward_param_12];
	ld.param.u64 	%rd97, [val_IPC_collisions_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd95, [val_IPC_collisions_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd93, [val_IPC_collisions_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd91, [val_IPC_collisions_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd89, [val_IPC_collisions_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd87, [val_IPC_collisions_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd85, [val_IPC_collisions_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd83, [val_IPC_collisions_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd81, [val_IPC_collisions_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd79, [val_IPC_collisions_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd77, [val_IPC_collisions_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd76, [val_IPC_collisions_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r65, [val_IPC_collisions_cuda_kernel_forward_param_0+16];
	mov.u32 	%r428, %ntid.x;
	cvt.u64.u32 	%rd1, %r428;
	mov.u32 	%r429, %ctaid.x;
	mul.wide.u32 	%rd119, %r428, %r429;
	mov.u32 	%r430, %tid.x;
	cvt.u64.u32 	%rd120, %r430;
	add.s64 	%rd256, %rd119, %rd120;
	setp.ge.u64 	%p2, %rd256, %rd76;
	@%p2 bra 	$L__BB8_126;

	cvta.to.global.u64 	%rd4, %rd103;
	cvta.to.global.u64 	%rd5, %rd99;
	cvta.to.global.u64 	%rd6, %rd95;
	cvta.to.global.u64 	%rd7, %rd91;
	cvta.to.global.u64 	%rd8, %rd89;
	cvta.to.global.u64 	%rd9, %rd87;
	cvta.to.global.u64 	%rd10, %rd85;
	cvta.to.global.u64 	%rd11, %rd83;
	cvta.to.global.u64 	%rd12, %rd81;
	cvta.to.global.u64 	%rd13, %rd79;
	cvta.to.global.u64 	%rd15, %rd117;
	cvta.to.global.u64 	%rd16, %rd115;
	cvta.to.global.u64 	%rd17, %rd113;
	cvta.to.global.u64 	%rd18, %rd111;
	cvta.to.global.u64 	%rd19, %rd109;
	cvta.to.global.u64 	%rd20, %rd107;
	cvta.to.global.u64 	%rd21, %rd105;
	cvta.to.global.u64 	%rd22, %rd101;
	cvta.to.global.u64 	%rd23, %rd97;
	cvta.to.global.u64 	%rd24, %rd93;
	cvt.s64.s32 	%rd25, %r259;
	cvt.s64.s32 	%rd26, %r258;
	cvt.s64.s32 	%rd27, %r257;
	cvt.s64.s32 	%rd28, %r328;
	cvt.s64.s32 	%rd29, %r304;
	cvt.s64.s32 	%rd30, %r296;
	cvt.s64.s32 	%rd31, %r424;
	cvt.s64.s32 	%rd32, %r344;
	cvt.s64.s32 	%rd33, %r336;
	cvt.s64.s32 	%rd34, %r352;
	cvt.s64.s32 	%rd35, %r280;
	cvt.s64.s32 	%rd36, %r376;
	cvt.s64.s32 	%rd37, %r392;
	mov.u32 	%r431, %nctaid.x;
	cvt.u64.u32 	%rd121, %r431;
	mul.lo.s64 	%rd38, %rd1, %rd121;
	cvt.s64.s32 	%rd39, %r416;
	cvt.s64.s32 	%rd40, %r384;
	mul.f64 	%fd1, %fd192, %fd192;
	add.f64 	%fd2, %fd192, %fd192;
	cvt.s64.s32 	%rd41, %r400;
	mov.f64 	%fd198, 0d0000000000000000;
	sub.f64 	%fd3, %fd198, %fd194;
	cvt.s64.s32 	%rd42, %r360;
	cvt.s64.s32 	%rd43, %r312;
	cvt.s64.s32 	%rd44, %r368;
	cvt.s64.s32 	%rd45, %r320;
	cvt.s64.s32 	%rd46, %r272;
	cvt.s64.s32 	%rd47, %r288;
	cvt.s64.s32 	%rd48, %r408;
	cvt.s64.s32 	%rd49, %r264;
	mul.f64 	%fd4, %fd193, %fd197;
	mul.f64 	%fd5, %fd4, %fd4;
	div.rn.f64 	%fd6, %fd4, 0d4008000000000000;

$L__BB8_2:
	setp.lt.s32 	%p3, %r65, 4;
	mov.u64 	%rd257, %rd256;
	@%p3 bra 	$L__BB8_6;

	or.b64  	%rd124, %rd256, %rd25;
	and.b64  	%rd125, %rd124, -4294967296;
	setp.eq.s64 	%p4, %rd125, 0;
	@%p4 bra 	$L__BB8_5;

	div.u64 	%rd257, %rd256, %rd25;
	bra.uni 	$L__BB8_6;

$L__BB8_5:
	cvt.u32.u64 	%r432, %rd25;
	cvt.u32.u64 	%r433, %rd256;
	div.u32 	%r434, %r433, %r432;
	cvt.u64.u32 	%rd257, %r434;

$L__BB8_6:
	setp.lt.s32 	%p5, %r65, 3;
	@%p5 bra 	$L__BB8_10;

	or.b64  	%rd126, %rd257, %rd26;
	and.b64  	%rd127, %rd126, -4294967296;
	setp.eq.s64 	%p6, %rd127, 0;
	@%p6 bra 	$L__BB8_9;

	div.u64 	%rd257, %rd257, %rd26;
	bra.uni 	$L__BB8_10;

$L__BB8_9:
	cvt.u32.u64 	%r435, %rd26;
	cvt.u32.u64 	%r436, %rd257;
	div.u32 	%r437, %r436, %r435;
	cvt.u64.u32 	%rd257, %r437;

$L__BB8_10:
	setp.lt.s32 	%p7, %r65, 2;
	@%p7 bra 	$L__BB8_14;

	or.b64  	%rd128, %rd257, %rd27;
	and.b64  	%rd129, %rd128, -4294967296;
	setp.eq.s64 	%p8, %rd129, 0;
	@%p8 bra 	$L__BB8_13;

	div.u64 	%rd257, %rd257, %rd27;
	bra.uni 	$L__BB8_14;

$L__BB8_13:
	cvt.u32.u64 	%r438, %rd27;
	cvt.u32.u64 	%r439, %rd257;
	div.u32 	%r440, %r439, %r438;
	cvt.u64.u32 	%rd257, %r440;

$L__BB8_14:
	cvt.s64.s32 	%rd130, %rd257;
	setp.gt.s32 	%p9, %r65, 0;
	selp.b64 	%rd62, %rd130, 0, %p9;
	mul.lo.s64 	%rd131, %rd62, %rd28;
	add.s64 	%rd132, %rd24, %rd131;
	ld.global.u32 	%r2, [%rd132];
	setp.gt.u32 	%p10, %r2, 1;
	mul.lo.s64 	%rd133, %rd62, %rd29;
	add.s64 	%rd63, %rd9, %rd133;
	@%p10 bra 	$L__BB8_59;

	mul.lo.s64 	%rd134, %rd62, %rd30;
	add.s64 	%rd135, %rd10, %rd134;
	ld.global.u32 	%r442, [%rd135];
	ld.global.u32 	%r443, [%rd63];
	setp.eq.s32 	%p11, %r2, 1;
	selp.b32 	%r444, %r442, %r443, %p11;
	selp.b32 	%r445, %r443, %r442, %p11;
	cvt.s64.s32 	%rd136, %r445;
	mul.lo.s64 	%rd137, %rd136, %rd31;
	add.s64 	%rd138, %rd15, %rd137;
	cvt.s64.s32 	%rd64, %r444;
	mul.lo.s64 	%rd139, %rd64, %rd32;
	add.s64 	%rd140, %rd23, %rd139;
	ld.global.s32 	%rd65, [%rd140];
	mul.lo.s64 	%rd141, %rd65, %rd33;
	add.s64 	%rd142, %rd6, %rd141;
	ld.global.s32 	%rd143, [%rd138];
	mul.lo.s64 	%rd144, %rd143, %rd33;
	add.s64 	%rd145, %rd6, %rd144;
	ld.global.s32 	%rd146, [%rd138+4];
	mul.lo.s64 	%rd147, %rd146, %rd33;
	add.s64 	%rd148, %rd6, %rd147;
	ld.global.s32 	%rd149, [%rd138+8];
	mul.lo.s64 	%rd150, %rd149, %rd33;
	add.s64 	%rd151, %rd6, %rd150;
	mul.lo.s64 	%rd152, %rd65, %rd34;
	add.s64 	%rd153, %rd5, %rd152;
	ld.global.f64 	%fd7, [%rd153];
	ld.global.f64 	%fd8, [%rd153+8];
	ld.global.f64 	%fd9, [%rd153+16];
	mul.lo.s64 	%rd154, %rd143, %rd34;
	add.s64 	%rd155, %rd5, %rd154;
	ld.global.f64 	%fd10, [%rd155];
	ld.global.f64 	%fd11, [%rd155+8];
	ld.global.f64 	%fd12, [%rd155+16];
	mul.lo.s64 	%rd156, %rd146, %rd34;
	add.s64 	%rd157, %rd5, %rd156;
	ld.global.f64 	%fd13, [%rd157];
	ld.global.f64 	%fd14, [%rd157+8];
	ld.global.f64 	%fd15, [%rd157+16];
	mul.lo.s64 	%rd158, %rd149, %rd34;
	add.s64 	%rd159, %rd5, %rd158;
	ld.global.f64 	%fd16, [%rd159];
	ld.global.f64 	%fd17, [%rd159+8];
	ld.global.f64 	%fd18, [%rd159+16];
	mul.lo.s64 	%rd160, %rd62, %rd35;
	add.s64 	%rd161, %rd12, %rd160;
	ld.global.f64 	%fd19, [%rd161];
	ld.global.f64 	%fd20, [%rd161+8];
	mul.lo.s64 	%rd162, %rd64, %rd36;
	add.s64 	%rd163, %rd21, %rd162;
	mul.lo.s64 	%rd164, %rd136, %rd37;
	add.s64 	%rd165, %rd19, %rd164;
	ld.global.f64 	%fd199, [%rd165];
	ld.global.f64 	%fd200, [%rd163];
	add.f64 	%fd21, %fd200, %fd199;
	ld.global.f64 	%fd22, [%rd148];
	ld.global.f64 	%fd23, [%rd145];
	sub.f64 	%fd201, %fd22, %fd23;
	ld.global.f64 	%fd24, [%rd148+8];
	ld.global.f64 	%fd25, [%rd145+8];
	sub.f64 	%fd202, %fd24, %fd25;
	ld.global.f64 	%fd26, [%rd148+16];
	ld.global.f64 	%fd27, [%rd145+16];
	sub.f64 	%fd203, %fd26, %fd27;
	ld.global.f64 	%fd28, [%rd151];
	sub.f64 	%fd204, %fd28, %fd23;
	ld.global.f64 	%fd29, [%rd151+8];
	sub.f64 	%fd205, %fd29, %fd25;
	ld.global.f64 	%fd30, [%rd151+16];
	sub.f64 	%fd206, %fd30, %fd27;
	mul.f64 	%fd207, %fd202, %fd206;
	mul.f64 	%fd208, %fd203, %fd205;
	sub.f64 	%fd31, %fd207, %fd208;
	mul.f64 	%fd209, %fd203, %fd204;
	mul.f64 	%fd210, %fd201, %fd206;
	sub.f64 	%fd32, %fd209, %fd210;
	mul.f64 	%fd211, %fd201, %fd205;
	mul.f64 	%fd212, %fd202, %fd204;
	sub.f64 	%fd33, %fd211, %fd212;
	mul.f64 	%fd213, %fd202, %fd33;
	mul.f64 	%fd214, %fd203, %fd32;
	sub.f64 	%fd215, %fd213, %fd214;
	mul.f64 	%fd216, %fd203, %fd31;
	mul.f64 	%fd217, %fd201, %fd33;
	sub.f64 	%fd218, %fd216, %fd217;
	mul.f64 	%fd219, %fd201, %fd32;
	mul.f64 	%fd220, %fd202, %fd31;
	sub.f64 	%fd221, %fd219, %fd220;
	mul.f64 	%fd222, %fd202, %fd202;
	fma.rn.f64 	%fd223, %fd201, %fd201, %fd222;
	fma.rn.f64 	%fd34, %fd203, %fd203, %fd223;
	mul.f64 	%fd224, %fd202, %fd218;
	fma.rn.f64 	%fd225, %fd201, %fd215, %fd224;
	fma.rn.f64 	%fd226, %fd203, %fd221, %fd225;
	mul.f64 	%fd227, %fd218, %fd218;
	fma.rn.f64 	%fd228, %fd215, %fd215, %fd227;
	fma.rn.f64 	%fd229, %fd221, %fd221, %fd228;
	ld.global.f64 	%fd35, [%rd142];
	sub.f64 	%fd36, %fd35, %fd23;
	ld.global.f64 	%fd37, [%rd142+8];
	sub.f64 	%fd38, %fd37, %fd25;
	ld.global.f64 	%fd39, [%rd142+16];
	sub.f64 	%fd40, %fd39, %fd27;
	mul.f64 	%fd230, %fd38, %fd202;
	fma.rn.f64 	%fd231, %fd36, %fd201, %fd230;
	fma.rn.f64 	%fd232, %fd40, %fd203, %fd231;
	mul.f64 	%fd233, %fd38, %fd218;
	fma.rn.f64 	%fd234, %fd36, %fd215, %fd233;
	fma.rn.f64 	%fd235, %fd40, %fd221, %fd234;
	div.rn.f64 	%fd236, %fd226, %fd34;
	mul.f64 	%fd237, %fd236, %fd236;
	mul.f64 	%fd238, %fd34, %fd237;
	sub.f64 	%fd239, %fd229, %fd238;
	mul.f64 	%fd240, %fd232, %fd236;
	sub.f64 	%fd241, %fd235, %fd240;
	div.rn.f64 	%fd242, %fd241, %fd239;
	mul.f64 	%fd243, %fd34, %fd236;
	mul.f64 	%fd244, %fd243, %fd242;
	sub.f64 	%fd245, %fd232, %fd244;
	div.rn.f64 	%fd41, %fd245, %fd34;
	setp.gt.f64 	%p12, %fd41, 0d0000000000000000;
	setp.lt.f64 	%p13, %fd41, 0d3FF0000000000000;
	setp.ge.f64 	%p14, %fd242, 0d0000000000000000;
	and.pred  	%p15, %p12, %p13;
	and.pred  	%p16, %p14, %p15;
	mov.u32 	%r526, 3;
	@%p16 bra 	$L__BB8_21;

	sub.f64 	%fd246, %fd28, %fd22;
	sub.f64 	%fd247, %fd29, %fd24;
	mul.f64 	%fd248, %fd247, %fd33;
	sub.f64 	%fd249, %fd30, %fd26;
	mul.f64 	%fd250, %fd249, %fd32;
	sub.f64 	%fd251, %fd248, %fd250;
	mul.f64 	%fd252, %fd249, %fd31;
	mul.f64 	%fd253, %fd246, %fd33;
	sub.f64 	%fd254, %fd252, %fd253;
	mul.f64 	%fd255, %fd246, %fd32;
	mul.f64 	%fd256, %fd247, %fd31;
	sub.f64 	%fd257, %fd255, %fd256;
	mul.f64 	%fd258, %fd247, %fd247;
	fma.rn.f64 	%fd259, %fd246, %fd246, %fd258;
	fma.rn.f64 	%fd260, %fd249, %fd249, %fd259;
	mul.f64 	%fd261, %fd247, %fd254;
	fma.rn.f64 	%fd262, %fd246, %fd251, %fd261;
	fma.rn.f64 	%fd263, %fd249, %fd257, %fd262;
	mul.f64 	%fd264, %fd254, %fd254;
	fma.rn.f64 	%fd265, %fd251, %fd251, %fd264;
	fma.rn.f64 	%fd266, %fd257, %fd257, %fd265;
	sub.f64 	%fd267, %fd35, %fd22;
	sub.f64 	%fd268, %fd37, %fd24;
	mul.f64 	%fd269, %fd268, %fd247;
	fma.rn.f64 	%fd270, %fd267, %fd246, %fd269;
	sub.f64 	%fd271, %fd39, %fd26;
	fma.rn.f64 	%fd272, %fd271, %fd249, %fd270;
	mul.f64 	%fd273, %fd268, %fd254;
	fma.rn.f64 	%fd274, %fd267, %fd251, %fd273;
	fma.rn.f64 	%fd275, %fd271, %fd257, %fd274;
	div.rn.f64 	%fd276, %fd263, %fd260;
	mul.f64 	%fd277, %fd276, %fd276;
	mul.f64 	%fd278, %fd260, %fd277;
	sub.f64 	%fd279, %fd266, %fd278;
	mul.f64 	%fd280, %fd272, %fd276;
	sub.f64 	%fd281, %fd275, %fd280;
	div.rn.f64 	%fd282, %fd281, %fd279;
	mul.f64 	%fd283, %fd260, %fd276;
	mul.f64 	%fd284, %fd283, %fd282;
	sub.f64 	%fd285, %fd272, %fd284;
	div.rn.f64 	%fd42, %fd285, %fd260;
	setp.gt.f64 	%p17, %fd42, 0d0000000000000000;
	setp.lt.f64 	%p18, %fd42, 0d3FF0000000000000;
	setp.ge.f64 	%p19, %fd282, 0d0000000000000000;
	and.pred  	%p20, %p17, %p18;
	and.pred  	%p21, %p19, %p20;
	mov.u32 	%r526, 4;
	@%p21 bra 	$L__BB8_21;

	sub.f64 	%fd286, %fd23, %fd28;
	sub.f64 	%fd287, %fd25, %fd29;
	mul.f64 	%fd288, %fd287, %fd33;
	sub.f64 	%fd289, %fd27, %fd30;
	mul.f64 	%fd290, %fd289, %fd32;
	sub.f64 	%fd291, %fd288, %fd290;
	mul.f64 	%fd292, %fd289, %fd31;
	mul.f64 	%fd293, %fd286, %fd33;
	sub.f64 	%fd294, %fd292, %fd293;
	mul.f64 	%fd295, %fd286, %fd32;
	mul.f64 	%fd296, %fd287, %fd31;
	sub.f64 	%fd297, %fd295, %fd296;
	mul.f64 	%fd298, %fd287, %fd287;
	fma.rn.f64 	%fd299, %fd286, %fd286, %fd298;
	fma.rn.f64 	%fd300, %fd289, %fd289, %fd299;
	mul.f64 	%fd301, %fd287, %fd294;
	fma.rn.f64 	%fd302, %fd286, %fd291, %fd301;
	fma.rn.f64 	%fd303, %fd289, %fd297, %fd302;
	mul.f64 	%fd304, %fd294, %fd294;
	fma.rn.f64 	%fd305, %fd291, %fd291, %fd304;
	fma.rn.f64 	%fd306, %fd297, %fd297, %fd305;
	sub.f64 	%fd307, %fd35, %fd28;
	sub.f64 	%fd308, %fd37, %fd29;
	mul.f64 	%fd309, %fd287, %fd308;
	fma.rn.f64 	%fd310, %fd286, %fd307, %fd309;
	sub.f64 	%fd311, %fd39, %fd30;
	fma.rn.f64 	%fd312, %fd289, %fd311, %fd310;
	mul.f64 	%fd313, %fd308, %fd294;
	fma.rn.f64 	%fd314, %fd307, %fd291, %fd313;
	fma.rn.f64 	%fd315, %fd311, %fd297, %fd314;
	div.rn.f64 	%fd316, %fd303, %fd300;
	mul.f64 	%fd317, %fd316, %fd316;
	mul.f64 	%fd318, %fd300, %fd317;
	sub.f64 	%fd319, %fd306, %fd318;
	mul.f64 	%fd320, %fd312, %fd316;
	sub.f64 	%fd321, %fd315, %fd320;
	div.rn.f64 	%fd322, %fd321, %fd319;
	mul.f64 	%fd323, %fd300, %fd316;
	mul.f64 	%fd324, %fd323, %fd322;
	sub.f64 	%fd325, %fd312, %fd324;
	div.rn.f64 	%fd43, %fd325, %fd300;
	setp.gt.f64 	%p22, %fd43, 0d0000000000000000;
	setp.lt.f64 	%p23, %fd43, 0d3FF0000000000000;
	setp.ge.f64 	%p24, %fd322, 0d0000000000000000;
	and.pred  	%p25, %p22, %p23;
	and.pred  	%p26, %p24, %p25;
	mov.u32 	%r526, 5;
	@%p26 bra 	$L__BB8_21;

	setp.le.f64 	%p27, %fd41, 0d0000000000000000;
	setp.ge.f64 	%p28, %fd43, 0d3FF0000000000000;
	and.pred  	%p29, %p27, %p28;
	mov.u32 	%r526, 0;
	@%p29 bra 	$L__BB8_21;

	setp.le.f64 	%p30, %fd42, 0d0000000000000000;
	setp.ge.f64 	%p31, %fd41, 0d3FF0000000000000;
	and.pred  	%p32, %p30, %p31;
	mov.u32 	%r526, 1;
	@%p32 bra 	$L__BB8_21;

	setp.le.f64 	%p33, %fd43, 0d0000000000000000;
	setp.ge.f64 	%p34, %fd42, 0d3FF0000000000000;
	and.pred  	%p35, %p33, %p34;
	selp.b32 	%r526, 2, 6, %p35;

$L__BB8_21:
	setp.eq.s32 	%p36, %r526, 0;
	@%p36 bra 	$L__BB8_33;

	setp.eq.s32 	%p37, %r526, 1;
	@%p37 bra 	$L__BB8_32;
	bra.uni 	$L__BB8_23;

$L__BB8_32:
	sub.f64 	%fd404, %fd35, %fd22;
	sub.f64 	%fd405, %fd37, %fd24;
	mul.f64 	%fd406, %fd405, %fd405;
	fma.rn.f64 	%fd407, %fd404, %fd404, %fd406;
	sub.f64 	%fd408, %fd39, %fd26;
	fma.rn.f64 	%fd1107, %fd408, %fd408, %fd407;
	bra.uni 	$L__BB8_34;

$L__BB8_33:
	sub.f64 	%fd1079, %fd39, %fd27;
	sub.f64 	%fd1078, %fd35, %fd23;
	sub.f64 	%fd1077, %fd37, %fd25;
	mul.f64 	%fd409, %fd1077, %fd1077;
	fma.rn.f64 	%fd410, %fd1078, %fd1078, %fd409;
	fma.rn.f64 	%fd1107, %fd1079, %fd1079, %fd410;
	bra.uni 	$L__BB8_34;

$L__BB8_23:
	setp.eq.s32 	%p38, %r526, 2;
	@%p38 bra 	$L__BB8_31;
	bra.uni 	$L__BB8_24;

$L__BB8_31:
	sub.f64 	%fd399, %fd35, %fd28;
	sub.f64 	%fd400, %fd37, %fd29;
	mul.f64 	%fd401, %fd400, %fd400;
	fma.rn.f64 	%fd402, %fd399, %fd399, %fd401;
	sub.f64 	%fd403, %fd39, %fd30;
	fma.rn.f64 	%fd1107, %fd403, %fd403, %fd402;
	bra.uni 	$L__BB8_34;

$L__BB8_24:
	setp.eq.s32 	%p39, %r526, 3;
	@%p39 bra 	$L__BB8_30;
	bra.uni 	$L__BB8_25;

$L__BB8_30:
	sub.f64 	%fd381, %fd23, %fd35;
	sub.f64 	%fd382, %fd26, %fd39;
	sub.f64 	%fd383, %fd25, %fd37;
	mul.f64 	%fd384, %fd383, %fd382;
	sub.f64 	%fd385, %fd24, %fd37;
	sub.f64 	%fd386, %fd27, %fd39;
	mul.f64 	%fd387, %fd386, %fd385;
	sub.f64 	%fd388, %fd384, %fd387;
	sub.f64 	%fd389, %fd22, %fd35;
	mul.f64 	%fd390, %fd386, %fd389;
	mul.f64 	%fd391, %fd381, %fd382;
	sub.f64 	%fd392, %fd390, %fd391;
	mul.f64 	%fd393, %fd381, %fd385;
	mul.f64 	%fd394, %fd383, %fd389;
	sub.f64 	%fd395, %fd393, %fd394;
	mul.f64 	%fd396, %fd392, %fd392;
	fma.rn.f64 	%fd397, %fd388, %fd388, %fd396;
	fma.rn.f64 	%fd398, %fd395, %fd395, %fd397;
	div.rn.f64 	%fd1107, %fd398, %fd34;
	bra.uni 	$L__BB8_34;

$L__BB8_25:
	setp.eq.s32 	%p40, %r526, 4;
	@%p40 bra 	$L__BB8_29;
	bra.uni 	$L__BB8_26;

$L__BB8_29:
	sub.f64 	%fd357, %fd22, %fd35;
	sub.f64 	%fd358, %fd30, %fd39;
	sub.f64 	%fd359, %fd24, %fd37;
	mul.f64 	%fd360, %fd359, %fd358;
	sub.f64 	%fd361, %fd29, %fd37;
	sub.f64 	%fd362, %fd26, %fd39;
	mul.f64 	%fd363, %fd362, %fd361;
	sub.f64 	%fd364, %fd360, %fd363;
	sub.f64 	%fd365, %fd28, %fd35;
	mul.f64 	%fd366, %fd362, %fd365;
	mul.f64 	%fd367, %fd357, %fd358;
	sub.f64 	%fd368, %fd366, %fd367;
	mul.f64 	%fd369, %fd357, %fd361;
	mul.f64 	%fd370, %fd359, %fd365;
	sub.f64 	%fd371, %fd369, %fd370;
	mul.f64 	%fd372, %fd368, %fd368;
	fma.rn.f64 	%fd373, %fd364, %fd364, %fd372;
	fma.rn.f64 	%fd374, %fd371, %fd371, %fd373;
	sub.f64 	%fd375, %fd28, %fd22;
	sub.f64 	%fd376, %fd29, %fd24;
	mul.f64 	%fd377, %fd376, %fd376;
	fma.rn.f64 	%fd378, %fd375, %fd375, %fd377;
	sub.f64 	%fd379, %fd30, %fd26;
	fma.rn.f64 	%fd380, %fd379, %fd379, %fd378;
	div.rn.f64 	%fd1107, %fd374, %fd380;
	bra.uni 	$L__BB8_34;

$L__BB8_26:
	setp.eq.s32 	%p41, %r526, 5;
	@%p41 bra 	$L__BB8_28;
	bra.uni 	$L__BB8_27;

$L__BB8_28:
	sub.f64 	%fd333, %fd28, %fd35;
	sub.f64 	%fd334, %fd27, %fd39;
	sub.f64 	%fd335, %fd29, %fd37;
	mul.f64 	%fd336, %fd334, %fd335;
	sub.f64 	%fd337, %fd25, %fd37;
	sub.f64 	%fd338, %fd30, %fd39;
	mul.f64 	%fd339, %fd337, %fd338;
	sub.f64 	%fd340, %fd336, %fd339;
	sub.f64 	%fd341, %fd23, %fd35;
	mul.f64 	%fd342, %fd341, %fd338;
	mul.f64 	%fd343, %fd334, %fd333;
	sub.f64 	%fd344, %fd342, %fd343;
	mul.f64 	%fd345, %fd337, %fd333;
	mul.f64 	%fd346, %fd341, %fd335;
	sub.f64 	%fd347, %fd345, %fd346;
	mul.f64 	%fd348, %fd344, %fd344;
	fma.rn.f64 	%fd349, %fd340, %fd340, %fd348;
	fma.rn.f64 	%fd350, %fd347, %fd347, %fd349;
	sub.f64 	%fd351, %fd23, %fd28;
	sub.f64 	%fd352, %fd25, %fd29;
	mul.f64 	%fd353, %fd352, %fd352;
	fma.rn.f64 	%fd354, %fd351, %fd351, %fd353;
	sub.f64 	%fd355, %fd27, %fd30;
	fma.rn.f64 	%fd356, %fd355, %fd355, %fd354;
	div.rn.f64 	%fd1107, %fd350, %fd356;
	bra.uni 	$L__BB8_34;

$L__BB8_27:
	sub.f64 	%fd1076, %fd39, %fd27;
	sub.f64 	%fd1075, %fd35, %fd23;
	sub.f64 	%fd1074, %fd37, %fd25;
	mul.f64 	%fd326, %fd1074, %fd32;
	fma.rn.f64 	%fd327, %fd1075, %fd31, %fd326;
	fma.rn.f64 	%fd328, %fd1076, %fd33, %fd327;
	mul.f64 	%fd329, %fd328, %fd328;
	mul.f64 	%fd330, %fd32, %fd32;
	fma.rn.f64 	%fd331, %fd31, %fd31, %fd330;
	fma.rn.f64 	%fd332, %fd33, %fd33, %fd331;
	div.rn.f64 	%fd1107, %fd329, %fd332;

$L__BB8_34:
	mul.f64 	%fd412, %fd21, %fd21;
	sub.f64 	%fd52, %fd1107, %fd412;
	mov.f64 	%fd413, 0d3FF0000000000000;
	sub.f64 	%fd414, %fd413, %fd19;
	sub.f64 	%fd415, %fd414, %fd20;
	mul.f64 	%fd416, %fd23, %fd415;
	mul.f64 	%fd417, %fd25, %fd415;
	mul.f64 	%fd418, %fd27, %fd415;
	sub.f64 	%fd419, %fd35, %fd416;
	sub.f64 	%fd420, %fd37, %fd417;
	sub.f64 	%fd421, %fd39, %fd418;
	mul.f64 	%fd422, %fd22, %fd19;
	sub.f64 	%fd423, %fd419, %fd422;
	mul.f64 	%fd424, %fd24, %fd19;
	sub.f64 	%fd425, %fd420, %fd424;
	mul.f64 	%fd426, %fd26, %fd19;
	sub.f64 	%fd427, %fd421, %fd426;
	mul.f64 	%fd428, %fd28, %fd20;
	sub.f64 	%fd53, %fd423, %fd428;
	mul.f64 	%fd429, %fd29, %fd20;
	sub.f64 	%fd54, %fd425, %fd429;
	mul.f64 	%fd430, %fd30, %fd20;
	sub.f64 	%fd55, %fd427, %fd430;
	mul.f64 	%fd431, %fd10, %fd415;
	mul.f64 	%fd432, %fd11, %fd415;
	mul.f64 	%fd433, %fd12, %fd415;
	sub.f64 	%fd434, %fd7, %fd431;
	sub.f64 	%fd435, %fd8, %fd432;
	sub.f64 	%fd436, %fd9, %fd433;
	mul.f64 	%fd437, %fd13, %fd19;
	sub.f64 	%fd438, %fd434, %fd437;
	mul.f64 	%fd439, %fd14, %fd19;
	sub.f64 	%fd440, %fd435, %fd439;
	mul.f64 	%fd441, %fd15, %fd19;
	sub.f64 	%fd442, %fd436, %fd441;
	mul.f64 	%fd443, %fd16, %fd20;
	sub.f64 	%fd56, %fd438, %fd443;
	mul.f64 	%fd444, %fd17, %fd20;
	sub.f64 	%fd57, %fd440, %fd444;
	mul.f64 	%fd445, %fd18, %fd20;
	sub.f64 	%fd58, %fd442, %fd445;
	fma.rn.f64 	%fd59, %fd2, %fd21, %fd1;
	mul.lo.s64 	%rd166, %rd64, %rd41;
	add.s64 	%rd66, %rd18, %rd166;
	ld.global.f64 	%fd60, [%rd66];
	setp.geu.f64 	%p42, %fd52, %fd59;
	@%p42 bra 	$L__BB8_43;

	div.rn.f64 	%fd1108, %fd52, %fd59;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r527}, %fd1108;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r528, %temp}, %fd1108;
	}
	setp.gt.s32 	%p43, %r527, 1048575;
	mov.u32 	%r529, -1023;
	@%p43 bra 	$L__BB8_37;

	mul.f64 	%fd1108, %fd1108, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r527}, %fd1108;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r528, %temp}, %fd1108;
	}
	mov.u32 	%r529, -1077;

$L__BB8_37:
	add.s32 	%r452, %r527, -1;
	setp.lt.u32 	%p44, %r452, 2146435071;
	@%p44 bra 	$L__BB8_39;
	bra.uni 	$L__BB8_38;

$L__BB8_39:
	shr.u32 	%r454, %r527, 20;
	add.s32 	%r530, %r529, %r454;
	and.b32  	%r455, %r527, -2146435073;
	or.b32  	%r456, %r455, 1072693248;
	mov.b64 	%fd1109, {%r528, %r456};
	setp.lt.s32 	%p46, %r456, 1073127583;
	@%p46 bra 	$L__BB8_41;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r457, %temp}, %fd1109;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r458}, %fd1109;
	}
	add.s32 	%r459, %r458, -1048576;
	mov.b64 	%fd1109, {%r457, %r459};
	add.s32 	%r530, %r530, 1;

$L__BB8_41:
	add.f64 	%fd448, %fd1109, 0d3FF0000000000000;
	mov.f64 	%fd449, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd450, %fd448;
	neg.f64 	%fd451, %fd448;
	fma.rn.f64 	%fd452, %fd451, %fd450, %fd449;
	fma.rn.f64 	%fd453, %fd452, %fd452, %fd452;
	fma.rn.f64 	%fd454, %fd453, %fd450, %fd450;
	add.f64 	%fd455, %fd1109, 0dBFF0000000000000;
	mul.f64 	%fd456, %fd455, %fd454;
	fma.rn.f64 	%fd457, %fd455, %fd454, %fd456;
	mul.f64 	%fd458, %fd457, %fd457;
	mov.f64 	%fd459, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd460, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd461, %fd460, %fd458, %fd459;
	mov.f64 	%fd462, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd463, %fd461, %fd458, %fd462;
	mov.f64 	%fd464, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd465, %fd463, %fd458, %fd464;
	mov.f64 	%fd466, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd467, %fd465, %fd458, %fd466;
	mov.f64 	%fd468, 0d3F624924923BE72D;
	fma.rn.f64 	%fd469, %fd467, %fd458, %fd468;
	mov.f64 	%fd470, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd471, %fd469, %fd458, %fd470;
	mov.f64 	%fd472, 0d3FB5555555555554;
	fma.rn.f64 	%fd473, %fd471, %fd458, %fd472;
	sub.f64 	%fd474, %fd455, %fd457;
	add.f64 	%fd475, %fd474, %fd474;
	neg.f64 	%fd476, %fd457;
	fma.rn.f64 	%fd477, %fd476, %fd455, %fd475;
	mul.f64 	%fd478, %fd454, %fd477;
	mul.f64 	%fd479, %fd458, %fd473;
	fma.rn.f64 	%fd480, %fd479, %fd457, %fd478;
	xor.b32  	%r460, %r530, -2147483648;
	mov.u32 	%r461, -2147483648;
	mov.u32 	%r462, 1127219200;
	mov.b64 	%fd481, {%r460, %r462};
	mov.b64 	%fd482, {%r461, %r462};
	sub.f64 	%fd483, %fd481, %fd482;
	mov.f64 	%fd484, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd485, %fd483, %fd484, %fd457;
	neg.f64 	%fd486, %fd483;
	fma.rn.f64 	%fd487, %fd486, %fd484, %fd485;
	sub.f64 	%fd488, %fd487, %fd457;
	sub.f64 	%fd489, %fd480, %fd488;
	mov.f64 	%fd490, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd491, %fd483, %fd490, %fd489;
	add.f64 	%fd1110, %fd485, %fd491;
	bra.uni 	$L__BB8_42;

$L__BB8_38:
	mov.f64 	%fd446, 0d7FF0000000000000;
	fma.rn.f64 	%fd447, %fd1108, %fd446, %fd446;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r453}, %fd1108;
	}
	mov.b32 	%f1, %r453;
	setp.eq.f32 	%p45, %f1, 0f00000000;
	selp.f64 	%fd1110, 0dFFF0000000000000, %fd447, %p45;

$L__BB8_42:
	sub.f64 	%fd492, %fd52, %fd59;
	div.rn.f64 	%fd493, %fd492, %fd59;
	mul.f64 	%fd494, %fd3, %fd493;
	mul.f64 	%fd495, %fd493, %fd494;
	mul.f64 	%fd1111, %fd495, %fd1110;

$L__BB8_43:
	setp.lt.f64 	%p47, %fd52, %fd59;
	selp.f64 	%fd496, %fd1111, 0d0000000000000000, %p47;
	mul.f64 	%fd497, %fd60, %fd195;
	mul.f64 	%fd498, %fd497, %fd192;
	mul.f64 	%fd499, %fd498, %fd496;
	mul.f64 	%fd500, %fd499, %fd196;
	setp.num.f64 	%p48, %fd500, %fd500;
	@%p48 bra 	$L__BB8_45;

	add.u64 	%rd251, %SP, 0;
	add.u64 	%rd250, %SP, 0;
	add.u64 	%rd249, %SPL, 0;
	mov.u64 	%rd167, $str$3;
	cvta.global.u64 	%rd168, %rd167;
	st.local.u64 	[%rd249], %rd168;
	mov.u64 	%rd169, $str$5;
	cvta.global.u64 	%rd170, %rd169;
	{ // callseq 508, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd170;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd250;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r463, [retval0+0];
	} // callseq 508

$L__BB8_45:
	setp.eq.s32 	%p49, %r255, 0;
	@%p49 bra 	$L__BB8_49;

	mul.lo.s64 	%rd172, %rd62, %rd43;
	add.s64 	%rd173, %rd8, %rd172;
	ld.global.s32 	%rd174, [%rd173];
	mul.lo.s64 	%rd175, %rd174, %rd44;
	add.s64 	%rd176, %rd4, %rd175;
	mul.lo.s64 	%rd177, %rd62, %rd45;
	add.s64 	%rd178, %rd7, %rd177;
	ld.global.s32 	%rd179, [%rd178];
	mul.lo.s64 	%rd180, %rd179, %rd44;
	add.s64 	%rd181, %rd4, %rd180;
	ld.global.f64 	%fd501, [%rd176];
	add.f64 	%fd502, %fd501, %fd501;
	ld.global.f64 	%fd503, [%rd181];
	mul.f64 	%fd504, %fd502, %fd503;
	add.f64 	%fd505, %fd501, %fd503;
	setp.neu.f64 	%p50, %fd505, 0d0000000000000000;
	mov.f64 	%fd506, 0d0000000000000000;
	div.rn.f64 	%fd507, %fd504, %fd505;
	selp.f64 	%fd508, %fd507, 0d0000000000000000, %p50;
	mul.lo.s64 	%rd182, %rd62, %rd46;
	add.s64 	%rd183, %rd13, %rd182;
	ld.global.f64 	%fd509, [%rd183];
	mul.f64 	%fd72, %fd509, %fd508;
	mul.lo.s64 	%rd184, %rd62, %rd47;
	add.s64 	%rd185, %rd11, %rd184;
	ld.global.f64 	%fd510, [%rd185];
	mul.f64 	%fd511, %fd510, %fd510;
	ld.global.f64 	%fd512, [%rd185+8];
	mul.f64 	%fd513, %fd510, %fd512;
	ld.global.f64 	%fd514, [%rd185+16];
	mul.f64 	%fd515, %fd510, %fd514;
	mul.f64 	%fd516, %fd512, %fd512;
	mul.f64 	%fd517, %fd512, %fd514;
	mul.f64 	%fd518, %fd514, %fd514;
	mov.f64 	%fd519, 0d3FF0000000000000;
	sub.f64 	%fd520, %fd519, %fd511;
	sub.f64 	%fd521, %fd506, %fd513;
	sub.f64 	%fd522, %fd506, %fd515;
	sub.f64 	%fd523, %fd519, %fd516;
	sub.f64 	%fd524, %fd506, %fd517;
	sub.f64 	%fd525, %fd519, %fd518;
	sub.f64 	%fd526, %fd53, %fd56;
	sub.f64 	%fd527, %fd54, %fd57;
	mul.f64 	%fd528, %fd521, %fd527;
	mul.f64 	%fd529, %fd523, %fd527;
	mul.f64 	%fd530, %fd524, %fd527;
	fma.rn.f64 	%fd531, %fd520, %fd526, %fd528;
	fma.rn.f64 	%fd532, %fd521, %fd526, %fd529;
	fma.rn.f64 	%fd533, %fd522, %fd526, %fd530;
	sub.f64 	%fd534, %fd55, %fd58;
	fma.rn.f64 	%fd535, %fd522, %fd534, %fd531;
	fma.rn.f64 	%fd536, %fd524, %fd534, %fd532;
	fma.rn.f64 	%fd537, %fd525, %fd534, %fd533;
	div.rn.f64 	%fd538, %fd535, %fd193;
	div.rn.f64 	%fd539, %fd536, %fd193;
	div.rn.f64 	%fd540, %fd537, %fd193;
	mul.f64 	%fd541, %fd539, %fd539;
	fma.rn.f64 	%fd542, %fd538, %fd538, %fd541;
	fma.rn.f64 	%fd543, %fd540, %fd540, %fd542;
	sqrt.rn.f64 	%fd544, %fd543;
	setp.ge.f64 	%p51, %fd544, %fd197;
	mul.f64 	%fd1112, %fd544, %fd193;
	@%p51 bra 	$L__BB8_48;

	mul.f64 	%fd545, %fd1112, %fd1112;
	sub.f64 	%fd547, %fd506, %fd1112;
	div.rn.f64 	%fd548, %fd547, 0d4008000000000000;
	add.f64 	%fd549, %fd4, %fd548;
	mul.f64 	%fd550, %fd545, %fd549;
	div.rn.f64 	%fd551, %fd550, %fd5;
	add.f64 	%fd1112, %fd6, %fd551;

$L__BB8_48:
	mul.lo.s64 	%rd187, %rd65, %rd49;
	add.s64 	%rd186, %rd77, %rd187;
	mul.f64 	%fd554, %fd72, %fd196;
	mul.f64 	%fd553, %fd554, %fd1112;
	// begin inline asm
	{ atom.add.f64 %fd552,[%rd186],%fd553; }

	// end inline asm
	bra.uni 	$L__BB8_59;

$L__BB8_49:
	ld.global.f64 	%fd76, [%rd66];
	@%p42 bra 	$L__BB8_58;

	div.rn.f64 	%fd1113, %fd52, %fd59;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r531}, %fd1113;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r532, %temp}, %fd1113;
	}
	setp.gt.s32 	%p53, %r531, 1048575;
	mov.u32 	%r533, -1023;
	@%p53 bra 	$L__BB8_52;

	mul.f64 	%fd1113, %fd1113, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r531}, %fd1113;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r532, %temp}, %fd1113;
	}
	mov.u32 	%r533, -1077;

$L__BB8_52:
	add.s32 	%r466, %r531, -1;
	setp.lt.u32 	%p54, %r466, 2146435071;
	@%p54 bra 	$L__BB8_54;
	bra.uni 	$L__BB8_53;

$L__BB8_54:
	shr.u32 	%r468, %r531, 20;
	add.s32 	%r534, %r533, %r468;
	and.b32  	%r469, %r531, -2146435073;
	or.b32  	%r470, %r469, 1072693248;
	mov.b64 	%fd1114, {%r532, %r470};
	setp.lt.s32 	%p56, %r470, 1073127583;
	@%p56 bra 	$L__BB8_56;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r471, %temp}, %fd1114;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r472}, %fd1114;
	}
	add.s32 	%r473, %r472, -1048576;
	mov.b64 	%fd1114, {%r471, %r473};
	add.s32 	%r534, %r534, 1;

$L__BB8_56:
	add.f64 	%fd558, %fd1114, 0d3FF0000000000000;
	mov.f64 	%fd559, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd560, %fd558;
	neg.f64 	%fd561, %fd558;
	fma.rn.f64 	%fd562, %fd561, %fd560, %fd559;
	fma.rn.f64 	%fd563, %fd562, %fd562, %fd562;
	fma.rn.f64 	%fd564, %fd563, %fd560, %fd560;
	add.f64 	%fd565, %fd1114, 0dBFF0000000000000;
	mul.f64 	%fd566, %fd565, %fd564;
	fma.rn.f64 	%fd567, %fd565, %fd564, %fd566;
	mul.f64 	%fd568, %fd567, %fd567;
	mov.f64 	%fd569, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd570, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd571, %fd570, %fd568, %fd569;
	mov.f64 	%fd572, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd573, %fd571, %fd568, %fd572;
	mov.f64 	%fd574, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd575, %fd573, %fd568, %fd574;
	mov.f64 	%fd576, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd577, %fd575, %fd568, %fd576;
	mov.f64 	%fd578, 0d3F624924923BE72D;
	fma.rn.f64 	%fd579, %fd577, %fd568, %fd578;
	mov.f64 	%fd580, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd581, %fd579, %fd568, %fd580;
	mov.f64 	%fd582, 0d3FB5555555555554;
	fma.rn.f64 	%fd583, %fd581, %fd568, %fd582;
	sub.f64 	%fd584, %fd565, %fd567;
	add.f64 	%fd585, %fd584, %fd584;
	neg.f64 	%fd586, %fd567;
	fma.rn.f64 	%fd587, %fd586, %fd565, %fd585;
	mul.f64 	%fd588, %fd564, %fd587;
	mul.f64 	%fd589, %fd568, %fd583;
	fma.rn.f64 	%fd590, %fd589, %fd567, %fd588;
	xor.b32  	%r474, %r534, -2147483648;
	mov.u32 	%r475, -2147483648;
	mov.u32 	%r476, 1127219200;
	mov.b64 	%fd591, {%r474, %r476};
	mov.b64 	%fd592, {%r475, %r476};
	sub.f64 	%fd593, %fd591, %fd592;
	mov.f64 	%fd594, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd595, %fd593, %fd594, %fd567;
	neg.f64 	%fd596, %fd593;
	fma.rn.f64 	%fd597, %fd596, %fd594, %fd595;
	sub.f64 	%fd598, %fd597, %fd567;
	sub.f64 	%fd599, %fd590, %fd598;
	mov.f64 	%fd600, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd601, %fd593, %fd600, %fd599;
	add.f64 	%fd1115, %fd595, %fd601;
	bra.uni 	$L__BB8_57;

$L__BB8_53:
	mov.f64 	%fd556, 0d7FF0000000000000;
	fma.rn.f64 	%fd557, %fd1113, %fd556, %fd556;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r467}, %fd1113;
	}
	mov.b32 	%f2, %r467;
	setp.eq.f32 	%p55, %f2, 0f00000000;
	selp.f64 	%fd1115, 0dFFF0000000000000, %fd557, %p55;

$L__BB8_57:
	sub.f64 	%fd602, %fd52, %fd59;
	div.rn.f64 	%fd603, %fd602, %fd59;
	mul.f64 	%fd604, %fd3, %fd603;
	mul.f64 	%fd605, %fd603, %fd604;
	mul.f64 	%fd1116, %fd605, %fd1115;

$L__BB8_58:
	selp.f64 	%fd608, %fd1116, 0d0000000000000000, %p47;
	mul.f64 	%fd609, %fd76, %fd195;
	mul.f64 	%fd610, %fd609, %fd192;
	mul.f64 	%fd611, %fd610, %fd608;
	mul.f64 	%fd607, %fd611, %fd196;
	mul.lo.s64 	%rd189, %rd65, %rd49;
	add.s64 	%rd188, %rd77, %rd189;
	// begin inline asm
	{ atom.add.f64 %fd606,[%rd188],%fd607; }

	// end inline asm

$L__BB8_59:
	setp.lt.u32 	%p58, %r2, 2;
	@%p58 bra 	$L__BB8_125;

	mul.lo.s64 	%rd190, %rd62, %rd30;
	add.s64 	%rd191, %rd10, %rd190;
	ld.global.s32 	%rd67, [%rd191];
	mul.lo.s64 	%rd192, %rd67, %rd39;
	add.s64 	%rd193, %rd16, %rd192;
	ld.global.s32 	%rd68, [%rd63];
	mul.lo.s64 	%rd194, %rd68, %rd39;
	add.s64 	%rd195, %rd16, %rd194;
	mul.lo.s64 	%rd196, %rd67, %rd40;
	add.s64 	%rd197, %rd20, %rd196;
	mul.lo.s64 	%rd198, %rd68, %rd40;
	add.s64 	%rd199, %rd20, %rd198;
	ld.global.f64 	%fd613, [%rd199];
	ld.global.f64 	%fd614, [%rd197];
	add.f64 	%fd88, %fd614, %fd613;
	ld.global.s32 	%rd69, [%rd193];
	mul.lo.s64 	%rd200, %rd69, %rd33;
	add.s64 	%rd201, %rd6, %rd200;
	ld.global.s32 	%rd70, [%rd193+4];
	mul.lo.s64 	%rd202, %rd70, %rd33;
	add.s64 	%rd203, %rd6, %rd202;
	ld.global.s32 	%rd71, [%rd195];
	mul.lo.s64 	%rd204, %rd71, %rd33;
	add.s64 	%rd205, %rd6, %rd204;
	ld.global.s32 	%rd72, [%rd195+4];
	mul.lo.s64 	%rd206, %rd72, %rd33;
	add.s64 	%rd207, %rd6, %rd206;
	mul.lo.s64 	%rd208, %rd69, %rd34;
	add.s64 	%rd209, %rd5, %rd208;
	mul.lo.s64 	%rd210, %rd70, %rd34;
	add.s64 	%rd211, %rd5, %rd210;
	mul.lo.s64 	%rd212, %rd71, %rd34;
	add.s64 	%rd213, %rd5, %rd212;
	mul.lo.s64 	%rd214, %rd72, %rd34;
	add.s64 	%rd215, %rd5, %rd214;
	mul.lo.s64 	%rd216, %rd62, %rd35;
	add.s64 	%rd217, %rd12, %rd216;
	ld.global.f64 	%fd615, [%rd217];
	mov.f64 	%fd616, 0d3FF0000000000000;
	sub.f64 	%fd617, %fd616, %fd615;
	ld.global.f64 	%fd89, [%rd201];
	mul.f64 	%fd618, %fd89, %fd617;
	ld.global.f64 	%fd90, [%rd203];
	fma.rn.f64 	%fd619, %fd90, %fd615, %fd618;
	ld.global.f64 	%fd620, [%rd217+8];
	sub.f64 	%fd621, %fd616, %fd620;
	ld.global.f64 	%fd91, [%rd205];
	mul.f64 	%fd622, %fd91, %fd621;
	sub.f64 	%fd623, %fd619, %fd622;
	ld.global.f64 	%fd92, [%rd207];
	mul.f64 	%fd624, %fd92, %fd620;
	sub.f64 	%fd93, %fd623, %fd624;
	ld.global.f64 	%fd94, [%rd201+8];
	mul.f64 	%fd625, %fd94, %fd617;
	ld.global.f64 	%fd95, [%rd203+8];
	fma.rn.f64 	%fd626, %fd95, %fd615, %fd625;
	ld.global.f64 	%fd96, [%rd205+8];
	mul.f64 	%fd627, %fd96, %fd621;
	sub.f64 	%fd628, %fd626, %fd627;
	ld.global.f64 	%fd97, [%rd207+8];
	mul.f64 	%fd629, %fd97, %fd620;
	sub.f64 	%fd98, %fd628, %fd629;
	ld.global.f64 	%fd99, [%rd201+16];
	mul.f64 	%fd630, %fd99, %fd617;
	ld.global.f64 	%fd100, [%rd203+16];
	fma.rn.f64 	%fd631, %fd100, %fd615, %fd630;
	ld.global.f64 	%fd101, [%rd205+16];
	mul.f64 	%fd632, %fd101, %fd621;
	sub.f64 	%fd633, %fd631, %fd632;
	ld.global.f64 	%fd102, [%rd207+16];
	mul.f64 	%fd634, %fd102, %fd620;
	sub.f64 	%fd103, %fd633, %fd634;
	ld.global.f64 	%fd635, [%rd209];
	mul.f64 	%fd636, %fd635, %fd617;
	ld.global.f64 	%fd637, [%rd211];
	fma.rn.f64 	%fd638, %fd637, %fd615, %fd636;
	ld.global.f64 	%fd639, [%rd213];
	mul.f64 	%fd640, %fd639, %fd621;
	sub.f64 	%fd641, %fd638, %fd640;
	ld.global.f64 	%fd642, [%rd215];
	mul.f64 	%fd643, %fd642, %fd620;
	sub.f64 	%fd104, %fd641, %fd643;
	ld.global.f64 	%fd644, [%rd209+8];
	mul.f64 	%fd645, %fd644, %fd617;
	ld.global.f64 	%fd646, [%rd211+8];
	fma.rn.f64 	%fd647, %fd646, %fd615, %fd645;
	ld.global.f64 	%fd648, [%rd213+8];
	mul.f64 	%fd649, %fd648, %fd621;
	sub.f64 	%fd650, %fd647, %fd649;
	ld.global.f64 	%fd651, [%rd215+8];
	mul.f64 	%fd652, %fd651, %fd620;
	sub.f64 	%fd105, %fd650, %fd652;
	ld.global.f64 	%fd653, [%rd209+16];
	mul.f64 	%fd654, %fd653, %fd617;
	ld.global.f64 	%fd655, [%rd211+16];
	fma.rn.f64 	%fd656, %fd655, %fd615, %fd654;
	ld.global.f64 	%fd657, [%rd213+16];
	mul.f64 	%fd658, %fd657, %fd621;
	sub.f64 	%fd659, %fd656, %fd658;
	ld.global.f64 	%fd660, [%rd215+16];
	mul.f64 	%fd661, %fd660, %fd620;
	sub.f64 	%fd106, %fd659, %fd661;
	sub.f64 	%fd107, %fd90, %fd89;
	sub.f64 	%fd108, %fd95, %fd94;
	sub.f64 	%fd109, %fd100, %fd99;
	sub.f64 	%fd110, %fd92, %fd91;
	sub.f64 	%fd111, %fd97, %fd96;
	sub.f64 	%fd112, %fd102, %fd101;
	sub.f64 	%fd113, %fd89, %fd91;
	sub.f64 	%fd114, %fd94, %fd96;
	sub.f64 	%fd115, %fd99, %fd101;
	mul.f64 	%fd662, %fd108, %fd108;
	fma.rn.f64 	%fd663, %fd107, %fd107, %fd662;
	fma.rn.f64 	%fd116, %fd109, %fd109, %fd663;
	mul.f64 	%fd664, %fd108, %fd111;
	fma.rn.f64 	%fd665, %fd107, %fd110, %fd664;
	fma.rn.f64 	%fd117, %fd109, %fd112, %fd665;
	mul.f64 	%fd666, %fd111, %fd111;
	fma.rn.f64 	%fd667, %fd110, %fd110, %fd666;
	fma.rn.f64 	%fd118, %fd112, %fd112, %fd667;
	mul.f64 	%fd668, %fd108, %fd114;
	fma.rn.f64 	%fd669, %fd107, %fd113, %fd668;
	fma.rn.f64 	%fd119, %fd109, %fd115, %fd669;
	mul.f64 	%fd670, %fd114, %fd111;
	fma.rn.f64 	%fd671, %fd113, %fd110, %fd670;
	fma.rn.f64 	%fd120, %fd115, %fd112, %fd671;
	mul.f64 	%fd672, %fd116, %fd118;
	mul.f64 	%fd673, %fd117, %fd117;
	sub.f64 	%fd121, %fd672, %fd673;
	mul.f64 	%fd674, %fd117, %fd120;
	mul.f64 	%fd675, %fd119, %fd118;
	sub.f64 	%fd122, %fd674, %fd675;
	setp.le.f64 	%p59, %fd122, 0d0000000000000000;
	@%p59 bra 	$L__BB8_64;

	setp.ge.f64 	%p1, %fd122, %fd121;
	add.f64 	%fd123, %fd120, %fd117;
	@%p1 bra 	$L__BB8_63;

	sub.f64 	%fd1088, %fd99, %fd101;
	sub.f64 	%fd1087, %fd89, %fd91;
	sub.f64 	%fd1086, %fd94, %fd96;
	sub.f64 	%fd1085, %fd92, %fd91;
	sub.f64 	%fd1084, %fd90, %fd89;
	sub.f64 	%fd1083, %fd102, %fd101;
	sub.f64 	%fd1082, %fd95, %fd94;
	sub.f64 	%fd1081, %fd97, %fd96;
	sub.f64 	%fd1080, %fd100, %fd99;
	selp.f64 	%fd677, %fd118, %fd121, %p1;
	mul.f64 	%fd678, %fd116, %fd120;
	mul.f64 	%fd679, %fd119, %fd117;
	sub.f64 	%fd680, %fd678, %fd679;
	mul.f64 	%fd681, %fd1080, %fd1081;
	mul.f64 	%fd682, %fd1082, %fd1083;
	sub.f64 	%fd683, %fd682, %fd681;
	mul.f64 	%fd684, %fd1084, %fd1083;
	mul.f64 	%fd685, %fd1080, %fd1085;
	sub.f64 	%fd686, %fd685, %fd684;
	mul.f64 	%fd687, %fd1082, %fd1085;
	mul.f64 	%fd688, %fd1084, %fd1081;
	sub.f64 	%fd689, %fd688, %fd687;
	setp.gt.f64 	%p60, %fd680, 0d0000000000000000;
	setp.lt.f64 	%p61, %fd680, %fd677;
	mul.f64 	%fd690, %fd1086, %fd686;
	fma.rn.f64 	%fd691, %fd1087, %fd683, %fd690;
	fma.rn.f64 	%fd692, %fd1088, %fd689, %fd691;
	setp.eq.f64 	%p62, %fd692, 0d0000000000000000;
	mul.f64 	%fd693, %fd686, %fd686;
	fma.rn.f64 	%fd694, %fd683, %fd683, %fd693;
	fma.rn.f64 	%fd695, %fd689, %fd689, %fd694;
	mul.f64 	%fd696, %fd116, 0d3BC79CA100000000;
	mul.f64 	%fd697, %fd696, %fd118;
	setp.lt.f64 	%p63, %fd695, %fd697;
	or.pred  	%p64, %p62, %p63;
	and.pred  	%p65, %p60, %p61;
	and.pred  	%p66, %p64, %p65;
	mul.f64 	%fd698, %fd121, 0d3FE0000000000000;
	setp.lt.f64 	%p67, %fd122, %fd698;
	selp.b32 	%r479, 2, 5, %p67;
	selp.f64 	%fd699, %fd120, %fd123, %p67;
	selp.f64 	%fd1118, %fd118, %fd677, %p66;
	selp.b32 	%r535, %r479, 8, %p66;
	selp.f64 	%fd1117, %fd699, %fd680, %p66;

$L__BB8_63:
	selp.f64 	%fd1120, %fd118, %fd1118, %p1;
	selp.b32 	%r536, 5, %r535, %p1;
	selp.f64 	%fd1119, %fd123, %fd1117, %p1;

$L__BB8_64:
	selp.f64 	%fd132, %fd118, %fd1120, %p59;
	selp.b32 	%r537, 2, %r536, %p59;
	selp.f64 	%fd133, %fd120, %fd1119, %p59;
	setp.gtu.f64 	%p70, %fd133, 0d0000000000000000;
	@%p70 bra 	$L__BB8_68;
	bra.uni 	$L__BB8_65;

$L__BB8_68:
	setp.ltu.f64 	%p73, %fd133, %fd132;
	@%p73 bra 	$L__BB8_72;

	mov.f64 	%fd701, 0d0000000000000000;
	sub.f64 	%fd702, %fd701, %fd119;
	add.f64 	%fd135, %fd702, %fd117;
	setp.le.f64 	%p74, %fd135, 0d0000000000000000;
	mov.u32 	%r537, 1;
	@%p74 bra 	$L__BB8_72;

	setp.ge.f64 	%p75, %fd135, %fd116;
	mov.u32 	%r537, 4;
	@%p75 bra 	$L__BB8_72;

	mov.u32 	%r537, 7;
	bra.uni 	$L__BB8_72;

$L__BB8_65:
	mov.f64 	%fd700, 0d0000000000000000;
	sub.f64 	%fd134, %fd700, %fd119;
	setp.le.f64 	%p71, %fd134, 0d0000000000000000;
	mov.u32 	%r537, 0;
	@%p71 bra 	$L__BB8_72;

	setp.ge.f64 	%p72, %fd134, %fd116;
	mov.u32 	%r537, 3;
	@%p72 bra 	$L__BB8_72;

	mov.u32 	%r537, 6;

$L__BB8_72:
	setp.eq.s32 	%p76, %r537, 0;
	@%p76 bra 	$L__BB8_88;

	setp.eq.s32 	%p77, %r537, 1;
	@%p77 bra 	$L__BB8_87;
	bra.uni 	$L__BB8_74;

$L__BB8_87:
	sub.f64 	%fd801, %fd89, %fd92;
	sub.f64 	%fd802, %fd94, %fd97;
	mul.f64 	%fd803, %fd802, %fd802;
	fma.rn.f64 	%fd804, %fd801, %fd801, %fd803;
	sub.f64 	%fd805, %fd99, %fd102;
	fma.rn.f64 	%fd1121, %fd805, %fd805, %fd804;
	bra.uni 	$L__BB8_89;

$L__BB8_88:
	sub.f64 	%fd1106, %fd99, %fd101;
	sub.f64 	%fd1105, %fd89, %fd91;
	sub.f64 	%fd1104, %fd94, %fd96;
	mul.f64 	%fd806, %fd1104, %fd1104;
	fma.rn.f64 	%fd807, %fd1105, %fd1105, %fd806;
	fma.rn.f64 	%fd1121, %fd1106, %fd1106, %fd807;
	bra.uni 	$L__BB8_89;

$L__BB8_74:
	setp.eq.s32 	%p78, %r537, 2;
	@%p78 bra 	$L__BB8_86;
	bra.uni 	$L__BB8_75;

$L__BB8_86:
	sub.f64 	%fd783, %fd91, %fd89;
	sub.f64 	%fd784, %fd102, %fd99;
	sub.f64 	%fd785, %fd96, %fd94;
	mul.f64 	%fd786, %fd785, %fd784;
	sub.f64 	%fd787, %fd97, %fd94;
	sub.f64 	%fd788, %fd101, %fd99;
	mul.f64 	%fd789, %fd788, %fd787;
	sub.f64 	%fd790, %fd786, %fd789;
	sub.f64 	%fd791, %fd92, %fd89;
	mul.f64 	%fd792, %fd788, %fd791;
	mul.f64 	%fd793, %fd783, %fd784;
	sub.f64 	%fd794, %fd792, %fd793;
	mul.f64 	%fd795, %fd783, %fd787;
	mul.f64 	%fd796, %fd785, %fd791;
	sub.f64 	%fd797, %fd795, %fd796;
	mul.f64 	%fd798, %fd794, %fd794;
	fma.rn.f64 	%fd799, %fd790, %fd790, %fd798;
	fma.rn.f64 	%fd800, %fd797, %fd797, %fd799;
	div.rn.f64 	%fd1121, %fd800, %fd118;
	bra.uni 	$L__BB8_89;

$L__BB8_75:
	setp.eq.s32 	%p79, %r537, 3;
	@%p79 bra 	$L__BB8_85;
	bra.uni 	$L__BB8_76;

$L__BB8_85:
	sub.f64 	%fd778, %fd90, %fd91;
	sub.f64 	%fd779, %fd95, %fd96;
	mul.f64 	%fd780, %fd779, %fd779;
	fma.rn.f64 	%fd781, %fd778, %fd778, %fd780;
	sub.f64 	%fd782, %fd100, %fd101;
	fma.rn.f64 	%fd1121, %fd782, %fd782, %fd781;
	bra.uni 	$L__BB8_89;

$L__BB8_76:
	setp.eq.s32 	%p80, %r537, 4;
	@%p80 bra 	$L__BB8_84;
	bra.uni 	$L__BB8_77;

$L__BB8_84:
	sub.f64 	%fd773, %fd90, %fd92;
	sub.f64 	%fd774, %fd95, %fd97;
	mul.f64 	%fd775, %fd774, %fd774;
	fma.rn.f64 	%fd776, %fd773, %fd773, %fd775;
	sub.f64 	%fd777, %fd100, %fd102;
	fma.rn.f64 	%fd1121, %fd777, %fd777, %fd776;
	bra.uni 	$L__BB8_89;

$L__BB8_77:
	setp.eq.s32 	%p81, %r537, 5;
	@%p81 bra 	$L__BB8_83;
	bra.uni 	$L__BB8_78;

$L__BB8_83:
	sub.f64 	%fd755, %fd91, %fd90;
	sub.f64 	%fd756, %fd102, %fd100;
	sub.f64 	%fd757, %fd96, %fd95;
	mul.f64 	%fd758, %fd757, %fd756;
	sub.f64 	%fd759, %fd97, %fd95;
	sub.f64 	%fd760, %fd101, %fd100;
	mul.f64 	%fd761, %fd760, %fd759;
	sub.f64 	%fd762, %fd758, %fd761;
	sub.f64 	%fd763, %fd92, %fd90;
	mul.f64 	%fd764, %fd760, %fd763;
	mul.f64 	%fd765, %fd755, %fd756;
	sub.f64 	%fd766, %fd764, %fd765;
	mul.f64 	%fd767, %fd755, %fd759;
	mul.f64 	%fd768, %fd757, %fd763;
	sub.f64 	%fd769, %fd767, %fd768;
	mul.f64 	%fd770, %fd766, %fd766;
	fma.rn.f64 	%fd771, %fd762, %fd762, %fd770;
	fma.rn.f64 	%fd772, %fd769, %fd769, %fd771;
	div.rn.f64 	%fd1121, %fd772, %fd118;
	bra.uni 	$L__BB8_89;

$L__BB8_78:
	setp.eq.s32 	%p82, %r537, 6;
	@%p82 bra 	$L__BB8_82;
	bra.uni 	$L__BB8_79;

$L__BB8_82:
	sub.f64 	%fd1103, %fd99, %fd101;
	sub.f64 	%fd1102, %fd89, %fd91;
	sub.f64 	%fd1101, %fd94, %fd96;
	sub.f64 	%fd740, %fd90, %fd91;
	sub.f64 	%fd741, %fd100, %fd101;
	mul.f64 	%fd742, %fd1101, %fd741;
	sub.f64 	%fd743, %fd95, %fd96;
	mul.f64 	%fd744, %fd743, %fd1103;
	sub.f64 	%fd745, %fd742, %fd744;
	mul.f64 	%fd746, %fd740, %fd1103;
	mul.f64 	%fd747, %fd1102, %fd741;
	sub.f64 	%fd748, %fd746, %fd747;
	mul.f64 	%fd749, %fd1102, %fd743;
	mul.f64 	%fd750, %fd740, %fd1101;
	sub.f64 	%fd751, %fd749, %fd750;
	mul.f64 	%fd752, %fd748, %fd748;
	fma.rn.f64 	%fd753, %fd745, %fd745, %fd752;
	fma.rn.f64 	%fd754, %fd751, %fd751, %fd753;
	div.rn.f64 	%fd1121, %fd754, %fd116;
	bra.uni 	$L__BB8_89;

$L__BB8_79:
	setp.eq.s32 	%p83, %r537, 7;
	@%p83 bra 	$L__BB8_81;
	bra.uni 	$L__BB8_80;

$L__BB8_81:
	sub.f64 	%fd722, %fd89, %fd92;
	sub.f64 	%fd723, %fd100, %fd102;
	sub.f64 	%fd724, %fd94, %fd97;
	mul.f64 	%fd725, %fd724, %fd723;
	sub.f64 	%fd726, %fd95, %fd97;
	sub.f64 	%fd727, %fd99, %fd102;
	mul.f64 	%fd728, %fd726, %fd727;
	sub.f64 	%fd729, %fd725, %fd728;
	sub.f64 	%fd730, %fd90, %fd92;
	mul.f64 	%fd731, %fd730, %fd727;
	mul.f64 	%fd732, %fd722, %fd723;
	sub.f64 	%fd733, %fd731, %fd732;
	mul.f64 	%fd734, %fd722, %fd726;
	mul.f64 	%fd735, %fd730, %fd724;
	sub.f64 	%fd736, %fd734, %fd735;
	mul.f64 	%fd737, %fd733, %fd733;
	fma.rn.f64 	%fd738, %fd729, %fd729, %fd737;
	fma.rn.f64 	%fd739, %fd736, %fd736, %fd738;
	div.rn.f64 	%fd1121, %fd739, %fd116;
	bra.uni 	$L__BB8_89;

$L__BB8_80:
	sub.f64 	%fd1094, %fd92, %fd91;
	sub.f64 	%fd1093, %fd90, %fd89;
	sub.f64 	%fd1092, %fd102, %fd101;
	sub.f64 	%fd1091, %fd95, %fd94;
	sub.f64 	%fd1090, %fd97, %fd96;
	sub.f64 	%fd1089, %fd100, %fd99;
	sub.f64 	%fd703, %fd91, %fd89;
	mul.f64 	%fd704, %fd1089, %fd1090;
	mul.f64 	%fd705, %fd1091, %fd1092;
	sub.f64 	%fd706, %fd705, %fd704;
	mul.f64 	%fd707, %fd1093, %fd1092;
	mul.f64 	%fd708, %fd1089, %fd1094;
	sub.f64 	%fd709, %fd708, %fd707;
	mul.f64 	%fd710, %fd1091, %fd1094;
	mul.f64 	%fd711, %fd1093, %fd1090;
	sub.f64 	%fd712, %fd711, %fd710;
	sub.f64 	%fd713, %fd96, %fd94;
	mul.f64 	%fd714, %fd713, %fd709;
	fma.rn.f64 	%fd715, %fd703, %fd706, %fd714;
	sub.f64 	%fd716, %fd101, %fd99;
	fma.rn.f64 	%fd717, %fd716, %fd712, %fd715;
	mul.f64 	%fd718, %fd717, %fd717;
	mul.f64 	%fd719, %fd709, %fd709;
	fma.rn.f64 	%fd720, %fd706, %fd706, %fd719;
	fma.rn.f64 	%fd721, %fd712, %fd712, %fd720;
	div.rn.f64 	%fd1121, %fd718, %fd721;

$L__BB8_89:
	sub.f64 	%fd1100, %fd92, %fd91;
	sub.f64 	%fd1099, %fd90, %fd89;
	sub.f64 	%fd1098, %fd102, %fd101;
	sub.f64 	%fd1097, %fd95, %fd94;
	sub.f64 	%fd1096, %fd97, %fd96;
	sub.f64 	%fd1095, %fd100, %fd99;
	mul.f64 	%fd809, %fd88, %fd88;
	sub.f64 	%fd146, %fd1121, %fd809;
	mul.lo.s64 	%rd218, %rd69, %rd42;
	add.s64 	%rd219, %rd22, %rd218;
	mul.lo.s64 	%rd220, %rd70, %rd42;
	add.s64 	%rd221, %rd22, %rd220;
	mul.lo.s64 	%rd222, %rd71, %rd42;
	add.s64 	%rd223, %rd22, %rd222;
	mul.lo.s64 	%rd224, %rd72, %rd42;
	add.s64 	%rd225, %rd22, %rd224;
	ld.global.f64 	%fd810, [%rd221];
	ld.global.f64 	%fd811, [%rd219];
	sub.f64 	%fd812, %fd810, %fd811;
	ld.global.f64 	%fd813, [%rd221+8];
	ld.global.f64 	%fd814, [%rd219+8];
	sub.f64 	%fd815, %fd813, %fd814;
	ld.global.f64 	%fd816, [%rd221+16];
	ld.global.f64 	%fd817, [%rd219+16];
	sub.f64 	%fd818, %fd816, %fd817;
	ld.global.f64 	%fd819, [%rd225];
	ld.global.f64 	%fd820, [%rd223];
	sub.f64 	%fd821, %fd819, %fd820;
	ld.global.f64 	%fd822, [%rd225+8];
	ld.global.f64 	%fd823, [%rd223+8];
	sub.f64 	%fd824, %fd822, %fd823;
	ld.global.f64 	%fd825, [%rd225+16];
	ld.global.f64 	%fd826, [%rd223+16];
	sub.f64 	%fd827, %fd825, %fd826;
	mul.f64 	%fd828, %fd815, %fd815;
	fma.rn.f64 	%fd829, %fd812, %fd812, %fd828;
	fma.rn.f64 	%fd830, %fd818, %fd818, %fd829;
	mul.f64 	%fd831, %fd830, 0d3F50624DE0000000;
	mul.f64 	%fd832, %fd824, %fd824;
	fma.rn.f64 	%fd833, %fd821, %fd821, %fd832;
	fma.rn.f64 	%fd834, %fd827, %fd827, %fd833;
	mul.f64 	%fd147, %fd831, %fd834;
	mul.f64 	%fd835, %fd1095, %fd1096;
	mul.f64 	%fd836, %fd1097, %fd1098;
	sub.f64 	%fd837, %fd836, %fd835;
	mul.f64 	%fd838, %fd1099, %fd1098;
	mul.f64 	%fd839, %fd1095, %fd1100;
	sub.f64 	%fd840, %fd839, %fd838;
	mul.f64 	%fd841, %fd1097, %fd1100;
	mul.f64 	%fd842, %fd1099, %fd1096;
	sub.f64 	%fd843, %fd842, %fd841;
	mul.f64 	%fd844, %fd840, %fd840;
	fma.rn.f64 	%fd845, %fd837, %fd837, %fd844;
	fma.rn.f64 	%fd148, %fd843, %fd843, %fd845;
	setp.geu.f64 	%p84, %fd148, %fd147;
	mov.f64 	%fd1122, 0d3FF0000000000000;
	@%p84 bra 	$L__BB8_91;

	div.rn.f64 	%fd846, %fd148, %fd147;
	mov.f64 	%fd847, 0d0000000000000000;
	sub.f64 	%fd848, %fd847, %fd846;
	add.f64 	%fd849, %fd848, 0d4000000000000000;
	mul.f64 	%fd1122, %fd846, %fd849;

$L__BB8_91:
	fma.rn.f64 	%fd151, %fd2, %fd88, %fd1;
	mul.lo.s64 	%rd226, %rd67, %rd48;
	add.s64 	%rd73, %rd17, %rd226;
	mul.lo.s64 	%rd227, %rd68, %rd48;
	add.s64 	%rd74, %rd17, %rd227;
	ld.global.f64 	%fd152, [%rd74];
	ld.global.f64 	%fd153, [%rd73];
	setp.geu.f64 	%p85, %fd146, %fd151;
	@%p85 bra 	$L__BB8_100;

	div.rn.f64 	%fd1123, %fd146, %fd151;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r538}, %fd1123;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r539, %temp}, %fd1123;
	}
	setp.gt.s32 	%p86, %r538, 1048575;
	mov.u32 	%r540, -1023;
	@%p86 bra 	$L__BB8_94;

	mul.f64 	%fd1123, %fd1123, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r538}, %fd1123;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r539, %temp}, %fd1123;
	}
	mov.u32 	%r540, -1077;

$L__BB8_94:
	add.s32 	%r488, %r538, -1;
	setp.lt.u32 	%p87, %r488, 2146435071;
	@%p87 bra 	$L__BB8_96;
	bra.uni 	$L__BB8_95;

$L__BB8_96:
	shr.u32 	%r490, %r538, 20;
	add.s32 	%r541, %r540, %r490;
	and.b32  	%r491, %r538, -2146435073;
	or.b32  	%r492, %r491, 1072693248;
	mov.b64 	%fd1124, {%r539, %r492};
	setp.lt.s32 	%p89, %r492, 1073127583;
	@%p89 bra 	$L__BB8_98;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r493, %temp}, %fd1124;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r494}, %fd1124;
	}
	add.s32 	%r495, %r494, -1048576;
	mov.b64 	%fd1124, {%r493, %r495};
	add.s32 	%r541, %r541, 1;

$L__BB8_98:
	add.f64 	%fd853, %fd1124, 0d3FF0000000000000;
	mov.f64 	%fd854, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd855, %fd853;
	neg.f64 	%fd856, %fd853;
	fma.rn.f64 	%fd857, %fd856, %fd855, %fd854;
	fma.rn.f64 	%fd858, %fd857, %fd857, %fd857;
	fma.rn.f64 	%fd859, %fd858, %fd855, %fd855;
	add.f64 	%fd860, %fd1124, 0dBFF0000000000000;
	mul.f64 	%fd861, %fd860, %fd859;
	fma.rn.f64 	%fd862, %fd860, %fd859, %fd861;
	mul.f64 	%fd863, %fd862, %fd862;
	mov.f64 	%fd864, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd865, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd866, %fd865, %fd863, %fd864;
	mov.f64 	%fd867, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd868, %fd866, %fd863, %fd867;
	mov.f64 	%fd869, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd870, %fd868, %fd863, %fd869;
	mov.f64 	%fd871, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd872, %fd870, %fd863, %fd871;
	mov.f64 	%fd873, 0d3F624924923BE72D;
	fma.rn.f64 	%fd874, %fd872, %fd863, %fd873;
	mov.f64 	%fd875, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd876, %fd874, %fd863, %fd875;
	mov.f64 	%fd877, 0d3FB5555555555554;
	fma.rn.f64 	%fd878, %fd876, %fd863, %fd877;
	sub.f64 	%fd879, %fd860, %fd862;
	add.f64 	%fd880, %fd879, %fd879;
	neg.f64 	%fd881, %fd862;
	fma.rn.f64 	%fd882, %fd881, %fd860, %fd880;
	mul.f64 	%fd883, %fd859, %fd882;
	mul.f64 	%fd884, %fd863, %fd878;
	fma.rn.f64 	%fd885, %fd884, %fd862, %fd883;
	xor.b32  	%r496, %r541, -2147483648;
	mov.u32 	%r497, -2147483648;
	mov.u32 	%r498, 1127219200;
	mov.b64 	%fd886, {%r496, %r498};
	mov.b64 	%fd887, {%r497, %r498};
	sub.f64 	%fd888, %fd886, %fd887;
	mov.f64 	%fd889, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd890, %fd888, %fd889, %fd862;
	neg.f64 	%fd891, %fd888;
	fma.rn.f64 	%fd892, %fd891, %fd889, %fd890;
	sub.f64 	%fd893, %fd892, %fd862;
	sub.f64 	%fd894, %fd885, %fd893;
	mov.f64 	%fd895, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd896, %fd888, %fd895, %fd894;
	add.f64 	%fd1125, %fd890, %fd896;
	bra.uni 	$L__BB8_99;

$L__BB8_95:
	mov.f64 	%fd851, 0d7FF0000000000000;
	fma.rn.f64 	%fd852, %fd1123, %fd851, %fd851;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r489}, %fd1123;
	}
	mov.b32 	%f3, %r489;
	setp.eq.f32 	%p88, %f3, 0f00000000;
	selp.f64 	%fd1125, 0dFFF0000000000000, %fd852, %p88;

$L__BB8_99:
	sub.f64 	%fd897, %fd146, %fd151;
	div.rn.f64 	%fd898, %fd897, %fd151;
	mul.f64 	%fd899, %fd3, %fd898;
	mul.f64 	%fd900, %fd898, %fd899;
	mul.f64 	%fd1126, %fd900, %fd1125;

$L__BB8_100:
	setp.lt.f64 	%p90, %fd146, %fd151;
	selp.f64 	%fd901, %fd1126, 0d0000000000000000, %p90;
	add.f64 	%fd902, %fd153, %fd152;
	mul.f64 	%fd903, %fd902, %fd195;
	mul.f64 	%fd904, %fd903, %fd192;
	mul.f64 	%fd905, %fd1122, %fd904;
	mul.f64 	%fd906, %fd905, %fd901;
	mul.f64 	%fd907, %fd906, %fd196;
	setp.num.f64 	%p91, %fd907, %fd907;
	@%p91 bra 	$L__BB8_111;

	@%p85 bra 	$L__BB8_110;

	div.rn.f64 	%fd1127, %fd146, %fd151;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r542}, %fd1127;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r543, %temp}, %fd1127;
	}
	setp.gt.s32 	%p93, %r542, 1048575;
	mov.u32 	%r544, -1023;
	@%p93 bra 	$L__BB8_104;

	mul.f64 	%fd1127, %fd1127, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r542}, %fd1127;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r543, %temp}, %fd1127;
	}
	mov.u32 	%r544, -1077;

$L__BB8_104:
	add.s32 	%r501, %r542, -1;
	setp.lt.u32 	%p94, %r501, 2146435071;
	@%p94 bra 	$L__BB8_106;
	bra.uni 	$L__BB8_105;

$L__BB8_106:
	shr.u32 	%r503, %r542, 20;
	add.s32 	%r545, %r544, %r503;
	and.b32  	%r504, %r542, -2146435073;
	or.b32  	%r505, %r504, 1072693248;
	mov.b64 	%fd1128, {%r543, %r505};
	setp.lt.s32 	%p96, %r505, 1073127583;
	@%p96 bra 	$L__BB8_108;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r506, %temp}, %fd1128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r507}, %fd1128;
	}
	add.s32 	%r508, %r507, -1048576;
	mov.b64 	%fd1128, {%r506, %r508};
	add.s32 	%r545, %r545, 1;

$L__BB8_108:
	add.f64 	%fd911, %fd1128, 0d3FF0000000000000;
	mov.f64 	%fd912, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd913, %fd911;
	neg.f64 	%fd914, %fd911;
	fma.rn.f64 	%fd915, %fd914, %fd913, %fd912;
	fma.rn.f64 	%fd916, %fd915, %fd915, %fd915;
	fma.rn.f64 	%fd917, %fd916, %fd913, %fd913;
	add.f64 	%fd918, %fd1128, 0dBFF0000000000000;
	mul.f64 	%fd919, %fd918, %fd917;
	fma.rn.f64 	%fd920, %fd918, %fd917, %fd919;
	mul.f64 	%fd921, %fd920, %fd920;
	mov.f64 	%fd922, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd923, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd924, %fd923, %fd921, %fd922;
	mov.f64 	%fd925, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd926, %fd924, %fd921, %fd925;
	mov.f64 	%fd927, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd928, %fd926, %fd921, %fd927;
	mov.f64 	%fd929, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd930, %fd928, %fd921, %fd929;
	mov.f64 	%fd931, 0d3F624924923BE72D;
	fma.rn.f64 	%fd932, %fd930, %fd921, %fd931;
	mov.f64 	%fd933, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd934, %fd932, %fd921, %fd933;
	mov.f64 	%fd935, 0d3FB5555555555554;
	fma.rn.f64 	%fd936, %fd934, %fd921, %fd935;
	sub.f64 	%fd937, %fd918, %fd920;
	add.f64 	%fd938, %fd937, %fd937;
	neg.f64 	%fd939, %fd920;
	fma.rn.f64 	%fd940, %fd939, %fd918, %fd938;
	mul.f64 	%fd941, %fd917, %fd940;
	mul.f64 	%fd942, %fd921, %fd936;
	fma.rn.f64 	%fd943, %fd942, %fd920, %fd941;
	xor.b32  	%r509, %r545, -2147483648;
	mov.u32 	%r510, -2147483648;
	mov.u32 	%r511, 1127219200;
	mov.b64 	%fd944, {%r509, %r511};
	mov.b64 	%fd945, {%r510, %r511};
	sub.f64 	%fd946, %fd944, %fd945;
	mov.f64 	%fd947, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd948, %fd946, %fd947, %fd920;
	neg.f64 	%fd949, %fd946;
	fma.rn.f64 	%fd950, %fd949, %fd947, %fd948;
	sub.f64 	%fd951, %fd950, %fd920;
	sub.f64 	%fd952, %fd943, %fd951;
	mov.f64 	%fd953, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd954, %fd946, %fd953, %fd952;
	add.f64 	%fd1129, %fd948, %fd954;
	bra.uni 	$L__BB8_109;

$L__BB8_105:
	mov.f64 	%fd909, 0d7FF0000000000000;
	fma.rn.f64 	%fd910, %fd1127, %fd909, %fd909;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r502}, %fd1127;
	}
	mov.b32 	%f4, %r502;
	setp.eq.f32 	%p95, %f4, 0f00000000;
	selp.f64 	%fd1129, 0dFFF0000000000000, %fd910, %p95;

$L__BB8_109:
	sub.f64 	%fd955, %fd146, %fd151;
	div.rn.f64 	%fd956, %fd955, %fd151;
	mul.f64 	%fd957, %fd3, %fd956;
	mul.f64 	%fd958, %fd956, %fd957;
	mul.f64 	%fd1130, %fd958, %fd1129;

$L__BB8_110:
	add.u64 	%rd254, %SP, 16;
	add.u64 	%rd253, %SP, 16;
	add.u64 	%rd252, %SPL, 16;
	st.local.v2.f64 	[%rd252], {%fd153, %fd152};
	selp.f64 	%fd959, %fd1130, 0d0000000000000000, %p90;
	st.local.v2.f64 	[%rd252+16], {%fd959, %fd146};
	st.local.v2.f64 	[%rd252+32], {%fd151, %fd88};
	mov.u64 	%rd228, $str$4;
	cvta.global.u64 	%rd229, %rd228;
	{ // callseq 509, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd229;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd253;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r512, [retval0+0];
	} // callseq 509

$L__BB8_111:
	setp.eq.s32 	%p98, %r255, 0;
	@%p98 bra 	$L__BB8_115;

	mul.lo.s64 	%rd231, %rd62, %rd43;
	add.s64 	%rd232, %rd8, %rd231;
	ld.global.s32 	%rd233, [%rd232];
	mul.lo.s64 	%rd234, %rd233, %rd44;
	add.s64 	%rd235, %rd4, %rd234;
	mul.lo.s64 	%rd236, %rd62, %rd45;
	add.s64 	%rd237, %rd7, %rd236;
	ld.global.s32 	%rd238, [%rd237];
	mul.lo.s64 	%rd239, %rd238, %rd44;
	add.s64 	%rd240, %rd4, %rd239;
	ld.global.f64 	%fd960, [%rd235];
	add.f64 	%fd961, %fd960, %fd960;
	ld.global.f64 	%fd962, [%rd240];
	mul.f64 	%fd963, %fd961, %fd962;
	add.f64 	%fd964, %fd960, %fd962;
	setp.neu.f64 	%p99, %fd964, 0d0000000000000000;
	mov.f64 	%fd965, 0d0000000000000000;
	div.rn.f64 	%fd966, %fd963, %fd964;
	selp.f64 	%fd967, %fd966, 0d0000000000000000, %p99;
	mul.lo.s64 	%rd241, %rd62, %rd46;
	add.s64 	%rd242, %rd13, %rd241;
	ld.global.f64 	%fd968, [%rd242];
	mul.f64 	%fd176, %fd968, %fd967;
	mul.lo.s64 	%rd243, %rd62, %rd47;
	add.s64 	%rd244, %rd11, %rd243;
	ld.global.f64 	%fd969, [%rd244];
	mul.f64 	%fd970, %fd969, %fd969;
	ld.global.f64 	%fd971, [%rd244+8];
	mul.f64 	%fd972, %fd969, %fd971;
	ld.global.f64 	%fd973, [%rd244+16];
	mul.f64 	%fd974, %fd969, %fd973;
	mul.f64 	%fd975, %fd971, %fd971;
	mul.f64 	%fd976, %fd971, %fd973;
	mul.f64 	%fd977, %fd973, %fd973;
	mov.f64 	%fd978, 0d3FF0000000000000;
	sub.f64 	%fd979, %fd978, %fd970;
	sub.f64 	%fd980, %fd965, %fd972;
	sub.f64 	%fd981, %fd965, %fd974;
	sub.f64 	%fd982, %fd978, %fd975;
	sub.f64 	%fd983, %fd965, %fd976;
	sub.f64 	%fd984, %fd978, %fd977;
	sub.f64 	%fd985, %fd93, %fd104;
	sub.f64 	%fd986, %fd98, %fd105;
	mul.f64 	%fd987, %fd980, %fd986;
	mul.f64 	%fd988, %fd982, %fd986;
	mul.f64 	%fd989, %fd983, %fd986;
	fma.rn.f64 	%fd990, %fd979, %fd985, %fd987;
	fma.rn.f64 	%fd991, %fd980, %fd985, %fd988;
	fma.rn.f64 	%fd992, %fd981, %fd985, %fd989;
	sub.f64 	%fd993, %fd103, %fd106;
	fma.rn.f64 	%fd994, %fd981, %fd993, %fd990;
	fma.rn.f64 	%fd995, %fd983, %fd993, %fd991;
	fma.rn.f64 	%fd996, %fd984, %fd993, %fd992;
	div.rn.f64 	%fd997, %fd994, %fd193;
	div.rn.f64 	%fd998, %fd995, %fd193;
	div.rn.f64 	%fd999, %fd996, %fd193;
	mul.f64 	%fd1000, %fd998, %fd998;
	fma.rn.f64 	%fd1001, %fd997, %fd997, %fd1000;
	fma.rn.f64 	%fd1002, %fd999, %fd999, %fd1001;
	sqrt.rn.f64 	%fd1003, %fd1002;
	setp.ge.f64 	%p100, %fd1003, %fd197;
	mul.f64 	%fd1131, %fd1003, %fd193;
	@%p100 bra 	$L__BB8_114;

	mul.f64 	%fd1004, %fd1131, %fd1131;
	sub.f64 	%fd1006, %fd965, %fd1131;
	div.rn.f64 	%fd1007, %fd1006, 0d4008000000000000;
	add.f64 	%fd1008, %fd4, %fd1007;
	mul.f64 	%fd1009, %fd1004, %fd1008;
	div.rn.f64 	%fd1010, %fd1009, %fd5;
	add.f64 	%fd1131, %fd6, %fd1010;

$L__BB8_114:
	mul.lo.s64 	%rd246, %rd69, %rd49;
	add.s64 	%rd245, %rd77, %rd246;
	mul.f64 	%fd1013, %fd176, %fd196;
	mul.f64 	%fd1012, %fd1013, %fd1131;
	// begin inline asm
	{ atom.add.f64 %fd1011,[%rd245],%fd1012; }

	// end inline asm
	bra.uni 	$L__BB8_125;

$L__BB8_115:
	ld.global.f64 	%fd1015, [%rd74];
	ld.global.f64 	%fd1016, [%rd73];
	add.f64 	%fd1017, %fd1016, %fd1015;
	mul.f64 	%fd1018, %fd1017, %fd195;
	mul.f64 	%fd1019, %fd1018, %fd192;
	mul.f64 	%fd180, %fd1122, %fd1019;
	@%p85 bra 	$L__BB8_124;

	div.rn.f64 	%fd1132, %fd146, %fd151;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r546}, %fd1132;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r547, %temp}, %fd1132;
	}
	setp.gt.s32 	%p102, %r546, 1048575;
	mov.u32 	%r548, -1023;
	@%p102 bra 	$L__BB8_118;

	mul.f64 	%fd1132, %fd1132, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r546}, %fd1132;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r547, %temp}, %fd1132;
	}
	mov.u32 	%r548, -1077;

$L__BB8_118:
	add.s32 	%r515, %r546, -1;
	setp.lt.u32 	%p103, %r515, 2146435071;
	@%p103 bra 	$L__BB8_120;
	bra.uni 	$L__BB8_119;

$L__BB8_120:
	shr.u32 	%r517, %r546, 20;
	add.s32 	%r549, %r548, %r517;
	and.b32  	%r518, %r546, -2146435073;
	or.b32  	%r519, %r518, 1072693248;
	mov.b64 	%fd1133, {%r547, %r519};
	setp.lt.s32 	%p105, %r519, 1073127583;
	@%p105 bra 	$L__BB8_122;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r520, %temp}, %fd1133;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r521}, %fd1133;
	}
	add.s32 	%r522, %r521, -1048576;
	mov.b64 	%fd1133, {%r520, %r522};
	add.s32 	%r549, %r549, 1;

$L__BB8_122:
	add.f64 	%fd1022, %fd1133, 0d3FF0000000000000;
	mov.f64 	%fd1023, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1024, %fd1022;
	neg.f64 	%fd1025, %fd1022;
	fma.rn.f64 	%fd1026, %fd1025, %fd1024, %fd1023;
	fma.rn.f64 	%fd1027, %fd1026, %fd1026, %fd1026;
	fma.rn.f64 	%fd1028, %fd1027, %fd1024, %fd1024;
	add.f64 	%fd1029, %fd1133, 0dBFF0000000000000;
	mul.f64 	%fd1030, %fd1029, %fd1028;
	fma.rn.f64 	%fd1031, %fd1029, %fd1028, %fd1030;
	mul.f64 	%fd1032, %fd1031, %fd1031;
	mov.f64 	%fd1033, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1034, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1035, %fd1034, %fd1032, %fd1033;
	mov.f64 	%fd1036, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1037, %fd1035, %fd1032, %fd1036;
	mov.f64 	%fd1038, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1039, %fd1037, %fd1032, %fd1038;
	mov.f64 	%fd1040, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1041, %fd1039, %fd1032, %fd1040;
	mov.f64 	%fd1042, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1043, %fd1041, %fd1032, %fd1042;
	mov.f64 	%fd1044, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1045, %fd1043, %fd1032, %fd1044;
	mov.f64 	%fd1046, 0d3FB5555555555554;
	fma.rn.f64 	%fd1047, %fd1045, %fd1032, %fd1046;
	sub.f64 	%fd1048, %fd1029, %fd1031;
	add.f64 	%fd1049, %fd1048, %fd1048;
	neg.f64 	%fd1050, %fd1031;
	fma.rn.f64 	%fd1051, %fd1050, %fd1029, %fd1049;
	mul.f64 	%fd1052, %fd1028, %fd1051;
	mul.f64 	%fd1053, %fd1032, %fd1047;
	fma.rn.f64 	%fd1054, %fd1053, %fd1031, %fd1052;
	xor.b32  	%r523, %r549, -2147483648;
	mov.u32 	%r524, -2147483648;
	mov.u32 	%r525, 1127219200;
	mov.b64 	%fd1055, {%r523, %r525};
	mov.b64 	%fd1056, {%r524, %r525};
	sub.f64 	%fd1057, %fd1055, %fd1056;
	mov.f64 	%fd1058, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1059, %fd1057, %fd1058, %fd1031;
	neg.f64 	%fd1060, %fd1057;
	fma.rn.f64 	%fd1061, %fd1060, %fd1058, %fd1059;
	sub.f64 	%fd1062, %fd1061, %fd1031;
	sub.f64 	%fd1063, %fd1054, %fd1062;
	mov.f64 	%fd1064, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1065, %fd1057, %fd1064, %fd1063;
	add.f64 	%fd1134, %fd1059, %fd1065;
	bra.uni 	$L__BB8_123;

$L__BB8_119:
	mov.f64 	%fd1020, 0d7FF0000000000000;
	fma.rn.f64 	%fd1021, %fd1132, %fd1020, %fd1020;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r516}, %fd1132;
	}
	mov.b32 	%f5, %r516;
	setp.eq.f32 	%p104, %f5, 0f00000000;
	selp.f64 	%fd1134, 0dFFF0000000000000, %fd1021, %p104;

$L__BB8_123:
	sub.f64 	%fd1066, %fd146, %fd151;
	div.rn.f64 	%fd1067, %fd1066, %fd151;
	mul.f64 	%fd1068, %fd3, %fd1067;
	mul.f64 	%fd1069, %fd1067, %fd1068;
	mul.f64 	%fd1135, %fd1069, %fd1134;

$L__BB8_124:
	selp.f64 	%fd1072, %fd1135, 0d0000000000000000, %p90;
	mul.f64 	%fd1073, %fd180, %fd1072;
	mul.f64 	%fd1071, %fd1073, %fd196;
	mul.lo.s64 	%rd248, %rd69, %rd49;
	add.s64 	%rd247, %rd77, %rd248;
	// begin inline asm
	{ atom.add.f64 %fd1070,[%rd247],%fd1071; }

	// end inline asm

$L__BB8_125:
	ld.param.u64 	%rd255, [val_IPC_collisions_cuda_kernel_forward_param_0+24];
	add.s64 	%rd256, %rd256, %rd38;
	setp.lt.u64 	%p107, %rd256, %rd255;
	@%p107 bra 	$L__BB8_2;

$L__BB8_126:
	ret;

}
	// .globl	val_IPC_collisions_cuda_kernel_backward
.visible .entry val_IPC_collisions_cuda_kernel_backward(
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_21[56],
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_22,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_23,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_24,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_25,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_26,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_27,
	.param .u32 val_IPC_collisions_cuda_kernel_backward_param_28,
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_29[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_30[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_31[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_32[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_33[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_34[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_35[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_36[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_37[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_38[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_39[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_40[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_41[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_42[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_43[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_44[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_45[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_46[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_47[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_48[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_49[56],
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_50,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_51,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_52,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_53,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_54,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_55,
	.param .u32 val_IPC_collisions_cuda_kernel_backward_param_56
)
{
	.local .align 16 .b8 	__local_depot9[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<324>;
	.reg .b16 	%rs<309>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<1068>;
	.reg .f64 	%fd<5047>;
	.reg .b64 	%rd<679>;


	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r506, %r507}, [val_IPC_collisions_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r508, %r509}, [val_IPC_collisions_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r514, %r515}, [val_IPC_collisions_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r522, %r523}, [val_IPC_collisions_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r530, %r531}, [val_IPC_collisions_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r538, %r539}, [val_IPC_collisions_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r546, %r547}, [val_IPC_collisions_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r554, %r555}, [val_IPC_collisions_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r562, %r563}, [val_IPC_collisions_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r570, %r571}, [val_IPC_collisions_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r578, %r579}, [val_IPC_collisions_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r586, %r587}, [val_IPC_collisions_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r594, %r595}, [val_IPC_collisions_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r602, %r603}, [val_IPC_collisions_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r610, %r611}, [val_IPC_collisions_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r618, %r619}, [val_IPC_collisions_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r626, %r627}, [val_IPC_collisions_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r634, %r635}, [val_IPC_collisions_cuda_kernel_backward_param_16+32];
	ld.param.v2.u32 	{%r642, %r643}, [val_IPC_collisions_cuda_kernel_backward_param_17+32];
	ld.param.v2.u32 	{%r650, %r651}, [val_IPC_collisions_cuda_kernel_backward_param_18+32];
	ld.param.v2.u32 	{%r658, %r659}, [val_IPC_collisions_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r666, %r667}, [val_IPC_collisions_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r674, %r675}, [val_IPC_collisions_cuda_kernel_backward_param_21+32];
	ld.param.f64 	%fd1176, [val_IPC_collisions_cuda_kernel_backward_param_22];
	ld.param.f64 	%fd1177, [val_IPC_collisions_cuda_kernel_backward_param_23];
	ld.param.f64 	%fd1178, [val_IPC_collisions_cuda_kernel_backward_param_24];
	ld.param.f64 	%fd1179, [val_IPC_collisions_cuda_kernel_backward_param_25];
	ld.param.f64 	%fd1180, [val_IPC_collisions_cuda_kernel_backward_param_26];
	ld.param.f64 	%fd1181, [val_IPC_collisions_cuda_kernel_backward_param_27];
	ld.param.u32 	%r388, [val_IPC_collisions_cuda_kernel_backward_param_28];
	ld.param.v2.u32 	{%r682, %r683}, [val_IPC_collisions_cuda_kernel_backward_param_29+32];
	ld.param.v2.u32 	{%r690, %r691}, [val_IPC_collisions_cuda_kernel_backward_param_30+32];
	ld.param.v2.u32 	{%r698, %r699}, [val_IPC_collisions_cuda_kernel_backward_param_31+32];
	ld.param.v2.u32 	{%r706, %r707}, [val_IPC_collisions_cuda_kernel_backward_param_32+32];
	ld.param.v2.u32 	{%r714, %r715}, [val_IPC_collisions_cuda_kernel_backward_param_38+32];
	ld.param.v2.u32 	{%r722, %r723}, [val_IPC_collisions_cuda_kernel_backward_param_40+32];
	ld.param.v2.u32 	{%r730, %r731}, [val_IPC_collisions_cuda_kernel_backward_param_41+32];
	ld.param.v2.u32 	{%r738, %r739}, [val_IPC_collisions_cuda_kernel_backward_param_42+32];
	ld.param.v2.u32 	{%r746, %r747}, [val_IPC_collisions_cuda_kernel_backward_param_43+32];
	ld.param.v2.u32 	{%r754, %r755}, [val_IPC_collisions_cuda_kernel_backward_param_44+32];
	ld.param.v2.u32 	{%r762, %r763}, [val_IPC_collisions_cuda_kernel_backward_param_45+32];
	ld.param.v2.u32 	{%r770, %r771}, [val_IPC_collisions_cuda_kernel_backward_param_46+32];
	ld.param.v2.u32 	{%r778, %r779}, [val_IPC_collisions_cuda_kernel_backward_param_47+32];
	ld.param.u64 	%rd178, [val_IPC_collisions_cuda_kernel_backward_param_47];
	ld.param.u64 	%rd176, [val_IPC_collisions_cuda_kernel_backward_param_46];
	ld.param.u64 	%rd174, [val_IPC_collisions_cuda_kernel_backward_param_45];
	ld.param.u64 	%rd172, [val_IPC_collisions_cuda_kernel_backward_param_44];
	ld.param.u64 	%rd170, [val_IPC_collisions_cuda_kernel_backward_param_43];
	ld.param.u64 	%rd168, [val_IPC_collisions_cuda_kernel_backward_param_42];
	ld.param.u64 	%rd166, [val_IPC_collisions_cuda_kernel_backward_param_41];
	ld.param.u64 	%rd164, [val_IPC_collisions_cuda_kernel_backward_param_40];
	ld.param.u64 	%rd162, [val_IPC_collisions_cuda_kernel_backward_param_38];
	ld.param.u64 	%rd160, [val_IPC_collisions_cuda_kernel_backward_param_32];
	ld.param.u64 	%rd158, [val_IPC_collisions_cuda_kernel_backward_param_31];
	ld.param.u64 	%rd156, [val_IPC_collisions_cuda_kernel_backward_param_30];
	ld.param.u64 	%rd154, [val_IPC_collisions_cuda_kernel_backward_param_29];
	ld.param.u64 	%rd152, [val_IPC_collisions_cuda_kernel_backward_param_21];
	ld.param.u64 	%rd150, [val_IPC_collisions_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd149, [val_IPC_collisions_cuda_kernel_backward_param_19+8];
	ld.param.u64 	%rd148, [val_IPC_collisions_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd147, [val_IPC_collisions_cuda_kernel_backward_param_18+8];
	ld.param.u64 	%rd146, [val_IPC_collisions_cuda_kernel_backward_param_18];
	ld.param.u64 	%rd145, [val_IPC_collisions_cuda_kernel_backward_param_17+8];
	ld.param.u64 	%rd144, [val_IPC_collisions_cuda_kernel_backward_param_17];
	ld.param.u64 	%rd143, [val_IPC_collisions_cuda_kernel_backward_param_16+8];
	ld.param.u64 	%rd142, [val_IPC_collisions_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd141, [val_IPC_collisions_cuda_kernel_backward_param_15+8];
	ld.param.u64 	%rd140, [val_IPC_collisions_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd139, [val_IPC_collisions_cuda_kernel_backward_param_14+8];
	ld.param.u64 	%rd138, [val_IPC_collisions_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd137, [val_IPC_collisions_cuda_kernel_backward_param_13+8];
	ld.param.u64 	%rd136, [val_IPC_collisions_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd135, [val_IPC_collisions_cuda_kernel_backward_param_12+8];
	ld.param.u64 	%rd134, [val_IPC_collisions_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd132, [val_IPC_collisions_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd131, [val_IPC_collisions_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd130, [val_IPC_collisions_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd128, [val_IPC_collisions_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd126, [val_IPC_collisions_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd124, [val_IPC_collisions_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd122, [val_IPC_collisions_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd120, [val_IPC_collisions_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd119, [val_IPC_collisions_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd118, [val_IPC_collisions_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd117, [val_IPC_collisions_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd116, [val_IPC_collisions_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd115, [val_IPC_collisions_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd114, [val_IPC_collisions_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd113, [val_IPC_collisions_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd111, [val_IPC_collisions_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r198, [val_IPC_collisions_cuda_kernel_backward_param_0+16];
	mov.u32 	%r782, %ntid.x;
	mov.u32 	%r783, %ctaid.x;
	mul.wide.u32 	%rd180, %r782, %r783;
	mov.u32 	%r784, %tid.x;
	cvt.u64.u32 	%rd181, %r784;
	add.s64 	%rd675, %rd180, %rd181;
	setp.ge.u64 	%p6, %rd675, %rd111;
	@%p6 bra 	$L__BB9_449;

	cvta.to.global.u64 	%rd29, %rd138;
	cvta.to.global.u64 	%rd30, %rd134;
	cvta.to.global.u64 	%rd31, %rd130;
	cvta.to.global.u64 	%rd32, %rd126;
	cvta.to.global.u64 	%rd33, %rd124;
	cvta.to.global.u64 	%rd34, %rd122;
	cvta.to.global.u64 	%rd35, %rd120;
	cvta.to.global.u64 	%rd36, %rd118;
	cvta.to.global.u64 	%rd37, %rd116;
	cvta.to.global.u64 	%rd38, %rd114;
	cvta.to.global.u64 	%rd39, %rd152;
	cvta.to.global.u64 	%rd40, %rd150;
	cvta.to.global.u64 	%rd41, %rd148;
	cvta.to.global.u64 	%rd42, %rd146;
	cvta.to.global.u64 	%rd43, %rd144;
	cvta.to.global.u64 	%rd44, %rd142;
	cvta.to.global.u64 	%rd45, %rd140;
	cvta.to.global.u64 	%rd46, %rd136;
	cvta.to.global.u64 	%rd47, %rd132;
	cvta.to.global.u64 	%rd48, %rd128;
	cvt.s64.s32 	%rd49, %r509;
	cvt.s64.s32 	%rd50, %r508;
	cvt.s64.s32 	%rd51, %r507;
	cvt.s64.s32 	%rd52, %r578;
	cvt.s64.s32 	%rd53, %r554;
	cvt.s64.s32 	%rd54, %r546;
	cvt.s64.s32 	%rd55, %r674;
	cvt.s64.s32 	%rd56, %r594;
	cvt.s64.s32 	%rd57, %r586;
	cvt.s64.s32 	%rd58, %r602;
	cvt.s64.s32 	%rd59, %r530;
	cvt.s64.s32 	%rd60, %r626;
	cvt.s64.s32 	%rd61, %r642;
	cvt.s64.s32 	%rd62, %r666;
	cvt.s64.s32 	%rd63, %r634;
	mul.f64 	%fd1, %fd1176, %fd1176;
	add.f64 	%fd4858, %fd1176, %fd1176;
	cvt.s64.s32 	%rd64, %r650;
	mov.f64 	%fd1183, 0d0000000000000000;
	sub.f64 	%fd3, %fd1183, %fd1178;
	cvt.s64.s32 	%rd65, %r682;
	cvt.s64.s32 	%rd66, %r610;
	cvt.s64.s32 	%rd67, %r514;
	fma.rn.f64 	%fd4, %fd1180, 0d0000000000000000, 0d0000000000000000;
	cvt.s64.s32 	%rd68, %r658;
	cvt.s64.s32 	%rd69, %r562;
	cvt.s64.s32 	%rd70, %r618;
	cvt.s64.s32 	%rd71, %r570;
	cvt.s64.s32 	%rd72, %r522;
	cvt.s64.s32 	%rd73, %r538;
	cvt.s64.s32 	%rd74, %r778;
	cvt.s64.s32 	%rd75, %r770;
	cvt.s64.s32 	%rd76, %r730;
	cvt.s64.s32 	%rd77, %r698;
	cvt.s64.s32 	%rd78, %r722;
	cvt.s64.s32 	%rd79, %r762;
	mul.f64 	%fd5, %fd1177, %fd1181;
	mul.f64 	%fd6, %fd5, %fd5;
	div.rn.f64 	%fd7, %fd5, 0d4008000000000000;
	cvt.s64.s32 	%rd80, %r746;
	cvt.s64.s32 	%rd81, %r714;
	cvt.s64.s32 	%rd82, %r754;
	cvt.s64.s32 	%rd83, %r706;
	cvt.s64.s32 	%rd84, %r690;
	cvt.s64.s32 	%rd85, %r738;

$L__BB9_2:
	setp.lt.s32 	%p7, %r198, 4;
	mov.u64 	%rd676, %rd675;
	@%p7 bra 	$L__BB9_6;

	or.b64  	%rd182, %rd675, %rd49;
	and.b64  	%rd183, %rd182, -4294967296;
	setp.eq.s64 	%p8, %rd183, 0;
	@%p8 bra 	$L__BB9_5;

	div.u64 	%rd676, %rd675, %rd49;
	bra.uni 	$L__BB9_6;

$L__BB9_5:
	cvt.u32.u64 	%r786, %rd49;
	cvt.u32.u64 	%r787, %rd675;
	div.u32 	%r788, %r787, %r786;
	cvt.u64.u32 	%rd676, %r788;

$L__BB9_6:
	setp.lt.s32 	%p9, %r198, 3;
	@%p9 bra 	$L__BB9_10;

	or.b64  	%rd184, %rd676, %rd50;
	and.b64  	%rd185, %rd184, -4294967296;
	setp.eq.s64 	%p10, %rd185, 0;
	@%p10 bra 	$L__BB9_9;

	div.u64 	%rd676, %rd676, %rd50;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r789, %rd50;
	cvt.u32.u64 	%r790, %rd676;
	div.u32 	%r791, %r790, %r789;
	cvt.u64.u32 	%rd676, %r791;

$L__BB9_10:
	setp.lt.s32 	%p11, %r198, 2;
	@%p11 bra 	$L__BB9_14;

	or.b64  	%rd186, %rd676, %rd51;
	and.b64  	%rd187, %rd186, -4294967296;
	setp.eq.s64 	%p12, %rd187, 0;
	@%p12 bra 	$L__BB9_13;

	div.u64 	%rd676, %rd676, %rd51;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r792, %rd51;
	cvt.u32.u64 	%r793, %rd676;
	div.u32 	%r794, %r793, %r792;
	cvt.u64.u32 	%rd676, %r794;

$L__BB9_14:
	cvt.s64.s32 	%rd188, %rd676;
	setp.gt.s32 	%p13, %r198, 0;
	selp.b64 	%rd96, %rd188, 0, %p13;
	mul.lo.s64 	%rd189, %rd96, %rd52;
	add.s64 	%rd190, %rd48, %rd189;
	ld.global.u32 	%r31, [%rd190];
	setp.gt.u32 	%p14, %r31, 1;
	mul.lo.s64 	%rd191, %rd96, %rd53;
	add.s64 	%rd97, %rd34, %rd191;
	mov.f64 	%fd4817, 0d0000000000000000;
	mov.f64 	%fd4713, %fd4817;
	mov.f64 	%fd4714, %fd4817;
	mov.f64 	%fd4715, %fd4817;
	mov.f64 	%fd4716, %fd4817;
	mov.f64 	%fd4717, %fd4817;
	mov.f64 	%fd4718, %fd4817;
	mov.f64 	%fd4719, %fd4817;
	mov.f64 	%fd4720, %fd4817;
	mov.f64 	%fd4721, %fd4817;
	mov.f64 	%fd4722, %fd4817;
	mov.f64 	%fd4723, %fd4817;
	mov.f64 	%fd4724, %fd4817;
	mov.f64 	%fd4725, %fd4817;
	mov.f64 	%fd4726, %fd4817;
	mov.f64 	%fd4727, %fd4817;
	mov.f64 	%fd4728, %fd4817;
	mov.f64 	%fd4729, %fd4817;
	mov.f64 	%fd4730, %fd4817;
	mov.f64 	%fd4731, %fd4817;
	mov.f64 	%fd4732, %fd4817;
	mov.f64 	%fd4733, %fd4817;
	mov.f64 	%fd4738, %fd4817;
	mov.f64 	%fd4739, %fd4817;
	mov.f64 	%fd4740, %fd4817;
	mov.f64 	%fd4741, %fd4817;
	mov.f64 	%fd4742, %fd4817;
	mov.f64 	%fd4743, %fd4817;
	mov.f64 	%fd4746, %fd4817;
	mov.f64 	%fd4747, %fd4817;
	mov.f64 	%fd4748, %fd4817;
	@%p14 bra 	$L__BB9_58;

	mul.lo.s64 	%rd192, %rd96, %rd54;
	add.s64 	%rd193, %rd35, %rd192;
	ld.global.u32 	%r796, [%rd193];
	ld.global.u32 	%r797, [%rd97];
	setp.eq.s32 	%p15, %r31, 1;
	selp.b32 	%r999, %r796, %r797, %p15;
	selp.b32 	%r1000, %r797, %r796, %p15;
	cvt.s64.s32 	%rd194, %r1000;
	mul.lo.s64 	%rd195, %rd194, %rd55;
	add.s64 	%rd196, %rd39, %rd195;
	cvt.s64.s32 	%rd98, %r999;
	mul.lo.s64 	%rd197, %rd98, %rd56;
	add.s64 	%rd198, %rd47, %rd197;
	ld.global.u32 	%r1001, [%rd198];
	cvt.s64.s32 	%rd199, %r1001;
	mul.lo.s64 	%rd200, %rd199, %rd57;
	add.s64 	%rd201, %rd31, %rd200;
	ld.global.u32 	%r1005, [%rd196];
	cvt.s64.s32 	%rd202, %r1005;
	mul.lo.s64 	%rd203, %rd202, %rd57;
	add.s64 	%rd204, %rd31, %rd203;
	ld.global.u32 	%r1006, [%rd196+4];
	cvt.s64.s32 	%rd205, %r1006;
	mul.lo.s64 	%rd206, %rd205, %rd57;
	add.s64 	%rd207, %rd31, %rd206;
	ld.global.u32 	%r1007, [%rd196+8];
	cvt.s64.s32 	%rd208, %r1007;
	mul.lo.s64 	%rd209, %rd208, %rd57;
	add.s64 	%rd210, %rd31, %rd209;
	mul.lo.s64 	%rd211, %rd199, %rd58;
	add.s64 	%rd212, %rd30, %rd211;
	ld.global.f64 	%fd80, [%rd212];
	ld.global.f64 	%fd81, [%rd212+8];
	ld.global.f64 	%fd82, [%rd212+16];
	mul.lo.s64 	%rd213, %rd202, %rd58;
	add.s64 	%rd214, %rd30, %rd213;
	ld.global.f64 	%fd4727, [%rd214];
	ld.global.f64 	%fd4726, [%rd214+8];
	ld.global.f64 	%fd4725, [%rd214+16];
	mul.lo.s64 	%rd215, %rd205, %rd58;
	add.s64 	%rd216, %rd30, %rd215;
	ld.global.f64 	%fd4730, [%rd216];
	ld.global.f64 	%fd4729, [%rd216+8];
	ld.global.f64 	%fd4728, [%rd216+16];
	mul.lo.s64 	%rd217, %rd208, %rd58;
	add.s64 	%rd218, %rd30, %rd217;
	ld.global.f64 	%fd4733, [%rd218];
	ld.global.f64 	%fd4732, [%rd218+8];
	ld.global.f64 	%fd4731, [%rd218+16];
	mul.lo.s64 	%rd219, %rd96, %rd59;
	add.s64 	%rd220, %rd37, %rd219;
	ld.global.f64 	%fd4734, [%rd220];
	ld.global.f64 	%fd4735, [%rd220+8];
	mul.lo.s64 	%rd221, %rd98, %rd60;
	add.s64 	%rd222, %rd45, %rd221;
	mul.lo.s64 	%rd223, %rd194, %rd61;
	add.s64 	%rd224, %rd43, %rd223;
	ld.global.f64 	%fd1214, [%rd224];
	ld.global.f64 	%fd1215, [%rd222];
	add.f64 	%fd4736, %fd1215, %fd1214;
	ld.global.f64 	%fd4721, [%rd207];
	ld.global.f64 	%fd4718, [%rd204];
	sub.f64 	%fd1216, %fd4721, %fd4718;
	ld.global.f64 	%fd4720, [%rd207+8];
	ld.global.f64 	%fd4717, [%rd204+8];
	sub.f64 	%fd1217, %fd4720, %fd4717;
	ld.global.f64 	%fd4719, [%rd207+16];
	ld.global.f64 	%fd4716, [%rd204+16];
	sub.f64 	%fd1218, %fd4719, %fd4716;
	ld.global.f64 	%fd4724, [%rd210];
	sub.f64 	%fd1219, %fd4724, %fd4718;
	ld.global.f64 	%fd4723, [%rd210+8];
	sub.f64 	%fd1220, %fd4723, %fd4717;
	ld.global.f64 	%fd4722, [%rd210+16];
	sub.f64 	%fd1221, %fd4722, %fd4716;
	mul.f64 	%fd1222, %fd1217, %fd1221;
	mul.f64 	%fd1223, %fd1218, %fd1220;
	sub.f64 	%fd104, %fd1222, %fd1223;
	mul.f64 	%fd1224, %fd1218, %fd1219;
	mul.f64 	%fd1225, %fd1216, %fd1221;
	sub.f64 	%fd105, %fd1224, %fd1225;
	mul.f64 	%fd1226, %fd1216, %fd1220;
	mul.f64 	%fd1227, %fd1217, %fd1219;
	sub.f64 	%fd106, %fd1226, %fd1227;
	mul.f64 	%fd1228, %fd1217, %fd106;
	mul.f64 	%fd1229, %fd1218, %fd105;
	sub.f64 	%fd1230, %fd1228, %fd1229;
	mul.f64 	%fd1231, %fd1218, %fd104;
	mul.f64 	%fd1232, %fd1216, %fd106;
	sub.f64 	%fd1233, %fd1231, %fd1232;
	mul.f64 	%fd1234, %fd1216, %fd105;
	mul.f64 	%fd1235, %fd1217, %fd104;
	sub.f64 	%fd1236, %fd1234, %fd1235;
	mul.f64 	%fd1237, %fd1217, %fd1217;
	fma.rn.f64 	%fd1238, %fd1216, %fd1216, %fd1237;
	fma.rn.f64 	%fd107, %fd1218, %fd1218, %fd1238;
	mul.f64 	%fd1239, %fd1217, %fd1233;
	fma.rn.f64 	%fd1240, %fd1216, %fd1230, %fd1239;
	fma.rn.f64 	%fd1241, %fd1218, %fd1236, %fd1240;
	mul.f64 	%fd1242, %fd1233, %fd1233;
	fma.rn.f64 	%fd1243, %fd1230, %fd1230, %fd1242;
	fma.rn.f64 	%fd1244, %fd1236, %fd1236, %fd1243;
	ld.global.f64 	%fd4715, [%rd201];
	sub.f64 	%fd109, %fd4715, %fd4718;
	ld.global.f64 	%fd4714, [%rd201+8];
	sub.f64 	%fd111, %fd4714, %fd4717;
	ld.global.f64 	%fd4713, [%rd201+16];
	sub.f64 	%fd113, %fd4713, %fd4716;
	mul.f64 	%fd1245, %fd111, %fd1217;
	fma.rn.f64 	%fd1246, %fd109, %fd1216, %fd1245;
	fma.rn.f64 	%fd1247, %fd113, %fd1218, %fd1246;
	mul.f64 	%fd1248, %fd111, %fd1233;
	fma.rn.f64 	%fd1249, %fd109, %fd1230, %fd1248;
	fma.rn.f64 	%fd1250, %fd113, %fd1236, %fd1249;
	div.rn.f64 	%fd1251, %fd1241, %fd107;
	mul.f64 	%fd1252, %fd1251, %fd1251;
	mul.f64 	%fd1253, %fd107, %fd1252;
	sub.f64 	%fd1254, %fd1244, %fd1253;
	mul.f64 	%fd1255, %fd1247, %fd1251;
	sub.f64 	%fd1256, %fd1250, %fd1255;
	div.rn.f64 	%fd1257, %fd1256, %fd1254;
	mul.f64 	%fd1258, %fd107, %fd1251;
	mul.f64 	%fd1259, %fd1258, %fd1257;
	sub.f64 	%fd1260, %fd1247, %fd1259;
	div.rn.f64 	%fd114, %fd1260, %fd107;
	setp.gt.f64 	%p16, %fd114, 0d0000000000000000;
	setp.lt.f64 	%p17, %fd114, 0d3FF0000000000000;
	setp.ge.f64 	%p18, %fd1257, 0d0000000000000000;
	and.pred  	%p19, %p16, %p17;
	and.pred  	%p20, %p18, %p19;
	mov.u32 	%r990, 3;
	@%p20 bra 	$L__BB9_21;

	sub.f64 	%fd1261, %fd4724, %fd4721;
	sub.f64 	%fd1262, %fd4723, %fd4720;
	mul.f64 	%fd1263, %fd1262, %fd106;
	sub.f64 	%fd1264, %fd4722, %fd4719;
	mul.f64 	%fd1265, %fd1264, %fd105;
	sub.f64 	%fd1266, %fd1263, %fd1265;
	mul.f64 	%fd1267, %fd1264, %fd104;
	mul.f64 	%fd1268, %fd1261, %fd106;
	sub.f64 	%fd1269, %fd1267, %fd1268;
	mul.f64 	%fd1270, %fd1261, %fd105;
	mul.f64 	%fd1271, %fd1262, %fd104;
	sub.f64 	%fd1272, %fd1270, %fd1271;
	mul.f64 	%fd1273, %fd1262, %fd1262;
	fma.rn.f64 	%fd1274, %fd1261, %fd1261, %fd1273;
	fma.rn.f64 	%fd1275, %fd1264, %fd1264, %fd1274;
	mul.f64 	%fd1276, %fd1262, %fd1269;
	fma.rn.f64 	%fd1277, %fd1261, %fd1266, %fd1276;
	fma.rn.f64 	%fd1278, %fd1264, %fd1272, %fd1277;
	mul.f64 	%fd1279, %fd1269, %fd1269;
	fma.rn.f64 	%fd1280, %fd1266, %fd1266, %fd1279;
	fma.rn.f64 	%fd1281, %fd1272, %fd1272, %fd1280;
	sub.f64 	%fd1282, %fd4715, %fd4721;
	sub.f64 	%fd1283, %fd4714, %fd4720;
	mul.f64 	%fd1284, %fd1283, %fd1262;
	fma.rn.f64 	%fd1285, %fd1282, %fd1261, %fd1284;
	sub.f64 	%fd1286, %fd4713, %fd4719;
	fma.rn.f64 	%fd1287, %fd1286, %fd1264, %fd1285;
	mul.f64 	%fd1288, %fd1283, %fd1269;
	fma.rn.f64 	%fd1289, %fd1282, %fd1266, %fd1288;
	fma.rn.f64 	%fd1290, %fd1286, %fd1272, %fd1289;
	div.rn.f64 	%fd1291, %fd1278, %fd1275;
	mul.f64 	%fd1292, %fd1291, %fd1291;
	mul.f64 	%fd1293, %fd1275, %fd1292;
	sub.f64 	%fd1294, %fd1281, %fd1293;
	mul.f64 	%fd1295, %fd1287, %fd1291;
	sub.f64 	%fd1296, %fd1290, %fd1295;
	div.rn.f64 	%fd1297, %fd1296, %fd1294;
	mul.f64 	%fd1298, %fd1275, %fd1291;
	mul.f64 	%fd1299, %fd1298, %fd1297;
	sub.f64 	%fd1300, %fd1287, %fd1299;
	div.rn.f64 	%fd115, %fd1300, %fd1275;
	setp.gt.f64 	%p21, %fd115, 0d0000000000000000;
	setp.lt.f64 	%p22, %fd115, 0d3FF0000000000000;
	setp.ge.f64 	%p23, %fd1297, 0d0000000000000000;
	and.pred  	%p24, %p21, %p22;
	and.pred  	%p25, %p23, %p24;
	mov.u32 	%r990, 4;
	@%p25 bra 	$L__BB9_21;

	sub.f64 	%fd1301, %fd4718, %fd4724;
	sub.f64 	%fd1302, %fd4717, %fd4723;
	mul.f64 	%fd1303, %fd1302, %fd106;
	sub.f64 	%fd1304, %fd4716, %fd4722;
	mul.f64 	%fd1305, %fd1304, %fd105;
	sub.f64 	%fd1306, %fd1303, %fd1305;
	mul.f64 	%fd1307, %fd1304, %fd104;
	mul.f64 	%fd1308, %fd1301, %fd106;
	sub.f64 	%fd1309, %fd1307, %fd1308;
	mul.f64 	%fd1310, %fd1301, %fd105;
	mul.f64 	%fd1311, %fd1302, %fd104;
	sub.f64 	%fd1312, %fd1310, %fd1311;
	mul.f64 	%fd1313, %fd1302, %fd1302;
	fma.rn.f64 	%fd1314, %fd1301, %fd1301, %fd1313;
	fma.rn.f64 	%fd1315, %fd1304, %fd1304, %fd1314;
	mul.f64 	%fd1316, %fd1302, %fd1309;
	fma.rn.f64 	%fd1317, %fd1301, %fd1306, %fd1316;
	fma.rn.f64 	%fd1318, %fd1304, %fd1312, %fd1317;
	mul.f64 	%fd1319, %fd1309, %fd1309;
	fma.rn.f64 	%fd1320, %fd1306, %fd1306, %fd1319;
	fma.rn.f64 	%fd1321, %fd1312, %fd1312, %fd1320;
	sub.f64 	%fd1322, %fd4715, %fd4724;
	sub.f64 	%fd1323, %fd4714, %fd4723;
	mul.f64 	%fd1324, %fd1302, %fd1323;
	fma.rn.f64 	%fd1325, %fd1301, %fd1322, %fd1324;
	sub.f64 	%fd1326, %fd4713, %fd4722;
	fma.rn.f64 	%fd1327, %fd1304, %fd1326, %fd1325;
	mul.f64 	%fd1328, %fd1323, %fd1309;
	fma.rn.f64 	%fd1329, %fd1322, %fd1306, %fd1328;
	fma.rn.f64 	%fd1330, %fd1326, %fd1312, %fd1329;
	div.rn.f64 	%fd1331, %fd1318, %fd1315;
	mul.f64 	%fd1332, %fd1331, %fd1331;
	mul.f64 	%fd1333, %fd1315, %fd1332;
	sub.f64 	%fd1334, %fd1321, %fd1333;
	mul.f64 	%fd1335, %fd1327, %fd1331;
	sub.f64 	%fd1336, %fd1330, %fd1335;
	div.rn.f64 	%fd1337, %fd1336, %fd1334;
	mul.f64 	%fd1338, %fd1315, %fd1331;
	mul.f64 	%fd1339, %fd1338, %fd1337;
	sub.f64 	%fd1340, %fd1327, %fd1339;
	div.rn.f64 	%fd116, %fd1340, %fd1315;
	setp.gt.f64 	%p26, %fd116, 0d0000000000000000;
	setp.lt.f64 	%p27, %fd116, 0d3FF0000000000000;
	setp.ge.f64 	%p28, %fd1337, 0d0000000000000000;
	and.pred  	%p29, %p26, %p27;
	and.pred  	%p30, %p28, %p29;
	mov.u32 	%r990, 5;
	@%p30 bra 	$L__BB9_21;

	setp.le.f64 	%p31, %fd114, 0d0000000000000000;
	setp.ge.f64 	%p32, %fd116, 0d3FF0000000000000;
	and.pred  	%p33, %p31, %p32;
	mov.u32 	%r990, 0;
	@%p33 bra 	$L__BB9_21;

	setp.le.f64 	%p34, %fd115, 0d0000000000000000;
	setp.ge.f64 	%p35, %fd114, 0d3FF0000000000000;
	and.pred  	%p36, %p34, %p35;
	mov.u32 	%r990, 1;
	@%p36 bra 	$L__BB9_21;

	setp.le.f64 	%p37, %fd116, 0d0000000000000000;
	setp.ge.f64 	%p38, %fd115, 0d3FF0000000000000;
	and.pred  	%p39, %p37, %p38;
	selp.b32 	%r990, 2, 6, %p39;

$L__BB9_21:
	setp.eq.s32 	%p40, %r990, 0;
	@%p40 bra 	$L__BB9_33;

	setp.eq.s32 	%p41, %r990, 1;
	@%p41 bra 	$L__BB9_32;
	bra.uni 	$L__BB9_23;

$L__BB9_32:
	sub.f64 	%fd1419, %fd4715, %fd4721;
	sub.f64 	%fd1420, %fd4714, %fd4720;
	mul.f64 	%fd1421, %fd1420, %fd1420;
	fma.rn.f64 	%fd1422, %fd1419, %fd1419, %fd1421;
	sub.f64 	%fd1423, %fd4713, %fd4719;
	fma.rn.f64 	%fd4702, %fd1423, %fd1423, %fd1422;
	bra.uni 	$L__BB9_34;

$L__BB9_33:
	mul.f64 	%fd1424, %fd111, %fd111;
	fma.rn.f64 	%fd1425, %fd109, %fd109, %fd1424;
	fma.rn.f64 	%fd4702, %fd113, %fd113, %fd1425;
	bra.uni 	$L__BB9_34;

$L__BB9_23:
	setp.eq.s32 	%p42, %r990, 2;
	@%p42 bra 	$L__BB9_31;
	bra.uni 	$L__BB9_24;

$L__BB9_31:
	sub.f64 	%fd1414, %fd4715, %fd4724;
	sub.f64 	%fd1415, %fd4714, %fd4723;
	mul.f64 	%fd1416, %fd1415, %fd1415;
	fma.rn.f64 	%fd1417, %fd1414, %fd1414, %fd1416;
	sub.f64 	%fd1418, %fd4713, %fd4722;
	fma.rn.f64 	%fd4702, %fd1418, %fd1418, %fd1417;
	bra.uni 	$L__BB9_34;

$L__BB9_24:
	setp.eq.s32 	%p43, %r990, 3;
	@%p43 bra 	$L__BB9_30;
	bra.uni 	$L__BB9_25;

$L__BB9_30:
	sub.f64 	%fd1396, %fd4718, %fd4715;
	sub.f64 	%fd1397, %fd4719, %fd4713;
	sub.f64 	%fd1398, %fd4717, %fd4714;
	mul.f64 	%fd1399, %fd1398, %fd1397;
	sub.f64 	%fd1400, %fd4720, %fd4714;
	sub.f64 	%fd1401, %fd4716, %fd4713;
	mul.f64 	%fd1402, %fd1401, %fd1400;
	sub.f64 	%fd1403, %fd1399, %fd1402;
	sub.f64 	%fd1404, %fd4721, %fd4715;
	mul.f64 	%fd1405, %fd1401, %fd1404;
	mul.f64 	%fd1406, %fd1396, %fd1397;
	sub.f64 	%fd1407, %fd1405, %fd1406;
	mul.f64 	%fd1408, %fd1396, %fd1400;
	mul.f64 	%fd1409, %fd1398, %fd1404;
	sub.f64 	%fd1410, %fd1408, %fd1409;
	mul.f64 	%fd1411, %fd1407, %fd1407;
	fma.rn.f64 	%fd1412, %fd1403, %fd1403, %fd1411;
	fma.rn.f64 	%fd1413, %fd1410, %fd1410, %fd1412;
	div.rn.f64 	%fd4702, %fd1413, %fd107;
	bra.uni 	$L__BB9_34;

$L__BB9_25:
	setp.eq.s32 	%p44, %r990, 4;
	@%p44 bra 	$L__BB9_29;
	bra.uni 	$L__BB9_26;

$L__BB9_29:
	sub.f64 	%fd1372, %fd4721, %fd4715;
	sub.f64 	%fd1373, %fd4722, %fd4713;
	sub.f64 	%fd1374, %fd4720, %fd4714;
	mul.f64 	%fd1375, %fd1374, %fd1373;
	sub.f64 	%fd1376, %fd4723, %fd4714;
	sub.f64 	%fd1377, %fd4719, %fd4713;
	mul.f64 	%fd1378, %fd1377, %fd1376;
	sub.f64 	%fd1379, %fd1375, %fd1378;
	sub.f64 	%fd1380, %fd4724, %fd4715;
	mul.f64 	%fd1381, %fd1377, %fd1380;
	mul.f64 	%fd1382, %fd1372, %fd1373;
	sub.f64 	%fd1383, %fd1381, %fd1382;
	mul.f64 	%fd1384, %fd1372, %fd1376;
	mul.f64 	%fd1385, %fd1374, %fd1380;
	sub.f64 	%fd1386, %fd1384, %fd1385;
	mul.f64 	%fd1387, %fd1383, %fd1383;
	fma.rn.f64 	%fd1388, %fd1379, %fd1379, %fd1387;
	fma.rn.f64 	%fd1389, %fd1386, %fd1386, %fd1388;
	sub.f64 	%fd1390, %fd4724, %fd4721;
	sub.f64 	%fd1391, %fd4723, %fd4720;
	mul.f64 	%fd1392, %fd1391, %fd1391;
	fma.rn.f64 	%fd1393, %fd1390, %fd1390, %fd1392;
	sub.f64 	%fd1394, %fd4722, %fd4719;
	fma.rn.f64 	%fd1395, %fd1394, %fd1394, %fd1393;
	div.rn.f64 	%fd4702, %fd1389, %fd1395;
	bra.uni 	$L__BB9_34;

$L__BB9_26:
	setp.eq.s32 	%p45, %r990, 5;
	@%p45 bra 	$L__BB9_28;
	bra.uni 	$L__BB9_27;

$L__BB9_28:
	sub.f64 	%fd1348, %fd4724, %fd4715;
	sub.f64 	%fd1349, %fd4716, %fd4713;
	sub.f64 	%fd1350, %fd4723, %fd4714;
	mul.f64 	%fd1351, %fd1349, %fd1350;
	sub.f64 	%fd1352, %fd4717, %fd4714;
	sub.f64 	%fd1353, %fd4722, %fd4713;
	mul.f64 	%fd1354, %fd1352, %fd1353;
	sub.f64 	%fd1355, %fd1351, %fd1354;
	sub.f64 	%fd1356, %fd4718, %fd4715;
	mul.f64 	%fd1357, %fd1356, %fd1353;
	mul.f64 	%fd1358, %fd1349, %fd1348;
	sub.f64 	%fd1359, %fd1357, %fd1358;
	mul.f64 	%fd1360, %fd1352, %fd1348;
	mul.f64 	%fd1361, %fd1356, %fd1350;
	sub.f64 	%fd1362, %fd1360, %fd1361;
	mul.f64 	%fd1363, %fd1359, %fd1359;
	fma.rn.f64 	%fd1364, %fd1355, %fd1355, %fd1363;
	fma.rn.f64 	%fd1365, %fd1362, %fd1362, %fd1364;
	sub.f64 	%fd1366, %fd4718, %fd4724;
	sub.f64 	%fd1367, %fd4717, %fd4723;
	mul.f64 	%fd1368, %fd1367, %fd1367;
	fma.rn.f64 	%fd1369, %fd1366, %fd1366, %fd1368;
	sub.f64 	%fd1370, %fd4716, %fd4722;
	fma.rn.f64 	%fd1371, %fd1370, %fd1370, %fd1369;
	div.rn.f64 	%fd4702, %fd1365, %fd1371;
	bra.uni 	$L__BB9_34;

$L__BB9_27:
	mul.f64 	%fd1341, %fd111, %fd105;
	fma.rn.f64 	%fd1342, %fd109, %fd104, %fd1341;
	fma.rn.f64 	%fd1343, %fd113, %fd106, %fd1342;
	mul.f64 	%fd1344, %fd1343, %fd1343;
	mul.f64 	%fd1345, %fd105, %fd105;
	fma.rn.f64 	%fd1346, %fd104, %fd104, %fd1345;
	fma.rn.f64 	%fd1347, %fd106, %fd106, %fd1346;
	div.rn.f64 	%fd4702, %fd1344, %fd1347;

$L__BB9_34:
	mul.f64 	%fd1427, %fd4736, %fd4736;
	sub.f64 	%fd4737, %fd4702, %fd1427;
	mov.f64 	%fd1428, 0d3FF0000000000000;
	sub.f64 	%fd1429, %fd1428, %fd4734;
	sub.f64 	%fd4750, %fd1429, %fd4735;
	fma.rn.f64 	%fd4744, %fd4858, %fd4736, %fd1;
	mul.lo.s64 	%rd225, %rd98, %rd64;
	add.s64 	%rd99, %rd42, %rd225;
	ld.global.f64 	%fd1430, [%rd99];
	mul.f64 	%fd1431, %fd1430, %fd1179;
	mul.f64 	%fd4752, %fd1431, %fd1176;
	setp.geu.f64 	%p46, %fd4737, %fd4744;
	@%p46 bra 	$L__BB9_43;

	div.rn.f64 	%fd4703, %fd4737, %fd4744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r991}, %fd4703;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r992, %temp}, %fd4703;
	}
	setp.gt.s32 	%p47, %r991, 1048575;
	mov.u32 	%r993, -1023;
	@%p47 bra 	$L__BB9_37;

	mul.f64 	%fd4703, %fd4703, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r991}, %fd4703;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r992, %temp}, %fd4703;
	}
	mov.u32 	%r993, -1077;

$L__BB9_37:
	add.s32 	%r804, %r991, -1;
	setp.lt.u32 	%p48, %r804, 2146435071;
	@%p48 bra 	$L__BB9_39;
	bra.uni 	$L__BB9_38;

$L__BB9_39:
	shr.u32 	%r806, %r991, 20;
	add.s32 	%r994, %r993, %r806;
	and.b32  	%r807, %r991, -2146435073;
	or.b32  	%r808, %r807, 1072693248;
	mov.b64 	%fd4704, {%r992, %r808};
	setp.lt.s32 	%p50, %r808, 1073127583;
	@%p50 bra 	$L__BB9_41;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r809, %temp}, %fd4704;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r810}, %fd4704;
	}
	add.s32 	%r811, %r810, -1048576;
	mov.b64 	%fd4704, {%r809, %r811};
	add.s32 	%r994, %r994, 1;

$L__BB9_41:
	add.f64 	%fd1434, %fd4704, 0d3FF0000000000000;
	mov.f64 	%fd1435, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1436, %fd1434;
	neg.f64 	%fd1437, %fd1434;
	fma.rn.f64 	%fd1438, %fd1437, %fd1436, %fd1435;
	fma.rn.f64 	%fd1439, %fd1438, %fd1438, %fd1438;
	fma.rn.f64 	%fd1440, %fd1439, %fd1436, %fd1436;
	add.f64 	%fd1441, %fd4704, 0dBFF0000000000000;
	mul.f64 	%fd1442, %fd1441, %fd1440;
	fma.rn.f64 	%fd1443, %fd1441, %fd1440, %fd1442;
	mul.f64 	%fd1444, %fd1443, %fd1443;
	mov.f64 	%fd1445, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1446, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1447, %fd1446, %fd1444, %fd1445;
	mov.f64 	%fd1448, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1449, %fd1447, %fd1444, %fd1448;
	mov.f64 	%fd1450, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1451, %fd1449, %fd1444, %fd1450;
	mov.f64 	%fd1452, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1453, %fd1451, %fd1444, %fd1452;
	mov.f64 	%fd1454, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1455, %fd1453, %fd1444, %fd1454;
	mov.f64 	%fd1456, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1457, %fd1455, %fd1444, %fd1456;
	mov.f64 	%fd1458, 0d3FB5555555555554;
	fma.rn.f64 	%fd1459, %fd1457, %fd1444, %fd1458;
	sub.f64 	%fd1460, %fd1441, %fd1443;
	add.f64 	%fd1461, %fd1460, %fd1460;
	neg.f64 	%fd1462, %fd1443;
	fma.rn.f64 	%fd1463, %fd1462, %fd1441, %fd1461;
	mul.f64 	%fd1464, %fd1440, %fd1463;
	mul.f64 	%fd1465, %fd1444, %fd1459;
	fma.rn.f64 	%fd1466, %fd1465, %fd1443, %fd1464;
	xor.b32  	%r812, %r994, -2147483648;
	mov.u32 	%r813, -2147483648;
	mov.u32 	%r814, 1127219200;
	mov.b64 	%fd1467, {%r812, %r814};
	mov.b64 	%fd1468, {%r813, %r814};
	sub.f64 	%fd1469, %fd1467, %fd1468;
	mov.f64 	%fd1470, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1471, %fd1469, %fd1470, %fd1443;
	neg.f64 	%fd1472, %fd1469;
	fma.rn.f64 	%fd1473, %fd1472, %fd1470, %fd1471;
	sub.f64 	%fd1474, %fd1473, %fd1443;
	sub.f64 	%fd1475, %fd1466, %fd1474;
	mov.f64 	%fd1476, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1477, %fd1469, %fd1476, %fd1475;
	add.f64 	%fd4705, %fd1471, %fd1477;
	bra.uni 	$L__BB9_42;

$L__BB9_38:
	mov.f64 	%fd1432, 0d7FF0000000000000;
	fma.rn.f64 	%fd1433, %fd4703, %fd1432, %fd1432;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r805}, %fd4703;
	}
	mov.b32 	%f1, %r805;
	setp.eq.f32 	%p49, %f1, 0f00000000;
	selp.f64 	%fd4705, 0dFFF0000000000000, %fd1433, %p49;

$L__BB9_42:
	sub.f64 	%fd1478, %fd4737, %fd4744;
	div.rn.f64 	%fd1479, %fd1478, %fd4744;
	mul.f64 	%fd1480, %fd3, %fd1479;
	mul.f64 	%fd1481, %fd1479, %fd1480;
	mul.f64 	%fd4706, %fd1481, %fd4705;

$L__BB9_43:
	setp.lt.f64 	%p51, %fd4737, %fd4744;
	selp.f64 	%fd4753, %fd4706, 0d0000000000000000, %p51;
	mul.f64 	%fd1482, %fd4752, %fd4753;
	mul.f64 	%fd1483, %fd1482, %fd1180;
	setp.nan.f64 	%p1, %fd1483, %fd1483;
	setp.num.f64 	%p52, %fd1483, %fd1483;
	@%p52 bra 	$L__BB9_45;

	add.u64 	%rd226, %SP, 0;
	add.u64 	%rd227, %SPL, 0;
	mov.u64 	%rd228, $str$3;
	cvta.global.u64 	%rd229, %rd228;
	st.local.u64 	[%rd227], %rd229;
	mov.u64 	%rd230, $str$5;
	cvta.global.u64 	%rd231, %rd230;
	{ // callseq 510, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd231;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd226;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r815, [retval0+0];
	} // callseq 510

$L__BB9_45:
	setp.ne.s32 	%p53, %r388, 0;
	@%p53 bra 	$L__BB9_56;

	ld.global.f64 	%fd141, [%rd99];
	@%p46 bra 	$L__BB9_55;

	div.rn.f64 	%fd4707, %fd4737, %fd4744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r995}, %fd4707;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r996, %temp}, %fd4707;
	}
	setp.gt.s32 	%p55, %r995, 1048575;
	mov.u32 	%r997, -1023;
	@%p55 bra 	$L__BB9_49;

	mul.f64 	%fd4707, %fd4707, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r995}, %fd4707;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r996, %temp}, %fd4707;
	}
	mov.u32 	%r997, -1077;

$L__BB9_49:
	add.s32 	%r818, %r995, -1;
	setp.lt.u32 	%p56, %r818, 2146435071;
	@%p56 bra 	$L__BB9_51;
	bra.uni 	$L__BB9_50;

$L__BB9_51:
	shr.u32 	%r820, %r995, 20;
	add.s32 	%r998, %r997, %r820;
	and.b32  	%r821, %r995, -2146435073;
	or.b32  	%r822, %r821, 1072693248;
	mov.b64 	%fd4708, {%r996, %r822};
	setp.lt.s32 	%p58, %r822, 1073127583;
	@%p58 bra 	$L__BB9_53;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r823, %temp}, %fd4708;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r824}, %fd4708;
	}
	add.s32 	%r825, %r824, -1048576;
	mov.b64 	%fd4708, {%r823, %r825};
	add.s32 	%r998, %r998, 1;

$L__BB9_53:
	add.f64 	%fd1487, %fd4708, 0d3FF0000000000000;
	mov.f64 	%fd1488, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1489, %fd1487;
	neg.f64 	%fd1490, %fd1487;
	fma.rn.f64 	%fd1491, %fd1490, %fd1489, %fd1488;
	fma.rn.f64 	%fd1492, %fd1491, %fd1491, %fd1491;
	fma.rn.f64 	%fd1493, %fd1492, %fd1489, %fd1489;
	add.f64 	%fd1494, %fd4708, 0dBFF0000000000000;
	mul.f64 	%fd1495, %fd1494, %fd1493;
	fma.rn.f64 	%fd1496, %fd1494, %fd1493, %fd1495;
	mul.f64 	%fd1497, %fd1496, %fd1496;
	mov.f64 	%fd1498, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1499, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1500, %fd1499, %fd1497, %fd1498;
	mov.f64 	%fd1501, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1502, %fd1500, %fd1497, %fd1501;
	mov.f64 	%fd1503, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1504, %fd1502, %fd1497, %fd1503;
	mov.f64 	%fd1505, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1506, %fd1504, %fd1497, %fd1505;
	mov.f64 	%fd1507, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1508, %fd1506, %fd1497, %fd1507;
	mov.f64 	%fd1509, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1510, %fd1508, %fd1497, %fd1509;
	mov.f64 	%fd1511, 0d3FB5555555555554;
	fma.rn.f64 	%fd1512, %fd1510, %fd1497, %fd1511;
	sub.f64 	%fd1513, %fd1494, %fd1496;
	add.f64 	%fd1514, %fd1513, %fd1513;
	neg.f64 	%fd1515, %fd1496;
	fma.rn.f64 	%fd1516, %fd1515, %fd1494, %fd1514;
	mul.f64 	%fd1517, %fd1493, %fd1516;
	mul.f64 	%fd1518, %fd1497, %fd1512;
	fma.rn.f64 	%fd1519, %fd1518, %fd1496, %fd1517;
	xor.b32  	%r826, %r998, -2147483648;
	mov.u32 	%r827, -2147483648;
	mov.u32 	%r828, 1127219200;
	mov.b64 	%fd1520, {%r826, %r828};
	mov.b64 	%fd1521, {%r827, %r828};
	sub.f64 	%fd1522, %fd1520, %fd1521;
	mov.f64 	%fd1523, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1524, %fd1522, %fd1523, %fd1496;
	neg.f64 	%fd1525, %fd1522;
	fma.rn.f64 	%fd1526, %fd1525, %fd1523, %fd1524;
	sub.f64 	%fd1527, %fd1526, %fd1496;
	sub.f64 	%fd1528, %fd1519, %fd1527;
	mov.f64 	%fd1529, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1530, %fd1522, %fd1529, %fd1528;
	add.f64 	%fd4709, %fd1524, %fd1530;
	bra.uni 	$L__BB9_54;

$L__BB9_50:
	mov.f64 	%fd1485, 0d7FF0000000000000;
	fma.rn.f64 	%fd1486, %fd4707, %fd1485, %fd1485;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r819}, %fd4707;
	}
	mov.b32 	%f2, %r819;
	setp.eq.f32 	%p57, %f2, 0f00000000;
	selp.f64 	%fd4709, 0dFFF0000000000000, %fd1486, %p57;

$L__BB9_54:
	sub.f64 	%fd1531, %fd4737, %fd4744;
	div.rn.f64 	%fd1532, %fd1531, %fd4744;
	mul.f64 	%fd1533, %fd3, %fd1532;
	mul.f64 	%fd1534, %fd1532, %fd1533;
	mul.f64 	%fd4710, %fd1534, %fd4709;

$L__BB9_55:
	selp.f64 	%fd4755, %fd4710, 0d0000000000000000, %p51;
	mul.f64 	%fd1535, %fd141, %fd1179;
	mul.f64 	%fd4754, %fd1535, %fd1176;

$L__BB9_56:
	mul.f64 	%fd1539, %fd4724, %fd4735;
	mul.f64 	%fd1540, %fd4721, %fd4734;
	mul.f64 	%fd1541, %fd4718, %fd4750;
	sub.f64 	%fd1542, %fd4715, %fd1541;
	sub.f64 	%fd1543, %fd1542, %fd1540;
	sub.f64 	%fd4740, %fd1543, %fd1539;
	mul.f64 	%fd1544, %fd4723, %fd4735;
	mul.f64 	%fd1545, %fd4720, %fd4734;
	mul.f64 	%fd1546, %fd4717, %fd4750;
	sub.f64 	%fd1547, %fd4714, %fd1546;
	sub.f64 	%fd1548, %fd1547, %fd1545;
	sub.f64 	%fd4739, %fd1548, %fd1544;
	mul.f64 	%fd1549, %fd4722, %fd4735;
	mul.f64 	%fd1550, %fd4719, %fd4734;
	mul.f64 	%fd1551, %fd4716, %fd4750;
	sub.f64 	%fd1552, %fd4713, %fd1551;
	sub.f64 	%fd1553, %fd1552, %fd1550;
	sub.f64 	%fd4738, %fd1553, %fd1549;
	mul.f64 	%fd1554, %fd4733, %fd4735;
	mul.f64 	%fd1555, %fd4730, %fd4734;
	mul.f64 	%fd1556, %fd4727, %fd4750;
	sub.f64 	%fd1557, %fd80, %fd1556;
	sub.f64 	%fd1558, %fd1557, %fd1555;
	sub.f64 	%fd4743, %fd1558, %fd1554;
	mul.f64 	%fd1559, %fd4732, %fd4735;
	mul.f64 	%fd1560, %fd4729, %fd4734;
	mul.f64 	%fd1561, %fd4726, %fd4750;
	sub.f64 	%fd1562, %fd81, %fd1561;
	sub.f64 	%fd1563, %fd1562, %fd1560;
	sub.f64 	%fd4742, %fd1563, %fd1559;
	mul.f64 	%fd1564, %fd4731, %fd4735;
	mul.f64 	%fd1565, %fd4728, %fd4734;
	mul.f64 	%fd1566, %fd4725, %fd4750;
	sub.f64 	%fd1567, %fd82, %fd1566;
	sub.f64 	%fd1568, %fd1567, %fd1565;
	sub.f64 	%fd4741, %fd1568, %fd1564;
	selp.u16 	%rs304, 1, 0, %p1;
	setp.eq.s32 	%p60, %r388, 0;
	mov.u16 	%rs305, 1;
	mov.f64 	%fd4746, %fd4817;
	mov.f64 	%fd4747, %fd4817;
	mov.f64 	%fd4748, %fd4817;
	@%p60 bra 	$L__BB9_58;

	mul.lo.s64 	%rd232, %rd96, %rd69;
	add.s64 	%rd233, %rd33, %rd232;
	ld.global.u32 	%r980, [%rd233];
	cvt.s64.s32 	%rd234, %r980;
	mul.lo.s64 	%rd235, %rd234, %rd70;
	add.s64 	%rd236, %rd29, %rd235;
	mul.lo.s64 	%rd237, %rd96, %rd71;
	add.s64 	%rd238, %rd32, %rd237;
	ld.global.u32 	%r979, [%rd238];
	cvt.s64.s32 	%rd239, %r979;
	mul.lo.s64 	%rd240, %rd239, %rd70;
	add.s64 	%rd241, %rd29, %rd240;
	ld.global.f64 	%fd4681, [%rd236];
	add.f64 	%fd1569, %fd4681, %fd4681;
	ld.global.f64 	%fd4680, [%rd241];
	mul.f64 	%fd1570, %fd1569, %fd4680;
	add.f64 	%fd1571, %fd4681, %fd4680;
	setp.neu.f64 	%p61, %fd1571, 0d0000000000000000;
	div.rn.f64 	%fd1572, %fd1570, %fd1571;
	selp.f64 	%fd4682, %fd1572, 0d0000000000000000, %p61;
	mul.lo.s64 	%rd242, %rd96, %rd72;
	add.s64 	%rd243, %rd38, %rd242;
	ld.global.f64 	%fd4679, [%rd243];
	mul.f64 	%fd1573, %fd4679, %fd4682;
	mul.lo.s64 	%rd244, %rd96, %rd73;
	add.s64 	%rd245, %rd36, %rd244;
	ld.global.f64 	%fd4748, [%rd245];
	ld.global.f64 	%fd4747, [%rd245+8];
	ld.global.f64 	%fd4746, [%rd245+16];
	mul.f64 	%fd4696, %fd1573, %fd1180;
	mov.u16 	%rs305, 0;

$L__BB9_58:
	setp.lt.u32 	%p62, %r31, 2;
	mov.f64 	%fd4782, %fd4817;
	mov.f64 	%fd4783, %fd4817;
	mov.f64 	%fd4784, %fd4817;
	mov.f64 	%fd372, %fd4817;
	mov.f64 	%fd373, %fd4817;
	mov.f64 	%fd374, %fd4817;
	mov.f64 	%fd4788, %fd4817;
	mov.f64 	%fd4789, %fd4817;
	mov.f64 	%fd4790, %fd4817;
	mov.f64 	%fd378, %fd4817;
	mov.f64 	%fd379, %fd4817;
	mov.f64 	%fd380, %fd4817;
	mov.f64 	%fd4796, %fd4817;
	mov.f64 	%fd4797, %fd4817;
	mov.f64 	%fd4798, %fd4817;
	mov.f64 	%fd4799, %fd4817;
	mov.f64 	%fd4800, %fd4817;
	mov.f64 	%fd4801, %fd4817;
	mov.f64 	%fd4803, %fd4817;
	mov.f64 	%fd4804, %fd4817;
	mov.f64 	%fd4805, %fd4817;
	mov.f64 	%fd4806, %fd4817;
	mov.f64 	%fd4807, %fd4817;
	mov.f64 	%fd4808, %fd4817;
	mov.f64 	%fd4809, %fd4817;
	mov.f64 	%fd4810, %fd4817;
	mov.f64 	%fd4811, %fd4817;
	mov.f64 	%fd4812, %fd4817;
	mov.f64 	%fd4813, %fd4817;
	mov.f64 	%fd4814, %fd4817;
	mov.f64 	%fd4818, %fd4817;
	mov.f64 	%fd4819, %fd4817;
	@%p62 bra 	$L__BB9_123;

	mul.lo.s64 	%rd246, %rd96, %rd54;
	add.s64 	%rd247, %rd35, %rd246;
	ld.global.u32 	%r1026, [%rd247];
	cvt.s64.s32 	%rd100, %r1026;
	mul.lo.s64 	%rd248, %rd100, %rd62;
	add.s64 	%rd249, %rd40, %rd248;
	ld.global.u32 	%r1027, [%rd97];
	cvt.s64.s32 	%rd101, %r1027;
	mul.lo.s64 	%rd250, %rd101, %rd62;
	add.s64 	%rd251, %rd40, %rd250;
	mul.lo.s64 	%rd252, %rd100, %rd63;
	add.s64 	%rd253, %rd44, %rd252;
	mul.lo.s64 	%rd254, %rd101, %rd63;
	add.s64 	%rd255, %rd44, %rd254;
	ld.global.f64 	%fd1608, [%rd255];
	ld.global.f64 	%fd1609, [%rd253];
	add.f64 	%fd4781, %fd1609, %fd1608;
	ld.global.u32 	%r1036, [%rd249];
	cvt.s64.s32 	%rd102, %r1036;
	mul.lo.s64 	%rd256, %rd102, %rd57;
	add.s64 	%rd257, %rd31, %rd256;
	ld.global.u32 	%r1037, [%rd249+4];
	cvt.s64.s32 	%rd103, %r1037;
	mul.lo.s64 	%rd258, %rd103, %rd57;
	add.s64 	%rd259, %rd31, %rd258;
	ld.global.u32 	%r1038, [%rd251];
	cvt.s64.s32 	%rd104, %r1038;
	mul.lo.s64 	%rd260, %rd104, %rd57;
	add.s64 	%rd261, %rd31, %rd260;
	ld.global.u32 	%r1039, [%rd251+4];
	cvt.s64.s32 	%rd105, %r1039;
	mul.lo.s64 	%rd262, %rd105, %rd57;
	add.s64 	%rd263, %rd31, %rd262;
	mul.lo.s64 	%rd264, %rd102, %rd58;
	add.s64 	%rd265, %rd30, %rd264;
	mul.lo.s64 	%rd266, %rd103, %rd58;
	add.s64 	%rd267, %rd30, %rd266;
	mul.lo.s64 	%rd268, %rd104, %rd58;
	add.s64 	%rd269, %rd30, %rd268;
	mul.lo.s64 	%rd270, %rd105, %rd58;
	add.s64 	%rd271, %rd30, %rd270;
	mul.lo.s64 	%rd272, %rd96, %rd59;
	add.s64 	%rd273, %rd37, %rd272;
	ld.global.f64 	%fd4794, [%rd273];
	mov.f64 	%fd1610, 0d3FF0000000000000;
	sub.f64 	%fd4850, %fd1610, %fd4794;
	ld.global.f64 	%fd4784, [%rd257];
	mul.f64 	%fd1611, %fd4784, %fd4850;
	ld.global.f64 	%fd374, [%rd259];
	fma.rn.f64 	%fd223, %fd374, %fd4794, %fd1611;
	ld.global.f64 	%fd4795, [%rd273+8];
	sub.f64 	%fd4853, %fd1610, %fd4795;
	ld.global.f64 	%fd4790, [%rd261];
	ld.global.f64 	%fd380, [%rd263];
	ld.global.f64 	%fd4783, [%rd257+8];
	mul.f64 	%fd1612, %fd4783, %fd4850;
	ld.global.f64 	%fd373, [%rd259+8];
	fma.rn.f64 	%fd230, %fd373, %fd4794, %fd1612;
	ld.global.f64 	%fd4789, [%rd261+8];
	ld.global.f64 	%fd379, [%rd263+8];
	ld.global.f64 	%fd4782, [%rd257+16];
	mul.f64 	%fd1613, %fd4782, %fd4850;
	ld.global.f64 	%fd372, [%rd259+16];
	fma.rn.f64 	%fd235, %fd372, %fd4794, %fd1613;
	ld.global.f64 	%fd4788, [%rd261+16];
	ld.global.f64 	%fd378, [%rd263+16];
	ld.global.f64 	%fd4839, [%rd265];
	mul.f64 	%fd1614, %fd4839, %fd4850;
	ld.global.f64 	%fd4840, [%rd267];
	fma.rn.f64 	%fd240, %fd4840, %fd4794, %fd1614;
	ld.global.f64 	%fd4842, [%rd269];
	ld.global.f64 	%fd4843, [%rd271];
	ld.global.f64 	%fd4845, [%rd265+8];
	mul.f64 	%fd1615, %fd4845, %fd4850;
	ld.global.f64 	%fd4846, [%rd267+8];
	fma.rn.f64 	%fd245, %fd4846, %fd4794, %fd1615;
	ld.global.f64 	%fd4848, [%rd269+8];
	ld.global.f64 	%fd4849, [%rd271+8];
	ld.global.f64 	%fd4851, [%rd265+16];
	mul.f64 	%fd1616, %fd4851, %fd4850;
	ld.global.f64 	%fd4852, [%rd267+16];
	fma.rn.f64 	%fd250, %fd4852, %fd4794, %fd1616;
	ld.global.f64 	%fd4854, [%rd269+16];
	ld.global.f64 	%fd4855, [%rd271+16];
	sub.f64 	%fd253, %fd374, %fd4784;
	sub.f64 	%fd254, %fd373, %fd4783;
	sub.f64 	%fd255, %fd372, %fd4782;
	sub.f64 	%fd256, %fd380, %fd4790;
	sub.f64 	%fd257, %fd379, %fd4789;
	sub.f64 	%fd258, %fd378, %fd4788;
	sub.f64 	%fd259, %fd4784, %fd4790;
	sub.f64 	%fd260, %fd4783, %fd4789;
	sub.f64 	%fd261, %fd4782, %fd4788;
	mul.f64 	%fd1617, %fd254, %fd254;
	fma.rn.f64 	%fd1618, %fd253, %fd253, %fd1617;
	fma.rn.f64 	%fd262, %fd255, %fd255, %fd1618;
	mul.f64 	%fd1619, %fd254, %fd257;
	fma.rn.f64 	%fd1620, %fd253, %fd256, %fd1619;
	fma.rn.f64 	%fd263, %fd255, %fd258, %fd1620;
	mul.f64 	%fd1621, %fd257, %fd257;
	fma.rn.f64 	%fd1622, %fd256, %fd256, %fd1621;
	fma.rn.f64 	%fd264, %fd258, %fd258, %fd1622;
	mul.f64 	%fd1623, %fd254, %fd260;
	fma.rn.f64 	%fd1624, %fd253, %fd259, %fd1623;
	fma.rn.f64 	%fd265, %fd255, %fd261, %fd1624;
	mul.f64 	%fd1625, %fd260, %fd257;
	fma.rn.f64 	%fd1626, %fd259, %fd256, %fd1625;
	fma.rn.f64 	%fd266, %fd261, %fd258, %fd1626;
	mul.f64 	%fd1627, %fd262, %fd264;
	mul.f64 	%fd1628, %fd263, %fd263;
	sub.f64 	%fd267, %fd1627, %fd1628;
	mul.f64 	%fd1629, %fd263, %fd266;
	mul.f64 	%fd1630, %fd265, %fd264;
	sub.f64 	%fd268, %fd1629, %fd1630;
	setp.le.f64 	%p63, %fd268, 0d0000000000000000;
	@%p63 bra 	$L__BB9_63;

	setp.ge.f64 	%p2, %fd268, %fd267;
	add.f64 	%fd269, %fd266, %fd263;
	@%p2 bra 	$L__BB9_62;

	selp.f64 	%fd1632, %fd264, %fd267, %p2;
	mul.f64 	%fd1633, %fd262, %fd266;
	mul.f64 	%fd1634, %fd265, %fd263;
	sub.f64 	%fd1635, %fd1633, %fd1634;
	mul.f64 	%fd1636, %fd255, %fd257;
	mul.f64 	%fd1637, %fd254, %fd258;
	sub.f64 	%fd1638, %fd1637, %fd1636;
	mul.f64 	%fd1639, %fd253, %fd258;
	mul.f64 	%fd1640, %fd255, %fd256;
	sub.f64 	%fd1641, %fd1640, %fd1639;
	mul.f64 	%fd1642, %fd254, %fd256;
	mul.f64 	%fd1643, %fd253, %fd257;
	sub.f64 	%fd1644, %fd1643, %fd1642;
	setp.gt.f64 	%p64, %fd1635, 0d0000000000000000;
	setp.lt.f64 	%p65, %fd1635, %fd1632;
	mul.f64 	%fd1645, %fd260, %fd1641;
	fma.rn.f64 	%fd1646, %fd259, %fd1638, %fd1645;
	fma.rn.f64 	%fd1647, %fd261, %fd1644, %fd1646;
	setp.eq.f64 	%p66, %fd1647, 0d0000000000000000;
	mul.f64 	%fd1648, %fd1641, %fd1641;
	fma.rn.f64 	%fd1649, %fd1638, %fd1638, %fd1648;
	fma.rn.f64 	%fd1650, %fd1644, %fd1644, %fd1649;
	mul.f64 	%fd1651, %fd262, 0d3BC79CA100000000;
	mul.f64 	%fd1652, %fd1651, %fd264;
	setp.lt.f64 	%p67, %fd1650, %fd1652;
	or.pred  	%p68, %p66, %p67;
	and.pred  	%p69, %p64, %p65;
	and.pred  	%p70, %p68, %p69;
	mul.f64 	%fd1653, %fd267, 0d3FE0000000000000;
	setp.lt.f64 	%p71, %fd268, %fd1653;
	selp.b32 	%r831, 2, 5, %p71;
	selp.f64 	%fd1654, %fd266, %fd269, %p71;
	selp.f64 	%fd4760, %fd264, %fd1632, %p70;
	selp.b32 	%r1010, %r831, 8, %p70;
	selp.f64 	%fd4761, %fd1654, %fd1635, %p70;

$L__BB9_62:
	selp.f64 	%fd4762, %fd264, %fd4760, %p2;
	selp.b32 	%r1011, 5, %r1010, %p2;
	selp.f64 	%fd4763, %fd269, %fd4761, %p2;

$L__BB9_63:
	mul.f64 	%fd1655, %fd380, %fd4795;
	mul.f64 	%fd1656, %fd4790, %fd4853;
	sub.f64 	%fd1657, %fd223, %fd1656;
	sub.f64 	%fd4798, %fd1657, %fd1655;
	mul.f64 	%fd1658, %fd379, %fd4795;
	mul.f64 	%fd1659, %fd4789, %fd4853;
	sub.f64 	%fd1660, %fd230, %fd1659;
	sub.f64 	%fd4797, %fd1660, %fd1658;
	mul.f64 	%fd1661, %fd378, %fd4795;
	mul.f64 	%fd1662, %fd4788, %fd4853;
	sub.f64 	%fd1663, %fd235, %fd1662;
	sub.f64 	%fd4796, %fd1663, %fd1661;
	mul.f64 	%fd1664, %fd4843, %fd4795;
	mul.f64 	%fd1665, %fd4842, %fd4853;
	sub.f64 	%fd1666, %fd240, %fd1665;
	sub.f64 	%fd4801, %fd1666, %fd1664;
	mul.f64 	%fd1667, %fd4849, %fd4795;
	mul.f64 	%fd1668, %fd4848, %fd4853;
	sub.f64 	%fd1669, %fd245, %fd1668;
	sub.f64 	%fd4800, %fd1669, %fd1667;
	mul.f64 	%fd1670, %fd4855, %fd4795;
	mul.f64 	%fd1671, %fd4854, %fd4853;
	sub.f64 	%fd1672, %fd250, %fd1671;
	sub.f64 	%fd4799, %fd1672, %fd1670;
	selp.f64 	%fd284, %fd264, %fd4762, %p63;
	selp.b32 	%r1012, 2, %r1011, %p63;
	selp.f64 	%fd285, %fd266, %fd4763, %p63;
	setp.gtu.f64 	%p74, %fd285, 0d0000000000000000;
	@%p74 bra 	$L__BB9_67;
	bra.uni 	$L__BB9_64;

$L__BB9_67:
	setp.ltu.f64 	%p77, %fd285, %fd284;
	@%p77 bra 	$L__BB9_71;

	mov.f64 	%fd1674, 0d0000000000000000;
	sub.f64 	%fd1675, %fd1674, %fd265;
	add.f64 	%fd287, %fd1675, %fd263;
	setp.le.f64 	%p78, %fd287, 0d0000000000000000;
	mov.u32 	%r1012, 1;
	@%p78 bra 	$L__BB9_71;

	setp.ge.f64 	%p79, %fd287, %fd262;
	mov.u32 	%r1012, 4;
	@%p79 bra 	$L__BB9_71;

	mov.u32 	%r1012, 7;
	bra.uni 	$L__BB9_71;

$L__BB9_64:
	mov.f64 	%fd1673, 0d0000000000000000;
	sub.f64 	%fd286, %fd1673, %fd265;
	setp.le.f64 	%p75, %fd286, 0d0000000000000000;
	mov.u32 	%r1012, 0;
	@%p75 bra 	$L__BB9_71;

	setp.ge.f64 	%p76, %fd286, %fd262;
	mov.u32 	%r1012, 3;
	@%p76 bra 	$L__BB9_71;

	mov.u32 	%r1012, 6;

$L__BB9_71:
	setp.eq.s32 	%p80, %r1012, 0;
	@%p80 bra 	$L__BB9_87;

	setp.eq.s32 	%p81, %r1012, 1;
	@%p81 bra 	$L__BB9_86;
	bra.uni 	$L__BB9_73;

$L__BB9_86:
	sub.f64 	%fd1774, %fd4784, %fd380;
	sub.f64 	%fd1775, %fd4783, %fd379;
	mul.f64 	%fd1776, %fd1775, %fd1775;
	fma.rn.f64 	%fd1777, %fd1774, %fd1774, %fd1776;
	sub.f64 	%fd1778, %fd4782, %fd378;
	fma.rn.f64 	%fd4764, %fd1778, %fd1778, %fd1777;
	bra.uni 	$L__BB9_88;

$L__BB9_87:
	mul.f64 	%fd1779, %fd260, %fd260;
	fma.rn.f64 	%fd1780, %fd259, %fd259, %fd1779;
	fma.rn.f64 	%fd4764, %fd261, %fd261, %fd1780;
	bra.uni 	$L__BB9_88;

$L__BB9_73:
	setp.eq.s32 	%p82, %r1012, 2;
	@%p82 bra 	$L__BB9_85;
	bra.uni 	$L__BB9_74;

$L__BB9_85:
	sub.f64 	%fd1756, %fd4790, %fd4784;
	sub.f64 	%fd1757, %fd378, %fd4782;
	sub.f64 	%fd1758, %fd4789, %fd4783;
	mul.f64 	%fd1759, %fd1758, %fd1757;
	sub.f64 	%fd1760, %fd379, %fd4783;
	sub.f64 	%fd1761, %fd4788, %fd4782;
	mul.f64 	%fd1762, %fd1761, %fd1760;
	sub.f64 	%fd1763, %fd1759, %fd1762;
	sub.f64 	%fd1764, %fd380, %fd4784;
	mul.f64 	%fd1765, %fd1761, %fd1764;
	mul.f64 	%fd1766, %fd1756, %fd1757;
	sub.f64 	%fd1767, %fd1765, %fd1766;
	mul.f64 	%fd1768, %fd1756, %fd1760;
	mul.f64 	%fd1769, %fd1758, %fd1764;
	sub.f64 	%fd1770, %fd1768, %fd1769;
	mul.f64 	%fd1771, %fd1767, %fd1767;
	fma.rn.f64 	%fd1772, %fd1763, %fd1763, %fd1771;
	fma.rn.f64 	%fd1773, %fd1770, %fd1770, %fd1772;
	div.rn.f64 	%fd4764, %fd1773, %fd264;
	bra.uni 	$L__BB9_88;

$L__BB9_74:
	setp.eq.s32 	%p83, %r1012, 3;
	@%p83 bra 	$L__BB9_84;
	bra.uni 	$L__BB9_75;

$L__BB9_84:
	sub.f64 	%fd1751, %fd374, %fd4790;
	sub.f64 	%fd1752, %fd373, %fd4789;
	mul.f64 	%fd1753, %fd1752, %fd1752;
	fma.rn.f64 	%fd1754, %fd1751, %fd1751, %fd1753;
	sub.f64 	%fd1755, %fd372, %fd4788;
	fma.rn.f64 	%fd4764, %fd1755, %fd1755, %fd1754;
	bra.uni 	$L__BB9_88;

$L__BB9_75:
	setp.eq.s32 	%p84, %r1012, 4;
	@%p84 bra 	$L__BB9_83;
	bra.uni 	$L__BB9_76;

$L__BB9_83:
	sub.f64 	%fd1746, %fd374, %fd380;
	sub.f64 	%fd1747, %fd373, %fd379;
	mul.f64 	%fd1748, %fd1747, %fd1747;
	fma.rn.f64 	%fd1749, %fd1746, %fd1746, %fd1748;
	sub.f64 	%fd1750, %fd372, %fd378;
	fma.rn.f64 	%fd4764, %fd1750, %fd1750, %fd1749;
	bra.uni 	$L__BB9_88;

$L__BB9_76:
	setp.eq.s32 	%p85, %r1012, 5;
	@%p85 bra 	$L__BB9_82;
	bra.uni 	$L__BB9_77;

$L__BB9_82:
	sub.f64 	%fd1728, %fd4790, %fd374;
	sub.f64 	%fd1729, %fd378, %fd372;
	sub.f64 	%fd1730, %fd4789, %fd373;
	mul.f64 	%fd1731, %fd1730, %fd1729;
	sub.f64 	%fd1732, %fd379, %fd373;
	sub.f64 	%fd1733, %fd4788, %fd372;
	mul.f64 	%fd1734, %fd1733, %fd1732;
	sub.f64 	%fd1735, %fd1731, %fd1734;
	sub.f64 	%fd1736, %fd380, %fd374;
	mul.f64 	%fd1737, %fd1733, %fd1736;
	mul.f64 	%fd1738, %fd1728, %fd1729;
	sub.f64 	%fd1739, %fd1737, %fd1738;
	mul.f64 	%fd1740, %fd1728, %fd1732;
	mul.f64 	%fd1741, %fd1730, %fd1736;
	sub.f64 	%fd1742, %fd1740, %fd1741;
	mul.f64 	%fd1743, %fd1739, %fd1739;
	fma.rn.f64 	%fd1744, %fd1735, %fd1735, %fd1743;
	fma.rn.f64 	%fd1745, %fd1742, %fd1742, %fd1744;
	div.rn.f64 	%fd4764, %fd1745, %fd264;
	bra.uni 	$L__BB9_88;

$L__BB9_77:
	setp.eq.s32 	%p86, %r1012, 6;
	@%p86 bra 	$L__BB9_81;
	bra.uni 	$L__BB9_78;

$L__BB9_81:
	sub.f64 	%fd1713, %fd374, %fd4790;
	sub.f64 	%fd1714, %fd372, %fd4788;
	mul.f64 	%fd1715, %fd260, %fd1714;
	sub.f64 	%fd1716, %fd373, %fd4789;
	mul.f64 	%fd1717, %fd1716, %fd261;
	sub.f64 	%fd1718, %fd1715, %fd1717;
	mul.f64 	%fd1719, %fd1713, %fd261;
	mul.f64 	%fd1720, %fd259, %fd1714;
	sub.f64 	%fd1721, %fd1719, %fd1720;
	mul.f64 	%fd1722, %fd259, %fd1716;
	mul.f64 	%fd1723, %fd1713, %fd260;
	sub.f64 	%fd1724, %fd1722, %fd1723;
	mul.f64 	%fd1725, %fd1721, %fd1721;
	fma.rn.f64 	%fd1726, %fd1718, %fd1718, %fd1725;
	fma.rn.f64 	%fd1727, %fd1724, %fd1724, %fd1726;
	div.rn.f64 	%fd4764, %fd1727, %fd262;
	bra.uni 	$L__BB9_88;

$L__BB9_78:
	setp.eq.s32 	%p87, %r1012, 7;
	@%p87 bra 	$L__BB9_80;
	bra.uni 	$L__BB9_79;

$L__BB9_80:
	sub.f64 	%fd1695, %fd4784, %fd380;
	sub.f64 	%fd1696, %fd372, %fd378;
	sub.f64 	%fd1697, %fd4783, %fd379;
	mul.f64 	%fd1698, %fd1697, %fd1696;
	sub.f64 	%fd1699, %fd373, %fd379;
	sub.f64 	%fd1700, %fd4782, %fd378;
	mul.f64 	%fd1701, %fd1699, %fd1700;
	sub.f64 	%fd1702, %fd1698, %fd1701;
	sub.f64 	%fd1703, %fd374, %fd380;
	mul.f64 	%fd1704, %fd1703, %fd1700;
	mul.f64 	%fd1705, %fd1695, %fd1696;
	sub.f64 	%fd1706, %fd1704, %fd1705;
	mul.f64 	%fd1707, %fd1695, %fd1699;
	mul.f64 	%fd1708, %fd1703, %fd1697;
	sub.f64 	%fd1709, %fd1707, %fd1708;
	mul.f64 	%fd1710, %fd1706, %fd1706;
	fma.rn.f64 	%fd1711, %fd1702, %fd1702, %fd1710;
	fma.rn.f64 	%fd1712, %fd1709, %fd1709, %fd1711;
	div.rn.f64 	%fd4764, %fd1712, %fd262;
	bra.uni 	$L__BB9_88;

$L__BB9_79:
	sub.f64 	%fd1676, %fd4790, %fd4784;
	mul.f64 	%fd1677, %fd255, %fd257;
	mul.f64 	%fd1678, %fd254, %fd258;
	sub.f64 	%fd1679, %fd1678, %fd1677;
	mul.f64 	%fd1680, %fd253, %fd258;
	mul.f64 	%fd1681, %fd255, %fd256;
	sub.f64 	%fd1682, %fd1681, %fd1680;
	mul.f64 	%fd1683, %fd254, %fd256;
	mul.f64 	%fd1684, %fd253, %fd257;
	sub.f64 	%fd1685, %fd1684, %fd1683;
	sub.f64 	%fd1686, %fd4789, %fd4783;
	mul.f64 	%fd1687, %fd1686, %fd1682;
	fma.rn.f64 	%fd1688, %fd1676, %fd1679, %fd1687;
	sub.f64 	%fd1689, %fd4788, %fd4782;
	fma.rn.f64 	%fd1690, %fd1689, %fd1685, %fd1688;
	mul.f64 	%fd1691, %fd1690, %fd1690;
	mul.f64 	%fd1692, %fd1682, %fd1682;
	fma.rn.f64 	%fd1693, %fd1679, %fd1679, %fd1692;
	fma.rn.f64 	%fd1694, %fd1685, %fd1685, %fd1693;
	div.rn.f64 	%fd4764, %fd1691, %fd1694;

$L__BB9_88:
	mul.f64 	%fd1782, %fd4781, %fd4781;
	sub.f64 	%fd4802, %fd4764, %fd1782;
	mul.lo.s64 	%rd274, %rd102, %rd66;
	add.s64 	%rd275, %rd46, %rd274;
	mul.lo.s64 	%rd276, %rd103, %rd66;
	add.s64 	%rd277, %rd46, %rd276;
	mul.lo.s64 	%rd278, %rd104, %rd66;
	add.s64 	%rd279, %rd46, %rd278;
	mul.lo.s64 	%rd280, %rd105, %rd66;
	add.s64 	%rd281, %rd46, %rd280;
	ld.global.f64 	%fd4808, [%rd277];
	ld.global.f64 	%fd4805, [%rd275];
	sub.f64 	%fd1783, %fd4808, %fd4805;
	ld.global.f64 	%fd4807, [%rd277+8];
	ld.global.f64 	%fd4804, [%rd275+8];
	sub.f64 	%fd1784, %fd4807, %fd4804;
	ld.global.f64 	%fd4806, [%rd277+16];
	ld.global.f64 	%fd4803, [%rd275+16];
	sub.f64 	%fd1785, %fd4806, %fd4803;
	ld.global.f64 	%fd4814, [%rd281];
	ld.global.f64 	%fd4811, [%rd279];
	sub.f64 	%fd1786, %fd4814, %fd4811;
	ld.global.f64 	%fd4813, [%rd281+8];
	ld.global.f64 	%fd4810, [%rd279+8];
	sub.f64 	%fd1787, %fd4813, %fd4810;
	ld.global.f64 	%fd4812, [%rd281+16];
	ld.global.f64 	%fd4809, [%rd279+16];
	sub.f64 	%fd1788, %fd4812, %fd4809;
	mul.f64 	%fd1789, %fd1784, %fd1784;
	fma.rn.f64 	%fd1790, %fd1783, %fd1783, %fd1789;
	fma.rn.f64 	%fd1791, %fd1785, %fd1785, %fd1790;
	mul.f64 	%fd1792, %fd1791, 0d3F50624DE0000000;
	mul.f64 	%fd1793, %fd1787, %fd1787;
	fma.rn.f64 	%fd1794, %fd1786, %fd1786, %fd1793;
	fma.rn.f64 	%fd1795, %fd1788, %fd1788, %fd1794;
	mul.f64 	%fd4856, %fd1792, %fd1795;
	mul.f64 	%fd1796, %fd255, %fd257;
	mul.f64 	%fd1797, %fd254, %fd258;
	sub.f64 	%fd1798, %fd1797, %fd1796;
	mul.f64 	%fd1799, %fd253, %fd258;
	mul.f64 	%fd1800, %fd255, %fd256;
	sub.f64 	%fd1801, %fd1800, %fd1799;
	mul.f64 	%fd1802, %fd254, %fd256;
	mul.f64 	%fd1803, %fd253, %fd257;
	sub.f64 	%fd1804, %fd1803, %fd1802;
	mul.f64 	%fd1805, %fd1801, %fd1801;
	fma.rn.f64 	%fd1806, %fd1798, %fd1798, %fd1805;
	fma.rn.f64 	%fd312, %fd1804, %fd1804, %fd1806;
	setp.geu.f64 	%p88, %fd312, %fd4856;
	mov.f64 	%fd4857, 0d3FF0000000000000;
	@%p88 bra 	$L__BB9_90;

	div.rn.f64 	%fd1807, %fd312, %fd4856;
	mov.f64 	%fd1808, 0d0000000000000000;
	sub.f64 	%fd1809, %fd1808, %fd1807;
	add.f64 	%fd1810, %fd1809, 0d4000000000000000;
	mul.f64 	%fd4857, %fd1807, %fd1810;

$L__BB9_90:
	fma.rn.f64 	%fd4815, %fd4858, %fd4781, %fd1;
	mul.lo.s64 	%rd282, %rd100, %rd68;
	add.s64 	%rd106, %rd41, %rd282;
	mul.lo.s64 	%rd283, %rd101, %rd68;
	add.s64 	%rd107, %rd41, %rd283;
	ld.global.f64 	%fd316, [%rd107];
	ld.global.f64 	%fd317, [%rd106];
	add.f64 	%fd1812, %fd317, %fd316;
	mul.f64 	%fd1813, %fd1812, %fd1179;
	mul.f64 	%fd4859, %fd1813, %fd1176;
	mul.f64 	%fd4860, %fd4857, %fd4859;
	setp.geu.f64 	%p89, %fd4802, %fd4815;
	@%p89 bra 	$L__BB9_99;

	div.rn.f64 	%fd4766, %fd4802, %fd4815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1013}, %fd4766;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1014, %temp}, %fd4766;
	}
	setp.gt.s32 	%p90, %r1013, 1048575;
	mov.u32 	%r1015, -1023;
	@%p90 bra 	$L__BB9_93;

	mul.f64 	%fd4766, %fd4766, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1013}, %fd4766;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1014, %temp}, %fd4766;
	}
	mov.u32 	%r1015, -1077;

$L__BB9_93:
	add.s32 	%r840, %r1013, -1;
	setp.lt.u32 	%p91, %r840, 2146435071;
	@%p91 bra 	$L__BB9_95;
	bra.uni 	$L__BB9_94;

$L__BB9_95:
	shr.u32 	%r842, %r1013, 20;
	add.s32 	%r1016, %r1015, %r842;
	and.b32  	%r843, %r1013, -2146435073;
	or.b32  	%r844, %r843, 1072693248;
	mov.b64 	%fd4767, {%r1014, %r844};
	setp.lt.s32 	%p93, %r844, 1073127583;
	@%p93 bra 	$L__BB9_97;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r845, %temp}, %fd4767;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r846}, %fd4767;
	}
	add.s32 	%r847, %r846, -1048576;
	mov.b64 	%fd4767, {%r845, %r847};
	add.s32 	%r1016, %r1016, 1;

$L__BB9_97:
	add.f64 	%fd1816, %fd4767, 0d3FF0000000000000;
	mov.f64 	%fd1817, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1818, %fd1816;
	neg.f64 	%fd1819, %fd1816;
	fma.rn.f64 	%fd1820, %fd1819, %fd1818, %fd1817;
	fma.rn.f64 	%fd1821, %fd1820, %fd1820, %fd1820;
	fma.rn.f64 	%fd1822, %fd1821, %fd1818, %fd1818;
	add.f64 	%fd1823, %fd4767, 0dBFF0000000000000;
	mul.f64 	%fd1824, %fd1823, %fd1822;
	fma.rn.f64 	%fd1825, %fd1823, %fd1822, %fd1824;
	mul.f64 	%fd1826, %fd1825, %fd1825;
	mov.f64 	%fd1827, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1828, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1829, %fd1828, %fd1826, %fd1827;
	mov.f64 	%fd1830, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1831, %fd1829, %fd1826, %fd1830;
	mov.f64 	%fd1832, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1833, %fd1831, %fd1826, %fd1832;
	mov.f64 	%fd1834, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1835, %fd1833, %fd1826, %fd1834;
	mov.f64 	%fd1836, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1837, %fd1835, %fd1826, %fd1836;
	mov.f64 	%fd1838, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1839, %fd1837, %fd1826, %fd1838;
	mov.f64 	%fd1840, 0d3FB5555555555554;
	fma.rn.f64 	%fd1841, %fd1839, %fd1826, %fd1840;
	sub.f64 	%fd1842, %fd1823, %fd1825;
	add.f64 	%fd1843, %fd1842, %fd1842;
	neg.f64 	%fd1844, %fd1825;
	fma.rn.f64 	%fd1845, %fd1844, %fd1823, %fd1843;
	mul.f64 	%fd1846, %fd1822, %fd1845;
	mul.f64 	%fd1847, %fd1826, %fd1841;
	fma.rn.f64 	%fd1848, %fd1847, %fd1825, %fd1846;
	xor.b32  	%r848, %r1016, -2147483648;
	mov.u32 	%r849, -2147483648;
	mov.u32 	%r850, 1127219200;
	mov.b64 	%fd1849, {%r848, %r850};
	mov.b64 	%fd1850, {%r849, %r850};
	sub.f64 	%fd1851, %fd1849, %fd1850;
	mov.f64 	%fd1852, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1853, %fd1851, %fd1852, %fd1825;
	neg.f64 	%fd1854, %fd1851;
	fma.rn.f64 	%fd1855, %fd1854, %fd1852, %fd1853;
	sub.f64 	%fd1856, %fd1855, %fd1825;
	sub.f64 	%fd1857, %fd1848, %fd1856;
	mov.f64 	%fd1858, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1859, %fd1851, %fd1858, %fd1857;
	add.f64 	%fd4768, %fd1853, %fd1859;
	bra.uni 	$L__BB9_98;

$L__BB9_94:
	mov.f64 	%fd1814, 0d7FF0000000000000;
	fma.rn.f64 	%fd1815, %fd4766, %fd1814, %fd1814;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r841}, %fd4766;
	}
	mov.b32 	%f3, %r841;
	setp.eq.f32 	%p92, %f3, 0f00000000;
	selp.f64 	%fd4768, 0dFFF0000000000000, %fd1815, %p92;

$L__BB9_98:
	sub.f64 	%fd1860, %fd4802, %fd4815;
	div.rn.f64 	%fd1861, %fd1860, %fd4815;
	mul.f64 	%fd1862, %fd3, %fd1861;
	mul.f64 	%fd1863, %fd1861, %fd1862;
	mul.f64 	%fd4769, %fd1863, %fd4768;

$L__BB9_99:
	setp.lt.f64 	%p94, %fd4802, %fd4815;
	selp.f64 	%fd4861, %fd4769, 0d0000000000000000, %p94;
	mul.f64 	%fd1864, %fd4860, %fd4861;
	mul.f64 	%fd1865, %fd1864, %fd1180;
	setp.nan.f64 	%p3, %fd1865, %fd1865;
	setp.num.f64 	%p95, %fd1865, %fd1865;
	@%p95 bra 	$L__BB9_110;

	@%p89 bra 	$L__BB9_109;

	div.rn.f64 	%fd4770, %fd4802, %fd4815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1017}, %fd4770;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1018, %temp}, %fd4770;
	}
	setp.gt.s32 	%p97, %r1017, 1048575;
	mov.u32 	%r1019, -1023;
	@%p97 bra 	$L__BB9_103;

	mul.f64 	%fd4770, %fd4770, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1017}, %fd4770;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1018, %temp}, %fd4770;
	}
	mov.u32 	%r1019, -1077;

$L__BB9_103:
	add.s32 	%r853, %r1017, -1;
	setp.lt.u32 	%p98, %r853, 2146435071;
	@%p98 bra 	$L__BB9_105;
	bra.uni 	$L__BB9_104;

$L__BB9_105:
	shr.u32 	%r855, %r1017, 20;
	add.s32 	%r1020, %r1019, %r855;
	and.b32  	%r856, %r1017, -2146435073;
	or.b32  	%r857, %r856, 1072693248;
	mov.b64 	%fd4771, {%r1018, %r857};
	setp.lt.s32 	%p100, %r857, 1073127583;
	@%p100 bra 	$L__BB9_107;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r858, %temp}, %fd4771;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r859}, %fd4771;
	}
	add.s32 	%r860, %r859, -1048576;
	mov.b64 	%fd4771, {%r858, %r860};
	add.s32 	%r1020, %r1020, 1;

$L__BB9_107:
	add.f64 	%fd1869, %fd4771, 0d3FF0000000000000;
	mov.f64 	%fd1870, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1871, %fd1869;
	neg.f64 	%fd1872, %fd1869;
	fma.rn.f64 	%fd1873, %fd1872, %fd1871, %fd1870;
	fma.rn.f64 	%fd1874, %fd1873, %fd1873, %fd1873;
	fma.rn.f64 	%fd1875, %fd1874, %fd1871, %fd1871;
	add.f64 	%fd1876, %fd4771, 0dBFF0000000000000;
	mul.f64 	%fd1877, %fd1876, %fd1875;
	fma.rn.f64 	%fd1878, %fd1876, %fd1875, %fd1877;
	mul.f64 	%fd1879, %fd1878, %fd1878;
	mov.f64 	%fd1880, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1881, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1882, %fd1881, %fd1879, %fd1880;
	mov.f64 	%fd1883, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1884, %fd1882, %fd1879, %fd1883;
	mov.f64 	%fd1885, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1886, %fd1884, %fd1879, %fd1885;
	mov.f64 	%fd1887, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1888, %fd1886, %fd1879, %fd1887;
	mov.f64 	%fd1889, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1890, %fd1888, %fd1879, %fd1889;
	mov.f64 	%fd1891, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1892, %fd1890, %fd1879, %fd1891;
	mov.f64 	%fd1893, 0d3FB5555555555554;
	fma.rn.f64 	%fd1894, %fd1892, %fd1879, %fd1893;
	sub.f64 	%fd1895, %fd1876, %fd1878;
	add.f64 	%fd1896, %fd1895, %fd1895;
	neg.f64 	%fd1897, %fd1878;
	fma.rn.f64 	%fd1898, %fd1897, %fd1876, %fd1896;
	mul.f64 	%fd1899, %fd1875, %fd1898;
	mul.f64 	%fd1900, %fd1879, %fd1894;
	fma.rn.f64 	%fd1901, %fd1900, %fd1878, %fd1899;
	xor.b32  	%r861, %r1020, -2147483648;
	mov.u32 	%r862, -2147483648;
	mov.u32 	%r863, 1127219200;
	mov.b64 	%fd1902, {%r861, %r863};
	mov.b64 	%fd1903, {%r862, %r863};
	sub.f64 	%fd1904, %fd1902, %fd1903;
	mov.f64 	%fd1905, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1906, %fd1904, %fd1905, %fd1878;
	neg.f64 	%fd1907, %fd1904;
	fma.rn.f64 	%fd1908, %fd1907, %fd1905, %fd1906;
	sub.f64 	%fd1909, %fd1908, %fd1878;
	sub.f64 	%fd1910, %fd1901, %fd1909;
	mov.f64 	%fd1911, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1912, %fd1904, %fd1911, %fd1910;
	add.f64 	%fd4772, %fd1906, %fd1912;
	bra.uni 	$L__BB9_108;

$L__BB9_104:
	mov.f64 	%fd1867, 0d7FF0000000000000;
	fma.rn.f64 	%fd1868, %fd4770, %fd1867, %fd1867;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r854}, %fd4770;
	}
	mov.b32 	%f4, %r854;
	setp.eq.f32 	%p99, %f4, 0f00000000;
	selp.f64 	%fd4772, 0dFFF0000000000000, %fd1868, %p99;

$L__BB9_108:
	sub.f64 	%fd1913, %fd4802, %fd4815;
	div.rn.f64 	%fd1914, %fd1913, %fd4815;
	mul.f64 	%fd1915, %fd3, %fd1914;
	mul.f64 	%fd1916, %fd1914, %fd1915;
	mul.f64 	%fd4773, %fd1916, %fd4772;

$L__BB9_109:
	add.u64 	%rd284, %SP, 16;
	add.u64 	%rd285, %SPL, 16;
	st.local.v2.f64 	[%rd285], {%fd317, %fd316};
	selp.f64 	%fd1917, %fd4773, 0d0000000000000000, %p94;
	st.local.v2.f64 	[%rd285+16], {%fd1917, %fd4802};
	st.local.v2.f64 	[%rd285+32], {%fd4815, %fd4781};
	mov.u64 	%rd286, $str$4;
	cvta.global.u64 	%rd287, %rd286;
	{ // callseq 511, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd287;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd284;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r864, [retval0+0];
	} // callseq 511

$L__BB9_110:
	setp.ne.s32 	%p102, %r388, 0;
	@%p102 bra 	$L__BB9_121;

	ld.global.f64 	%fd1919, [%rd107];
	ld.global.f64 	%fd1920, [%rd106];
	add.f64 	%fd1921, %fd1920, %fd1919;
	mul.f64 	%fd1922, %fd1921, %fd1179;
	mul.f64 	%fd4862, %fd1922, %fd1176;
	@%p89 bra 	$L__BB9_120;

	div.rn.f64 	%fd4774, %fd4802, %fd4815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1021}, %fd4774;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1022, %temp}, %fd4774;
	}
	setp.gt.s32 	%p104, %r1021, 1048575;
	mov.u32 	%r1023, -1023;
	@%p104 bra 	$L__BB9_114;

	mul.f64 	%fd4774, %fd4774, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1021}, %fd4774;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1022, %temp}, %fd4774;
	}
	mov.u32 	%r1023, -1077;

$L__BB9_114:
	add.s32 	%r867, %r1021, -1;
	setp.lt.u32 	%p105, %r867, 2146435071;
	@%p105 bra 	$L__BB9_116;
	bra.uni 	$L__BB9_115;

$L__BB9_116:
	shr.u32 	%r869, %r1021, 20;
	add.s32 	%r1024, %r1023, %r869;
	and.b32  	%r870, %r1021, -2146435073;
	or.b32  	%r871, %r870, 1072693248;
	mov.b64 	%fd4775, {%r1022, %r871};
	setp.lt.s32 	%p107, %r871, 1073127583;
	@%p107 bra 	$L__BB9_118;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r872, %temp}, %fd4775;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r873}, %fd4775;
	}
	add.s32 	%r874, %r873, -1048576;
	mov.b64 	%fd4775, {%r872, %r874};
	add.s32 	%r1024, %r1024, 1;

$L__BB9_118:
	add.f64 	%fd1925, %fd4775, 0d3FF0000000000000;
	mov.f64 	%fd1926, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1927, %fd1925;
	neg.f64 	%fd1928, %fd1925;
	fma.rn.f64 	%fd1929, %fd1928, %fd1927, %fd1926;
	fma.rn.f64 	%fd1930, %fd1929, %fd1929, %fd1929;
	fma.rn.f64 	%fd1931, %fd1930, %fd1927, %fd1927;
	add.f64 	%fd1932, %fd4775, 0dBFF0000000000000;
	mul.f64 	%fd1933, %fd1932, %fd1931;
	fma.rn.f64 	%fd1934, %fd1932, %fd1931, %fd1933;
	mul.f64 	%fd1935, %fd1934, %fd1934;
	mov.f64 	%fd1936, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1937, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1938, %fd1937, %fd1935, %fd1936;
	mov.f64 	%fd1939, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1940, %fd1938, %fd1935, %fd1939;
	mov.f64 	%fd1941, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1942, %fd1940, %fd1935, %fd1941;
	mov.f64 	%fd1943, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1944, %fd1942, %fd1935, %fd1943;
	mov.f64 	%fd1945, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1946, %fd1944, %fd1935, %fd1945;
	mov.f64 	%fd1947, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1948, %fd1946, %fd1935, %fd1947;
	mov.f64 	%fd1949, 0d3FB5555555555554;
	fma.rn.f64 	%fd1950, %fd1948, %fd1935, %fd1949;
	sub.f64 	%fd1951, %fd1932, %fd1934;
	add.f64 	%fd1952, %fd1951, %fd1951;
	neg.f64 	%fd1953, %fd1934;
	fma.rn.f64 	%fd1954, %fd1953, %fd1932, %fd1952;
	mul.f64 	%fd1955, %fd1931, %fd1954;
	mul.f64 	%fd1956, %fd1935, %fd1950;
	fma.rn.f64 	%fd1957, %fd1956, %fd1934, %fd1955;
	xor.b32  	%r875, %r1024, -2147483648;
	mov.u32 	%r876, -2147483648;
	mov.u32 	%r877, 1127219200;
	mov.b64 	%fd1958, {%r875, %r877};
	mov.b64 	%fd1959, {%r876, %r877};
	sub.f64 	%fd1960, %fd1958, %fd1959;
	mov.f64 	%fd1961, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1962, %fd1960, %fd1961, %fd1934;
	neg.f64 	%fd1963, %fd1960;
	fma.rn.f64 	%fd1964, %fd1963, %fd1961, %fd1962;
	sub.f64 	%fd1965, %fd1964, %fd1934;
	sub.f64 	%fd1966, %fd1957, %fd1965;
	mov.f64 	%fd1967, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1968, %fd1960, %fd1967, %fd1966;
	add.f64 	%fd4776, %fd1962, %fd1968;
	bra.uni 	$L__BB9_119;

$L__BB9_115:
	mov.f64 	%fd1923, 0d7FF0000000000000;
	fma.rn.f64 	%fd1924, %fd4774, %fd1923, %fd1923;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r868}, %fd4774;
	}
	mov.b32 	%f5, %r868;
	setp.eq.f32 	%p106, %f5, 0f00000000;
	selp.f64 	%fd4776, 0dFFF0000000000000, %fd1924, %p106;

$L__BB9_119:
	sub.f64 	%fd1969, %fd4802, %fd4815;
	div.rn.f64 	%fd1970, %fd1969, %fd4815;
	mul.f64 	%fd1971, %fd3, %fd1970;
	mul.f64 	%fd1972, %fd1970, %fd1971;
	mul.f64 	%fd4777, %fd1972, %fd4776;

$L__BB9_120:
	selp.f64 	%fd4864, %fd4777, 0d0000000000000000, %p94;
	mul.f64 	%fd4863, %fd4857, %fd4862;
	mov.u32 	%r1040, %r1036;

$L__BB9_121:
	selp.u16 	%rs307, 1, 0, %p3;
	setp.eq.s32 	%p109, %r388, 0;
	mov.u16 	%rs306, 1;
	mov.f64 	%fd4818, %fd4817;
	mov.f64 	%fd4819, %fd4817;
	mov.f64 	%fd4821, %fd4784;
	mov.f64 	%fd4822, %fd374;
	mov.f64 	%fd4824, %fd4790;
	mov.f64 	%fd4825, %fd380;
	mov.f64 	%fd4827, %fd4783;
	mov.f64 	%fd4828, %fd373;
	mov.f64 	%fd4830, %fd4789;
	mov.f64 	%fd4831, %fd379;
	mov.f64 	%fd4833, %fd4782;
	mov.f64 	%fd4834, %fd372;
	mov.f64 	%fd4836, %fd4788;
	mov.f64 	%fd4837, %fd378;
	@%p109 bra 	$L__BB9_123;

	mul.lo.s64 	%rd288, %rd96, %rd69;
	add.s64 	%rd289, %rd33, %rd288;
	ld.global.u32 	%r963, [%rd289];
	cvt.s64.s32 	%rd290, %r963;
	mul.lo.s64 	%rd291, %rd290, %rd70;
	add.s64 	%rd292, %rd29, %rd291;
	mul.lo.s64 	%rd293, %rd96, %rd71;
	add.s64 	%rd294, %rd32, %rd293;
	ld.global.u32 	%r962, [%rd294];
	cvt.s64.s32 	%rd295, %r962;
	mul.lo.s64 	%rd296, %rd295, %rd70;
	add.s64 	%rd297, %rd29, %rd296;
	ld.global.f64 	%fd4632, [%rd292];
	add.f64 	%fd1976, %fd4632, %fd4632;
	ld.global.f64 	%fd4631, [%rd297];
	mul.f64 	%fd1977, %fd1976, %fd4631;
	add.f64 	%fd1978, %fd4632, %fd4631;
	setp.neu.f64 	%p110, %fd1978, 0d0000000000000000;
	div.rn.f64 	%fd1979, %fd1977, %fd1978;
	selp.f64 	%fd4633, %fd1979, 0d0000000000000000, %p110;
	mul.lo.s64 	%rd298, %rd96, %rd72;
	add.s64 	%rd299, %rd38, %rd298;
	ld.global.f64 	%fd4630, [%rd299];
	mul.f64 	%fd1980, %fd4630, %fd4633;
	mul.lo.s64 	%rd300, %rd96, %rd73;
	add.s64 	%rd301, %rd36, %rd300;
	ld.global.f64 	%fd4819, [%rd301];
	ld.global.f64 	%fd4818, [%rd301+8];
	ld.global.f64 	%fd4817, [%rd301+16];
	mul.f64 	%fd4690, %fd1980, %fd1180;
	mov.u16 	%rs306, 0;
	mov.f64 	%fd4821, %fd4784;
	mov.f64 	%fd4822, %fd374;
	mov.f64 	%fd4824, %fd4790;
	mov.f64 	%fd4825, %fd380;
	mov.f64 	%fd4827, %fd4783;
	mov.f64 	%fd4828, %fd373;
	mov.f64 	%fd4830, %fd4789;
	mov.f64 	%fd4831, %fd379;
	mov.f64 	%fd4833, %fd4782;
	mov.f64 	%fd4834, %fd372;
	mov.f64 	%fd4836, %fd4788;
	mov.f64 	%fd4837, %fd378;
	mov.u32 	%r961, %r1036;

$L__BB9_123:
	@%p62 bra 	$L__BB9_314;

	and.b16  	%rs289, %rs306, 255;
	setp.ne.s16 	%p112, %rs289, 0;
	mov.f64 	%fd4891, 0d0000000000000000;
	mov.f64 	%fd4875, %fd4891;
	mov.f64 	%fd4876, %fd4891;
	mov.f64 	%fd4877, %fd4891;
	mov.f64 	%fd4878, %fd4891;
	mov.f64 	%fd4879, %fd4891;
	mov.f64 	%fd4880, %fd4891;
	@%p112 bra 	$L__BB9_151;

	setp.eq.s64 	%p113, %rd154, 0;
	@%p113 bra 	$L__BB9_127;

	cvta.to.global.u64 	%rd302, %rd154;
	cvt.s64.s32 	%rd303, %r961;
	mul.lo.s64 	%rd304, %rd303, %rd65;
	add.s64 	%rd305, %rd302, %rd304;
	ld.global.f64 	%fd1987, [%rd305];
	add.f64 	%fd4869, %fd1987, 0d0000000000000000;
	bra.uni 	$L__BB9_129;

$L__BB9_127:
	setp.eq.s64 	%p114, %rd113, 0;
	mov.f64 	%fd4869, 0d0000000000000000;
	@%p114 bra 	$L__BB9_129;

	cvta.to.global.u64 	%rd306, %rd113;
	cvt.s64.s32 	%rd307, %r961;
	mul.lo.s64 	%rd308, %rd307, %rd67;
	add.s64 	%rd309, %rd306, %rd308;
	ld.global.f64 	%fd1989, [%rd309];
	add.f64 	%fd4869, %fd1989, 0d0000000000000000;

$L__BB9_129:
	mul.f64 	%fd1990, %fd4819, %fd4819;
	mov.f64 	%fd1991, 0d3FF0000000000000;
	sub.f64 	%fd459, %fd1991, %fd1990;
	mul.f64 	%fd1992, %fd4819, %fd4818;
	mov.f64 	%fd1993, 0d0000000000000000;
	sub.f64 	%fd460, %fd1993, %fd1992;
	mul.f64 	%fd1994, %fd4819, %fd4817;
	sub.f64 	%fd461, %fd1993, %fd1994;
	mul.f64 	%fd1995, %fd4818, %fd4818;
	sub.f64 	%fd462, %fd1991, %fd1995;
	mul.f64 	%fd1996, %fd4818, %fd4817;
	sub.f64 	%fd463, %fd1993, %fd1996;
	mul.f64 	%fd1997, %fd4817, %fd4817;
	sub.f64 	%fd464, %fd1991, %fd1997;
	sub.f64 	%fd465, %fd4798, %fd4801;
	sub.f64 	%fd466, %fd4797, %fd4800;
	mul.f64 	%fd1998, %fd460, %fd466;
	mul.f64 	%fd1999, %fd462, %fd466;
	mul.f64 	%fd2000, %fd463, %fd466;
	fma.rn.f64 	%fd2001, %fd459, %fd465, %fd1998;
	fma.rn.f64 	%fd2002, %fd460, %fd465, %fd1999;
	fma.rn.f64 	%fd2003, %fd461, %fd465, %fd2000;
	sub.f64 	%fd467, %fd4796, %fd4799;
	fma.rn.f64 	%fd2004, %fd461, %fd467, %fd2001;
	fma.rn.f64 	%fd2005, %fd463, %fd467, %fd2002;
	fma.rn.f64 	%fd2006, %fd464, %fd467, %fd2003;
	div.rn.f64 	%fd468, %fd2004, %fd1177;
	div.rn.f64 	%fd469, %fd2005, %fd1177;
	div.rn.f64 	%fd470, %fd2006, %fd1177;
	mul.f64 	%fd2007, %fd469, %fd469;
	fma.rn.f64 	%fd2008, %fd468, %fd468, %fd2007;
	fma.rn.f64 	%fd2009, %fd470, %fd470, %fd2008;
	sqrt.rn.f64 	%fd471, %fd2009;
	setp.ge.f64 	%p115, %fd471, %fd1181;
	mul.f64 	%fd4870, %fd471, %fd1177;
	@%p115 bra 	$L__BB9_131;

	mov.f64 	%fd4623, 0d0000000000000000;
	mul.f64 	%fd4583, %fd471, %fd1177;
	mul.f64 	%fd2010, %fd4583, %fd4583;
	sub.f64 	%fd2012, %fd4623, %fd4583;
	div.rn.f64 	%fd2013, %fd2012, 0d4008000000000000;
	add.f64 	%fd2014, %fd5, %fd2013;
	mul.f64 	%fd2015, %fd2010, %fd2014;
	div.rn.f64 	%fd2016, %fd2015, %fd6;
	add.f64 	%fd4870, %fd7, %fd2016;

$L__BB9_131:
	add.f64 	%fd2017, %fd4869, 0d0000000000000000;
	fma.rn.f64 	%fd475, %fd2017, %fd4870, 0d0000000000000000;
	fma.rn.f64 	%fd4871, %fd4690, %fd2017, 0d0000000000000000;
	@%p115 bra 	$L__BB9_133;

	mul.f64 	%fd4582, %fd471, %fd1177;
	mul.f64 	%fd2018, %fd4582, %fd4582;
	mov.f64 	%fd2019, 0d0000000000000000;
	sub.f64 	%fd2020, %fd2019, %fd4582;
	div.rn.f64 	%fd2021, %fd2020, 0d4008000000000000;
	add.f64 	%fd2022, %fd5, %fd2021;
	div.rn.f64 	%fd2023, %fd4871, %fd6;
	add.f64 	%fd2024, %fd2023, 0d0000000000000000;
	fma.rn.f64 	%fd2025, %fd2024, %fd2022, 0d0000000000000000;
	fma.rn.f64 	%fd2026, %fd2024, %fd2018, 0d0000000000000000;
	div.rn.f64 	%fd2027, %fd2026, 0d4008000000000000;
	add.f64 	%fd2028, %fd2027, 0d0000000000000000;
	sub.f64 	%fd2029, %fd2019, %fd2028;
	fma.rn.f64 	%fd2030, %fd4582, %fd2025, %fd2029;
	fma.rn.f64 	%fd4871, %fd4582, %fd2025, %fd2030;

$L__BB9_133:
	fma.rn.f64 	%fd479, %fd4871, %fd1177, 0d0000000000000000;
	mov.f64 	%fd2033, 0d0000000000000000;
	setp.leu.f64 	%p117, %fd471, 0d0000000000000000;
	mov.f64 	%fd4872, %fd2033;
	mov.f64 	%fd4873, %fd2033;
	mov.f64 	%fd4874, %fd2033;
	@%p117 bra 	$L__BB9_135;

	div.rn.f64 	%fd2034, %fd468, %fd471;
	div.rn.f64 	%fd2035, %fd469, %fd471;
	div.rn.f64 	%fd2036, %fd470, %fd471;
	fma.rn.f64 	%fd4872, %fd2034, %fd479, 0d0000000000000000;
	fma.rn.f64 	%fd4873, %fd2035, %fd479, 0d0000000000000000;
	fma.rn.f64 	%fd4874, %fd2036, %fd479, 0d0000000000000000;

$L__BB9_135:
	mov.f64 	%fd4581, 0d3FF0000000000000;
	mul.f64 	%fd4580, %fd4817, %fd4817;
	sub.f64 	%fd4579, %fd4581, %fd4580;
	mul.f64 	%fd4578, %fd4819, %fd4817;
	sub.f64 	%fd4577, %fd1993, %fd4578;
	mul.f64 	%fd4576, %fd4819, %fd4819;
	sub.f64 	%fd4575, %fd4581, %fd4576;
	mul.f64 	%fd4574, %fd4818, %fd4817;
	sub.f64 	%fd4573, %fd1993, %fd4574;
	mul.f64 	%fd4572, %fd4818, %fd4818;
	sub.f64 	%fd4571, %fd4581, %fd4572;
	mul.f64 	%fd4570, %fd4819, %fd4818;
	sub.f64 	%fd4569, %fd1993, %fd4570;
	div.rn.f64 	%fd2037, %fd4872, %fd1177;
	add.f64 	%fd2038, %fd2037, 0d0000000000000000;
	div.rn.f64 	%fd2040, %fd4873, %fd1177;
	add.f64 	%fd2041, %fd2040, 0d0000000000000000;
	div.rn.f64 	%fd2042, %fd4874, %fd1177;
	add.f64 	%fd2043, %fd2042, 0d0000000000000000;
	fma.rn.f64 	%fd2044, %fd2038, %fd465, 0d0000000000000000;
	fma.rn.f64 	%fd2045, %fd2038, %fd466, 0d0000000000000000;
	fma.rn.f64 	%fd2046, %fd2038, %fd467, 0d0000000000000000;
	fma.rn.f64 	%fd2047, %fd2041, %fd465, 0d0000000000000000;
	fma.rn.f64 	%fd2048, %fd2041, %fd466, 0d0000000000000000;
	fma.rn.f64 	%fd2049, %fd2041, %fd467, 0d0000000000000000;
	fma.rn.f64 	%fd2050, %fd2043, %fd465, 0d0000000000000000;
	fma.rn.f64 	%fd2051, %fd2043, %fd466, 0d0000000000000000;
	fma.rn.f64 	%fd2052, %fd2043, %fd467, 0d0000000000000000;
	mul.f64 	%fd2053, %fd4569, %fd2041;
	mul.f64 	%fd2054, %fd4571, %fd2041;
	mul.f64 	%fd2055, %fd4573, %fd2041;
	fma.rn.f64 	%fd2056, %fd4575, %fd2038, %fd2053;
	fma.rn.f64 	%fd2057, %fd4569, %fd2038, %fd2054;
	fma.rn.f64 	%fd2058, %fd4577, %fd2038, %fd2055;
	fma.rn.f64 	%fd2059, %fd4573, %fd2043, %fd2057;
	fma.rn.f64 	%fd2060, %fd4579, %fd2043, %fd2058;
	add.f64 	%fd4876, %fd2059, 0d0000000000000000;
	fma.rn.f64 	%fd2061, %fd4577, %fd2043, %fd2056;
	add.f64 	%fd4877, %fd2061, 0d0000000000000000;
	add.f64 	%fd4875, %fd2060, 0d0000000000000000;
	sub.f64 	%fd2062, %fd2033, %fd2044;
	sub.f64 	%fd2063, %fd2033, %fd2047;
	sub.f64 	%fd2064, %fd2033, %fd2050;
	sub.f64 	%fd2065, %fd2033, %fd2045;
	sub.f64 	%fd2066, %fd2033, %fd2048;
	sub.f64 	%fd2067, %fd2033, %fd2051;
	sub.f64 	%fd2068, %fd2033, %fd2046;
	sub.f64 	%fd2069, %fd2033, %fd2049;
	sub.f64 	%fd2070, %fd2033, %fd2052;
	mul.f64 	%fd2071, %fd2063, %fd4818;
	mul.f64 	%fd2072, %fd2066, %fd4818;
	mul.f64 	%fd2073, %fd2069, %fd4818;
	fma.rn.f64 	%fd2074, %fd2062, %fd4819, %fd2071;
	fma.rn.f64 	%fd2075, %fd2065, %fd4819, %fd2072;
	fma.rn.f64 	%fd2076, %fd2068, %fd4819, %fd2073;
	fma.rn.f64 	%fd2077, %fd2067, %fd4817, %fd2075;
	fma.rn.f64 	%fd2078, %fd2070, %fd4817, %fd2076;
	add.f64 	%fd2079, %fd2077, 0d0000000000000000;
	fma.rn.f64 	%fd2080, %fd2064, %fd4817, %fd2074;
	add.f64 	%fd2081, %fd2080, 0d0000000000000000;
	add.f64 	%fd2082, %fd2078, 0d0000000000000000;
	mul.f64 	%fd2083, %fd2065, %fd4818;
	mul.f64 	%fd2084, %fd2067, %fd4818;
	fma.rn.f64 	%fd2085, %fd2062, %fd4819, %fd2083;
	fma.rn.f64 	%fd2086, %fd2063, %fd4819, %fd2072;
	fma.rn.f64 	%fd2087, %fd2064, %fd4819, %fd2084;
	fma.rn.f64 	%fd2088, %fd2068, %fd4817, %fd2085;
	fma.rn.f64 	%fd2089, %fd2069, %fd4817, %fd2086;
	fma.rn.f64 	%fd2090, %fd2070, %fd4817, %fd2087;
	add.f64 	%fd489, %fd2081, %fd2088;
	add.f64 	%fd490, %fd2079, %fd2089;
	add.f64 	%fd491, %fd2082, %fd2090;
	setp.eq.s64 	%p118, %rd160, 0;
	@%p118 bra 	$L__BB9_137;

	mul.lo.s64 	%rd313, %rd96, %rd83;
	add.s64 	%rd310, %rd160, %rd313;
	// begin inline asm
	{ atom.add.f64 %fd2091,[%rd310],%fd489; }

	// end inline asm
	add.s64 	%rd311, %rd310, 8;
	// begin inline asm
	{ atom.add.f64 %fd2093,[%rd311],%fd490; }

	// end inline asm
	add.s64 	%rd312, %rd310, 16;
	// begin inline asm
	{ atom.add.f64 %fd2095,[%rd312],%fd491; }

	// end inline asm
	bra.uni 	$L__BB9_139;

$L__BB9_137:
	setp.eq.s64 	%p119, %rd119, 0;
	@%p119 bra 	$L__BB9_139;

	mul.lo.s64 	%rd317, %rd96, %rd73;
	add.s64 	%rd314, %rd119, %rd317;
	// begin inline asm
	{ atom.add.f64 %fd2097,[%rd314],%fd489; }

	// end inline asm
	add.s64 	%rd315, %rd314, 8;
	// begin inline asm
	{ atom.add.f64 %fd2099,[%rd315],%fd490; }

	// end inline asm
	add.s64 	%rd316, %rd314, 16;
	// begin inline asm
	{ atom.add.f64 %fd2101,[%rd316],%fd491; }

	// end inline asm

$L__BB9_139:
	setp.eq.s64 	%p120, %rd156, 0;
	fma.rn.f64 	%fd2103, %fd475, %fd1180, 0d0000000000000000;
	fma.rn.f64 	%fd492, %fd4630, %fd2103, 0d0000000000000000;
	fma.rn.f64 	%fd493, %fd4633, %fd2103, 0d0000000000000000;
	@%p120 bra 	$L__BB9_141;

	mul.lo.s64 	%rd319, %rd96, %rd84;
	add.s64 	%rd318, %rd156, %rd319;
	// begin inline asm
	{ atom.add.f64 %fd2104,[%rd318],%fd493; }

	// end inline asm
	bra.uni 	$L__BB9_143;

$L__BB9_141:
	setp.eq.s64 	%p121, %rd115, 0;
	@%p121 bra 	$L__BB9_143;

	mul.lo.s64 	%rd321, %rd96, %rd72;
	add.s64 	%rd320, %rd115, %rd321;
	// begin inline asm
	{ atom.add.f64 %fd2106,[%rd320],%fd493; }

	// end inline asm

$L__BB9_143:
	setp.eq.s64 	%p122, %rd168, 0;
	add.f64 	%fd2108, %fd4632, %fd4632;
	mul.f64 	%fd2109, %fd2108, %fd4631;
	add.f64 	%fd2110, %fd4632, %fd4631;
	setp.neu.f64 	%p123, %fd2110, 0d0000000000000000;
	mov.f64 	%fd2111, 0d0000000000000000;
	div.rn.f64 	%fd2112, %fd2109, %fd2110;
	selp.f64 	%fd2113, %fd492, 0d0000000000000000, %p123;
	div.rn.f64 	%fd2114, %fd2113, %fd2110;
	add.f64 	%fd2115, %fd2114, 0d0000000000000000;
	mul.f64 	%fd2116, %fd2112, %fd2113;
	div.rn.f64 	%fd2117, %fd2116, %fd2110;
	sub.f64 	%fd2118, %fd2111, %fd2117;
	add.f64 	%fd2119, %fd2118, 0d0000000000000000;
	fma.rn.f64 	%fd2120, %fd4631, %fd2115, 0d0000000000000000;
	fma.rn.f64 	%fd494, %fd2108, %fd2115, %fd2119;
	fma.rn.f64 	%fd495, %fd2120, 0d4000000000000000, %fd2119;
	@%p122 bra 	$L__BB9_145;

	cvt.s64.s32 	%rd323, %r962;
	mul.lo.s64 	%rd324, %rd323, %rd85;
	add.s64 	%rd322, %rd168, %rd324;
	// begin inline asm
	{ atom.add.f64 %fd2121,[%rd322],%fd494; }

	// end inline asm
	bra.uni 	$L__BB9_147;

$L__BB9_145:
	setp.eq.s64 	%p124, %rd139, 0;
	@%p124 bra 	$L__BB9_147;

	cvt.s64.s32 	%rd326, %r962;
	mul.lo.s64 	%rd327, %rd326, %rd70;
	add.s64 	%rd325, %rd139, %rd327;
	// begin inline asm
	{ atom.add.f64 %fd2123,[%rd325],%fd494; }

	// end inline asm

$L__BB9_147:
	mov.f64 	%fd2125, 0d0000000000000000;
	sub.f64 	%fd4879, %fd2125, %fd4876;
	sub.f64 	%fd4880, %fd2125, %fd4877;
	sub.f64 	%fd4878, %fd2125, %fd4875;
	@%p122 bra 	$L__BB9_149;

	cvt.s64.s32 	%rd329, %r963;
	mul.lo.s64 	%rd330, %rd329, %rd85;
	add.s64 	%rd328, %rd168, %rd330;
	// begin inline asm
	{ atom.add.f64 %fd2126,[%rd328],%fd495; }

	// end inline asm
	bra.uni 	$L__BB9_151;

$L__BB9_149:
	setp.eq.s64 	%p126, %rd139, 0;
	@%p126 bra 	$L__BB9_151;

	cvt.s64.s32 	%rd332, %r963;
	mul.lo.s64 	%rd333, %rd332, %rd70;
	add.s64 	%rd331, %rd139, %rd333;
	// begin inline asm
	{ atom.add.f64 %fd2128,[%rd331],%fd495; }

	// end inline asm

$L__BB9_151:
	setp.eq.s16 	%p127, %rs289, 0;
	mov.f64 	%fd4903, %fd4891;
	mov.f64 	%fd4904, %fd4891;
	@%p127 bra 	$L__BB9_174;

	setp.eq.s64 	%p128, %rd154, 0;
	@%p128 bra 	$L__BB9_154;

	cvta.to.global.u64 	%rd334, %rd154;
	cvt.s64.s32 	%rd335, %r1040;
	mul.lo.s64 	%rd336, %rd335, %rd65;
	add.s64 	%rd337, %rd334, %rd336;
	ld.global.f64 	%fd2133, [%rd337];
	add.f64 	%fd4881, %fd2133, 0d0000000000000000;
	bra.uni 	$L__BB9_156;

$L__BB9_154:
	setp.eq.s64 	%p129, %rd113, 0;
	mov.f64 	%fd4881, 0d0000000000000000;
	@%p129 bra 	$L__BB9_156;

	cvta.to.global.u64 	%rd338, %rd113;
	cvt.s64.s32 	%rd339, %r1040;
	mul.lo.s64 	%rd340, %rd339, %rd67;
	add.s64 	%rd341, %rd338, %rd340;
	ld.global.f64 	%fd2135, [%rd341];
	add.f64 	%fd4881, %fd2135, 0d0000000000000000;

$L__BB9_156:
	setp.geu.f64 	%p130, %fd4802, %fd4815;
	@%p130 bra 	$L__BB9_164;

	sub.f64 	%fd2138, %fd4802, %fd4815;
	div.rn.f64 	%fd4884, %fd2138, %fd4815;
	mul.f64 	%fd4885, %fd3, %fd4884;
	div.rn.f64 	%fd4887, %fd4802, %fd4815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1044}, %fd4887;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1045, %temp}, %fd4887;
	}
	setp.gt.s32 	%p131, %r1044, 1048575;
	mov.u32 	%r1046, -1023;
	mov.f64 	%fd4882, %fd4887;
	@%p131 bra 	$L__BB9_159;

	mul.f64 	%fd4882, %fd4887, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1044}, %fd4882;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1045, %temp}, %fd4882;
	}
	mov.u32 	%r1046, -1077;

$L__BB9_159:
	mul.f64 	%fd4886, %fd4884, %fd4885;
	add.s32 	%r880, %r1044, -1;
	setp.lt.u32 	%p132, %r880, 2146435071;
	@%p132 bra 	$L__BB9_161;
	bra.uni 	$L__BB9_160;

$L__BB9_161:
	shr.u32 	%r882, %r1044, 20;
	add.s32 	%r1047, %r1046, %r882;
	and.b32  	%r883, %r1044, -2146435073;
	or.b32  	%r884, %r883, 1072693248;
	mov.b64 	%fd4883, {%r1045, %r884};
	setp.lt.s32 	%p134, %r884, 1073127583;
	@%p134 bra 	$L__BB9_163;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r885, %temp}, %fd4883;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r886}, %fd4883;
	}
	add.s32 	%r887, %r886, -1048576;
	mov.b64 	%fd4883, {%r885, %r887};
	add.s32 	%r1047, %r1047, 1;

$L__BB9_163:
	add.f64 	%fd2141, %fd4883, 0d3FF0000000000000;
	mov.f64 	%fd2142, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2143, %fd2141;
	neg.f64 	%fd2144, %fd2141;
	fma.rn.f64 	%fd2145, %fd2144, %fd2143, %fd2142;
	fma.rn.f64 	%fd2146, %fd2145, %fd2145, %fd2145;
	fma.rn.f64 	%fd2147, %fd2146, %fd2143, %fd2143;
	add.f64 	%fd2148, %fd4883, 0dBFF0000000000000;
	mul.f64 	%fd2149, %fd2148, %fd2147;
	fma.rn.f64 	%fd2150, %fd2148, %fd2147, %fd2149;
	mul.f64 	%fd2151, %fd2150, %fd2150;
	mov.f64 	%fd2152, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2153, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2154, %fd2153, %fd2151, %fd2152;
	mov.f64 	%fd2155, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2156, %fd2154, %fd2151, %fd2155;
	mov.f64 	%fd2157, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2158, %fd2156, %fd2151, %fd2157;
	mov.f64 	%fd2159, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2160, %fd2158, %fd2151, %fd2159;
	mov.f64 	%fd2161, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2162, %fd2160, %fd2151, %fd2161;
	mov.f64 	%fd2163, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2164, %fd2162, %fd2151, %fd2163;
	mov.f64 	%fd2165, 0d3FB5555555555554;
	fma.rn.f64 	%fd2166, %fd2164, %fd2151, %fd2165;
	sub.f64 	%fd2167, %fd2148, %fd2150;
	add.f64 	%fd2168, %fd2167, %fd2167;
	neg.f64 	%fd2169, %fd2150;
	fma.rn.f64 	%fd2170, %fd2169, %fd2148, %fd2168;
	mul.f64 	%fd2171, %fd2147, %fd2170;
	mul.f64 	%fd2172, %fd2151, %fd2166;
	fma.rn.f64 	%fd2173, %fd2172, %fd2150, %fd2171;
	xor.b32  	%r888, %r1047, -2147483648;
	mov.u32 	%r889, -2147483648;
	mov.u32 	%r890, 1127219200;
	mov.b64 	%fd2174, {%r888, %r890};
	mov.b64 	%fd2175, {%r889, %r890};
	sub.f64 	%fd2176, %fd2174, %fd2175;
	mov.f64 	%fd2177, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2178, %fd2176, %fd2177, %fd2150;
	neg.f64 	%fd2179, %fd2176;
	fma.rn.f64 	%fd2180, %fd2179, %fd2177, %fd2178;
	sub.f64 	%fd2181, %fd2180, %fd2150;
	sub.f64 	%fd2182, %fd2173, %fd2181;
	mov.f64 	%fd2183, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2184, %fd2176, %fd2183, %fd2182;
	add.f64 	%fd4888, %fd2178, %fd2184;
	bra.uni 	$L__BB9_164;

$L__BB9_160:
	mov.f64 	%fd2139, 0d7FF0000000000000;
	fma.rn.f64 	%fd2140, %fd4882, %fd2139, %fd2139;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r881}, %fd4882;
	}
	mov.b32 	%f6, %r881;
	setp.eq.f32 	%p133, %f6, 0f00000000;
	selp.f64 	%fd4888, 0dFFF0000000000000, %fd2140, %p133;

$L__BB9_164:
	fma.rn.f64 	%fd4625, %fd4881, %fd1180, 0d0000000000000000;
	fma.rn.f64 	%fd4624, %fd4863, %fd4625, 0d0000000000000000;
	setp.lt.f64 	%p136, %fd4802, %fd4815;
	selp.f64 	%fd526, %fd4624, 0d0000000000000000, %p136;
	mov.f64 	%fd4903, 0d0000000000000000;
	mov.f64 	%fd4904, %fd4903;
	@%p130 bra 	$L__BB9_166;

	fma.rn.f64 	%fd2187, %fd526, %fd4888, 0d0000000000000000;
	mov.f64 	%fd2188, 0d0000000000000000;
	fma.rn.f64 	%fd2189, %fd526, %fd4886, 0d0000000000000000;
	rcp.rn.f64 	%fd2190, %fd4887;
	fma.rn.f64 	%fd2191, %fd2190, %fd2189, 0d0000000000000000;
	div.rn.f64 	%fd2192, %fd2191, %fd4815;
	add.f64 	%fd2193, %fd2192, 0d0000000000000000;
	mul.f64 	%fd2194, %fd4887, %fd2191;
	div.rn.f64 	%fd2195, %fd2194, %fd4815;
	sub.f64 	%fd2196, %fd2188, %fd2195;
	fma.rn.f64 	%fd2197, %fd4884, %fd2187, 0d0000000000000000;
	fma.rn.f64 	%fd2198, %fd4885, %fd2187, 0d0000000000000000;
	div.rn.f64 	%fd2199, %fd2198, %fd4815;
	add.f64 	%fd2200, %fd2199, 0d0000000000000000;
	mul.f64 	%fd2201, %fd4884, %fd2198;
	div.rn.f64 	%fd2202, %fd2201, %fd4815;
	sub.f64 	%fd2203, %fd2196, %fd2202;
	fma.rn.f64 	%fd2204, %fd3, %fd2197, 0d0000000000000000;
	div.rn.f64 	%fd2205, %fd2204, %fd4815;
	add.f64 	%fd2206, %fd2200, %fd2205;
	mul.f64 	%fd2207, %fd4884, %fd2204;
	div.rn.f64 	%fd2208, %fd2207, %fd4815;
	sub.f64 	%fd2209, %fd2203, %fd2208;
	add.f64 	%fd4903, %fd2193, %fd2206;
	sub.f64 	%fd4904, %fd2209, %fd2206;

$L__BB9_166:
	fma.rn.f64 	%fd4627, %fd4881, %fd1180, 0d0000000000000000;
	fma.rn.f64 	%fd4626, %fd4864, %fd4627, 0d0000000000000000;
	fma.rn.f64 	%fd2210, %fd4857, %fd4626, 0d0000000000000000;
	fma.rn.f64 	%fd4891, %fd4862, %fd4626, 0d0000000000000000;
	fma.rn.f64 	%fd2211, %fd2210, %fd1176, 0d0000000000000000;
	fma.rn.f64 	%fd532, %fd2211, %fd1179, 0d0000000000000000;
	setp.eq.s64 	%p137, %rd178, 0;
	@%p137 bra 	$L__BB9_168;

	cvt.s64.s32 	%rd343, %r1027;
	mul.lo.s64 	%rd344, %rd343, %rd74;
	add.s64 	%rd342, %rd178, %rd344;
	// begin inline asm
	{ atom.add.f64 %fd2212,[%rd342],%fd532; }

	// end inline asm
	bra.uni 	$L__BB9_170;

$L__BB9_168:
	setp.eq.s64 	%p138, %rd149, 0;
	@%p138 bra 	$L__BB9_170;

	cvt.s64.s32 	%rd346, %r1027;
	mul.lo.s64 	%rd347, %rd346, %rd68;
	add.s64 	%rd345, %rd149, %rd347;
	// begin inline asm
	{ atom.add.f64 %fd2214,[%rd345],%fd532; }

	// end inline asm

$L__BB9_170:
	@%p137 bra 	$L__BB9_172;

	cvt.s64.s32 	%rd349, %r1026;
	mul.lo.s64 	%rd350, %rd349, %rd74;
	add.s64 	%rd348, %rd178, %rd350;
	// begin inline asm
	{ atom.add.f64 %fd2216,[%rd348],%fd532; }

	// end inline asm
	bra.uni 	$L__BB9_174;

$L__BB9_172:
	setp.eq.s64 	%p140, %rd149, 0;
	@%p140 bra 	$L__BB9_174;

	cvt.s64.s32 	%rd352, %r1026;
	mul.lo.s64 	%rd353, %rd352, %rd68;
	add.s64 	%rd351, %rd149, %rd353;
	// begin inline asm
	{ atom.add.f64 %fd2218,[%rd351],%fd532; }

	// end inline asm

$L__BB9_174:
	and.b16  	%rs291, %rs307, 255;
	setp.eq.s16 	%p141, %rs291, 0;
	@%p141 bra 	$L__BB9_193;

	setp.geu.f64 	%p142, %fd4802, %fd4815;
	@%p142 bra 	$L__BB9_183;

	sub.f64 	%fd2221, %fd4802, %fd4815;
	div.rn.f64 	%fd4896, %fd2221, %fd4815;
	mul.f64 	%fd4897, %fd3, %fd4896;
	div.rn.f64 	%fd4899, %fd4802, %fd4815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1048}, %fd4899;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1049, %temp}, %fd4899;
	}
	setp.gt.s32 	%p143, %r1048, 1048575;
	mov.u32 	%r1050, -1023;
	mov.f64 	%fd4894, %fd4899;
	@%p143 bra 	$L__BB9_178;

	mul.f64 	%fd4894, %fd4899, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1048}, %fd4894;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1049, %temp}, %fd4894;
	}
	mov.u32 	%r1050, -1077;

$L__BB9_178:
	mul.f64 	%fd4898, %fd4896, %fd4897;
	add.s32 	%r893, %r1048, -1;
	setp.lt.u32 	%p144, %r893, 2146435071;
	@%p144 bra 	$L__BB9_180;
	bra.uni 	$L__BB9_179;

$L__BB9_180:
	shr.u32 	%r895, %r1048, 20;
	add.s32 	%r1051, %r1050, %r895;
	and.b32  	%r896, %r1048, -2146435073;
	or.b32  	%r897, %r896, 1072693248;
	mov.b64 	%fd4895, {%r1049, %r897};
	setp.lt.s32 	%p146, %r897, 1073127583;
	@%p146 bra 	$L__BB9_182;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r898, %temp}, %fd4895;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r899}, %fd4895;
	}
	add.s32 	%r900, %r899, -1048576;
	mov.b64 	%fd4895, {%r898, %r900};
	add.s32 	%r1051, %r1051, 1;

$L__BB9_182:
	add.f64 	%fd2224, %fd4895, 0d3FF0000000000000;
	mov.f64 	%fd2225, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2226, %fd2224;
	neg.f64 	%fd2227, %fd2224;
	fma.rn.f64 	%fd2228, %fd2227, %fd2226, %fd2225;
	fma.rn.f64 	%fd2229, %fd2228, %fd2228, %fd2228;
	fma.rn.f64 	%fd2230, %fd2229, %fd2226, %fd2226;
	add.f64 	%fd2231, %fd4895, 0dBFF0000000000000;
	mul.f64 	%fd2232, %fd2231, %fd2230;
	fma.rn.f64 	%fd2233, %fd2231, %fd2230, %fd2232;
	mul.f64 	%fd2234, %fd2233, %fd2233;
	mov.f64 	%fd2235, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2236, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2237, %fd2236, %fd2234, %fd2235;
	mov.f64 	%fd2238, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2239, %fd2237, %fd2234, %fd2238;
	mov.f64 	%fd2240, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2241, %fd2239, %fd2234, %fd2240;
	mov.f64 	%fd2242, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2243, %fd2241, %fd2234, %fd2242;
	mov.f64 	%fd2244, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2245, %fd2243, %fd2234, %fd2244;
	mov.f64 	%fd2246, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2247, %fd2245, %fd2234, %fd2246;
	mov.f64 	%fd2248, 0d3FB5555555555554;
	fma.rn.f64 	%fd2249, %fd2247, %fd2234, %fd2248;
	sub.f64 	%fd2250, %fd2231, %fd2233;
	add.f64 	%fd2251, %fd2250, %fd2250;
	neg.f64 	%fd2252, %fd2233;
	fma.rn.f64 	%fd2253, %fd2252, %fd2231, %fd2251;
	mul.f64 	%fd2254, %fd2230, %fd2253;
	mul.f64 	%fd2255, %fd2234, %fd2249;
	fma.rn.f64 	%fd2256, %fd2255, %fd2233, %fd2254;
	xor.b32  	%r901, %r1051, -2147483648;
	mov.u32 	%r902, -2147483648;
	mov.u32 	%r903, 1127219200;
	mov.b64 	%fd2257, {%r901, %r903};
	mov.b64 	%fd2258, {%r902, %r903};
	sub.f64 	%fd2259, %fd2257, %fd2258;
	mov.f64 	%fd2260, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2261, %fd2259, %fd2260, %fd2233;
	neg.f64 	%fd2262, %fd2259;
	fma.rn.f64 	%fd2263, %fd2262, %fd2260, %fd2261;
	sub.f64 	%fd2264, %fd2263, %fd2233;
	sub.f64 	%fd2265, %fd2256, %fd2264;
	mov.f64 	%fd2266, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2267, %fd2259, %fd2266, %fd2265;
	add.f64 	%fd4900, %fd2261, %fd2267;
	bra.uni 	$L__BB9_183;

$L__BB9_179:
	mov.f64 	%fd2222, 0d7FF0000000000000;
	fma.rn.f64 	%fd2223, %fd4894, %fd2222, %fd2222;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r894}, %fd4894;
	}
	mov.b32 	%f7, %r894;
	setp.eq.f32 	%p145, %f7, 0f00000000;
	selp.f64 	%fd4900, 0dFFF0000000000000, %fd2223, %p145;

$L__BB9_183:
	@%p142 bra 	$L__BB9_185;

	fma.rn.f64 	%fd2268, %fd4900, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2269, %fd4898, 0d0000000000000000, 0d0000000000000000;
	rcp.rn.f64 	%fd2270, %fd4899;
	fma.rn.f64 	%fd2271, %fd2270, %fd2269, 0d0000000000000000;
	div.rn.f64 	%fd2272, %fd2271, %fd4815;
	add.f64 	%fd2273, %fd4903, %fd2272;
	mul.f64 	%fd2274, %fd4899, %fd2271;
	div.rn.f64 	%fd2275, %fd2274, %fd4815;
	sub.f64 	%fd2276, %fd4904, %fd2275;
	fma.rn.f64 	%fd2277, %fd4896, %fd2268, 0d0000000000000000;
	fma.rn.f64 	%fd2278, %fd4897, %fd2268, 0d0000000000000000;
	div.rn.f64 	%fd2279, %fd2278, %fd4815;
	add.f64 	%fd2280, %fd2279, 0d0000000000000000;
	mul.f64 	%fd2281, %fd4896, %fd2278;
	div.rn.f64 	%fd2282, %fd2281, %fd4815;
	sub.f64 	%fd2283, %fd2276, %fd2282;
	fma.rn.f64 	%fd2284, %fd3, %fd2277, 0d0000000000000000;
	div.rn.f64 	%fd2285, %fd2284, %fd4815;
	add.f64 	%fd2286, %fd2280, %fd2285;
	mul.f64 	%fd2287, %fd4896, %fd2284;
	div.rn.f64 	%fd2288, %fd2287, %fd4815;
	sub.f64 	%fd2289, %fd2283, %fd2288;
	add.f64 	%fd4903, %fd2273, %fd2286;
	sub.f64 	%fd4904, %fd2289, %fd2286;

$L__BB9_185:
	setp.eq.s64 	%p148, %rd178, 0;
	@%p148 bra 	$L__BB9_187;

	cvt.s64.s32 	%rd355, %r1027;
	mul.lo.s64 	%rd356, %rd355, %rd74;
	add.s64 	%rd354, %rd178, %rd356;
	mov.f64 	%fd2291, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2290,[%rd354],%fd2291; }

	// end inline asm
	bra.uni 	$L__BB9_189;

$L__BB9_187:
	setp.eq.s64 	%p149, %rd149, 0;
	@%p149 bra 	$L__BB9_189;

	cvt.s64.s32 	%rd358, %r1027;
	mul.lo.s64 	%rd359, %rd358, %rd68;
	add.s64 	%rd357, %rd149, %rd359;
	mov.f64 	%fd2293, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2292,[%rd357],%fd2293; }

	// end inline asm

$L__BB9_189:
	@%p148 bra 	$L__BB9_191;

	cvt.s64.s32 	%rd361, %r1026;
	mul.lo.s64 	%rd362, %rd361, %rd74;
	add.s64 	%rd360, %rd178, %rd362;
	mov.f64 	%fd2295, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2294,[%rd360],%fd2295; }

	// end inline asm
	bra.uni 	$L__BB9_193;

$L__BB9_191:
	setp.eq.s64 	%p151, %rd149, 0;
	@%p151 bra 	$L__BB9_193;

	cvt.s64.s32 	%rd364, %r1026;
	mul.lo.s64 	%rd365, %rd364, %rd68;
	add.s64 	%rd363, %rd149, %rd365;
	mov.f64 	%fd2297, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2296,[%rd363],%fd2297; }

	// end inline asm

$L__BB9_193:
	setp.geu.f64 	%p152, %fd4802, %fd4815;
	@%p152 bra 	$L__BB9_201;

	sub.f64 	%fd2299, %fd4802, %fd4815;
	div.rn.f64 	%fd4907, %fd2299, %fd4815;
	mul.f64 	%fd4908, %fd3, %fd4907;
	div.rn.f64 	%fd4910, %fd4802, %fd4815;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1052}, %fd4910;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1053, %temp}, %fd4910;
	}
	setp.gt.s32 	%p153, %r1052, 1048575;
	mov.u32 	%r1054, -1023;
	mov.f64 	%fd4905, %fd4910;
	@%p153 bra 	$L__BB9_196;

	mul.f64 	%fd4905, %fd4910, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1052}, %fd4905;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1053, %temp}, %fd4905;
	}
	mov.u32 	%r1054, -1077;

$L__BB9_196:
	mul.f64 	%fd4909, %fd4907, %fd4908;
	add.s32 	%r906, %r1052, -1;
	setp.lt.u32 	%p154, %r906, 2146435071;
	@%p154 bra 	$L__BB9_198;
	bra.uni 	$L__BB9_197;

$L__BB9_198:
	shr.u32 	%r908, %r1052, 20;
	add.s32 	%r1055, %r1054, %r908;
	and.b32  	%r909, %r1052, -2146435073;
	or.b32  	%r910, %r909, 1072693248;
	mov.b64 	%fd4906, {%r1053, %r910};
	setp.lt.s32 	%p156, %r910, 1073127583;
	@%p156 bra 	$L__BB9_200;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r911, %temp}, %fd4906;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r912}, %fd4906;
	}
	add.s32 	%r913, %r912, -1048576;
	mov.b64 	%fd4906, {%r911, %r913};
	add.s32 	%r1055, %r1055, 1;

$L__BB9_200:
	add.f64 	%fd2302, %fd4906, 0d3FF0000000000000;
	mov.f64 	%fd2303, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2304, %fd2302;
	neg.f64 	%fd2305, %fd2302;
	fma.rn.f64 	%fd2306, %fd2305, %fd2304, %fd2303;
	fma.rn.f64 	%fd2307, %fd2306, %fd2306, %fd2306;
	fma.rn.f64 	%fd2308, %fd2307, %fd2304, %fd2304;
	add.f64 	%fd2309, %fd4906, 0dBFF0000000000000;
	mul.f64 	%fd2310, %fd2309, %fd2308;
	fma.rn.f64 	%fd2311, %fd2309, %fd2308, %fd2310;
	mul.f64 	%fd2312, %fd2311, %fd2311;
	mov.f64 	%fd2313, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2314, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2315, %fd2314, %fd2312, %fd2313;
	mov.f64 	%fd2316, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2317, %fd2315, %fd2312, %fd2316;
	mov.f64 	%fd2318, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2319, %fd2317, %fd2312, %fd2318;
	mov.f64 	%fd2320, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2321, %fd2319, %fd2312, %fd2320;
	mov.f64 	%fd2322, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2323, %fd2321, %fd2312, %fd2322;
	mov.f64 	%fd2324, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2325, %fd2323, %fd2312, %fd2324;
	mov.f64 	%fd2326, 0d3FB5555555555554;
	fma.rn.f64 	%fd2327, %fd2325, %fd2312, %fd2326;
	sub.f64 	%fd2328, %fd2309, %fd2311;
	add.f64 	%fd2329, %fd2328, %fd2328;
	neg.f64 	%fd2330, %fd2311;
	fma.rn.f64 	%fd2331, %fd2330, %fd2309, %fd2329;
	mul.f64 	%fd2332, %fd2308, %fd2331;
	mul.f64 	%fd2333, %fd2312, %fd2327;
	fma.rn.f64 	%fd2334, %fd2333, %fd2311, %fd2332;
	xor.b32  	%r914, %r1055, -2147483648;
	mov.u32 	%r915, -2147483648;
	mov.u32 	%r916, 1127219200;
	mov.b64 	%fd2335, {%r914, %r916};
	mov.b64 	%fd2336, {%r915, %r916};
	sub.f64 	%fd2337, %fd2335, %fd2336;
	mov.f64 	%fd2338, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2339, %fd2337, %fd2338, %fd2311;
	neg.f64 	%fd2340, %fd2337;
	fma.rn.f64 	%fd2341, %fd2340, %fd2338, %fd2339;
	sub.f64 	%fd2342, %fd2341, %fd2311;
	sub.f64 	%fd2343, %fd2334, %fd2342;
	mov.f64 	%fd2344, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2345, %fd2337, %fd2344, %fd2343;
	add.f64 	%fd4911, %fd2339, %fd2345;
	bra.uni 	$L__BB9_201;

$L__BB9_197:
	mov.f64 	%fd2300, 0d7FF0000000000000;
	fma.rn.f64 	%fd2301, %fd4905, %fd2300, %fd2300;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r907}, %fd4905;
	}
	mov.b32 	%f8, %r907;
	setp.eq.f32 	%p155, %f8, 0f00000000;
	selp.f64 	%fd4911, 0dFFF0000000000000, %fd2301, %p155;

$L__BB9_201:
	fma.rn.f64 	%fd4628, %fd4, %fd4860, 0d0000000000000000;
	setp.lt.f64 	%p158, %fd4802, %fd4815;
	selp.f64 	%fd576, %fd4628, 0d0000000000000000, %p158;
	@%p152 bra 	$L__BB9_203;

	fma.rn.f64 	%fd2346, %fd576, %fd4911, 0d0000000000000000;
	fma.rn.f64 	%fd2347, %fd576, %fd4909, 0d0000000000000000;
	rcp.rn.f64 	%fd2348, %fd4910;
	fma.rn.f64 	%fd2349, %fd2348, %fd2347, 0d0000000000000000;
	div.rn.f64 	%fd2350, %fd2349, %fd4815;
	add.f64 	%fd2351, %fd4903, %fd2350;
	mul.f64 	%fd2352, %fd4910, %fd2349;
	div.rn.f64 	%fd2353, %fd2352, %fd4815;
	sub.f64 	%fd2354, %fd4904, %fd2353;
	fma.rn.f64 	%fd2355, %fd4907, %fd2346, 0d0000000000000000;
	fma.rn.f64 	%fd2356, %fd4908, %fd2346, 0d0000000000000000;
	div.rn.f64 	%fd2357, %fd2356, %fd4815;
	add.f64 	%fd2358, %fd2357, 0d0000000000000000;
	mul.f64 	%fd2359, %fd4907, %fd2356;
	div.rn.f64 	%fd2360, %fd2359, %fd4815;
	sub.f64 	%fd2361, %fd2354, %fd2360;
	fma.rn.f64 	%fd2362, %fd3, %fd2355, 0d0000000000000000;
	div.rn.f64 	%fd2363, %fd2362, %fd4815;
	add.f64 	%fd2364, %fd2358, %fd2363;
	mul.f64 	%fd2365, %fd4907, %fd2362;
	div.rn.f64 	%fd2366, %fd2365, %fd4815;
	sub.f64 	%fd2367, %fd2361, %fd2366;
	add.f64 	%fd4903, %fd2351, %fd2364;
	sub.f64 	%fd4904, %fd2367, %fd2364;

$L__BB9_203:
	fma.rn.f64 	%fd4629, %fd4, %fd4861, 0d0000000000000000;
	fma.rn.f64 	%fd2368, %fd4857, %fd4629, 0d0000000000000000;
	fma.rn.f64 	%fd581, %fd4859, %fd4629, %fd4891;
	fma.rn.f64 	%fd2369, %fd2368, %fd1176, 0d0000000000000000;
	fma.rn.f64 	%fd582, %fd2369, %fd1179, 0d0000000000000000;
	setp.eq.s64 	%p159, %rd178, 0;
	@%p159 bra 	$L__BB9_205;

	cvt.s64.s32 	%rd367, %r1027;
	mul.lo.s64 	%rd368, %rd367, %rd74;
	add.s64 	%rd366, %rd178, %rd368;
	// begin inline asm
	{ atom.add.f64 %fd2370,[%rd366],%fd582; }

	// end inline asm
	bra.uni 	$L__BB9_207;

$L__BB9_205:
	setp.eq.s64 	%p160, %rd149, 0;
	@%p160 bra 	$L__BB9_207;

	cvt.s64.s32 	%rd370, %r1027;
	mul.lo.s64 	%rd371, %rd370, %rd68;
	add.s64 	%rd369, %rd149, %rd371;
	// begin inline asm
	{ atom.add.f64 %fd2372,[%rd369],%fd582; }

	// end inline asm

$L__BB9_207:
	@%p159 bra 	$L__BB9_209;

	cvt.s64.s32 	%rd373, %r1026;
	mul.lo.s64 	%rd374, %rd373, %rd74;
	add.s64 	%rd372, %rd178, %rd374;
	// begin inline asm
	{ atom.add.f64 %fd2374,[%rd372],%fd582; }

	// end inline asm
	bra.uni 	$L__BB9_211;

$L__BB9_209:
	setp.eq.s64 	%p162, %rd149, 0;
	@%p162 bra 	$L__BB9_211;

	cvt.s64.s32 	%rd376, %r1026;
	mul.lo.s64 	%rd377, %rd376, %rd68;
	add.s64 	%rd375, %rd149, %rd377;
	// begin inline asm
	{ atom.add.f64 %fd2376,[%rd375],%fd582; }

	// end inline asm

$L__BB9_211:
	add.f64 	%fd2380, %fd4904, 0d0000000000000000;
	mov.f64 	%fd4914, 0d0000000000000000;
	fma.rn.f64 	%fd583, %fd4858, %fd2380, 0d0000000000000000;
	sub.f64 	%fd584, %fd378, %fd4788;
	sub.f64 	%fd585, %fd373, %fd4783;
	mul.f64 	%fd2381, %fd585, %fd584;
	sub.f64 	%fd586, %fd379, %fd4789;
	sub.f64 	%fd587, %fd372, %fd4782;
	mul.f64 	%fd2382, %fd587, %fd586;
	sub.f64 	%fd588, %fd2381, %fd2382;
	sub.f64 	%fd589, %fd380, %fd4790;
	mul.f64 	%fd2383, %fd587, %fd589;
	sub.f64 	%fd590, %fd374, %fd4784;
	mul.f64 	%fd2384, %fd590, %fd584;
	sub.f64 	%fd591, %fd2383, %fd2384;
	mul.f64 	%fd2385, %fd590, %fd586;
	mul.f64 	%fd2386, %fd585, %fd589;
	sub.f64 	%fd592, %fd2385, %fd2386;
	mul.f64 	%fd2387, %fd591, %fd591;
	fma.rn.f64 	%fd2388, %fd588, %fd588, %fd2387;
	fma.rn.f64 	%fd593, %fd592, %fd592, %fd2388;
	setp.geu.f64 	%p163, %fd593, %fd4856;
	mov.f64 	%fd4915, %fd4914;
	@%p163 bra 	$L__BB9_213;

	add.f64 	%fd2389, %fd581, 0d0000000000000000;
	mov.f64 	%fd2390, 0d0000000000000000;
	div.rn.f64 	%fd2391, %fd593, %fd4856;
	sub.f64 	%fd2392, %fd2390, %fd2391;
	add.f64 	%fd2393, %fd2392, 0d4000000000000000;
	fma.rn.f64 	%fd2394, %fd2391, %fd2389, 0d0000000000000000;
	fma.rn.f64 	%fd2395, %fd2393, %fd2389, 0d0000000000000000;
	sub.f64 	%fd2396, %fd2395, %fd2394;
	div.rn.f64 	%fd2397, %fd2396, %fd4856;
	add.f64 	%fd4915, %fd2397, 0d0000000000000000;
	mul.f64 	%fd2398, %fd2391, %fd2396;
	div.rn.f64 	%fd2399, %fd2398, %fd4856;
	sub.f64 	%fd4914, %fd2390, %fd2399;

$L__BB9_213:
	add.f64 	%fd598, %fd588, %fd588;
	add.f64 	%fd599, %fd591, %fd591;
	add.f64 	%fd600, %fd592, %fd592;
	fma.rn.f64 	%fd2400, %fd598, %fd4915, 0d0000000000000000;
	fma.rn.f64 	%fd2401, %fd599, %fd4915, 0d0000000000000000;
	fma.rn.f64 	%fd2402, %fd600, %fd4915, 0d0000000000000000;
	mul.f64 	%fd2403, %fd586, %fd2402;
	mul.f64 	%fd2404, %fd584, %fd2401;
	sub.f64 	%fd2405, %fd2403, %fd2404;
	mul.f64 	%fd2406, %fd584, %fd2400;
	mul.f64 	%fd2407, %fd589, %fd2402;
	sub.f64 	%fd2408, %fd2406, %fd2407;
	mul.f64 	%fd2409, %fd589, %fd2401;
	mul.f64 	%fd2410, %fd586, %fd2400;
	sub.f64 	%fd2411, %fd2409, %fd2410;
	add.f64 	%fd601, %fd2405, 0d0000000000000000;
	add.f64 	%fd602, %fd2408, 0d0000000000000000;
	add.f64 	%fd603, %fd2411, 0d0000000000000000;
	mul.f64 	%fd2412, %fd585, %fd2402;
	mul.f64 	%fd2413, %fd587, %fd2401;
	mul.f64 	%fd2414, %fd587, %fd2400;
	mul.f64 	%fd2415, %fd590, %fd2402;
	mul.f64 	%fd2416, %fd590, %fd2401;
	mul.f64 	%fd2417, %fd585, %fd2400;
	sub.f64 	%fd2418, %fd2413, %fd2412;
	add.f64 	%fd604, %fd2418, 0d0000000000000000;
	sub.f64 	%fd2419, %fd2415, %fd2414;
	add.f64 	%fd605, %fd2419, 0d0000000000000000;
	sub.f64 	%fd2420, %fd2417, %fd2416;
	add.f64 	%fd606, %fd2420, 0d0000000000000000;
	sub.f64 	%fd2421, %fd4808, %fd4805;
	mul.f64 	%fd2422, %fd2421, %fd2421;
	sub.f64 	%fd2423, %fd4807, %fd4804;
	fma.rn.f64 	%fd2424, %fd2423, %fd2423, %fd2422;
	sub.f64 	%fd2425, %fd4806, %fd4803;
	fma.rn.f64 	%fd2426, %fd2425, %fd2425, %fd2424;
	mul.f64 	%fd2427, %fd2426, 0d3F50624DE0000000;
	sub.f64 	%fd2428, %fd4814, %fd4811;
	mul.f64 	%fd2429, %fd2428, %fd2428;
	sub.f64 	%fd2430, %fd4813, %fd4810;
	fma.rn.f64 	%fd2431, %fd2430, %fd2430, %fd2429;
	sub.f64 	%fd2432, %fd4812, %fd4809;
	fma.rn.f64 	%fd2433, %fd2432, %fd2432, %fd2431;
	add.f64 	%fd2434, %fd4914, 0d0000000000000000;
	fma.rn.f64 	%fd2435, %fd2433, %fd2434, 0d0000000000000000;
	fma.rn.f64 	%fd2436, %fd2427, %fd2434, 0d0000000000000000;
	add.f64 	%fd2437, %fd2428, %fd2428;
	add.f64 	%fd2438, %fd2430, %fd2430;
	add.f64 	%fd2439, %fd2432, %fd2432;
	fma.rn.f64 	%fd607, %fd2437, %fd2436, 0d0000000000000000;
	fma.rn.f64 	%fd608, %fd2438, %fd2436, 0d0000000000000000;
	fma.rn.f64 	%fd609, %fd2439, %fd2436, 0d0000000000000000;
	fma.rn.f64 	%fd2440, %fd2435, 0d3F50624DE0000000, 0d0000000000000000;
	add.f64 	%fd2441, %fd2421, %fd2421;
	add.f64 	%fd2442, %fd2423, %fd2423;
	add.f64 	%fd2443, %fd2425, %fd2425;
	fma.rn.f64 	%fd610, %fd2441, %fd2440, 0d0000000000000000;
	fma.rn.f64 	%fd611, %fd2442, %fd2440, 0d0000000000000000;
	fma.rn.f64 	%fd612, %fd2443, %fd2440, 0d0000000000000000;
	setp.eq.s64 	%p164, %rd166, 0;
	@%p164 bra 	$L__BB9_215;

	cvt.s64.s32 	%rd381, %r1039;
	mul.lo.s64 	%rd382, %rd381, %rd76;
	add.s64 	%rd378, %rd166, %rd382;
	// begin inline asm
	{ atom.add.f64 %fd2444,[%rd378],%fd607; }

	// end inline asm
	add.s64 	%rd379, %rd378, 8;
	// begin inline asm
	{ atom.add.f64 %fd2446,[%rd379],%fd608; }

	// end inline asm
	add.s64 	%rd380, %rd378, 16;
	// begin inline asm
	{ atom.add.f64 %fd2448,[%rd380],%fd609; }

	// end inline asm
	bra.uni 	$L__BB9_217;

$L__BB9_215:
	setp.eq.s64 	%p165, %rd137, 0;
	@%p165 bra 	$L__BB9_217;

	cvt.s64.s32 	%rd386, %r1039;
	mul.lo.s64 	%rd387, %rd386, %rd66;
	add.s64 	%rd383, %rd137, %rd387;
	// begin inline asm
	{ atom.add.f64 %fd2450,[%rd383],%fd607; }

	// end inline asm
	add.s64 	%rd384, %rd383, 8;
	// begin inline asm
	{ atom.add.f64 %fd2452,[%rd384],%fd608; }

	// end inline asm
	add.s64 	%rd385, %rd383, 16;
	// begin inline asm
	{ atom.add.f64 %fd2454,[%rd385],%fd609; }

	// end inline asm

$L__BB9_217:
	mov.f64 	%fd2456, 0d0000000000000000;
	sub.f64 	%fd2457, %fd2456, %fd607;
	add.f64 	%fd613, %fd2457, 0d0000000000000000;
	sub.f64 	%fd2458, %fd2456, %fd608;
	add.f64 	%fd614, %fd2458, 0d0000000000000000;
	sub.f64 	%fd2459, %fd2456, %fd609;
	add.f64 	%fd615, %fd2459, 0d0000000000000000;
	@%p164 bra 	$L__BB9_219;

	cvt.s64.s32 	%rd391, %r1038;
	mul.lo.s64 	%rd392, %rd391, %rd76;
	add.s64 	%rd388, %rd166, %rd392;
	// begin inline asm
	{ atom.add.f64 %fd2460,[%rd388],%fd613; }

	// end inline asm
	add.s64 	%rd389, %rd388, 8;
	// begin inline asm
	{ atom.add.f64 %fd2462,[%rd389],%fd614; }

	// end inline asm
	add.s64 	%rd390, %rd388, 16;
	// begin inline asm
	{ atom.add.f64 %fd2464,[%rd390],%fd615; }

	// end inline asm
	bra.uni 	$L__BB9_221;

$L__BB9_219:
	setp.eq.s64 	%p167, %rd137, 0;
	@%p167 bra 	$L__BB9_221;

	cvt.s64.s32 	%rd396, %r1038;
	mul.lo.s64 	%rd397, %rd396, %rd66;
	add.s64 	%rd393, %rd137, %rd397;
	// begin inline asm
	{ atom.add.f64 %fd2466,[%rd393],%fd613; }

	// end inline asm
	add.s64 	%rd394, %rd393, 8;
	// begin inline asm
	{ atom.add.f64 %fd2468,[%rd394],%fd614; }

	// end inline asm
	add.s64 	%rd395, %rd393, 16;
	// begin inline asm
	{ atom.add.f64 %fd2470,[%rd395],%fd615; }

	// end inline asm

$L__BB9_221:
	@%p164 bra 	$L__BB9_223;

	cvt.s64.s32 	%rd401, %r1037;
	mul.lo.s64 	%rd402, %rd401, %rd76;
	add.s64 	%rd398, %rd166, %rd402;
	// begin inline asm
	{ atom.add.f64 %fd2472,[%rd398],%fd610; }

	// end inline asm
	add.s64 	%rd399, %rd398, 8;
	// begin inline asm
	{ atom.add.f64 %fd2474,[%rd399],%fd611; }

	// end inline asm
	add.s64 	%rd400, %rd398, 16;
	// begin inline asm
	{ atom.add.f64 %fd2476,[%rd400],%fd612; }

	// end inline asm
	bra.uni 	$L__BB9_225;

$L__BB9_223:
	setp.eq.s64 	%p169, %rd137, 0;
	@%p169 bra 	$L__BB9_225;

	cvt.s64.s32 	%rd406, %r1037;
	mul.lo.s64 	%rd407, %rd406, %rd66;
	add.s64 	%rd403, %rd137, %rd407;
	// begin inline asm
	{ atom.add.f64 %fd2478,[%rd403],%fd610; }

	// end inline asm
	add.s64 	%rd404, %rd403, 8;
	// begin inline asm
	{ atom.add.f64 %fd2480,[%rd404],%fd611; }

	// end inline asm
	add.s64 	%rd405, %rd403, 16;
	// begin inline asm
	{ atom.add.f64 %fd2482,[%rd405],%fd612; }

	// end inline asm

$L__BB9_225:
	mov.f64 	%fd2484, 0d0000000000000000;
	sub.f64 	%fd2485, %fd2484, %fd610;
	add.f64 	%fd616, %fd2485, 0d0000000000000000;
	sub.f64 	%fd2486, %fd2484, %fd611;
	add.f64 	%fd617, %fd2486, 0d0000000000000000;
	sub.f64 	%fd2487, %fd2484, %fd612;
	add.f64 	%fd618, %fd2487, 0d0000000000000000;
	@%p164 bra 	$L__BB9_227;

	cvt.s64.s32 	%rd411, %r1036;
	mul.lo.s64 	%rd412, %rd411, %rd76;
	add.s64 	%rd408, %rd166, %rd412;
	// begin inline asm
	{ atom.add.f64 %fd2488,[%rd408],%fd616; }

	// end inline asm
	add.s64 	%rd409, %rd408, 8;
	// begin inline asm
	{ atom.add.f64 %fd2490,[%rd409],%fd617; }

	// end inline asm
	add.s64 	%rd410, %rd408, 16;
	// begin inline asm
	{ atom.add.f64 %fd2492,[%rd410],%fd618; }

	// end inline asm
	bra.uni 	$L__BB9_229;

$L__BB9_227:
	setp.eq.s64 	%p171, %rd137, 0;
	@%p171 bra 	$L__BB9_229;

	cvt.s64.s32 	%rd416, %r1036;
	mul.lo.s64 	%rd417, %rd416, %rd66;
	add.s64 	%rd413, %rd137, %rd417;
	// begin inline asm
	{ atom.add.f64 %fd2494,[%rd413],%fd616; }

	// end inline asm
	add.s64 	%rd414, %rd413, 8;
	// begin inline asm
	{ atom.add.f64 %fd2496,[%rd414],%fd617; }

	// end inline asm
	add.s64 	%rd415, %rd413, 16;
	// begin inline asm
	{ atom.add.f64 %fd2498,[%rd415],%fd618; }

	// end inline asm

$L__BB9_229:
	add.f64 	%fd619, %fd4903, 0d0000000000000000;
	mov.f64 	%fd2501, 0d0000000000000000;
	sub.f64 	%fd2502, %fd2501, %fd4903;
	fma.rn.f64 	%fd2503, %fd4781, %fd2502, %fd583;
	fma.rn.f64 	%fd620, %fd4781, %fd2502, %fd2503;
	mul.f64 	%fd2504, %fd590, %fd590;
	fma.rn.f64 	%fd2505, %fd585, %fd585, %fd2504;
	fma.rn.f64 	%fd621, %fd587, %fd587, %fd2505;
	mul.f64 	%fd2506, %fd590, %fd589;
	fma.rn.f64 	%fd2507, %fd585, %fd586, %fd2506;
	fma.rn.f64 	%fd622, %fd587, %fd584, %fd2507;
	mul.f64 	%fd2508, %fd589, %fd589;
	fma.rn.f64 	%fd2509, %fd586, %fd586, %fd2508;
	fma.rn.f64 	%fd623, %fd584, %fd584, %fd2509;
	sub.f64 	%fd624, %fd4784, %fd4790;
	mul.f64 	%fd2510, %fd590, %fd624;
	sub.f64 	%fd625, %fd4783, %fd4789;
	fma.rn.f64 	%fd2511, %fd585, %fd625, %fd2510;
	sub.f64 	%fd626, %fd4782, %fd4788;
	fma.rn.f64 	%fd627, %fd587, %fd626, %fd2511;
	mul.f64 	%fd2512, %fd624, %fd589;
	fma.rn.f64 	%fd2513, %fd625, %fd586, %fd2512;
	fma.rn.f64 	%fd628, %fd626, %fd584, %fd2513;
	mul.f64 	%fd2514, %fd621, %fd623;
	mul.f64 	%fd2515, %fd622, %fd622;
	sub.f64 	%fd629, %fd2514, %fd2515;
	mul.f64 	%fd2516, %fd622, %fd628;
	mul.f64 	%fd2517, %fd627, %fd623;
	sub.f64 	%fd630, %fd2516, %fd2517;
	setp.le.f64 	%p172, %fd630, 0d0000000000000000;
	@%p172 bra 	$L__BB9_233;

	setp.ge.f64 	%p4, %fd630, %fd629;
	add.f64 	%fd631, %fd628, %fd622;
	@%p4 bra 	$L__BB9_232;

	sub.f64 	%fd4586, %fd4782, %fd4788;
	sub.f64 	%fd4585, %fd4784, %fd4790;
	sub.f64 	%fd4584, %fd4783, %fd4789;
	selp.f64 	%fd2519, %fd623, %fd629, %p4;
	mul.f64 	%fd2520, %fd621, %fd628;
	mul.f64 	%fd2521, %fd627, %fd622;
	sub.f64 	%fd2522, %fd2520, %fd2521;
	setp.gt.f64 	%p173, %fd2522, 0d0000000000000000;
	setp.lt.f64 	%p174, %fd2522, %fd2519;
	mul.f64 	%fd2523, %fd4584, %fd591;
	fma.rn.f64 	%fd2524, %fd4585, %fd588, %fd2523;
	fma.rn.f64 	%fd2525, %fd4586, %fd592, %fd2524;
	setp.eq.f64 	%p175, %fd2525, 0d0000000000000000;
	mul.f64 	%fd2526, %fd621, 0d3BC79CA100000000;
	mul.f64 	%fd2527, %fd2526, %fd623;
	setp.lt.f64 	%p176, %fd593, %fd2527;
	or.pred  	%p177, %p175, %p176;
	and.pred  	%p178, %p173, %p174;
	and.pred  	%p179, %p177, %p178;
	mul.f64 	%fd2528, %fd629, 0d3FE0000000000000;
	setp.lt.f64 	%p180, %fd630, %fd2528;
	selp.b32 	%r919, 2, 5, %p180;
	selp.f64 	%fd2529, %fd628, %fd631, %p180;
	selp.f64 	%fd4916, %fd623, %fd2519, %p179;
	selp.b32 	%r1056, %r919, 8, %p179;
	selp.f64 	%fd4917, %fd2529, %fd2522, %p179;

$L__BB9_232:
	selp.f64 	%fd4918, %fd623, %fd4916, %p4;
	selp.b32 	%r1057, 5, %r1056, %p4;
	selp.f64 	%fd4919, %fd631, %fd4917, %p4;

$L__BB9_233:
	selp.f64 	%fd640, %fd623, %fd4918, %p172;
	selp.b32 	%r1058, 2, %r1057, %p172;
	selp.f64 	%fd641, %fd628, %fd4919, %p172;
	setp.gtu.f64 	%p183, %fd641, 0d0000000000000000;
	@%p183 bra 	$L__BB9_237;
	bra.uni 	$L__BB9_234;

$L__BB9_237:
	setp.ltu.f64 	%p186, %fd641, %fd640;
	@%p186 bra 	$L__BB9_241;

	mov.f64 	%fd2531, 0d0000000000000000;
	sub.f64 	%fd2532, %fd2531, %fd627;
	add.f64 	%fd643, %fd2532, %fd622;
	setp.le.f64 	%p187, %fd643, 0d0000000000000000;
	mov.u32 	%r1058, 1;
	@%p187 bra 	$L__BB9_241;

	setp.ge.f64 	%p188, %fd643, %fd621;
	mov.u32 	%r1058, 4;
	@%p188 bra 	$L__BB9_241;

	mov.u32 	%r1058, 7;
	bra.uni 	$L__BB9_241;

$L__BB9_234:
	mov.f64 	%fd2530, 0d0000000000000000;
	sub.f64 	%fd642, %fd2530, %fd627;
	setp.le.f64 	%p184, %fd642, 0d0000000000000000;
	mov.u32 	%r1058, 0;
	@%p184 bra 	$L__BB9_241;

	setp.ge.f64 	%p185, %fd642, %fd621;
	mov.u32 	%r1058, 3;
	@%p185 bra 	$L__BB9_241;

	mov.u32 	%r1058, 6;

$L__BB9_241:
	setp.eq.s32 	%p189, %r1058, 0;
	@%p189 bra 	$L__BB9_257;

	setp.eq.s32 	%p190, %r1058, 1;
	@%p190 bra 	$L__BB9_256;
	bra.uni 	$L__BB9_243;

$L__BB9_256:
	sub.f64 	%fd2912, %fd4784, %fd380;
	add.f64 	%fd2913, %fd2912, %fd2912;
	sub.f64 	%fd2914, %fd4783, %fd379;
	add.f64 	%fd2915, %fd2914, %fd2914;
	sub.f64 	%fd2916, %fd4782, %fd378;
	add.f64 	%fd2917, %fd2916, %fd2916;
	fma.rn.f64 	%fd2918, %fd2913, %fd619, 0d0000000000000000;
	mov.f64 	%fd2919, 0d0000000000000000;
	fma.rn.f64 	%fd2920, %fd2915, %fd619, 0d0000000000000000;
	fma.rn.f64 	%fd2921, %fd2917, %fd619, 0d0000000000000000;
	sub.f64 	%fd2922, %fd2919, %fd601;
	add.f64 	%fd4922, %fd2918, %fd2922;
	sub.f64 	%fd2923, %fd2919, %fd602;
	add.f64 	%fd4921, %fd2920, %fd2923;
	sub.f64 	%fd2924, %fd2919, %fd603;
	add.f64 	%fd4920, %fd2921, %fd2924;
	sub.f64 	%fd4931, %fd604, %fd2918;
	sub.f64 	%fd4930, %fd605, %fd2920;
	sub.f64 	%fd4929, %fd606, %fd2921;
	sub.f64 	%fd4928, %fd2919, %fd604;
	sub.f64 	%fd4927, %fd2919, %fd605;
	sub.f64 	%fd4926, %fd2919, %fd606;
	mov.f64 	%fd4923, %fd603;
	mov.f64 	%fd4924, %fd602;
	mov.f64 	%fd4925, %fd601;
	bra.uni 	$L__BB9_258;

$L__BB9_257:
	sub.f64 	%fd4598, %fd4782, %fd4788;
	sub.f64 	%fd4597, %fd4784, %fd4790;
	sub.f64 	%fd4596, %fd4783, %fd4789;
	add.f64 	%fd2925, %fd4597, %fd4597;
	add.f64 	%fd2926, %fd4596, %fd4596;
	add.f64 	%fd2927, %fd4598, %fd4598;
	fma.rn.f64 	%fd2928, %fd2925, %fd619, 0d0000000000000000;
	mov.f64 	%fd2929, 0d0000000000000000;
	fma.rn.f64 	%fd2930, %fd2926, %fd619, 0d0000000000000000;
	fma.rn.f64 	%fd2931, %fd2927, %fd619, 0d0000000000000000;
	sub.f64 	%fd2932, %fd2929, %fd601;
	add.f64 	%fd4922, %fd2928, %fd2932;
	sub.f64 	%fd2933, %fd2929, %fd602;
	add.f64 	%fd4921, %fd2930, %fd2933;
	sub.f64 	%fd2934, %fd2929, %fd603;
	add.f64 	%fd4920, %fd2931, %fd2934;
	sub.f64 	%fd2935, %fd2929, %fd604;
	sub.f64 	%fd4928, %fd2935, %fd2928;
	sub.f64 	%fd2936, %fd2929, %fd605;
	sub.f64 	%fd4927, %fd2936, %fd2930;
	sub.f64 	%fd2937, %fd2929, %fd606;
	sub.f64 	%fd4926, %fd2937, %fd2931;
	mov.f64 	%fd4923, %fd603;
	mov.f64 	%fd4924, %fd602;
	mov.f64 	%fd4925, %fd601;
	mov.f64 	%fd4929, %fd606;
	mov.f64 	%fd4930, %fd605;
	mov.f64 	%fd4931, %fd604;
	bra.uni 	$L__BB9_258;

$L__BB9_243:
	setp.eq.s32 	%p191, %r1058, 2;
	@%p191 bra 	$L__BB9_255;
	bra.uni 	$L__BB9_244;

$L__BB9_255:
	sub.f64 	%fd2836, %fd4790, %fd4784;
	sub.f64 	%fd2837, %fd378, %fd4782;
	sub.f64 	%fd2838, %fd4789, %fd4783;
	mul.f64 	%fd2839, %fd2838, %fd2837;
	sub.f64 	%fd2840, %fd379, %fd4783;
	sub.f64 	%fd2841, %fd4788, %fd4782;
	mul.f64 	%fd2842, %fd2841, %fd2840;
	sub.f64 	%fd2843, %fd2839, %fd2842;
	sub.f64 	%fd2844, %fd380, %fd4784;
	mul.f64 	%fd2845, %fd2841, %fd2844;
	mul.f64 	%fd2846, %fd2836, %fd2837;
	sub.f64 	%fd2847, %fd2845, %fd2846;
	mul.f64 	%fd2848, %fd2836, %fd2840;
	mul.f64 	%fd2849, %fd2838, %fd2844;
	sub.f64 	%fd2850, %fd2848, %fd2849;
	mul.f64 	%fd2851, %fd2847, %fd2847;
	fma.rn.f64 	%fd2852, %fd2843, %fd2843, %fd2851;
	fma.rn.f64 	%fd2853, %fd2850, %fd2850, %fd2852;
	div.rn.f64 	%fd2854, %fd2853, %fd623;
	div.rn.f64 	%fd2855, %fd619, %fd623;
	add.f64 	%fd2856, %fd2855, 0d0000000000000000;
	mov.f64 	%fd2857, 0d0000000000000000;
	mul.f64 	%fd2858, %fd2854, %fd619;
	div.rn.f64 	%fd2859, %fd2858, %fd623;
	sub.f64 	%fd2860, %fd2857, %fd2859;
	add.f64 	%fd2861, %fd589, %fd589;
	add.f64 	%fd2862, %fd586, %fd586;
	add.f64 	%fd2863, %fd584, %fd584;
	fma.rn.f64 	%fd2864, %fd2861, %fd2860, 0d0000000000000000;
	fma.rn.f64 	%fd2865, %fd2862, %fd2860, 0d0000000000000000;
	fma.rn.f64 	%fd2866, %fd2863, %fd2860, 0d0000000000000000;
	add.f64 	%fd2867, %fd2843, %fd2843;
	add.f64 	%fd2868, %fd2847, %fd2847;
	add.f64 	%fd2869, %fd2850, %fd2850;
	fma.rn.f64 	%fd2870, %fd2867, %fd2856, 0d0000000000000000;
	fma.rn.f64 	%fd2871, %fd2868, %fd2856, 0d0000000000000000;
	fma.rn.f64 	%fd2872, %fd2869, %fd2856, 0d0000000000000000;
	mul.f64 	%fd2873, %fd2840, %fd2872;
	mul.f64 	%fd2874, %fd2837, %fd2871;
	sub.f64 	%fd2875, %fd2873, %fd2874;
	mul.f64 	%fd2876, %fd2837, %fd2870;
	mul.f64 	%fd2877, %fd2844, %fd2872;
	sub.f64 	%fd2878, %fd2876, %fd2877;
	mul.f64 	%fd2879, %fd2844, %fd2871;
	mul.f64 	%fd2880, %fd2840, %fd2870;
	sub.f64 	%fd2881, %fd2879, %fd2880;
	add.f64 	%fd2882, %fd2875, 0d0000000000000000;
	add.f64 	%fd2883, %fd2878, 0d0000000000000000;
	add.f64 	%fd2884, %fd2881, 0d0000000000000000;
	mul.f64 	%fd2885, %fd2838, %fd2872;
	mul.f64 	%fd2886, %fd2841, %fd2871;
	mul.f64 	%fd2887, %fd2841, %fd2870;
	mul.f64 	%fd2888, %fd2836, %fd2872;
	mul.f64 	%fd2889, %fd2836, %fd2871;
	mul.f64 	%fd2890, %fd2838, %fd2870;
	sub.f64 	%fd2891, %fd2886, %fd2885;
	add.f64 	%fd2892, %fd2891, 0d0000000000000000;
	sub.f64 	%fd2893, %fd2888, %fd2887;
	add.f64 	%fd2894, %fd2893, 0d0000000000000000;
	sub.f64 	%fd2895, %fd2890, %fd2889;
	add.f64 	%fd2896, %fd2895, 0d0000000000000000;
	add.f64 	%fd2897, %fd2864, %fd604;
	add.f64 	%fd2898, %fd2865, %fd605;
	add.f64 	%fd2899, %fd2866, %fd606;
	sub.f64 	%fd2900, %fd2857, %fd604;
	sub.f64 	%fd2901, %fd2900, %fd2864;
	sub.f64 	%fd2902, %fd2857, %fd605;
	sub.f64 	%fd2903, %fd2902, %fd2865;
	sub.f64 	%fd2904, %fd2857, %fd606;
	sub.f64 	%fd2905, %fd2904, %fd2866;
	add.f64 	%fd4931, %fd2892, %fd2897;
	add.f64 	%fd4930, %fd2894, %fd2898;
	add.f64 	%fd4929, %fd2896, %fd2899;
	sub.f64 	%fd2906, %fd2857, %fd601;
	sub.f64 	%fd2907, %fd2906, %fd2892;
	sub.f64 	%fd2908, %fd2857, %fd602;
	sub.f64 	%fd2909, %fd2908, %fd2894;
	sub.f64 	%fd2910, %fd2857, %fd603;
	sub.f64 	%fd2911, %fd2910, %fd2896;
	add.f64 	%fd4928, %fd2882, %fd2901;
	add.f64 	%fd4927, %fd2883, %fd2903;
	add.f64 	%fd4926, %fd2884, %fd2905;
	sub.f64 	%fd4922, %fd2907, %fd2882;
	sub.f64 	%fd4921, %fd2909, %fd2883;
	sub.f64 	%fd4920, %fd2911, %fd2884;
	mov.f64 	%fd4923, %fd603;
	mov.f64 	%fd4924, %fd602;
	mov.f64 	%fd4925, %fd601;
	bra.uni 	$L__BB9_258;

$L__BB9_244:
	setp.eq.s32 	%p192, %r1058, 3;
	@%p192 bra 	$L__BB9_254;
	bra.uni 	$L__BB9_245;

$L__BB9_254:
	sub.f64 	%fd2823, %fd374, %fd4790;
	add.f64 	%fd2824, %fd2823, %fd2823;
	sub.f64 	%fd2825, %fd373, %fd4789;
	add.f64 	%fd2826, %fd2825, %fd2825;
	sub.f64 	%fd2827, %fd372, %fd4788;
	add.f64 	%fd2828, %fd2827, %fd2827;
	fma.rn.f64 	%fd2829, %fd2824, %fd619, 0d0000000000000000;
	mov.f64 	%fd2830, 0d0000000000000000;
	fma.rn.f64 	%fd2831, %fd2826, %fd619, 0d0000000000000000;
	fma.rn.f64 	%fd2832, %fd2828, %fd619, 0d0000000000000000;
	add.f64 	%fd4925, %fd2829, %fd601;
	add.f64 	%fd4924, %fd2831, %fd602;
	add.f64 	%fd4923, %fd2832, %fd603;
	sub.f64 	%fd2833, %fd2830, %fd604;
	sub.f64 	%fd4928, %fd2833, %fd2829;
	sub.f64 	%fd2834, %fd2830, %fd605;
	sub.f64 	%fd4927, %fd2834, %fd2831;
	sub.f64 	%fd2835, %fd2830, %fd606;
	sub.f64 	%fd4926, %fd2835, %fd2832;
	sub.f64 	%fd4922, %fd2830, %fd601;
	sub.f64 	%fd4921, %fd2830, %fd602;
	sub.f64 	%fd4920, %fd2830, %fd603;
	mov.f64 	%fd4929, %fd606;
	mov.f64 	%fd4930, %fd605;
	mov.f64 	%fd4931, %fd604;
	bra.uni 	$L__BB9_258;

$L__BB9_245:
	setp.eq.s32 	%p193, %r1058, 4;
	@%p193 bra 	$L__BB9_253;
	bra.uni 	$L__BB9_246;

$L__BB9_253:
	sub.f64 	%fd2813, %fd374, %fd380;
	add.f64 	%fd2814, %fd2813, %fd2813;
	sub.f64 	%fd2815, %fd373, %fd379;
	add.f64 	%fd2816, %fd2815, %fd2815;
	sub.f64 	%fd2817, %fd372, %fd378;
	add.f64 	%fd2818, %fd2817, %fd2817;
	fma.rn.f64 	%fd2819, %fd2814, %fd619, 0d0000000000000000;
	mov.f64 	%fd2820, 0d0000000000000000;
	fma.rn.f64 	%fd2821, %fd2816, %fd619, 0d0000000000000000;
	fma.rn.f64 	%fd2822, %fd2818, %fd619, 0d0000000000000000;
	add.f64 	%fd4925, %fd2819, %fd601;
	add.f64 	%fd4924, %fd2821, %fd602;
	add.f64 	%fd4923, %fd2822, %fd603;
	sub.f64 	%fd4931, %fd604, %fd2819;
	sub.f64 	%fd4930, %fd605, %fd2821;
	sub.f64 	%fd4929, %fd606, %fd2822;
	sub.f64 	%fd4922, %fd2820, %fd601;
	sub.f64 	%fd4921, %fd2820, %fd602;
	sub.f64 	%fd4920, %fd2820, %fd603;
	sub.f64 	%fd4928, %fd2820, %fd604;
	sub.f64 	%fd4927, %fd2820, %fd605;
	sub.f64 	%fd4926, %fd2820, %fd606;
	bra.uni 	$L__BB9_258;

$L__BB9_246:
	setp.eq.s32 	%p194, %r1058, 5;
	@%p194 bra 	$L__BB9_252;
	bra.uni 	$L__BB9_247;

$L__BB9_252:
	sub.f64 	%fd2740, %fd4790, %fd374;
	sub.f64 	%fd2741, %fd378, %fd372;
	sub.f64 	%fd2742, %fd4789, %fd373;
	mul.f64 	%fd2743, %fd2742, %fd2741;
	sub.f64 	%fd2744, %fd379, %fd373;
	sub.f64 	%fd2745, %fd4788, %fd372;
	mul.f64 	%fd2746, %fd2745, %fd2744;
	sub.f64 	%fd2747, %fd2743, %fd2746;
	sub.f64 	%fd2748, %fd380, %fd374;
	mul.f64 	%fd2749, %fd2745, %fd2748;
	mul.f64 	%fd2750, %fd2740, %fd2741;
	sub.f64 	%fd2751, %fd2749, %fd2750;
	mul.f64 	%fd2752, %fd2740, %fd2744;
	mul.f64 	%fd2753, %fd2742, %fd2748;
	sub.f64 	%fd2754, %fd2752, %fd2753;
	mul.f64 	%fd2755, %fd2751, %fd2751;
	fma.rn.f64 	%fd2756, %fd2747, %fd2747, %fd2755;
	fma.rn.f64 	%fd2757, %fd2754, %fd2754, %fd2756;
	div.rn.f64 	%fd2758, %fd2757, %fd623;
	div.rn.f64 	%fd2759, %fd619, %fd623;
	add.f64 	%fd2760, %fd2759, 0d0000000000000000;
	mov.f64 	%fd2761, 0d0000000000000000;
	mul.f64 	%fd2762, %fd2758, %fd619;
	div.rn.f64 	%fd2763, %fd2762, %fd623;
	sub.f64 	%fd2764, %fd2761, %fd2763;
	add.f64 	%fd2765, %fd589, %fd589;
	add.f64 	%fd2766, %fd586, %fd586;
	add.f64 	%fd2767, %fd584, %fd584;
	fma.rn.f64 	%fd2768, %fd2765, %fd2764, 0d0000000000000000;
	fma.rn.f64 	%fd2769, %fd2766, %fd2764, 0d0000000000000000;
	fma.rn.f64 	%fd2770, %fd2767, %fd2764, 0d0000000000000000;
	add.f64 	%fd2771, %fd2747, %fd2747;
	add.f64 	%fd2772, %fd2751, %fd2751;
	add.f64 	%fd2773, %fd2754, %fd2754;
	fma.rn.f64 	%fd2774, %fd2771, %fd2760, 0d0000000000000000;
	fma.rn.f64 	%fd2775, %fd2772, %fd2760, 0d0000000000000000;
	fma.rn.f64 	%fd2776, %fd2773, %fd2760, 0d0000000000000000;
	mul.f64 	%fd2777, %fd2744, %fd2776;
	mul.f64 	%fd2778, %fd2741, %fd2775;
	sub.f64 	%fd2779, %fd2777, %fd2778;
	mul.f64 	%fd2780, %fd2741, %fd2774;
	mul.f64 	%fd2781, %fd2748, %fd2776;
	sub.f64 	%fd2782, %fd2780, %fd2781;
	mul.f64 	%fd2783, %fd2748, %fd2775;
	mul.f64 	%fd2784, %fd2744, %fd2774;
	sub.f64 	%fd2785, %fd2783, %fd2784;
	add.f64 	%fd2786, %fd2779, 0d0000000000000000;
	add.f64 	%fd2787, %fd2782, 0d0000000000000000;
	add.f64 	%fd2788, %fd2785, 0d0000000000000000;
	mul.f64 	%fd2789, %fd2742, %fd2776;
	mul.f64 	%fd2790, %fd2745, %fd2775;
	mul.f64 	%fd2791, %fd2745, %fd2774;
	mul.f64 	%fd2792, %fd2740, %fd2776;
	mul.f64 	%fd2793, %fd2740, %fd2775;
	mul.f64 	%fd2794, %fd2742, %fd2774;
	sub.f64 	%fd2795, %fd2790, %fd2789;
	add.f64 	%fd2796, %fd2795, 0d0000000000000000;
	sub.f64 	%fd2797, %fd2792, %fd2791;
	add.f64 	%fd2798, %fd2797, 0d0000000000000000;
	sub.f64 	%fd2799, %fd2794, %fd2793;
	add.f64 	%fd2800, %fd2799, 0d0000000000000000;
	add.f64 	%fd2801, %fd2768, %fd604;
	add.f64 	%fd2802, %fd2769, %fd605;
	add.f64 	%fd2803, %fd2770, %fd606;
	sub.f64 	%fd2804, %fd2761, %fd604;
	sub.f64 	%fd2805, %fd2804, %fd2768;
	sub.f64 	%fd2806, %fd2761, %fd605;
	sub.f64 	%fd2807, %fd2806, %fd2769;
	sub.f64 	%fd2808, %fd2761, %fd606;
	sub.f64 	%fd2809, %fd2808, %fd2770;
	add.f64 	%fd4931, %fd2796, %fd2801;
	add.f64 	%fd4930, %fd2798, %fd2802;
	add.f64 	%fd4929, %fd2800, %fd2803;
	sub.f64 	%fd2810, %fd601, %fd2796;
	sub.f64 	%fd2811, %fd602, %fd2798;
	sub.f64 	%fd2812, %fd603, %fd2800;
	add.f64 	%fd4928, %fd2786, %fd2805;
	add.f64 	%fd4927, %fd2787, %fd2807;
	add.f64 	%fd4926, %fd2788, %fd2809;
	sub.f64 	%fd4925, %fd2810, %fd2786;
	sub.f64 	%fd4924, %fd2811, %fd2787;
	sub.f64 	%fd4923, %fd2812, %fd2788;
	sub.f64 	%fd4922, %fd2761, %fd601;
	sub.f64 	%fd4921, %fd2761, %fd602;
	sub.f64 	%fd4920, %fd2761, %fd603;
	bra.uni 	$L__BB9_258;

$L__BB9_247:
	setp.eq.s32 	%p195, %r1058, 6;
	@%p195 bra 	$L__BB9_251;
	bra.uni 	$L__BB9_248;

$L__BB9_251:
	sub.f64 	%fd4595, %fd4782, %fd4788;
	sub.f64 	%fd4594, %fd4784, %fd4790;
	sub.f64 	%fd4593, %fd4783, %fd4789;
	sub.f64 	%fd2667, %fd374, %fd4790;
	sub.f64 	%fd2668, %fd372, %fd4788;
	mul.f64 	%fd2669, %fd2668, %fd4593;
	sub.f64 	%fd2670, %fd373, %fd4789;
	mul.f64 	%fd2671, %fd4595, %fd2670;
	sub.f64 	%fd2672, %fd2669, %fd2671;
	mul.f64 	%fd2673, %fd4595, %fd2667;
	mul.f64 	%fd2674, %fd2668, %fd4594;
	sub.f64 	%fd2675, %fd2673, %fd2674;
	mul.f64 	%fd2676, %fd2670, %fd4594;
	mul.f64 	%fd2677, %fd4593, %fd2667;
	sub.f64 	%fd2678, %fd2676, %fd2677;
	mul.f64 	%fd2679, %fd2675, %fd2675;
	fma.rn.f64 	%fd2680, %fd2672, %fd2672, %fd2679;
	fma.rn.f64 	%fd2681, %fd2678, %fd2678, %fd2680;
	div.rn.f64 	%fd2682, %fd2681, %fd621;
	div.rn.f64 	%fd2683, %fd619, %fd621;
	add.f64 	%fd2684, %fd2683, 0d0000000000000000;
	mov.f64 	%fd2685, 0d0000000000000000;
	mul.f64 	%fd2686, %fd2682, %fd619;
	div.rn.f64 	%fd2687, %fd2686, %fd621;
	sub.f64 	%fd2688, %fd2685, %fd2687;
	add.f64 	%fd2689, %fd590, %fd590;
	add.f64 	%fd2690, %fd585, %fd585;
	add.f64 	%fd2691, %fd587, %fd587;
	fma.rn.f64 	%fd2692, %fd2689, %fd2688, 0d0000000000000000;
	fma.rn.f64 	%fd2693, %fd2690, %fd2688, 0d0000000000000000;
	fma.rn.f64 	%fd2694, %fd2691, %fd2688, 0d0000000000000000;
	add.f64 	%fd2695, %fd2672, %fd2672;
	add.f64 	%fd2696, %fd2675, %fd2675;
	add.f64 	%fd2697, %fd2678, %fd2678;
	fma.rn.f64 	%fd2698, %fd2695, %fd2684, 0d0000000000000000;
	fma.rn.f64 	%fd2699, %fd2696, %fd2684, 0d0000000000000000;
	fma.rn.f64 	%fd2700, %fd2697, %fd2684, 0d0000000000000000;
	mul.f64 	%fd2701, %fd2670, %fd2700;
	mul.f64 	%fd2702, %fd2668, %fd2699;
	sub.f64 	%fd2703, %fd2701, %fd2702;
	mul.f64 	%fd2704, %fd2668, %fd2698;
	mul.f64 	%fd2705, %fd2667, %fd2700;
	sub.f64 	%fd2706, %fd2704, %fd2705;
	mul.f64 	%fd2707, %fd2667, %fd2699;
	mul.f64 	%fd2708, %fd2670, %fd2698;
	sub.f64 	%fd2709, %fd2707, %fd2708;
	add.f64 	%fd2710, %fd2703, 0d0000000000000000;
	add.f64 	%fd2711, %fd2706, 0d0000000000000000;
	add.f64 	%fd2712, %fd2709, 0d0000000000000000;
	mul.f64 	%fd2713, %fd4593, %fd2700;
	mul.f64 	%fd2714, %fd4595, %fd2699;
	mul.f64 	%fd2715, %fd4595, %fd2698;
	mul.f64 	%fd2716, %fd4594, %fd2700;
	mul.f64 	%fd2717, %fd4594, %fd2699;
	mul.f64 	%fd2718, %fd4593, %fd2698;
	sub.f64 	%fd2719, %fd2714, %fd2713;
	add.f64 	%fd2720, %fd2719, 0d0000000000000000;
	sub.f64 	%fd2721, %fd2716, %fd2715;
	add.f64 	%fd2722, %fd2721, 0d0000000000000000;
	sub.f64 	%fd2723, %fd2718, %fd2717;
	add.f64 	%fd2724, %fd2723, 0d0000000000000000;
	add.f64 	%fd2725, %fd2692, %fd601;
	add.f64 	%fd2726, %fd2693, %fd602;
	add.f64 	%fd2727, %fd2694, %fd603;
	sub.f64 	%fd2728, %fd2685, %fd601;
	sub.f64 	%fd2729, %fd2728, %fd2692;
	sub.f64 	%fd2730, %fd2685, %fd602;
	sub.f64 	%fd2731, %fd2730, %fd2693;
	sub.f64 	%fd2732, %fd2685, %fd603;
	sub.f64 	%fd2733, %fd2732, %fd2694;
	add.f64 	%fd4925, %fd2720, %fd2725;
	add.f64 	%fd4924, %fd2722, %fd2726;
	add.f64 	%fd4923, %fd2724, %fd2727;
	sub.f64 	%fd2734, %fd2685, %fd604;
	sub.f64 	%fd2735, %fd2734, %fd2720;
	sub.f64 	%fd2736, %fd2685, %fd605;
	sub.f64 	%fd2737, %fd2736, %fd2722;
	sub.f64 	%fd2738, %fd2685, %fd606;
	sub.f64 	%fd2739, %fd2738, %fd2724;
	add.f64 	%fd4922, %fd2710, %fd2729;
	add.f64 	%fd4921, %fd2711, %fd2731;
	add.f64 	%fd4920, %fd2712, %fd2733;
	sub.f64 	%fd4928, %fd2735, %fd2710;
	sub.f64 	%fd4927, %fd2737, %fd2711;
	sub.f64 	%fd4926, %fd2739, %fd2712;
	mov.f64 	%fd4929, %fd606;
	mov.f64 	%fd4930, %fd605;
	mov.f64 	%fd4931, %fd604;
	bra.uni 	$L__BB9_258;

$L__BB9_248:
	setp.eq.s32 	%p196, %r1058, 7;
	@%p196 bra 	$L__BB9_250;
	bra.uni 	$L__BB9_249;

$L__BB9_250:
	sub.f64 	%fd2594, %fd4784, %fd380;
	sub.f64 	%fd2595, %fd372, %fd378;
	sub.f64 	%fd2596, %fd4783, %fd379;
	mul.f64 	%fd2597, %fd2595, %fd2596;
	sub.f64 	%fd2598, %fd373, %fd379;
	sub.f64 	%fd2599, %fd4782, %fd378;
	mul.f64 	%fd2600, %fd2599, %fd2598;
	sub.f64 	%fd2601, %fd2597, %fd2600;
	sub.f64 	%fd2602, %fd374, %fd380;
	mul.f64 	%fd2603, %fd2599, %fd2602;
	mul.f64 	%fd2604, %fd2595, %fd2594;
	sub.f64 	%fd2605, %fd2603, %fd2604;
	mul.f64 	%fd2606, %fd2598, %fd2594;
	mul.f64 	%fd2607, %fd2596, %fd2602;
	sub.f64 	%fd2608, %fd2606, %fd2607;
	mul.f64 	%fd2609, %fd2605, %fd2605;
	fma.rn.f64 	%fd2610, %fd2601, %fd2601, %fd2609;
	fma.rn.f64 	%fd2611, %fd2608, %fd2608, %fd2610;
	div.rn.f64 	%fd2612, %fd2611, %fd621;
	div.rn.f64 	%fd2613, %fd619, %fd621;
	add.f64 	%fd2614, %fd2613, 0d0000000000000000;
	mov.f64 	%fd2615, 0d0000000000000000;
	mul.f64 	%fd2616, %fd2612, %fd619;
	div.rn.f64 	%fd2617, %fd2616, %fd621;
	sub.f64 	%fd2618, %fd2615, %fd2617;
	add.f64 	%fd2619, %fd590, %fd590;
	add.f64 	%fd2620, %fd585, %fd585;
	add.f64 	%fd2621, %fd587, %fd587;
	fma.rn.f64 	%fd2622, %fd2619, %fd2618, 0d0000000000000000;
	fma.rn.f64 	%fd2623, %fd2620, %fd2618, 0d0000000000000000;
	fma.rn.f64 	%fd2624, %fd2621, %fd2618, 0d0000000000000000;
	add.f64 	%fd2625, %fd2601, %fd2601;
	add.f64 	%fd2626, %fd2605, %fd2605;
	add.f64 	%fd2627, %fd2608, %fd2608;
	fma.rn.f64 	%fd2628, %fd2625, %fd2614, 0d0000000000000000;
	fma.rn.f64 	%fd2629, %fd2626, %fd2614, 0d0000000000000000;
	fma.rn.f64 	%fd2630, %fd2627, %fd2614, 0d0000000000000000;
	mul.f64 	%fd2631, %fd2598, %fd2630;
	mul.f64 	%fd2632, %fd2595, %fd2629;
	sub.f64 	%fd2633, %fd2631, %fd2632;
	mul.f64 	%fd2634, %fd2595, %fd2628;
	mul.f64 	%fd2635, %fd2602, %fd2630;
	sub.f64 	%fd2636, %fd2634, %fd2635;
	mul.f64 	%fd2637, %fd2602, %fd2629;
	mul.f64 	%fd2638, %fd2598, %fd2628;
	sub.f64 	%fd2639, %fd2637, %fd2638;
	add.f64 	%fd2640, %fd2633, 0d0000000000000000;
	add.f64 	%fd2641, %fd2636, 0d0000000000000000;
	add.f64 	%fd2642, %fd2639, 0d0000000000000000;
	mul.f64 	%fd2643, %fd2596, %fd2630;
	mul.f64 	%fd2644, %fd2599, %fd2629;
	mul.f64 	%fd2645, %fd2599, %fd2628;
	mul.f64 	%fd2646, %fd2594, %fd2630;
	mul.f64 	%fd2647, %fd2594, %fd2629;
	mul.f64 	%fd2648, %fd2596, %fd2628;
	sub.f64 	%fd2649, %fd2644, %fd2643;
	add.f64 	%fd2650, %fd2649, 0d0000000000000000;
	sub.f64 	%fd2651, %fd2646, %fd2645;
	add.f64 	%fd2652, %fd2651, 0d0000000000000000;
	sub.f64 	%fd2653, %fd2648, %fd2647;
	add.f64 	%fd2654, %fd2653, 0d0000000000000000;
	add.f64 	%fd2655, %fd2622, %fd601;
	add.f64 	%fd2656, %fd2623, %fd602;
	add.f64 	%fd2657, %fd2624, %fd603;
	sub.f64 	%fd2658, %fd2615, %fd601;
	sub.f64 	%fd2659, %fd2658, %fd2622;
	sub.f64 	%fd2660, %fd2615, %fd602;
	sub.f64 	%fd2661, %fd2660, %fd2623;
	sub.f64 	%fd2662, %fd2615, %fd603;
	sub.f64 	%fd2663, %fd2662, %fd2624;
	add.f64 	%fd4925, %fd2650, %fd2655;
	add.f64 	%fd4924, %fd2652, %fd2656;
	add.f64 	%fd4923, %fd2654, %fd2657;
	sub.f64 	%fd2664, %fd604, %fd2650;
	sub.f64 	%fd2665, %fd605, %fd2652;
	sub.f64 	%fd2666, %fd606, %fd2654;
	add.f64 	%fd4922, %fd2640, %fd2659;
	add.f64 	%fd4921, %fd2641, %fd2661;
	add.f64 	%fd4920, %fd2642, %fd2663;
	sub.f64 	%fd4931, %fd2664, %fd2640;
	sub.f64 	%fd4930, %fd2665, %fd2641;
	sub.f64 	%fd4929, %fd2666, %fd2642;
	sub.f64 	%fd4928, %fd2615, %fd604;
	sub.f64 	%fd4927, %fd2615, %fd605;
	sub.f64 	%fd4926, %fd2615, %fd606;
	bra.uni 	$L__BB9_258;

$L__BB9_249:
	sub.f64 	%fd2533, %fd4790, %fd4784;
	sub.f64 	%fd2534, %fd4789, %fd4783;
	mul.f64 	%fd2535, %fd2534, %fd591;
	fma.rn.f64 	%fd2536, %fd2533, %fd588, %fd2535;
	sub.f64 	%fd2537, %fd4788, %fd4782;
	fma.rn.f64 	%fd2538, %fd2537, %fd592, %fd2536;
	mul.f64 	%fd2539, %fd2538, %fd2538;
	div.rn.f64 	%fd2540, %fd2539, %fd593;
	div.rn.f64 	%fd2541, %fd619, %fd593;
	add.f64 	%fd2542, %fd2541, 0d0000000000000000;
	mov.f64 	%fd2543, 0d0000000000000000;
	mul.f64 	%fd2544, %fd2540, %fd619;
	div.rn.f64 	%fd2545, %fd2544, %fd593;
	sub.f64 	%fd2546, %fd2543, %fd2545;
	fma.rn.f64 	%fd2547, %fd598, %fd2546, 0d0000000000000000;
	fma.rn.f64 	%fd2548, %fd599, %fd2546, 0d0000000000000000;
	fma.rn.f64 	%fd2549, %fd600, %fd2546, 0d0000000000000000;
	fma.rn.f64 	%fd2550, %fd2538, %fd2542, 0d0000000000000000;
	fma.rn.f64 	%fd2551, %fd2538, %fd2542, %fd2550;
	fma.rn.f64 	%fd2552, %fd588, %fd2551, 0d0000000000000000;
	fma.rn.f64 	%fd2553, %fd591, %fd2551, 0d0000000000000000;
	fma.rn.f64 	%fd2554, %fd592, %fd2551, 0d0000000000000000;
	fma.rn.f64 	%fd2555, %fd2533, %fd2551, %fd2547;
	fma.rn.f64 	%fd2556, %fd2534, %fd2551, %fd2548;
	fma.rn.f64 	%fd2557, %fd2537, %fd2551, %fd2549;
	mul.f64 	%fd2558, %fd586, %fd2557;
	mul.f64 	%fd2559, %fd584, %fd2556;
	sub.f64 	%fd2560, %fd2558, %fd2559;
	mul.f64 	%fd2561, %fd584, %fd2555;
	mul.f64 	%fd2562, %fd589, %fd2557;
	sub.f64 	%fd2563, %fd2561, %fd2562;
	mul.f64 	%fd2564, %fd589, %fd2556;
	mul.f64 	%fd2565, %fd586, %fd2555;
	sub.f64 	%fd2566, %fd2564, %fd2565;
	add.f64 	%fd2567, %fd2560, 0d0000000000000000;
	add.f64 	%fd2568, %fd2563, 0d0000000000000000;
	add.f64 	%fd2569, %fd2566, 0d0000000000000000;
	mul.f64 	%fd2570, %fd585, %fd2557;
	mul.f64 	%fd2571, %fd587, %fd2556;
	mul.f64 	%fd2572, %fd587, %fd2555;
	mul.f64 	%fd2573, %fd590, %fd2557;
	mul.f64 	%fd2574, %fd590, %fd2556;
	mul.f64 	%fd2575, %fd585, %fd2555;
	sub.f64 	%fd2576, %fd2571, %fd2570;
	add.f64 	%fd2577, %fd2576, 0d0000000000000000;
	sub.f64 	%fd2578, %fd2573, %fd2572;
	add.f64 	%fd2579, %fd2578, 0d0000000000000000;
	sub.f64 	%fd2580, %fd2575, %fd2574;
	add.f64 	%fd2581, %fd2580, 0d0000000000000000;
	sub.f64 	%fd2582, %fd2543, %fd604;
	add.f64 	%fd2583, %fd2552, %fd2582;
	sub.f64 	%fd2584, %fd2543, %fd605;
	add.f64 	%fd2585, %fd2553, %fd2584;
	sub.f64 	%fd2586, %fd2543, %fd606;
	add.f64 	%fd2587, %fd2554, %fd2586;
	sub.f64 	%fd2588, %fd2543, %fd601;
	sub.f64 	%fd2589, %fd2588, %fd2552;
	sub.f64 	%fd2590, %fd2543, %fd602;
	sub.f64 	%fd2591, %fd2590, %fd2553;
	sub.f64 	%fd2592, %fd2543, %fd603;
	sub.f64 	%fd2593, %fd2592, %fd2554;
	add.f64 	%fd4931, %fd2577, %fd604;
	add.f64 	%fd4930, %fd2579, %fd605;
	add.f64 	%fd4929, %fd2581, %fd606;
	sub.f64 	%fd4928, %fd2583, %fd2577;
	sub.f64 	%fd4927, %fd2585, %fd2579;
	sub.f64 	%fd4926, %fd2587, %fd2581;
	add.f64 	%fd4925, %fd2567, %fd601;
	add.f64 	%fd4924, %fd2568, %fd602;
	add.f64 	%fd4923, %fd2569, %fd603;
	sub.f64 	%fd4922, %fd2589, %fd2567;
	sub.f64 	%fd4921, %fd2591, %fd2568;
	sub.f64 	%fd4920, %fd2593, %fd2569;

$L__BB9_258:
	mov.f64 	%fd2942, 0d7FF8000000000000;
	add.f64 	%fd4935, %fd2942, 0d0000000000000000;
	mov.f64 	%fd4932, 0d0000000000000000;
	mov.f64 	%fd4933, 0d0000000000000000;
	mov.f64 	%fd4950, 0d0000000000000000;
	mov.f64 	%fd4934, %fd4950;
	@%p172 bra 	$L__BB9_261;

	setp.ge.f64 	%p198, %fd630, %fd629;
	mov.u16 	%rs308, 1;
	@%p198 bra 	$L__BB9_261;

	mul.f64 	%fd2948, %fd621, 0d3BC79CA100000000;
	fma.rn.f64 	%fd4935, %fd2948, 0d0000000000000000, 0d0000000000000000;
	mov.u16 	%rs308, 0;
	mov.f64 	%fd4932, %fd588;
	mov.f64 	%fd4933, %fd591;
	mov.f64 	%fd4934, %fd592;

$L__BB9_261:
	mov.f64 	%fd4951, %fd4950;
	mov.f64 	%fd4952, %fd4950;
	mov.f64 	%fd4953, %fd4950;
	mov.f64 	%fd4954, %fd4950;
	mov.f64 	%fd4955, %fd4950;
	mov.f64 	%fd4956, %fd4950;
	mov.f64 	%fd4957, %fd4950;
	mov.f64 	%fd4958, %fd4950;
	mov.f64 	%fd4959, %fd4950;
	mov.f64 	%fd4960, %fd4950;
	mov.f64 	%fd4961, %fd4950;
	mov.f64 	%fd4962, %fd4950;
	mov.f64 	%fd4963, %fd4950;
	@%p172 bra 	$L__BB9_266;

	and.b16  	%rs295, %rs308, 255;
	setp.ne.s16 	%p200, %rs295, 0;
	mov.f64 	%fd4950, 0d0000000000000000;
	mov.f64 	%fd4951, %fd4950;
	mov.f64 	%fd4952, %fd4950;
	mov.f64 	%fd4953, %fd4950;
	mov.f64 	%fd4954, %fd4950;
	mov.f64 	%fd4955, %fd4950;
	mov.f64 	%fd4956, %fd4950;
	mov.f64 	%fd4957, %fd4950;
	mov.f64 	%fd4958, %fd4950;
	mov.f64 	%fd4959, %fd4950;
	mov.f64 	%fd4960, %fd4950;
	mov.f64 	%fd4961, %fd4950;
	mov.f64 	%fd4962, %fd4950;
	mov.f64 	%fd4963, %fd4950;
	@%p200 bra 	$L__BB9_264;

	sub.f64 	%fd4589, %fd4782, %fd4788;
	sub.f64 	%fd4588, %fd4784, %fd4790;
	sub.f64 	%fd4587, %fd4783, %fd4789;
	fma.rn.f64 	%fd2977, %fd623, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2978, %fd2977, 0d3BC79CA100000000, 0d0000000000000000;
	add.f64 	%fd2979, %fd4932, %fd4932;
	add.f64 	%fd2980, %fd4933, %fd4933;
	add.f64 	%fd2981, %fd4934, %fd4934;
	fma.rn.f64 	%fd2982, %fd2979, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2983, %fd2980, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2984, %fd2981, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2985, %fd4588, 0d0000000000000000, %fd2982;
	fma.rn.f64 	%fd2986, %fd4587, 0d0000000000000000, %fd2983;
	fma.rn.f64 	%fd2987, %fd4589, 0d0000000000000000, %fd2984;
	fma.rn.f64 	%fd4955, %fd4932, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4956, %fd4933, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4957, %fd4934, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd2988, %fd586, %fd2987;
	mul.f64 	%fd2989, %fd584, %fd2986;
	sub.f64 	%fd2990, %fd2988, %fd2989;
	mul.f64 	%fd2991, %fd584, %fd2985;
	mul.f64 	%fd2992, %fd589, %fd2987;
	sub.f64 	%fd2993, %fd2991, %fd2992;
	mul.f64 	%fd2994, %fd589, %fd2986;
	mul.f64 	%fd2995, %fd586, %fd2985;
	sub.f64 	%fd2996, %fd2994, %fd2995;
	add.f64 	%fd4961, %fd2990, 0d0000000000000000;
	add.f64 	%fd4962, %fd2993, 0d0000000000000000;
	add.f64 	%fd4963, %fd2996, 0d0000000000000000;
	mul.f64 	%fd2997, %fd585, %fd2987;
	mul.f64 	%fd2998, %fd587, %fd2986;
	mul.f64 	%fd2999, %fd587, %fd2985;
	mul.f64 	%fd3000, %fd590, %fd2987;
	mul.f64 	%fd3001, %fd590, %fd2986;
	mul.f64 	%fd3002, %fd585, %fd2985;
	sub.f64 	%fd3003, %fd2998, %fd2997;
	add.f64 	%fd4958, %fd3003, 0d0000000000000000;
	sub.f64 	%fd3004, %fd3000, %fd2999;
	add.f64 	%fd4959, %fd3004, 0d0000000000000000;
	sub.f64 	%fd3005, %fd3002, %fd3001;
	add.f64 	%fd4960, %fd3005, 0d0000000000000000;
	fma.rn.f64 	%fd4953, %fd627, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4951, %fd622, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4954, %fd628, 0d0000000000000000, %fd2978;
	fma.rn.f64 	%fd4950, %fd621, 0d0000000000000000, 0d0000000000000000;
	mov.f64 	%fd4952, %fd4935;

$L__BB9_264:
	setp.eq.s16 	%p201, %rs295, 0;
	@%p201 bra 	$L__BB9_266;

	add.f64 	%fd4952, %fd4952, 0d0000000000000000;
	add.f64 	%fd4950, %fd4950, 0d0000000000000000;
	add.f64 	%fd4953, %fd4953, 0d0000000000000000;

$L__BB9_266:
	sub.f64 	%fd4592, %fd4782, %fd4788;
	sub.f64 	%fd4591, %fd4784, %fd4790;
	sub.f64 	%fd4590, %fd4783, %fd4789;
	add.f64 	%fd3006, %fd4950, 0d0000000000000000;
	mov.f64 	%fd3007, 0d0000000000000000;
	selp.f64 	%fd3008, %fd3006, %fd4950, %p172;
	add.f64 	%fd3009, %fd4952, 0d0000000000000000;
	selp.f64 	%fd3010, %fd3009, %fd4952, %p172;
	fma.rn.f64 	%fd3011, %fd627, 0d0000000000000000, %fd3010;
	fma.rn.f64 	%fd3012, %fd623, 0d0000000000000000, %fd4951;
	fma.rn.f64 	%fd3013, %fd628, 0d0000000000000000, %fd4953;
	fma.rn.f64 	%fd3014, %fd622, 0d0000000000000000, %fd3008;
	fma.rn.f64 	%fd3015, %fd622, 0d0000000000000000, %fd3013;
	fma.rn.f64 	%fd3016, %fd622, 0d0000000000000000, %fd3015;
	fma.rn.f64 	%fd3017, %fd623, 0d0000000000000000, %fd4954;
	fma.rn.f64 	%fd3018, %fd621, 0d0000000000000000, %fd3011;
	fma.rn.f64 	%fd3019, %fd4591, %fd3014, %fd4958;
	fma.rn.f64 	%fd3020, %fd4590, %fd3014, %fd4959;
	fma.rn.f64 	%fd3021, %fd4592, %fd3014, %fd4960;
	fma.rn.f64 	%fd3022, %fd589, %fd3014, %fd4955;
	fma.rn.f64 	%fd3023, %fd586, %fd3014, %fd4956;
	fma.rn.f64 	%fd3024, %fd584, %fd3014, %fd4957;
	fma.rn.f64 	%fd3025, %fd4591, %fd3012, %fd4961;
	fma.rn.f64 	%fd3026, %fd4590, %fd3012, %fd4962;
	fma.rn.f64 	%fd3027, %fd4592, %fd3012, %fd4963;
	fma.rn.f64 	%fd3028, %fd590, %fd3012, %fd3022;
	fma.rn.f64 	%fd3029, %fd585, %fd3012, %fd3023;
	fma.rn.f64 	%fd3030, %fd587, %fd3012, %fd3024;
	add.f64 	%fd3031, %fd589, %fd589;
	add.f64 	%fd3032, %fd586, %fd586;
	add.f64 	%fd3033, %fd584, %fd584;
	fma.rn.f64 	%fd3034, %fd3031, %fd3018, %fd3019;
	fma.rn.f64 	%fd3035, %fd3032, %fd3018, %fd3020;
	fma.rn.f64 	%fd3036, %fd3033, %fd3018, %fd3021;
	fma.rn.f64 	%fd3037, %fd589, %fd3016, %fd3025;
	fma.rn.f64 	%fd3038, %fd586, %fd3016, %fd3026;
	fma.rn.f64 	%fd3039, %fd584, %fd3016, %fd3027;
	fma.rn.f64 	%fd3040, %fd590, %fd3016, %fd3034;
	fma.rn.f64 	%fd3041, %fd585, %fd3016, %fd3035;
	fma.rn.f64 	%fd3042, %fd587, %fd3016, %fd3036;
	add.f64 	%fd3043, %fd590, %fd590;
	add.f64 	%fd3044, %fd585, %fd585;
	add.f64 	%fd3045, %fd587, %fd587;
	fma.rn.f64 	%fd3046, %fd3043, %fd3017, %fd3037;
	fma.rn.f64 	%fd3047, %fd3044, %fd3017, %fd3038;
	fma.rn.f64 	%fd3048, %fd3045, %fd3017, %fd3039;
	add.f64 	%fd3049, %fd4922, %fd3028;
	add.f64 	%fd3050, %fd4921, %fd3029;
	add.f64 	%fd3051, %fd4920, %fd3030;
	sub.f64 	%fd3052, %fd4928, %fd3028;
	sub.f64 	%fd3053, %fd4927, %fd3029;
	sub.f64 	%fd3054, %fd4926, %fd3030;
	add.f64 	%fd3055, %fd4931, %fd3040;
	add.f64 	%fd3056, %fd4930, %fd3041;
	add.f64 	%fd3057, %fd4929, %fd3042;
	sub.f64 	%fd3058, %fd3052, %fd3040;
	sub.f64 	%fd3059, %fd3053, %fd3041;
	sub.f64 	%fd3060, %fd3054, %fd3042;
	add.f64 	%fd3061, %fd4925, %fd3046;
	add.f64 	%fd3062, %fd4924, %fd3047;
	add.f64 	%fd3063, %fd4923, %fd3048;
	sub.f64 	%fd3064, %fd3049, %fd3046;
	sub.f64 	%fd3065, %fd3050, %fd3047;
	sub.f64 	%fd3066, %fd3051, %fd3048;
	add.f64 	%fd3067, %fd4878, 0d0000000000000000;
	sub.f64 	%fd3068, %fd3007, %fd3067;
	fma.rn.f64 	%fd3069, %fd4855, %fd3068, 0d0000000000000000;
	fma.rn.f64 	%fd795, %fd4795, %fd3068, 0d0000000000000000;
	fma.rn.f64 	%fd3070, %fd4854, %fd3068, 0d0000000000000000;
	fma.rn.f64 	%fd796, %fd4853, %fd3068, 0d0000000000000000;
	sub.f64 	%fd3071, %fd3069, %fd3070;
	fma.rn.f64 	%fd3072, %fd4852, %fd3067, 0d0000000000000000;
	fma.rn.f64 	%fd797, %fd4794, %fd3067, 0d0000000000000000;
	fma.rn.f64 	%fd3073, %fd4851, %fd3067, 0d0000000000000000;
	fma.rn.f64 	%fd798, %fd4850, %fd3067, 0d0000000000000000;
	sub.f64 	%fd3074, %fd3072, %fd3073;
	add.f64 	%fd3075, %fd4879, 0d0000000000000000;
	sub.f64 	%fd3076, %fd3007, %fd3075;
	fma.rn.f64 	%fd3077, %fd4849, %fd3076, %fd3071;
	fma.rn.f64 	%fd799, %fd4795, %fd3076, 0d0000000000000000;
	fma.rn.f64 	%fd3078, %fd4848, %fd3076, 0d0000000000000000;
	fma.rn.f64 	%fd800, %fd4853, %fd3076, 0d0000000000000000;
	sub.f64 	%fd3079, %fd3077, %fd3078;
	fma.rn.f64 	%fd3080, %fd4846, %fd3075, %fd3074;
	fma.rn.f64 	%fd801, %fd4794, %fd3075, 0d0000000000000000;
	fma.rn.f64 	%fd3081, %fd4845, %fd3075, 0d0000000000000000;
	fma.rn.f64 	%fd802, %fd4850, %fd3075, 0d0000000000000000;
	sub.f64 	%fd3082, %fd3080, %fd3081;
	add.f64 	%fd3083, %fd4880, 0d0000000000000000;
	sub.f64 	%fd3084, %fd3007, %fd3083;
	fma.rn.f64 	%fd3085, %fd4843, %fd3084, %fd3079;
	fma.rn.f64 	%fd803, %fd4795, %fd3084, 0d0000000000000000;
	fma.rn.f64 	%fd3086, %fd4842, %fd3084, 0d0000000000000000;
	fma.rn.f64 	%fd804, %fd4853, %fd3084, 0d0000000000000000;
	sub.f64 	%fd3087, %fd3085, %fd3086;
	fma.rn.f64 	%fd3088, %fd4840, %fd3083, %fd3082;
	fma.rn.f64 	%fd805, %fd4794, %fd3083, 0d0000000000000000;
	fma.rn.f64 	%fd3089, %fd4839, %fd3083, 0d0000000000000000;
	fma.rn.f64 	%fd806, %fd4850, %fd3083, 0d0000000000000000;
	sub.f64 	%fd3090, %fd3088, %fd3089;
	add.f64 	%fd3091, %fd4875, 0d0000000000000000;
	sub.f64 	%fd3092, %fd3007, %fd3091;
	fma.rn.f64 	%fd3093, %fd4837, %fd3092, %fd3087;
	fma.rn.f64 	%fd3094, %fd4795, %fd3092, 0d0000000000000000;
	add.f64 	%fd807, %fd3094, %fd3057;
	fma.rn.f64 	%fd3095, %fd4836, %fd3092, 0d0000000000000000;
	fma.rn.f64 	%fd3096, %fd4853, %fd3092, 0d0000000000000000;
	add.f64 	%fd808, %fd3096, %fd3060;
	sub.f64 	%fd3097, %fd3093, %fd3095;
	fma.rn.f64 	%fd3098, %fd4834, %fd3091, %fd3090;
	fma.rn.f64 	%fd3099, %fd4794, %fd3091, 0d0000000000000000;
	add.f64 	%fd809, %fd3099, %fd3063;
	fma.rn.f64 	%fd3100, %fd4833, %fd3091, 0d0000000000000000;
	fma.rn.f64 	%fd3101, %fd4850, %fd3091, 0d0000000000000000;
	add.f64 	%fd810, %fd3101, %fd3066;
	sub.f64 	%fd3102, %fd3098, %fd3100;
	add.f64 	%fd3103, %fd4876, 0d0000000000000000;
	sub.f64 	%fd3104, %fd3007, %fd3103;
	fma.rn.f64 	%fd3105, %fd4831, %fd3104, %fd3097;
	fma.rn.f64 	%fd3106, %fd4795, %fd3104, 0d0000000000000000;
	add.f64 	%fd811, %fd3106, %fd3056;
	fma.rn.f64 	%fd3107, %fd4830, %fd3104, 0d0000000000000000;
	fma.rn.f64 	%fd3108, %fd4853, %fd3104, 0d0000000000000000;
	add.f64 	%fd812, %fd3108, %fd3059;
	sub.f64 	%fd3109, %fd3105, %fd3107;
	fma.rn.f64 	%fd3110, %fd4828, %fd3103, %fd3102;
	fma.rn.f64 	%fd3111, %fd4794, %fd3103, 0d0000000000000000;
	add.f64 	%fd813, %fd3111, %fd3062;
	fma.rn.f64 	%fd3112, %fd4827, %fd3103, 0d0000000000000000;
	fma.rn.f64 	%fd3113, %fd4850, %fd3103, 0d0000000000000000;
	add.f64 	%fd814, %fd3113, %fd3065;
	sub.f64 	%fd3114, %fd3110, %fd3112;
	add.f64 	%fd3115, %fd4877, 0d0000000000000000;
	sub.f64 	%fd3116, %fd3007, %fd3115;
	fma.rn.f64 	%fd3117, %fd4825, %fd3116, %fd3109;
	fma.rn.f64 	%fd3118, %fd4795, %fd3116, 0d0000000000000000;
	add.f64 	%fd815, %fd3118, %fd3055;
	fma.rn.f64 	%fd3119, %fd4824, %fd3116, 0d0000000000000000;
	fma.rn.f64 	%fd3120, %fd4853, %fd3116, 0d0000000000000000;
	add.f64 	%fd816, %fd3120, %fd3058;
	sub.f64 	%fd3121, %fd3117, %fd3119;
	fma.rn.f64 	%fd3122, %fd4822, %fd3115, %fd3114;
	fma.rn.f64 	%fd3123, %fd4794, %fd3115, 0d0000000000000000;
	add.f64 	%fd817, %fd3123, %fd3061;
	fma.rn.f64 	%fd3124, %fd4821, %fd3115, 0d0000000000000000;
	fma.rn.f64 	%fd3125, %fd4850, %fd3115, 0d0000000000000000;
	add.f64 	%fd818, %fd3125, %fd3064;
	sub.f64 	%fd819, %fd3122, %fd3124;
	add.f64 	%fd820, %fd3121, 0d0000000000000000;
	setp.eq.s64 	%p203, %rd158, 0;
	@%p203 bra 	$L__BB9_268;

	mul.lo.s64 	%rd420, %rd96, %rd77;
	add.s64 	%rd418, %rd158, %rd420;
	// begin inline asm
	{ atom.add.f64 %fd3126,[%rd418],%fd3007; }

	// end inline asm
	add.s64 	%rd419, %rd418, 8;
	// begin inline asm
	{ atom.add.f64 %fd3128,[%rd419],%fd820; }

	// end inline asm
	bra.uni 	$L__BB9_270;

$L__BB9_268:
	setp.eq.s64 	%p204, %rd117, 0;
	@%p204 bra 	$L__BB9_270;

	mul.lo.s64 	%rd423, %rd96, %rd59;
	add.s64 	%rd421, %rd117, %rd423;
	mov.f64 	%fd3131, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd3130,[%rd421],%fd3131; }

	// end inline asm
	add.s64 	%rd422, %rd421, 8;
	// begin inline asm
	{ atom.add.f64 %fd3132,[%rd422],%fd820; }

	// end inline asm

$L__BB9_270:
	add.f64 	%fd821, %fd819, 0d0000000000000000;
	@%p203 bra 	$L__BB9_272;

	mul.lo.s64 	%rd426, %rd96, %rd77;
	add.s64 	%rd424, %rd158, %rd426;
	// begin inline asm
	{ atom.add.f64 %fd3134,[%rd424],%fd821; }

	// end inline asm
	add.s64 	%rd425, %rd424, 8;
	mov.f64 	%fd3137, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd3136,[%rd425],%fd3137; }

	// end inline asm
	bra.uni 	$L__BB9_274;

$L__BB9_272:
	setp.eq.s64 	%p206, %rd117, 0;
	@%p206 bra 	$L__BB9_274;

	mul.lo.s64 	%rd429, %rd96, %rd59;
	add.s64 	%rd427, %rd117, %rd429;
	// begin inline asm
	{ atom.add.f64 %fd3138,[%rd427],%fd821; }

	// end inline asm
	add.s64 	%rd428, %rd427, 8;
	mov.f64 	%fd3141, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd3140,[%rd428],%fd3141; }

	// end inline asm

$L__BB9_274:
	setp.eq.s64 	%p207, %rd164, 0;
	@%p207 bra 	$L__BB9_276;

	cvt.s64.s32 	%rd433, %r1039;
	mul.lo.s64 	%rd434, %rd433, %rd78;
	add.s64 	%rd430, %rd164, %rd434;
	// begin inline asm
	{ atom.add.f64 %fd3142,[%rd430],%fd803; }

	// end inline asm
	add.s64 	%rd431, %rd430, 8;
	// begin inline asm
	{ atom.add.f64 %fd3144,[%rd431],%fd799; }

	// end inline asm
	add.s64 	%rd432, %rd430, 16;
	// begin inline asm
	{ atom.add.f64 %fd3146,[%rd432],%fd795; }

	// end inline asm
	bra.uni 	$L__BB9_278;

$L__BB9_276:
	setp.eq.s64 	%p208, %rd135, 0;
	@%p208 bra 	$L__BB9_278;

	cvt.s64.s32 	%rd438, %r1039;
	mul.lo.s64 	%rd439, %rd438, %rd58;
	add.s64 	%rd435, %rd135, %rd439;
	// begin inline asm
	{ atom.add.f64 %fd3148,[%rd435],%fd803; }

	// end inline asm
	add.s64 	%rd436, %rd435, 8;
	// begin inline asm
	{ atom.add.f64 %fd3150,[%rd436],%fd799; }

	// end inline asm
	add.s64 	%rd437, %rd435, 16;
	// begin inline asm
	{ atom.add.f64 %fd3152,[%rd437],%fd795; }

	// end inline asm

$L__BB9_278:
	@%p207 bra 	$L__BB9_280;

	cvt.s64.s32 	%rd443, %r1038;
	mul.lo.s64 	%rd444, %rd443, %rd78;
	add.s64 	%rd440, %rd164, %rd444;
	// begin inline asm
	{ atom.add.f64 %fd3154,[%rd440],%fd804; }

	// end inline asm
	add.s64 	%rd441, %rd440, 8;
	// begin inline asm
	{ atom.add.f64 %fd3156,[%rd441],%fd800; }

	// end inline asm
	add.s64 	%rd442, %rd440, 16;
	// begin inline asm
	{ atom.add.f64 %fd3158,[%rd442],%fd796; }

	// end inline asm
	bra.uni 	$L__BB9_282;

$L__BB9_280:
	setp.eq.s64 	%p210, %rd135, 0;
	@%p210 bra 	$L__BB9_282;

	cvt.s64.s32 	%rd448, %r1038;
	mul.lo.s64 	%rd449, %rd448, %rd58;
	add.s64 	%rd445, %rd135, %rd449;
	// begin inline asm
	{ atom.add.f64 %fd3160,[%rd445],%fd804; }

	// end inline asm
	add.s64 	%rd446, %rd445, 8;
	// begin inline asm
	{ atom.add.f64 %fd3162,[%rd446],%fd800; }

	// end inline asm
	add.s64 	%rd447, %rd445, 16;
	// begin inline asm
	{ atom.add.f64 %fd3164,[%rd447],%fd796; }

	// end inline asm

$L__BB9_282:
	@%p207 bra 	$L__BB9_284;

	cvt.s64.s32 	%rd453, %r1037;
	mul.lo.s64 	%rd454, %rd453, %rd78;
	add.s64 	%rd450, %rd164, %rd454;
	// begin inline asm
	{ atom.add.f64 %fd3166,[%rd450],%fd805; }

	// end inline asm
	add.s64 	%rd451, %rd450, 8;
	// begin inline asm
	{ atom.add.f64 %fd3168,[%rd451],%fd801; }

	// end inline asm
	add.s64 	%rd452, %rd450, 16;
	// begin inline asm
	{ atom.add.f64 %fd3170,[%rd452],%fd797; }

	// end inline asm
	bra.uni 	$L__BB9_286;

$L__BB9_284:
	setp.eq.s64 	%p212, %rd135, 0;
	@%p212 bra 	$L__BB9_286;

	cvt.s64.s32 	%rd458, %r1037;
	mul.lo.s64 	%rd459, %rd458, %rd58;
	add.s64 	%rd455, %rd135, %rd459;
	// begin inline asm
	{ atom.add.f64 %fd3172,[%rd455],%fd805; }

	// end inline asm
	add.s64 	%rd456, %rd455, 8;
	// begin inline asm
	{ atom.add.f64 %fd3174,[%rd456],%fd801; }

	// end inline asm
	add.s64 	%rd457, %rd455, 16;
	// begin inline asm
	{ atom.add.f64 %fd3176,[%rd457],%fd797; }

	// end inline asm

$L__BB9_286:
	@%p207 bra 	$L__BB9_288;

	cvt.s64.s32 	%rd463, %r1036;
	mul.lo.s64 	%rd464, %rd463, %rd78;
	add.s64 	%rd460, %rd164, %rd464;
	// begin inline asm
	{ atom.add.f64 %fd3178,[%rd460],%fd806; }

	// end inline asm
	add.s64 	%rd461, %rd460, 8;
	// begin inline asm
	{ atom.add.f64 %fd3180,[%rd461],%fd802; }

	// end inline asm
	add.s64 	%rd462, %rd460, 16;
	// begin inline asm
	{ atom.add.f64 %fd3182,[%rd462],%fd798; }

	// end inline asm
	bra.uni 	$L__BB9_290;

$L__BB9_288:
	setp.eq.s64 	%p214, %rd135, 0;
	@%p214 bra 	$L__BB9_290;

	cvt.s64.s32 	%rd468, %r1036;
	mul.lo.s64 	%rd469, %rd468, %rd58;
	add.s64 	%rd465, %rd135, %rd469;
	// begin inline asm
	{ atom.add.f64 %fd3184,[%rd465],%fd806; }

	// end inline asm
	add.s64 	%rd466, %rd465, 8;
	// begin inline asm
	{ atom.add.f64 %fd3186,[%rd466],%fd802; }

	// end inline asm
	add.s64 	%rd467, %rd465, 16;
	// begin inline asm
	{ atom.add.f64 %fd3188,[%rd467],%fd798; }

	// end inline asm

$L__BB9_290:
	setp.eq.s64 	%p215, %rd162, 0;
	add.f64 	%fd822, %fd815, 0d0000000000000000;
	add.f64 	%fd823, %fd811, 0d0000000000000000;
	add.f64 	%fd824, %fd807, 0d0000000000000000;
	@%p215 bra 	$L__BB9_292;

	cvt.s64.s32 	%rd473, %r1039;
	mul.lo.s64 	%rd474, %rd473, %rd81;
	add.s64 	%rd470, %rd162, %rd474;
	// begin inline asm
	{ atom.add.f64 %fd3190,[%rd470],%fd822; }

	// end inline asm
	add.s64 	%rd471, %rd470, 8;
	// begin inline asm
	{ atom.add.f64 %fd3192,[%rd471],%fd823; }

	// end inline asm
	add.s64 	%rd472, %rd470, 16;
	// begin inline asm
	{ atom.add.f64 %fd3194,[%rd472],%fd824; }

	// end inline asm
	bra.uni 	$L__BB9_294;

$L__BB9_292:
	setp.eq.s64 	%p216, %rd131, 0;
	@%p216 bra 	$L__BB9_294;

	cvt.s64.s32 	%rd478, %r1039;
	mul.lo.s64 	%rd479, %rd478, %rd57;
	add.s64 	%rd475, %rd131, %rd479;
	// begin inline asm
	{ atom.add.f64 %fd3196,[%rd475],%fd822; }

	// end inline asm
	add.s64 	%rd476, %rd475, 8;
	// begin inline asm
	{ atom.add.f64 %fd3198,[%rd476],%fd823; }

	// end inline asm
	add.s64 	%rd477, %rd475, 16;
	// begin inline asm
	{ atom.add.f64 %fd3200,[%rd477],%fd824; }

	// end inline asm

$L__BB9_294:
	add.f64 	%fd825, %fd816, 0d0000000000000000;
	add.f64 	%fd826, %fd812, 0d0000000000000000;
	add.f64 	%fd827, %fd808, 0d0000000000000000;
	@%p215 bra 	$L__BB9_296;

	cvt.s64.s32 	%rd483, %r1038;
	mul.lo.s64 	%rd484, %rd483, %rd81;
	add.s64 	%rd480, %rd162, %rd484;
	// begin inline asm
	{ atom.add.f64 %fd3202,[%rd480],%fd825; }

	// end inline asm
	add.s64 	%rd481, %rd480, 8;
	// begin inline asm
	{ atom.add.f64 %fd3204,[%rd481],%fd826; }

	// end inline asm
	add.s64 	%rd482, %rd480, 16;
	// begin inline asm
	{ atom.add.f64 %fd3206,[%rd482],%fd827; }

	// end inline asm
	bra.uni 	$L__BB9_298;

$L__BB9_296:
	setp.eq.s64 	%p218, %rd131, 0;
	@%p218 bra 	$L__BB9_298;

	cvt.s64.s32 	%rd488, %r1038;
	mul.lo.s64 	%rd489, %rd488, %rd57;
	add.s64 	%rd485, %rd131, %rd489;
	// begin inline asm
	{ atom.add.f64 %fd3208,[%rd485],%fd825; }

	// end inline asm
	add.s64 	%rd486, %rd485, 8;
	// begin inline asm
	{ atom.add.f64 %fd3210,[%rd486],%fd826; }

	// end inline asm
	add.s64 	%rd487, %rd485, 16;
	// begin inline asm
	{ atom.add.f64 %fd3212,[%rd487],%fd827; }

	// end inline asm

$L__BB9_298:
	add.f64 	%fd828, %fd817, 0d0000000000000000;
	add.f64 	%fd829, %fd813, 0d0000000000000000;
	add.f64 	%fd830, %fd809, 0d0000000000000000;
	@%p215 bra 	$L__BB9_300;

	cvt.s64.s32 	%rd493, %r1037;
	mul.lo.s64 	%rd494, %rd493, %rd81;
	add.s64 	%rd490, %rd162, %rd494;
	// begin inline asm
	{ atom.add.f64 %fd3214,[%rd490],%fd828; }

	// end inline asm
	add.s64 	%rd491, %rd490, 8;
	// begin inline asm
	{ atom.add.f64 %fd3216,[%rd491],%fd829; }

	// end inline asm
	add.s64 	%rd492, %rd490, 16;
	// begin inline asm
	{ atom.add.f64 %fd3218,[%rd492],%fd830; }

	// end inline asm
	bra.uni 	$L__BB9_302;

$L__BB9_300:
	setp.eq.s64 	%p220, %rd131, 0;
	@%p220 bra 	$L__BB9_302;

	cvt.s64.s32 	%rd498, %r1037;
	mul.lo.s64 	%rd499, %rd498, %rd57;
	add.s64 	%rd495, %rd131, %rd499;
	// begin inline asm
	{ atom.add.f64 %fd3220,[%rd495],%fd828; }

	// end inline asm
	add.s64 	%rd496, %rd495, 8;
	// begin inline asm
	{ atom.add.f64 %fd3222,[%rd496],%fd829; }

	// end inline asm
	add.s64 	%rd497, %rd495, 16;
	// begin inline asm
	{ atom.add.f64 %fd3224,[%rd497],%fd830; }

	// end inline asm

$L__BB9_302:
	add.f64 	%fd831, %fd818, 0d0000000000000000;
	add.f64 	%fd832, %fd814, 0d0000000000000000;
	add.f64 	%fd833, %fd810, 0d0000000000000000;
	@%p215 bra 	$L__BB9_304;

	cvt.s64.s32 	%rd503, %r1036;
	mul.lo.s64 	%rd504, %rd503, %rd81;
	add.s64 	%rd500, %rd162, %rd504;
	// begin inline asm
	{ atom.add.f64 %fd3226,[%rd500],%fd831; }

	// end inline asm
	add.s64 	%rd501, %rd500, 8;
	// begin inline asm
	{ atom.add.f64 %fd3228,[%rd501],%fd832; }

	// end inline asm
	add.s64 	%rd502, %rd500, 16;
	// begin inline asm
	{ atom.add.f64 %fd3230,[%rd502],%fd833; }

	// end inline asm
	bra.uni 	$L__BB9_306;

$L__BB9_304:
	setp.eq.s64 	%p222, %rd131, 0;
	@%p222 bra 	$L__BB9_306;

	cvt.s64.s32 	%rd508, %r1036;
	mul.lo.s64 	%rd509, %rd508, %rd57;
	add.s64 	%rd505, %rd131, %rd509;
	// begin inline asm
	{ atom.add.f64 %fd3232,[%rd505],%fd831; }

	// end inline asm
	add.s64 	%rd506, %rd505, 8;
	// begin inline asm
	{ atom.add.f64 %fd3234,[%rd506],%fd832; }

	// end inline asm
	add.s64 	%rd507, %rd505, 16;
	// begin inline asm
	{ atom.add.f64 %fd3236,[%rd507],%fd833; }

	// end inline asm

$L__BB9_306:
	setp.eq.s64 	%p223, %rd172, 0;
	add.f64 	%fd834, %fd620, 0d0000000000000000;
	@%p223 bra 	$L__BB9_308;

	cvt.s64.s32 	%rd511, %r1027;
	mul.lo.s64 	%rd512, %rd511, %rd82;
	add.s64 	%rd510, %rd172, %rd512;
	// begin inline asm
	{ atom.add.f64 %fd3238,[%rd510],%fd834; }

	// end inline asm
	bra.uni 	$L__BB9_310;

$L__BB9_308:
	setp.eq.s64 	%p224, %rd143, 0;
	@%p224 bra 	$L__BB9_310;

	cvt.s64.s32 	%rd514, %r1027;
	mul.lo.s64 	%rd515, %rd514, %rd63;
	add.s64 	%rd513, %rd143, %rd515;
	// begin inline asm
	{ atom.add.f64 %fd3240,[%rd513],%fd834; }

	// end inline asm

$L__BB9_310:
	@%p223 bra 	$L__BB9_312;

	cvt.s64.s32 	%rd517, %r1026;
	mul.lo.s64 	%rd518, %rd517, %rd82;
	add.s64 	%rd516, %rd172, %rd518;
	// begin inline asm
	{ atom.add.f64 %fd3242,[%rd516],%fd834; }

	// end inline asm
	bra.uni 	$L__BB9_314;

$L__BB9_312:
	setp.eq.s64 	%p226, %rd143, 0;
	@%p226 bra 	$L__BB9_314;

	cvt.s64.s32 	%rd520, %r1026;
	mul.lo.s64 	%rd521, %rd520, %rd63;
	add.s64 	%rd519, %rd143, %rd521;
	// begin inline asm
	{ atom.add.f64 %fd3244,[%rd519],%fd834; }

	// end inline asm

$L__BB9_314:
	@%p14 bra 	$L__BB9_448;

	and.b16  	%rs297, %rs305, 255;
	setp.ne.s16 	%p228, %rs297, 0;
	cvt.s64.s32 	%rd522, %r1001;
	mul.lo.s64 	%rd523, %rd522, %rd65;
	cvta.to.global.u64 	%rd524, %rd154;
	add.s64 	%rd108, %rd524, %rd523;
	mul.lo.s64 	%rd525, %rd522, %rd67;
	cvta.to.global.u64 	%rd526, %rd113;
	add.s64 	%rd109, %rd526, %rd525;
	mov.f64 	%fd4986, 0d0000000000000000;
	mov.f64 	%fd4970, %fd4986;
	mov.f64 	%fd4971, %fd4986;
	mov.f64 	%fd4972, %fd4986;
	mov.f64 	%fd4973, %fd4986;
	mov.f64 	%fd4974, %fd4986;
	mov.f64 	%fd4975, %fd4986;
	@%p228 bra 	$L__BB9_342;

	setp.eq.s64 	%p229, %rd154, 0;
	@%p229 bra 	$L__BB9_318;

	ld.global.f64 	%fd3252, [%rd108];
	add.f64 	%fd4964, %fd3252, 0d0000000000000000;
	bra.uni 	$L__BB9_320;

$L__BB9_318:
	setp.eq.s64 	%p230, %rd113, 0;
	mov.f64 	%fd4964, 0d0000000000000000;
	@%p230 bra 	$L__BB9_320;

	ld.global.f64 	%fd3254, [%rd109];
	add.f64 	%fd4964, %fd3254, 0d0000000000000000;

$L__BB9_320:
	mul.f64 	%fd3255, %fd4748, %fd4748;
	mov.f64 	%fd3256, 0d3FF0000000000000;
	sub.f64 	%fd838, %fd3256, %fd3255;
	mul.f64 	%fd3257, %fd4748, %fd4747;
	mov.f64 	%fd3258, 0d0000000000000000;
	sub.f64 	%fd839, %fd3258, %fd3257;
	mul.f64 	%fd3259, %fd4748, %fd4746;
	sub.f64 	%fd840, %fd3258, %fd3259;
	mul.f64 	%fd3260, %fd4747, %fd4747;
	sub.f64 	%fd841, %fd3256, %fd3260;
	mul.f64 	%fd3261, %fd4747, %fd4746;
	sub.f64 	%fd842, %fd3258, %fd3261;
	mul.f64 	%fd3262, %fd4746, %fd4746;
	sub.f64 	%fd843, %fd3256, %fd3262;
	sub.f64 	%fd844, %fd4740, %fd4743;
	sub.f64 	%fd845, %fd4739, %fd4742;
	mul.f64 	%fd3263, %fd839, %fd845;
	mul.f64 	%fd3264, %fd841, %fd845;
	mul.f64 	%fd3265, %fd842, %fd845;
	fma.rn.f64 	%fd3266, %fd838, %fd844, %fd3263;
	fma.rn.f64 	%fd3267, %fd839, %fd844, %fd3264;
	fma.rn.f64 	%fd3268, %fd840, %fd844, %fd3265;
	sub.f64 	%fd846, %fd4738, %fd4741;
	fma.rn.f64 	%fd3269, %fd840, %fd846, %fd3266;
	fma.rn.f64 	%fd3270, %fd842, %fd846, %fd3267;
	fma.rn.f64 	%fd3271, %fd843, %fd846, %fd3268;
	div.rn.f64 	%fd847, %fd3269, %fd1177;
	div.rn.f64 	%fd848, %fd3270, %fd1177;
	div.rn.f64 	%fd849, %fd3271, %fd1177;
	mul.f64 	%fd3272, %fd848, %fd848;
	fma.rn.f64 	%fd3273, %fd847, %fd847, %fd3272;
	fma.rn.f64 	%fd3274, %fd849, %fd849, %fd3273;
	sqrt.rn.f64 	%fd850, %fd3274;
	setp.ge.f64 	%p231, %fd850, %fd1181;
	mul.f64 	%fd851, %fd850, %fd1177;
	mov.f64 	%fd4965, %fd851;
	@%p231 bra 	$L__BB9_322;

	mul.f64 	%fd3275, %fd851, %fd851;
	sub.f64 	%fd3277, %fd3258, %fd851;
	div.rn.f64 	%fd3278, %fd3277, 0d4008000000000000;
	add.f64 	%fd3279, %fd5, %fd3278;
	mul.f64 	%fd3280, %fd3275, %fd3279;
	div.rn.f64 	%fd3281, %fd3280, %fd6;
	add.f64 	%fd4965, %fd7, %fd3281;

$L__BB9_322:
	add.f64 	%fd3282, %fd4964, 0d0000000000000000;
	fma.rn.f64 	%fd854, %fd3282, %fd4965, 0d0000000000000000;
	fma.rn.f64 	%fd4966, %fd4696, %fd3282, 0d0000000000000000;
	@%p231 bra 	$L__BB9_324;

	mul.f64 	%fd3283, %fd851, %fd851;
	mov.f64 	%fd3284, 0d0000000000000000;
	sub.f64 	%fd3285, %fd3284, %fd851;
	div.rn.f64 	%fd3286, %fd3285, 0d4008000000000000;
	add.f64 	%fd3287, %fd5, %fd3286;
	div.rn.f64 	%fd3288, %fd4966, %fd6;
	add.f64 	%fd3289, %fd3288, 0d0000000000000000;
	fma.rn.f64 	%fd3290, %fd3289, %fd3287, 0d0000000000000000;
	fma.rn.f64 	%fd3291, %fd3289, %fd3283, 0d0000000000000000;
	div.rn.f64 	%fd3292, %fd3291, 0d4008000000000000;
	add.f64 	%fd3293, %fd3292, 0d0000000000000000;
	sub.f64 	%fd3294, %fd3284, %fd3293;
	fma.rn.f64 	%fd3295, %fd851, %fd3290, %fd3294;
	fma.rn.f64 	%fd4966, %fd851, %fd3290, %fd3295;

$L__BB9_324:
	fma.rn.f64 	%fd858, %fd4966, %fd1177, 0d0000000000000000;
	mov.f64 	%fd3298, 0d0000000000000000;
	setp.leu.f64 	%p233, %fd850, 0d0000000000000000;
	mov.f64 	%fd4967, %fd3298;
	mov.f64 	%fd4968, %fd3298;
	mov.f64 	%fd4969, %fd3298;
	@%p233 bra 	$L__BB9_326;

	div.rn.f64 	%fd3299, %fd847, %fd850;
	div.rn.f64 	%fd3300, %fd848, %fd850;
	div.rn.f64 	%fd3301, %fd849, %fd850;
	fma.rn.f64 	%fd4969, %fd3299, %fd858, 0d0000000000000000;
	fma.rn.f64 	%fd4968, %fd3300, %fd858, 0d0000000000000000;
	fma.rn.f64 	%fd4967, %fd3301, %fd858, 0d0000000000000000;

$L__BB9_326:
	div.rn.f64 	%fd3302, %fd4969, %fd1177;
	add.f64 	%fd3303, %fd3302, 0d0000000000000000;
	div.rn.f64 	%fd3305, %fd4968, %fd1177;
	add.f64 	%fd3306, %fd3305, 0d0000000000000000;
	div.rn.f64 	%fd3307, %fd4967, %fd1177;
	add.f64 	%fd3308, %fd3307, 0d0000000000000000;
	fma.rn.f64 	%fd3309, %fd3303, %fd844, 0d0000000000000000;
	fma.rn.f64 	%fd3310, %fd3303, %fd845, 0d0000000000000000;
	fma.rn.f64 	%fd3311, %fd3303, %fd846, 0d0000000000000000;
	fma.rn.f64 	%fd3312, %fd3306, %fd844, 0d0000000000000000;
	fma.rn.f64 	%fd3313, %fd3306, %fd845, 0d0000000000000000;
	fma.rn.f64 	%fd3314, %fd3306, %fd846, 0d0000000000000000;
	fma.rn.f64 	%fd3315, %fd3308, %fd844, 0d0000000000000000;
	fma.rn.f64 	%fd3316, %fd3308, %fd845, 0d0000000000000000;
	fma.rn.f64 	%fd3317, %fd3308, %fd846, 0d0000000000000000;
	mul.f64 	%fd3318, %fd839, %fd3306;
	mul.f64 	%fd3319, %fd841, %fd3306;
	mul.f64 	%fd3320, %fd842, %fd3306;
	fma.rn.f64 	%fd3321, %fd838, %fd3303, %fd3318;
	fma.rn.f64 	%fd3322, %fd839, %fd3303, %fd3319;
	fma.rn.f64 	%fd3323, %fd840, %fd3303, %fd3320;
	fma.rn.f64 	%fd3324, %fd842, %fd3308, %fd3322;
	fma.rn.f64 	%fd3325, %fd843, %fd3308, %fd3323;
	add.f64 	%fd4971, %fd3324, 0d0000000000000000;
	fma.rn.f64 	%fd3326, %fd840, %fd3308, %fd3321;
	add.f64 	%fd4972, %fd3326, 0d0000000000000000;
	add.f64 	%fd4970, %fd3325, 0d0000000000000000;
	sub.f64 	%fd3327, %fd3298, %fd3309;
	sub.f64 	%fd3328, %fd3298, %fd3312;
	sub.f64 	%fd3329, %fd3298, %fd3315;
	sub.f64 	%fd3330, %fd3298, %fd3310;
	sub.f64 	%fd3331, %fd3298, %fd3313;
	sub.f64 	%fd3332, %fd3298, %fd3316;
	sub.f64 	%fd3333, %fd3298, %fd3311;
	sub.f64 	%fd3334, %fd3298, %fd3314;
	sub.f64 	%fd3335, %fd3298, %fd3317;
	mul.f64 	%fd3336, %fd3328, %fd4747;
	mul.f64 	%fd3337, %fd3331, %fd4747;
	mul.f64 	%fd3338, %fd3334, %fd4747;
	fma.rn.f64 	%fd3339, %fd3327, %fd4748, %fd3336;
	fma.rn.f64 	%fd3340, %fd3330, %fd4748, %fd3337;
	fma.rn.f64 	%fd3341, %fd3333, %fd4748, %fd3338;
	fma.rn.f64 	%fd3342, %fd3332, %fd4746, %fd3340;
	fma.rn.f64 	%fd3343, %fd3335, %fd4746, %fd3341;
	add.f64 	%fd3344, %fd3342, 0d0000000000000000;
	fma.rn.f64 	%fd3345, %fd3329, %fd4746, %fd3339;
	add.f64 	%fd3346, %fd3345, 0d0000000000000000;
	add.f64 	%fd3347, %fd3343, 0d0000000000000000;
	mul.f64 	%fd3348, %fd3330, %fd4747;
	mul.f64 	%fd3349, %fd3332, %fd4747;
	fma.rn.f64 	%fd3350, %fd3327, %fd4748, %fd3348;
	fma.rn.f64 	%fd3351, %fd3328, %fd4748, %fd3337;
	fma.rn.f64 	%fd3352, %fd3329, %fd4748, %fd3349;
	fma.rn.f64 	%fd3353, %fd3333, %fd4746, %fd3350;
	fma.rn.f64 	%fd3354, %fd3334, %fd4746, %fd3351;
	fma.rn.f64 	%fd3355, %fd3335, %fd4746, %fd3352;
	add.f64 	%fd868, %fd3346, %fd3353;
	add.f64 	%fd869, %fd3344, %fd3354;
	add.f64 	%fd870, %fd3347, %fd3355;
	setp.eq.s64 	%p234, %rd160, 0;
	@%p234 bra 	$L__BB9_328;

	mul.lo.s64 	%rd530, %rd96, %rd83;
	add.s64 	%rd527, %rd160, %rd530;
	// begin inline asm
	{ atom.add.f64 %fd3356,[%rd527],%fd868; }

	// end inline asm
	add.s64 	%rd528, %rd527, 8;
	// begin inline asm
	{ atom.add.f64 %fd3358,[%rd528],%fd869; }

	// end inline asm
	add.s64 	%rd529, %rd527, 16;
	// begin inline asm
	{ atom.add.f64 %fd3360,[%rd529],%fd870; }

	// end inline asm
	bra.uni 	$L__BB9_330;

$L__BB9_328:
	setp.eq.s64 	%p235, %rd119, 0;
	@%p235 bra 	$L__BB9_330;

	mul.lo.s64 	%rd534, %rd96, %rd73;
	add.s64 	%rd531, %rd119, %rd534;
	// begin inline asm
	{ atom.add.f64 %fd3362,[%rd531],%fd868; }

	// end inline asm
	add.s64 	%rd532, %rd531, 8;
	// begin inline asm
	{ atom.add.f64 %fd3364,[%rd532],%fd869; }

	// end inline asm
	add.s64 	%rd533, %rd531, 16;
	// begin inline asm
	{ atom.add.f64 %fd3366,[%rd533],%fd870; }

	// end inline asm

$L__BB9_330:
	setp.eq.s64 	%p236, %rd156, 0;
	fma.rn.f64 	%fd3368, %fd854, %fd1180, 0d0000000000000000;
	fma.rn.f64 	%fd871, %fd4679, %fd3368, 0d0000000000000000;
	fma.rn.f64 	%fd872, %fd4682, %fd3368, 0d0000000000000000;
	@%p236 bra 	$L__BB9_332;

	mul.lo.s64 	%rd536, %rd96, %rd84;
	add.s64 	%rd535, %rd156, %rd536;
	// begin inline asm
	{ atom.add.f64 %fd3369,[%rd535],%fd872; }

	// end inline asm
	bra.uni 	$L__BB9_334;

$L__BB9_332:
	setp.eq.s64 	%p237, %rd115, 0;
	@%p237 bra 	$L__BB9_334;

	mul.lo.s64 	%rd538, %rd96, %rd72;
	add.s64 	%rd537, %rd115, %rd538;
	// begin inline asm
	{ atom.add.f64 %fd3371,[%rd537],%fd872; }

	// end inline asm

$L__BB9_334:
	setp.eq.s64 	%p238, %rd168, 0;
	add.f64 	%fd3373, %fd4681, %fd4681;
	mul.f64 	%fd3374, %fd3373, %fd4680;
	add.f64 	%fd3375, %fd4681, %fd4680;
	setp.neu.f64 	%p239, %fd3375, 0d0000000000000000;
	mov.f64 	%fd3376, 0d0000000000000000;
	div.rn.f64 	%fd3377, %fd3374, %fd3375;
	selp.f64 	%fd3378, %fd871, 0d0000000000000000, %p239;
	div.rn.f64 	%fd3379, %fd3378, %fd3375;
	add.f64 	%fd3380, %fd3379, 0d0000000000000000;
	mul.f64 	%fd3381, %fd3377, %fd3378;
	div.rn.f64 	%fd3382, %fd3381, %fd3375;
	sub.f64 	%fd3383, %fd3376, %fd3382;
	add.f64 	%fd3384, %fd3383, 0d0000000000000000;
	fma.rn.f64 	%fd3385, %fd4680, %fd3380, 0d0000000000000000;
	fma.rn.f64 	%fd873, %fd3373, %fd3380, %fd3384;
	fma.rn.f64 	%fd874, %fd3385, 0d4000000000000000, %fd3384;
	@%p238 bra 	$L__BB9_336;

	cvt.s64.s32 	%rd540, %r979;
	mul.lo.s64 	%rd541, %rd540, %rd85;
	add.s64 	%rd539, %rd168, %rd541;
	// begin inline asm
	{ atom.add.f64 %fd3386,[%rd539],%fd873; }

	// end inline asm
	bra.uni 	$L__BB9_338;

$L__BB9_336:
	setp.eq.s64 	%p240, %rd139, 0;
	@%p240 bra 	$L__BB9_338;

	cvt.s64.s32 	%rd543, %r979;
	mul.lo.s64 	%rd544, %rd543, %rd70;
	add.s64 	%rd542, %rd139, %rd544;
	// begin inline asm
	{ atom.add.f64 %fd3388,[%rd542],%fd873; }

	// end inline asm

$L__BB9_338:
	mov.f64 	%fd3390, 0d0000000000000000;
	sub.f64 	%fd4974, %fd3390, %fd4971;
	sub.f64 	%fd4975, %fd3390, %fd4972;
	sub.f64 	%fd4973, %fd3390, %fd4970;
	@%p238 bra 	$L__BB9_340;

	cvt.s64.s32 	%rd546, %r980;
	mul.lo.s64 	%rd547, %rd546, %rd85;
	add.s64 	%rd545, %rd168, %rd547;
	// begin inline asm
	{ atom.add.f64 %fd3391,[%rd545],%fd874; }

	// end inline asm
	bra.uni 	$L__BB9_342;

$L__BB9_340:
	setp.eq.s64 	%p242, %rd139, 0;
	@%p242 bra 	$L__BB9_342;

	cvt.s64.s32 	%rd549, %r980;
	mul.lo.s64 	%rd550, %rd549, %rd70;
	add.s64 	%rd548, %rd139, %rd550;
	// begin inline asm
	{ atom.add.f64 %fd3393,[%rd548],%fd874; }

	// end inline asm

$L__BB9_342:
	setp.eq.s16 	%p243, %rs297, 0;
	mov.f64 	%fd4987, %fd4986;
	@%p243 bra 	$L__BB9_361;

	setp.eq.s64 	%p244, %rd154, 0;
	@%p244 bra 	$L__BB9_345;

	ld.global.f64 	%fd3397, [%rd108];
	add.f64 	%fd4976, %fd3397, 0d0000000000000000;
	bra.uni 	$L__BB9_347;

$L__BB9_345:
	setp.eq.s64 	%p245, %rd113, 0;
	mov.f64 	%fd4976, 0d0000000000000000;
	@%p245 bra 	$L__BB9_347;

	ld.global.f64 	%fd3399, [%rd109];
	add.f64 	%fd4976, %fd3399, 0d0000000000000000;

$L__BB9_347:
	fma.rn.f64 	%fd3401, %fd4976, %fd1180, 0d0000000000000000;
	fma.rn.f64 	%fd887, %fd4755, %fd3401, 0d0000000000000000;
	fma.rn.f64 	%fd888, %fd4754, %fd3401, 0d0000000000000000;
	setp.geu.f64 	%p246, %fd4737, %fd4744;
	@%p246 bra 	$L__BB9_355;

	sub.f64 	%fd3402, %fd4737, %fd4744;
	div.rn.f64 	%fd4979, %fd3402, %fd4744;
	mul.f64 	%fd4980, %fd3, %fd4979;
	div.rn.f64 	%fd4982, %fd4737, %fd4744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1059}, %fd4982;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1060, %temp}, %fd4982;
	}
	setp.gt.s32 	%p247, %r1059, 1048575;
	mov.u32 	%r1061, -1023;
	mov.f64 	%fd4977, %fd4982;
	@%p247 bra 	$L__BB9_350;

	mul.f64 	%fd4977, %fd4982, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1059}, %fd4977;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1060, %temp}, %fd4977;
	}
	mov.u32 	%r1061, -1077;

$L__BB9_350:
	mul.f64 	%fd4981, %fd4979, %fd4980;
	add.s32 	%r928, %r1059, -1;
	setp.lt.u32 	%p248, %r928, 2146435071;
	@%p248 bra 	$L__BB9_352;
	bra.uni 	$L__BB9_351;

$L__BB9_352:
	shr.u32 	%r930, %r1059, 20;
	add.s32 	%r1062, %r1061, %r930;
	and.b32  	%r931, %r1059, -2146435073;
	or.b32  	%r932, %r931, 1072693248;
	mov.b64 	%fd4978, {%r1060, %r932};
	setp.lt.s32 	%p250, %r932, 1073127583;
	@%p250 bra 	$L__BB9_354;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r933, %temp}, %fd4978;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r934}, %fd4978;
	}
	add.s32 	%r935, %r934, -1048576;
	mov.b64 	%fd4978, {%r933, %r935};
	add.s32 	%r1062, %r1062, 1;

$L__BB9_354:
	add.f64 	%fd3405, %fd4978, 0d3FF0000000000000;
	mov.f64 	%fd3406, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd3407, %fd3405;
	neg.f64 	%fd3408, %fd3405;
	fma.rn.f64 	%fd3409, %fd3408, %fd3407, %fd3406;
	fma.rn.f64 	%fd3410, %fd3409, %fd3409, %fd3409;
	fma.rn.f64 	%fd3411, %fd3410, %fd3407, %fd3407;
	add.f64 	%fd3412, %fd4978, 0dBFF0000000000000;
	mul.f64 	%fd3413, %fd3412, %fd3411;
	fma.rn.f64 	%fd3414, %fd3412, %fd3411, %fd3413;
	mul.f64 	%fd3415, %fd3414, %fd3414;
	mov.f64 	%fd3416, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd3417, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd3418, %fd3417, %fd3415, %fd3416;
	mov.f64 	%fd3419, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd3420, %fd3418, %fd3415, %fd3419;
	mov.f64 	%fd3421, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd3422, %fd3420, %fd3415, %fd3421;
	mov.f64 	%fd3423, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd3424, %fd3422, %fd3415, %fd3423;
	mov.f64 	%fd3425, 0d3F624924923BE72D;
	fma.rn.f64 	%fd3426, %fd3424, %fd3415, %fd3425;
	mov.f64 	%fd3427, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd3428, %fd3426, %fd3415, %fd3427;
	mov.f64 	%fd3429, 0d3FB5555555555554;
	fma.rn.f64 	%fd3430, %fd3428, %fd3415, %fd3429;
	sub.f64 	%fd3431, %fd3412, %fd3414;
	add.f64 	%fd3432, %fd3431, %fd3431;
	neg.f64 	%fd3433, %fd3414;
	fma.rn.f64 	%fd3434, %fd3433, %fd3412, %fd3432;
	mul.f64 	%fd3435, %fd3411, %fd3434;
	mul.f64 	%fd3436, %fd3415, %fd3430;
	fma.rn.f64 	%fd3437, %fd3436, %fd3414, %fd3435;
	xor.b32  	%r936, %r1062, -2147483648;
	mov.u32 	%r937, -2147483648;
	mov.u32 	%r938, 1127219200;
	mov.b64 	%fd3438, {%r936, %r938};
	mov.b64 	%fd3439, {%r937, %r938};
	sub.f64 	%fd3440, %fd3438, %fd3439;
	mov.f64 	%fd3441, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd3442, %fd3440, %fd3441, %fd3414;
	neg.f64 	%fd3443, %fd3440;
	fma.rn.f64 	%fd3444, %fd3443, %fd3441, %fd3442;
	sub.f64 	%fd3445, %fd3444, %fd3414;
	sub.f64 	%fd3446, %fd3437, %fd3445;
	mov.f64 	%fd3447, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd3448, %fd3440, %fd3447, %fd3446;
	add.f64 	%fd4983, %fd3442, %fd3448;
	bra.uni 	$L__BB9_355;

$L__BB9_351:
	mov.f64 	%fd3403, 0d7FF0000000000000;
	fma.rn.f64 	%fd3404, %fd4977, %fd3403, %fd3403;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r929}, %fd4977;
	}
	mov.b32 	%f9, %r929;
	setp.eq.f32 	%p249, %f9, 0f00000000;
	selp.f64 	%fd4983, 0dFFF0000000000000, %fd3404, %p249;

$L__BB9_355:
	setp.lt.f64 	%p252, %fd4737, %fd4744;
	selp.f64 	%fd905, %fd888, 0d0000000000000000, %p252;
	mov.f64 	%fd4986, 0d0000000000000000;
	mov.f64 	%fd4987, %fd4986;
	@%p246 bra 	$L__BB9_357;

	fma.rn.f64 	%fd3451, %fd905, %fd4983, 0d0000000000000000;
	mov.f64 	%fd3452, 0d0000000000000000;
	fma.rn.f64 	%fd3453, %fd905, %fd4981, 0d0000000000000000;
	rcp.rn.f64 	%fd3454, %fd4982;
	fma.rn.f64 	%fd3455, %fd3454, %fd3453, 0d0000000000000000;
	div.rn.f64 	%fd3456, %fd3455, %fd4744;
	add.f64 	%fd3457, %fd3456, 0d0000000000000000;
	mul.f64 	%fd3458, %fd4982, %fd3455;
	div.rn.f64 	%fd3459, %fd3458, %fd4744;
	sub.f64 	%fd3460, %fd3452, %fd3459;
	fma.rn.f64 	%fd3461, %fd4979, %fd3451, 0d0000000000000000;
	fma.rn.f64 	%fd3462, %fd4980, %fd3451, 0d0000000000000000;
	div.rn.f64 	%fd3463, %fd3462, %fd4744;
	add.f64 	%fd3464, %fd3463, 0d0000000000000000;
	mul.f64 	%fd3465, %fd4979, %fd3462;
	div.rn.f64 	%fd3466, %fd3465, %fd4744;
	sub.f64 	%fd3467, %fd3460, %fd3466;
	fma.rn.f64 	%fd3468, %fd3, %fd3461, 0d0000000000000000;
	div.rn.f64 	%fd3469, %fd3468, %fd4744;
	add.f64 	%fd3470, %fd3464, %fd3469;
	mul.f64 	%fd3471, %fd4979, %fd3468;
	div.rn.f64 	%fd3472, %fd3471, %fd4744;
	sub.f64 	%fd3473, %fd3467, %fd3472;
	add.f64 	%fd4986, %fd3457, %fd3470;
	sub.f64 	%fd4987, %fd3473, %fd3470;

$L__BB9_357:
	fma.rn.f64 	%fd3474, %fd887, %fd1176, 0d0000000000000000;
	fma.rn.f64 	%fd910, %fd3474, %fd1179, 0d0000000000000000;
	setp.eq.s64 	%p253, %rd176, 0;
	@%p253 bra 	$L__BB9_359;

	cvt.s64.s32 	%rd552, %r999;
	mul.lo.s64 	%rd553, %rd552, %rd75;
	add.s64 	%rd551, %rd176, %rd553;
	// begin inline asm
	{ atom.add.f64 %fd3475,[%rd551],%fd910; }

	// end inline asm
	bra.uni 	$L__BB9_361;

$L__BB9_359:
	setp.eq.s64 	%p254, %rd147, 0;
	@%p254 bra 	$L__BB9_361;

	cvt.s64.s32 	%rd555, %r999;
	mul.lo.s64 	%rd556, %rd555, %rd64;
	add.s64 	%rd554, %rd147, %rd556;
	// begin inline asm
	{ atom.add.f64 %fd3477,[%rd554],%fd910; }

	// end inline asm

$L__BB9_361:
	and.b16  	%rs299, %rs304, 255;
	setp.eq.s16 	%p255, %rs299, 0;
	@%p255 bra 	$L__BB9_363;

	add.u64 	%rd557, %SP, 0;
	add.u64 	%rd558, %SPL, 0;
	mov.u64 	%rd559, $str$3;
	cvta.global.u64 	%rd560, %rd559;
	st.local.u64 	[%rd558], %rd560;
	mov.u64 	%rd561, $str$6;
	cvta.global.u64 	%rd562, %rd561;
	{ // callseq 512, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd562;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd557;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r939, [retval0+0];
	} // callseq 512

$L__BB9_363:
	fma.rn.f64 	%fd913, %fd4, %fd4753, 0d0000000000000000;
	fma.rn.f64 	%fd914, %fd4, %fd4752, 0d0000000000000000;
	setp.geu.f64 	%p256, %fd4737, %fd4744;
	@%p256 bra 	$L__BB9_371;

	sub.f64 	%fd3480, %fd4737, %fd4744;
	div.rn.f64 	%fd4990, %fd3480, %fd4744;
	mul.f64 	%fd4991, %fd3, %fd4990;
	div.rn.f64 	%fd4993, %fd4737, %fd4744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1063}, %fd4993;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1064, %temp}, %fd4993;
	}
	setp.gt.s32 	%p257, %r1063, 1048575;
	mov.u32 	%r1065, -1023;
	mov.f64 	%fd4988, %fd4993;
	@%p257 bra 	$L__BB9_366;

	mul.f64 	%fd4988, %fd4993, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1063}, %fd4988;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r1064, %temp}, %fd4988;
	}
	mov.u32 	%r1065, -1077;

$L__BB9_366:
	mul.f64 	%fd4992, %fd4990, %fd4991;
	add.s32 	%r942, %r1063, -1;
	setp.lt.u32 	%p258, %r942, 2146435071;
	@%p258 bra 	$L__BB9_368;
	bra.uni 	$L__BB9_367;

$L__BB9_368:
	shr.u32 	%r944, %r1063, 20;
	add.s32 	%r1066, %r1065, %r944;
	and.b32  	%r945, %r1063, -2146435073;
	or.b32  	%r946, %r945, 1072693248;
	mov.b64 	%fd4989, {%r1064, %r946};
	setp.lt.s32 	%p260, %r946, 1073127583;
	@%p260 bra 	$L__BB9_370;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r947, %temp}, %fd4989;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r948}, %fd4989;
	}
	add.s32 	%r949, %r948, -1048576;
	mov.b64 	%fd4989, {%r947, %r949};
	add.s32 	%r1066, %r1066, 1;

$L__BB9_370:
	add.f64 	%fd3483, %fd4989, 0d3FF0000000000000;
	mov.f64 	%fd3484, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd3485, %fd3483;
	neg.f64 	%fd3486, %fd3483;
	fma.rn.f64 	%fd3487, %fd3486, %fd3485, %fd3484;
	fma.rn.f64 	%fd3488, %fd3487, %fd3487, %fd3487;
	fma.rn.f64 	%fd3489, %fd3488, %fd3485, %fd3485;
	add.f64 	%fd3490, %fd4989, 0dBFF0000000000000;
	mul.f64 	%fd3491, %fd3490, %fd3489;
	fma.rn.f64 	%fd3492, %fd3490, %fd3489, %fd3491;
	mul.f64 	%fd3493, %fd3492, %fd3492;
	mov.f64 	%fd3494, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd3495, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd3496, %fd3495, %fd3493, %fd3494;
	mov.f64 	%fd3497, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd3498, %fd3496, %fd3493, %fd3497;
	mov.f64 	%fd3499, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd3500, %fd3498, %fd3493, %fd3499;
	mov.f64 	%fd3501, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd3502, %fd3500, %fd3493, %fd3501;
	mov.f64 	%fd3503, 0d3F624924923BE72D;
	fma.rn.f64 	%fd3504, %fd3502, %fd3493, %fd3503;
	mov.f64 	%fd3505, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd3506, %fd3504, %fd3493, %fd3505;
	mov.f64 	%fd3507, 0d3FB5555555555554;
	fma.rn.f64 	%fd3508, %fd3506, %fd3493, %fd3507;
	sub.f64 	%fd3509, %fd3490, %fd3492;
	add.f64 	%fd3510, %fd3509, %fd3509;
	neg.f64 	%fd3511, %fd3492;
	fma.rn.f64 	%fd3512, %fd3511, %fd3490, %fd3510;
	mul.f64 	%fd3513, %fd3489, %fd3512;
	mul.f64 	%fd3514, %fd3493, %fd3508;
	fma.rn.f64 	%fd3515, %fd3514, %fd3492, %fd3513;
	xor.b32  	%r950, %r1066, -2147483648;
	mov.u32 	%r951, -2147483648;
	mov.u32 	%r952, 1127219200;
	mov.b64 	%fd3516, {%r950, %r952};
	mov.b64 	%fd3517, {%r951, %r952};
	sub.f64 	%fd3518, %fd3516, %fd3517;
	mov.f64 	%fd3519, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd3520, %fd3518, %fd3519, %fd3492;
	neg.f64 	%fd3521, %fd3518;
	fma.rn.f64 	%fd3522, %fd3521, %fd3519, %fd3520;
	sub.f64 	%fd3523, %fd3522, %fd3492;
	sub.f64 	%fd3524, %fd3515, %fd3523;
	mov.f64 	%fd3525, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd3526, %fd3518, %fd3525, %fd3524;
	add.f64 	%fd4994, %fd3520, %fd3526;
	bra.uni 	$L__BB9_371;

$L__BB9_367:
	mov.f64 	%fd3481, 0d7FF0000000000000;
	fma.rn.f64 	%fd3482, %fd4988, %fd3481, %fd3481;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r943}, %fd4988;
	}
	mov.b32 	%f10, %r943;
	setp.eq.f32 	%p259, %f10, 0f00000000;
	selp.f64 	%fd4994, 0dFFF0000000000000, %fd3482, %p259;

$L__BB9_371:
	setp.lt.f64 	%p262, %fd4737, %fd4744;
	selp.f64 	%fd931, %fd914, 0d0000000000000000, %p262;
	@%p256 bra 	$L__BB9_373;

	fma.rn.f64 	%fd3527, %fd931, %fd4994, 0d0000000000000000;
	fma.rn.f64 	%fd3528, %fd931, %fd4992, 0d0000000000000000;
	rcp.rn.f64 	%fd3529, %fd4993;
	fma.rn.f64 	%fd3530, %fd3529, %fd3528, 0d0000000000000000;
	div.rn.f64 	%fd3531, %fd3530, %fd4744;
	add.f64 	%fd3532, %fd4986, %fd3531;
	mul.f64 	%fd3533, %fd4993, %fd3530;
	div.rn.f64 	%fd3534, %fd3533, %fd4744;
	sub.f64 	%fd3535, %fd4987, %fd3534;
	fma.rn.f64 	%fd3536, %fd4990, %fd3527, 0d0000000000000000;
	fma.rn.f64 	%fd3537, %fd4991, %fd3527, 0d0000000000000000;
	div.rn.f64 	%fd3538, %fd3537, %fd4744;
	add.f64 	%fd3539, %fd3538, 0d0000000000000000;
	mul.f64 	%fd3540, %fd4990, %fd3537;
	div.rn.f64 	%fd3541, %fd3540, %fd4744;
	sub.f64 	%fd3542, %fd3535, %fd3541;
	fma.rn.f64 	%fd3543, %fd3, %fd3536, 0d0000000000000000;
	div.rn.f64 	%fd3544, %fd3543, %fd4744;
	add.f64 	%fd3545, %fd3539, %fd3544;
	mul.f64 	%fd3546, %fd4990, %fd3543;
	div.rn.f64 	%fd3547, %fd3546, %fd4744;
	sub.f64 	%fd3548, %fd3542, %fd3547;
	add.f64 	%fd4986, %fd3532, %fd3545;
	sub.f64 	%fd4987, %fd3548, %fd3545;

$L__BB9_373:
	fma.rn.f64 	%fd3549, %fd913, %fd1176, 0d0000000000000000;
	fma.rn.f64 	%fd936, %fd3549, %fd1179, 0d0000000000000000;
	setp.eq.s64 	%p263, %rd176, 0;
	@%p263 bra 	$L__BB9_375;

	cvt.s64.s32 	%rd564, %r999;
	mul.lo.s64 	%rd565, %rd564, %rd75;
	add.s64 	%rd563, %rd176, %rd565;
	// begin inline asm
	{ atom.add.f64 %fd3550,[%rd563],%fd936; }

	// end inline asm
	bra.uni 	$L__BB9_377;

$L__BB9_375:
	setp.eq.s64 	%p264, %rd147, 0;
	@%p264 bra 	$L__BB9_377;

	cvt.s64.s32 	%rd567, %r999;
	mul.lo.s64 	%rd568, %rd567, %rd64;
	add.s64 	%rd566, %rd147, %rd568;
	// begin inline asm
	{ atom.add.f64 %fd3552,[%rd566],%fd936; }

	// end inline asm

$L__BB9_377:
	add.f64 	%fd3554, %fd4987, 0d0000000000000000;
	mov.f64 	%fd3555, 0d0000000000000000;
	sub.f64 	%fd3556, %fd3555, %fd4975;
	fma.rn.f64 	%fd937, %fd4735, %fd3556, 0d0000000000000000;
	sub.f64 	%fd3557, %fd3555, %fd4974;
	fma.rn.f64 	%fd938, %fd4735, %fd3557, 0d0000000000000000;
	sub.f64 	%fd3558, %fd3555, %fd4973;
	fma.rn.f64 	%fd939, %fd4735, %fd3558, 0d0000000000000000;
	mul.f64 	%fd3559, %fd4732, %fd3557;
	fma.rn.f64 	%fd3560, %fd4733, %fd3556, %fd3559;
	fma.rn.f64 	%fd3561, %fd4731, %fd3558, %fd3560;
	add.f64 	%fd940, %fd4975, 0d0000000000000000;
	sub.f64 	%fd3562, %fd3555, %fd940;
	add.f64 	%fd941, %fd4974, 0d0000000000000000;
	sub.f64 	%fd3563, %fd3555, %fd941;
	add.f64 	%fd942, %fd4973, 0d0000000000000000;
	sub.f64 	%fd3564, %fd3555, %fd942;
	fma.rn.f64 	%fd943, %fd4734, %fd3562, 0d0000000000000000;
	fma.rn.f64 	%fd944, %fd4734, %fd3563, 0d0000000000000000;
	fma.rn.f64 	%fd945, %fd4734, %fd3564, 0d0000000000000000;
	fma.rn.f64 	%fd3565, %fd4858, %fd3554, 0d0000000000000000;
	mul.f64 	%fd3566, %fd4729, %fd3563;
	fma.rn.f64 	%fd3567, %fd4730, %fd3562, %fd3566;
	fma.rn.f64 	%fd3568, %fd4728, %fd3564, %fd3567;
	fma.rn.f64 	%fd946, %fd4750, %fd3562, 0d0000000000000000;
	fma.rn.f64 	%fd947, %fd4750, %fd3563, 0d0000000000000000;
	fma.rn.f64 	%fd948, %fd4750, %fd3564, 0d0000000000000000;
	add.f64 	%fd3569, %fd3568, 0d0000000000000000;
	mul.f64 	%fd3570, %fd4726, %fd3563;
	fma.rn.f64 	%fd3571, %fd4727, %fd3562, %fd3570;
	fma.rn.f64 	%fd3572, %fd4725, %fd3564, %fd3571;
	add.f64 	%fd3573, %fd3572, 0d0000000000000000;
	sub.f64 	%fd3574, %fd3569, %fd3573;
	add.f64 	%fd3575, %fd3561, 0d0000000000000000;
	sub.f64 	%fd3576, %fd3555, %fd4972;
	fma.rn.f64 	%fd5005, %fd4735, %fd3576, 0d0000000000000000;
	sub.f64 	%fd3577, %fd3555, %fd4971;
	fma.rn.f64 	%fd5004, %fd4735, %fd3577, 0d0000000000000000;
	sub.f64 	%fd3578, %fd3555, %fd4970;
	fma.rn.f64 	%fd5003, %fd4735, %fd3578, 0d0000000000000000;
	sub.f64 	%fd3579, %fd3575, %fd3573;
	mul.f64 	%fd3580, %fd4723, %fd3577;
	fma.rn.f64 	%fd3581, %fd4724, %fd3576, %fd3580;
	fma.rn.f64 	%fd3582, %fd4722, %fd3578, %fd3581;
	add.f64 	%fd952, %fd4972, 0d0000000000000000;
	sub.f64 	%fd3583, %fd3555, %fd952;
	add.f64 	%fd953, %fd4971, 0d0000000000000000;
	sub.f64 	%fd3584, %fd3555, %fd953;
	add.f64 	%fd954, %fd4970, 0d0000000000000000;
	sub.f64 	%fd3585, %fd3555, %fd954;
	fma.rn.f64 	%fd5002, %fd4734, %fd3583, 0d0000000000000000;
	fma.rn.f64 	%fd5001, %fd4734, %fd3584, 0d0000000000000000;
	fma.rn.f64 	%fd5000, %fd4734, %fd3585, 0d0000000000000000;
	mul.f64 	%fd3586, %fd4720, %fd3584;
	fma.rn.f64 	%fd3587, %fd4721, %fd3583, %fd3586;
	fma.rn.f64 	%fd3588, %fd4719, %fd3585, %fd3587;
	fma.rn.f64 	%fd4999, %fd4750, %fd3583, 0d0000000000000000;
	fma.rn.f64 	%fd4998, %fd4750, %fd3584, 0d0000000000000000;
	fma.rn.f64 	%fd4997, %fd4750, %fd3585, 0d0000000000000000;
	add.f64 	%fd3589, %fd3579, %fd3582;
	add.f64 	%fd3590, %fd3574, %fd3588;
	mul.f64 	%fd3591, %fd4717, %fd3584;
	fma.rn.f64 	%fd3592, %fd4718, %fd3583, %fd3591;
	fma.rn.f64 	%fd3593, %fd4716, %fd3585, %fd3592;
	add.f64 	%fd3594, %fd3593, 0d0000000000000000;
	sub.f64 	%fd961, %fd3589, %fd3594;
	sub.f64 	%fd962, %fd3590, %fd3594;
	add.f64 	%fd963, %fd4986, 0d0000000000000000;
	sub.f64 	%fd3595, %fd3555, %fd4986;
	fma.rn.f64 	%fd3596, %fd4736, %fd3595, %fd3565;
	fma.rn.f64 	%fd964, %fd4736, %fd3595, %fd3596;
	sub.f64 	%fd965, %fd4722, %fd4716;
	sub.f64 	%fd966, %fd4720, %fd4717;
	mul.f64 	%fd3597, %fd966, %fd965;
	sub.f64 	%fd967, %fd4723, %fd4717;
	sub.f64 	%fd968, %fd4719, %fd4716;
	mul.f64 	%fd3598, %fd968, %fd967;
	sub.f64 	%fd969, %fd3597, %fd3598;
	sub.f64 	%fd970, %fd4724, %fd4718;
	mul.f64 	%fd3599, %fd968, %fd970;
	sub.f64 	%fd971, %fd4721, %fd4718;
	mul.f64 	%fd3600, %fd971, %fd965;
	sub.f64 	%fd972, %fd3599, %fd3600;
	mul.f64 	%fd3601, %fd971, %fd967;
	mul.f64 	%fd3602, %fd966, %fd970;
	sub.f64 	%fd973, %fd3601, %fd3602;
	mul.f64 	%fd3603, %fd966, %fd973;
	mul.f64 	%fd3604, %fd968, %fd972;
	sub.f64 	%fd974, %fd3603, %fd3604;
	mul.f64 	%fd3605, %fd968, %fd969;
	mul.f64 	%fd3606, %fd971, %fd973;
	sub.f64 	%fd975, %fd3605, %fd3606;
	mul.f64 	%fd3607, %fd971, %fd972;
	mul.f64 	%fd3608, %fd966, %fd969;
	sub.f64 	%fd976, %fd3607, %fd3608;
	mul.f64 	%fd3609, %fd971, %fd971;
	fma.rn.f64 	%fd3610, %fd966, %fd966, %fd3609;
	fma.rn.f64 	%fd977, %fd968, %fd968, %fd3610;
	mul.f64 	%fd3611, %fd966, %fd975;
	fma.rn.f64 	%fd3612, %fd971, %fd974, %fd3611;
	fma.rn.f64 	%fd3613, %fd968, %fd976, %fd3612;
	mul.f64 	%fd3614, %fd975, %fd975;
	fma.rn.f64 	%fd3615, %fd974, %fd974, %fd3614;
	fma.rn.f64 	%fd3616, %fd976, %fd976, %fd3615;
	sub.f64 	%fd978, %fd4715, %fd4718;
	mul.f64 	%fd3617, %fd978, %fd971;
	sub.f64 	%fd979, %fd4714, %fd4717;
	fma.rn.f64 	%fd3618, %fd979, %fd966, %fd3617;
	sub.f64 	%fd980, %fd4713, %fd4716;
	fma.rn.f64 	%fd981, %fd980, %fd968, %fd3618;
	mul.f64 	%fd3619, %fd979, %fd975;
	fma.rn.f64 	%fd3620, %fd978, %fd974, %fd3619;
	fma.rn.f64 	%fd3621, %fd980, %fd976, %fd3620;
	div.rn.f64 	%fd982, %fd3613, %fd977;
	mul.f64 	%fd983, %fd982, %fd982;
	mul.f64 	%fd3622, %fd977, %fd983;
	sub.f64 	%fd984, %fd3616, %fd3622;
	mul.f64 	%fd3623, %fd981, %fd982;
	sub.f64 	%fd3624, %fd3621, %fd3623;
	div.rn.f64 	%fd985, %fd3624, %fd984;
	mul.f64 	%fd986, %fd977, %fd982;
	mul.f64 	%fd3625, %fd986, %fd985;
	sub.f64 	%fd3626, %fd981, %fd3625;
	div.rn.f64 	%fd987, %fd3626, %fd977;
	setp.gt.f64 	%p265, %fd987, 0d0000000000000000;
	setp.lt.f64 	%p266, %fd987, 0d3FF0000000000000;
	setp.ge.f64 	%p267, %fd985, 0d0000000000000000;
	and.pred  	%p268, %p265, %p266;
	and.pred  	%p5, %p267, %p268;
	mov.u32 	%r1067, 3;
	@%p5 bra 	$L__BB9_383;

	sub.f64 	%fd3627, %fd4724, %fd4721;
	sub.f64 	%fd3628, %fd4723, %fd4720;
	mul.f64 	%fd3629, %fd3628, %fd973;
	sub.f64 	%fd3630, %fd4722, %fd4719;
	mul.f64 	%fd3631, %fd3630, %fd972;
	sub.f64 	%fd3632, %fd3629, %fd3631;
	mul.f64 	%fd3633, %fd3630, %fd969;
	mul.f64 	%fd3634, %fd3627, %fd973;
	sub.f64 	%fd3635, %fd3633, %fd3634;
	mul.f64 	%fd3636, %fd3627, %fd972;
	mul.f64 	%fd3637, %fd3628, %fd969;
	sub.f64 	%fd3638, %fd3636, %fd3637;
	mul.f64 	%fd3639, %fd3627, %fd3627;
	fma.rn.f64 	%fd3640, %fd3628, %fd3628, %fd3639;
	fma.rn.f64 	%fd3641, %fd3630, %fd3630, %fd3640;
	mul.f64 	%fd3642, %fd3628, %fd3635;
	fma.rn.f64 	%fd3643, %fd3627, %fd3632, %fd3642;
	fma.rn.f64 	%fd3644, %fd3630, %fd3638, %fd3643;
	mul.f64 	%fd3645, %fd3635, %fd3635;
	fma.rn.f64 	%fd3646, %fd3632, %fd3632, %fd3645;
	fma.rn.f64 	%fd3647, %fd3638, %fd3638, %fd3646;
	sub.f64 	%fd3648, %fd4715, %fd4721;
	mul.f64 	%fd3649, %fd3648, %fd3627;
	sub.f64 	%fd3650, %fd4714, %fd4720;
	fma.rn.f64 	%fd3651, %fd3650, %fd3628, %fd3649;
	sub.f64 	%fd3652, %fd4713, %fd4719;
	fma.rn.f64 	%fd3653, %fd3652, %fd3630, %fd3651;
	mul.f64 	%fd3654, %fd3650, %fd3635;
	fma.rn.f64 	%fd3655, %fd3648, %fd3632, %fd3654;
	fma.rn.f64 	%fd3656, %fd3652, %fd3638, %fd3655;
	div.rn.f64 	%fd3657, %fd3644, %fd3641;
	mul.f64 	%fd3658, %fd3657, %fd3657;
	mul.f64 	%fd3659, %fd3641, %fd3658;
	sub.f64 	%fd3660, %fd3647, %fd3659;
	mul.f64 	%fd3661, %fd3653, %fd3657;
	sub.f64 	%fd3662, %fd3656, %fd3661;
	div.rn.f64 	%fd3663, %fd3662, %fd3660;
	mul.f64 	%fd3664, %fd3641, %fd3657;
	mul.f64 	%fd3665, %fd3664, %fd3663;
	sub.f64 	%fd3666, %fd3653, %fd3665;
	div.rn.f64 	%fd988, %fd3666, %fd3641;
	setp.gt.f64 	%p269, %fd988, 0d0000000000000000;
	setp.lt.f64 	%p270, %fd988, 0d3FF0000000000000;
	setp.ge.f64 	%p271, %fd3663, 0d0000000000000000;
	and.pred  	%p272, %p269, %p270;
	and.pred  	%p273, %p271, %p272;
	mov.u32 	%r1067, 4;
	@%p273 bra 	$L__BB9_383;

	sub.f64 	%fd3667, %fd4718, %fd4724;
	sub.f64 	%fd3668, %fd4717, %fd4723;
	mul.f64 	%fd3669, %fd3668, %fd973;
	sub.f64 	%fd3670, %fd4716, %fd4722;
	mul.f64 	%fd3671, %fd3670, %fd972;
	sub.f64 	%fd3672, %fd3669, %fd3671;
	mul.f64 	%fd3673, %fd3670, %fd969;
	mul.f64 	%fd3674, %fd3667, %fd973;
	sub.f64 	%fd3675, %fd3673, %fd3674;
	mul.f64 	%fd3676, %fd3667, %fd972;
	mul.f64 	%fd3677, %fd3668, %fd969;
	sub.f64 	%fd3678, %fd3676, %fd3677;
	mul.f64 	%fd3679, %fd3667, %fd3667;
	fma.rn.f64 	%fd3680, %fd3668, %fd3668, %fd3679;
	fma.rn.f64 	%fd3681, %fd3670, %fd3670, %fd3680;
	mul.f64 	%fd3682, %fd3668, %fd3675;
	fma.rn.f64 	%fd3683, %fd3667, %fd3672, %fd3682;
	fma.rn.f64 	%fd3684, %fd3670, %fd3678, %fd3683;
	mul.f64 	%fd3685, %fd3675, %fd3675;
	fma.rn.f64 	%fd3686, %fd3672, %fd3672, %fd3685;
	fma.rn.f64 	%fd3687, %fd3678, %fd3678, %fd3686;
	sub.f64 	%fd3688, %fd4715, %fd4724;
	mul.f64 	%fd3689, %fd3667, %fd3688;
	sub.f64 	%fd3690, %fd4714, %fd4723;
	fma.rn.f64 	%fd3691, %fd3668, %fd3690, %fd3689;
	sub.f64 	%fd3692, %fd4713, %fd4722;
	fma.rn.f64 	%fd3693, %fd3670, %fd3692, %fd3691;
	mul.f64 	%fd3694, %fd3690, %fd3675;
	fma.rn.f64 	%fd3695, %fd3688, %fd3672, %fd3694;
	fma.rn.f64 	%fd3696, %fd3692, %fd3678, %fd3695;
	div.rn.f64 	%fd3697, %fd3684, %fd3681;
	mul.f64 	%fd3698, %fd3697, %fd3697;
	mul.f64 	%fd3699, %fd3681, %fd3698;
	sub.f64 	%fd3700, %fd3687, %fd3699;
	mul.f64 	%fd3701, %fd3693, %fd3697;
	sub.f64 	%fd3702, %fd3696, %fd3701;
	div.rn.f64 	%fd3703, %fd3702, %fd3700;
	mul.f64 	%fd3704, %fd3681, %fd3697;
	mul.f64 	%fd3705, %fd3704, %fd3703;
	sub.f64 	%fd3706, %fd3693, %fd3705;
	div.rn.f64 	%fd989, %fd3706, %fd3681;
	setp.gt.f64 	%p274, %fd989, 0d0000000000000000;
	setp.lt.f64 	%p275, %fd989, 0d3FF0000000000000;
	setp.ge.f64 	%p276, %fd3703, 0d0000000000000000;
	and.pred  	%p277, %p274, %p275;
	and.pred  	%p278, %p276, %p277;
	mov.u32 	%r1067, 5;
	@%p278 bra 	$L__BB9_383;

	setp.le.f64 	%p279, %fd987, 0d0000000000000000;
	setp.ge.f64 	%p280, %fd989, 0d3FF0000000000000;
	and.pred  	%p281, %p279, %p280;
	mov.u32 	%r1067, 0;
	@%p281 bra 	$L__BB9_383;

	setp.le.f64 	%p282, %fd988, 0d0000000000000000;
	setp.ge.f64 	%p283, %fd987, 0d3FF0000000000000;
	and.pred  	%p284, %p282, %p283;
	mov.u32 	%r1067, 1;
	@%p284 bra 	$L__BB9_383;

	setp.le.f64 	%p285, %fd989, 0d0000000000000000;
	setp.ge.f64 	%p286, %fd988, 0d3FF0000000000000;
	and.pred  	%p287, %p285, %p286;
	selp.b32 	%r1067, 2, 6, %p287;

$L__BB9_383:
	setp.eq.s32 	%p288, %r1067, 0;
	@%p288 bra 	$L__BB9_395;

	setp.eq.s32 	%p289, %r1067, 1;
	@%p289 bra 	$L__BB9_394;
	bra.uni 	$L__BB9_385;

$L__BB9_394:
	sub.f64 	%fd3996, %fd4715, %fd4721;
	add.f64 	%fd3997, %fd3996, %fd3996;
	sub.f64 	%fd3998, %fd4714, %fd4720;
	add.f64 	%fd3999, %fd3998, %fd3998;
	sub.f64 	%fd4000, %fd4713, %fd4719;
	add.f64 	%fd4001, %fd4000, %fd4000;
	fma.rn.f64 	%fd4002, %fd3997, %fd963, 0d0000000000000000;
	fma.rn.f64 	%fd4003, %fd3999, %fd963, 0d0000000000000000;
	fma.rn.f64 	%fd4004, %fd4001, %fd963, 0d0000000000000000;
	add.f64 	%fd5008, %fd952, %fd4002;
	add.f64 	%fd5007, %fd953, %fd4003;
	add.f64 	%fd5006, %fd954, %fd4004;
	sub.f64 	%fd5002, %fd5002, %fd4002;
	sub.f64 	%fd5001, %fd5001, %fd4003;
	sub.f64 	%fd5000, %fd5000, %fd4004;
	bra.uni 	$L__BB9_396;

$L__BB9_395:
	sub.f64 	%fd4622, %fd4713, %fd4716;
	sub.f64 	%fd4621, %fd4715, %fd4718;
	sub.f64 	%fd4620, %fd4714, %fd4717;
	add.f64 	%fd4005, %fd4621, %fd4621;
	add.f64 	%fd4006, %fd4620, %fd4620;
	add.f64 	%fd4007, %fd4622, %fd4622;
	fma.rn.f64 	%fd4008, %fd4005, %fd963, 0d0000000000000000;
	fma.rn.f64 	%fd4009, %fd4006, %fd963, 0d0000000000000000;
	fma.rn.f64 	%fd4010, %fd4007, %fd963, 0d0000000000000000;
	add.f64 	%fd5008, %fd952, %fd4008;
	add.f64 	%fd5007, %fd953, %fd4009;
	add.f64 	%fd5006, %fd954, %fd4010;
	sub.f64 	%fd4999, %fd4999, %fd4008;
	sub.f64 	%fd4998, %fd4998, %fd4009;
	sub.f64 	%fd4997, %fd4997, %fd4010;
	bra.uni 	$L__BB9_396;

$L__BB9_385:
	setp.eq.s32 	%p290, %r1067, 2;
	@%p290 bra 	$L__BB9_393;
	bra.uni 	$L__BB9_386;

$L__BB9_393:
	sub.f64 	%fd3987, %fd4715, %fd4724;
	add.f64 	%fd3988, %fd3987, %fd3987;
	sub.f64 	%fd3989, %fd4714, %fd4723;
	add.f64 	%fd3990, %fd3989, %fd3989;
	sub.f64 	%fd3991, %fd4713, %fd4722;
	add.f64 	%fd3992, %fd3991, %fd3991;
	fma.rn.f64 	%fd3993, %fd3988, %fd963, 0d0000000000000000;
	fma.rn.f64 	%fd3994, %fd3990, %fd963, 0d0000000000000000;
	fma.rn.f64 	%fd3995, %fd3992, %fd963, 0d0000000000000000;
	add.f64 	%fd5008, %fd952, %fd3993;
	add.f64 	%fd5007, %fd953, %fd3994;
	add.f64 	%fd5006, %fd954, %fd3995;
	sub.f64 	%fd5005, %fd5005, %fd3993;
	sub.f64 	%fd5004, %fd5004, %fd3994;
	sub.f64 	%fd5003, %fd5003, %fd3995;
	bra.uni 	$L__BB9_396;

$L__BB9_386:
	setp.eq.s32 	%p291, %r1067, 3;
	@%p291 bra 	$L__BB9_392;
	bra.uni 	$L__BB9_387;

$L__BB9_392:
	sub.f64 	%fd4619, %fd4721, %fd4718;
	sub.f64 	%fd4618, %fd4719, %fd4716;
	sub.f64 	%fd4617, %fd4720, %fd4717;
	sub.f64 	%fd3917, %fd4718, %fd4715;
	sub.f64 	%fd3918, %fd4719, %fd4713;
	sub.f64 	%fd3919, %fd4717, %fd4714;
	mul.f64 	%fd3920, %fd3919, %fd3918;
	sub.f64 	%fd3921, %fd4720, %fd4714;
	sub.f64 	%fd3922, %fd4716, %fd4713;
	mul.f64 	%fd3923, %fd3922, %fd3921;
	sub.f64 	%fd3924, %fd3920, %fd3923;
	sub.f64 	%fd3925, %fd4721, %fd4715;
	mul.f64 	%fd3926, %fd3922, %fd3925;
	mul.f64 	%fd3927, %fd3917, %fd3918;
	sub.f64 	%fd3928, %fd3926, %fd3927;
	mul.f64 	%fd3929, %fd3917, %fd3921;
	mul.f64 	%fd3930, %fd3919, %fd3925;
	sub.f64 	%fd3931, %fd3929, %fd3930;
	mul.f64 	%fd3932, %fd3928, %fd3928;
	fma.rn.f64 	%fd3933, %fd3924, %fd3924, %fd3932;
	fma.rn.f64 	%fd3934, %fd3931, %fd3931, %fd3933;
	div.rn.f64 	%fd3935, %fd3934, %fd977;
	div.rn.f64 	%fd3936, %fd963, %fd977;
	add.f64 	%fd3937, %fd3936, 0d0000000000000000;
	mov.f64 	%fd3938, 0d0000000000000000;
	mul.f64 	%fd3939, %fd3935, %fd963;
	div.rn.f64 	%fd3940, %fd3939, %fd977;
	sub.f64 	%fd3941, %fd3938, %fd3940;
	add.f64 	%fd3942, %fd4619, %fd4619;
	add.f64 	%fd3943, %fd4617, %fd4617;
	add.f64 	%fd3944, %fd4618, %fd4618;
	fma.rn.f64 	%fd3945, %fd3942, %fd3941, 0d0000000000000000;
	fma.rn.f64 	%fd3946, %fd3943, %fd3941, 0d0000000000000000;
	fma.rn.f64 	%fd3947, %fd3944, %fd3941, 0d0000000000000000;
	add.f64 	%fd3948, %fd3924, %fd3924;
	add.f64 	%fd3949, %fd3928, %fd3928;
	add.f64 	%fd3950, %fd3931, %fd3931;
	fma.rn.f64 	%fd3951, %fd3948, %fd3937, 0d0000000000000000;
	fma.rn.f64 	%fd3952, %fd3949, %fd3937, 0d0000000000000000;
	fma.rn.f64 	%fd3953, %fd3950, %fd3937, 0d0000000000000000;
	mul.f64 	%fd3954, %fd3921, %fd3953;
	mul.f64 	%fd3955, %fd3918, %fd3952;
	sub.f64 	%fd3956, %fd3954, %fd3955;
	mul.f64 	%fd3957, %fd3918, %fd3951;
	mul.f64 	%fd3958, %fd3925, %fd3953;
	sub.f64 	%fd3959, %fd3957, %fd3958;
	mul.f64 	%fd3960, %fd3925, %fd3952;
	mul.f64 	%fd3961, %fd3921, %fd3951;
	sub.f64 	%fd3962, %fd3960, %fd3961;
	add.f64 	%fd3963, %fd3956, 0d0000000000000000;
	add.f64 	%fd3964, %fd3959, 0d0000000000000000;
	add.f64 	%fd3965, %fd3962, 0d0000000000000000;
	mul.f64 	%fd3966, %fd3919, %fd3953;
	mul.f64 	%fd3967, %fd3922, %fd3952;
	mul.f64 	%fd3968, %fd3922, %fd3951;
	mul.f64 	%fd3969, %fd3917, %fd3953;
	mul.f64 	%fd3970, %fd3917, %fd3952;
	mul.f64 	%fd3971, %fd3919, %fd3951;
	sub.f64 	%fd3972, %fd3967, %fd3966;
	add.f64 	%fd3973, %fd3972, 0d0000000000000000;
	sub.f64 	%fd3974, %fd3969, %fd3968;
	add.f64 	%fd3975, %fd3974, 0d0000000000000000;
	sub.f64 	%fd3976, %fd3971, %fd3970;
	add.f64 	%fd3977, %fd3976, 0d0000000000000000;
	add.f64 	%fd3978, %fd3945, %fd5002;
	add.f64 	%fd3979, %fd3946, %fd5001;
	add.f64 	%fd3980, %fd3947, %fd5000;
	sub.f64 	%fd3981, %fd4999, %fd3945;
	sub.f64 	%fd3982, %fd4998, %fd3946;
	sub.f64 	%fd3983, %fd4997, %fd3947;
	add.f64 	%fd5002, %fd3973, %fd3978;
	add.f64 	%fd5001, %fd3975, %fd3979;
	add.f64 	%fd5000, %fd3977, %fd3980;
	sub.f64 	%fd3984, %fd952, %fd3973;
	sub.f64 	%fd3985, %fd953, %fd3975;
	sub.f64 	%fd3986, %fd954, %fd3977;
	add.f64 	%fd4999, %fd3963, %fd3981;
	add.f64 	%fd4998, %fd3964, %fd3982;
	add.f64 	%fd4997, %fd3965, %fd3983;
	sub.f64 	%fd5008, %fd3984, %fd3963;
	sub.f64 	%fd5007, %fd3985, %fd3964;
	sub.f64 	%fd5006, %fd3986, %fd3965;
	bra.uni 	$L__BB9_396;

$L__BB9_387:
	setp.eq.s32 	%p292, %r1067, 4;
	@%p292 bra 	$L__BB9_391;
	bra.uni 	$L__BB9_388;

$L__BB9_391:
	sub.f64 	%fd3841, %fd4721, %fd4715;
	sub.f64 	%fd3842, %fd4722, %fd4713;
	sub.f64 	%fd3843, %fd4720, %fd4714;
	mul.f64 	%fd3844, %fd3843, %fd3842;
	sub.f64 	%fd3845, %fd4723, %fd4714;
	sub.f64 	%fd3846, %fd4719, %fd4713;
	mul.f64 	%fd3847, %fd3846, %fd3845;
	sub.f64 	%fd3848, %fd3844, %fd3847;
	sub.f64 	%fd3849, %fd4724, %fd4715;
	mul.f64 	%fd3850, %fd3846, %fd3849;
	mul.f64 	%fd3851, %fd3841, %fd3842;
	sub.f64 	%fd3852, %fd3850, %fd3851;
	mul.f64 	%fd3853, %fd3841, %fd3845;
	mul.f64 	%fd3854, %fd3843, %fd3849;
	sub.f64 	%fd3855, %fd3853, %fd3854;
	mul.f64 	%fd3856, %fd3852, %fd3852;
	fma.rn.f64 	%fd3857, %fd3848, %fd3848, %fd3856;
	fma.rn.f64 	%fd3858, %fd3855, %fd3855, %fd3857;
	sub.f64 	%fd3859, %fd4724, %fd4721;
	mul.f64 	%fd3860, %fd3859, %fd3859;
	sub.f64 	%fd3861, %fd4723, %fd4720;
	fma.rn.f64 	%fd3862, %fd3861, %fd3861, %fd3860;
	sub.f64 	%fd3863, %fd4722, %fd4719;
	fma.rn.f64 	%fd3864, %fd3863, %fd3863, %fd3862;
	div.rn.f64 	%fd3865, %fd3858, %fd3864;
	div.rn.f64 	%fd3866, %fd963, %fd3864;
	add.f64 	%fd3867, %fd3866, 0d0000000000000000;
	mov.f64 	%fd3868, 0d0000000000000000;
	mul.f64 	%fd3869, %fd3865, %fd963;
	div.rn.f64 	%fd3870, %fd3869, %fd3864;
	sub.f64 	%fd3871, %fd3868, %fd3870;
	add.f64 	%fd3872, %fd3859, %fd3859;
	add.f64 	%fd3873, %fd3861, %fd3861;
	add.f64 	%fd3874, %fd3863, %fd3863;
	fma.rn.f64 	%fd3875, %fd3872, %fd3871, 0d0000000000000000;
	fma.rn.f64 	%fd3876, %fd3873, %fd3871, 0d0000000000000000;
	fma.rn.f64 	%fd3877, %fd3874, %fd3871, 0d0000000000000000;
	add.f64 	%fd3878, %fd3848, %fd3848;
	add.f64 	%fd3879, %fd3852, %fd3852;
	add.f64 	%fd3880, %fd3855, %fd3855;
	fma.rn.f64 	%fd3881, %fd3878, %fd3867, 0d0000000000000000;
	fma.rn.f64 	%fd3882, %fd3879, %fd3867, 0d0000000000000000;
	fma.rn.f64 	%fd3883, %fd3880, %fd3867, 0d0000000000000000;
	mul.f64 	%fd3884, %fd3845, %fd3883;
	mul.f64 	%fd3885, %fd3842, %fd3882;
	sub.f64 	%fd3886, %fd3884, %fd3885;
	mul.f64 	%fd3887, %fd3842, %fd3881;
	mul.f64 	%fd3888, %fd3849, %fd3883;
	sub.f64 	%fd3889, %fd3887, %fd3888;
	mul.f64 	%fd3890, %fd3849, %fd3882;
	mul.f64 	%fd3891, %fd3845, %fd3881;
	sub.f64 	%fd3892, %fd3890, %fd3891;
	add.f64 	%fd3893, %fd3886, 0d0000000000000000;
	add.f64 	%fd3894, %fd3889, 0d0000000000000000;
	add.f64 	%fd3895, %fd3892, 0d0000000000000000;
	mul.f64 	%fd3896, %fd3843, %fd3883;
	mul.f64 	%fd3897, %fd3846, %fd3882;
	mul.f64 	%fd3898, %fd3846, %fd3881;
	mul.f64 	%fd3899, %fd3841, %fd3883;
	mul.f64 	%fd3900, %fd3841, %fd3882;
	mul.f64 	%fd3901, %fd3843, %fd3881;
	sub.f64 	%fd3902, %fd3897, %fd3896;
	add.f64 	%fd3903, %fd3902, 0d0000000000000000;
	sub.f64 	%fd3904, %fd3899, %fd3898;
	add.f64 	%fd3905, %fd3904, 0d0000000000000000;
	sub.f64 	%fd3906, %fd3901, %fd3900;
	add.f64 	%fd3907, %fd3906, 0d0000000000000000;
	add.f64 	%fd3908, %fd3875, %fd5005;
	add.f64 	%fd3909, %fd3876, %fd5004;
	add.f64 	%fd3910, %fd3877, %fd5003;
	sub.f64 	%fd3911, %fd5002, %fd3875;
	sub.f64 	%fd3912, %fd5001, %fd3876;
	sub.f64 	%fd3913, %fd5000, %fd3877;
	add.f64 	%fd5005, %fd3903, %fd3908;
	add.f64 	%fd5004, %fd3905, %fd3909;
	add.f64 	%fd5003, %fd3907, %fd3910;
	sub.f64 	%fd3914, %fd952, %fd3903;
	sub.f64 	%fd3915, %fd953, %fd3905;
	sub.f64 	%fd3916, %fd954, %fd3907;
	add.f64 	%fd5002, %fd3893, %fd3911;
	add.f64 	%fd5001, %fd3894, %fd3912;
	add.f64 	%fd5000, %fd3895, %fd3913;
	sub.f64 	%fd5008, %fd3914, %fd3893;
	sub.f64 	%fd5007, %fd3915, %fd3894;
	sub.f64 	%fd5006, %fd3916, %fd3895;
	bra.uni 	$L__BB9_396;

$L__BB9_388:
	setp.eq.s32 	%p293, %r1067, 5;
	@%p293 bra 	$L__BB9_390;
	bra.uni 	$L__BB9_389;

$L__BB9_390:
	sub.f64 	%fd3765, %fd4724, %fd4715;
	sub.f64 	%fd3766, %fd4716, %fd4713;
	sub.f64 	%fd3767, %fd4723, %fd4714;
	mul.f64 	%fd3768, %fd3766, %fd3767;
	sub.f64 	%fd3769, %fd4717, %fd4714;
	sub.f64 	%fd3770, %fd4722, %fd4713;
	mul.f64 	%fd3771, %fd3769, %fd3770;
	sub.f64 	%fd3772, %fd3768, %fd3771;
	sub.f64 	%fd3773, %fd4718, %fd4715;
	mul.f64 	%fd3774, %fd3773, %fd3770;
	mul.f64 	%fd3775, %fd3766, %fd3765;
	sub.f64 	%fd3776, %fd3774, %fd3775;
	mul.f64 	%fd3777, %fd3769, %fd3765;
	mul.f64 	%fd3778, %fd3773, %fd3767;
	sub.f64 	%fd3779, %fd3777, %fd3778;
	mul.f64 	%fd3780, %fd3776, %fd3776;
	fma.rn.f64 	%fd3781, %fd3772, %fd3772, %fd3780;
	fma.rn.f64 	%fd3782, %fd3779, %fd3779, %fd3781;
	sub.f64 	%fd3783, %fd4718, %fd4724;
	mul.f64 	%fd3784, %fd3783, %fd3783;
	sub.f64 	%fd3785, %fd4717, %fd4723;
	fma.rn.f64 	%fd3786, %fd3785, %fd3785, %fd3784;
	sub.f64 	%fd3787, %fd4716, %fd4722;
	fma.rn.f64 	%fd3788, %fd3787, %fd3787, %fd3786;
	div.rn.f64 	%fd3789, %fd3782, %fd3788;
	div.rn.f64 	%fd3790, %fd963, %fd3788;
	add.f64 	%fd3791, %fd3790, 0d0000000000000000;
	mov.f64 	%fd3792, 0d0000000000000000;
	mul.f64 	%fd3793, %fd3789, %fd963;
	div.rn.f64 	%fd3794, %fd3793, %fd3788;
	sub.f64 	%fd3795, %fd3792, %fd3794;
	add.f64 	%fd3796, %fd3783, %fd3783;
	add.f64 	%fd3797, %fd3785, %fd3785;
	add.f64 	%fd3798, %fd3787, %fd3787;
	fma.rn.f64 	%fd3799, %fd3796, %fd3795, 0d0000000000000000;
	fma.rn.f64 	%fd3800, %fd3797, %fd3795, 0d0000000000000000;
	fma.rn.f64 	%fd3801, %fd3798, %fd3795, 0d0000000000000000;
	add.f64 	%fd3802, %fd3772, %fd3772;
	add.f64 	%fd3803, %fd3776, %fd3776;
	add.f64 	%fd3804, %fd3779, %fd3779;
	fma.rn.f64 	%fd3805, %fd3802, %fd3791, 0d0000000000000000;
	fma.rn.f64 	%fd3806, %fd3803, %fd3791, 0d0000000000000000;
	fma.rn.f64 	%fd3807, %fd3804, %fd3791, 0d0000000000000000;
	mul.f64 	%fd3808, %fd3769, %fd3807;
	mul.f64 	%fd3809, %fd3766, %fd3806;
	sub.f64 	%fd3810, %fd3808, %fd3809;
	mul.f64 	%fd3811, %fd3766, %fd3805;
	mul.f64 	%fd3812, %fd3773, %fd3807;
	sub.f64 	%fd3813, %fd3811, %fd3812;
	mul.f64 	%fd3814, %fd3773, %fd3806;
	mul.f64 	%fd3815, %fd3769, %fd3805;
	sub.f64 	%fd3816, %fd3814, %fd3815;
	add.f64 	%fd3817, %fd3810, 0d0000000000000000;
	add.f64 	%fd3818, %fd3813, 0d0000000000000000;
	add.f64 	%fd3819, %fd3816, 0d0000000000000000;
	mul.f64 	%fd3820, %fd3767, %fd3807;
	mul.f64 	%fd3821, %fd3770, %fd3806;
	mul.f64 	%fd3822, %fd3770, %fd3805;
	mul.f64 	%fd3823, %fd3765, %fd3807;
	mul.f64 	%fd3824, %fd3765, %fd3806;
	mul.f64 	%fd3825, %fd3767, %fd3805;
	sub.f64 	%fd3826, %fd3821, %fd3820;
	add.f64 	%fd3827, %fd3826, 0d0000000000000000;
	sub.f64 	%fd3828, %fd3823, %fd3822;
	add.f64 	%fd3829, %fd3828, 0d0000000000000000;
	sub.f64 	%fd3830, %fd3825, %fd3824;
	add.f64 	%fd3831, %fd3830, 0d0000000000000000;
	add.f64 	%fd3832, %fd3799, %fd4999;
	add.f64 	%fd3833, %fd3800, %fd4998;
	add.f64 	%fd3834, %fd3801, %fd4997;
	sub.f64 	%fd3835, %fd5005, %fd3799;
	sub.f64 	%fd3836, %fd5004, %fd3800;
	sub.f64 	%fd3837, %fd5003, %fd3801;
	add.f64 	%fd4999, %fd3827, %fd3832;
	add.f64 	%fd4998, %fd3829, %fd3833;
	add.f64 	%fd4997, %fd3831, %fd3834;
	sub.f64 	%fd3838, %fd952, %fd3827;
	sub.f64 	%fd3839, %fd953, %fd3829;
	sub.f64 	%fd3840, %fd954, %fd3831;
	add.f64 	%fd5005, %fd3817, %fd3835;
	add.f64 	%fd5004, %fd3818, %fd3836;
	add.f64 	%fd5003, %fd3819, %fd3837;
	sub.f64 	%fd5008, %fd3838, %fd3817;
	sub.f64 	%fd5007, %fd3839, %fd3818;
	sub.f64 	%fd5006, %fd3840, %fd3819;
	bra.uni 	$L__BB9_396;

$L__BB9_389:
	sub.f64 	%fd4607, %fd4721, %fd4718;
	sub.f64 	%fd4606, %fd4719, %fd4716;
	sub.f64 	%fd4605, %fd4720, %fd4717;
	sub.f64 	%fd4604, %fd4724, %fd4718;
	sub.f64 	%fd4603, %fd4722, %fd4716;
	sub.f64 	%fd4602, %fd4723, %fd4717;
	sub.f64 	%fd4601, %fd4713, %fd4716;
	sub.f64 	%fd4600, %fd4715, %fd4718;
	sub.f64 	%fd4599, %fd4714, %fd4717;
	mul.f64 	%fd3707, %fd4599, %fd972;
	fma.rn.f64 	%fd3708, %fd4600, %fd969, %fd3707;
	fma.rn.f64 	%fd3709, %fd4601, %fd973, %fd3708;
	mul.f64 	%fd3710, %fd3709, %fd3709;
	mul.f64 	%fd3711, %fd972, %fd972;
	fma.rn.f64 	%fd3712, %fd969, %fd969, %fd3711;
	fma.rn.f64 	%fd3713, %fd973, %fd973, %fd3712;
	div.rn.f64 	%fd3714, %fd3710, %fd3713;
	div.rn.f64 	%fd3715, %fd963, %fd3713;
	add.f64 	%fd3716, %fd3715, 0d0000000000000000;
	mov.f64 	%fd3717, 0d0000000000000000;
	mul.f64 	%fd3718, %fd3714, %fd963;
	div.rn.f64 	%fd3719, %fd3718, %fd3713;
	sub.f64 	%fd3720, %fd3717, %fd3719;
	add.f64 	%fd3721, %fd969, %fd969;
	add.f64 	%fd3722, %fd972, %fd972;
	add.f64 	%fd3723, %fd973, %fd973;
	fma.rn.f64 	%fd3724, %fd3721, %fd3720, 0d0000000000000000;
	fma.rn.f64 	%fd3725, %fd3722, %fd3720, 0d0000000000000000;
	fma.rn.f64 	%fd3726, %fd3723, %fd3720, 0d0000000000000000;
	fma.rn.f64 	%fd3727, %fd3709, %fd3716, 0d0000000000000000;
	fma.rn.f64 	%fd3728, %fd3709, %fd3716, %fd3727;
	fma.rn.f64 	%fd3729, %fd969, %fd3728, 0d0000000000000000;
	fma.rn.f64 	%fd3730, %fd972, %fd3728, 0d0000000000000000;
	fma.rn.f64 	%fd3731, %fd973, %fd3728, 0d0000000000000000;
	fma.rn.f64 	%fd3732, %fd4600, %fd3728, %fd3724;
	fma.rn.f64 	%fd3733, %fd4599, %fd3728, %fd3725;
	fma.rn.f64 	%fd3734, %fd4601, %fd3728, %fd3726;
	mul.f64 	%fd3735, %fd4602, %fd3734;
	mul.f64 	%fd3736, %fd4603, %fd3733;
	sub.f64 	%fd3737, %fd3735, %fd3736;
	mul.f64 	%fd3738, %fd4603, %fd3732;
	mul.f64 	%fd3739, %fd4604, %fd3734;
	sub.f64 	%fd3740, %fd3738, %fd3739;
	mul.f64 	%fd3741, %fd4604, %fd3733;
	mul.f64 	%fd3742, %fd4602, %fd3732;
	sub.f64 	%fd3743, %fd3741, %fd3742;
	add.f64 	%fd3744, %fd3737, 0d0000000000000000;
	add.f64 	%fd3745, %fd3740, 0d0000000000000000;
	add.f64 	%fd3746, %fd3743, 0d0000000000000000;
	mul.f64 	%fd3747, %fd4605, %fd3734;
	mul.f64 	%fd3748, %fd4606, %fd3733;
	mul.f64 	%fd3749, %fd4606, %fd3732;
	mul.f64 	%fd3750, %fd4607, %fd3734;
	mul.f64 	%fd3751, %fd4607, %fd3733;
	mul.f64 	%fd3752, %fd4605, %fd3732;
	sub.f64 	%fd3753, %fd3748, %fd3747;
	add.f64 	%fd3754, %fd3753, 0d0000000000000000;
	sub.f64 	%fd3755, %fd3750, %fd3749;
	add.f64 	%fd3756, %fd3755, 0d0000000000000000;
	sub.f64 	%fd3757, %fd3752, %fd3751;
	add.f64 	%fd3758, %fd3757, 0d0000000000000000;
	add.f64 	%fd5008, %fd952, %fd3729;
	add.f64 	%fd5007, %fd953, %fd3730;
	add.f64 	%fd5006, %fd954, %fd3731;
	sub.f64 	%fd3759, %fd4999, %fd3729;
	sub.f64 	%fd3760, %fd4998, %fd3730;
	sub.f64 	%fd3761, %fd4997, %fd3731;
	add.f64 	%fd5005, %fd3754, %fd5005;
	add.f64 	%fd5004, %fd3756, %fd5004;
	add.f64 	%fd5003, %fd3758, %fd5003;
	sub.f64 	%fd3762, %fd3759, %fd3754;
	sub.f64 	%fd3763, %fd3760, %fd3756;
	sub.f64 	%fd3764, %fd3761, %fd3758;
	add.f64 	%fd5002, %fd3744, %fd5002;
	add.f64 	%fd5001, %fd3745, %fd5001;
	add.f64 	%fd5000, %fd3746, %fd5000;
	sub.f64 	%fd4999, %fd3762, %fd3744;
	sub.f64 	%fd4998, %fd3763, %fd3745;
	sub.f64 	%fd4997, %fd3764, %fd3746;

$L__BB9_396:
	mov.f64 	%fd4019, 0d0000000000000000;
	mov.f64 	%fd5038, %fd4019;
	mov.f64 	%fd5039, %fd4019;
	mov.f64 	%fd5040, %fd4019;
	mov.f64 	%fd5041, %fd4019;
	mov.f64 	%fd5042, %fd4019;
	mov.f64 	%fd5043, %fd4019;
	mov.f64 	%fd5044, %fd4019;
	mov.f64 	%fd5045, %fd4019;
	mov.f64 	%fd5046, %fd4019;
	@%p5 bra 	$L__BB9_400;

	sub.f64 	%fd1059, %fd4724, %fd4721;
	sub.f64 	%fd1060, %fd4723, %fd4720;
	mul.f64 	%fd4028, %fd1060, %fd973;
	sub.f64 	%fd1061, %fd4722, %fd4719;
	mul.f64 	%fd4029, %fd1061, %fd972;
	sub.f64 	%fd1062, %fd4028, %fd4029;
	mul.f64 	%fd4030, %fd1061, %fd969;
	mul.f64 	%fd4031, %fd1059, %fd973;
	sub.f64 	%fd1063, %fd4030, %fd4031;
	mul.f64 	%fd4032, %fd1059, %fd972;
	mul.f64 	%fd4033, %fd1060, %fd969;
	sub.f64 	%fd1064, %fd4032, %fd4033;
	mul.f64 	%fd4034, %fd1059, %fd1059;
	fma.rn.f64 	%fd4035, %fd1060, %fd1060, %fd4034;
	fma.rn.f64 	%fd1065, %fd1061, %fd1061, %fd4035;
	mul.f64 	%fd4036, %fd1060, %fd1063;
	fma.rn.f64 	%fd4037, %fd1059, %fd1062, %fd4036;
	fma.rn.f64 	%fd4038, %fd1061, %fd1064, %fd4037;
	mul.f64 	%fd4039, %fd1063, %fd1063;
	fma.rn.f64 	%fd4040, %fd1062, %fd1062, %fd4039;
	fma.rn.f64 	%fd4041, %fd1064, %fd1064, %fd4040;
	sub.f64 	%fd1066, %fd4715, %fd4721;
	mul.f64 	%fd4042, %fd1066, %fd1059;
	sub.f64 	%fd1067, %fd4714, %fd4720;
	fma.rn.f64 	%fd4043, %fd1067, %fd1060, %fd4042;
	sub.f64 	%fd1068, %fd4713, %fd4719;
	fma.rn.f64 	%fd1069, %fd1068, %fd1061, %fd4043;
	mul.f64 	%fd4044, %fd1067, %fd1063;
	fma.rn.f64 	%fd4045, %fd1066, %fd1062, %fd4044;
	fma.rn.f64 	%fd4046, %fd1068, %fd1064, %fd4045;
	div.rn.f64 	%fd1070, %fd4038, %fd1065;
	mul.f64 	%fd1071, %fd1070, %fd1070;
	mul.f64 	%fd4047, %fd1065, %fd1071;
	sub.f64 	%fd1072, %fd4041, %fd4047;
	mul.f64 	%fd4048, %fd1069, %fd1070;
	sub.f64 	%fd4049, %fd4046, %fd4048;
	div.rn.f64 	%fd1073, %fd4049, %fd1072;
	mul.f64 	%fd1074, %fd1065, %fd1070;
	mul.f64 	%fd4050, %fd1074, %fd1073;
	sub.f64 	%fd4051, %fd1069, %fd4050;
	div.rn.f64 	%fd1075, %fd4051, %fd1065;
	setp.gt.f64 	%p294, %fd1075, 0d0000000000000000;
	mov.f64 	%fd4027, 0d0000000000000000;
	setp.lt.f64 	%p295, %fd1075, 0d3FF0000000000000;
	setp.ge.f64 	%p296, %fd1073, 0d0000000000000000;
	and.pred  	%p297, %p294, %p295;
	and.pred  	%p298, %p296, %p297;
	mov.f64 	%fd5018, %fd4027;
	mov.f64 	%fd5019, %fd4027;
	mov.f64 	%fd5020, %fd4027;
	mov.f64 	%fd5021, %fd4027;
	mov.f64 	%fd5022, %fd4027;
	mov.f64 	%fd5023, %fd4027;
	mov.f64 	%fd5024, %fd4027;
	mov.f64 	%fd5025, %fd4027;
	@%p298 bra 	$L__BB9_399;

	sub.f64 	%fd4052, %fd4718, %fd4724;
	sub.f64 	%fd4053, %fd4717, %fd4723;
	mul.f64 	%fd4054, %fd4053, %fd973;
	sub.f64 	%fd4055, %fd4716, %fd4722;
	mul.f64 	%fd4056, %fd4055, %fd972;
	sub.f64 	%fd4057, %fd4054, %fd4056;
	mul.f64 	%fd4058, %fd4055, %fd969;
	mul.f64 	%fd4059, %fd4052, %fd973;
	sub.f64 	%fd4060, %fd4058, %fd4059;
	mul.f64 	%fd4061, %fd4052, %fd972;
	mul.f64 	%fd4062, %fd4053, %fd969;
	sub.f64 	%fd4063, %fd4061, %fd4062;
	mul.f64 	%fd4064, %fd4052, %fd4052;
	fma.rn.f64 	%fd4065, %fd4053, %fd4053, %fd4064;
	fma.rn.f64 	%fd4066, %fd4055, %fd4055, %fd4065;
	mul.f64 	%fd4067, %fd4053, %fd4060;
	fma.rn.f64 	%fd4068, %fd4052, %fd4057, %fd4067;
	fma.rn.f64 	%fd4069, %fd4055, %fd4063, %fd4068;
	mul.f64 	%fd4070, %fd4060, %fd4060;
	fma.rn.f64 	%fd4071, %fd4057, %fd4057, %fd4070;
	fma.rn.f64 	%fd4072, %fd4063, %fd4063, %fd4071;
	sub.f64 	%fd4073, %fd4715, %fd4724;
	mul.f64 	%fd4074, %fd4052, %fd4073;
	sub.f64 	%fd4075, %fd4714, %fd4723;
	fma.rn.f64 	%fd4076, %fd4053, %fd4075, %fd4074;
	sub.f64 	%fd4077, %fd4713, %fd4722;
	fma.rn.f64 	%fd4078, %fd4055, %fd4077, %fd4076;
	mul.f64 	%fd4079, %fd4075, %fd4060;
	fma.rn.f64 	%fd4080, %fd4073, %fd4057, %fd4079;
	fma.rn.f64 	%fd4081, %fd4077, %fd4063, %fd4080;
	div.rn.f64 	%fd4082, %fd4069, %fd4066;
	mul.f64 	%fd4083, %fd4082, %fd4082;
	mul.f64 	%fd4084, %fd4066, %fd4083;
	sub.f64 	%fd4085, %fd4072, %fd4084;
	mul.f64 	%fd4086, %fd4078, %fd4082;
	sub.f64 	%fd4087, %fd4081, %fd4086;
	div.rn.f64 	%fd4088, %fd4087, %fd4085;
	mul.f64 	%fd4089, %fd4066, %fd4082;
	mul.f64 	%fd4090, %fd4089, %fd4088;
	sub.f64 	%fd4091, %fd4078, %fd4090;
	div.rn.f64 	%fd4092, %fd4091, %fd4066;
	mov.f64 	%fd4093, 0d0000000000000000;
	div.rn.f64 	%fd4094, %fd4093, %fd4066;
	add.f64 	%fd4095, %fd4094, 0d0000000000000000;
	mul.f64 	%fd4096, %fd4092, 0d0000000000000000;
	div.rn.f64 	%fd4097, %fd4096, %fd4066;
	sub.f64 	%fd4098, %fd4093, %fd4097;
	sub.f64 	%fd4099, %fd4093, %fd4095;
	fma.rn.f64 	%fd4100, %fd4099, %fd4088, 0d0000000000000000;
	fma.rn.f64 	%fd4101, %fd4099, %fd4089, 0d0000000000000000;
	fma.rn.f64 	%fd4102, %fd4082, %fd4100, %fd4098;
	fma.rn.f64 	%fd4103, %fd4066, %fd4100, 0d0000000000000000;
	div.rn.f64 	%fd4104, %fd4101, %fd4085;
	add.f64 	%fd5019, %fd4104, 0d0000000000000000;
	mul.f64 	%fd4105, %fd4101, %fd4088;
	div.rn.f64 	%fd4106, %fd4105, %fd4085;
	sub.f64 	%fd4107, %fd4093, %fd4106;
	sub.f64 	%fd4108, %fd4093, %fd5019;
	fma.rn.f64 	%fd4109, %fd4078, %fd4108, %fd4103;
	fma.rn.f64 	%fd4110, %fd4082, %fd4108, %fd4095;
	add.f64 	%fd5018, %fd4110, 0d0000000000000000;
	add.f64 	%fd5022, %fd4107, 0d0000000000000000;
	sub.f64 	%fd4111, %fd4093, %fd4107;
	fma.rn.f64 	%fd4112, %fd4066, %fd4111, 0d0000000000000000;
	fma.rn.f64 	%fd4113, %fd4083, %fd4111, %fd4102;
	fma.rn.f64 	%fd4114, %fd4082, %fd4112, %fd4109;
	fma.rn.f64 	%fd4115, %fd4082, %fd4112, %fd4114;
	div.rn.f64 	%fd4116, %fd4115, %fd4066;
	add.f64 	%fd5021, %fd4116, 0d0000000000000000;
	mul.f64 	%fd4117, %fd4082, %fd4115;
	div.rn.f64 	%fd4118, %fd4117, %fd4066;
	sub.f64 	%fd4119, %fd4113, %fd4118;
	add.f64 	%fd5020, %fd4119, 0d0000000000000000;
	fma.rn.f64 	%fd4120, %fd4073, %fd5019, 0d0000000000000000;
	fma.rn.f64 	%fd4121, %fd4075, %fd5019, 0d0000000000000000;
	fma.rn.f64 	%fd4122, %fd4077, %fd5019, 0d0000000000000000;
	fma.rn.f64 	%fd4123, %fd4057, %fd5019, 0d0000000000000000;
	fma.rn.f64 	%fd4124, %fd4060, %fd5019, 0d0000000000000000;
	fma.rn.f64 	%fd4125, %fd4063, %fd5019, 0d0000000000000000;
	fma.rn.f64 	%fd4126, %fd4073, %fd5018, 0d0000000000000000;
	fma.rn.f64 	%fd4127, %fd4075, %fd5018, 0d0000000000000000;
	fma.rn.f64 	%fd4128, %fd4077, %fd5018, 0d0000000000000000;
	fma.rn.f64 	%fd4129, %fd4052, %fd5018, %fd4123;
	fma.rn.f64 	%fd4130, %fd4053, %fd5018, %fd4124;
	fma.rn.f64 	%fd4131, %fd4055, %fd5018, %fd4125;
	add.f64 	%fd5008, %fd4129, %fd5008;
	add.f64 	%fd5007, %fd4130, %fd5007;
	add.f64 	%fd5006, %fd4131, %fd5006;
	sub.f64 	%fd4132, %fd5005, %fd4129;
	sub.f64 	%fd4133, %fd5004, %fd4130;
	sub.f64 	%fd4134, %fd5003, %fd4131;
	fma.rn.f64 	%fd4135, %fd4063, %fd5022, 0d0000000000000000;
	add.f64 	%fd4136, %fd4122, %fd4135;
	add.f64 	%fd4137, %fd4135, %fd4136;
	fma.rn.f64 	%fd4138, %fd4060, %fd5022, 0d0000000000000000;
	add.f64 	%fd4139, %fd4121, %fd4138;
	add.f64 	%fd4140, %fd4138, %fd4139;
	fma.rn.f64 	%fd4141, %fd4057, %fd5022, 0d0000000000000000;
	add.f64 	%fd4142, %fd4120, %fd4141;
	add.f64 	%fd4143, %fd4141, %fd4142;
	fma.rn.f64 	%fd4144, %fd4063, %fd5021, 0d0000000000000000;
	fma.rn.f64 	%fd4145, %fd4055, %fd5021, 0d0000000000000000;
	add.f64 	%fd4146, %fd4137, %fd4145;
	add.f64 	%fd4147, %fd4128, %fd4144;
	fma.rn.f64 	%fd4148, %fd4060, %fd5021, 0d0000000000000000;
	fma.rn.f64 	%fd4149, %fd4053, %fd5021, 0d0000000000000000;
	add.f64 	%fd4150, %fd4140, %fd4149;
	add.f64 	%fd4151, %fd4127, %fd4148;
	fma.rn.f64 	%fd4152, %fd4057, %fd5021, 0d0000000000000000;
	fma.rn.f64 	%fd4153, %fd4052, %fd5021, 0d0000000000000000;
	add.f64 	%fd4154, %fd4143, %fd4153;
	add.f64 	%fd4155, %fd4126, %fd4152;
	fma.rn.f64 	%fd4156, %fd4055, %fd5020, 0d0000000000000000;
	add.f64 	%fd4157, %fd4147, %fd4156;
	add.f64 	%fd4158, %fd4156, %fd4157;
	fma.rn.f64 	%fd4159, %fd4053, %fd5020, 0d0000000000000000;
	add.f64 	%fd4160, %fd4151, %fd4159;
	add.f64 	%fd4161, %fd4159, %fd4160;
	fma.rn.f64 	%fd4162, %fd4052, %fd5020, 0d0000000000000000;
	add.f64 	%fd4163, %fd4155, %fd4162;
	add.f64 	%fd4164, %fd4162, %fd4163;
	mul.f64 	%fd4165, %fd972, %fd4146;
	mul.f64 	%fd4166, %fd973, %fd4150;
	sub.f64 	%fd4167, %fd4165, %fd4166;
	mul.f64 	%fd4168, %fd973, %fd4154;
	mul.f64 	%fd4169, %fd969, %fd4146;
	sub.f64 	%fd4170, %fd4168, %fd4169;
	mul.f64 	%fd4171, %fd969, %fd4150;
	mul.f64 	%fd4172, %fd972, %fd4154;
	sub.f64 	%fd4173, %fd4171, %fd4172;
	add.f64 	%fd4174, %fd4167, %fd4164;
	add.f64 	%fd4175, %fd4170, %fd4161;
	add.f64 	%fd4176, %fd4173, %fd4158;
	mul.f64 	%fd4177, %fd4053, %fd4146;
	mul.f64 	%fd4178, %fd4055, %fd4150;
	mul.f64 	%fd4179, %fd4055, %fd4154;
	mul.f64 	%fd4180, %fd4052, %fd4146;
	mul.f64 	%fd4181, %fd4052, %fd4150;
	mul.f64 	%fd4182, %fd4053, %fd4154;
	sub.f64 	%fd4183, %fd4178, %fd4177;
	add.f64 	%fd5023, %fd4183, 0d0000000000000000;
	sub.f64 	%fd4184, %fd4180, %fd4179;
	add.f64 	%fd5024, %fd4184, 0d0000000000000000;
	sub.f64 	%fd4185, %fd4182, %fd4181;
	add.f64 	%fd5025, %fd4185, 0d0000000000000000;
	add.f64 	%fd4999, %fd4174, %fd4999;
	add.f64 	%fd4998, %fd4175, %fd4998;
	add.f64 	%fd4997, %fd4176, %fd4997;
	sub.f64 	%fd5005, %fd4132, %fd4174;
	sub.f64 	%fd5004, %fd4133, %fd4175;
	sub.f64 	%fd5003, %fd4134, %fd4176;

$L__BB9_399:
	div.rn.f64 	%fd4187, %fd4027, %fd1065;
	add.f64 	%fd4188, %fd4187, 0d0000000000000000;
	mul.f64 	%fd4189, %fd1075, 0d0000000000000000;
	div.rn.f64 	%fd4190, %fd4189, %fd1065;
	sub.f64 	%fd4191, %fd4027, %fd4190;
	sub.f64 	%fd4192, %fd4027, %fd4188;
	fma.rn.f64 	%fd4193, %fd4192, %fd1073, 0d0000000000000000;
	fma.rn.f64 	%fd4194, %fd4192, %fd1074, 0d0000000000000000;
	fma.rn.f64 	%fd4195, %fd1070, %fd4193, %fd4191;
	fma.rn.f64 	%fd4196, %fd1065, %fd4193, 0d0000000000000000;
	div.rn.f64 	%fd4197, %fd4194, %fd1072;
	add.f64 	%fd4198, %fd4197, 0d0000000000000000;
	mul.f64 	%fd4199, %fd4194, %fd1073;
	div.rn.f64 	%fd4200, %fd4199, %fd1072;
	sub.f64 	%fd4201, %fd4027, %fd4200;
	sub.f64 	%fd4202, %fd4027, %fd4198;
	fma.rn.f64 	%fd4203, %fd1069, %fd4202, %fd4196;
	fma.rn.f64 	%fd4204, %fd1070, %fd4202, %fd4188;
	add.f64 	%fd4205, %fd4204, 0d0000000000000000;
	add.f64 	%fd4206, %fd4201, 0d0000000000000000;
	sub.f64 	%fd4207, %fd4027, %fd4201;
	fma.rn.f64 	%fd4208, %fd1065, %fd4207, 0d0000000000000000;
	fma.rn.f64 	%fd4209, %fd1071, %fd4207, %fd4195;
	fma.rn.f64 	%fd4210, %fd1070, %fd4208, %fd4203;
	fma.rn.f64 	%fd4211, %fd1070, %fd4208, %fd4210;
	div.rn.f64 	%fd4212, %fd4211, %fd1065;
	add.f64 	%fd4213, %fd4212, 0d0000000000000000;
	mul.f64 	%fd4214, %fd1070, %fd4211;
	div.rn.f64 	%fd4215, %fd4214, %fd1065;
	sub.f64 	%fd4216, %fd4209, %fd4215;
	add.f64 	%fd4217, %fd4216, 0d0000000000000000;
	add.f64 	%fd5039, %fd4198, %fd5019;
	add.f64 	%fd5038, %fd4205, %fd5018;
	add.f64 	%fd5043, %fd4206, %fd5022;
	add.f64 	%fd5041, %fd4213, %fd5021;
	add.f64 	%fd5040, %fd4217, %fd5020;
	add.f64 	%fd4218, %fd5039, 0d0000000000000000;
	fma.rn.f64 	%fd4219, %fd1066, %fd4218, 0d0000000000000000;
	fma.rn.f64 	%fd4220, %fd1067, %fd4218, 0d0000000000000000;
	fma.rn.f64 	%fd4221, %fd1068, %fd4218, 0d0000000000000000;
	fma.rn.f64 	%fd4222, %fd1062, %fd4218, 0d0000000000000000;
	fma.rn.f64 	%fd4223, %fd1063, %fd4218, 0d0000000000000000;
	fma.rn.f64 	%fd4224, %fd1064, %fd4218, 0d0000000000000000;
	add.f64 	%fd4225, %fd5038, 0d0000000000000000;
	fma.rn.f64 	%fd4226, %fd1066, %fd4225, 0d0000000000000000;
	fma.rn.f64 	%fd4227, %fd1067, %fd4225, 0d0000000000000000;
	fma.rn.f64 	%fd4228, %fd1068, %fd4225, 0d0000000000000000;
	fma.rn.f64 	%fd4229, %fd1059, %fd4225, %fd4222;
	fma.rn.f64 	%fd4230, %fd1060, %fd4225, %fd4223;
	fma.rn.f64 	%fd4231, %fd1061, %fd4225, %fd4224;
	add.f64 	%fd5008, %fd5008, %fd4229;
	add.f64 	%fd5007, %fd5007, %fd4230;
	add.f64 	%fd5006, %fd5006, %fd4231;
	sub.f64 	%fd4232, %fd5002, %fd4229;
	sub.f64 	%fd4233, %fd5001, %fd4230;
	sub.f64 	%fd4234, %fd5000, %fd4231;
	add.f64 	%fd4235, %fd5043, 0d0000000000000000;
	fma.rn.f64 	%fd4236, %fd1064, %fd4235, 0d0000000000000000;
	add.f64 	%fd4237, %fd4221, %fd4236;
	add.f64 	%fd4238, %fd4236, %fd4237;
	fma.rn.f64 	%fd4239, %fd1063, %fd4235, 0d0000000000000000;
	add.f64 	%fd4240, %fd4220, %fd4239;
	add.f64 	%fd4241, %fd4239, %fd4240;
	fma.rn.f64 	%fd4242, %fd1062, %fd4235, 0d0000000000000000;
	add.f64 	%fd4243, %fd4219, %fd4242;
	add.f64 	%fd4244, %fd4242, %fd4243;
	add.f64 	%fd4245, %fd5041, 0d0000000000000000;
	add.f64 	%fd5042, %fd5021, %fd4245;
	add.f64 	%fd4246, %fd5042, 0d0000000000000000;
	fma.rn.f64 	%fd4247, %fd1064, %fd4246, 0d0000000000000000;
	fma.rn.f64 	%fd4248, %fd1061, %fd4246, 0d0000000000000000;
	add.f64 	%fd4249, %fd4248, %fd4238;
	add.f64 	%fd4250, %fd4228, %fd4247;
	fma.rn.f64 	%fd4251, %fd1063, %fd4246, 0d0000000000000000;
	fma.rn.f64 	%fd4252, %fd1060, %fd4246, 0d0000000000000000;
	add.f64 	%fd4253, %fd4252, %fd4241;
	add.f64 	%fd4254, %fd4227, %fd4251;
	fma.rn.f64 	%fd4255, %fd1062, %fd4246, 0d0000000000000000;
	fma.rn.f64 	%fd4256, %fd1059, %fd4246, 0d0000000000000000;
	add.f64 	%fd4257, %fd4256, %fd4244;
	add.f64 	%fd4258, %fd4226, %fd4255;
	add.f64 	%fd4259, %fd5040, 0d0000000000000000;
	fma.rn.f64 	%fd4260, %fd1061, %fd4259, 0d0000000000000000;
	add.f64 	%fd4261, %fd4260, %fd4250;
	add.f64 	%fd4262, %fd4260, %fd4261;
	fma.rn.f64 	%fd4263, %fd1060, %fd4259, 0d0000000000000000;
	add.f64 	%fd4264, %fd4263, %fd4254;
	add.f64 	%fd4265, %fd4263, %fd4264;
	fma.rn.f64 	%fd4266, %fd1059, %fd4259, 0d0000000000000000;
	add.f64 	%fd4267, %fd4266, %fd4258;
	add.f64 	%fd4268, %fd4266, %fd4267;
	mul.f64 	%fd4269, %fd972, %fd4249;
	mul.f64 	%fd4270, %fd973, %fd4253;
	sub.f64 	%fd4271, %fd4269, %fd4270;
	mul.f64 	%fd4272, %fd973, %fd4257;
	mul.f64 	%fd4273, %fd969, %fd4249;
	sub.f64 	%fd4274, %fd4272, %fd4273;
	mul.f64 	%fd4275, %fd969, %fd4253;
	mul.f64 	%fd4276, %fd972, %fd4257;
	sub.f64 	%fd4277, %fd4275, %fd4276;
	add.f64 	%fd4278, %fd4268, %fd4271;
	add.f64 	%fd4279, %fd4265, %fd4274;
	add.f64 	%fd4280, %fd4262, %fd4277;
	mul.f64 	%fd4281, %fd1060, %fd4249;
	mul.f64 	%fd4282, %fd1061, %fd4253;
	sub.f64 	%fd4283, %fd4281, %fd4282;
	mul.f64 	%fd4284, %fd1061, %fd4257;
	mul.f64 	%fd4285, %fd1059, %fd4249;
	sub.f64 	%fd4286, %fd4284, %fd4285;
	mul.f64 	%fd4287, %fd1059, %fd4253;
	mul.f64 	%fd4288, %fd1060, %fd4257;
	sub.f64 	%fd4289, %fd4287, %fd4288;
	sub.f64 	%fd5044, %fd5023, %fd4283;
	sub.f64 	%fd5045, %fd5024, %fd4286;
	sub.f64 	%fd5046, %fd5025, %fd4289;
	add.f64 	%fd5005, %fd4278, %fd5005;
	add.f64 	%fd5004, %fd4279, %fd5004;
	add.f64 	%fd5003, %fd4280, %fd5003;
	sub.f64 	%fd5002, %fd4232, %fd4278;
	sub.f64 	%fd5001, %fd4233, %fd4279;
	sub.f64 	%fd5000, %fd4234, %fd4280;

$L__BB9_400:
	sub.f64 	%fd4616, %fd4721, %fd4718;
	sub.f64 	%fd4615, %fd4719, %fd4716;
	sub.f64 	%fd4614, %fd4720, %fd4717;
	sub.f64 	%fd4613, %fd4724, %fd4718;
	sub.f64 	%fd4612, %fd4722, %fd4716;
	sub.f64 	%fd4611, %fd4723, %fd4717;
	sub.f64 	%fd4610, %fd4713, %fd4716;
	sub.f64 	%fd4609, %fd4715, %fd4718;
	sub.f64 	%fd4608, %fd4714, %fd4717;
	mul.f64 	%fd4562, %fd4614, %fd969;
	mul.f64 	%fd4561, %fd4616, %fd972;
	sub.f64 	%fd4560, %fd4561, %fd4562;
	mul.f64 	%fd4559, %fd4616, %fd973;
	mul.f64 	%fd4558, %fd4615, %fd969;
	sub.f64 	%fd4557, %fd4558, %fd4559;
	mul.f64 	%fd4556, %fd4615, %fd972;
	mul.f64 	%fd4555, %fd4614, %fd973;
	sub.f64 	%fd4554, %fd4555, %fd4556;
	mul.f64 	%fd4553, %fd982, %fd982;
	div.rn.f64 	%fd4291, %fd4019, %fd977;
	add.f64 	%fd4292, %fd4291, 0d0000000000000000;
	mul.f64 	%fd4293, %fd987, 0d0000000000000000;
	div.rn.f64 	%fd4294, %fd4293, %fd977;
	sub.f64 	%fd4295, %fd4019, %fd4294;
	sub.f64 	%fd4296, %fd4019, %fd4292;
	fma.rn.f64 	%fd4297, %fd4296, %fd985, 0d0000000000000000;
	fma.rn.f64 	%fd4298, %fd4296, %fd986, 0d0000000000000000;
	fma.rn.f64 	%fd4299, %fd982, %fd4297, %fd4295;
	fma.rn.f64 	%fd4300, %fd977, %fd4297, 0d0000000000000000;
	div.rn.f64 	%fd4301, %fd4298, %fd984;
	add.f64 	%fd4302, %fd4301, 0d0000000000000000;
	mul.f64 	%fd4303, %fd4298, %fd985;
	div.rn.f64 	%fd4304, %fd4303, %fd984;
	sub.f64 	%fd4305, %fd4019, %fd4304;
	sub.f64 	%fd4306, %fd4019, %fd4302;
	fma.rn.f64 	%fd4307, %fd981, %fd4306, %fd4300;
	fma.rn.f64 	%fd4308, %fd982, %fd4306, %fd4292;
	add.f64 	%fd4309, %fd4308, 0d0000000000000000;
	add.f64 	%fd4310, %fd4305, 0d0000000000000000;
	sub.f64 	%fd4311, %fd4019, %fd4305;
	fma.rn.f64 	%fd4312, %fd977, %fd4311, 0d0000000000000000;
	fma.rn.f64 	%fd4313, %fd4553, %fd4311, %fd4299;
	fma.rn.f64 	%fd4314, %fd982, %fd4312, %fd4307;
	fma.rn.f64 	%fd4315, %fd982, %fd4312, %fd4314;
	div.rn.f64 	%fd4316, %fd4315, %fd977;
	add.f64 	%fd4317, %fd4316, 0d0000000000000000;
	mul.f64 	%fd4318, %fd982, %fd4315;
	div.rn.f64 	%fd4319, %fd4318, %fd977;
	sub.f64 	%fd4320, %fd4313, %fd4319;
	add.f64 	%fd4321, %fd4320, 0d0000000000000000;
	add.f64 	%fd4322, %fd4302, %fd5039;
	add.f64 	%fd4323, %fd4309, %fd5038;
	add.f64 	%fd4324, %fd4310, %fd5043;
	add.f64 	%fd4325, %fd4317, %fd5041;
	add.f64 	%fd4326, %fd4321, %fd5040;
	add.f64 	%fd4327, %fd4322, 0d0000000000000000;
	fma.rn.f64 	%fd4328, %fd4609, %fd4327, 0d0000000000000000;
	fma.rn.f64 	%fd4329, %fd4608, %fd4327, 0d0000000000000000;
	fma.rn.f64 	%fd4330, %fd4610, %fd4327, 0d0000000000000000;
	fma.rn.f64 	%fd4331, %fd4554, %fd4327, 0d0000000000000000;
	fma.rn.f64 	%fd4332, %fd4557, %fd4327, 0d0000000000000000;
	fma.rn.f64 	%fd4333, %fd4560, %fd4327, 0d0000000000000000;
	add.f64 	%fd4334, %fd4323, 0d0000000000000000;
	fma.rn.f64 	%fd4335, %fd4609, %fd4334, 0d0000000000000000;
	fma.rn.f64 	%fd4336, %fd4608, %fd4334, 0d0000000000000000;
	fma.rn.f64 	%fd4337, %fd4610, %fd4334, 0d0000000000000000;
	fma.rn.f64 	%fd4338, %fd4616, %fd4334, %fd4331;
	fma.rn.f64 	%fd4339, %fd4614, %fd4334, %fd4332;
	fma.rn.f64 	%fd4340, %fd4615, %fd4334, %fd4333;
	add.f64 	%fd1149, %fd5008, %fd4338;
	add.f64 	%fd1150, %fd5007, %fd4339;
	add.f64 	%fd1151, %fd5006, %fd4340;
	sub.f64 	%fd4341, %fd4999, %fd4338;
	sub.f64 	%fd4342, %fd4998, %fd4339;
	sub.f64 	%fd4343, %fd4997, %fd4340;
	add.f64 	%fd4344, %fd4324, 0d0000000000000000;
	fma.rn.f64 	%fd4345, %fd4560, %fd4344, 0d0000000000000000;
	add.f64 	%fd4346, %fd4330, %fd4345;
	add.f64 	%fd4347, %fd4345, %fd4346;
	fma.rn.f64 	%fd4348, %fd4557, %fd4344, 0d0000000000000000;
	add.f64 	%fd4349, %fd4329, %fd4348;
	add.f64 	%fd4350, %fd4348, %fd4349;
	fma.rn.f64 	%fd4351, %fd4554, %fd4344, 0d0000000000000000;
	add.f64 	%fd4352, %fd4328, %fd4351;
	add.f64 	%fd4353, %fd4351, %fd4352;
	add.f64 	%fd4354, %fd4325, 0d0000000000000000;
	add.f64 	%fd4355, %fd5042, %fd4354;
	add.f64 	%fd4356, %fd4355, 0d0000000000000000;
	fma.rn.f64 	%fd4357, %fd4560, %fd4356, 0d0000000000000000;
	fma.rn.f64 	%fd4358, %fd4615, %fd4356, 0d0000000000000000;
	add.f64 	%fd4359, %fd4358, %fd4347;
	add.f64 	%fd4360, %fd4337, %fd4357;
	fma.rn.f64 	%fd4361, %fd4557, %fd4356, 0d0000000000000000;
	fma.rn.f64 	%fd4362, %fd4614, %fd4356, 0d0000000000000000;
	add.f64 	%fd4363, %fd4362, %fd4350;
	add.f64 	%fd4364, %fd4336, %fd4361;
	fma.rn.f64 	%fd4365, %fd4554, %fd4356, 0d0000000000000000;
	fma.rn.f64 	%fd4366, %fd4616, %fd4356, 0d0000000000000000;
	add.f64 	%fd4367, %fd4366, %fd4353;
	add.f64 	%fd4368, %fd4335, %fd4365;
	add.f64 	%fd4369, %fd4326, 0d0000000000000000;
	fma.rn.f64 	%fd4370, %fd4615, %fd4369, 0d0000000000000000;
	add.f64 	%fd4371, %fd4370, %fd4360;
	add.f64 	%fd4372, %fd4370, %fd4371;
	fma.rn.f64 	%fd4373, %fd4614, %fd4369, 0d0000000000000000;
	add.f64 	%fd4374, %fd4373, %fd4364;
	add.f64 	%fd4375, %fd4373, %fd4374;
	fma.rn.f64 	%fd4376, %fd4616, %fd4369, 0d0000000000000000;
	add.f64 	%fd4377, %fd4376, %fd4368;
	add.f64 	%fd4378, %fd4376, %fd4377;
	mul.f64 	%fd4379, %fd972, %fd4359;
	mul.f64 	%fd4380, %fd973, %fd4363;
	sub.f64 	%fd4381, %fd4379, %fd4380;
	mul.f64 	%fd4382, %fd973, %fd4367;
	mul.f64 	%fd4383, %fd969, %fd4359;
	sub.f64 	%fd4384, %fd4382, %fd4383;
	mul.f64 	%fd4385, %fd969, %fd4363;
	mul.f64 	%fd4386, %fd972, %fd4367;
	sub.f64 	%fd4387, %fd4385, %fd4386;
	add.f64 	%fd4388, %fd4378, %fd4381;
	add.f64 	%fd4389, %fd4375, %fd4384;
	add.f64 	%fd4390, %fd4372, %fd4387;
	mul.f64 	%fd4391, %fd4614, %fd4359;
	mul.f64 	%fd4392, %fd4615, %fd4363;
	sub.f64 	%fd4393, %fd4391, %fd4392;
	mul.f64 	%fd4394, %fd4615, %fd4367;
	mul.f64 	%fd4395, %fd4616, %fd4359;
	sub.f64 	%fd4396, %fd4394, %fd4395;
	mul.f64 	%fd4397, %fd4616, %fd4363;
	mul.f64 	%fd4398, %fd4614, %fd4367;
	sub.f64 	%fd4399, %fd4397, %fd4398;
	sub.f64 	%fd4400, %fd5044, %fd4393;
	sub.f64 	%fd4401, %fd5045, %fd4396;
	sub.f64 	%fd4402, %fd5046, %fd4399;
	mul.f64 	%fd4403, %fd4611, %fd4402;
	mul.f64 	%fd4404, %fd4612, %fd4401;
	sub.f64 	%fd4405, %fd4403, %fd4404;
	mul.f64 	%fd4406, %fd4612, %fd4400;
	mul.f64 	%fd4407, %fd4613, %fd4402;
	sub.f64 	%fd4408, %fd4406, %fd4407;
	mul.f64 	%fd4409, %fd4613, %fd4401;
	mul.f64 	%fd4410, %fd4611, %fd4400;
	sub.f64 	%fd4411, %fd4409, %fd4410;
	add.f64 	%fd4412, %fd4388, %fd4405;
	add.f64 	%fd4413, %fd4389, %fd4408;
	add.f64 	%fd4414, %fd4390, %fd4411;
	mul.f64 	%fd4415, %fd4614, %fd4402;
	mul.f64 	%fd4416, %fd4615, %fd4401;
	mul.f64 	%fd4417, %fd4615, %fd4400;
	mul.f64 	%fd4418, %fd4616, %fd4402;
	mul.f64 	%fd4419, %fd4616, %fd4401;
	mul.f64 	%fd4420, %fd4614, %fd4400;
	sub.f64 	%fd4421, %fd4416, %fd4415;
	add.f64 	%fd4422, %fd4421, 0d0000000000000000;
	sub.f64 	%fd4423, %fd4418, %fd4417;
	add.f64 	%fd4424, %fd4423, 0d0000000000000000;
	sub.f64 	%fd4425, %fd4420, %fd4419;
	add.f64 	%fd4426, %fd4425, 0d0000000000000000;
	add.f64 	%fd1152, %fd5005, %fd4422;
	add.f64 	%fd1153, %fd5004, %fd4424;
	add.f64 	%fd1154, %fd4426, %fd5003;
	sub.f64 	%fd4427, %fd4341, %fd4422;
	sub.f64 	%fd4428, %fd4342, %fd4424;
	sub.f64 	%fd4429, %fd4343, %fd4426;
	add.f64 	%fd1155, %fd4412, %fd5002;
	add.f64 	%fd1156, %fd4413, %fd5001;
	add.f64 	%fd1157, %fd4414, %fd5000;
	sub.f64 	%fd1158, %fd4427, %fd4412;
	sub.f64 	%fd1159, %fd4428, %fd4413;
	sub.f64 	%fd1160, %fd4429, %fd4414;
	add.f64 	%fd1161, %fd964, 0d0000000000000000;
	setp.eq.s64 	%p299, %rd174, 0;
	@%p299 bra 	$L__BB9_402;

	cvt.s64.s32 	%rd570, %r1000;
	mul.lo.s64 	%rd571, %rd570, %rd79;
	add.s64 	%rd569, %rd174, %rd571;
	// begin inline asm
	{ atom.add.f64 %fd4430,[%rd569],%fd1161; }

	// end inline asm
	bra.uni 	$L__BB9_404;

$L__BB9_402:
	setp.eq.s64 	%p300, %rd145, 0;
	@%p300 bra 	$L__BB9_404;

	cvt.s64.s32 	%rd573, %r1000;
	mul.lo.s64 	%rd574, %rd573, %rd61;
	add.s64 	%rd572, %rd145, %rd574;
	// begin inline asm
	{ atom.add.f64 %fd4432,[%rd572],%fd1161; }

	// end inline asm

$L__BB9_404:
	setp.eq.s64 	%p301, %rd170, 0;
	@%p301 bra 	$L__BB9_406;

	cvt.s64.s32 	%rd576, %r999;
	mul.lo.s64 	%rd577, %rd576, %rd80;
	add.s64 	%rd575, %rd170, %rd577;
	// begin inline asm
	{ atom.add.f64 %fd4434,[%rd575],%fd1161; }

	// end inline asm
	bra.uni 	$L__BB9_408;

$L__BB9_406:
	setp.eq.s64 	%p302, %rd141, 0;
	@%p302 bra 	$L__BB9_408;

	cvt.s64.s32 	%rd579, %r999;
	mul.lo.s64 	%rd580, %rd579, %rd60;
	add.s64 	%rd578, %rd141, %rd580;
	// begin inline asm
	{ atom.add.f64 %fd4436,[%rd578],%fd1161; }

	// end inline asm

$L__BB9_408:
	setp.eq.s64 	%p303, %rd158, 0;
	add.f64 	%fd1162, %fd961, 0d0000000000000000;
	@%p303 bra 	$L__BB9_410;

	mul.lo.s64 	%rd583, %rd96, %rd77;
	add.s64 	%rd581, %rd158, %rd583;
	mov.f64 	%fd4439, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4438,[%rd581],%fd4439; }

	// end inline asm
	add.s64 	%rd582, %rd581, 8;
	// begin inline asm
	{ atom.add.f64 %fd4440,[%rd582],%fd1162; }

	// end inline asm
	bra.uni 	$L__BB9_412;

$L__BB9_410:
	setp.eq.s64 	%p304, %rd117, 0;
	@%p304 bra 	$L__BB9_412;

	mul.lo.s64 	%rd586, %rd96, %rd59;
	add.s64 	%rd584, %rd117, %rd586;
	mov.f64 	%fd4443, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4442,[%rd584],%fd4443; }

	// end inline asm
	add.s64 	%rd585, %rd584, 8;
	// begin inline asm
	{ atom.add.f64 %fd4444,[%rd585],%fd1162; }

	// end inline asm

$L__BB9_412:
	add.f64 	%fd1163, %fd962, 0d0000000000000000;
	@%p303 bra 	$L__BB9_414;

	mul.lo.s64 	%rd589, %rd96, %rd77;
	add.s64 	%rd587, %rd158, %rd589;
	// begin inline asm
	{ atom.add.f64 %fd4446,[%rd587],%fd1163; }

	// end inline asm
	add.s64 	%rd588, %rd587, 8;
	mov.f64 	%fd4449, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4448,[%rd588],%fd4449; }

	// end inline asm
	bra.uni 	$L__BB9_416;

$L__BB9_414:
	setp.eq.s64 	%p306, %rd117, 0;
	@%p306 bra 	$L__BB9_416;

	mul.lo.s64 	%rd592, %rd96, %rd59;
	add.s64 	%rd590, %rd117, %rd592;
	// begin inline asm
	{ atom.add.f64 %fd4450,[%rd590],%fd1163; }

	// end inline asm
	add.s64 	%rd591, %rd590, 8;
	mov.f64 	%fd4453, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4452,[%rd591],%fd4453; }

	// end inline asm

$L__BB9_416:
	setp.eq.s64 	%p307, %rd164, 0;
	@%p307 bra 	$L__BB9_418;

	cvt.s64.s32 	%rd596, %r1007;
	mul.lo.s64 	%rd597, %rd596, %rd78;
	add.s64 	%rd593, %rd164, %rd597;
	// begin inline asm
	{ atom.add.f64 %fd4454,[%rd593],%fd937; }

	// end inline asm
	add.s64 	%rd594, %rd593, 8;
	// begin inline asm
	{ atom.add.f64 %fd4456,[%rd594],%fd938; }

	// end inline asm
	add.s64 	%rd595, %rd593, 16;
	// begin inline asm
	{ atom.add.f64 %fd4458,[%rd595],%fd939; }

	// end inline asm
	bra.uni 	$L__BB9_420;

$L__BB9_418:
	setp.eq.s64 	%p308, %rd135, 0;
	@%p308 bra 	$L__BB9_420;

	cvt.s64.s32 	%rd601, %r1007;
	mul.lo.s64 	%rd602, %rd601, %rd58;
	add.s64 	%rd598, %rd135, %rd602;
	// begin inline asm
	{ atom.add.f64 %fd4460,[%rd598],%fd937; }

	// end inline asm
	add.s64 	%rd599, %rd598, 8;
	// begin inline asm
	{ atom.add.f64 %fd4462,[%rd599],%fd938; }

	// end inline asm
	add.s64 	%rd600, %rd598, 16;
	// begin inline asm
	{ atom.add.f64 %fd4464,[%rd600],%fd939; }

	// end inline asm

$L__BB9_420:
	@%p307 bra 	$L__BB9_422;

	cvt.s64.s32 	%rd606, %r1006;
	mul.lo.s64 	%rd607, %rd606, %rd78;
	add.s64 	%rd603, %rd164, %rd607;
	// begin inline asm
	{ atom.add.f64 %fd4466,[%rd603],%fd943; }

	// end inline asm
	add.s64 	%rd604, %rd603, 8;
	// begin inline asm
	{ atom.add.f64 %fd4468,[%rd604],%fd944; }

	// end inline asm
	add.s64 	%rd605, %rd603, 16;
	// begin inline asm
	{ atom.add.f64 %fd4470,[%rd605],%fd945; }

	// end inline asm
	bra.uni 	$L__BB9_424;

$L__BB9_422:
	setp.eq.s64 	%p310, %rd135, 0;
	@%p310 bra 	$L__BB9_424;

	cvt.s64.s32 	%rd611, %r1006;
	mul.lo.s64 	%rd612, %rd611, %rd58;
	add.s64 	%rd608, %rd135, %rd612;
	// begin inline asm
	{ atom.add.f64 %fd4472,[%rd608],%fd943; }

	// end inline asm
	add.s64 	%rd609, %rd608, 8;
	// begin inline asm
	{ atom.add.f64 %fd4474,[%rd609],%fd944; }

	// end inline asm
	add.s64 	%rd610, %rd608, 16;
	// begin inline asm
	{ atom.add.f64 %fd4476,[%rd610],%fd945; }

	// end inline asm

$L__BB9_424:
	@%p307 bra 	$L__BB9_426;

	cvt.s64.s32 	%rd616, %r1005;
	mul.lo.s64 	%rd617, %rd616, %rd78;
	add.s64 	%rd613, %rd164, %rd617;
	// begin inline asm
	{ atom.add.f64 %fd4478,[%rd613],%fd946; }

	// end inline asm
	add.s64 	%rd614, %rd613, 8;
	// begin inline asm
	{ atom.add.f64 %fd4480,[%rd614],%fd947; }

	// end inline asm
	add.s64 	%rd615, %rd613, 16;
	// begin inline asm
	{ atom.add.f64 %fd4482,[%rd615],%fd948; }

	// end inline asm
	bra.uni 	$L__BB9_428;

$L__BB9_426:
	setp.eq.s64 	%p312, %rd135, 0;
	@%p312 bra 	$L__BB9_428;

	cvt.s64.s32 	%rd621, %r1005;
	mul.lo.s64 	%rd622, %rd621, %rd58;
	add.s64 	%rd618, %rd135, %rd622;
	// begin inline asm
	{ atom.add.f64 %fd4484,[%rd618],%fd946; }

	// end inline asm
	add.s64 	%rd619, %rd618, 8;
	// begin inline asm
	{ atom.add.f64 %fd4486,[%rd619],%fd947; }

	// end inline asm
	add.s64 	%rd620, %rd618, 16;
	// begin inline asm
	{ atom.add.f64 %fd4488,[%rd620],%fd948; }

	// end inline asm

$L__BB9_428:
	@%p307 bra 	$L__BB9_430;

	add.f64 	%fd4565, %fd4973, 0d0000000000000000;
	add.f64 	%fd4564, %fd4974, 0d0000000000000000;
	add.f64 	%fd4563, %fd4975, 0d0000000000000000;
	mul.lo.s64 	%rd627, %rd522, %rd78;
	add.s64 	%rd623, %rd164, %rd627;
	// begin inline asm
	{ atom.add.f64 %fd4490,[%rd623],%fd4563; }

	// end inline asm
	add.s64 	%rd624, %rd623, 8;
	// begin inline asm
	{ atom.add.f64 %fd4492,[%rd624],%fd4564; }

	// end inline asm
	add.s64 	%rd625, %rd623, 16;
	// begin inline asm
	{ atom.add.f64 %fd4494,[%rd625],%fd4565; }

	// end inline asm
	bra.uni 	$L__BB9_432;

$L__BB9_430:
	setp.eq.s64 	%p314, %rd135, 0;
	@%p314 bra 	$L__BB9_432;

	add.f64 	%fd4568, %fd4973, 0d0000000000000000;
	add.f64 	%fd4567, %fd4974, 0d0000000000000000;
	add.f64 	%fd4566, %fd4975, 0d0000000000000000;
	mul.lo.s64 	%rd632, %rd522, %rd58;
	add.s64 	%rd628, %rd135, %rd632;
	// begin inline asm
	{ atom.add.f64 %fd4496,[%rd628],%fd4566; }

	// end inline asm
	add.s64 	%rd629, %rd628, 8;
	// begin inline asm
	{ atom.add.f64 %fd4498,[%rd629],%fd4567; }

	// end inline asm
	add.s64 	%rd630, %rd628, 16;
	// begin inline asm
	{ atom.add.f64 %fd4500,[%rd630],%fd4568; }

	// end inline asm

$L__BB9_432:
	setp.eq.s64 	%p315, %rd162, 0;
	add.f64 	%fd1164, %fd1152, 0d0000000000000000;
	add.f64 	%fd1165, %fd1153, 0d0000000000000000;
	add.f64 	%fd1166, %fd1154, 0d0000000000000000;
	@%p315 bra 	$L__BB9_434;

	cvt.s64.s32 	%rd636, %r1007;
	mul.lo.s64 	%rd637, %rd636, %rd81;
	add.s64 	%rd633, %rd162, %rd637;
	// begin inline asm
	{ atom.add.f64 %fd4502,[%rd633],%fd1164; }

	// end inline asm
	add.s64 	%rd634, %rd633, 8;
	// begin inline asm
	{ atom.add.f64 %fd4504,[%rd634],%fd1165; }

	// end inline asm
	add.s64 	%rd635, %rd633, 16;
	// begin inline asm
	{ atom.add.f64 %fd4506,[%rd635],%fd1166; }

	// end inline asm
	bra.uni 	$L__BB9_436;

$L__BB9_434:
	setp.eq.s64 	%p316, %rd131, 0;
	@%p316 bra 	$L__BB9_436;

	cvt.s64.s32 	%rd641, %r1007;
	mul.lo.s64 	%rd642, %rd641, %rd57;
	add.s64 	%rd638, %rd131, %rd642;
	// begin inline asm
	{ atom.add.f64 %fd4508,[%rd638],%fd1164; }

	// end inline asm
	add.s64 	%rd639, %rd638, 8;
	// begin inline asm
	{ atom.add.f64 %fd4510,[%rd639],%fd1165; }

	// end inline asm
	add.s64 	%rd640, %rd638, 16;
	// begin inline asm
	{ atom.add.f64 %fd4512,[%rd640],%fd1166; }

	// end inline asm

$L__BB9_436:
	add.f64 	%fd1167, %fd1155, 0d0000000000000000;
	add.f64 	%fd1168, %fd1156, 0d0000000000000000;
	add.f64 	%fd1169, %fd1157, 0d0000000000000000;
	@%p315 bra 	$L__BB9_438;

	cvt.s64.s32 	%rd646, %r1006;
	mul.lo.s64 	%rd647, %rd646, %rd81;
	add.s64 	%rd643, %rd162, %rd647;
	// begin inline asm
	{ atom.add.f64 %fd4514,[%rd643],%fd1167; }

	// end inline asm
	add.s64 	%rd644, %rd643, 8;
	// begin inline asm
	{ atom.add.f64 %fd4516,[%rd644],%fd1168; }

	// end inline asm
	add.s64 	%rd645, %rd643, 16;
	// begin inline asm
	{ atom.add.f64 %fd4518,[%rd645],%fd1169; }

	// end inline asm
	bra.uni 	$L__BB9_440;

$L__BB9_438:
	setp.eq.s64 	%p318, %rd131, 0;
	@%p318 bra 	$L__BB9_440;

	cvt.s64.s32 	%rd651, %r1006;
	mul.lo.s64 	%rd652, %rd651, %rd57;
	add.s64 	%rd648, %rd131, %rd652;
	// begin inline asm
	{ atom.add.f64 %fd4520,[%rd648],%fd1167; }

	// end inline asm
	add.s64 	%rd649, %rd648, 8;
	// begin inline asm
	{ atom.add.f64 %fd4522,[%rd649],%fd1168; }

	// end inline asm
	add.s64 	%rd650, %rd648, 16;
	// begin inline asm
	{ atom.add.f64 %fd4524,[%rd650],%fd1169; }

	// end inline asm

$L__BB9_440:
	add.f64 	%fd1170, %fd1158, 0d0000000000000000;
	add.f64 	%fd1171, %fd1159, 0d0000000000000000;
	add.f64 	%fd1172, %fd1160, 0d0000000000000000;
	@%p315 bra 	$L__BB9_442;

	cvt.s64.s32 	%rd656, %r1005;
	mul.lo.s64 	%rd657, %rd656, %rd81;
	add.s64 	%rd653, %rd162, %rd657;
	// begin inline asm
	{ atom.add.f64 %fd4526,[%rd653],%fd1170; }

	// end inline asm
	add.s64 	%rd654, %rd653, 8;
	// begin inline asm
	{ atom.add.f64 %fd4528,[%rd654],%fd1171; }

	// end inline asm
	add.s64 	%rd655, %rd653, 16;
	// begin inline asm
	{ atom.add.f64 %fd4530,[%rd655],%fd1172; }

	// end inline asm
	bra.uni 	$L__BB9_444;

$L__BB9_442:
	setp.eq.s64 	%p320, %rd131, 0;
	@%p320 bra 	$L__BB9_444;

	cvt.s64.s32 	%rd661, %r1005;
	mul.lo.s64 	%rd662, %rd661, %rd57;
	add.s64 	%rd658, %rd131, %rd662;
	// begin inline asm
	{ atom.add.f64 %fd4532,[%rd658],%fd1170; }

	// end inline asm
	add.s64 	%rd659, %rd658, 8;
	// begin inline asm
	{ atom.add.f64 %fd4534,[%rd659],%fd1171; }

	// end inline asm
	add.s64 	%rd660, %rd658, 16;
	// begin inline asm
	{ atom.add.f64 %fd4536,[%rd660],%fd1172; }

	// end inline asm

$L__BB9_444:
	add.f64 	%fd1173, %fd1149, 0d0000000000000000;
	add.f64 	%fd1174, %fd1150, 0d0000000000000000;
	add.f64 	%fd1175, %fd1151, 0d0000000000000000;
	@%p315 bra 	$L__BB9_446;

	mul.lo.s64 	%rd667, %rd522, %rd81;
	add.s64 	%rd663, %rd162, %rd667;
	// begin inline asm
	{ atom.add.f64 %fd4538,[%rd663],%fd1173; }

	// end inline asm
	add.s64 	%rd664, %rd663, 8;
	// begin inline asm
	{ atom.add.f64 %fd4540,[%rd664],%fd1174; }

	// end inline asm
	add.s64 	%rd665, %rd663, 16;
	// begin inline asm
	{ atom.add.f64 %fd4542,[%rd665],%fd1175; }

	// end inline asm
	bra.uni 	$L__BB9_448;

$L__BB9_446:
	setp.eq.s64 	%p322, %rd131, 0;
	@%p322 bra 	$L__BB9_448;

	mul.lo.s64 	%rd672, %rd522, %rd57;
	add.s64 	%rd668, %rd131, %rd672;
	// begin inline asm
	{ atom.add.f64 %fd4544,[%rd668],%fd1173; }

	// end inline asm
	add.s64 	%rd669, %rd668, 8;
	// begin inline asm
	{ atom.add.f64 %fd4546,[%rd669],%fd1174; }

	// end inline asm
	add.s64 	%rd670, %rd668, 16;
	// begin inline asm
	{ atom.add.f64 %fd4548,[%rd670],%fd1175; }

	// end inline asm

$L__BB9_448:
	ld.param.u64 	%rd674, [val_IPC_collisions_cuda_kernel_backward_param_0+24];
	mov.u32 	%r960, %ntid.x;
	mov.u32 	%r959, %nctaid.x;
	mul.wide.u32 	%rd673, %r960, %r959;
	add.s64 	%rd675, %rd675, %rd673;
	setp.lt.u64 	%p323, %rd675, %rd674;
	@%p323 bra 	$L__BB9_2;

$L__BB9_449:
	ret;

}
	// .globl	add_x_to_soft_x_cuda_kernel_forward
.visible .entry add_x_to_soft_x_cuda_kernel_forward(
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_forward_param_2[56],
	.param .u32 add_x_to_soft_x_cuda_kernel_forward_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<62>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<49>;


	ld.param.v2.u32 	{%r26, %r27}, [add_x_to_soft_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [add_x_to_soft_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [add_x_to_soft_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [add_x_to_soft_x_cuda_kernel_forward_param_2+32];
	ld.param.u32 	%r25, [add_x_to_soft_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd26, [add_x_to_soft_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd24, [add_x_to_soft_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd23, [add_x_to_soft_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [add_x_to_soft_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd45, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd45, %rd23;
	@%p1 bra 	$L__BB10_15;

	cvta.to.global.u64 	%rd5, %rd24;
	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r34;
	cvt.s64.s32 	%rd10, %r42;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB10_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd46, %rd45;
	@%p2 bra 	$L__BB10_6;

	or.b64  	%rd31, %rd45, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB10_5;

	div.u64 	%rd46, %rd45, %rd6;
	bra.uni 	$L__BB10_6;

$L__BB10_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd45;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd46, %r52;

$L__BB10_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB10_10;

	or.b64  	%rd33, %rd46, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB10_9;

	div.u64 	%rd46, %rd46, %rd7;
	bra.uni 	$L__BB10_10;

$L__BB10_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd46;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd46, %r55;

$L__BB10_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB10_14;

	or.b64  	%rd35, %rd46, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB10_13;

	div.u64 	%rd46, %rd46, %rd8;
	bra.uni 	$L__BB10_14;

$L__BB10_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd46;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd46, %r58;

$L__BB10_14:
	cvt.u32.u64 	%r59, %rd46;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r60, %r59, 0, %p8;
	add.s32 	%r61, %r60, %r25;
	cvt.s64.s32 	%rd40, %r61;
	mul.lo.s64 	%rd41, %rd40, %rd9;
	add.s64 	%rd42, %rd5, %rd41;
	ld.global.f64 	%fd2, [%rd42];
	ld.global.f64 	%fd4, [%rd42+8];
	ld.global.f64 	%fd6, [%rd42+16];
	cvt.s64.s32 	%rd43, %r60;
	mul.lo.s64 	%rd44, %rd43, %rd10;
	add.s64 	%rd37, %rd26, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd37],%fd2; }

	// end inline asm
	add.s64 	%rd38, %rd37, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd38],%fd4; }

	// end inline asm
	add.s64 	%rd39, %rd37, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd39],%fd6; }

	// end inline asm
	add.s64 	%rd45, %rd45, %rd11;
	setp.lt.u64 	%p9, %rd45, %rd23;
	@%p9 bra 	$L__BB10_2;

$L__BB10_15:
	ret;

}
	// .globl	add_x_to_soft_x_cuda_kernel_backward
.visible .entry add_x_to_soft_x_cuda_kernel_backward(
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_2[56],
	.param .u32 add_x_to_soft_x_cuda_kernel_backward_param_3,
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_5[56],
	.param .u32 add_x_to_soft_x_cuda_kernel_backward_param_6
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<96>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r46, %r47}, [add_x_to_soft_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [add_x_to_soft_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [add_x_to_soft_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r62, %r63}, [add_x_to_soft_x_cuda_kernel_backward_param_2+32];
	ld.param.u32 	%r27, [add_x_to_soft_x_cuda_kernel_backward_param_3];
	ld.param.v2.u32 	{%r70, %r71}, [add_x_to_soft_x_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r78, %r79}, [add_x_to_soft_x_cuda_kernel_backward_param_5+32];
	ld.param.u64 	%rd36, [add_x_to_soft_x_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd34, [add_x_to_soft_x_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd33, [add_x_to_soft_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [add_x_to_soft_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [add_x_to_soft_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r8, [add_x_to_soft_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd38, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd39, %r84;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB11_23;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r49;
	cvt.s64.s32 	%rd11, %r48;
	cvt.s64.s32 	%rd12, %r47;
	cvt.s64.s32 	%rd13, %r78;
	cvt.s64.s32 	%rd14, %r62;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd40, %r85;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r70;
	cvt.s64.s32 	%rd17, %r54;

$L__BB11_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB11_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB11_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB11_6;

$L__BB11_5:
	cvt.u32.u64 	%r86, %rd10;
	cvt.u32.u64 	%r87, %rd63;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd64, %r88;

$L__BB11_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB11_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB11_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r89, %rd11;
	cvt.u32.u64 	%r90, %rd64;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd64, %r91;

$L__BB11_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB11_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB11_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r92, %rd12;
	cvt.u32.u64 	%r93, %rd64;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd64, %r94;

$L__BB11_14:
	cvt.u32.u64 	%r95, %rd64;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r95, 0, %p8;
	add.s32 	%r3, %r2, %r27;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB11_16;

	cvt.s64.s32 	%rd47, %r2;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB11_18;

$L__BB11_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB11_18;

	cvt.s64.s32 	%rd50, %r2;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB11_18:
	setp.eq.s64 	%p11, %rd34, 0;
	@%p11 bra 	$L__BB11_20;

	cvt.s64.s32 	%rd56, %r3;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd34, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB11_22;

$L__BB11_20:
	setp.eq.s64 	%p12, %rd31, 0;
	@%p12 bra 	$L__BB11_22;

	cvt.s64.s32 	%rd61, %r3;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd31, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB11_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB11_2;

$L__BB11_23:
	ret;

}
	// .globl	initialize_friction_hs_cuda_kernel_forward
.visible .entry initialize_friction_hs_cuda_kernel_forward(
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_9[56],
	.param .f64 initialize_friction_hs_cuda_kernel_forward_param_10,
	.param .f64 initialize_friction_hs_cuda_kernel_forward_param_11
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<73>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<204>;
	.reg .f64 	%fd<93>;
	.reg .b64 	%rd<94>;


	ld.param.v2.u32 	{%r98, %r99}, [initialize_friction_hs_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r100, %r101}, [initialize_friction_hs_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r106, %r107}, [initialize_friction_hs_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r114, %r115}, [initialize_friction_hs_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r122, %r123}, [initialize_friction_hs_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r130, %r131}, [initialize_friction_hs_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r138, %r139}, [initialize_friction_hs_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r146, %r147}, [initialize_friction_hs_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r154, %r155}, [initialize_friction_hs_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r162, %r163}, [initialize_friction_hs_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r170, %r171}, [initialize_friction_hs_cuda_kernel_forward_param_9+32];
	ld.param.f64 	%fd12, [initialize_friction_hs_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd56, [initialize_friction_hs_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd54, [initialize_friction_hs_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd52, [initialize_friction_hs_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd50, [initialize_friction_hs_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [initialize_friction_hs_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd46, [initialize_friction_hs_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd44, [initialize_friction_hs_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd42, [initialize_friction_hs_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd40, [initialize_friction_hs_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd39, [initialize_friction_hs_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r16, [initialize_friction_hs_cuda_kernel_forward_param_0+16];
	mov.u32 	%r174, %ntid.x;
	cvt.u64.u32 	%rd1, %r174;
	mov.u32 	%r175, %ctaid.x;
	mul.wide.u32 	%rd58, %r174, %r175;
	mov.u32 	%r176, %tid.x;
	cvt.u64.u32 	%rd59, %r176;
	add.s64 	%rd90, %rd58, %rd59;
	setp.ge.u64 	%p1, %rd90, %rd39;
	@%p1 bra 	$L__BB12_24;

	cvta.to.global.u64 	%rd4, %rd56;
	cvta.to.global.u64 	%rd5, %rd54;
	cvta.to.global.u64 	%rd6, %rd52;
	cvta.to.global.u64 	%rd7, %rd50;
	cvta.to.global.u64 	%rd8, %rd48;
	cvta.to.global.u64 	%rd9, %rd46;
	cvta.to.global.u64 	%rd10, %rd44;
	cvta.to.global.u64 	%rd11, %rd42;
	cvta.to.global.u64 	%rd12, %rd40;
	cvt.s64.s32 	%rd13, %r101;
	cvt.s64.s32 	%rd14, %r100;
	cvt.s64.s32 	%rd15, %r99;
	cvt.s64.s32 	%rd16, %r106;
	cvt.s64.s32 	%rd17, %r114;
	cvt.s64.s32 	%rd18, %r146;
	cvt.s64.s32 	%rd19, %r138;
	cvt.s64.s32 	%rd20, %r122;
	cvt.s64.s32 	%rd21, %r162;
	cvt.s64.s32 	%rd22, %r170;
	cvt.s64.s32 	%rd23, %r130;
	cvt.s64.s32 	%rd24, %r154;
	mov.u32 	%r177, %nctaid.x;
	cvt.u64.u32 	%rd60, %r177;
	mul.lo.s64 	%rd25, %rd1, %rd60;

$L__BB12_2:
	setp.lt.s32 	%p2, %r16, 4;
	mov.u64 	%rd91, %rd90;
	@%p2 bra 	$L__BB12_6;

	or.b64  	%rd61, %rd90, %rd13;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p3, %rd62, 0;
	@%p3 bra 	$L__BB12_5;

	div.u64 	%rd91, %rd90, %rd13;
	bra.uni 	$L__BB12_6;

$L__BB12_5:
	cvt.u32.u64 	%r178, %rd13;
	cvt.u32.u64 	%r179, %rd90;
	div.u32 	%r180, %r179, %r178;
	cvt.u64.u32 	%rd91, %r180;

$L__BB12_6:
	setp.lt.s32 	%p4, %r16, 3;
	@%p4 bra 	$L__BB12_10;

	or.b64  	%rd63, %rd91, %rd14;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p5, %rd64, 0;
	@%p5 bra 	$L__BB12_9;

	div.u64 	%rd91, %rd91, %rd14;
	bra.uni 	$L__BB12_10;

$L__BB12_9:
	cvt.u32.u64 	%r181, %rd14;
	cvt.u32.u64 	%r182, %rd91;
	div.u32 	%r183, %r182, %r181;
	cvt.u64.u32 	%rd91, %r183;

$L__BB12_10:
	setp.lt.s32 	%p6, %r16, 2;
	@%p6 bra 	$L__BB12_14;

	or.b64  	%rd65, %rd91, %rd15;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p7, %rd66, 0;
	@%p7 bra 	$L__BB12_13;

	div.u64 	%rd91, %rd91, %rd15;
	bra.uni 	$L__BB12_14;

$L__BB12_13:
	cvt.u32.u64 	%r184, %rd15;
	cvt.u32.u64 	%r185, %rd91;
	div.u32 	%r186, %r185, %r184;
	cvt.u64.u32 	%rd91, %r186;

$L__BB12_14:
	cvt.s64.s32 	%rd67, %rd91;
	setp.gt.s32 	%p8, %r16, 0;
	selp.b64 	%rd68, %rd67, 0, %p8;
	mov.u64 	%rd69, 0;
	mul.lo.s64 	%rd70, %rd68, %rd16;
	add.s64 	%rd36, %rd12, %rd70;
	st.global.u64 	[%rd36], %rd69;
	mul.lo.s64 	%rd71, %rd68, %rd17;
	add.s64 	%rd72, %rd11, %rd71;
	ld.global.s32 	%rd37, [%rd72];
	mul.lo.s64 	%rd73, %rd37, %rd18;
	add.s64 	%rd74, %rd7, %rd73;
	mul.lo.s64 	%rd75, %rd37, %rd19;
	add.s64 	%rd76, %rd8, %rd75;
	mul.lo.s64 	%rd77, %rd68, %rd20;
	add.s64 	%rd78, %rd10, %rd77;
	ld.global.s32 	%rd79, [%rd78];
	mul.lo.s64 	%rd80, %rd79, %rd21;
	add.s64 	%rd81, %rd5, %rd80;
	mul.lo.s64 	%rd82, %rd79, %rd22;
	add.s64 	%rd83, %rd4, %rd82;
	ld.global.s32 	%rd84, [%rd76];
	mul.lo.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd86, %rd9, %rd85;
	ld.global.f64 	%fd14, [%rd86];
	ld.global.f64 	%fd15, [%rd83];
	sub.f64 	%fd16, %fd14, %fd15;
	ld.global.f64 	%fd17, [%rd86+8];
	ld.global.f64 	%fd18, [%rd83+8];
	sub.f64 	%fd19, %fd17, %fd18;
	ld.global.f64 	%fd20, [%rd86+16];
	ld.global.f64 	%fd21, [%rd83+16];
	sub.f64 	%fd22, %fd20, %fd21;
	ld.global.f64 	%fd23, [%rd81];
	ld.global.f64 	%fd24, [%rd81+8];
	mul.f64 	%fd25, %fd24, %fd19;
	fma.rn.f64 	%fd26, %fd23, %fd16, %fd25;
	ld.global.f64 	%fd27, [%rd81+16];
	fma.rn.f64 	%fd28, %fd27, %fd22, %fd26;
	ld.global.f64 	%fd29, [%rd74];
	sub.f64 	%fd1, %fd28, %fd29;
	setp.geu.f64 	%p9, %fd1, %fd12;
	@%p9 bra 	$L__BB12_23;

	mul.lo.s64 	%rd87, %rd37, %rd24;
	add.s64 	%rd88, %rd6, %rd87;
	ld.global.f64 	%fd2, [%rd88];
	div.rn.f64 	%fd90, %fd1, %fd12;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r200}, %fd90;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r201, %temp}, %fd90;
	}
	setp.gt.s32 	%p10, %r200, 1048575;
	mov.u32 	%r202, -1023;
	@%p10 bra 	$L__BB12_17;

	mul.f64 	%fd90, %fd90, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r200}, %fd90;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r201, %temp}, %fd90;
	}
	mov.u32 	%r202, -1077;

$L__BB12_17:
	add.s32 	%r189, %r200, -1;
	setp.lt.u32 	%p11, %r189, 2146435071;
	@%p11 bra 	$L__BB12_19;
	bra.uni 	$L__BB12_18;

$L__BB12_19:
	shr.u32 	%r191, %r200, 20;
	add.s32 	%r203, %r202, %r191;
	and.b32  	%r192, %r200, -2146435073;
	or.b32  	%r193, %r192, 1072693248;
	mov.b64 	%fd91, {%r201, %r193};
	setp.lt.s32 	%p13, %r193, 1073127583;
	@%p13 bra 	$L__BB12_21;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r194, %temp}, %fd91;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r195}, %fd91;
	}
	add.s32 	%r196, %r195, -1048576;
	mov.b64 	%fd91, {%r194, %r196};
	add.s32 	%r203, %r203, 1;

$L__BB12_21:
	add.f64 	%fd32, %fd91, 0d3FF0000000000000;
	mov.f64 	%fd33, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd34, %fd32;
	neg.f64 	%fd35, %fd32;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd33;
	fma.rn.f64 	%fd37, %fd36, %fd36, %fd36;
	fma.rn.f64 	%fd38, %fd37, %fd34, %fd34;
	add.f64 	%fd39, %fd91, 0dBFF0000000000000;
	mul.f64 	%fd40, %fd39, %fd38;
	fma.rn.f64 	%fd41, %fd39, %fd38, %fd40;
	mul.f64 	%fd42, %fd41, %fd41;
	mov.f64 	%fd43, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd44, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd45, %fd44, %fd42, %fd43;
	mov.f64 	%fd46, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd47, %fd45, %fd42, %fd46;
	mov.f64 	%fd48, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd49, %fd47, %fd42, %fd48;
	mov.f64 	%fd50, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd51, %fd49, %fd42, %fd50;
	mov.f64 	%fd52, 0d3F624924923BE72D;
	fma.rn.f64 	%fd53, %fd51, %fd42, %fd52;
	mov.f64 	%fd54, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd55, %fd53, %fd42, %fd54;
	mov.f64 	%fd56, 0d3FB5555555555554;
	fma.rn.f64 	%fd57, %fd55, %fd42, %fd56;
	sub.f64 	%fd58, %fd39, %fd41;
	add.f64 	%fd59, %fd58, %fd58;
	neg.f64 	%fd60, %fd41;
	fma.rn.f64 	%fd61, %fd60, %fd39, %fd59;
	mul.f64 	%fd62, %fd38, %fd61;
	mul.f64 	%fd63, %fd42, %fd57;
	fma.rn.f64 	%fd64, %fd63, %fd41, %fd62;
	xor.b32  	%r197, %r203, -2147483648;
	mov.u32 	%r198, -2147483648;
	mov.u32 	%r199, 1127219200;
	mov.b64 	%fd65, {%r197, %r199};
	mov.b64 	%fd66, {%r198, %r199};
	sub.f64 	%fd67, %fd65, %fd66;
	mov.f64 	%fd68, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd69, %fd67, %fd68, %fd41;
	neg.f64 	%fd70, %fd67;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	sub.f64 	%fd72, %fd71, %fd41;
	sub.f64 	%fd73, %fd64, %fd72;
	mov.f64 	%fd74, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd75, %fd67, %fd74, %fd73;
	add.f64 	%fd92, %fd69, %fd75;
	bra.uni 	$L__BB12_22;

$L__BB12_18:
	mov.f64 	%fd30, 0d7FF0000000000000;
	fma.rn.f64 	%fd31, %fd90, %fd30, %fd30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd90;
	}
	mov.b32 	%f1, %r190;
	setp.eq.f32 	%p12, %f1, 0f00000000;
	selp.f64 	%fd92, 0dFFF0000000000000, %fd31, %p12;

$L__BB12_22:
	ld.param.f64 	%fd89, [initialize_friction_hs_cuda_kernel_forward_param_11];
	sub.f64 	%fd76, %fd1, %fd12;
	div.rn.f64 	%fd77, %fd76, %fd12;
	mul.f64 	%fd78, %fd77, %fd92;
	mul.f64 	%fd79, %fd78, 0dC000000000000000;
	div.rn.f64 	%fd80, %fd79, %fd12;
	mul.f64 	%fd81, %fd77, %fd77;
	div.rn.f64 	%fd82, %fd81, %fd1;
	sub.f64 	%fd83, %fd80, %fd82;
	mul.f64 	%fd84, %fd83, %fd89;
	mul.f64 	%fd85, %fd2, %fd12;
	mul.f64 	%fd86, %fd85, %fd84;
	mov.f64 	%fd87, 0d0000000000000000;
	sub.f64 	%fd88, %fd87, %fd86;
	st.global.f64 	[%rd36], %fd88;

$L__BB12_23:
	ld.param.u64 	%rd89, [initialize_friction_hs_cuda_kernel_forward_param_0+24];
	add.s64 	%rd90, %rd90, %rd25;
	setp.lt.u64 	%p14, %rd90, %rd89;
	@%p14 bra 	$L__BB12_2;

$L__BB12_24:
	ret;

}
	// .globl	initialize_friction_hs_cuda_kernel_backward
.visible .entry initialize_friction_hs_cuda_kernel_backward(
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_9[56],
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_10,
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_11,
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_20[56],
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_21,
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_22
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<121>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<333>;
	.reg .f64 	%fd<265>;
	.reg .b64 	%rd<164>;


	ld.param.v2.u32 	{%r162, %r163}, [initialize_friction_hs_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r164, %r165}, [initialize_friction_hs_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r170, %r171}, [initialize_friction_hs_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r178, %r179}, [initialize_friction_hs_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r186, %r187}, [initialize_friction_hs_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r194, %r195}, [initialize_friction_hs_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r202, %r203}, [initialize_friction_hs_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r210, %r211}, [initialize_friction_hs_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r218, %r219}, [initialize_friction_hs_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r226, %r227}, [initialize_friction_hs_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r234, %r235}, [initialize_friction_hs_cuda_kernel_backward_param_9+32];
	ld.param.f64 	%fd51, [initialize_friction_hs_cuda_kernel_backward_param_10];
	ld.param.f64 	%fd52, [initialize_friction_hs_cuda_kernel_backward_param_11];
	ld.param.v2.u32 	{%r242, %r243}, [initialize_friction_hs_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r250, %r251}, [initialize_friction_hs_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r258, %r259}, [initialize_friction_hs_cuda_kernel_backward_param_17+32];
	ld.param.v2.u32 	{%r266, %r267}, [initialize_friction_hs_cuda_kernel_backward_param_18+32];
	ld.param.v2.u32 	{%r274, %r275}, [initialize_friction_hs_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r282, %r283}, [initialize_friction_hs_cuda_kernel_backward_param_20+32];
	ld.param.u64 	%rd93, [initialize_friction_hs_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd91, [initialize_friction_hs_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd89, [initialize_friction_hs_cuda_kernel_backward_param_18];
	ld.param.u64 	%rd87, [initialize_friction_hs_cuda_kernel_backward_param_17];
	ld.param.u64 	%rd85, [initialize_friction_hs_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd83, [initialize_friction_hs_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd82, [initialize_friction_hs_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd81, [initialize_friction_hs_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd80, [initialize_friction_hs_cuda_kernel_backward_param_8+8];
	ld.param.u64 	%rd79, [initialize_friction_hs_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd78, [initialize_friction_hs_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd77, [initialize_friction_hs_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd76, [initialize_friction_hs_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd75, [initialize_friction_hs_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd73, [initialize_friction_hs_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd72, [initialize_friction_hs_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd71, [initialize_friction_hs_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd69, [initialize_friction_hs_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd67, [initialize_friction_hs_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd66, [initialize_friction_hs_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd64, [initialize_friction_hs_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r26, [initialize_friction_hs_cuda_kernel_backward_param_0+16];
	mov.u32 	%r286, %ntid.x;
	cvt.u64.u32 	%rd1, %r286;
	mov.u32 	%r287, %ctaid.x;
	mul.wide.u32 	%rd95, %r286, %r287;
	mov.u32 	%r288, %tid.x;
	cvt.u64.u32 	%rd96, %r288;
	add.s64 	%rd160, %rd95, %rd96;
	setp.ge.u64 	%p1, %rd160, %rd64;
	@%p1 bra 	$L__BB13_56;

	cvta.to.global.u64 	%rd16, %rd83;
	cvta.to.global.u64 	%rd17, %rd81;
	cvta.to.global.u64 	%rd18, %rd79;
	cvta.to.global.u64 	%rd19, %rd77;
	cvta.to.global.u64 	%rd20, %rd75;
	cvta.to.global.u64 	%rd21, %rd73;
	cvta.to.global.u64 	%rd22, %rd71;
	cvta.to.global.u64 	%rd23, %rd69;
	cvta.to.global.u64 	%rd24, %rd67;
	cvta.to.global.u64 	%rd25, %rd66;
	cvt.s64.s32 	%rd26, %r165;
	cvt.s64.s32 	%rd27, %r164;
	cvt.s64.s32 	%rd28, %r163;
	cvt.s64.s32 	%rd29, %r178;
	cvt.s64.s32 	%rd30, %r210;
	cvt.s64.s32 	%rd31, %r202;
	cvt.s64.s32 	%rd32, %r186;
	cvt.s64.s32 	%rd33, %r226;
	cvt.s64.s32 	%rd34, %r234;
	cvt.s64.s32 	%rd35, %r194;
	cvt.s64.s32 	%rd36, %r218;
	cvt.s64.s32 	%rd37, %r242;
	cvt.s64.s32 	%rd38, %r250;
	cvt.s64.s32 	%rd39, %r170;
	cvt.s64.s32 	%rd40, %r282;
	cvt.s64.s32 	%rd41, %r274;
	cvt.s64.s32 	%rd42, %r266;
	mov.u32 	%r289, %nctaid.x;
	cvt.u64.u32 	%rd97, %r289;
	mul.lo.s64 	%rd43, %rd1, %rd97;
	cvt.s64.s32 	%rd44, %r258;

$L__BB13_2:
	setp.lt.s32 	%p2, %r26, 4;
	mov.u64 	%rd161, %rd160;
	@%p2 bra 	$L__BB13_6;

	or.b64  	%rd98, %rd160, %rd26;
	and.b64  	%rd99, %rd98, -4294967296;
	setp.eq.s64 	%p3, %rd99, 0;
	@%p3 bra 	$L__BB13_5;

	div.u64 	%rd161, %rd160, %rd26;
	bra.uni 	$L__BB13_6;

$L__BB13_5:
	cvt.u32.u64 	%r290, %rd26;
	cvt.u32.u64 	%r291, %rd160;
	div.u32 	%r292, %r291, %r290;
	cvt.u64.u32 	%rd161, %r292;

$L__BB13_6:
	setp.lt.s32 	%p4, %r26, 3;
	@%p4 bra 	$L__BB13_10;

	or.b64  	%rd100, %rd161, %rd27;
	and.b64  	%rd101, %rd100, -4294967296;
	setp.eq.s64 	%p5, %rd101, 0;
	@%p5 bra 	$L__BB13_9;

	div.u64 	%rd161, %rd161, %rd27;
	bra.uni 	$L__BB13_10;

$L__BB13_9:
	cvt.u32.u64 	%r293, %rd27;
	cvt.u32.u64 	%r294, %rd161;
	div.u32 	%r295, %r294, %r293;
	cvt.u64.u32 	%rd161, %r295;

$L__BB13_10:
	setp.lt.s32 	%p6, %r26, 2;
	@%p6 bra 	$L__BB13_14;

	or.b64  	%rd102, %rd161, %rd28;
	and.b64  	%rd103, %rd102, -4294967296;
	setp.eq.s64 	%p7, %rd103, 0;
	@%p7 bra 	$L__BB13_13;

	div.u64 	%rd161, %rd161, %rd28;
	bra.uni 	$L__BB13_14;

$L__BB13_13:
	cvt.u32.u64 	%r296, %rd28;
	cvt.u32.u64 	%r297, %rd161;
	div.u32 	%r298, %r297, %r296;
	cvt.u64.u32 	%rd161, %r298;

$L__BB13_14:
	cvt.s64.s32 	%rd153, %r210;
	cvt.s64.s32 	%rd152, %r226;
	cvt.s64.s32 	%rd151, %r234;
	cvt.s64.s32 	%rd150, %r194;
	cvt.s64.s32 	%rd104, %rd161;
	setp.gt.s32 	%p8, %r26, 0;
	selp.b64 	%rd55, %rd104, 0, %p8;
	mul.lo.s64 	%rd105, %rd55, %rd29;
	add.s64 	%rd106, %rd24, %rd105;
	ld.global.s32 	%rd56, [%rd106];
	mul.lo.s64 	%rd57, %rd56, %rd153;
	add.s64 	%rd107, %rd20, %rd57;
	mul.lo.s64 	%rd108, %rd56, %rd31;
	add.s64 	%rd109, %rd21, %rd108;
	mul.lo.s64 	%rd110, %rd55, %rd32;
	add.s64 	%rd111, %rd23, %rd110;
	ld.global.s32 	%rd58, [%rd111];
	mul.lo.s64 	%rd59, %rd58, %rd152;
	add.s64 	%rd112, %rd18, %rd59;
	mul.lo.s64 	%rd60, %rd58, %rd151;
	add.s64 	%rd113, %rd17, %rd60;
	ld.global.s32 	%rd61, [%rd109];
	mul.lo.s64 	%rd62, %rd61, %rd150;
	add.s64 	%rd114, %rd22, %rd62;
	ld.global.f64 	%fd54, [%rd114];
	ld.global.f64 	%fd55, [%rd113];
	sub.f64 	%fd3, %fd54, %fd55;
	ld.global.f64 	%fd56, [%rd114+8];
	ld.global.f64 	%fd57, [%rd113+8];
	sub.f64 	%fd4, %fd56, %fd57;
	ld.global.f64 	%fd58, [%rd114+16];
	ld.global.f64 	%fd59, [%rd113+16];
	sub.f64 	%fd5, %fd58, %fd59;
	ld.global.f64 	%fd6, [%rd112];
	ld.global.f64 	%fd7, [%rd112+8];
	mul.f64 	%fd60, %fd7, %fd4;
	fma.rn.f64 	%fd61, %fd6, %fd3, %fd60;
	ld.global.f64 	%fd8, [%rd112+16];
	fma.rn.f64 	%fd62, %fd8, %fd5, %fd61;
	ld.global.f64 	%fd63, [%rd107];
	sub.f64 	%fd9, %fd62, %fd63;
	setp.geu.f64 	%p9, %fd9, %fd51;
	@%p9 bra 	$L__BB13_23;

	mul.lo.s64 	%rd115, %rd56, %rd36;
	add.s64 	%rd116, %rd19, %rd115;
	ld.global.f64 	%fd10, [%rd116];
	div.rn.f64 	%fd255, %fd9, %fd51;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r325}, %fd255;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r326, %temp}, %fd255;
	}
	setp.gt.s32 	%p10, %r325, 1048575;
	mov.u32 	%r327, -1023;
	@%p10 bra 	$L__BB13_17;

	mul.f64 	%fd255, %fd255, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r325}, %fd255;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r326, %temp}, %fd255;
	}
	mov.u32 	%r327, -1077;

$L__BB13_17:
	add.s32 	%r301, %r325, -1;
	setp.lt.u32 	%p11, %r301, 2146435071;
	@%p11 bra 	$L__BB13_19;
	bra.uni 	$L__BB13_18;

$L__BB13_19:
	shr.u32 	%r303, %r325, 20;
	add.s32 	%r328, %r327, %r303;
	and.b32  	%r304, %r325, -2146435073;
	or.b32  	%r305, %r304, 1072693248;
	mov.b64 	%fd256, {%r326, %r305};
	setp.lt.s32 	%p13, %r305, 1073127583;
	@%p13 bra 	$L__BB13_21;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r306, %temp}, %fd256;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r307}, %fd256;
	}
	add.s32 	%r308, %r307, -1048576;
	mov.b64 	%fd256, {%r306, %r308};
	add.s32 	%r328, %r328, 1;

$L__BB13_21:
	add.f64 	%fd66, %fd256, 0d3FF0000000000000;
	mov.f64 	%fd67, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd68, %fd66;
	neg.f64 	%fd69, %fd66;
	fma.rn.f64 	%fd70, %fd69, %fd68, %fd67;
	fma.rn.f64 	%fd71, %fd70, %fd70, %fd70;
	fma.rn.f64 	%fd72, %fd71, %fd68, %fd68;
	add.f64 	%fd73, %fd256, 0dBFF0000000000000;
	mul.f64 	%fd74, %fd73, %fd72;
	fma.rn.f64 	%fd75, %fd73, %fd72, %fd74;
	mul.f64 	%fd76, %fd75, %fd75;
	mov.f64 	%fd77, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd78, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd79, %fd78, %fd76, %fd77;
	mov.f64 	%fd80, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd81, %fd79, %fd76, %fd80;
	mov.f64 	%fd82, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd83, %fd81, %fd76, %fd82;
	mov.f64 	%fd84, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd85, %fd83, %fd76, %fd84;
	mov.f64 	%fd86, 0d3F624924923BE72D;
	fma.rn.f64 	%fd87, %fd85, %fd76, %fd86;
	mov.f64 	%fd88, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd89, %fd87, %fd76, %fd88;
	mov.f64 	%fd90, 0d3FB5555555555554;
	fma.rn.f64 	%fd91, %fd89, %fd76, %fd90;
	sub.f64 	%fd92, %fd73, %fd75;
	add.f64 	%fd93, %fd92, %fd92;
	neg.f64 	%fd94, %fd75;
	fma.rn.f64 	%fd95, %fd94, %fd73, %fd93;
	mul.f64 	%fd96, %fd72, %fd95;
	mul.f64 	%fd97, %fd76, %fd91;
	fma.rn.f64 	%fd98, %fd97, %fd75, %fd96;
	xor.b32  	%r309, %r328, -2147483648;
	mov.u32 	%r310, -2147483648;
	mov.u32 	%r311, 1127219200;
	mov.b64 	%fd99, {%r309, %r311};
	mov.b64 	%fd100, {%r310, %r311};
	sub.f64 	%fd101, %fd99, %fd100;
	mov.f64 	%fd102, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd103, %fd101, %fd102, %fd75;
	neg.f64 	%fd104, %fd101;
	fma.rn.f64 	%fd105, %fd104, %fd102, %fd103;
	sub.f64 	%fd106, %fd105, %fd75;
	sub.f64 	%fd107, %fd98, %fd106;
	mov.f64 	%fd108, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd109, %fd101, %fd108, %fd107;
	add.f64 	%fd257, %fd103, %fd109;
	bra.uni 	$L__BB13_22;

$L__BB13_18:
	mov.f64 	%fd64, 0d7FF0000000000000;
	fma.rn.f64 	%fd65, %fd255, %fd64, %fd64;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r302}, %fd255;
	}
	mov.b32 	%f1, %r302;
	setp.eq.f32 	%p12, %f1, 0f00000000;
	selp.f64 	%fd257, 0dFFF0000000000000, %fd65, %p12;

$L__BB13_22:
	sub.f64 	%fd110, %fd9, %fd51;
	div.rn.f64 	%fd111, %fd110, %fd51;
	mul.f64 	%fd112, %fd111, %fd257;
	mul.f64 	%fd113, %fd112, 0dC000000000000000;
	div.rn.f64 	%fd114, %fd113, %fd51;
	mul.f64 	%fd115, %fd111, %fd111;
	div.rn.f64 	%fd116, %fd115, %fd9;
	sub.f64 	%fd117, %fd114, %fd116;
	mul.f64 	%fd259, %fd117, %fd52;
	mul.f64 	%fd258, %fd10, %fd51;

$L__BB13_23:
	mov.f64 	%fd264, 0d0000000000000000;
	@%p9 bra 	$L__BB13_39;

	ld.param.u64 	%rd154, [initialize_friction_hs_cuda_kernel_backward_param_12];
	setp.eq.s64 	%p15, %rd154, 0;
	@%p15 bra 	$L__BB13_26;

	mul.lo.s64 	%rd117, %rd55, %rd37;
	add.s64 	%rd118, %rd16, %rd117;
	ld.global.f64 	%fd119, [%rd118];
	add.f64 	%fd260, %fd119, 0d0000000000000000;
	bra.uni 	$L__BB13_28;

$L__BB13_26:
	ld.param.u64 	%rd155, [initialize_friction_hs_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p16, %rd155, 0;
	mov.f64 	%fd260, 0d0000000000000000;
	@%p16 bra 	$L__BB13_28;

	mul.lo.s64 	%rd119, %rd55, %rd39;
	add.s64 	%rd120, %rd25, %rd119;
	ld.global.f64 	%fd121, [%rd120];
	add.f64 	%fd260, %fd121, 0d0000000000000000;

$L__BB13_28:
	div.rn.f64 	%fd29, %fd9, %fd51;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r329}, %fd29;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r330, %temp}, %fd29;
	}
	setp.gt.s32 	%p17, %r329, 1048575;
	mov.u32 	%r331, -1023;
	mov.f64 	%fd261, %fd29;
	@%p17 bra 	$L__BB13_30;

	mul.f64 	%fd261, %fd29, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r329}, %fd261;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r330, %temp}, %fd261;
	}
	mov.u32 	%r331, -1077;

$L__BB13_30:
	add.s32 	%r314, %r329, -1;
	setp.lt.u32 	%p18, %r314, 2146435071;
	@%p18 bra 	$L__BB13_32;
	bra.uni 	$L__BB13_31;

$L__BB13_32:
	shr.u32 	%r316, %r329, 20;
	add.s32 	%r332, %r331, %r316;
	and.b32  	%r317, %r329, -2146435073;
	or.b32  	%r318, %r317, 1072693248;
	mov.b64 	%fd262, {%r330, %r318};
	setp.lt.s32 	%p20, %r318, 1073127583;
	@%p20 bra 	$L__BB13_34;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r319, %temp}, %fd262;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r320}, %fd262;
	}
	add.s32 	%r321, %r320, -1048576;
	mov.b64 	%fd262, {%r319, %r321};
	add.s32 	%r332, %r332, 1;

$L__BB13_34:
	add.f64 	%fd126, %fd262, 0d3FF0000000000000;
	mov.f64 	%fd127, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd128, %fd126;
	neg.f64 	%fd129, %fd126;
	fma.rn.f64 	%fd130, %fd129, %fd128, %fd127;
	fma.rn.f64 	%fd131, %fd130, %fd130, %fd130;
	fma.rn.f64 	%fd132, %fd131, %fd128, %fd128;
	add.f64 	%fd133, %fd262, 0dBFF0000000000000;
	mul.f64 	%fd134, %fd133, %fd132;
	fma.rn.f64 	%fd135, %fd133, %fd132, %fd134;
	mul.f64 	%fd136, %fd135, %fd135;
	mov.f64 	%fd137, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd138, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd139, %fd138, %fd136, %fd137;
	mov.f64 	%fd140, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd141, %fd139, %fd136, %fd140;
	mov.f64 	%fd142, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd143, %fd141, %fd136, %fd142;
	mov.f64 	%fd144, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd145, %fd143, %fd136, %fd144;
	mov.f64 	%fd146, 0d3F624924923BE72D;
	fma.rn.f64 	%fd147, %fd145, %fd136, %fd146;
	mov.f64 	%fd148, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd149, %fd147, %fd136, %fd148;
	mov.f64 	%fd150, 0d3FB5555555555554;
	fma.rn.f64 	%fd151, %fd149, %fd136, %fd150;
	sub.f64 	%fd152, %fd133, %fd135;
	add.f64 	%fd153, %fd152, %fd152;
	neg.f64 	%fd154, %fd135;
	fma.rn.f64 	%fd155, %fd154, %fd133, %fd153;
	mul.f64 	%fd156, %fd132, %fd155;
	mul.f64 	%fd157, %fd136, %fd151;
	fma.rn.f64 	%fd158, %fd157, %fd135, %fd156;
	xor.b32  	%r322, %r332, -2147483648;
	mov.u32 	%r323, -2147483648;
	mov.u32 	%r324, 1127219200;
	mov.b64 	%fd159, {%r322, %r324};
	mov.b64 	%fd160, {%r323, %r324};
	sub.f64 	%fd161, %fd159, %fd160;
	mov.f64 	%fd162, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd163, %fd161, %fd162, %fd135;
	neg.f64 	%fd164, %fd161;
	fma.rn.f64 	%fd165, %fd164, %fd162, %fd163;
	sub.f64 	%fd166, %fd165, %fd135;
	sub.f64 	%fd167, %fd158, %fd166;
	mov.f64 	%fd168, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd169, %fd161, %fd168, %fd167;
	add.f64 	%fd263, %fd163, %fd169;
	bra.uni 	$L__BB13_35;

$L__BB13_31:
	mov.f64 	%fd124, 0d7FF0000000000000;
	fma.rn.f64 	%fd125, %fd261, %fd124, %fd124;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r315}, %fd261;
	}
	mov.b32 	%f2, %r315;
	setp.eq.f32 	%p19, %f2, 0f00000000;
	selp.f64 	%fd263, 0dFFF0000000000000, %fd125, %p19;

$L__BB13_35:
	mov.f64 	%fd252, 0d0000000000000000;
	sub.f64 	%fd251, %fd252, %fd260;
	fma.rn.f64 	%fd250, %fd259, %fd251, 0d0000000000000000;
	fma.rn.f64 	%fd249, %fd258, %fd251, 0d0000000000000000;
	sub.f64 	%fd170, %fd9, %fd51;
	div.rn.f64 	%fd171, %fd170, %fd51;
	mul.f64 	%fd172, %fd171, %fd171;
	div.rn.f64 	%fd173, %fd172, %fd9;
	fma.rn.f64 	%fd174, %fd249, %fd52, 0d0000000000000000;
	mov.f64 	%fd175, 0d0000000000000000;
	sub.f64 	%fd176, %fd175, %fd174;
	div.rn.f64 	%fd177, %fd176, %fd9;
	add.f64 	%fd178, %fd177, 0d0000000000000000;
	mul.f64 	%fd179, %fd173, %fd176;
	div.rn.f64 	%fd180, %fd179, %fd9;
	sub.f64 	%fd181, %fd175, %fd180;
	fma.rn.f64 	%fd182, %fd171, %fd178, 0d0000000000000000;
	div.rn.f64 	%fd183, %fd182, %fd51;
	add.f64 	%fd184, %fd183, 0d0000000000000000;
	add.f64 	%fd185, %fd183, %fd184;
	div.rn.f64 	%fd186, %fd174, %fd51;
	add.f64 	%fd187, %fd186, 0d0000000000000000;
	add.f64 	%fd188, %fd187, %fd187;
	sub.f64 	%fd189, %fd175, %fd188;
	fma.rn.f64 	%fd190, %fd189, %fd263, 0d0000000000000000;
	fma.rn.f64 	%fd191, %fd171, %fd189, 0d0000000000000000;
	rcp.rn.f64 	%fd192, %fd29;
	fma.rn.f64 	%fd193, %fd192, %fd191, 0d0000000000000000;
	div.rn.f64 	%fd194, %fd193, %fd51;
	add.f64 	%fd195, %fd181, %fd194;
	div.rn.f64 	%fd196, %fd190, %fd51;
	add.f64 	%fd197, %fd185, %fd196;
	add.f64 	%fd264, %fd195, %fd197;
	fma.rn.f64 	%fd39, %fd250, %fd51, 0d0000000000000000;
	setp.eq.s64 	%p21, %rd89, 0;
	@%p21 bra 	$L__BB13_37;

	mul.lo.s64 	%rd122, %rd56, %rd42;
	add.s64 	%rd121, %rd89, %rd122;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd121],%fd39; }

	// end inline asm
	bra.uni 	$L__BB13_39;

$L__BB13_37:
	setp.eq.s64 	%p22, %rd78, 0;
	@%p22 bra 	$L__BB13_39;

	mul.lo.s64 	%rd124, %rd56, %rd36;
	add.s64 	%rd123, %rd78, %rd124;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd123],%fd39; }

	// end inline asm

$L__BB13_39:
	add.f64 	%fd202, %fd264, 0d0000000000000000;
	fma.rn.f64 	%fd41, %fd6, %fd202, 0d0000000000000000;
	fma.rn.f64 	%fd42, %fd7, %fd202, 0d0000000000000000;
	fma.rn.f64 	%fd43, %fd8, %fd202, 0d0000000000000000;
	fma.rn.f64 	%fd44, %fd3, %fd202, 0d0000000000000000;
	fma.rn.f64 	%fd45, %fd4, %fd202, 0d0000000000000000;
	fma.rn.f64 	%fd46, %fd5, %fd202, 0d0000000000000000;
	setp.eq.s64 	%p23, %rd85, 0;
	@%p23 bra 	$L__BB13_41;

	mul.lo.s64 	%rd128, %rd61, %rd38;
	add.s64 	%rd125, %rd85, %rd128;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd125],%fd41; }

	// end inline asm
	add.s64 	%rd126, %rd125, 8;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd126],%fd42; }

	// end inline asm
	add.s64 	%rd127, %rd125, 16;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd127],%fd43; }

	// end inline asm
	bra.uni 	$L__BB13_43;

$L__BB13_41:
	setp.eq.s64 	%p24, %rd72, 0;
	@%p24 bra 	$L__BB13_43;

	mul.lo.s64 	%rd159, %rd61, %rd150;
	add.s64 	%rd129, %rd72, %rd159;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd129],%fd41; }

	// end inline asm
	add.s64 	%rd130, %rd129, 8;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd130],%fd42; }

	// end inline asm
	add.s64 	%rd131, %rd129, 16;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd131],%fd43; }

	// end inline asm

$L__BB13_43:
	setp.eq.s64 	%p25, %rd93, 0;
	mov.f64 	%fd215, 0d0000000000000000;
	sub.f64 	%fd216, %fd215, %fd41;
	add.f64 	%fd47, %fd216, 0d0000000000000000;
	sub.f64 	%fd217, %fd215, %fd42;
	add.f64 	%fd48, %fd217, 0d0000000000000000;
	sub.f64 	%fd218, %fd215, %fd43;
	add.f64 	%fd49, %fd218, 0d0000000000000000;
	@%p25 bra 	$L__BB13_45;

	mul.lo.s64 	%rd135, %rd58, %rd40;
	add.s64 	%rd132, %rd93, %rd135;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd132],%fd47; }

	// end inline asm
	add.s64 	%rd133, %rd132, 8;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd133],%fd48; }

	// end inline asm
	add.s64 	%rd134, %rd132, 16;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd134],%fd49; }

	// end inline asm
	bra.uni 	$L__BB13_47;

$L__BB13_45:
	setp.eq.s64 	%p26, %rd82, 0;
	@%p26 bra 	$L__BB13_47;

	mul.lo.s64 	%rd158, %rd58, %rd151;
	add.s64 	%rd136, %rd82, %rd158;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd136],%fd47; }

	// end inline asm
	add.s64 	%rd137, %rd136, 8;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd137],%fd48; }

	// end inline asm
	add.s64 	%rd138, %rd136, 16;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd138],%fd49; }

	// end inline asm

$L__BB13_47:
	setp.eq.s64 	%p27, %rd91, 0;
	@%p27 bra 	$L__BB13_49;

	mul.lo.s64 	%rd142, %rd58, %rd41;
	add.s64 	%rd139, %rd91, %rd142;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd139],%fd44; }

	// end inline asm
	add.s64 	%rd140, %rd139, 8;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd140],%fd45; }

	// end inline asm
	add.s64 	%rd141, %rd139, 16;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd141],%fd46; }

	// end inline asm
	bra.uni 	$L__BB13_51;

$L__BB13_49:
	setp.eq.s64 	%p28, %rd80, 0;
	@%p28 bra 	$L__BB13_51;

	mul.lo.s64 	%rd157, %rd58, %rd152;
	add.s64 	%rd143, %rd80, %rd157;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd143],%fd44; }

	// end inline asm
	add.s64 	%rd144, %rd143, 8;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd144],%fd45; }

	// end inline asm
	add.s64 	%rd145, %rd143, 16;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd145],%fd46; }

	// end inline asm

$L__BB13_51:
	setp.eq.s64 	%p29, %rd87, 0;
	mov.f64 	%fd243, 0d0000000000000000;
	sub.f64 	%fd244, %fd243, %fd264;
	add.f64 	%fd50, %fd244, 0d0000000000000000;
	@%p29 bra 	$L__BB13_53;

	mul.lo.s64 	%rd147, %rd56, %rd44;
	add.s64 	%rd146, %rd87, %rd147;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd146],%fd50; }

	// end inline asm
	bra.uni 	$L__BB13_55;

$L__BB13_53:
	setp.eq.s64 	%p30, %rd76, 0;
	@%p30 bra 	$L__BB13_55;

	mul.lo.s64 	%rd156, %rd56, %rd153;
	add.s64 	%rd148, %rd76, %rd156;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd148],%fd50; }

	// end inline asm

$L__BB13_55:
	ld.param.u64 	%rd149, [initialize_friction_hs_cuda_kernel_backward_param_0+24];
	add.s64 	%rd160, %rd160, %rd43;
	setp.lt.u64 	%p31, %rd160, %rd149;
	@%p31 bra 	$L__BB13_2;

$L__BB13_56:
	ret;

}

 