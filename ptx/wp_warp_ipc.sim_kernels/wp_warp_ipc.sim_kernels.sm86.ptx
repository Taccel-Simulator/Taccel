//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34431801
// Cuda compilation tools, release 12.6, V12.6.20
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_86
.address_size 64

	// .globl	negate_arr_vec12d_cuda_kernel_forward
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN57_INTERNAL_00000000_26_wp_warp_ipc_sim_kernels_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;
.global .align 1 .b8 $str[92] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 105, 58, 32, 37, 100, 44, 32, 106, 58, 32, 37, 100, 44, 32, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 110, 95, 114, 111, 119, 115, 58, 32, 37, 100, 44, 32, 110, 95, 99, 111, 108, 115, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10};

.visible .entry negate_arr_vec12d_cuda_kernel_forward(
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<47>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<60>;


	ld.param.v2.u32 	{%r16, %r17}, [negate_arr_vec12d_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [negate_arr_vec12d_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [negate_arr_vec12d_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd30, [negate_arr_vec12d_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd29, [negate_arr_vec12d_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [negate_arr_vec12d_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd32, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd33, %r30;
	add.s64 	%rd53, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd53, %rd29;
	@%p1 bra 	$L__BB0_25;

	cvta.to.global.u64 	%rd4, %rd30;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd34, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd34;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_2;

$L__BB0_12:
	cvt.u32.u64 	%r38, %rd5;
	cvt.u32.u64 	%r41, %rd6;
	cvt.u32.u64 	%r44, %rd7;

$L__BB0_13:
	or.b64  	%rd43, %rd53, %rd5;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p9, %rd44, 0;
	@%p9 bra 	$L__BB0_15;

	div.u64 	%rd58, %rd53, %rd5;
	bra.uni 	$L__BB0_16;

$L__BB0_15:
	cvt.u32.u64 	%r39, %rd53;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd58, %r40;

$L__BB0_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB0_20;

	or.b64  	%rd45, %rd58, %rd6;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p11, %rd46, 0;
	@%p11 bra 	$L__BB0_19;

	div.u64 	%rd58, %rd58, %rd6;
	bra.uni 	$L__BB0_20;

$L__BB0_19:
	cvt.u32.u64 	%r42, %rd58;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd58, %r43;

$L__BB0_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB0_24;

	or.b64  	%rd47, %rd58, %rd7;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p13, %rd48, 0;
	@%p13 bra 	$L__BB0_23;

	div.u64 	%rd58, %rd58, %rd7;
	bra.uni 	$L__BB0_24;

$L__BB0_23:
	cvt.u32.u64 	%r45, %rd58;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd58, %r46;

$L__BB0_24:
	cvt.s64.s32 	%rd49, %rd58;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd50, %rd49, 0, %p14;
	mul.lo.s64 	%rd51, %rd50, %rd8;
	add.s64 	%rd52, %rd4, %rd51;
	ld.global.f64 	%fd25, [%rd52];
	neg.f64 	%fd26, %fd25;
	ld.global.f64 	%fd27, [%rd52+8];
	neg.f64 	%fd28, %fd27;
	ld.global.f64 	%fd29, [%rd52+16];
	neg.f64 	%fd30, %fd29;
	ld.global.f64 	%fd31, [%rd52+24];
	neg.f64 	%fd32, %fd31;
	ld.global.f64 	%fd33, [%rd52+32];
	neg.f64 	%fd34, %fd33;
	ld.global.f64 	%fd35, [%rd52+40];
	neg.f64 	%fd36, %fd35;
	ld.global.f64 	%fd37, [%rd52+48];
	neg.f64 	%fd38, %fd37;
	ld.global.f64 	%fd39, [%rd52+56];
	neg.f64 	%fd40, %fd39;
	ld.global.f64 	%fd41, [%rd52+64];
	neg.f64 	%fd42, %fd41;
	ld.global.f64 	%fd43, [%rd52+72];
	neg.f64 	%fd44, %fd43;
	ld.global.f64 	%fd45, [%rd52+80];
	neg.f64 	%fd46, %fd45;
	ld.global.f64 	%fd47, [%rd52+88];
	neg.f64 	%fd48, %fd47;
	st.global.f64 	[%rd52], %fd26;
	st.global.f64 	[%rd52+8], %fd28;
	st.global.f64 	[%rd52+16], %fd30;
	st.global.f64 	[%rd52+24], %fd32;
	st.global.f64 	[%rd52+32], %fd34;
	st.global.f64 	[%rd52+40], %fd36;
	st.global.f64 	[%rd52+48], %fd38;
	st.global.f64 	[%rd52+56], %fd40;
	st.global.f64 	[%rd52+64], %fd42;
	st.global.f64 	[%rd52+72], %fd44;
	st.global.f64 	[%rd52+80], %fd46;
	st.global.f64 	[%rd52+88], %fd48;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p15, %rd53, %rd29;
	@%p15 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_25;

$L__BB0_2:
	cvt.u32.u64 	%r32, %rd6;
	cvt.u32.u64 	%r35, %rd7;

$L__BB0_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd54, %rd53;
	@%p3 bra 	$L__BB0_7;

	or.b64  	%rd35, %rd53, %rd6;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p4, %rd36, 0;
	@%p4 bra 	$L__BB0_6;

	div.u64 	%rd54, %rd53, %rd6;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r33, %rd53;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd54, %r34;

$L__BB0_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB0_11;

	or.b64  	%rd37, %rd54, %rd7;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB0_10;

	div.u64 	%rd54, %rd54, %rd7;
	bra.uni 	$L__BB0_11;

$L__BB0_10:
	cvt.u32.u64 	%r36, %rd54;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd54, %r37;

$L__BB0_11:
	cvt.s64.s32 	%rd39, %rd54;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd40, %rd39, 0, %p7;
	mul.lo.s64 	%rd41, %rd40, %rd8;
	add.s64 	%rd42, %rd4, %rd41;
	ld.global.f64 	%fd1, [%rd42];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd42+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd42+16];
	neg.f64 	%fd6, %fd5;
	ld.global.f64 	%fd7, [%rd42+24];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd42+32];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd42+40];
	neg.f64 	%fd12, %fd11;
	ld.global.f64 	%fd13, [%rd42+48];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd42+56];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd42+64];
	neg.f64 	%fd18, %fd17;
	ld.global.f64 	%fd19, [%rd42+72];
	neg.f64 	%fd20, %fd19;
	ld.global.f64 	%fd21, [%rd42+80];
	neg.f64 	%fd22, %fd21;
	ld.global.f64 	%fd23, [%rd42+88];
	neg.f64 	%fd24, %fd23;
	st.global.f64 	[%rd42], %fd2;
	st.global.f64 	[%rd42+8], %fd4;
	st.global.f64 	[%rd42+16], %fd6;
	st.global.f64 	[%rd42+24], %fd8;
	st.global.f64 	[%rd42+32], %fd10;
	st.global.f64 	[%rd42+40], %fd12;
	st.global.f64 	[%rd42+48], %fd14;
	st.global.f64 	[%rd42+56], %fd16;
	st.global.f64 	[%rd42+64], %fd18;
	st.global.f64 	[%rd42+72], %fd20;
	st.global.f64 	[%rd42+80], %fd22;
	st.global.f64 	[%rd42+88], %fd24;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p8, %rd53, %rd29;
	@%p8 bra 	$L__BB0_3;

$L__BB0_25:
	ret;

}
	// .globl	negate_arr_vec12d_cuda_kernel_backward
.visible .entry negate_arr_vec12d_cuda_kernel_backward(
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 negate_arr_vec12d_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<146>;
	.reg .b64 	%rd<74>;


	ld.param.v2.u32 	{%r25, %r26}, [negate_arr_vec12d_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [negate_arr_vec12d_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [negate_arr_vec12d_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [negate_arr_vec12d_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd30, [negate_arr_vec12d_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd29, [negate_arr_vec12d_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd27, [negate_arr_vec12d_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [negate_arr_vec12d_cuda_kernel_backward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd32, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd33, %r47;
	add.s64 	%rd70, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd70, %rd27;
	@%p1 bra 	$L__BB1_23;

	cvta.to.global.u64 	%rd6, %rd30;
	cvta.to.global.u64 	%rd7, %rd29;
	cvt.s64.s32 	%rd8, %r28;
	cvt.s64.s32 	%rd9, %r27;
	cvt.s64.s32 	%rd10, %r26;
	cvt.s64.s32 	%rd11, %r33;
	cvt.s64.s32 	%rd12, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd34, %r48;
	mul.lo.s64 	%rd13, %rd1, %rd34;

$L__BB1_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd71, %rd70;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd35, %rd70, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p3, %rd36, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd71, %rd70, %rd8;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r49, %rd8;
	cvt.u32.u64 	%r50, %rd70;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd71, %r51;

$L__BB1_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd37, %rd71, %rd9;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p5, %rd38, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd71, %rd71, %rd9;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r52, %rd9;
	cvt.u32.u64 	%r53, %rd71;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd71, %r54;

$L__BB1_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd39, %rd71, %rd10;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p7, %rd40, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd71, %rd71, %rd10;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r55, %rd10;
	cvt.u32.u64 	%r56, %rd71;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd71, %r57;

$L__BB1_14:
	cvt.s64.s32 	%rd41, %rd71;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd24, %rd41, 0, %p8;
	mul.lo.s64 	%rd25, %rd24, %rd11;
	setp.eq.s64 	%p9, %rd30, 0;
	@%p9 bra 	$L__BB1_16;

	mul.lo.s64 	%rd42, %rd24, %rd12;
	add.s64 	%rd43, %rd6, %rd42;
	ld.global.f64 	%fd49, [%rd43];
	add.f64 	%fd145, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd43+8];
	add.f64 	%fd144, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd43+16];
	add.f64 	%fd143, %fd51, 0d0000000000000000;
	ld.global.f64 	%fd52, [%rd43+24];
	add.f64 	%fd142, %fd52, 0d0000000000000000;
	ld.global.f64 	%fd53, [%rd43+32];
	add.f64 	%fd141, %fd53, 0d0000000000000000;
	ld.global.f64 	%fd54, [%rd43+40];
	add.f64 	%fd140, %fd54, 0d0000000000000000;
	ld.global.f64 	%fd55, [%rd43+48];
	add.f64 	%fd139, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd43+56];
	add.f64 	%fd138, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd43+64];
	add.f64 	%fd137, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd43+72];
	add.f64 	%fd136, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd43+80];
	add.f64 	%fd135, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd43+88];
	add.f64 	%fd134, %fd60, 0d0000000000000000;
	bra.uni 	$L__BB1_18;

$L__BB1_16:
	setp.eq.s64 	%p10, %rd29, 0;
	mov.f64 	%fd134, 0d0000000000000000;
	mov.f64 	%fd135, %fd134;
	mov.f64 	%fd136, %fd134;
	mov.f64 	%fd137, %fd134;
	mov.f64 	%fd138, %fd134;
	mov.f64 	%fd139, %fd134;
	mov.f64 	%fd140, %fd134;
	mov.f64 	%fd141, %fd134;
	mov.f64 	%fd142, %fd134;
	mov.f64 	%fd143, %fd134;
	mov.f64 	%fd144, %fd134;
	mov.f64 	%fd145, %fd134;
	@%p10 bra 	$L__BB1_18;

	add.s64 	%rd44, %rd7, %rd25;
	ld.global.f64 	%fd73, [%rd44];
	add.f64 	%fd145, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd44+8];
	add.f64 	%fd144, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd44+16];
	add.f64 	%fd143, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd44+24];
	add.f64 	%fd142, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd44+32];
	add.f64 	%fd141, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd44+40];
	add.f64 	%fd140, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd44+48];
	add.f64 	%fd139, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd44+56];
	add.f64 	%fd138, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd44+64];
	add.f64 	%fd137, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd44+72];
	add.f64 	%fd136, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd44+80];
	add.f64 	%fd135, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd44+88];
	add.f64 	%fd134, %fd84, 0d0000000000000000;

$L__BB1_18:
	mov.f64 	%fd85, 0d0000000000000000;
	sub.f64 	%fd37, %fd85, %fd145;
	sub.f64 	%fd38, %fd85, %fd144;
	sub.f64 	%fd39, %fd85, %fd143;
	sub.f64 	%fd40, %fd85, %fd142;
	sub.f64 	%fd41, %fd85, %fd141;
	sub.f64 	%fd42, %fd85, %fd140;
	sub.f64 	%fd43, %fd85, %fd139;
	sub.f64 	%fd44, %fd85, %fd138;
	sub.f64 	%fd45, %fd85, %fd137;
	sub.f64 	%fd46, %fd85, %fd136;
	sub.f64 	%fd47, %fd85, %fd135;
	sub.f64 	%fd48, %fd85, %fd134;
	@%p9 bra 	$L__BB1_20;

	mul.lo.s64 	%rd57, %rd24, %rd12;
	add.s64 	%rd45, %rd30, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd45],%fd37; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd46],%fd38; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd47],%fd39; }

	// end inline asm
	add.s64 	%rd48, %rd45, 24;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd48],%fd40; }

	// end inline asm
	add.s64 	%rd49, %rd45, 32;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd49],%fd41; }

	// end inline asm
	add.s64 	%rd50, %rd45, 40;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd50],%fd42; }

	// end inline asm
	add.s64 	%rd51, %rd45, 48;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd51],%fd43; }

	// end inline asm
	add.s64 	%rd52, %rd45, 56;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd52],%fd44; }

	// end inline asm
	add.s64 	%rd53, %rd45, 64;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd53],%fd45; }

	// end inline asm
	add.s64 	%rd54, %rd45, 72;
	// begin inline asm
	{ atom.add.f64 %fd104,[%rd54],%fd46; }

	// end inline asm
	add.s64 	%rd55, %rd45, 80;
	// begin inline asm
	{ atom.add.f64 %fd106,[%rd55],%fd47; }

	// end inline asm
	add.s64 	%rd56, %rd45, 88;
	// begin inline asm
	{ atom.add.f64 %fd108,[%rd56],%fd48; }

	// end inline asm
	bra.uni 	$L__BB1_22;

$L__BB1_20:
	setp.eq.s64 	%p12, %rd29, 0;
	@%p12 bra 	$L__BB1_22;

	add.s64 	%rd58, %rd29, %rd25;
	// begin inline asm
	{ atom.add.f64 %fd110,[%rd58],%fd37; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd112,[%rd59],%fd38; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd114,[%rd60],%fd39; }

	// end inline asm
	add.s64 	%rd61, %rd58, 24;
	// begin inline asm
	{ atom.add.f64 %fd116,[%rd61],%fd40; }

	// end inline asm
	add.s64 	%rd62, %rd58, 32;
	// begin inline asm
	{ atom.add.f64 %fd118,[%rd62],%fd41; }

	// end inline asm
	add.s64 	%rd63, %rd58, 40;
	// begin inline asm
	{ atom.add.f64 %fd120,[%rd63],%fd42; }

	// end inline asm
	add.s64 	%rd64, %rd58, 48;
	// begin inline asm
	{ atom.add.f64 %fd122,[%rd64],%fd43; }

	// end inline asm
	add.s64 	%rd65, %rd58, 56;
	// begin inline asm
	{ atom.add.f64 %fd124,[%rd65],%fd44; }

	// end inline asm
	add.s64 	%rd66, %rd58, 64;
	// begin inline asm
	{ atom.add.f64 %fd126,[%rd66],%fd45; }

	// end inline asm
	add.s64 	%rd67, %rd58, 72;
	// begin inline asm
	{ atom.add.f64 %fd128,[%rd67],%fd46; }

	// end inline asm
	add.s64 	%rd68, %rd58, 80;
	// begin inline asm
	{ atom.add.f64 %fd130,[%rd68],%fd47; }

	// end inline asm
	add.s64 	%rd69, %rd58, 88;
	// begin inline asm
	{ atom.add.f64 %fd132,[%rd69],%fd48; }

	// end inline asm

$L__BB1_22:
	add.s64 	%rd70, %rd70, %rd13;
	setp.lt.u64 	%p13, %rd70, %rd27;
	@%p13 bra 	$L__BB1_2;

$L__BB1_23:
	ret;

}
	// .globl	update_x_kernel_cuda_kernel_forward
.visible .entry update_x_kernel_cuda_kernel_forward(
	.param .align 8 .b8 update_x_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_forward_param_2[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<67>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<83>;


	ld.param.v2.u32 	{%r25, %r26}, [update_x_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [update_x_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [update_x_kernel_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [update_x_kernel_cuda_kernel_forward_param_2+32];
	ld.param.u64 	%rd39, [update_x_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [update_x_kernel_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd36, [update_x_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [update_x_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd41, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd42, %r47;
	add.s64 	%rd74, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd74, %rd36;
	@%p1 bra 	$L__BB2_31;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvt.s64.s32 	%rd6, %r28;
	cvt.s64.s32 	%rd7, %r27;
	cvt.s64.s32 	%rd8, %r26;
	cvt.s64.s32 	%rd9, %r41;
	cvt.s64.s32 	%rd10, %r33;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd43, %r48;
	mul.lo.s64 	%rd11, %rd1, %rd43;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB2_18;
	bra.uni 	$L__BB2_2;

$L__BB2_18:
	cvt.u32.u64 	%r58, %rd6;
	cvt.u32.u64 	%r61, %rd7;
	cvt.u32.u64 	%r64, %rd8;

$L__BB2_19:
	or.b64  	%rd62, %rd74, %rd6;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB2_21;

	div.u64 	%rd81, %rd74, %rd6;
	bra.uni 	$L__BB2_22;

$L__BB2_21:
	cvt.u32.u64 	%r59, %rd74;
	div.u32 	%r60, %r59, %r58;
	cvt.u64.u32 	%rd81, %r60;

$L__BB2_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB2_26;

	or.b64  	%rd64, %rd81, %rd7;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB2_25;

	div.u64 	%rd81, %rd81, %rd7;
	bra.uni 	$L__BB2_26;

$L__BB2_25:
	cvt.u32.u64 	%r62, %rd81;
	div.u32 	%r63, %r62, %r61;
	cvt.u64.u32 	%rd81, %r63;

$L__BB2_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB2_30;

	or.b64  	%rd66, %rd81, %rd8;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p17, %rd67, 0;
	@%p17 bra 	$L__BB2_29;

	div.u64 	%rd81, %rd81, %rd8;
	bra.uni 	$L__BB2_30;

$L__BB2_29:
	cvt.u32.u64 	%r65, %rd81;
	div.u32 	%r66, %r65, %r64;
	cvt.u64.u32 	%rd81, %r66;

$L__BB2_30:
	cvt.s64.s32 	%rd68, %rd81;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd69, %rd68, 0, %p18;
	mul.lo.s64 	%rd70, %rd69, %rd9;
	add.s64 	%rd71, %rd4, %rd70;
	ld.global.f64 	%fd7, [%rd71];
	ld.global.f64 	%fd8, [%rd71+8];
	ld.global.f64 	%fd9, [%rd71+16];
	mul.lo.s64 	%rd72, %rd69, %rd10;
	add.s64 	%rd73, %rd5, %rd72;
	st.global.f64 	[%rd73], %fd7;
	st.global.f64 	[%rd73+8], %fd8;
	st.global.f64 	[%rd73+16], %fd9;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p19, %rd74, %rd36;
	@%p19 bra 	$L__BB2_19;
	bra.uni 	$L__BB2_31;

$L__BB2_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB2_9;
	bra.uni 	$L__BB2_3;

$L__BB2_9:
	cvt.u32.u64 	%r52, %rd7;
	cvt.u32.u64 	%r55, %rd8;

$L__BB2_10:
	or.b64  	%rd52, %rd74, %rd7;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p8, %rd53, 0;
	@%p8 bra 	$L__BB2_12;

	div.u64 	%rd78, %rd74, %rd7;
	bra.uni 	$L__BB2_13;

$L__BB2_12:
	cvt.u32.u64 	%r53, %rd74;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd78, %r54;

$L__BB2_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB2_17;

	or.b64  	%rd54, %rd78, %rd8;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p10, %rd55, 0;
	@%p10 bra 	$L__BB2_16;

	div.u64 	%rd78, %rd78, %rd8;
	bra.uni 	$L__BB2_17;

$L__BB2_16:
	cvt.u32.u64 	%r56, %rd78;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd78, %r57;

$L__BB2_17:
	cvt.s64.s32 	%rd56, %rd78;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd57, %rd56, 0, %p11;
	mul.lo.s64 	%rd58, %rd57, %rd9;
	add.s64 	%rd59, %rd4, %rd58;
	ld.global.f64 	%fd4, [%rd59];
	ld.global.f64 	%fd5, [%rd59+8];
	ld.global.f64 	%fd6, [%rd59+16];
	mul.lo.s64 	%rd60, %rd57, %rd10;
	add.s64 	%rd61, %rd5, %rd60;
	st.global.f64 	[%rd61], %fd4;
	st.global.f64 	[%rd61+8], %fd5;
	st.global.f64 	[%rd61+16], %fd6;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p12, %rd74, %rd36;
	@%p12 bra 	$L__BB2_10;
	bra.uni 	$L__BB2_31;

$L__BB2_3:
	cvt.u32.u64 	%r49, %rd8;

$L__BB2_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd75, %rd74;
	@%p4 bra 	$L__BB2_8;

	or.b64  	%rd44, %rd74, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB2_7;

	div.u64 	%rd75, %rd74, %rd8;
	bra.uni 	$L__BB2_8;

$L__BB2_7:
	cvt.u32.u64 	%r50, %rd74;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd75, %r51;

$L__BB2_8:
	cvt.s64.s32 	%rd46, %rd75;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd47, %rd46, 0, %p6;
	mul.lo.s64 	%rd48, %rd47, %rd9;
	add.s64 	%rd49, %rd4, %rd48;
	ld.global.f64 	%fd1, [%rd49];
	ld.global.f64 	%fd2, [%rd49+8];
	ld.global.f64 	%fd3, [%rd49+16];
	mul.lo.s64 	%rd50, %rd47, %rd10;
	add.s64 	%rd51, %rd5, %rd50;
	st.global.f64 	[%rd51], %fd1;
	st.global.f64 	[%rd51+8], %fd2;
	st.global.f64 	[%rd51+16], %fd3;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p7, %rd74, %rd36;
	@%p7 bra 	$L__BB2_4;

$L__BB2_31:
	ret;

}
	// .globl	update_x_kernel_cuda_kernel_backward
.visible .entry update_x_kernel_cuda_kernel_backward(
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 update_x_kernel_cuda_kernel_backward_param_4[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<94>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r44, %r45}, [update_x_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r46, %r47}, [update_x_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r52, %r53}, [update_x_kernel_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r60, %r61}, [update_x_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r68, %r69}, [update_x_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r76, %r77}, [update_x_kernel_cuda_kernel_backward_param_4+32];
	ld.param.u64 	%rd36, [update_x_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd34, [update_x_kernel_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd33, [update_x_kernel_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [update_x_kernel_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [update_x_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [update_x_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r80, %ntid.x;
	cvt.u64.u32 	%rd1, %r80;
	mov.u32 	%r81, %ctaid.x;
	mul.wide.u32 	%rd38, %r80, %r81;
	mov.u32 	%r82, %tid.x;
	cvt.u64.u32 	%rd39, %r82;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB3_23;

	cvta.to.global.u64 	%rd8, %rd34;
	cvta.to.global.u64 	%rd9, %rd31;
	cvt.s64.s32 	%rd10, %r47;
	cvt.s64.s32 	%rd11, %r46;
	cvt.s64.s32 	%rd12, %r45;
	cvt.s64.s32 	%rd13, %r68;
	cvt.s64.s32 	%rd14, %r52;
	mov.u32 	%r83, %nctaid.x;
	cvt.u64.u32 	%rd40, %r83;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r76;
	cvt.s64.s32 	%rd17, %r60;

$L__BB3_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r84, %rd10;
	cvt.u32.u64 	%r85, %rd63;
	div.u32 	%r86, %r85, %r84;
	cvt.u64.u32 	%rd64, %r86;

$L__BB3_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r87, %rd11;
	cvt.u32.u64 	%r88, %rd64;
	div.u32 	%r89, %r88, %r87;
	cvt.u64.u32 	%rd64, %r89;

$L__BB3_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r90, %rd12;
	cvt.u32.u64 	%r91, %rd64;
	div.u32 	%r92, %r91, %r90;
	cvt.u64.u32 	%rd64, %r92;

$L__BB3_14:
	cvt.u32.u64 	%r93, %rd64;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r93, 0, %p8;
	setp.eq.s64 	%p9, %rd34, 0;
	@%p9 bra 	$L__BB3_16;

	cvt.s64.s32 	%rd47, %r2;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB3_18;

$L__BB3_16:
	setp.eq.s64 	%p10, %rd31, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB3_18;

	cvt.s64.s32 	%rd50, %r2;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB3_18:
	setp.eq.s64 	%p11, %rd36, 0;
	@%p11 bra 	$L__BB3_20;

	cvt.s64.s32 	%rd56, %r2;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd36, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB3_22;

$L__BB3_20:
	setp.eq.s64 	%p12, %rd33, 0;
	@%p12 bra 	$L__BB3_22;

	cvt.s64.s32 	%rd61, %r2;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd33, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB3_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB3_2;

$L__BB3_23:
	ret;

}
	// .globl	multiply_arr_vec3d_mul_scalar_cuda_kernel_forward
.visible .entry multiply_arr_vec3d_mul_scalar_cuda_kernel_forward(
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r16, %r17}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd35, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [multiply_arr_vec3d_mul_scalar_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd37, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd38, %r30;
	add.s64 	%rd64, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd64, %rd34;
	@%p1 bra 	$L__BB4_31;

	cvta.to.global.u64 	%rd4, %rd35;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd39, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd39;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB4_18;
	bra.uni 	$L__BB4_2;

$L__BB4_18:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r47, %rd7;

$L__BB4_19:
	or.b64  	%rd54, %rd64, %rd5;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p13, %rd55, 0;
	@%p13 bra 	$L__BB4_21;

	div.u64 	%rd71, %rd64, %rd5;
	bra.uni 	$L__BB4_22;

$L__BB4_21:
	cvt.u32.u64 	%r42, %rd64;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB4_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB4_26;

	or.b64  	%rd56, %rd71, %rd6;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB4_25;

	div.u64 	%rd71, %rd71, %rd6;
	bra.uni 	$L__BB4_26;

$L__BB4_25:
	cvt.u32.u64 	%r45, %rd71;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd71, %r46;

$L__BB4_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB4_30;

	or.b64  	%rd58, %rd71, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB4_29;

	div.u64 	%rd71, %rd71, %rd7;
	bra.uni 	$L__BB4_30;

$L__BB4_29:
	cvt.u32.u64 	%r48, %rd71;
	div.u32 	%r49, %r48, %r47;
	cvt.u64.u32 	%rd71, %r49;

$L__BB4_30:
	cvt.s64.s32 	%rd60, %rd71;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd61, %rd60, 0, %p18;
	mul.lo.s64 	%rd62, %rd61, %rd8;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.f64 	%fd13, [%rd63];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd63+8];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd63+16];
	neg.f64 	%fd18, %fd17;
	st.global.f64 	[%rd63], %fd14;
	st.global.f64 	[%rd63+8], %fd16;
	st.global.f64 	[%rd63+16], %fd18;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p19, %rd64, %rd34;
	@%p19 bra 	$L__BB4_19;
	bra.uni 	$L__BB4_31;

$L__BB4_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB4_9;
	bra.uni 	$L__BB4_3;

$L__BB4_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r38, %rd7;

$L__BB4_10:
	or.b64  	%rd46, %rd64, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p8, %rd47, 0;
	@%p8 bra 	$L__BB4_12;

	div.u64 	%rd68, %rd64, %rd6;
	bra.uni 	$L__BB4_13;

$L__BB4_12:
	cvt.u32.u64 	%r36, %rd64;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd68, %r37;

$L__BB4_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB4_17;

	or.b64  	%rd48, %rd68, %rd7;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB4_16;

	div.u64 	%rd68, %rd68, %rd7;
	bra.uni 	$L__BB4_17;

$L__BB4_16:
	cvt.u32.u64 	%r39, %rd68;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB4_17:
	cvt.s64.s32 	%rd50, %rd68;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd51, %rd50, 0, %p11;
	mul.lo.s64 	%rd52, %rd51, %rd8;
	add.s64 	%rd53, %rd4, %rd52;
	ld.global.f64 	%fd7, [%rd53];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd53+8];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd53+16];
	neg.f64 	%fd12, %fd11;
	st.global.f64 	[%rd53], %fd8;
	st.global.f64 	[%rd53+8], %fd10;
	st.global.f64 	[%rd53+16], %fd12;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p12, %rd64, %rd34;
	@%p12 bra 	$L__BB4_10;
	bra.uni 	$L__BB4_31;

$L__BB4_3:
	cvt.u32.u64 	%r32, %rd7;

$L__BB4_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd65, %rd64;
	@%p4 bra 	$L__BB4_8;

	or.b64  	%rd40, %rd64, %rd7;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB4_7;

	div.u64 	%rd65, %rd64, %rd7;
	bra.uni 	$L__BB4_8;

$L__BB4_7:
	cvt.u32.u64 	%r33, %rd64;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd65, %r34;

$L__BB4_8:
	cvt.s64.s32 	%rd42, %rd65;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd43, %rd42, 0, %p6;
	mul.lo.s64 	%rd44, %rd43, %rd8;
	add.s64 	%rd45, %rd4, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd45+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd45+16];
	neg.f64 	%fd6, %fd5;
	st.global.f64 	[%rd45], %fd2;
	st.global.f64 	[%rd45+8], %fd4;
	st.global.f64 	[%rd45+16], %fd6;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p7, %rd64, %rd34;
	@%p7 bra 	$L__BB4_4;

$L__BB4_31:
	ret;

}
	// .globl	multiply_arr_vec3d_mul_scalar_cuda_kernel_backward
.visible .entry multiply_arr_vec3d_mul_scalar_cuda_kernel_backward(
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<59>;


	ld.param.v2.u32 	{%r26, %r27}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd26, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd25, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd23, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [multiply_arr_vec3d_mul_scalar_cuda_kernel_backward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd55, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd55, %rd23;
	@%p1 bra 	$L__BB5_23;

	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r42;
	cvt.s64.s32 	%rd10, %r34;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB5_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd56, %rd55;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd31, %rd55, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd56, %rd55, %rd6;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd55;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd56, %r52;

$L__BB5_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd33, %rd56, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd56, %rd56, %rd7;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd56;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd56, %r55;

$L__BB5_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd35, %rd56, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd56, %rd56, %rd8;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd56;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd56, %r58;

$L__BB5_14:
	cvt.u32.u64 	%r59, %rd56;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r59, 0, %p8;
	setp.eq.s64 	%p9, %rd26, 0;
	@%p9 bra 	$L__BB5_16;

	cvta.to.global.u64 	%rd37, %rd26;
	cvt.s64.s32 	%rd38, %r2;
	mul.lo.s64 	%rd39, %rd38, %rd9;
	add.s64 	%rd40, %rd37, %rd39;
	ld.global.f64 	%fd13, [%rd40];
	add.f64 	%fd37, %fd13, 0d0000000000000000;
	ld.global.f64 	%fd14, [%rd40+8];
	add.f64 	%fd36, %fd14, 0d0000000000000000;
	ld.global.f64 	%fd15, [%rd40+16];
	add.f64 	%fd35, %fd15, 0d0000000000000000;
	bra.uni 	$L__BB5_18;

$L__BB5_16:
	setp.eq.s64 	%p10, %rd25, 0;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.f64 	%fd36, %fd35;
	mov.f64 	%fd37, %fd35;
	@%p10 bra 	$L__BB5_18;

	cvta.to.global.u64 	%rd41, %rd25;
	cvt.s64.s32 	%rd42, %r2;
	mul.lo.s64 	%rd43, %rd42, %rd10;
	add.s64 	%rd44, %rd41, %rd43;
	ld.global.f64 	%fd19, [%rd44];
	add.f64 	%fd37, %fd19, 0d0000000000000000;
	ld.global.f64 	%fd20, [%rd44+8];
	add.f64 	%fd36, %fd20, 0d0000000000000000;
	ld.global.f64 	%fd21, [%rd44+16];
	add.f64 	%fd35, %fd21, 0d0000000000000000;

$L__BB5_18:
	mov.f64 	%fd22, 0d0000000000000000;
	sub.f64 	%fd10, %fd22, %fd37;
	sub.f64 	%fd11, %fd22, %fd36;
	sub.f64 	%fd12, %fd22, %fd35;
	@%p9 bra 	$L__BB5_20;

	cvt.s64.s32 	%rd48, %r2;
	mul.lo.s64 	%rd49, %rd48, %rd9;
	add.s64 	%rd45, %rd26, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd45],%fd10; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd46],%fd11; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd47],%fd12; }

	// end inline asm
	bra.uni 	$L__BB5_22;

$L__BB5_20:
	setp.eq.s64 	%p12, %rd25, 0;
	@%p12 bra 	$L__BB5_22;

	cvt.s64.s32 	%rd53, %r2;
	mul.lo.s64 	%rd54, %rd53, %rd10;
	add.s64 	%rd50, %rd25, %rd54;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd50],%fd10; }

	// end inline asm
	add.s64 	%rd51, %rd50, 8;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd51],%fd11; }

	// end inline asm
	add.s64 	%rd52, %rd50, 16;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd52],%fd12; }

	// end inline asm

$L__BB5_22:
	add.s64 	%rd55, %rd55, %rd11;
	setp.lt.u64 	%p13, %rd55, %rd23;
	@%p13 bra 	$L__BB5_2;

$L__BB5_23:
	ret;

}
	// .globl	advection_x_cuda_kernel_forward
.visible .entry advection_x_cuda_kernel_forward(
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 advection_x_cuda_kernel_forward_param_4[56],
	.param .f64 advection_x_cuda_kernel_forward_param_5,
	.param .u32 advection_x_cuda_kernel_forward_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<103>;
	.reg .f64 	%fd<69>;
	.reg .b64 	%rd<84>;


	ld.param.v2.u32 	{%r46, %r47}, [advection_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [advection_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [advection_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r62, %r63}, [advection_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r70, %r71}, [advection_x_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r78, %r79}, [advection_x_cuda_kernel_forward_param_4+32];
	ld.param.f64 	%fd2, [advection_x_cuda_kernel_forward_param_5];
	ld.param.u32 	%r45, [advection_x_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [advection_x_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd46, [advection_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd44, [advection_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd42, [advection_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd41, [advection_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r8, [advection_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd50, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd51, %r84;
	add.s64 	%rd77, %rd50, %rd51;
	setp.ge.u64 	%p1, %rd77, %rd41;
	@%p1 bra 	$L__BB6_33;

	cvta.to.global.u64 	%rd4, %rd48;
	cvta.to.global.u64 	%rd5, %rd46;
	cvta.to.global.u64 	%rd6, %rd42;
	cvta.to.global.u64 	%rd7, %rd44;
	cvt.s64.s32 	%rd8, %r49;
	cvt.s64.s32 	%rd9, %r48;
	cvt.s64.s32 	%rd10, %r47;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd52, %r85;
	mul.lo.s64 	%rd11, %rd1, %rd52;
	cvt.s64.s32 	%rd12, %r70;
	cvt.s64.s32 	%rd13, %r54;
	mul.f64 	%fd1, %fd2, %fd2;
	cvt.s64.s32 	%rd14, %r78;
	cvt.s64.s32 	%rd15, %r62;
	setp.gt.s32 	%p2, %r8, 3;
	@%p2 bra 	$L__BB6_16;
	bra.uni 	$L__BB6_2;

$L__BB6_16:
	cvt.u32.u64 	%r93, %rd8;
	cvt.u32.u64 	%r96, %rd9;
	cvt.u32.u64 	%r99, %rd10;

$L__BB6_17:
	or.b64  	%rd64, %rd77, %rd8;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p11, %rd65, 0;
	@%p11 bra 	$L__BB6_19;

	div.u64 	%rd82, %rd77, %rd8;
	bra.uni 	$L__BB6_20;

$L__BB6_19:
	cvt.u32.u64 	%r94, %rd77;
	div.u32 	%r95, %r94, %r93;
	cvt.u64.u32 	%rd82, %r95;

$L__BB6_20:
	setp.lt.s32 	%p12, %r8, 3;
	@%p12 bra 	$L__BB6_24;

	or.b64  	%rd66, %rd82, %rd9;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB6_23;

	div.u64 	%rd82, %rd82, %rd9;
	bra.uni 	$L__BB6_24;

$L__BB6_23:
	cvt.u32.u64 	%r97, %rd82;
	div.u32 	%r98, %r97, %r96;
	cvt.u64.u32 	%rd82, %r98;

$L__BB6_24:
	setp.lt.s32 	%p14, %r8, 2;
	@%p14 bra 	$L__BB6_28;

	or.b64  	%rd68, %rd82, %rd10;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p15, %rd69, 0;
	@%p15 bra 	$L__BB6_27;

	div.u64 	%rd82, %rd82, %rd10;
	bra.uni 	$L__BB6_28;

$L__BB6_27:
	cvt.u32.u64 	%r100, %rd82;
	div.u32 	%r101, %r100, %r99;
	cvt.u64.u32 	%rd82, %r101;

$L__BB6_28:
	cvt.u32.u64 	%r102, %rd82;
	setp.gt.s32 	%p16, %r8, 0;
	selp.b32 	%r3, %r102, 0, %p16;
	cvt.s64.s32 	%rd70, %r3;
	mul.lo.s64 	%rd71, %rd70, %rd12;
	add.s64 	%rd37, %rd5, %rd71;
	mul.lo.s64 	%rd72, %rd70, %rd13;
	add.s64 	%rd38, %rd6, %rd72;
	mul.lo.s64 	%rd73, %rd70, %rd14;
	add.s64 	%rd39, %rd4, %rd73;
	setp.eq.s32 	%p17, %r45, 0;
	@%p17 bra 	$L__BB6_31;

	setp.ne.s32 	%p18, %r45, 1;
	@%p18 bra 	$L__BB6_32;

	ld.global.f64 	%fd36, [%rd38];
	ld.global.f64 	%fd37, [%rd37];
	sub.f64 	%fd38, %fd37, %fd36;
	ld.global.f64 	%fd39, [%rd38+8];
	ld.global.f64 	%fd40, [%rd37+8];
	sub.f64 	%fd41, %fd40, %fd39;
	ld.global.f64 	%fd42, [%rd38+16];
	ld.global.f64 	%fd43, [%rd37+16];
	sub.f64 	%fd44, %fd43, %fd42;
	div.rn.f64 	%fd45, %fd38, %fd2;
	div.rn.f64 	%fd46, %fd41, %fd2;
	div.rn.f64 	%fd47, %fd44, %fd2;
	mul.lo.s64 	%rd75, %rd70, %rd15;
	add.s64 	%rd76, %rd7, %rd75;
	ld.global.f64 	%fd48, [%rd76];
	sub.f64 	%fd49, %fd45, %fd48;
	ld.global.f64 	%fd50, [%rd76+8];
	sub.f64 	%fd51, %fd46, %fd50;
	ld.global.f64 	%fd52, [%rd76+16];
	sub.f64 	%fd53, %fd47, %fd52;
	div.rn.f64 	%fd54, %fd49, %fd2;
	div.rn.f64 	%fd55, %fd51, %fd2;
	div.rn.f64 	%fd56, %fd53, %fd2;
	st.global.f64 	[%rd39], %fd54;
	st.global.f64 	[%rd39+8], %fd55;
	st.global.f64 	[%rd39+16], %fd56;
	st.global.f64 	[%rd76], %fd45;
	st.global.f64 	[%rd76+8], %fd46;
	st.global.f64 	[%rd76+16], %fd47;
	bra.uni 	$L__BB6_32;

$L__BB6_31:
	ld.global.f64 	%fd57, [%rd37];
	ld.global.f64 	%fd58, [%rd38];
	sub.f64 	%fd59, %fd57, %fd58;
	ld.global.f64 	%fd60, [%rd38+8];
	ld.global.f64 	%fd61, [%rd37+8];
	sub.f64 	%fd62, %fd61, %fd60;
	ld.global.f64 	%fd63, [%rd38+16];
	ld.global.f64 	%fd64, [%rd37+16];
	sub.f64 	%fd65, %fd64, %fd63;
	div.rn.f64 	%fd66, %fd59, %fd1;
	div.rn.f64 	%fd67, %fd62, %fd1;
	div.rn.f64 	%fd68, %fd65, %fd1;
	st.global.f64 	[%rd39], %fd66;
	st.global.f64 	[%rd39+8], %fd67;
	st.global.f64 	[%rd39+16], %fd68;

$L__BB6_32:
	add.s64 	%rd77, %rd77, %rd11;
	setp.lt.u64 	%p19, %rd77, %rd41;
	@%p19 bra 	$L__BB6_17;
	bra.uni 	$L__BB6_33;

$L__BB6_2:
	cvt.u32.u64 	%r86, %rd9;
	cvt.u32.u64 	%r89, %rd10;

$L__BB6_3:
	setp.lt.s32 	%p3, %r8, 3;
	mov.u64 	%rd78, %rd77;
	@%p3 bra 	$L__BB6_7;

	or.b64  	%rd53, %rd77, %rd9;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p4, %rd54, 0;
	@%p4 bra 	$L__BB6_6;

	div.u64 	%rd78, %rd77, %rd9;
	bra.uni 	$L__BB6_7;

$L__BB6_6:
	cvt.u32.u64 	%r87, %rd77;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd78, %r88;

$L__BB6_7:
	setp.lt.s32 	%p5, %r8, 2;
	@%p5 bra 	$L__BB6_11;

	or.b64  	%rd55, %rd78, %rd10;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p6, %rd56, 0;
	@%p6 bra 	$L__BB6_10;

	div.u64 	%rd78, %rd78, %rd10;
	bra.uni 	$L__BB6_11;

$L__BB6_10:
	cvt.u32.u64 	%r90, %rd78;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd78, %r91;

$L__BB6_11:
	cvt.u32.u64 	%r92, %rd78;
	setp.gt.s32 	%p7, %r8, 0;
	selp.b32 	%r2, %r92, 0, %p7;
	cvt.s64.s32 	%rd57, %r2;
	mul.lo.s64 	%rd58, %rd57, %rd12;
	add.s64 	%rd23, %rd5, %rd58;
	mul.lo.s64 	%rd59, %rd57, %rd13;
	add.s64 	%rd24, %rd6, %rd59;
	mul.lo.s64 	%rd60, %rd57, %rd14;
	add.s64 	%rd25, %rd4, %rd60;
	setp.eq.s32 	%p8, %r45, 0;
	@%p8 bra 	$L__BB6_14;

	setp.ne.s32 	%p9, %r45, 1;
	@%p9 bra 	$L__BB6_15;

	ld.global.f64 	%fd3, [%rd24];
	ld.global.f64 	%fd4, [%rd23];
	sub.f64 	%fd5, %fd4, %fd3;
	ld.global.f64 	%fd6, [%rd24+8];
	ld.global.f64 	%fd7, [%rd23+8];
	sub.f64 	%fd8, %fd7, %fd6;
	ld.global.f64 	%fd9, [%rd24+16];
	ld.global.f64 	%fd10, [%rd23+16];
	sub.f64 	%fd11, %fd10, %fd9;
	div.rn.f64 	%fd12, %fd5, %fd2;
	div.rn.f64 	%fd13, %fd8, %fd2;
	div.rn.f64 	%fd14, %fd11, %fd2;
	mul.lo.s64 	%rd62, %rd57, %rd15;
	add.s64 	%rd63, %rd7, %rd62;
	ld.global.f64 	%fd15, [%rd63];
	sub.f64 	%fd16, %fd12, %fd15;
	ld.global.f64 	%fd17, [%rd63+8];
	sub.f64 	%fd18, %fd13, %fd17;
	ld.global.f64 	%fd19, [%rd63+16];
	sub.f64 	%fd20, %fd14, %fd19;
	div.rn.f64 	%fd21, %fd16, %fd2;
	div.rn.f64 	%fd22, %fd18, %fd2;
	div.rn.f64 	%fd23, %fd20, %fd2;
	st.global.f64 	[%rd25], %fd21;
	st.global.f64 	[%rd25+8], %fd22;
	st.global.f64 	[%rd25+16], %fd23;
	st.global.f64 	[%rd63], %fd12;
	st.global.f64 	[%rd63+8], %fd13;
	st.global.f64 	[%rd63+16], %fd14;
	bra.uni 	$L__BB6_15;

$L__BB6_14:
	ld.global.f64 	%fd24, [%rd23];
	ld.global.f64 	%fd25, [%rd24];
	sub.f64 	%fd26, %fd24, %fd25;
	ld.global.f64 	%fd27, [%rd24+8];
	ld.global.f64 	%fd28, [%rd23+8];
	sub.f64 	%fd29, %fd28, %fd27;
	ld.global.f64 	%fd30, [%rd24+16];
	ld.global.f64 	%fd31, [%rd23+16];
	sub.f64 	%fd32, %fd31, %fd30;
	div.rn.f64 	%fd33, %fd26, %fd1;
	div.rn.f64 	%fd34, %fd29, %fd1;
	div.rn.f64 	%fd35, %fd32, %fd1;
	st.global.f64 	[%rd25], %fd33;
	st.global.f64 	[%rd25+8], %fd34;
	st.global.f64 	[%rd25+16], %fd35;

$L__BB6_15:
	add.s64 	%rd77, %rd77, %rd11;
	setp.lt.u64 	%p10, %rd77, %rd41;
	@%p10 bra 	$L__BB6_3;

$L__BB6_33:
	ret;

}
	// .globl	advection_x_cuda_kernel_backward
.visible .entry advection_x_cuda_kernel_backward(
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_4[56],
	.param .f64 advection_x_cuda_kernel_backward_param_5,
	.param .u32 advection_x_cuda_kernel_backward_param_6,
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 advection_x_cuda_kernel_backward_param_10[56],
	.param .f64 advection_x_cuda_kernel_backward_param_11,
	.param .u32 advection_x_cuda_kernel_backward_param_12
)
{
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<71>;
	.reg .b32 	%r<164>;
	.reg .f64 	%fd<195>;
	.reg .b64 	%rd<135>;


	ld.param.v2.u32 	{%r81, %r82}, [advection_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r83, %r84}, [advection_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r89, %r90}, [advection_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r97, %r98}, [advection_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r105, %r106}, [advection_x_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r113, %r114}, [advection_x_cuda_kernel_backward_param_4+32];
	ld.param.f64 	%fd49, [advection_x_cuda_kernel_backward_param_5];
	ld.param.u32 	%r44, [advection_x_cuda_kernel_backward_param_6];
	ld.param.v2.u32 	{%r121, %r122}, [advection_x_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r129, %r130}, [advection_x_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r137, %r138}, [advection_x_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r145, %r146}, [advection_x_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd56, [advection_x_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd54, [advection_x_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd52, [advection_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd50, [advection_x_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd49, [advection_x_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd47, [advection_x_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd45, [advection_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd43, [advection_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd41, [advection_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [advection_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r149, %ntid.x;
	cvt.u64.u32 	%rd1, %r149;
	mov.u32 	%r150, %ctaid.x;
	mul.wide.u32 	%rd58, %r149, %r150;
	mov.u32 	%r151, %tid.x;
	cvt.u64.u32 	%rd59, %r151;
	add.s64 	%rd131, %rd58, %rd59;
	setp.ge.u64 	%p2, %rd131, %rd41;
	@%p2 bra 	$L__BB7_49;

	cvt.s64.s32 	%rd16, %r84;
	cvt.s64.s32 	%rd17, %r83;
	cvt.s64.s32 	%rd18, %r82;
	cvt.s64.s32 	%rd19, %r105;
	cvt.s64.s32 	%rd20, %r89;
	mul.f64 	%fd1, %fd49, %fd49;
	setp.ne.s32 	%p1, %r44, 0;
	cvt.s64.s32 	%rd21, %r97;
	cvt.s64.s32 	%rd22, %r129;
	mov.u32 	%r152, %nctaid.x;
	cvt.u64.u32 	%rd60, %r152;
	mul.lo.s64 	%rd23, %rd1, %rd60;
	cvt.s64.s32 	%rd24, %r145;
	cvt.s64.s32 	%rd25, %r113;
	cvt.s64.s32 	%rd26, %r121;
	cvt.s64.s32 	%rd27, %r137;
	not.pred 	%p13, %p1;

$L__BB7_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd132, %rd131;
	@%p3 bra 	$L__BB7_6;

	or.b64  	%rd61, %rd131, %rd16;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p4, %rd62, 0;
	@%p4 bra 	$L__BB7_5;

	div.u64 	%rd132, %rd131, %rd16;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r153, %rd16;
	cvt.u32.u64 	%r154, %rd131;
	div.u32 	%r155, %r154, %r153;
	cvt.u64.u32 	%rd132, %r155;

$L__BB7_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB7_10;

	or.b64  	%rd63, %rd132, %rd17;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p6, %rd64, 0;
	@%p6 bra 	$L__BB7_9;

	div.u64 	%rd132, %rd132, %rd17;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r156, %rd17;
	cvt.u32.u64 	%r157, %rd132;
	div.u32 	%r158, %r157, %r156;
	cvt.u64.u32 	%rd132, %r158;

$L__BB7_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB7_14;

	or.b64  	%rd65, %rd132, %rd18;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p8, %rd66, 0;
	@%p8 bra 	$L__BB7_13;

	div.u64 	%rd132, %rd132, %rd18;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r159, %rd18;
	cvt.u32.u64 	%r160, %rd132;
	div.u32 	%r161, %r160, %r159;
	cvt.u64.u32 	%rd132, %r161;

$L__BB7_14:
	cvta.to.global.u64 	%rd128, %rd49;
	cvta.to.global.u64 	%rd127, %rd56;
	ld.param.u32 	%r163, [advection_x_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r162, %rd132;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r2, %r162, 0, %p9;
	setp.eq.s32 	%p10, %r163, 1;
	selp.u16 	%rs68, 1, 0, %p10;
	setp.eq.s32 	%p11, %r163, 0;
	selp.f64 	%fd185, %fd1, %fd185, %p11;
	selp.b16 	%rs70, %rs70, %rs68, %p11;
	and.b16  	%rs69, %rs70, 255;
	setp.eq.s16 	%p12, %rs69, 0;
	cvt.s64.s32 	%rd67, %r2;
	mul.lo.s64 	%rd68, %rd67, %rd24;
	add.s64 	%rd38, %rd127, %rd68;
	mul.lo.s64 	%rd69, %rd67, %rd25;
	add.s64 	%rd39, %rd128, %rd69;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB7_35;

	setp.eq.s64 	%p15, %rd52, 0;
	@%p15 bra 	$L__BB7_17;

	cvta.to.global.u64 	%rd129, %rd52;
	mul.lo.s64 	%rd71, %rd67, %rd22;
	add.s64 	%rd72, %rd129, %rd71;
	ld.global.f64 	%fd51, [%rd72];
	add.f64 	%fd188, %fd51, 0d0000000000000000;
	ld.global.f64 	%fd52, [%rd72+8];
	add.f64 	%fd187, %fd52, 0d0000000000000000;
	ld.global.f64 	%fd53, [%rd72+16];
	add.f64 	%fd186, %fd53, 0d0000000000000000;
	bra.uni 	$L__BB7_19;

$L__BB7_17:
	setp.eq.s64 	%p16, %rd45, 0;
	mov.f64 	%fd186, 0d0000000000000000;
	mov.f64 	%fd187, %fd186;
	mov.f64 	%fd188, %fd186;
	@%p16 bra 	$L__BB7_19;

	cvta.to.global.u64 	%rd130, %rd45;
	mul.lo.s64 	%rd74, %rd67, %rd21;
	add.s64 	%rd75, %rd130, %rd74;
	ld.global.f64 	%fd57, [%rd75];
	add.f64 	%fd188, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd75+8];
	add.f64 	%fd187, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd75+16];
	add.f64 	%fd186, %fd59, 0d0000000000000000;

$L__BB7_19:
	setp.eq.s64 	%p17, %rd56, 0;
	@%p17 bra 	$L__BB7_21;

	ld.global.f64 	%fd60, [%rd38];
	add.f64 	%fd191, %fd60, 0d0000000000000000;
	ld.global.f64 	%fd61, [%rd38+8];
	add.f64 	%fd190, %fd61, 0d0000000000000000;
	ld.global.f64 	%fd62, [%rd38+16];
	add.f64 	%fd189, %fd62, 0d0000000000000000;
	bra.uni 	$L__BB7_23;

$L__BB7_21:
	setp.eq.s64 	%p18, %rd49, 0;
	mov.f64 	%fd189, 0d0000000000000000;
	mov.f64 	%fd190, %fd189;
	mov.f64 	%fd191, %fd189;
	@%p18 bra 	$L__BB7_23;

	ld.global.f64 	%fd66, [%rd39];
	add.f64 	%fd191, %fd66, 0d0000000000000000;
	ld.global.f64 	%fd67, [%rd39+8];
	add.f64 	%fd190, %fd67, 0d0000000000000000;
	ld.global.f64 	%fd68, [%rd39+16];
	add.f64 	%fd189, %fd68, 0d0000000000000000;

$L__BB7_23:
	setp.eq.s64 	%p33, %rd52, 0;
	div.rn.f64 	%fd69, %fd191, %fd49;
	add.f64 	%fd70, %fd69, 0d0000000000000000;
	mov.f64 	%fd71, 0d0000000000000000;
	div.rn.f64 	%fd72, %fd190, %fd49;
	add.f64 	%fd73, %fd72, 0d0000000000000000;
	div.rn.f64 	%fd74, %fd189, %fd49;
	add.f64 	%fd75, %fd74, 0d0000000000000000;
	add.f64 	%fd22, %fd70, %fd188;
	add.f64 	%fd23, %fd73, %fd187;
	add.f64 	%fd24, %fd75, %fd186;
	sub.f64 	%fd25, %fd71, %fd70;
	sub.f64 	%fd26, %fd71, %fd73;
	sub.f64 	%fd27, %fd71, %fd75;
	@%p33 bra 	$L__BB7_25;

	mul.lo.s64 	%rd80, %rd67, %rd22;
	add.s64 	%rd76, %rd52, %rd80;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd76],%fd25; }

	// end inline asm
	add.s64 	%rd77, %rd76, 8;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd77],%fd26; }

	// end inline asm
	add.s64 	%rd78, %rd76, 16;
	// begin inline asm
	{ atom.add.f64 %fd80,[%rd78],%fd27; }

	// end inline asm
	bra.uni 	$L__BB7_27;

$L__BB7_25:
	setp.eq.s64 	%p20, %rd45, 0;
	@%p20 bra 	$L__BB7_27;

	mul.lo.s64 	%rd85, %rd67, %rd21;
	add.s64 	%rd81, %rd45, %rd85;
	// begin inline asm
	{ atom.add.f64 %fd82,[%rd81],%fd25; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd84,[%rd82],%fd26; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd83],%fd27; }

	// end inline asm

$L__BB7_27:
	setp.eq.s64 	%p21, %rd50, 0;
	div.rn.f64 	%fd88, %fd22, %fd49;
	mov.f64 	%fd89, 0d0000000000000000;
	div.rn.f64 	%fd90, %fd23, %fd49;
	div.rn.f64 	%fd91, %fd24, %fd49;
	add.f64 	%fd30, %fd91, 0d0000000000000000;
	sub.f64 	%fd33, %fd89, %fd30;
	@%p21 bra 	$L__BB7_29;

	mov.f64 	%fd157, 0d0000000000000000;
	add.f64 	%fd156, %fd90, 0d0000000000000000;
	sub.f64 	%fd155, %fd157, %fd156;
	add.f64 	%fd154, %fd88, 0d0000000000000000;
	sub.f64 	%fd153, %fd157, %fd154;
	mul.lo.s64 	%rd90, %rd67, %rd26;
	add.s64 	%rd86, %rd50, %rd90;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd86],%fd153; }

	// end inline asm
	add.s64 	%rd87, %rd86, 8;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd87],%fd155; }

	// end inline asm
	add.s64 	%rd88, %rd86, 16;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd88],%fd33; }

	// end inline asm
	bra.uni 	$L__BB7_31;

$L__BB7_29:
	setp.eq.s64 	%p22, %rd43, 0;
	@%p22 bra 	$L__BB7_31;

	mov.f64 	%fd168, 0d0000000000000000;
	add.f64 	%fd167, %fd90, 0d0000000000000000;
	sub.f64 	%fd166, %fd168, %fd167;
	add.f64 	%fd165, %fd88, 0d0000000000000000;
	sub.f64 	%fd164, %fd168, %fd165;
	mul.lo.s64 	%rd95, %rd67, %rd20;
	add.s64 	%rd91, %rd43, %rd95;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd91],%fd164; }

	// end inline asm
	add.s64 	%rd92, %rd91, 8;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd92],%fd166; }

	// end inline asm
	add.s64 	%rd93, %rd91, 16;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd93],%fd33; }

	// end inline asm

$L__BB7_31:
	setp.eq.s64 	%p23, %rd54, 0;
	@%p23 bra 	$L__BB7_33;

	add.f64 	%fd160, %fd91, 0d0000000000000000;
	add.f64 	%fd159, %fd90, 0d0000000000000000;
	add.f64 	%fd158, %fd88, 0d0000000000000000;
	mul.lo.s64 	%rd100, %rd67, %rd27;
	add.s64 	%rd96, %rd54, %rd100;
	// begin inline asm
	{ atom.add.f64 %fd104,[%rd96],%fd158; }

	// end inline asm
	add.s64 	%rd97, %rd96, 8;
	// begin inline asm
	{ atom.add.f64 %fd106,[%rd97],%fd159; }

	// end inline asm
	add.s64 	%rd98, %rd96, 16;
	// begin inline asm
	{ atom.add.f64 %fd108,[%rd98],%fd160; }

	// end inline asm
	bra.uni 	$L__BB7_35;

$L__BB7_33:
	setp.eq.s64 	%p24, %rd47, 0;
	@%p24 bra 	$L__BB7_35;

	add.f64 	%fd163, %fd91, 0d0000000000000000;
	add.f64 	%fd162, %fd90, 0d0000000000000000;
	add.f64 	%fd161, %fd88, 0d0000000000000000;
	mul.lo.s64 	%rd105, %rd67, %rd19;
	add.s64 	%rd101, %rd47, %rd105;
	// begin inline asm
	{ atom.add.f64 %fd110,[%rd101],%fd161; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	// begin inline asm
	{ atom.add.f64 %fd112,[%rd102],%fd162; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd114,[%rd103],%fd163; }

	// end inline asm

$L__BB7_35:
	@%p1 bra 	$L__BB7_48;

	setp.eq.s64 	%p26, %rd56, 0;
	@%p26 bra 	$L__BB7_38;

	ld.global.f64 	%fd116, [%rd38];
	add.f64 	%fd194, %fd116, 0d0000000000000000;
	ld.global.f64 	%fd117, [%rd38+8];
	add.f64 	%fd193, %fd117, 0d0000000000000000;
	ld.global.f64 	%fd118, [%rd38+16];
	add.f64 	%fd192, %fd118, 0d0000000000000000;
	bra.uni 	$L__BB7_40;

$L__BB7_38:
	setp.eq.s64 	%p27, %rd49, 0;
	mov.f64 	%fd192, 0d0000000000000000;
	mov.f64 	%fd193, %fd192;
	mov.f64 	%fd194, %fd192;
	@%p27 bra 	$L__BB7_40;

	ld.global.f64 	%fd122, [%rd39];
	add.f64 	%fd194, %fd122, 0d0000000000000000;
	ld.global.f64 	%fd123, [%rd39+8];
	add.f64 	%fd193, %fd123, 0d0000000000000000;
	ld.global.f64 	%fd124, [%rd39+16];
	add.f64 	%fd192, %fd124, 0d0000000000000000;

$L__BB7_40:
	div.rn.f64 	%fd125, %fd194, %fd185;
	mov.f64 	%fd126, 0d0000000000000000;
	div.rn.f64 	%fd127, %fd193, %fd185;
	div.rn.f64 	%fd128, %fd192, %fd185;
	add.f64 	%fd45, %fd128, 0d0000000000000000;
	sub.f64 	%fd48, %fd126, %fd45;
	setp.eq.s64 	%p28, %rd50, 0;
	@%p28 bra 	$L__BB7_42;

	mov.f64 	%fd173, 0d0000000000000000;
	add.f64 	%fd172, %fd127, 0d0000000000000000;
	sub.f64 	%fd171, %fd173, %fd172;
	add.f64 	%fd170, %fd125, 0d0000000000000000;
	sub.f64 	%fd169, %fd173, %fd170;
	mul.lo.s64 	%rd110, %rd67, %rd26;
	add.s64 	%rd106, %rd50, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd106],%fd169; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd107],%fd171; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd108],%fd48; }

	// end inline asm
	bra.uni 	$L__BB7_44;

$L__BB7_42:
	setp.eq.s64 	%p29, %rd43, 0;
	@%p29 bra 	$L__BB7_44;

	mov.f64 	%fd184, 0d0000000000000000;
	add.f64 	%fd183, %fd127, 0d0000000000000000;
	sub.f64 	%fd182, %fd184, %fd183;
	add.f64 	%fd181, %fd125, 0d0000000000000000;
	sub.f64 	%fd180, %fd184, %fd181;
	mul.lo.s64 	%rd115, %rd67, %rd20;
	add.s64 	%rd111, %rd43, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd111],%fd180; }

	// end inline asm
	add.s64 	%rd112, %rd111, 8;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd112],%fd182; }

	// end inline asm
	add.s64 	%rd113, %rd111, 16;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd113],%fd48; }

	// end inline asm

$L__BB7_44:
	setp.eq.s64 	%p30, %rd54, 0;
	@%p30 bra 	$L__BB7_46;

	add.f64 	%fd176, %fd128, 0d0000000000000000;
	add.f64 	%fd175, %fd127, 0d0000000000000000;
	add.f64 	%fd174, %fd125, 0d0000000000000000;
	mul.lo.s64 	%rd120, %rd67, %rd27;
	add.s64 	%rd116, %rd54, %rd120;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd116],%fd174; }

	// end inline asm
	add.s64 	%rd117, %rd116, 8;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd117],%fd175; }

	// end inline asm
	add.s64 	%rd118, %rd116, 16;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd118],%fd176; }

	// end inline asm
	bra.uni 	$L__BB7_48;

$L__BB7_46:
	setp.eq.s64 	%p31, %rd47, 0;
	@%p31 bra 	$L__BB7_48;

	add.f64 	%fd179, %fd128, 0d0000000000000000;
	add.f64 	%fd178, %fd127, 0d0000000000000000;
	add.f64 	%fd177, %fd125, 0d0000000000000000;
	mul.lo.s64 	%rd125, %rd67, %rd19;
	add.s64 	%rd121, %rd47, %rd125;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd121],%fd177; }

	// end inline asm
	add.s64 	%rd122, %rd121, 8;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd122],%fd178; }

	// end inline asm
	add.s64 	%rd123, %rd121, 16;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd123],%fd179; }

	// end inline asm

$L__BB7_48:
	ld.param.u64 	%rd126, [advection_x_cuda_kernel_backward_param_0+24];
	add.s64 	%rd131, %rd131, %rd23;
	setp.lt.u64 	%p32, %rd131, %rd126;
	@%p32 bra 	$L__BB7_2;

$L__BB7_49:
	ret;

}
	// .globl	multiply_arr_vec12d_mul_scalar_cuda_kernel_forward
.visible .entry multiply_arr_vec12d_mul_scalar_cuda_kernel_forward(
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<47>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<60>;


	ld.param.v2.u32 	{%r16, %r17}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd30, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd29, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [multiply_arr_vec12d_mul_scalar_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd32, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd33, %r30;
	add.s64 	%rd53, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd53, %rd29;
	@%p1 bra 	$L__BB8_25;

	cvta.to.global.u64 	%rd4, %rd30;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd34, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd34;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB8_12;
	bra.uni 	$L__BB8_2;

$L__BB8_12:
	cvt.u32.u64 	%r38, %rd5;
	cvt.u32.u64 	%r41, %rd6;
	cvt.u32.u64 	%r44, %rd7;

$L__BB8_13:
	or.b64  	%rd43, %rd53, %rd5;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p9, %rd44, 0;
	@%p9 bra 	$L__BB8_15;

	div.u64 	%rd58, %rd53, %rd5;
	bra.uni 	$L__BB8_16;

$L__BB8_15:
	cvt.u32.u64 	%r39, %rd53;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd58, %r40;

$L__BB8_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB8_20;

	or.b64  	%rd45, %rd58, %rd6;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p11, %rd46, 0;
	@%p11 bra 	$L__BB8_19;

	div.u64 	%rd58, %rd58, %rd6;
	bra.uni 	$L__BB8_20;

$L__BB8_19:
	cvt.u32.u64 	%r42, %rd58;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd58, %r43;

$L__BB8_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB8_24;

	or.b64  	%rd47, %rd58, %rd7;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p13, %rd48, 0;
	@%p13 bra 	$L__BB8_23;

	div.u64 	%rd58, %rd58, %rd7;
	bra.uni 	$L__BB8_24;

$L__BB8_23:
	cvt.u32.u64 	%r45, %rd58;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd58, %r46;

$L__BB8_24:
	cvt.s64.s32 	%rd49, %rd58;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd50, %rd49, 0, %p14;
	mul.lo.s64 	%rd51, %rd50, %rd8;
	add.s64 	%rd52, %rd4, %rd51;
	ld.global.f64 	%fd25, [%rd52];
	neg.f64 	%fd26, %fd25;
	ld.global.f64 	%fd27, [%rd52+8];
	neg.f64 	%fd28, %fd27;
	ld.global.f64 	%fd29, [%rd52+16];
	neg.f64 	%fd30, %fd29;
	ld.global.f64 	%fd31, [%rd52+24];
	neg.f64 	%fd32, %fd31;
	ld.global.f64 	%fd33, [%rd52+32];
	neg.f64 	%fd34, %fd33;
	ld.global.f64 	%fd35, [%rd52+40];
	neg.f64 	%fd36, %fd35;
	ld.global.f64 	%fd37, [%rd52+48];
	neg.f64 	%fd38, %fd37;
	ld.global.f64 	%fd39, [%rd52+56];
	neg.f64 	%fd40, %fd39;
	ld.global.f64 	%fd41, [%rd52+64];
	neg.f64 	%fd42, %fd41;
	ld.global.f64 	%fd43, [%rd52+72];
	neg.f64 	%fd44, %fd43;
	ld.global.f64 	%fd45, [%rd52+80];
	neg.f64 	%fd46, %fd45;
	ld.global.f64 	%fd47, [%rd52+88];
	neg.f64 	%fd48, %fd47;
	st.global.f64 	[%rd52], %fd26;
	st.global.f64 	[%rd52+8], %fd28;
	st.global.f64 	[%rd52+16], %fd30;
	st.global.f64 	[%rd52+24], %fd32;
	st.global.f64 	[%rd52+32], %fd34;
	st.global.f64 	[%rd52+40], %fd36;
	st.global.f64 	[%rd52+48], %fd38;
	st.global.f64 	[%rd52+56], %fd40;
	st.global.f64 	[%rd52+64], %fd42;
	st.global.f64 	[%rd52+72], %fd44;
	st.global.f64 	[%rd52+80], %fd46;
	st.global.f64 	[%rd52+88], %fd48;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p15, %rd53, %rd29;
	@%p15 bra 	$L__BB8_13;
	bra.uni 	$L__BB8_25;

$L__BB8_2:
	cvt.u32.u64 	%r32, %rd6;
	cvt.u32.u64 	%r35, %rd7;

$L__BB8_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd54, %rd53;
	@%p3 bra 	$L__BB8_7;

	or.b64  	%rd35, %rd53, %rd6;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p4, %rd36, 0;
	@%p4 bra 	$L__BB8_6;

	div.u64 	%rd54, %rd53, %rd6;
	bra.uni 	$L__BB8_7;

$L__BB8_6:
	cvt.u32.u64 	%r33, %rd53;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd54, %r34;

$L__BB8_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB8_11;

	or.b64  	%rd37, %rd54, %rd7;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p6, %rd38, 0;
	@%p6 bra 	$L__BB8_10;

	div.u64 	%rd54, %rd54, %rd7;
	bra.uni 	$L__BB8_11;

$L__BB8_10:
	cvt.u32.u64 	%r36, %rd54;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd54, %r37;

$L__BB8_11:
	cvt.s64.s32 	%rd39, %rd54;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd40, %rd39, 0, %p7;
	mul.lo.s64 	%rd41, %rd40, %rd8;
	add.s64 	%rd42, %rd4, %rd41;
	ld.global.f64 	%fd1, [%rd42];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd42+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd42+16];
	neg.f64 	%fd6, %fd5;
	ld.global.f64 	%fd7, [%rd42+24];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd42+32];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd42+40];
	neg.f64 	%fd12, %fd11;
	ld.global.f64 	%fd13, [%rd42+48];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd42+56];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd42+64];
	neg.f64 	%fd18, %fd17;
	ld.global.f64 	%fd19, [%rd42+72];
	neg.f64 	%fd20, %fd19;
	ld.global.f64 	%fd21, [%rd42+80];
	neg.f64 	%fd22, %fd21;
	ld.global.f64 	%fd23, [%rd42+88];
	neg.f64 	%fd24, %fd23;
	st.global.f64 	[%rd42], %fd2;
	st.global.f64 	[%rd42+8], %fd4;
	st.global.f64 	[%rd42+16], %fd6;
	st.global.f64 	[%rd42+24], %fd8;
	st.global.f64 	[%rd42+32], %fd10;
	st.global.f64 	[%rd42+40], %fd12;
	st.global.f64 	[%rd42+48], %fd14;
	st.global.f64 	[%rd42+56], %fd16;
	st.global.f64 	[%rd42+64], %fd18;
	st.global.f64 	[%rd42+72], %fd20;
	st.global.f64 	[%rd42+80], %fd22;
	st.global.f64 	[%rd42+88], %fd24;
	add.s64 	%rd53, %rd53, %rd9;
	setp.lt.u64 	%p8, %rd53, %rd29;
	@%p8 bra 	$L__BB8_3;

$L__BB8_25:
	ret;

}
	// .globl	multiply_arr_vec12d_mul_scalar_cuda_kernel_backward
.visible .entry multiply_arr_vec12d_mul_scalar_cuda_kernel_backward(
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<146>;
	.reg .b64 	%rd<74>;


	ld.param.v2.u32 	{%r25, %r26}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd30, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd29, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd27, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [multiply_arr_vec12d_mul_scalar_cuda_kernel_backward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd32, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd33, %r47;
	add.s64 	%rd70, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd70, %rd27;
	@%p1 bra 	$L__BB9_23;

	cvta.to.global.u64 	%rd6, %rd30;
	cvta.to.global.u64 	%rd7, %rd29;
	cvt.s64.s32 	%rd8, %r28;
	cvt.s64.s32 	%rd9, %r27;
	cvt.s64.s32 	%rd10, %r26;
	cvt.s64.s32 	%rd11, %r33;
	cvt.s64.s32 	%rd12, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd34, %r48;
	mul.lo.s64 	%rd13, %rd1, %rd34;

$L__BB9_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd71, %rd70;
	@%p2 bra 	$L__BB9_6;

	or.b64  	%rd35, %rd70, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p3, %rd36, 0;
	@%p3 bra 	$L__BB9_5;

	div.u64 	%rd71, %rd70, %rd8;
	bra.uni 	$L__BB9_6;

$L__BB9_5:
	cvt.u32.u64 	%r49, %rd8;
	cvt.u32.u64 	%r50, %rd70;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd71, %r51;

$L__BB9_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB9_10;

	or.b64  	%rd37, %rd71, %rd9;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.eq.s64 	%p5, %rd38, 0;
	@%p5 bra 	$L__BB9_9;

	div.u64 	%rd71, %rd71, %rd9;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r52, %rd9;
	cvt.u32.u64 	%r53, %rd71;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd71, %r54;

$L__BB9_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB9_14;

	or.b64  	%rd39, %rd71, %rd10;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p7, %rd40, 0;
	@%p7 bra 	$L__BB9_13;

	div.u64 	%rd71, %rd71, %rd10;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r55, %rd10;
	cvt.u32.u64 	%r56, %rd71;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd71, %r57;

$L__BB9_14:
	cvt.s64.s32 	%rd41, %rd71;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd24, %rd41, 0, %p8;
	mul.lo.s64 	%rd25, %rd24, %rd11;
	setp.eq.s64 	%p9, %rd30, 0;
	@%p9 bra 	$L__BB9_16;

	mul.lo.s64 	%rd42, %rd24, %rd12;
	add.s64 	%rd43, %rd6, %rd42;
	ld.global.f64 	%fd49, [%rd43];
	add.f64 	%fd145, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd43+8];
	add.f64 	%fd144, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd43+16];
	add.f64 	%fd143, %fd51, 0d0000000000000000;
	ld.global.f64 	%fd52, [%rd43+24];
	add.f64 	%fd142, %fd52, 0d0000000000000000;
	ld.global.f64 	%fd53, [%rd43+32];
	add.f64 	%fd141, %fd53, 0d0000000000000000;
	ld.global.f64 	%fd54, [%rd43+40];
	add.f64 	%fd140, %fd54, 0d0000000000000000;
	ld.global.f64 	%fd55, [%rd43+48];
	add.f64 	%fd139, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd43+56];
	add.f64 	%fd138, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd43+64];
	add.f64 	%fd137, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd43+72];
	add.f64 	%fd136, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd43+80];
	add.f64 	%fd135, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd43+88];
	add.f64 	%fd134, %fd60, 0d0000000000000000;
	bra.uni 	$L__BB9_18;

$L__BB9_16:
	setp.eq.s64 	%p10, %rd29, 0;
	mov.f64 	%fd134, 0d0000000000000000;
	mov.f64 	%fd135, %fd134;
	mov.f64 	%fd136, %fd134;
	mov.f64 	%fd137, %fd134;
	mov.f64 	%fd138, %fd134;
	mov.f64 	%fd139, %fd134;
	mov.f64 	%fd140, %fd134;
	mov.f64 	%fd141, %fd134;
	mov.f64 	%fd142, %fd134;
	mov.f64 	%fd143, %fd134;
	mov.f64 	%fd144, %fd134;
	mov.f64 	%fd145, %fd134;
	@%p10 bra 	$L__BB9_18;

	add.s64 	%rd44, %rd7, %rd25;
	ld.global.f64 	%fd73, [%rd44];
	add.f64 	%fd145, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd44+8];
	add.f64 	%fd144, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd44+16];
	add.f64 	%fd143, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd44+24];
	add.f64 	%fd142, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd44+32];
	add.f64 	%fd141, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd44+40];
	add.f64 	%fd140, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd44+48];
	add.f64 	%fd139, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd44+56];
	add.f64 	%fd138, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd44+64];
	add.f64 	%fd137, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd44+72];
	add.f64 	%fd136, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd44+80];
	add.f64 	%fd135, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd44+88];
	add.f64 	%fd134, %fd84, 0d0000000000000000;

$L__BB9_18:
	mov.f64 	%fd85, 0d0000000000000000;
	sub.f64 	%fd37, %fd85, %fd145;
	sub.f64 	%fd38, %fd85, %fd144;
	sub.f64 	%fd39, %fd85, %fd143;
	sub.f64 	%fd40, %fd85, %fd142;
	sub.f64 	%fd41, %fd85, %fd141;
	sub.f64 	%fd42, %fd85, %fd140;
	sub.f64 	%fd43, %fd85, %fd139;
	sub.f64 	%fd44, %fd85, %fd138;
	sub.f64 	%fd45, %fd85, %fd137;
	sub.f64 	%fd46, %fd85, %fd136;
	sub.f64 	%fd47, %fd85, %fd135;
	sub.f64 	%fd48, %fd85, %fd134;
	@%p9 bra 	$L__BB9_20;

	mul.lo.s64 	%rd57, %rd24, %rd12;
	add.s64 	%rd45, %rd30, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd45],%fd37; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd46],%fd38; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd47],%fd39; }

	// end inline asm
	add.s64 	%rd48, %rd45, 24;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd48],%fd40; }

	// end inline asm
	add.s64 	%rd49, %rd45, 32;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd49],%fd41; }

	// end inline asm
	add.s64 	%rd50, %rd45, 40;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd50],%fd42; }

	// end inline asm
	add.s64 	%rd51, %rd45, 48;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd51],%fd43; }

	// end inline asm
	add.s64 	%rd52, %rd45, 56;
	// begin inline asm
	{ atom.add.f64 %fd100,[%rd52],%fd44; }

	// end inline asm
	add.s64 	%rd53, %rd45, 64;
	// begin inline asm
	{ atom.add.f64 %fd102,[%rd53],%fd45; }

	// end inline asm
	add.s64 	%rd54, %rd45, 72;
	// begin inline asm
	{ atom.add.f64 %fd104,[%rd54],%fd46; }

	// end inline asm
	add.s64 	%rd55, %rd45, 80;
	// begin inline asm
	{ atom.add.f64 %fd106,[%rd55],%fd47; }

	// end inline asm
	add.s64 	%rd56, %rd45, 88;
	// begin inline asm
	{ atom.add.f64 %fd108,[%rd56],%fd48; }

	// end inline asm
	bra.uni 	$L__BB9_22;

$L__BB9_20:
	setp.eq.s64 	%p12, %rd29, 0;
	@%p12 bra 	$L__BB9_22;

	add.s64 	%rd58, %rd29, %rd25;
	// begin inline asm
	{ atom.add.f64 %fd110,[%rd58],%fd37; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd112,[%rd59],%fd38; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd114,[%rd60],%fd39; }

	// end inline asm
	add.s64 	%rd61, %rd58, 24;
	// begin inline asm
	{ atom.add.f64 %fd116,[%rd61],%fd40; }

	// end inline asm
	add.s64 	%rd62, %rd58, 32;
	// begin inline asm
	{ atom.add.f64 %fd118,[%rd62],%fd41; }

	// end inline asm
	add.s64 	%rd63, %rd58, 40;
	// begin inline asm
	{ atom.add.f64 %fd120,[%rd63],%fd42; }

	// end inline asm
	add.s64 	%rd64, %rd58, 48;
	// begin inline asm
	{ atom.add.f64 %fd122,[%rd64],%fd43; }

	// end inline asm
	add.s64 	%rd65, %rd58, 56;
	// begin inline asm
	{ atom.add.f64 %fd124,[%rd65],%fd44; }

	// end inline asm
	add.s64 	%rd66, %rd58, 64;
	// begin inline asm
	{ atom.add.f64 %fd126,[%rd66],%fd45; }

	// end inline asm
	add.s64 	%rd67, %rd58, 72;
	// begin inline asm
	{ atom.add.f64 %fd128,[%rd67],%fd46; }

	// end inline asm
	add.s64 	%rd68, %rd58, 80;
	// begin inline asm
	{ atom.add.f64 %fd130,[%rd68],%fd47; }

	// end inline asm
	add.s64 	%rd69, %rd58, 88;
	// begin inline asm
	{ atom.add.f64 %fd132,[%rd69],%fd48; }

	// end inline asm

$L__BB9_22:
	add.s64 	%rd70, %rd70, %rd13;
	setp.lt.u64 	%p13, %rd70, %rd27;
	@%p13 bra 	$L__BB9_2;

$L__BB9_23:
	ret;

}
	// .globl	absolutize_arr_vec3d_cuda_kernel_forward
.visible .entry absolutize_arr_vec3d_cuda_kernel_forward(
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r16, %r17}, [absolutize_arr_vec3d_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [absolutize_arr_vec3d_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [absolutize_arr_vec3d_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd35, [absolutize_arr_vec3d_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [absolutize_arr_vec3d_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [absolutize_arr_vec3d_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd37, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd38, %r30;
	add.s64 	%rd64, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd64, %rd34;
	@%p1 bra 	$L__BB10_31;

	cvta.to.global.u64 	%rd4, %rd35;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd39, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd39;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB10_18;
	bra.uni 	$L__BB10_2;

$L__BB10_18:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r47, %rd7;

$L__BB10_19:
	or.b64  	%rd54, %rd64, %rd5;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p13, %rd55, 0;
	@%p13 bra 	$L__BB10_21;

	div.u64 	%rd71, %rd64, %rd5;
	bra.uni 	$L__BB10_22;

$L__BB10_21:
	cvt.u32.u64 	%r42, %rd64;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB10_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB10_26;

	or.b64  	%rd56, %rd71, %rd6;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB10_25;

	div.u64 	%rd71, %rd71, %rd6;
	bra.uni 	$L__BB10_26;

$L__BB10_25:
	cvt.u32.u64 	%r45, %rd71;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd71, %r46;

$L__BB10_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB10_30;

	or.b64  	%rd58, %rd71, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB10_29;

	div.u64 	%rd71, %rd71, %rd7;
	bra.uni 	$L__BB10_30;

$L__BB10_29:
	cvt.u32.u64 	%r48, %rd71;
	div.u32 	%r49, %r48, %r47;
	cvt.u64.u32 	%rd71, %r49;

$L__BB10_30:
	cvt.s64.s32 	%rd60, %rd71;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd61, %rd60, 0, %p18;
	mul.lo.s64 	%rd62, %rd61, %rd8;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.f64 	%fd13, [%rd63];
	abs.f64 	%fd14, %fd13;
	st.global.f64 	[%rd63], %fd14;
	ld.global.f64 	%fd15, [%rd63+8];
	abs.f64 	%fd16, %fd15;
	st.global.f64 	[%rd63+8], %fd16;
	ld.global.f64 	%fd17, [%rd63+16];
	abs.f64 	%fd18, %fd17;
	st.global.f64 	[%rd63+16], %fd18;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p19, %rd64, %rd34;
	@%p19 bra 	$L__BB10_19;
	bra.uni 	$L__BB10_31;

$L__BB10_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB10_9;
	bra.uni 	$L__BB10_3;

$L__BB10_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r38, %rd7;

$L__BB10_10:
	or.b64  	%rd46, %rd64, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p8, %rd47, 0;
	@%p8 bra 	$L__BB10_12;

	div.u64 	%rd68, %rd64, %rd6;
	bra.uni 	$L__BB10_13;

$L__BB10_12:
	cvt.u32.u64 	%r36, %rd64;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd68, %r37;

$L__BB10_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB10_17;

	or.b64  	%rd48, %rd68, %rd7;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB10_16;

	div.u64 	%rd68, %rd68, %rd7;
	bra.uni 	$L__BB10_17;

$L__BB10_16:
	cvt.u32.u64 	%r39, %rd68;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB10_17:
	cvt.s64.s32 	%rd50, %rd68;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd51, %rd50, 0, %p11;
	mul.lo.s64 	%rd52, %rd51, %rd8;
	add.s64 	%rd53, %rd4, %rd52;
	ld.global.f64 	%fd7, [%rd53];
	abs.f64 	%fd8, %fd7;
	st.global.f64 	[%rd53], %fd8;
	ld.global.f64 	%fd9, [%rd53+8];
	abs.f64 	%fd10, %fd9;
	st.global.f64 	[%rd53+8], %fd10;
	ld.global.f64 	%fd11, [%rd53+16];
	abs.f64 	%fd12, %fd11;
	st.global.f64 	[%rd53+16], %fd12;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p12, %rd64, %rd34;
	@%p12 bra 	$L__BB10_10;
	bra.uni 	$L__BB10_31;

$L__BB10_3:
	cvt.u32.u64 	%r32, %rd7;

$L__BB10_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd65, %rd64;
	@%p4 bra 	$L__BB10_8;

	or.b64  	%rd40, %rd64, %rd7;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB10_7;

	div.u64 	%rd65, %rd64, %rd7;
	bra.uni 	$L__BB10_8;

$L__BB10_7:
	cvt.u32.u64 	%r33, %rd64;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd65, %r34;

$L__BB10_8:
	cvt.s64.s32 	%rd42, %rd65;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd43, %rd42, 0, %p6;
	mul.lo.s64 	%rd44, %rd43, %rd8;
	add.s64 	%rd45, %rd4, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	abs.f64 	%fd2, %fd1;
	st.global.f64 	[%rd45], %fd2;
	ld.global.f64 	%fd3, [%rd45+8];
	abs.f64 	%fd4, %fd3;
	st.global.f64 	[%rd45+8], %fd4;
	ld.global.f64 	%fd5, [%rd45+16];
	abs.f64 	%fd6, %fd5;
	st.global.f64 	[%rd45+16], %fd6;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p7, %rd64, %rd34;
	@%p7 bra 	$L__BB10_4;

$L__BB10_31:
	ret;

}
	// .globl	absolutize_arr_vec3d_cuda_kernel_backward
.visible .entry absolutize_arr_vec3d_cuda_kernel_backward(
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 absolutize_arr_vec3d_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<73>;
	.reg .b64 	%rd<101>;


	ld.param.v2.u32 	{%r26, %r27}, [absolutize_arr_vec3d_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [absolutize_arr_vec3d_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [absolutize_arr_vec3d_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [absolutize_arr_vec3d_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd26, [absolutize_arr_vec3d_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd25, [absolutize_arr_vec3d_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd23, [absolutize_arr_vec3d_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [absolutize_arr_vec3d_cuda_kernel_backward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd97, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd97, %rd23;
	@%p1 bra 	$L__BB11_39;

	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r42;
	cvt.s64.s32 	%rd10, %r34;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB11_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd98, %rd97;
	@%p2 bra 	$L__BB11_6;

	or.b64  	%rd31, %rd97, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB11_5;

	div.u64 	%rd98, %rd97, %rd6;
	bra.uni 	$L__BB11_6;

$L__BB11_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd97;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd98, %r52;

$L__BB11_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB11_10;

	or.b64  	%rd33, %rd98, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB11_9;

	div.u64 	%rd98, %rd98, %rd7;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd98;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd98, %r55;

$L__BB11_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB11_14;

	or.b64  	%rd35, %rd98, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB11_13;

	div.u64 	%rd98, %rd98, %rd8;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd98;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd98, %r58;

$L__BB11_14:
	cvt.u32.u64 	%r59, %rd98;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r59, 0, %p8;
	setp.eq.s64 	%p9, %rd26, 0;
	@%p9 bra 	$L__BB11_16;

	cvt.s64.s32 	%rd40, %r2;
	mul.lo.s64 	%rd41, %rd40, %rd9;
	add.s64 	%rd37, %rd26, %rd41;
	mov.f64 	%fd6, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd37],%fd6; }

	// end inline asm
	add.s64 	%rd38, %rd37, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd38],%fd6; }

	// end inline asm
	add.s64 	%rd39, %rd37, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd39],%fd6; }

	// end inline asm
	bra.uni 	$L__BB11_18;

$L__BB11_16:
	setp.eq.s64 	%p10, %rd25, 0;
	@%p10 bra 	$L__BB11_18;

	cvt.s64.s32 	%rd45, %r2;
	mul.lo.s64 	%rd46, %rd45, %rd10;
	add.s64 	%rd42, %rd25, %rd46;
	mov.f64 	%fd12, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd42],%fd12; }

	// end inline asm
	add.s64 	%rd43, %rd42, 8;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd43],%fd12; }

	// end inline asm
	add.s64 	%rd44, %rd42, 16;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd44],%fd12; }

	// end inline asm

$L__BB11_18:
	@%p9 bra 	$L__BB11_20;

	cvt.s64.s32 	%rd50, %r2;
	mul.lo.s64 	%rd51, %rd50, %rd9;
	add.s64 	%rd47, %rd26, %rd51;
	mov.f64 	%fd18, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd47],%fd18; }

	// end inline asm
	add.s64 	%rd48, %rd47, 8;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd48],%fd18; }

	// end inline asm
	add.s64 	%rd49, %rd47, 16;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd49],%fd18; }

	// end inline asm
	bra.uni 	$L__BB11_22;

$L__BB11_20:
	setp.eq.s64 	%p12, %rd25, 0;
	@%p12 bra 	$L__BB11_26;

	cvt.s64.s32 	%rd55, %r2;
	mul.lo.s64 	%rd56, %rd55, %rd10;
	add.s64 	%rd52, %rd25, %rd56;
	mov.f64 	%fd24, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd52],%fd24; }

	// end inline asm
	add.s64 	%rd53, %rd52, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd53],%fd24; }

	// end inline asm
	add.s64 	%rd54, %rd52, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd54],%fd24; }

	// end inline asm

$L__BB11_22:
	@%p9 bra 	$L__BB11_24;

	cvt.s64.s32 	%rd60, %r2;
	mul.lo.s64 	%rd61, %rd60, %rd9;
	add.s64 	%rd57, %rd26, %rd61;
	mov.f64 	%fd30, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd57],%fd30; }

	// end inline asm
	add.s64 	%rd58, %rd57, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd58],%fd30; }

	// end inline asm
	add.s64 	%rd59, %rd57, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd59],%fd30; }

	// end inline asm
	bra.uni 	$L__BB11_26;

$L__BB11_24:
	setp.eq.s64 	%p14, %rd25, 0;
	@%p14 bra 	$L__BB11_26;

	cvt.s64.s32 	%rd65, %r2;
	mul.lo.s64 	%rd66, %rd65, %rd10;
	add.s64 	%rd62, %rd25, %rd66;
	mov.f64 	%fd36, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd62],%fd36; }

	// end inline asm
	add.s64 	%rd63, %rd62, 8;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd63],%fd36; }

	// end inline asm
	add.s64 	%rd64, %rd62, 16;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd64],%fd36; }

	// end inline asm

$L__BB11_26:
	@%p9 bra 	$L__BB11_28;

	cvt.s64.s32 	%rd70, %r2;
	mul.lo.s64 	%rd71, %rd70, %rd9;
	add.s64 	%rd67, %rd26, %rd71;
	mov.f64 	%fd42, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd67],%fd42; }

	// end inline asm
	add.s64 	%rd68, %rd67, 8;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd68],%fd42; }

	// end inline asm
	add.s64 	%rd69, %rd67, 16;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd69],%fd42; }

	// end inline asm
	bra.uni 	$L__BB11_30;

$L__BB11_28:
	setp.eq.s64 	%p16, %rd25, 0;
	@%p16 bra 	$L__BB11_34;

	cvt.s64.s32 	%rd75, %r2;
	mul.lo.s64 	%rd76, %rd75, %rd10;
	add.s64 	%rd72, %rd25, %rd76;
	mov.f64 	%fd48, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd72],%fd48; }

	// end inline asm
	add.s64 	%rd73, %rd72, 8;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd73],%fd48; }

	// end inline asm
	add.s64 	%rd74, %rd72, 16;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd74],%fd48; }

	// end inline asm

$L__BB11_30:
	@%p9 bra 	$L__BB11_32;

	cvt.s64.s32 	%rd80, %r2;
	mul.lo.s64 	%rd81, %rd80, %rd9;
	add.s64 	%rd77, %rd26, %rd81;
	mov.f64 	%fd54, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd77],%fd54; }

	// end inline asm
	add.s64 	%rd78, %rd77, 8;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd78],%fd54; }

	// end inline asm
	add.s64 	%rd79, %rd77, 16;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd79],%fd54; }

	// end inline asm
	bra.uni 	$L__BB11_34;

$L__BB11_32:
	setp.eq.s64 	%p18, %rd25, 0;
	@%p18 bra 	$L__BB11_34;

	cvt.s64.s32 	%rd85, %r2;
	mul.lo.s64 	%rd86, %rd85, %rd10;
	add.s64 	%rd82, %rd25, %rd86;
	mov.f64 	%fd60, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd82],%fd60; }

	// end inline asm
	add.s64 	%rd83, %rd82, 8;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd83],%fd60; }

	// end inline asm
	add.s64 	%rd84, %rd82, 16;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd84],%fd60; }

	// end inline asm

$L__BB11_34:
	@%p9 bra 	$L__BB11_36;

	cvt.s64.s32 	%rd90, %r2;
	mul.lo.s64 	%rd91, %rd90, %rd9;
	add.s64 	%rd87, %rd26, %rd91;
	mov.f64 	%fd66, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd61,[%rd87],%fd66; }

	// end inline asm
	add.s64 	%rd88, %rd87, 8;
	// begin inline asm
	{ atom.add.f64 %fd63,[%rd88],%fd66; }

	// end inline asm
	add.s64 	%rd89, %rd87, 16;
	// begin inline asm
	{ atom.add.f64 %fd65,[%rd89],%fd66; }

	// end inline asm
	bra.uni 	$L__BB11_38;

$L__BB11_36:
	setp.eq.s64 	%p20, %rd25, 0;
	@%p20 bra 	$L__BB11_38;

	cvt.s64.s32 	%rd95, %r2;
	mul.lo.s64 	%rd96, %rd95, %rd10;
	add.s64 	%rd92, %rd25, %rd96;
	mov.f64 	%fd72, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd67,[%rd92],%fd72; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	// begin inline asm
	{ atom.add.f64 %fd69,[%rd93],%fd72; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd71,[%rd94],%fd72; }

	// end inline asm

$L__BB11_38:
	add.s64 	%rd97, %rd97, %rd11;
	setp.lt.u64 	%p21, %rd97, %rd23;
	@%p21 bra 	$L__BB11_2;

$L__BB11_39:
	ret;

}
	// .globl	safeguard_direction_x_kernel_cuda_kernel_forward
.visible .entry safeguard_direction_x_kernel_cuda_kernel_forward(
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_forward_param_3[56]
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<92>;
	.reg .b64 	%rd<100>;


	ld.param.v2.u32 	{%r34, %r35}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r36, %r37}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r42, %r43}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r50, %r51}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r58, %r59}, [safeguard_direction_x_kernel_cuda_kernel_forward_param_3+32];
	ld.param.u64 	%rd46, [safeguard_direction_x_kernel_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd44, [safeguard_direction_x_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd42, [safeguard_direction_x_kernel_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd41, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [safeguard_direction_x_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r62, %ntid.x;
	cvt.u64.u32 	%rd1, %r62;
	mov.u32 	%r63, %ctaid.x;
	mul.wide.u32 	%rd48, %r62, %r63;
	mov.u32 	%r64, %tid.x;
	cvt.u64.u32 	%rd49, %r64;
	add.s64 	%rd91, %rd48, %rd49;
	setp.ge.u64 	%p1, %rd91, %rd41;
	@%p1 bra 	$L__BB12_37;

	cvta.to.global.u64 	%rd4, %rd46;
	cvta.to.global.u64 	%rd5, %rd44;
	cvta.to.global.u64 	%rd6, %rd42;
	cvt.s64.s32 	%rd7, %r37;
	cvt.s64.s32 	%rd8, %r36;
	cvt.s64.s32 	%rd9, %r35;
	cvt.s64.s32 	%rd10, %r50;
	cvt.s64.s32 	%rd11, %r58;
	cvt.s64.s32 	%rd12, %r42;
	mov.u32 	%r65, %nctaid.x;
	cvt.u64.u32 	%rd50, %r65;
	mul.lo.s64 	%rd13, %rd1, %rd50;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB12_22;
	bra.uni 	$L__BB12_2;

$L__BB12_22:
	cvt.u32.u64 	%r79, %rd7;
	cvt.u32.u64 	%r82, %rd8;
	cvt.u32.u64 	%r85, %rd9;

$L__BB12_23:
	or.b64  	%rd75, %rd91, %rd7;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p15, %rd76, 0;
	@%p15 bra 	$L__BB12_25;

	div.u64 	%rd98, %rd91, %rd7;
	bra.uni 	$L__BB12_26;

$L__BB12_25:
	cvt.u32.u64 	%r80, %rd91;
	div.u32 	%r81, %r80, %r79;
	cvt.u64.u32 	%rd98, %r81;

$L__BB12_26:
	setp.lt.s32 	%p16, %r6, 3;
	@%p16 bra 	$L__BB12_30;

	or.b64  	%rd77, %rd98, %rd8;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p17, %rd78, 0;
	@%p17 bra 	$L__BB12_29;

	div.u64 	%rd98, %rd98, %rd8;
	bra.uni 	$L__BB12_30;

$L__BB12_29:
	cvt.u32.u64 	%r83, %rd98;
	div.u32 	%r84, %r83, %r82;
	cvt.u64.u32 	%rd98, %r84;

$L__BB12_30:
	setp.lt.s32 	%p18, %r6, 2;
	@%p18 bra 	$L__BB12_34;

	or.b64  	%rd79, %rd98, %rd9;
	and.b64  	%rd80, %rd79, -4294967296;
	setp.eq.s64 	%p19, %rd80, 0;
	@%p19 bra 	$L__BB12_33;

	div.u64 	%rd98, %rd98, %rd9;
	bra.uni 	$L__BB12_34;

$L__BB12_33:
	cvt.u32.u64 	%r86, %rd98;
	div.u32 	%r87, %r86, %r85;
	cvt.u64.u32 	%rd98, %r87;

$L__BB12_34:
	cvt.s64.s32 	%rd81, %rd98;
	setp.gt.s32 	%p20, %r6, 0;
	selp.b64 	%rd39, %rd81, 0, %p20;
	mul.lo.s64 	%rd82, %rd39, %rd10;
	add.s64 	%rd83, %rd5, %rd82;
	ld.global.s32 	%rd84, [%rd83];
	mul.lo.s64 	%rd85, %rd84, %rd11;
	add.s64 	%rd86, %rd4, %rd85;
	ld.global.u32 	%r88, [%rd86];
	add.s32 	%r89, %r88, -1;
	setp.gt.u32 	%p21, %r89, 1;
	@%p21 bra 	$L__BB12_36;

	mul.lo.s64 	%rd87, %rd39, %rd12;
	add.s64 	%rd88, %rd6, %rd87;
	mov.u64 	%rd89, 0;
	st.global.u64 	[%rd88], %rd89;
	st.global.u64 	[%rd88+8], %rd89;
	st.global.u64 	[%rd88+16], %rd89;

$L__BB12_36:
	mul.wide.u32 	%rd90, %r62, %r65;
	add.s64 	%rd91, %rd91, %rd90;
	setp.lt.u64 	%p22, %rd91, %rd41;
	@%p22 bra 	$L__BB12_23;
	bra.uni 	$L__BB12_37;

$L__BB12_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB12_11;
	bra.uni 	$L__BB12_3;

$L__BB12_11:
	cvt.u32.u64 	%r71, %rd8;
	cvt.u32.u64 	%r74, %rd9;

$L__BB12_12:
	or.b64  	%rd62, %rd91, %rd8;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p9, %rd63, 0;
	@%p9 bra 	$L__BB12_14;

	div.u64 	%rd95, %rd91, %rd8;
	bra.uni 	$L__BB12_15;

$L__BB12_14:
	cvt.u32.u64 	%r72, %rd91;
	div.u32 	%r73, %r72, %r71;
	cvt.u64.u32 	%rd95, %r73;

$L__BB12_15:
	setp.lt.s32 	%p10, %r6, 2;
	@%p10 bra 	$L__BB12_19;

	or.b64  	%rd64, %rd95, %rd9;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p11, %rd65, 0;
	@%p11 bra 	$L__BB12_18;

	div.u64 	%rd95, %rd95, %rd9;
	bra.uni 	$L__BB12_19;

$L__BB12_18:
	cvt.u32.u64 	%r75, %rd95;
	div.u32 	%r76, %r75, %r74;
	cvt.u64.u32 	%rd95, %r76;

$L__BB12_19:
	cvt.s64.s32 	%rd66, %rd95;
	setp.gt.s32 	%p12, %r6, 0;
	selp.b64 	%rd27, %rd66, 0, %p12;
	mul.lo.s64 	%rd67, %rd27, %rd10;
	add.s64 	%rd68, %rd5, %rd67;
	ld.global.s32 	%rd69, [%rd68];
	mul.lo.s64 	%rd70, %rd69, %rd11;
	add.s64 	%rd71, %rd4, %rd70;
	ld.global.u32 	%r77, [%rd71];
	add.s32 	%r78, %r77, -1;
	setp.gt.u32 	%p13, %r78, 1;
	@%p13 bra 	$L__BB12_21;

	mul.lo.s64 	%rd72, %rd27, %rd12;
	add.s64 	%rd73, %rd6, %rd72;
	mov.u64 	%rd74, 0;
	st.global.u64 	[%rd73], %rd74;
	st.global.u64 	[%rd73+8], %rd74;
	st.global.u64 	[%rd73+16], %rd74;

$L__BB12_21:
	add.s64 	%rd91, %rd91, %rd13;
	setp.lt.u64 	%p14, %rd91, %rd41;
	@%p14 bra 	$L__BB12_12;
	bra.uni 	$L__BB12_37;

$L__BB12_3:
	cvt.u32.u64 	%r66, %rd9;

$L__BB12_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd92, %rd91;
	@%p4 bra 	$L__BB12_8;

	or.b64  	%rd51, %rd91, %rd9;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.eq.s64 	%p5, %rd52, 0;
	@%p5 bra 	$L__BB12_7;

	div.u64 	%rd92, %rd91, %rd9;
	bra.uni 	$L__BB12_8;

$L__BB12_7:
	cvt.u32.u64 	%r67, %rd91;
	div.u32 	%r68, %r67, %r66;
	cvt.u64.u32 	%rd92, %r68;

$L__BB12_8:
	cvt.s64.s32 	%rd53, %rd92;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd18, %rd53, 0, %p6;
	mul.lo.s64 	%rd54, %rd18, %rd10;
	add.s64 	%rd55, %rd5, %rd54;
	ld.global.s32 	%rd56, [%rd55];
	mul.lo.s64 	%rd57, %rd56, %rd11;
	add.s64 	%rd58, %rd4, %rd57;
	ld.global.u32 	%r69, [%rd58];
	add.s32 	%r70, %r69, -1;
	setp.gt.u32 	%p7, %r70, 1;
	@%p7 bra 	$L__BB12_10;

	mul.lo.s64 	%rd59, %rd18, %rd12;
	add.s64 	%rd60, %rd6, %rd59;
	mov.u64 	%rd61, 0;
	st.global.u64 	[%rd60], %rd61;
	st.global.u64 	[%rd60+8], %rd61;
	st.global.u64 	[%rd60+16], %rd61;

$L__BB12_10:
	add.s64 	%rd91, %rd91, %rd13;
	setp.lt.u64 	%p8, %rd91, %rd41;
	@%p8 bra 	$L__BB12_4;

$L__BB12_37:
	ret;

}
	// .globl	safeguard_direction_x_kernel_cuda_kernel_backward
.visible .entry safeguard_direction_x_kernel_cuda_kernel_backward(
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 safeguard_direction_x_kernel_cuda_kernel_backward_param_6[56]
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<94>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<80>;


	ld.param.v2.u32 	{%r43, %r44}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [safeguard_direction_x_kernel_cuda_kernel_backward_param_4+32];
	ld.param.u64 	%rd35, [safeguard_direction_x_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd33, [safeguard_direction_x_kernel_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd31, [safeguard_direction_x_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd30, [safeguard_direction_x_kernel_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd28, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [safeguard_direction_x_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd37, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd38, %r81;
	add.s64 	%rd76, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd76, %rd28;
	@%p1 bra 	$L__BB13_28;

	cvta.to.global.u64 	%rd6, %rd33;
	cvta.to.global.u64 	%rd7, %rd31;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r59;
	cvt.s64.s32 	%rd12, %r67;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd39, %r82;
	mul.lo.s64 	%rd13, %rd1, %rd39;
	cvt.s64.s32 	%rd14, %r75;
	cvt.s64.s32 	%rd15, %r51;

$L__BB13_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd77, %rd76;
	@%p2 bra 	$L__BB13_6;

	or.b64  	%rd40, %rd76, %rd8;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p3, %rd41, 0;
	@%p3 bra 	$L__BB13_5;

	div.u64 	%rd77, %rd76, %rd8;
	bra.uni 	$L__BB13_6;

$L__BB13_5:
	cvt.u32.u64 	%r83, %rd8;
	cvt.u32.u64 	%r84, %rd76;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd77, %r85;

$L__BB13_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB13_10;

	or.b64  	%rd42, %rd77, %rd9;
	and.b64  	%rd43, %rd42, -4294967296;
	setp.eq.s64 	%p5, %rd43, 0;
	@%p5 bra 	$L__BB13_9;

	div.u64 	%rd77, %rd77, %rd9;
	bra.uni 	$L__BB13_10;

$L__BB13_9:
	cvt.u32.u64 	%r86, %rd9;
	cvt.u32.u64 	%r87, %rd77;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd77, %r88;

$L__BB13_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB13_14;

	or.b64  	%rd44, %rd77, %rd10;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p7, %rd45, 0;
	@%p7 bra 	$L__BB13_13;

	div.u64 	%rd77, %rd77, %rd10;
	bra.uni 	$L__BB13_14;

$L__BB13_13:
	cvt.u32.u64 	%r89, %rd10;
	cvt.u32.u64 	%r90, %rd77;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd77, %r91;

$L__BB13_14:
	cvt.s64.s32 	%rd46, %rd77;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd26, %rd46, 0, %p8;
	mul.lo.s64 	%rd47, %rd26, %rd11;
	add.s64 	%rd48, %rd7, %rd47;
	ld.global.s32 	%rd49, [%rd48];
	mul.lo.s64 	%rd50, %rd49, %rd12;
	add.s64 	%rd51, %rd6, %rd50;
	ld.global.u32 	%r92, [%rd51];
	add.s32 	%r93, %r92, -1;
	setp.gt.u32 	%p9, %r93, 1;
	@%p9 bra 	$L__BB13_27;

	setp.eq.s64 	%p10, %rd35, 0;
	@%p10 bra 	$L__BB13_17;

	mul.lo.s64 	%rd55, %rd26, %rd14;
	add.s64 	%rd52, %rd35, %rd55;
	mov.f64 	%fd6, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd52],%fd6; }

	// end inline asm
	add.s64 	%rd53, %rd52, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd53],%fd6; }

	// end inline asm
	add.s64 	%rd54, %rd52, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd54],%fd6; }

	// end inline asm
	bra.uni 	$L__BB13_19;

$L__BB13_17:
	setp.eq.s64 	%p11, %rd30, 0;
	@%p11 bra 	$L__BB13_27;

	mul.lo.s64 	%rd59, %rd26, %rd15;
	add.s64 	%rd56, %rd30, %rd59;
	mov.f64 	%fd12, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd56],%fd12; }

	// end inline asm
	add.s64 	%rd57, %rd56, 8;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd57],%fd12; }

	// end inline asm
	add.s64 	%rd58, %rd56, 16;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd58],%fd12; }

	// end inline asm

$L__BB13_19:
	@%p10 bra 	$L__BB13_21;

	mul.lo.s64 	%rd63, %rd26, %rd14;
	add.s64 	%rd60, %rd35, %rd63;
	mov.f64 	%fd18, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd60],%fd18; }

	// end inline asm
	add.s64 	%rd61, %rd60, 8;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd61],%fd18; }

	// end inline asm
	add.s64 	%rd62, %rd60, 16;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd62],%fd18; }

	// end inline asm
	bra.uni 	$L__BB13_23;

$L__BB13_21:
	setp.eq.s64 	%p13, %rd30, 0;
	@%p13 bra 	$L__BB13_27;

	mul.lo.s64 	%rd67, %rd26, %rd15;
	add.s64 	%rd64, %rd30, %rd67;
	mov.f64 	%fd24, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd64],%fd24; }

	// end inline asm
	add.s64 	%rd65, %rd64, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd65],%fd24; }

	// end inline asm
	add.s64 	%rd66, %rd64, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd66],%fd24; }

	// end inline asm

$L__BB13_23:
	@%p10 bra 	$L__BB13_25;

	mul.lo.s64 	%rd71, %rd26, %rd14;
	add.s64 	%rd68, %rd35, %rd71;
	mov.f64 	%fd30, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd68],%fd30; }

	// end inline asm
	add.s64 	%rd69, %rd68, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd69],%fd30; }

	// end inline asm
	add.s64 	%rd70, %rd68, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd70],%fd30; }

	// end inline asm
	bra.uni 	$L__BB13_27;

$L__BB13_25:
	setp.eq.s64 	%p15, %rd30, 0;
	@%p15 bra 	$L__BB13_27;

	mul.lo.s64 	%rd75, %rd26, %rd15;
	add.s64 	%rd72, %rd30, %rd75;
	mov.f64 	%fd36, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd72],%fd36; }

	// end inline asm
	add.s64 	%rd73, %rd72, 8;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd73],%fd36; }

	// end inline asm
	add.s64 	%rd74, %rd72, 16;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd74],%fd36; }

	// end inline asm

$L__BB13_27:
	add.s64 	%rd76, %rd76, %rd13;
	setp.lt.u64 	%p16, %rd76, %rd28;
	@%p16 bra 	$L__BB13_2;

$L__BB13_28:
	ret;

}
	// .globl	step_affine_y_cuda_kernel_forward
.visible .entry step_affine_y_cuda_kernel_forward(
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_forward_param_5[56]
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<41>;
	.reg .b32 	%r<115>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<75>;


	ld.param.v2.u32 	{%r52, %r53}, [step_affine_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r54, %r55}, [step_affine_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r60, %r61}, [step_affine_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r68, %r69}, [step_affine_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r76, %r77}, [step_affine_y_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r84, %r85}, [step_affine_y_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r92, %r93}, [step_affine_y_cuda_kernel_forward_param_5+32];
	ld.param.u64 	%rd38, [step_affine_y_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd36, [step_affine_y_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd34, [step_affine_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd32, [step_affine_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd30, [step_affine_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd29, [step_affine_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [step_affine_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r96, %ntid.x;
	cvt.u64.u32 	%rd1, %r96;
	mov.u32 	%r97, %ctaid.x;
	mul.wide.u32 	%rd40, %r96, %r97;
	mov.u32 	%r98, %tid.x;
	cvt.u64.u32 	%rd41, %r98;
	add.s64 	%rd71, %rd40, %rd41;
	setp.ge.u64 	%p1, %rd71, %rd29;
	@%p1 bra 	$L__BB14_15;

	cvta.to.global.u64 	%rd4, %rd38;
	cvta.to.global.u64 	%rd5, %rd36;
	cvta.to.global.u64 	%rd6, %rd34;
	cvta.to.global.u64 	%rd7, %rd32;
	cvta.to.global.u64 	%rd8, %rd30;
	cvt.s64.s32 	%rd9, %r55;
	cvt.s64.s32 	%rd10, %r54;
	cvt.s64.s32 	%rd11, %r53;
	cvt.s64.s32 	%rd12, %r68;
	cvt.s64.s32 	%rd13, %r60;
	cvt.s64.s32 	%rd14, %r92;
	cvt.s64.s32 	%rd15, %r76;
	cvt.s64.s32 	%rd16, %r84;
	mov.u32 	%r99, %nctaid.x;
	cvt.u64.u32 	%rd42, %r99;
	mul.lo.s64 	%rd17, %rd1, %rd42;

$L__BB14_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd72, %rd71;
	@%p2 bra 	$L__BB14_6;

	or.b64  	%rd43, %rd71, %rd9;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p3, %rd44, 0;
	@%p3 bra 	$L__BB14_5;

	div.u64 	%rd72, %rd71, %rd9;
	bra.uni 	$L__BB14_6;

$L__BB14_5:
	cvt.u32.u64 	%r100, %rd9;
	cvt.u32.u64 	%r101, %rd71;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd72, %r102;

$L__BB14_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB14_10;

	or.b64  	%rd45, %rd72, %rd10;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p5, %rd46, 0;
	@%p5 bra 	$L__BB14_9;

	div.u64 	%rd72, %rd72, %rd10;
	bra.uni 	$L__BB14_10;

$L__BB14_9:
	cvt.u32.u64 	%r103, %rd10;
	cvt.u32.u64 	%r104, %rd72;
	div.u32 	%r105, %r104, %r103;
	cvt.u64.u32 	%rd72, %r105;

$L__BB14_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB14_14;

	or.b64  	%rd47, %rd72, %rd11;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p7, %rd48, 0;
	@%p7 bra 	$L__BB14_13;

	div.u64 	%rd72, %rd72, %rd11;
	bra.uni 	$L__BB14_14;

$L__BB14_13:
	cvt.u32.u64 	%r106, %rd11;
	cvt.u32.u64 	%r107, %rd72;
	div.u32 	%r108, %r107, %r106;
	cvt.u64.u32 	%rd72, %r108;

$L__BB14_14:
	cvt.u32.u64 	%r109, %rd72;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r110, %r109, 0, %p8;
	shl.b32 	%r111, %r110, 2;
	cvt.s64.s32 	%rd49, %r111;
	mul.lo.s64 	%rd50, %rd49, %rd12;
	add.s64 	%rd51, %rd7, %rd50;
	or.b32  	%r112, %r111, 1;
	cvt.s64.s32 	%rd52, %r112;
	mul.lo.s64 	%rd53, %rd52, %rd12;
	add.s64 	%rd54, %rd7, %rd53;
	or.b32  	%r113, %r111, 2;
	cvt.s64.s32 	%rd55, %r113;
	mul.lo.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd57, %rd7, %rd56;
	or.b32  	%r114, %r111, 3;
	cvt.s64.s32 	%rd58, %r114;
	mul.lo.s64 	%rd59, %rd58, %rd12;
	add.s64 	%rd60, %rd7, %rd59;
	cvt.s64.s32 	%rd61, %r110;
	mul.lo.s64 	%rd62, %rd61, %rd13;
	mul.lo.s64 	%rd63, %rd61, %rd14;
	add.s64 	%rd64, %rd4, %rd63;
	ld.global.s32 	%rd65, [%rd64];
	mul.lo.s64 	%rd66, %rd65, %rd15;
	add.s64 	%rd67, %rd6, %rd66;
	ld.global.f64 	%fd1, [%rd67];
	ld.global.f64 	%fd2, [%rd51];
	ld.global.f64 	%fd3, [%rd51+8];
	ld.global.f64 	%fd4, [%rd51+16];
	ld.global.f64 	%fd5, [%rd54];
	ld.global.f64 	%fd6, [%rd54+8];
	ld.global.f64 	%fd7, [%rd54+16];
	ld.global.f64 	%fd8, [%rd57];
	ld.global.f64 	%fd9, [%rd57+8];
	ld.global.f64 	%fd10, [%rd57+16];
	ld.global.f64 	%fd11, [%rd60];
	ld.global.f64 	%fd12, [%rd60+8];
	ld.global.f64 	%fd13, [%rd60+16];
	add.s64 	%rd68, %rd8, %rd62;
	ld.global.f64 	%fd14, [%rd68];
	fma.rn.f64 	%fd15, %fd1, %fd2, %fd14;
	ld.global.f64 	%fd16, [%rd68+8];
	fma.rn.f64 	%fd17, %fd1, %fd3, %fd16;
	ld.global.f64 	%fd18, [%rd68+16];
	fma.rn.f64 	%fd19, %fd1, %fd4, %fd18;
	ld.global.f64 	%fd20, [%rd68+24];
	fma.rn.f64 	%fd21, %fd1, %fd5, %fd20;
	ld.global.f64 	%fd22, [%rd68+32];
	fma.rn.f64 	%fd23, %fd1, %fd6, %fd22;
	ld.global.f64 	%fd24, [%rd68+40];
	fma.rn.f64 	%fd25, %fd1, %fd7, %fd24;
	ld.global.f64 	%fd26, [%rd68+48];
	fma.rn.f64 	%fd27, %fd1, %fd8, %fd26;
	ld.global.f64 	%fd28, [%rd68+56];
	fma.rn.f64 	%fd29, %fd1, %fd9, %fd28;
	ld.global.f64 	%fd30, [%rd68+64];
	fma.rn.f64 	%fd31, %fd1, %fd10, %fd30;
	ld.global.f64 	%fd32, [%rd68+72];
	fma.rn.f64 	%fd33, %fd1, %fd11, %fd32;
	ld.global.f64 	%fd34, [%rd68+80];
	fma.rn.f64 	%fd35, %fd1, %fd12, %fd34;
	ld.global.f64 	%fd36, [%rd68+88];
	fma.rn.f64 	%fd37, %fd1, %fd13, %fd36;
	mul.lo.s64 	%rd69, %rd61, %rd16;
	add.s64 	%rd70, %rd5, %rd69;
	st.global.f64 	[%rd70], %fd15;
	st.global.f64 	[%rd70+8], %fd17;
	st.global.f64 	[%rd70+16], %fd19;
	st.global.f64 	[%rd70+24], %fd21;
	st.global.f64 	[%rd70+32], %fd23;
	st.global.f64 	[%rd70+40], %fd25;
	st.global.f64 	[%rd70+48], %fd27;
	st.global.f64 	[%rd70+56], %fd29;
	st.global.f64 	[%rd70+64], %fd31;
	st.global.f64 	[%rd70+72], %fd33;
	st.global.f64 	[%rd70+80], %fd35;
	st.global.f64 	[%rd70+88], %fd37;
	add.s64 	%rd71, %rd71, %rd17;
	setp.lt.u64 	%p9, %rd71, %rd29;
	@%p9 bra 	$L__BB14_2;

$L__BB14_15:
	ret;

}
	// .globl	step_affine_y_cuda_kernel_backward
.visible .entry step_affine_y_cuda_kernel_backward(
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 step_affine_y_cuda_kernel_backward_param_10[56]
)
{
	.reg .pred 	%p<40>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<180>;
	.reg .f64 	%fd<331>;
	.reg .b64 	%rd<222>;


	ld.param.v2.u32 	{%r88, %r89}, [step_affine_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r90, %r91}, [step_affine_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r96, %r97}, [step_affine_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r104, %r105}, [step_affine_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r112, %r113}, [step_affine_y_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r120, %r121}, [step_affine_y_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r128, %r129}, [step_affine_y_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r136, %r137}, [step_affine_y_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r144, %r145}, [step_affine_y_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r152, %r153}, [step_affine_y_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r160, %r161}, [step_affine_y_cuda_kernel_backward_param_9+32];
	ld.param.u64 	%rd66, [step_affine_y_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd64, [step_affine_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd62, [step_affine_y_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd60, [step_affine_y_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd58, [step_affine_y_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd57, [step_affine_y_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd55, [step_affine_y_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd54, [step_affine_y_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd53, [step_affine_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd52, [step_affine_y_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd51, [step_affine_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd49, [step_affine_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [step_affine_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r164, %ntid.x;
	cvt.u64.u32 	%rd1, %r164;
	mov.u32 	%r165, %ctaid.x;
	mul.wide.u32 	%rd68, %r164, %r165;
	mov.u32 	%r166, %tid.x;
	cvt.u64.u32 	%rd69, %r166;
	add.s64 	%rd218, %rd68, %rd69;
	setp.ge.u64 	%p1, %rd218, %rd49;
	@%p1 bra 	$L__BB15_75;

	cvta.to.global.u64 	%rd12, %rd66;
	cvta.to.global.u64 	%rd13, %rd58;
	cvta.to.global.u64 	%rd14, %rd57;
	cvta.to.global.u64 	%rd15, %rd54;
	cvta.to.global.u64 	%rd16, %rd52;
	cvt.s64.s32 	%rd17, %r91;
	cvt.s64.s32 	%rd18, %r90;
	cvt.s64.s32 	%rd19, %r89;
	cvt.s64.s32 	%rd20, %r104;
	cvt.s64.s32 	%rd21, %r96;
	cvt.s64.s32 	%rd22, %r128;
	cvt.s64.s32 	%rd23, %r112;
	cvt.s64.s32 	%rd24, %r160;
	cvt.s64.s32 	%rd25, %r120;
	cvt.s64.s32 	%rd26, %r152;
	cvt.s64.s32 	%rd27, %r136;
	cvt.s64.s32 	%rd28, %r144;
	mov.u32 	%r167, %nctaid.x;
	cvt.u64.u32 	%rd70, %r167;
	mul.lo.s64 	%rd29, %rd1, %rd70;

$L__BB15_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd219, %rd218;
	@%p2 bra 	$L__BB15_6;

	or.b64  	%rd71, %rd218, %rd17;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p3, %rd72, 0;
	@%p3 bra 	$L__BB15_5;

	div.u64 	%rd219, %rd218, %rd17;
	bra.uni 	$L__BB15_6;

$L__BB15_5:
	cvt.u32.u64 	%r168, %rd17;
	cvt.u32.u64 	%r169, %rd218;
	div.u32 	%r170, %r169, %r168;
	cvt.u64.u32 	%rd219, %r170;

$L__BB15_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB15_10;

	or.b64  	%rd73, %rd219, %rd18;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB15_9;

	div.u64 	%rd219, %rd219, %rd18;
	bra.uni 	$L__BB15_10;

$L__BB15_9:
	cvt.u32.u64 	%r171, %rd18;
	cvt.u32.u64 	%r172, %rd219;
	div.u32 	%r173, %r172, %r171;
	cvt.u64.u32 	%rd219, %r173;

$L__BB15_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB15_14;

	or.b64  	%rd75, %rd219, %rd19;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB15_13;

	div.u64 	%rd219, %rd219, %rd19;
	bra.uni 	$L__BB15_14;

$L__BB15_13:
	cvt.u32.u64 	%r174, %rd19;
	cvt.u32.u64 	%r175, %rd219;
	div.u32 	%r176, %r175, %r174;
	cvt.u64.u32 	%rd219, %r176;

$L__BB15_14:
	cvt.s64.s32 	%rd215, %r112;
	ld.param.u64 	%rd214, [step_affine_y_cuda_kernel_backward_param_9];
	cvt.u32.u64 	%r177, %rd219;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r178, %r177, 0, %p8;
	shl.b32 	%r179, %r178, 2;
	cvt.s64.s32 	%rd40, %r179;
	mul.lo.s64 	%rd41, %rd40, %rd20;
	add.s64 	%rd77, %rd16, %rd41;
	ld.global.f64 	%fd1, [%rd77];
	ld.global.f64 	%fd2, [%rd77+8];
	ld.global.f64 	%fd3, [%rd77+16];
	or.b64  	%rd78, %rd40, 1;
	mul.lo.s64 	%rd42, %rd78, %rd20;
	add.s64 	%rd79, %rd16, %rd42;
	ld.global.f64 	%fd4, [%rd79];
	ld.global.f64 	%fd5, [%rd79+8];
	ld.global.f64 	%fd6, [%rd79+16];
	or.b64  	%rd80, %rd40, 2;
	mul.lo.s64 	%rd43, %rd80, %rd20;
	add.s64 	%rd81, %rd16, %rd43;
	ld.global.f64 	%fd7, [%rd81];
	ld.global.f64 	%fd8, [%rd81+8];
	ld.global.f64 	%fd9, [%rd81+16];
	or.b64  	%rd82, %rd40, 3;
	mul.lo.s64 	%rd44, %rd82, %rd20;
	add.s64 	%rd83, %rd16, %rd44;
	ld.global.f64 	%fd10, [%rd83];
	ld.global.f64 	%fd11, [%rd83+8];
	ld.global.f64 	%fd12, [%rd83+16];
	cvt.s64.s32 	%rd45, %r178;
	mul.lo.s64 	%rd84, %rd45, %rd22;
	add.s64 	%rd85, %rd13, %rd84;
	ld.global.s32 	%rd46, [%rd85];
	mul.lo.s64 	%rd47, %rd46, %rd215;
	add.s64 	%rd86, %rd15, %rd47;
	ld.global.f64 	%fd13, [%rd86];
	setp.eq.s64 	%p9, %rd214, 0;
	@%p9 bra 	$L__BB15_16;

	mul.lo.s64 	%rd87, %rd45, %rd24;
	add.s64 	%rd88, %rd12, %rd87;
	ld.global.f64 	%fd75, [%rd88];
	add.f64 	%fd330, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd88+8];
	add.f64 	%fd329, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd88+16];
	add.f64 	%fd328, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd88+24];
	add.f64 	%fd327, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd88+32];
	add.f64 	%fd326, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd88+40];
	add.f64 	%fd325, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd88+48];
	add.f64 	%fd324, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd88+56];
	add.f64 	%fd323, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd88+64];
	add.f64 	%fd322, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd88+72];
	add.f64 	%fd321, %fd84, 0d0000000000000000;
	ld.global.f64 	%fd85, [%rd88+80];
	add.f64 	%fd320, %fd85, 0d0000000000000000;
	ld.global.f64 	%fd86, [%rd88+88];
	add.f64 	%fd319, %fd86, 0d0000000000000000;
	bra.uni 	$L__BB15_18;

$L__BB15_16:
	ld.param.u64 	%rd216, [step_affine_y_cuda_kernel_backward_param_4+8];
	setp.eq.s64 	%p10, %rd216, 0;
	mov.f64 	%fd319, 0d0000000000000000;
	mov.f64 	%fd320, %fd319;
	mov.f64 	%fd321, %fd319;
	mov.f64 	%fd322, %fd319;
	mov.f64 	%fd323, %fd319;
	mov.f64 	%fd324, %fd319;
	mov.f64 	%fd325, %fd319;
	mov.f64 	%fd326, %fd319;
	mov.f64 	%fd327, %fd319;
	mov.f64 	%fd328, %fd319;
	mov.f64 	%fd329, %fd319;
	mov.f64 	%fd330, %fd319;
	@%p10 bra 	$L__BB15_18;

	mul.lo.s64 	%rd89, %rd45, %rd25;
	add.s64 	%rd90, %rd14, %rd89;
	ld.global.f64 	%fd99, [%rd90];
	add.f64 	%fd330, %fd99, 0d0000000000000000;
	ld.global.f64 	%fd100, [%rd90+8];
	add.f64 	%fd329, %fd100, 0d0000000000000000;
	ld.global.f64 	%fd101, [%rd90+16];
	add.f64 	%fd328, %fd101, 0d0000000000000000;
	ld.global.f64 	%fd102, [%rd90+24];
	add.f64 	%fd327, %fd102, 0d0000000000000000;
	ld.global.f64 	%fd103, [%rd90+32];
	add.f64 	%fd326, %fd103, 0d0000000000000000;
	ld.global.f64 	%fd104, [%rd90+40];
	add.f64 	%fd325, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd90+48];
	add.f64 	%fd324, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd90+56];
	add.f64 	%fd323, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd90+64];
	add.f64 	%fd322, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd90+72];
	add.f64 	%fd321, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd90+80];
	add.f64 	%fd320, %fd109, 0d0000000000000000;
	ld.global.f64 	%fd110, [%rd90+88];
	add.f64 	%fd319, %fd110, 0d0000000000000000;

$L__BB15_18:
	add.f64 	%fd50, %fd330, 0d0000000000000000;
	fma.rn.f64 	%fd51, %fd13, %fd50, 0d0000000000000000;
	add.f64 	%fd52, %fd329, 0d0000000000000000;
	fma.rn.f64 	%fd53, %fd13, %fd52, 0d0000000000000000;
	add.f64 	%fd54, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd55, %fd13, %fd54, 0d0000000000000000;
	add.f64 	%fd56, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd57, %fd13, %fd56, 0d0000000000000000;
	add.f64 	%fd58, %fd326, 0d0000000000000000;
	fma.rn.f64 	%fd59, %fd13, %fd58, 0d0000000000000000;
	add.f64 	%fd60, %fd325, 0d0000000000000000;
	fma.rn.f64 	%fd61, %fd13, %fd60, 0d0000000000000000;
	add.f64 	%fd62, %fd324, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd13, %fd62, 0d0000000000000000;
	add.f64 	%fd64, %fd323, 0d0000000000000000;
	fma.rn.f64 	%fd65, %fd13, %fd64, 0d0000000000000000;
	add.f64 	%fd66, %fd322, 0d0000000000000000;
	fma.rn.f64 	%fd67, %fd13, %fd66, 0d0000000000000000;
	add.f64 	%fd68, %fd321, 0d0000000000000000;
	fma.rn.f64 	%fd69, %fd13, %fd68, 0d0000000000000000;
	add.f64 	%fd70, %fd320, 0d0000000000000000;
	fma.rn.f64 	%fd71, %fd13, %fd70, 0d0000000000000000;
	add.f64 	%fd72, %fd319, 0d0000000000000000;
	fma.rn.f64 	%fd73, %fd13, %fd72, 0d0000000000000000;
	fma.rn.f64 	%fd111, %fd1, %fd50, 0d0000000000000000;
	fma.rn.f64 	%fd112, %fd2, %fd52, %fd111;
	fma.rn.f64 	%fd113, %fd3, %fd54, %fd112;
	fma.rn.f64 	%fd114, %fd4, %fd56, %fd113;
	fma.rn.f64 	%fd115, %fd5, %fd58, %fd114;
	fma.rn.f64 	%fd116, %fd6, %fd60, %fd115;
	fma.rn.f64 	%fd117, %fd7, %fd62, %fd116;
	fma.rn.f64 	%fd118, %fd8, %fd64, %fd117;
	fma.rn.f64 	%fd119, %fd9, %fd66, %fd118;
	fma.rn.f64 	%fd120, %fd10, %fd68, %fd119;
	fma.rn.f64 	%fd121, %fd11, %fd70, %fd120;
	fma.rn.f64 	%fd122, %fd12, %fd72, %fd121;
	add.f64 	%fd74, %fd122, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd64, 0;
	@%p11 bra 	$L__BB15_20;

	mul.lo.s64 	%rd92, %rd46, %rd26;
	add.s64 	%rd91, %rd64, %rd92;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd91],%fd74; }

	// end inline asm
	bra.uni 	$L__BB15_22;

$L__BB15_20:
	setp.eq.s64 	%p12, %rd55, 0;
	@%p12 bra 	$L__BB15_22;

	mul.lo.s64 	%rd217, %rd46, %rd215;
	add.s64 	%rd93, %rd55, %rd217;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd93],%fd74; }

	// end inline asm

$L__BB15_22:
	setp.eq.s64 	%p13, %rd60, 0;
	@%p13 bra 	$L__BB15_24;

	mul.lo.s64 	%rd106, %rd45, %rd27;
	add.s64 	%rd94, %rd60, %rd106;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd94],%fd50; }

	// end inline asm
	add.s64 	%rd95, %rd94, 8;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd95],%fd52; }

	// end inline asm
	add.s64 	%rd96, %rd94, 16;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd96],%fd54; }

	// end inline asm
	add.s64 	%rd97, %rd94, 24;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd97],%fd56; }

	// end inline asm
	add.s64 	%rd98, %rd94, 32;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd98],%fd58; }

	// end inline asm
	add.s64 	%rd99, %rd94, 40;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd99],%fd60; }

	// end inline asm
	add.s64 	%rd100, %rd94, 48;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd100],%fd62; }

	// end inline asm
	add.s64 	%rd101, %rd94, 56;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd101],%fd64; }

	// end inline asm
	add.s64 	%rd102, %rd94, 64;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd102],%fd66; }

	// end inline asm
	add.s64 	%rd103, %rd94, 72;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd103],%fd68; }

	// end inline asm
	add.s64 	%rd104, %rd94, 80;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd104],%fd70; }

	// end inline asm
	add.s64 	%rd105, %rd94, 88;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd105],%fd72; }

	// end inline asm
	bra.uni 	$L__BB15_26;

$L__BB15_24:
	setp.eq.s64 	%p14, %rd51, 0;
	@%p14 bra 	$L__BB15_26;

	mul.lo.s64 	%rd119, %rd45, %rd21;
	add.s64 	%rd107, %rd51, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd107],%fd50; }

	// end inline asm
	add.s64 	%rd108, %rd107, 8;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd108],%fd52; }

	// end inline asm
	add.s64 	%rd109, %rd107, 16;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd109],%fd54; }

	// end inline asm
	add.s64 	%rd110, %rd107, 24;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd110],%fd56; }

	// end inline asm
	add.s64 	%rd111, %rd107, 32;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd111],%fd58; }

	// end inline asm
	add.s64 	%rd112, %rd107, 40;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd112],%fd60; }

	// end inline asm
	add.s64 	%rd113, %rd107, 48;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd113],%fd62; }

	// end inline asm
	add.s64 	%rd114, %rd107, 56;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd114],%fd64; }

	// end inline asm
	add.s64 	%rd115, %rd107, 64;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd115],%fd66; }

	// end inline asm
	add.s64 	%rd116, %rd107, 72;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd116],%fd68; }

	// end inline asm
	add.s64 	%rd117, %rd107, 80;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd117],%fd70; }

	// end inline asm
	add.s64 	%rd118, %rd107, 88;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd118],%fd72; }

	// end inline asm

$L__BB15_26:
	setp.eq.s64 	%p15, %rd62, 0;
	@%p15 bra 	$L__BB15_28;

	add.s64 	%rd123, %rd40, 3;
	mul.lo.s64 	%rd124, %rd123, %rd28;
	add.s64 	%rd120, %rd62, %rd124;
	mov.f64 	%fd178, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd120],%fd178; }

	// end inline asm
	add.s64 	%rd121, %rd120, 8;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd121],%fd178; }

	// end inline asm
	add.s64 	%rd122, %rd120, 16;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd122],%fd73; }

	// end inline asm
	bra.uni 	$L__BB15_30;

$L__BB15_28:
	setp.eq.s64 	%p16, %rd53, 0;
	@%p16 bra 	$L__BB15_30;

	add.s64 	%rd125, %rd53, %rd44;
	mov.f64 	%fd184, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd125],%fd184; }

	// end inline asm
	add.s64 	%rd126, %rd125, 8;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd126],%fd184; }

	// end inline asm
	add.s64 	%rd127, %rd125, 16;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd127],%fd73; }

	// end inline asm

$L__BB15_30:
	@%p15 bra 	$L__BB15_32;

	add.s64 	%rd131, %rd40, 3;
	mul.lo.s64 	%rd132, %rd131, %rd28;
	add.s64 	%rd128, %rd62, %rd132;
	mov.f64 	%fd192, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd128],%fd192; }

	// end inline asm
	add.s64 	%rd129, %rd128, 8;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd129],%fd71; }

	// end inline asm
	add.s64 	%rd130, %rd128, 16;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd130],%fd192; }

	// end inline asm
	bra.uni 	$L__BB15_34;

$L__BB15_32:
	setp.eq.s64 	%p18, %rd53, 0;
	@%p18 bra 	$L__BB15_34;

	add.s64 	%rd133, %rd53, %rd44;
	mov.f64 	%fd198, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd133],%fd198; }

	// end inline asm
	add.s64 	%rd134, %rd133, 8;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd134],%fd71; }

	// end inline asm
	add.s64 	%rd135, %rd133, 16;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd135],%fd198; }

	// end inline asm

$L__BB15_34:
	@%p15 bra 	$L__BB15_36;

	add.s64 	%rd139, %rd40, 3;
	mul.lo.s64 	%rd140, %rd139, %rd28;
	add.s64 	%rd136, %rd62, %rd140;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd136],%fd69; }

	// end inline asm
	add.s64 	%rd137, %rd136, 8;
	mov.f64 	%fd204, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd137],%fd204; }

	// end inline asm
	add.s64 	%rd138, %rd136, 16;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd138],%fd204; }

	// end inline asm
	bra.uni 	$L__BB15_38;

$L__BB15_36:
	setp.eq.s64 	%p20, %rd53, 0;
	@%p20 bra 	$L__BB15_38;

	add.s64 	%rd141, %rd53, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd141],%fd69; }

	// end inline asm
	add.s64 	%rd142, %rd141, 8;
	mov.f64 	%fd210, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd142],%fd210; }

	// end inline asm
	add.s64 	%rd143, %rd141, 16;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd143],%fd210; }

	// end inline asm

$L__BB15_38:
	@%p15 bra 	$L__BB15_40;

	add.s64 	%rd147, %rd40, 2;
	mul.lo.s64 	%rd148, %rd147, %rd28;
	add.s64 	%rd144, %rd62, %rd148;
	mov.f64 	%fd214, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd144],%fd214; }

	// end inline asm
	add.s64 	%rd145, %rd144, 8;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd145],%fd214; }

	// end inline asm
	add.s64 	%rd146, %rd144, 16;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd146],%fd67; }

	// end inline asm
	bra.uni 	$L__BB15_42;

$L__BB15_40:
	setp.eq.s64 	%p22, %rd53, 0;
	@%p22 bra 	$L__BB15_42;

	add.s64 	%rd149, %rd53, %rd43;
	mov.f64 	%fd220, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd149],%fd220; }

	// end inline asm
	add.s64 	%rd150, %rd149, 8;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd150],%fd220; }

	// end inline asm
	add.s64 	%rd151, %rd149, 16;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd151],%fd67; }

	// end inline asm

$L__BB15_42:
	@%p15 bra 	$L__BB15_44;

	add.s64 	%rd155, %rd40, 2;
	mul.lo.s64 	%rd156, %rd155, %rd28;
	add.s64 	%rd152, %rd62, %rd156;
	mov.f64 	%fd228, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd152],%fd228; }

	// end inline asm
	add.s64 	%rd153, %rd152, 8;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd153],%fd65; }

	// end inline asm
	add.s64 	%rd154, %rd152, 16;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd154],%fd228; }

	// end inline asm
	bra.uni 	$L__BB15_46;

$L__BB15_44:
	setp.eq.s64 	%p24, %rd53, 0;
	@%p24 bra 	$L__BB15_46;

	add.s64 	%rd157, %rd53, %rd43;
	mov.f64 	%fd234, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd157],%fd234; }

	// end inline asm
	add.s64 	%rd158, %rd157, 8;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd158],%fd65; }

	// end inline asm
	add.s64 	%rd159, %rd157, 16;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd159],%fd234; }

	// end inline asm

$L__BB15_46:
	@%p15 bra 	$L__BB15_48;

	add.s64 	%rd163, %rd40, 2;
	mul.lo.s64 	%rd164, %rd163, %rd28;
	add.s64 	%rd160, %rd62, %rd164;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd160],%fd63; }

	// end inline asm
	add.s64 	%rd161, %rd160, 8;
	mov.f64 	%fd240, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd161],%fd240; }

	// end inline asm
	add.s64 	%rd162, %rd160, 16;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd162],%fd240; }

	// end inline asm
	bra.uni 	$L__BB15_50;

$L__BB15_48:
	setp.eq.s64 	%p26, %rd53, 0;
	@%p26 bra 	$L__BB15_50;

	add.s64 	%rd165, %rd53, %rd43;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd165],%fd63; }

	// end inline asm
	add.s64 	%rd166, %rd165, 8;
	mov.f64 	%fd246, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd243,[%rd166],%fd246; }

	// end inline asm
	add.s64 	%rd167, %rd165, 16;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd167],%fd246; }

	// end inline asm

$L__BB15_50:
	@%p15 bra 	$L__BB15_52;

	add.s64 	%rd171, %rd40, 1;
	mul.lo.s64 	%rd172, %rd171, %rd28;
	add.s64 	%rd168, %rd62, %rd172;
	mov.f64 	%fd250, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd168],%fd250; }

	// end inline asm
	add.s64 	%rd169, %rd168, 8;
	// begin inline asm
	{ atom.add.f64 %fd249,[%rd169],%fd250; }

	// end inline asm
	add.s64 	%rd170, %rd168, 16;
	// begin inline asm
	{ atom.add.f64 %fd251,[%rd170],%fd61; }

	// end inline asm
	bra.uni 	$L__BB15_54;

$L__BB15_52:
	setp.eq.s64 	%p28, %rd53, 0;
	@%p28 bra 	$L__BB15_54;

	add.s64 	%rd173, %rd53, %rd42;
	mov.f64 	%fd256, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd253,[%rd173],%fd256; }

	// end inline asm
	add.s64 	%rd174, %rd173, 8;
	// begin inline asm
	{ atom.add.f64 %fd255,[%rd174],%fd256; }

	// end inline asm
	add.s64 	%rd175, %rd173, 16;
	// begin inline asm
	{ atom.add.f64 %fd257,[%rd175],%fd61; }

	// end inline asm

$L__BB15_54:
	@%p15 bra 	$L__BB15_56;

	add.s64 	%rd179, %rd40, 1;
	mul.lo.s64 	%rd180, %rd179, %rd28;
	add.s64 	%rd176, %rd62, %rd180;
	mov.f64 	%fd264, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd259,[%rd176],%fd264; }

	// end inline asm
	add.s64 	%rd177, %rd176, 8;
	// begin inline asm
	{ atom.add.f64 %fd261,[%rd177],%fd59; }

	// end inline asm
	add.s64 	%rd178, %rd176, 16;
	// begin inline asm
	{ atom.add.f64 %fd263,[%rd178],%fd264; }

	// end inline asm
	bra.uni 	$L__BB15_58;

$L__BB15_56:
	setp.eq.s64 	%p30, %rd53, 0;
	@%p30 bra 	$L__BB15_58;

	add.s64 	%rd181, %rd53, %rd42;
	mov.f64 	%fd270, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd265,[%rd181],%fd270; }

	// end inline asm
	add.s64 	%rd182, %rd181, 8;
	// begin inline asm
	{ atom.add.f64 %fd267,[%rd182],%fd59; }

	// end inline asm
	add.s64 	%rd183, %rd181, 16;
	// begin inline asm
	{ atom.add.f64 %fd269,[%rd183],%fd270; }

	// end inline asm

$L__BB15_58:
	@%p15 bra 	$L__BB15_60;

	add.s64 	%rd187, %rd40, 1;
	mul.lo.s64 	%rd188, %rd187, %rd28;
	add.s64 	%rd184, %rd62, %rd188;
	// begin inline asm
	{ atom.add.f64 %fd271,[%rd184],%fd57; }

	// end inline asm
	add.s64 	%rd185, %rd184, 8;
	mov.f64 	%fd276, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd273,[%rd185],%fd276; }

	// end inline asm
	add.s64 	%rd186, %rd184, 16;
	// begin inline asm
	{ atom.add.f64 %fd275,[%rd186],%fd276; }

	// end inline asm
	bra.uni 	$L__BB15_62;

$L__BB15_60:
	setp.eq.s64 	%p32, %rd53, 0;
	@%p32 bra 	$L__BB15_62;

	add.s64 	%rd189, %rd53, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd277,[%rd189],%fd57; }

	// end inline asm
	add.s64 	%rd190, %rd189, 8;
	mov.f64 	%fd282, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd279,[%rd190],%fd282; }

	// end inline asm
	add.s64 	%rd191, %rd189, 16;
	// begin inline asm
	{ atom.add.f64 %fd281,[%rd191],%fd282; }

	// end inline asm

$L__BB15_62:
	@%p15 bra 	$L__BB15_64;

	mul.lo.s64 	%rd195, %rd40, %rd28;
	add.s64 	%rd192, %rd62, %rd195;
	mov.f64 	%fd286, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd283,[%rd192],%fd286; }

	// end inline asm
	add.s64 	%rd193, %rd192, 8;
	// begin inline asm
	{ atom.add.f64 %fd285,[%rd193],%fd286; }

	// end inline asm
	add.s64 	%rd194, %rd192, 16;
	// begin inline asm
	{ atom.add.f64 %fd287,[%rd194],%fd55; }

	// end inline asm
	bra.uni 	$L__BB15_66;

$L__BB15_64:
	setp.eq.s64 	%p34, %rd53, 0;
	@%p34 bra 	$L__BB15_66;

	add.s64 	%rd196, %rd53, %rd41;
	mov.f64 	%fd292, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd289,[%rd196],%fd292; }

	// end inline asm
	add.s64 	%rd197, %rd196, 8;
	// begin inline asm
	{ atom.add.f64 %fd291,[%rd197],%fd292; }

	// end inline asm
	add.s64 	%rd198, %rd196, 16;
	// begin inline asm
	{ atom.add.f64 %fd293,[%rd198],%fd55; }

	// end inline asm

$L__BB15_66:
	@%p15 bra 	$L__BB15_68;

	mul.lo.s64 	%rd202, %rd40, %rd28;
	add.s64 	%rd199, %rd62, %rd202;
	mov.f64 	%fd300, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd295,[%rd199],%fd300; }

	// end inline asm
	add.s64 	%rd200, %rd199, 8;
	// begin inline asm
	{ atom.add.f64 %fd297,[%rd200],%fd53; }

	// end inline asm
	add.s64 	%rd201, %rd199, 16;
	// begin inline asm
	{ atom.add.f64 %fd299,[%rd201],%fd300; }

	// end inline asm
	bra.uni 	$L__BB15_70;

$L__BB15_68:
	setp.eq.s64 	%p36, %rd53, 0;
	@%p36 bra 	$L__BB15_70;

	add.s64 	%rd203, %rd53, %rd41;
	mov.f64 	%fd306, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd301,[%rd203],%fd306; }

	// end inline asm
	add.s64 	%rd204, %rd203, 8;
	// begin inline asm
	{ atom.add.f64 %fd303,[%rd204],%fd53; }

	// end inline asm
	add.s64 	%rd205, %rd203, 16;
	// begin inline asm
	{ atom.add.f64 %fd305,[%rd205],%fd306; }

	// end inline asm

$L__BB15_70:
	@%p15 bra 	$L__BB15_72;

	mul.lo.s64 	%rd209, %rd40, %rd28;
	add.s64 	%rd206, %rd62, %rd209;
	// begin inline asm
	{ atom.add.f64 %fd307,[%rd206],%fd51; }

	// end inline asm
	add.s64 	%rd207, %rd206, 8;
	mov.f64 	%fd312, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd309,[%rd207],%fd312; }

	// end inline asm
	add.s64 	%rd208, %rd206, 16;
	// begin inline asm
	{ atom.add.f64 %fd311,[%rd208],%fd312; }

	// end inline asm
	bra.uni 	$L__BB15_74;

$L__BB15_72:
	setp.eq.s64 	%p38, %rd53, 0;
	@%p38 bra 	$L__BB15_74;

	add.s64 	%rd210, %rd53, %rd41;
	// begin inline asm
	{ atom.add.f64 %fd313,[%rd210],%fd51; }

	// end inline asm
	add.s64 	%rd211, %rd210, 8;
	mov.f64 	%fd318, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd315,[%rd211],%fd318; }

	// end inline asm
	add.s64 	%rd212, %rd210, 16;
	// begin inline asm
	{ atom.add.f64 %fd317,[%rd212],%fd318; }

	// end inline asm

$L__BB15_74:
	ld.param.u64 	%rd213, [step_affine_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd218, %rd218, %rd29;
	setp.lt.u64 	%p39, %rd218, %rd213;
	@%p39 bra 	$L__BB15_2;

$L__BB15_75:
	ret;

}
	// .globl	y_to_x_cuda_kernel_forward
.visible .entry y_to_x_cuda_kernel_forward(
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_forward_param_4[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<98>;
	.reg .f64 	%fd<63>;
	.reg .b64 	%rd<86>;


	ld.param.v2.u32 	{%r43, %r44}, [y_to_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [y_to_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [y_to_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [y_to_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [y_to_x_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [y_to_x_cuda_kernel_forward_param_4+32];
	ld.param.u64 	%rd42, [y_to_x_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd40, [y_to_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [y_to_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd36, [y_to_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd35, [y_to_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [y_to_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd44, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd45, %r81;
	add.s64 	%rd79, %rd44, %rd45;
	setp.ge.u64 	%p1, %rd79, %rd35;
	@%p1 bra 	$L__BB16_25;

	cvta.to.global.u64 	%rd4, %rd42;
	cvta.to.global.u64 	%rd5, %rd40;
	cvta.to.global.u64 	%rd6, %rd38;
	cvta.to.global.u64 	%rd7, %rd36;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r67;
	cvt.s64.s32 	%rd12, %r75;
	cvt.s64.s32 	%rd13, %r59;
	cvt.s64.s32 	%rd14, %r51;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd46, %r82;
	mul.lo.s64 	%rd15, %rd1, %rd46;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB16_12;
	bra.uni 	$L__BB16_2;

$L__BB16_12:
	cvt.u32.u64 	%r89, %rd8;
	cvt.u32.u64 	%r92, %rd9;
	cvt.u32.u64 	%r95, %rd10;

$L__BB16_13:
	or.b64  	%rd62, %rd79, %rd8;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p9, %rd63, 0;
	@%p9 bra 	$L__BB16_15;

	div.u64 	%rd84, %rd79, %rd8;
	bra.uni 	$L__BB16_16;

$L__BB16_15:
	cvt.u32.u64 	%r90, %rd79;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd84, %r91;

$L__BB16_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB16_20;

	or.b64  	%rd64, %rd84, %rd9;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p11, %rd65, 0;
	@%p11 bra 	$L__BB16_19;

	div.u64 	%rd84, %rd84, %rd9;
	bra.uni 	$L__BB16_20;

$L__BB16_19:
	cvt.u32.u64 	%r93, %rd84;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd84, %r94;

$L__BB16_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB16_24;

	or.b64  	%rd66, %rd84, %rd10;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB16_23;

	div.u64 	%rd84, %rd84, %rd10;
	bra.uni 	$L__BB16_24;

$L__BB16_23:
	cvt.u32.u64 	%r96, %rd84;
	div.u32 	%r97, %r96, %r95;
	cvt.u64.u32 	%rd84, %r97;

$L__BB16_24:
	cvt.s64.s32 	%rd68, %rd84;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd69, %rd68, 0, %p14;
	mul.lo.s64 	%rd70, %rd69, %rd11;
	add.s64 	%rd71, %rd5, %rd70;
	ld.global.s32 	%rd72, [%rd71];
	mul.lo.s64 	%rd73, %rd72, %rd12;
	add.s64 	%rd74, %rd4, %rd73;
	mul.lo.s64 	%rd75, %rd69, %rd13;
	add.s64 	%rd76, %rd6, %rd75;
	ld.global.f64 	%fd32, [%rd76];
	mov.f64 	%fd33, 0d3FF0000000000000;
	sub.f64 	%fd34, %fd33, %fd32;
	ld.global.f64 	%fd35, [%rd76+8];
	sub.f64 	%fd36, %fd34, %fd35;
	ld.global.f64 	%fd37, [%rd76+16];
	sub.f64 	%fd38, %fd36, %fd37;
	ld.global.f64 	%fd39, [%rd74];
	mul.f64 	%fd40, %fd39, %fd38;
	ld.global.f64 	%fd41, [%rd74+8];
	mul.f64 	%fd42, %fd41, %fd38;
	ld.global.f64 	%fd43, [%rd74+16];
	mul.f64 	%fd44, %fd43, %fd38;
	ld.global.f64 	%fd45, [%rd74+24];
	ld.global.f64 	%fd46, [%rd74+32];
	ld.global.f64 	%fd47, [%rd74+40];
	fma.rn.f64 	%fd48, %fd45, %fd32, %fd40;
	fma.rn.f64 	%fd49, %fd46, %fd32, %fd42;
	fma.rn.f64 	%fd50, %fd47, %fd32, %fd44;
	ld.global.f64 	%fd51, [%rd74+48];
	ld.global.f64 	%fd52, [%rd74+56];
	ld.global.f64 	%fd53, [%rd74+64];
	fma.rn.f64 	%fd54, %fd51, %fd35, %fd48;
	fma.rn.f64 	%fd55, %fd52, %fd35, %fd49;
	fma.rn.f64 	%fd56, %fd53, %fd35, %fd50;
	ld.global.f64 	%fd57, [%rd74+72];
	ld.global.f64 	%fd58, [%rd74+80];
	ld.global.f64 	%fd59, [%rd74+88];
	fma.rn.f64 	%fd60, %fd57, %fd37, %fd54;
	fma.rn.f64 	%fd61, %fd58, %fd37, %fd55;
	fma.rn.f64 	%fd62, %fd59, %fd37, %fd56;
	mul.lo.s64 	%rd77, %rd69, %rd14;
	add.s64 	%rd78, %rd7, %rd77;
	st.global.f64 	[%rd78], %fd60;
	st.global.f64 	[%rd78+8], %fd61;
	st.global.f64 	[%rd78+16], %fd62;
	add.s64 	%rd79, %rd79, %rd15;
	setp.lt.u64 	%p15, %rd79, %rd35;
	@%p15 bra 	$L__BB16_13;
	bra.uni 	$L__BB16_25;

$L__BB16_2:
	cvt.u32.u64 	%r83, %rd9;
	cvt.u32.u64 	%r86, %rd10;

$L__BB16_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd80, %rd79;
	@%p3 bra 	$L__BB16_7;

	or.b64  	%rd47, %rd79, %rd9;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB16_6;

	div.u64 	%rd80, %rd79, %rd9;
	bra.uni 	$L__BB16_7;

$L__BB16_6:
	cvt.u32.u64 	%r84, %rd79;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd80, %r85;

$L__BB16_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB16_11;

	or.b64  	%rd49, %rd80, %rd10;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p6, %rd50, 0;
	@%p6 bra 	$L__BB16_10;

	div.u64 	%rd80, %rd80, %rd10;
	bra.uni 	$L__BB16_11;

$L__BB16_10:
	cvt.u32.u64 	%r87, %rd80;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd80, %r88;

$L__BB16_11:
	cvt.s64.s32 	%rd51, %rd80;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd52, %rd51, 0, %p7;
	mul.lo.s64 	%rd53, %rd52, %rd11;
	add.s64 	%rd54, %rd5, %rd53;
	ld.global.s32 	%rd55, [%rd54];
	mul.lo.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd57, %rd4, %rd56;
	mul.lo.s64 	%rd58, %rd52, %rd13;
	add.s64 	%rd59, %rd6, %rd58;
	ld.global.f64 	%fd1, [%rd59];
	mov.f64 	%fd2, 0d3FF0000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	ld.global.f64 	%fd4, [%rd59+8];
	sub.f64 	%fd5, %fd3, %fd4;
	ld.global.f64 	%fd6, [%rd59+16];
	sub.f64 	%fd7, %fd5, %fd6;
	ld.global.f64 	%fd8, [%rd57];
	mul.f64 	%fd9, %fd8, %fd7;
	ld.global.f64 	%fd10, [%rd57+8];
	mul.f64 	%fd11, %fd10, %fd7;
	ld.global.f64 	%fd12, [%rd57+16];
	mul.f64 	%fd13, %fd12, %fd7;
	ld.global.f64 	%fd14, [%rd57+24];
	ld.global.f64 	%fd15, [%rd57+32];
	ld.global.f64 	%fd16, [%rd57+40];
	fma.rn.f64 	%fd17, %fd14, %fd1, %fd9;
	fma.rn.f64 	%fd18, %fd15, %fd1, %fd11;
	fma.rn.f64 	%fd19, %fd16, %fd1, %fd13;
	ld.global.f64 	%fd20, [%rd57+48];
	ld.global.f64 	%fd21, [%rd57+56];
	ld.global.f64 	%fd22, [%rd57+64];
	fma.rn.f64 	%fd23, %fd20, %fd4, %fd17;
	fma.rn.f64 	%fd24, %fd21, %fd4, %fd18;
	fma.rn.f64 	%fd25, %fd22, %fd4, %fd19;
	ld.global.f64 	%fd26, [%rd57+72];
	ld.global.f64 	%fd27, [%rd57+80];
	ld.global.f64 	%fd28, [%rd57+88];
	fma.rn.f64 	%fd29, %fd26, %fd6, %fd23;
	fma.rn.f64 	%fd30, %fd27, %fd6, %fd24;
	fma.rn.f64 	%fd31, %fd28, %fd6, %fd25;
	mul.lo.s64 	%rd60, %rd52, %rd14;
	add.s64 	%rd61, %rd7, %rd60;
	st.global.f64 	[%rd61], %fd29;
	st.global.f64 	[%rd61+8], %fd30;
	st.global.f64 	[%rd61+16], %fd31;
	add.s64 	%rd79, %rd79, %rd15;
	setp.lt.u64 	%p8, %rd79, %rd35;
	@%p8 bra 	$L__BB16_3;

$L__BB16_25:
	ret;

}
	// .globl	y_to_x_cuda_kernel_backward
.visible .entry y_to_x_cuda_kernel_backward(
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 y_to_x_cuda_kernel_backward_param_8[56]
)
{
	.reg .pred 	%p<38>;
	.reg .b16 	%rs<57>;
	.reg .b32 	%r<143>;
	.reg .f64 	%fd<668>;
	.reg .b64 	%rd<388>;


	ld.param.v2.u32 	{%r70, %r71}, [y_to_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [y_to_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [y_to_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [y_to_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [y_to_x_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r102, %r103}, [y_to_x_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r110, %r111}, [y_to_x_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r118, %r119}, [y_to_x_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r126, %r127}, [y_to_x_cuda_kernel_backward_param_8+32];
	ld.param.u64 	%rd54, [y_to_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd52, [y_to_x_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd50, [y_to_x_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd49, [y_to_x_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd48, [y_to_x_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd46, [y_to_x_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd45, [y_to_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd44, [y_to_x_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd43, [y_to_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd41, [y_to_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [y_to_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd56, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd57, %r132;
	add.s64 	%rd384, %rd56, %rd57;
	setp.ge.u64 	%p1, %rd384, %rd41;
	@%p1 bra 	$L__BB17_71;

	cvta.to.global.u64 	%rd10, %rd50;
	cvta.to.global.u64 	%rd11, %rd48;
	cvta.to.global.u64 	%rd12, %rd46;
	cvta.to.global.u64 	%rd13, %rd44;
	cvta.to.global.u64 	%rd14, %rd43;
	cvt.s64.s32 	%rd15, %r73;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r71;
	cvt.s64.s32 	%rd18, %r94;
	cvt.s64.s32 	%rd19, %r102;
	cvt.s64.s32 	%rd20, %r86;
	cvt.s64.s32 	%rd21, %r110;
	cvt.s64.s32 	%rd22, %r78;
	cvt.s64.s32 	%rd23, %r118;
	cvt.s64.s32 	%rd24, %r126;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd58, %r133;
	mul.lo.s64 	%rd25, %rd1, %rd58;

$L__BB17_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd385, %rd384;
	@%p2 bra 	$L__BB17_6;

	or.b64  	%rd59, %rd384, %rd15;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p3, %rd60, 0;
	@%p3 bra 	$L__BB17_5;

	div.u64 	%rd385, %rd384, %rd15;
	bra.uni 	$L__BB17_6;

$L__BB17_5:
	cvt.u32.u64 	%r134, %rd15;
	cvt.u32.u64 	%r135, %rd384;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd385, %r136;

$L__BB17_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB17_10;

	or.b64  	%rd61, %rd385, %rd16;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p5, %rd62, 0;
	@%p5 bra 	$L__BB17_9;

	div.u64 	%rd385, %rd385, %rd16;
	bra.uni 	$L__BB17_10;

$L__BB17_9:
	cvt.u32.u64 	%r137, %rd16;
	cvt.u32.u64 	%r138, %rd385;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd385, %r139;

$L__BB17_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB17_14;

	or.b64  	%rd63, %rd385, %rd17;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p7, %rd64, 0;
	@%p7 bra 	$L__BB17_13;

	div.u64 	%rd385, %rd385, %rd17;
	bra.uni 	$L__BB17_14;

$L__BB17_13:
	cvt.u32.u64 	%r140, %rd17;
	cvt.u32.u64 	%r141, %rd385;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd385, %r142;

$L__BB17_14:
	ld.param.u64 	%rd382, [y_to_x_cuda_kernel_backward_param_5];
	cvt.s64.s32 	%rd65, %rd385;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd36, %rd65, 0, %p8;
	mul.lo.s64 	%rd66, %rd36, %rd18;
	add.s64 	%rd67, %rd12, %rd66;
	ld.global.s32 	%rd37, [%rd67];
	mul.lo.s64 	%rd38, %rd37, %rd19;
	add.s64 	%rd68, %rd11, %rd38;
	ld.global.f64 	%fd1, [%rd68];
	ld.global.f64 	%fd2, [%rd68+8];
	ld.global.f64 	%fd3, [%rd68+16];
	ld.global.f64 	%fd4, [%rd68+24];
	ld.global.f64 	%fd5, [%rd68+32];
	ld.global.f64 	%fd6, [%rd68+40];
	ld.global.f64 	%fd7, [%rd68+48];
	ld.global.f64 	%fd8, [%rd68+56];
	ld.global.f64 	%fd9, [%rd68+64];
	ld.global.f64 	%fd10, [%rd68+72];
	ld.global.f64 	%fd11, [%rd68+80];
	ld.global.f64 	%fd12, [%rd68+88];
	mul.lo.s64 	%rd39, %rd36, %rd20;
	add.s64 	%rd69, %rd13, %rd39;
	ld.global.f64 	%fd13, [%rd69];
	ld.global.f64 	%fd14, [%rd69+8];
	ld.global.f64 	%fd15, [%rd69+16];
	setp.eq.s64 	%p9, %rd382, 0;
	@%p9 bra 	$L__BB17_16;

	mul.lo.s64 	%rd70, %rd36, %rd21;
	add.s64 	%rd71, %rd10, %rd70;
	ld.global.f64 	%fd40, [%rd71];
	add.f64 	%fd667, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd71+8];
	add.f64 	%fd666, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd71+16];
	add.f64 	%fd665, %fd42, 0d0000000000000000;
	bra.uni 	$L__BB17_18;

$L__BB17_16:
	ld.param.u64 	%rd383, [y_to_x_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p10, %rd383, 0;
	mov.f64 	%fd665, 0d0000000000000000;
	mov.f64 	%fd666, %fd665;
	mov.f64 	%fd667, %fd665;
	@%p10 bra 	$L__BB17_18;

	mul.lo.s64 	%rd72, %rd36, %rd22;
	add.s64 	%rd73, %rd14, %rd72;
	ld.global.f64 	%fd46, [%rd73];
	add.f64 	%fd667, %fd46, 0d0000000000000000;
	ld.global.f64 	%fd47, [%rd73+8];
	add.f64 	%fd666, %fd47, 0d0000000000000000;
	ld.global.f64 	%fd48, [%rd73+16];
	add.f64 	%fd665, %fd48, 0d0000000000000000;

$L__BB17_18:
	mov.f64 	%fd49, 0d3FF0000000000000;
	sub.f64 	%fd50, %fd49, %fd13;
	sub.f64 	%fd51, %fd50, %fd14;
	sub.f64 	%fd52, %fd51, %fd15;
	add.f64 	%fd53, %fd667, 0d0000000000000000;
	mov.f64 	%fd54, 0d0000000000000000;
	fma.rn.f64 	%fd25, %fd15, %fd53, 0d0000000000000000;
	add.f64 	%fd55, %fd666, 0d0000000000000000;
	fma.rn.f64 	%fd26, %fd15, %fd55, 0d0000000000000000;
	add.f64 	%fd56, %fd665, 0d0000000000000000;
	fma.rn.f64 	%fd27, %fd15, %fd56, 0d0000000000000000;
	mul.f64 	%fd57, %fd11, %fd55;
	fma.rn.f64 	%fd58, %fd10, %fd53, %fd57;
	fma.rn.f64 	%fd59, %fd12, %fd56, %fd58;
	fma.rn.f64 	%fd28, %fd14, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd29, %fd14, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd30, %fd14, %fd56, 0d0000000000000000;
	mul.f64 	%fd60, %fd8, %fd55;
	fma.rn.f64 	%fd61, %fd7, %fd53, %fd60;
	fma.rn.f64 	%fd62, %fd9, %fd56, %fd61;
	fma.rn.f64 	%fd31, %fd13, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd32, %fd13, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd33, %fd13, %fd56, 0d0000000000000000;
	mul.f64 	%fd63, %fd5, %fd55;
	fma.rn.f64 	%fd64, %fd4, %fd53, %fd63;
	fma.rn.f64 	%fd65, %fd6, %fd56, %fd64;
	add.f64 	%fd66, %fd59, 0d0000000000000000;
	fma.rn.f64 	%fd34, %fd52, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd35, %fd52, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd52, %fd56, 0d0000000000000000;
	add.f64 	%fd67, %fd62, 0d0000000000000000;
	add.f64 	%fd68, %fd65, 0d0000000000000000;
	mul.f64 	%fd69, %fd2, %fd55;
	fma.rn.f64 	%fd70, %fd1, %fd53, %fd69;
	fma.rn.f64 	%fd71, %fd3, %fd56, %fd70;
	add.f64 	%fd72, %fd71, 0d0000000000000000;
	sub.f64 	%fd73, %fd54, %fd72;
	add.f64 	%fd74, %fd66, %fd73;
	add.f64 	%fd75, %fd67, %fd73;
	add.f64 	%fd76, %fd68, %fd73;
	add.f64 	%fd37, %fd76, 0d0000000000000000;
	add.f64 	%fd38, %fd75, 0d0000000000000000;
	add.f64 	%fd39, %fd74, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd52, 0;
	@%p11 bra 	$L__BB17_20;

	mul.lo.s64 	%rd77, %rd36, %rd23;
	add.s64 	%rd74, %rd52, %rd77;
	// begin inline asm
	{ atom.add.f64 %fd77,[%rd74],%fd37; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd79,[%rd75],%fd38; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd81,[%rd76],%fd39; }

	// end inline asm
	bra.uni 	$L__BB17_22;

$L__BB17_20:
	setp.eq.s64 	%p12, %rd45, 0;
	@%p12 bra 	$L__BB17_22;

	add.s64 	%rd78, %rd45, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd83,[%rd78],%fd37; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd79],%fd38; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd80],%fd39; }

	// end inline asm

$L__BB17_22:
	setp.eq.s64 	%p13, %rd54, 0;
	@%p13 bra 	$L__BB17_24;

	mul.lo.s64 	%rd93, %rd37, %rd24;
	add.s64 	%rd81, %rd54, %rd93;
	mov.f64 	%fd110, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd81],%fd110; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd82],%fd110; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd83],%fd110; }

	// end inline asm
	add.s64 	%rd84, %rd81, 24;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd84],%fd110; }

	// end inline asm
	add.s64 	%rd85, %rd81, 32;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd85],%fd110; }

	// end inline asm
	add.s64 	%rd86, %rd81, 40;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd86],%fd110; }

	// end inline asm
	add.s64 	%rd87, %rd81, 48;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd87],%fd110; }

	// end inline asm
	add.s64 	%rd88, %rd81, 56;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd88],%fd110; }

	// end inline asm
	add.s64 	%rd89, %rd81, 64;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd89],%fd110; }

	// end inline asm
	add.s64 	%rd90, %rd81, 72;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd90],%fd110; }

	// end inline asm
	add.s64 	%rd91, %rd81, 80;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd91],%fd110; }

	// end inline asm
	add.s64 	%rd92, %rd81, 88;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd92],%fd27; }

	// end inline asm
	bra.uni 	$L__BB17_26;

$L__BB17_24:
	setp.eq.s64 	%p14, %rd49, 0;
	@%p14 bra 	$L__BB17_26;

	add.s64 	%rd94, %rd49, %rd38;
	mov.f64 	%fd134, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd94],%fd134; }

	// end inline asm
	add.s64 	%rd95, %rd94, 8;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd95],%fd134; }

	// end inline asm
	add.s64 	%rd96, %rd94, 16;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd96],%fd134; }

	// end inline asm
	add.s64 	%rd97, %rd94, 24;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd97],%fd134; }

	// end inline asm
	add.s64 	%rd98, %rd94, 32;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd98],%fd134; }

	// end inline asm
	add.s64 	%rd99, %rd94, 40;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd99],%fd134; }

	// end inline asm
	add.s64 	%rd100, %rd94, 48;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd100],%fd134; }

	// end inline asm
	add.s64 	%rd101, %rd94, 56;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd101],%fd134; }

	// end inline asm
	add.s64 	%rd102, %rd94, 64;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd102],%fd134; }

	// end inline asm
	add.s64 	%rd103, %rd94, 72;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd103],%fd134; }

	// end inline asm
	add.s64 	%rd104, %rd94, 80;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd104],%fd134; }

	// end inline asm
	add.s64 	%rd105, %rd94, 88;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd105],%fd27; }

	// end inline asm

$L__BB17_26:
	@%p13 bra 	$L__BB17_28;

	mul.lo.s64 	%rd118, %rd37, %rd24;
	add.s64 	%rd106, %rd54, %rd118;
	mov.f64 	%fd160, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd106],%fd160; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd107],%fd160; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd108],%fd160; }

	// end inline asm
	add.s64 	%rd109, %rd106, 24;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd109],%fd160; }

	// end inline asm
	add.s64 	%rd110, %rd106, 32;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd110],%fd160; }

	// end inline asm
	add.s64 	%rd111, %rd106, 40;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd111],%fd160; }

	// end inline asm
	add.s64 	%rd112, %rd106, 48;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd112],%fd160; }

	// end inline asm
	add.s64 	%rd113, %rd106, 56;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd113],%fd160; }

	// end inline asm
	add.s64 	%rd114, %rd106, 64;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd114],%fd160; }

	// end inline asm
	add.s64 	%rd115, %rd106, 72;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd115],%fd160; }

	// end inline asm
	add.s64 	%rd116, %rd106, 80;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd116],%fd26; }

	// end inline asm
	add.s64 	%rd117, %rd106, 88;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd117],%fd160; }

	// end inline asm
	bra.uni 	$L__BB17_30;

$L__BB17_28:
	setp.eq.s64 	%p16, %rd49, 0;
	@%p16 bra 	$L__BB17_30;

	add.s64 	%rd119, %rd49, %rd38;
	mov.f64 	%fd184, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd119],%fd184; }

	// end inline asm
	add.s64 	%rd120, %rd119, 8;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd120],%fd184; }

	// end inline asm
	add.s64 	%rd121, %rd119, 16;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd121],%fd184; }

	// end inline asm
	add.s64 	%rd122, %rd119, 24;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd122],%fd184; }

	// end inline asm
	add.s64 	%rd123, %rd119, 32;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd123],%fd184; }

	// end inline asm
	add.s64 	%rd124, %rd119, 40;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd124],%fd184; }

	// end inline asm
	add.s64 	%rd125, %rd119, 48;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd125],%fd184; }

	// end inline asm
	add.s64 	%rd126, %rd119, 56;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd126],%fd184; }

	// end inline asm
	add.s64 	%rd127, %rd119, 64;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd127],%fd184; }

	// end inline asm
	add.s64 	%rd128, %rd119, 72;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd128],%fd184; }

	// end inline asm
	add.s64 	%rd129, %rd119, 80;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd129],%fd26; }

	// end inline asm
	add.s64 	%rd130, %rd119, 88;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd130],%fd184; }

	// end inline asm

$L__BB17_30:
	@%p13 bra 	$L__BB17_32;

	mul.lo.s64 	%rd143, %rd37, %rd24;
	add.s64 	%rd131, %rd54, %rd143;
	mov.f64 	%fd208, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd131],%fd208; }

	// end inline asm
	add.s64 	%rd132, %rd131, 8;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd132],%fd208; }

	// end inline asm
	add.s64 	%rd133, %rd131, 16;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd133],%fd208; }

	// end inline asm
	add.s64 	%rd134, %rd131, 24;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd134],%fd208; }

	// end inline asm
	add.s64 	%rd135, %rd131, 32;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd135],%fd208; }

	// end inline asm
	add.s64 	%rd136, %rd131, 40;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd136],%fd208; }

	// end inline asm
	add.s64 	%rd137, %rd131, 48;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd137],%fd208; }

	// end inline asm
	add.s64 	%rd138, %rd131, 56;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd138],%fd208; }

	// end inline asm
	add.s64 	%rd139, %rd131, 64;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd139],%fd208; }

	// end inline asm
	add.s64 	%rd140, %rd131, 72;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd140],%fd25; }

	// end inline asm
	add.s64 	%rd141, %rd131, 80;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd141],%fd208; }

	// end inline asm
	add.s64 	%rd142, %rd131, 88;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd142],%fd208; }

	// end inline asm
	bra.uni 	$L__BB17_34;

$L__BB17_32:
	setp.eq.s64 	%p18, %rd49, 0;
	@%p18 bra 	$L__BB17_34;

	add.s64 	%rd144, %rd49, %rd38;
	mov.f64 	%fd232, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd144],%fd232; }

	// end inline asm
	add.s64 	%rd145, %rd144, 8;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd145],%fd232; }

	// end inline asm
	add.s64 	%rd146, %rd144, 16;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd146],%fd232; }

	// end inline asm
	add.s64 	%rd147, %rd144, 24;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd147],%fd232; }

	// end inline asm
	add.s64 	%rd148, %rd144, 32;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd148],%fd232; }

	// end inline asm
	add.s64 	%rd149, %rd144, 40;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd149],%fd232; }

	// end inline asm
	add.s64 	%rd150, %rd144, 48;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd150],%fd232; }

	// end inline asm
	add.s64 	%rd151, %rd144, 56;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd151],%fd232; }

	// end inline asm
	add.s64 	%rd152, %rd144, 64;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd152],%fd232; }

	// end inline asm
	add.s64 	%rd153, %rd144, 72;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd153],%fd25; }

	// end inline asm
	add.s64 	%rd154, %rd144, 80;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd154],%fd232; }

	// end inline asm
	add.s64 	%rd155, %rd144, 88;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd155],%fd232; }

	// end inline asm

$L__BB17_34:
	@%p13 bra 	$L__BB17_36;

	mul.lo.s64 	%rd168, %rd37, %rd24;
	add.s64 	%rd156, %rd54, %rd168;
	mov.f64 	%fd256, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd156],%fd256; }

	// end inline asm
	add.s64 	%rd157, %rd156, 8;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd157],%fd256; }

	// end inline asm
	add.s64 	%rd158, %rd156, 16;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd158],%fd256; }

	// end inline asm
	add.s64 	%rd159, %rd156, 24;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd159],%fd256; }

	// end inline asm
	add.s64 	%rd160, %rd156, 32;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd160],%fd256; }

	// end inline asm
	add.s64 	%rd161, %rd156, 40;
	// begin inline asm
	{ atom.add.f64 %fd243,[%rd161],%fd256; }

	// end inline asm
	add.s64 	%rd162, %rd156, 48;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd162],%fd256; }

	// end inline asm
	add.s64 	%rd163, %rd156, 56;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd163],%fd256; }

	// end inline asm
	add.s64 	%rd164, %rd156, 64;
	// begin inline asm
	{ atom.add.f64 %fd249,[%rd164],%fd30; }

	// end inline asm
	add.s64 	%rd165, %rd156, 72;
	// begin inline asm
	{ atom.add.f64 %fd251,[%rd165],%fd256; }

	// end inline asm
	add.s64 	%rd166, %rd156, 80;
	// begin inline asm
	{ atom.add.f64 %fd253,[%rd166],%fd256; }

	// end inline asm
	add.s64 	%rd167, %rd156, 88;
	// begin inline asm
	{ atom.add.f64 %fd255,[%rd167],%fd256; }

	// end inline asm
	bra.uni 	$L__BB17_38;

$L__BB17_36:
	setp.eq.s64 	%p20, %rd49, 0;
	@%p20 bra 	$L__BB17_38;

	add.s64 	%rd169, %rd49, %rd38;
	mov.f64 	%fd280, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd257,[%rd169],%fd280; }

	// end inline asm
	add.s64 	%rd170, %rd169, 8;
	// begin inline asm
	{ atom.add.f64 %fd259,[%rd170],%fd280; }

	// end inline asm
	add.s64 	%rd171, %rd169, 16;
	// begin inline asm
	{ atom.add.f64 %fd261,[%rd171],%fd280; }

	// end inline asm
	add.s64 	%rd172, %rd169, 24;
	// begin inline asm
	{ atom.add.f64 %fd263,[%rd172],%fd280; }

	// end inline asm
	add.s64 	%rd173, %rd169, 32;
	// begin inline asm
	{ atom.add.f64 %fd265,[%rd173],%fd280; }

	// end inline asm
	add.s64 	%rd174, %rd169, 40;
	// begin inline asm
	{ atom.add.f64 %fd267,[%rd174],%fd280; }

	// end inline asm
	add.s64 	%rd175, %rd169, 48;
	// begin inline asm
	{ atom.add.f64 %fd269,[%rd175],%fd280; }

	// end inline asm
	add.s64 	%rd176, %rd169, 56;
	// begin inline asm
	{ atom.add.f64 %fd271,[%rd176],%fd280; }

	// end inline asm
	add.s64 	%rd177, %rd169, 64;
	// begin inline asm
	{ atom.add.f64 %fd273,[%rd177],%fd30; }

	// end inline asm
	add.s64 	%rd178, %rd169, 72;
	// begin inline asm
	{ atom.add.f64 %fd275,[%rd178],%fd280; }

	// end inline asm
	add.s64 	%rd179, %rd169, 80;
	// begin inline asm
	{ atom.add.f64 %fd277,[%rd179],%fd280; }

	// end inline asm
	add.s64 	%rd180, %rd169, 88;
	// begin inline asm
	{ atom.add.f64 %fd279,[%rd180],%fd280; }

	// end inline asm

$L__BB17_38:
	@%p13 bra 	$L__BB17_40;

	mul.lo.s64 	%rd193, %rd37, %rd24;
	add.s64 	%rd181, %rd54, %rd193;
	mov.f64 	%fd304, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd281,[%rd181],%fd304; }

	// end inline asm
	add.s64 	%rd182, %rd181, 8;
	// begin inline asm
	{ atom.add.f64 %fd283,[%rd182],%fd304; }

	// end inline asm
	add.s64 	%rd183, %rd181, 16;
	// begin inline asm
	{ atom.add.f64 %fd285,[%rd183],%fd304; }

	// end inline asm
	add.s64 	%rd184, %rd181, 24;
	// begin inline asm
	{ atom.add.f64 %fd287,[%rd184],%fd304; }

	// end inline asm
	add.s64 	%rd185, %rd181, 32;
	// begin inline asm
	{ atom.add.f64 %fd289,[%rd185],%fd304; }

	// end inline asm
	add.s64 	%rd186, %rd181, 40;
	// begin inline asm
	{ atom.add.f64 %fd291,[%rd186],%fd304; }

	// end inline asm
	add.s64 	%rd187, %rd181, 48;
	// begin inline asm
	{ atom.add.f64 %fd293,[%rd187],%fd304; }

	// end inline asm
	add.s64 	%rd188, %rd181, 56;
	// begin inline asm
	{ atom.add.f64 %fd295,[%rd188],%fd29; }

	// end inline asm
	add.s64 	%rd189, %rd181, 64;
	// begin inline asm
	{ atom.add.f64 %fd297,[%rd189],%fd304; }

	// end inline asm
	add.s64 	%rd190, %rd181, 72;
	// begin inline asm
	{ atom.add.f64 %fd299,[%rd190],%fd304; }

	// end inline asm
	add.s64 	%rd191, %rd181, 80;
	// begin inline asm
	{ atom.add.f64 %fd301,[%rd191],%fd304; }

	// end inline asm
	add.s64 	%rd192, %rd181, 88;
	// begin inline asm
	{ atom.add.f64 %fd303,[%rd192],%fd304; }

	// end inline asm
	bra.uni 	$L__BB17_42;

$L__BB17_40:
	setp.eq.s64 	%p22, %rd49, 0;
	@%p22 bra 	$L__BB17_42;

	add.s64 	%rd194, %rd49, %rd38;
	mov.f64 	%fd328, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd305,[%rd194],%fd328; }

	// end inline asm
	add.s64 	%rd195, %rd194, 8;
	// begin inline asm
	{ atom.add.f64 %fd307,[%rd195],%fd328; }

	// end inline asm
	add.s64 	%rd196, %rd194, 16;
	// begin inline asm
	{ atom.add.f64 %fd309,[%rd196],%fd328; }

	// end inline asm
	add.s64 	%rd197, %rd194, 24;
	// begin inline asm
	{ atom.add.f64 %fd311,[%rd197],%fd328; }

	// end inline asm
	add.s64 	%rd198, %rd194, 32;
	// begin inline asm
	{ atom.add.f64 %fd313,[%rd198],%fd328; }

	// end inline asm
	add.s64 	%rd199, %rd194, 40;
	// begin inline asm
	{ atom.add.f64 %fd315,[%rd199],%fd328; }

	// end inline asm
	add.s64 	%rd200, %rd194, 48;
	// begin inline asm
	{ atom.add.f64 %fd317,[%rd200],%fd328; }

	// end inline asm
	add.s64 	%rd201, %rd194, 56;
	// begin inline asm
	{ atom.add.f64 %fd319,[%rd201],%fd29; }

	// end inline asm
	add.s64 	%rd202, %rd194, 64;
	// begin inline asm
	{ atom.add.f64 %fd321,[%rd202],%fd328; }

	// end inline asm
	add.s64 	%rd203, %rd194, 72;
	// begin inline asm
	{ atom.add.f64 %fd323,[%rd203],%fd328; }

	// end inline asm
	add.s64 	%rd204, %rd194, 80;
	// begin inline asm
	{ atom.add.f64 %fd325,[%rd204],%fd328; }

	// end inline asm
	add.s64 	%rd205, %rd194, 88;
	// begin inline asm
	{ atom.add.f64 %fd327,[%rd205],%fd328; }

	// end inline asm

$L__BB17_42:
	@%p13 bra 	$L__BB17_44;

	mul.lo.s64 	%rd218, %rd37, %rd24;
	add.s64 	%rd206, %rd54, %rd218;
	mov.f64 	%fd352, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd329,[%rd206],%fd352; }

	// end inline asm
	add.s64 	%rd207, %rd206, 8;
	// begin inline asm
	{ atom.add.f64 %fd331,[%rd207],%fd352; }

	// end inline asm
	add.s64 	%rd208, %rd206, 16;
	// begin inline asm
	{ atom.add.f64 %fd333,[%rd208],%fd352; }

	// end inline asm
	add.s64 	%rd209, %rd206, 24;
	// begin inline asm
	{ atom.add.f64 %fd335,[%rd209],%fd352; }

	// end inline asm
	add.s64 	%rd210, %rd206, 32;
	// begin inline asm
	{ atom.add.f64 %fd337,[%rd210],%fd352; }

	// end inline asm
	add.s64 	%rd211, %rd206, 40;
	// begin inline asm
	{ atom.add.f64 %fd339,[%rd211],%fd352; }

	// end inline asm
	add.s64 	%rd212, %rd206, 48;
	// begin inline asm
	{ atom.add.f64 %fd341,[%rd212],%fd28; }

	// end inline asm
	add.s64 	%rd213, %rd206, 56;
	// begin inline asm
	{ atom.add.f64 %fd343,[%rd213],%fd352; }

	// end inline asm
	add.s64 	%rd214, %rd206, 64;
	// begin inline asm
	{ atom.add.f64 %fd345,[%rd214],%fd352; }

	// end inline asm
	add.s64 	%rd215, %rd206, 72;
	// begin inline asm
	{ atom.add.f64 %fd347,[%rd215],%fd352; }

	// end inline asm
	add.s64 	%rd216, %rd206, 80;
	// begin inline asm
	{ atom.add.f64 %fd349,[%rd216],%fd352; }

	// end inline asm
	add.s64 	%rd217, %rd206, 88;
	// begin inline asm
	{ atom.add.f64 %fd351,[%rd217],%fd352; }

	// end inline asm
	bra.uni 	$L__BB17_46;

$L__BB17_44:
	setp.eq.s64 	%p24, %rd49, 0;
	@%p24 bra 	$L__BB17_46;

	add.s64 	%rd219, %rd49, %rd38;
	mov.f64 	%fd376, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd353,[%rd219],%fd376; }

	// end inline asm
	add.s64 	%rd220, %rd219, 8;
	// begin inline asm
	{ atom.add.f64 %fd355,[%rd220],%fd376; }

	// end inline asm
	add.s64 	%rd221, %rd219, 16;
	// begin inline asm
	{ atom.add.f64 %fd357,[%rd221],%fd376; }

	// end inline asm
	add.s64 	%rd222, %rd219, 24;
	// begin inline asm
	{ atom.add.f64 %fd359,[%rd222],%fd376; }

	// end inline asm
	add.s64 	%rd223, %rd219, 32;
	// begin inline asm
	{ atom.add.f64 %fd361,[%rd223],%fd376; }

	// end inline asm
	add.s64 	%rd224, %rd219, 40;
	// begin inline asm
	{ atom.add.f64 %fd363,[%rd224],%fd376; }

	// end inline asm
	add.s64 	%rd225, %rd219, 48;
	// begin inline asm
	{ atom.add.f64 %fd365,[%rd225],%fd28; }

	// end inline asm
	add.s64 	%rd226, %rd219, 56;
	// begin inline asm
	{ atom.add.f64 %fd367,[%rd226],%fd376; }

	// end inline asm
	add.s64 	%rd227, %rd219, 64;
	// begin inline asm
	{ atom.add.f64 %fd369,[%rd227],%fd376; }

	// end inline asm
	add.s64 	%rd228, %rd219, 72;
	// begin inline asm
	{ atom.add.f64 %fd371,[%rd228],%fd376; }

	// end inline asm
	add.s64 	%rd229, %rd219, 80;
	// begin inline asm
	{ atom.add.f64 %fd373,[%rd229],%fd376; }

	// end inline asm
	add.s64 	%rd230, %rd219, 88;
	// begin inline asm
	{ atom.add.f64 %fd375,[%rd230],%fd376; }

	// end inline asm

$L__BB17_46:
	@%p13 bra 	$L__BB17_48;

	mul.lo.s64 	%rd243, %rd37, %rd24;
	add.s64 	%rd231, %rd54, %rd243;
	mov.f64 	%fd400, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd377,[%rd231],%fd400; }

	// end inline asm
	add.s64 	%rd232, %rd231, 8;
	// begin inline asm
	{ atom.add.f64 %fd379,[%rd232],%fd400; }

	// end inline asm
	add.s64 	%rd233, %rd231, 16;
	// begin inline asm
	{ atom.add.f64 %fd381,[%rd233],%fd400; }

	// end inline asm
	add.s64 	%rd234, %rd231, 24;
	// begin inline asm
	{ atom.add.f64 %fd383,[%rd234],%fd400; }

	// end inline asm
	add.s64 	%rd235, %rd231, 32;
	// begin inline asm
	{ atom.add.f64 %fd385,[%rd235],%fd400; }

	// end inline asm
	add.s64 	%rd236, %rd231, 40;
	// begin inline asm
	{ atom.add.f64 %fd387,[%rd236],%fd33; }

	// end inline asm
	add.s64 	%rd237, %rd231, 48;
	// begin inline asm
	{ atom.add.f64 %fd389,[%rd237],%fd400; }

	// end inline asm
	add.s64 	%rd238, %rd231, 56;
	// begin inline asm
	{ atom.add.f64 %fd391,[%rd238],%fd400; }

	// end inline asm
	add.s64 	%rd239, %rd231, 64;
	// begin inline asm
	{ atom.add.f64 %fd393,[%rd239],%fd400; }

	// end inline asm
	add.s64 	%rd240, %rd231, 72;
	// begin inline asm
	{ atom.add.f64 %fd395,[%rd240],%fd400; }

	// end inline asm
	add.s64 	%rd241, %rd231, 80;
	// begin inline asm
	{ atom.add.f64 %fd397,[%rd241],%fd400; }

	// end inline asm
	add.s64 	%rd242, %rd231, 88;
	// begin inline asm
	{ atom.add.f64 %fd399,[%rd242],%fd400; }

	// end inline asm
	bra.uni 	$L__BB17_50;

$L__BB17_48:
	setp.eq.s64 	%p26, %rd49, 0;
	@%p26 bra 	$L__BB17_50;

	add.s64 	%rd244, %rd49, %rd38;
	mov.f64 	%fd424, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd401,[%rd244],%fd424; }

	// end inline asm
	add.s64 	%rd245, %rd244, 8;
	// begin inline asm
	{ atom.add.f64 %fd403,[%rd245],%fd424; }

	// end inline asm
	add.s64 	%rd246, %rd244, 16;
	// begin inline asm
	{ atom.add.f64 %fd405,[%rd246],%fd424; }

	// end inline asm
	add.s64 	%rd247, %rd244, 24;
	// begin inline asm
	{ atom.add.f64 %fd407,[%rd247],%fd424; }

	// end inline asm
	add.s64 	%rd248, %rd244, 32;
	// begin inline asm
	{ atom.add.f64 %fd409,[%rd248],%fd424; }

	// end inline asm
	add.s64 	%rd249, %rd244, 40;
	// begin inline asm
	{ atom.add.f64 %fd411,[%rd249],%fd33; }

	// end inline asm
	add.s64 	%rd250, %rd244, 48;
	// begin inline asm
	{ atom.add.f64 %fd413,[%rd250],%fd424; }

	// end inline asm
	add.s64 	%rd251, %rd244, 56;
	// begin inline asm
	{ atom.add.f64 %fd415,[%rd251],%fd424; }

	// end inline asm
	add.s64 	%rd252, %rd244, 64;
	// begin inline asm
	{ atom.add.f64 %fd417,[%rd252],%fd424; }

	// end inline asm
	add.s64 	%rd253, %rd244, 72;
	// begin inline asm
	{ atom.add.f64 %fd419,[%rd253],%fd424; }

	// end inline asm
	add.s64 	%rd254, %rd244, 80;
	// begin inline asm
	{ atom.add.f64 %fd421,[%rd254],%fd424; }

	// end inline asm
	add.s64 	%rd255, %rd244, 88;
	// begin inline asm
	{ atom.add.f64 %fd423,[%rd255],%fd424; }

	// end inline asm

$L__BB17_50:
	@%p13 bra 	$L__BB17_52;

	mul.lo.s64 	%rd268, %rd37, %rd24;
	add.s64 	%rd256, %rd54, %rd268;
	mov.f64 	%fd448, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd425,[%rd256],%fd448; }

	// end inline asm
	add.s64 	%rd257, %rd256, 8;
	// begin inline asm
	{ atom.add.f64 %fd427,[%rd257],%fd448; }

	// end inline asm
	add.s64 	%rd258, %rd256, 16;
	// begin inline asm
	{ atom.add.f64 %fd429,[%rd258],%fd448; }

	// end inline asm
	add.s64 	%rd259, %rd256, 24;
	// begin inline asm
	{ atom.add.f64 %fd431,[%rd259],%fd448; }

	// end inline asm
	add.s64 	%rd260, %rd256, 32;
	// begin inline asm
	{ atom.add.f64 %fd433,[%rd260],%fd32; }

	// end inline asm
	add.s64 	%rd261, %rd256, 40;
	// begin inline asm
	{ atom.add.f64 %fd435,[%rd261],%fd448; }

	// end inline asm
	add.s64 	%rd262, %rd256, 48;
	// begin inline asm
	{ atom.add.f64 %fd437,[%rd262],%fd448; }

	// end inline asm
	add.s64 	%rd263, %rd256, 56;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd263],%fd448; }

	// end inline asm
	add.s64 	%rd264, %rd256, 64;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd264],%fd448; }

	// end inline asm
	add.s64 	%rd265, %rd256, 72;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd265],%fd448; }

	// end inline asm
	add.s64 	%rd266, %rd256, 80;
	// begin inline asm
	{ atom.add.f64 %fd445,[%rd266],%fd448; }

	// end inline asm
	add.s64 	%rd267, %rd256, 88;
	// begin inline asm
	{ atom.add.f64 %fd447,[%rd267],%fd448; }

	// end inline asm
	bra.uni 	$L__BB17_54;

$L__BB17_52:
	setp.eq.s64 	%p28, %rd49, 0;
	@%p28 bra 	$L__BB17_54;

	add.s64 	%rd269, %rd49, %rd38;
	mov.f64 	%fd472, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd449,[%rd269],%fd472; }

	// end inline asm
	add.s64 	%rd270, %rd269, 8;
	// begin inline asm
	{ atom.add.f64 %fd451,[%rd270],%fd472; }

	// end inline asm
	add.s64 	%rd271, %rd269, 16;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd271],%fd472; }

	// end inline asm
	add.s64 	%rd272, %rd269, 24;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd272],%fd472; }

	// end inline asm
	add.s64 	%rd273, %rd269, 32;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd273],%fd32; }

	// end inline asm
	add.s64 	%rd274, %rd269, 40;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd274],%fd472; }

	// end inline asm
	add.s64 	%rd275, %rd269, 48;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd275],%fd472; }

	// end inline asm
	add.s64 	%rd276, %rd269, 56;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd276],%fd472; }

	// end inline asm
	add.s64 	%rd277, %rd269, 64;
	// begin inline asm
	{ atom.add.f64 %fd465,[%rd277],%fd472; }

	// end inline asm
	add.s64 	%rd278, %rd269, 72;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd278],%fd472; }

	// end inline asm
	add.s64 	%rd279, %rd269, 80;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd279],%fd472; }

	// end inline asm
	add.s64 	%rd280, %rd269, 88;
	// begin inline asm
	{ atom.add.f64 %fd471,[%rd280],%fd472; }

	// end inline asm

$L__BB17_54:
	@%p13 bra 	$L__BB17_56;

	mul.lo.s64 	%rd293, %rd37, %rd24;
	add.s64 	%rd281, %rd54, %rd293;
	mov.f64 	%fd496, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd473,[%rd281],%fd496; }

	// end inline asm
	add.s64 	%rd282, %rd281, 8;
	// begin inline asm
	{ atom.add.f64 %fd475,[%rd282],%fd496; }

	// end inline asm
	add.s64 	%rd283, %rd281, 16;
	// begin inline asm
	{ atom.add.f64 %fd477,[%rd283],%fd496; }

	// end inline asm
	add.s64 	%rd284, %rd281, 24;
	// begin inline asm
	{ atom.add.f64 %fd479,[%rd284],%fd31; }

	// end inline asm
	add.s64 	%rd285, %rd281, 32;
	// begin inline asm
	{ atom.add.f64 %fd481,[%rd285],%fd496; }

	// end inline asm
	add.s64 	%rd286, %rd281, 40;
	// begin inline asm
	{ atom.add.f64 %fd483,[%rd286],%fd496; }

	// end inline asm
	add.s64 	%rd287, %rd281, 48;
	// begin inline asm
	{ atom.add.f64 %fd485,[%rd287],%fd496; }

	// end inline asm
	add.s64 	%rd288, %rd281, 56;
	// begin inline asm
	{ atom.add.f64 %fd487,[%rd288],%fd496; }

	// end inline asm
	add.s64 	%rd289, %rd281, 64;
	// begin inline asm
	{ atom.add.f64 %fd489,[%rd289],%fd496; }

	// end inline asm
	add.s64 	%rd290, %rd281, 72;
	// begin inline asm
	{ atom.add.f64 %fd491,[%rd290],%fd496; }

	// end inline asm
	add.s64 	%rd291, %rd281, 80;
	// begin inline asm
	{ atom.add.f64 %fd493,[%rd291],%fd496; }

	// end inline asm
	add.s64 	%rd292, %rd281, 88;
	// begin inline asm
	{ atom.add.f64 %fd495,[%rd292],%fd496; }

	// end inline asm
	bra.uni 	$L__BB17_58;

$L__BB17_56:
	setp.eq.s64 	%p30, %rd49, 0;
	@%p30 bra 	$L__BB17_58;

	add.s64 	%rd294, %rd49, %rd38;
	mov.f64 	%fd520, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd497,[%rd294],%fd520; }

	// end inline asm
	add.s64 	%rd295, %rd294, 8;
	// begin inline asm
	{ atom.add.f64 %fd499,[%rd295],%fd520; }

	// end inline asm
	add.s64 	%rd296, %rd294, 16;
	// begin inline asm
	{ atom.add.f64 %fd501,[%rd296],%fd520; }

	// end inline asm
	add.s64 	%rd297, %rd294, 24;
	// begin inline asm
	{ atom.add.f64 %fd503,[%rd297],%fd31; }

	// end inline asm
	add.s64 	%rd298, %rd294, 32;
	// begin inline asm
	{ atom.add.f64 %fd505,[%rd298],%fd520; }

	// end inline asm
	add.s64 	%rd299, %rd294, 40;
	// begin inline asm
	{ atom.add.f64 %fd507,[%rd299],%fd520; }

	// end inline asm
	add.s64 	%rd300, %rd294, 48;
	// begin inline asm
	{ atom.add.f64 %fd509,[%rd300],%fd520; }

	// end inline asm
	add.s64 	%rd301, %rd294, 56;
	// begin inline asm
	{ atom.add.f64 %fd511,[%rd301],%fd520; }

	// end inline asm
	add.s64 	%rd302, %rd294, 64;
	// begin inline asm
	{ atom.add.f64 %fd513,[%rd302],%fd520; }

	// end inline asm
	add.s64 	%rd303, %rd294, 72;
	// begin inline asm
	{ atom.add.f64 %fd515,[%rd303],%fd520; }

	// end inline asm
	add.s64 	%rd304, %rd294, 80;
	// begin inline asm
	{ atom.add.f64 %fd517,[%rd304],%fd520; }

	// end inline asm
	add.s64 	%rd305, %rd294, 88;
	// begin inline asm
	{ atom.add.f64 %fd519,[%rd305],%fd520; }

	// end inline asm

$L__BB17_58:
	@%p13 bra 	$L__BB17_60;

	mul.lo.s64 	%rd318, %rd37, %rd24;
	add.s64 	%rd306, %rd54, %rd318;
	mov.f64 	%fd544, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd521,[%rd306],%fd544; }

	// end inline asm
	add.s64 	%rd307, %rd306, 8;
	// begin inline asm
	{ atom.add.f64 %fd523,[%rd307],%fd544; }

	// end inline asm
	add.s64 	%rd308, %rd306, 16;
	// begin inline asm
	{ atom.add.f64 %fd525,[%rd308],%fd36; }

	// end inline asm
	add.s64 	%rd309, %rd306, 24;
	// begin inline asm
	{ atom.add.f64 %fd527,[%rd309],%fd544; }

	// end inline asm
	add.s64 	%rd310, %rd306, 32;
	// begin inline asm
	{ atom.add.f64 %fd529,[%rd310],%fd544; }

	// end inline asm
	add.s64 	%rd311, %rd306, 40;
	// begin inline asm
	{ atom.add.f64 %fd531,[%rd311],%fd544; }

	// end inline asm
	add.s64 	%rd312, %rd306, 48;
	// begin inline asm
	{ atom.add.f64 %fd533,[%rd312],%fd544; }

	// end inline asm
	add.s64 	%rd313, %rd306, 56;
	// begin inline asm
	{ atom.add.f64 %fd535,[%rd313],%fd544; }

	// end inline asm
	add.s64 	%rd314, %rd306, 64;
	// begin inline asm
	{ atom.add.f64 %fd537,[%rd314],%fd544; }

	// end inline asm
	add.s64 	%rd315, %rd306, 72;
	// begin inline asm
	{ atom.add.f64 %fd539,[%rd315],%fd544; }

	// end inline asm
	add.s64 	%rd316, %rd306, 80;
	// begin inline asm
	{ atom.add.f64 %fd541,[%rd316],%fd544; }

	// end inline asm
	add.s64 	%rd317, %rd306, 88;
	// begin inline asm
	{ atom.add.f64 %fd543,[%rd317],%fd544; }

	// end inline asm
	bra.uni 	$L__BB17_62;

$L__BB17_60:
	setp.eq.s64 	%p32, %rd49, 0;
	@%p32 bra 	$L__BB17_62;

	add.s64 	%rd319, %rd49, %rd38;
	mov.f64 	%fd568, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd545,[%rd319],%fd568; }

	// end inline asm
	add.s64 	%rd320, %rd319, 8;
	// begin inline asm
	{ atom.add.f64 %fd547,[%rd320],%fd568; }

	// end inline asm
	add.s64 	%rd321, %rd319, 16;
	// begin inline asm
	{ atom.add.f64 %fd549,[%rd321],%fd36; }

	// end inline asm
	add.s64 	%rd322, %rd319, 24;
	// begin inline asm
	{ atom.add.f64 %fd551,[%rd322],%fd568; }

	// end inline asm
	add.s64 	%rd323, %rd319, 32;
	// begin inline asm
	{ atom.add.f64 %fd553,[%rd323],%fd568; }

	// end inline asm
	add.s64 	%rd324, %rd319, 40;
	// begin inline asm
	{ atom.add.f64 %fd555,[%rd324],%fd568; }

	// end inline asm
	add.s64 	%rd325, %rd319, 48;
	// begin inline asm
	{ atom.add.f64 %fd557,[%rd325],%fd568; }

	// end inline asm
	add.s64 	%rd326, %rd319, 56;
	// begin inline asm
	{ atom.add.f64 %fd559,[%rd326],%fd568; }

	// end inline asm
	add.s64 	%rd327, %rd319, 64;
	// begin inline asm
	{ atom.add.f64 %fd561,[%rd327],%fd568; }

	// end inline asm
	add.s64 	%rd328, %rd319, 72;
	// begin inline asm
	{ atom.add.f64 %fd563,[%rd328],%fd568; }

	// end inline asm
	add.s64 	%rd329, %rd319, 80;
	// begin inline asm
	{ atom.add.f64 %fd565,[%rd329],%fd568; }

	// end inline asm
	add.s64 	%rd330, %rd319, 88;
	// begin inline asm
	{ atom.add.f64 %fd567,[%rd330],%fd568; }

	// end inline asm

$L__BB17_62:
	@%p13 bra 	$L__BB17_64;

	mul.lo.s64 	%rd343, %rd37, %rd24;
	add.s64 	%rd331, %rd54, %rd343;
	mov.f64 	%fd592, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd569,[%rd331],%fd592; }

	// end inline asm
	add.s64 	%rd332, %rd331, 8;
	// begin inline asm
	{ atom.add.f64 %fd571,[%rd332],%fd35; }

	// end inline asm
	add.s64 	%rd333, %rd331, 16;
	// begin inline asm
	{ atom.add.f64 %fd573,[%rd333],%fd592; }

	// end inline asm
	add.s64 	%rd334, %rd331, 24;
	// begin inline asm
	{ atom.add.f64 %fd575,[%rd334],%fd592; }

	// end inline asm
	add.s64 	%rd335, %rd331, 32;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd335],%fd592; }

	// end inline asm
	add.s64 	%rd336, %rd331, 40;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd336],%fd592; }

	// end inline asm
	add.s64 	%rd337, %rd331, 48;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd337],%fd592; }

	// end inline asm
	add.s64 	%rd338, %rd331, 56;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd338],%fd592; }

	// end inline asm
	add.s64 	%rd339, %rd331, 64;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd339],%fd592; }

	// end inline asm
	add.s64 	%rd340, %rd331, 72;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd340],%fd592; }

	// end inline asm
	add.s64 	%rd341, %rd331, 80;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd341],%fd592; }

	// end inline asm
	add.s64 	%rd342, %rd331, 88;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd342],%fd592; }

	// end inline asm
	bra.uni 	$L__BB17_66;

$L__BB17_64:
	setp.eq.s64 	%p34, %rd49, 0;
	@%p34 bra 	$L__BB17_66;

	add.s64 	%rd344, %rd49, %rd38;
	mov.f64 	%fd616, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd344],%fd616; }

	// end inline asm
	add.s64 	%rd345, %rd344, 8;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd345],%fd35; }

	// end inline asm
	add.s64 	%rd346, %rd344, 16;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd346],%fd616; }

	// end inline asm
	add.s64 	%rd347, %rd344, 24;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd347],%fd616; }

	// end inline asm
	add.s64 	%rd348, %rd344, 32;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd348],%fd616; }

	// end inline asm
	add.s64 	%rd349, %rd344, 40;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd349],%fd616; }

	// end inline asm
	add.s64 	%rd350, %rd344, 48;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd350],%fd616; }

	// end inline asm
	add.s64 	%rd351, %rd344, 56;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd351],%fd616; }

	// end inline asm
	add.s64 	%rd352, %rd344, 64;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd352],%fd616; }

	// end inline asm
	add.s64 	%rd353, %rd344, 72;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd353],%fd616; }

	// end inline asm
	add.s64 	%rd354, %rd344, 80;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd354],%fd616; }

	// end inline asm
	add.s64 	%rd355, %rd344, 88;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd355],%fd616; }

	// end inline asm

$L__BB17_66:
	@%p13 bra 	$L__BB17_68;

	mul.lo.s64 	%rd368, %rd37, %rd24;
	add.s64 	%rd356, %rd54, %rd368;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd356],%fd34; }

	// end inline asm
	add.s64 	%rd357, %rd356, 8;
	mov.f64 	%fd640, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd357],%fd640; }

	// end inline asm
	add.s64 	%rd358, %rd356, 16;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd358],%fd640; }

	// end inline asm
	add.s64 	%rd359, %rd356, 24;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd359],%fd640; }

	// end inline asm
	add.s64 	%rd360, %rd356, 32;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd360],%fd640; }

	// end inline asm
	add.s64 	%rd361, %rd356, 40;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd361],%fd640; }

	// end inline asm
	add.s64 	%rd362, %rd356, 48;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd362],%fd640; }

	// end inline asm
	add.s64 	%rd363, %rd356, 56;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd363],%fd640; }

	// end inline asm
	add.s64 	%rd364, %rd356, 64;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd364],%fd640; }

	// end inline asm
	add.s64 	%rd365, %rd356, 72;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd365],%fd640; }

	// end inline asm
	add.s64 	%rd366, %rd356, 80;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd366],%fd640; }

	// end inline asm
	add.s64 	%rd367, %rd356, 88;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd367],%fd640; }

	// end inline asm
	bra.uni 	$L__BB17_70;

$L__BB17_68:
	setp.eq.s64 	%p36, %rd49, 0;
	@%p36 bra 	$L__BB17_70;

	add.s64 	%rd369, %rd49, %rd38;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd369],%fd34; }

	// end inline asm
	add.s64 	%rd370, %rd369, 8;
	mov.f64 	%fd664, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd370],%fd664; }

	// end inline asm
	add.s64 	%rd371, %rd369, 16;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd371],%fd664; }

	// end inline asm
	add.s64 	%rd372, %rd369, 24;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd372],%fd664; }

	// end inline asm
	add.s64 	%rd373, %rd369, 32;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd373],%fd664; }

	// end inline asm
	add.s64 	%rd374, %rd369, 40;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd374],%fd664; }

	// end inline asm
	add.s64 	%rd375, %rd369, 48;
	// begin inline asm
	{ atom.add.f64 %fd653,[%rd375],%fd664; }

	// end inline asm
	add.s64 	%rd376, %rd369, 56;
	// begin inline asm
	{ atom.add.f64 %fd655,[%rd376],%fd664; }

	// end inline asm
	add.s64 	%rd377, %rd369, 64;
	// begin inline asm
	{ atom.add.f64 %fd657,[%rd377],%fd664; }

	// end inline asm
	add.s64 	%rd378, %rd369, 72;
	// begin inline asm
	{ atom.add.f64 %fd659,[%rd378],%fd664; }

	// end inline asm
	add.s64 	%rd379, %rd369, 80;
	// begin inline asm
	{ atom.add.f64 %fd661,[%rd379],%fd664; }

	// end inline asm
	add.s64 	%rd380, %rd369, 88;
	// begin inline asm
	{ atom.add.f64 %fd663,[%rd380],%fd664; }

	// end inline asm

$L__BB17_70:
	ld.param.u64 	%rd381, [y_to_x_cuda_kernel_backward_param_0+24];
	add.s64 	%rd384, %rd384, %rd25;
	setp.lt.u64 	%p37, %rd384, %rd381;
	@%p37 bra 	$L__BB17_2;

$L__BB17_71:
	ret;

}
	// .globl	advection_y_cuda_kernel_forward
.visible .entry advection_y_cuda_kernel_forward(
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 advection_y_cuda_kernel_forward_param_4[56],
	.param .f64 advection_y_cuda_kernel_forward_param_5,
	.param .u32 advection_y_cuda_kernel_forward_param_6
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<95>;
	.reg .f64 	%fd<159>;
	.reg .b64 	%rd<59>;


	ld.param.v2.u32 	{%r45, %r46}, [advection_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r47, %r48}, [advection_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r53, %r54}, [advection_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r61, %r62}, [advection_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r69, %r70}, [advection_y_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r77, %r78}, [advection_y_cuda_kernel_forward_param_4+32];
	ld.param.f64 	%fd2, [advection_y_cuda_kernel_forward_param_5];
	ld.param.u32 	%r44, [advection_y_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd37, [advection_y_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd35, [advection_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [advection_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [advection_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [advection_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [advection_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r81, %ntid.x;
	cvt.u64.u32 	%rd1, %r81;
	mov.u32 	%r82, %ctaid.x;
	mul.wide.u32 	%rd39, %r81, %r82;
	mov.u32 	%r83, %tid.x;
	cvt.u64.u32 	%rd40, %r83;
	add.s64 	%rd55, %rd39, %rd40;
	setp.ge.u64 	%p1, %rd55, %rd30;
	@%p1 bra 	$L__BB18_19;

	cvta.to.global.u64 	%rd4, %rd37;
	cvta.to.global.u64 	%rd5, %rd35;
	cvta.to.global.u64 	%rd6, %rd31;
	cvta.to.global.u64 	%rd7, %rd33;
	cvt.s64.s32 	%rd8, %r48;
	cvt.s64.s32 	%rd9, %r47;
	cvt.s64.s32 	%rd10, %r46;
	mov.u32 	%r84, %nctaid.x;
	cvt.u64.u32 	%rd41, %r84;
	mul.lo.s64 	%rd11, %rd1, %rd41;
	cvt.s64.s32 	%rd12, %r69;
	cvt.s64.s32 	%rd13, %r53;
	mul.f64 	%fd1, %fd2, %fd2;
	cvt.s64.s32 	%rd14, %r77;
	cvt.s64.s32 	%rd15, %r61;

$L__BB18_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd56, %rd55;
	@%p2 bra 	$L__BB18_6;

	or.b64  	%rd42, %rd55, %rd8;
	and.b64  	%rd43, %rd42, -4294967296;
	setp.eq.s64 	%p3, %rd43, 0;
	@%p3 bra 	$L__BB18_5;

	div.u64 	%rd56, %rd55, %rd8;
	bra.uni 	$L__BB18_6;

$L__BB18_5:
	cvt.u32.u64 	%r85, %rd8;
	cvt.u32.u64 	%r86, %rd55;
	div.u32 	%r87, %r86, %r85;
	cvt.u64.u32 	%rd56, %r87;

$L__BB18_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB18_10;

	or.b64  	%rd44, %rd56, %rd9;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB18_9;

	div.u64 	%rd56, %rd56, %rd9;
	bra.uni 	$L__BB18_10;

$L__BB18_9:
	cvt.u32.u64 	%r88, %rd9;
	cvt.u32.u64 	%r89, %rd56;
	div.u32 	%r90, %r89, %r88;
	cvt.u64.u32 	%rd56, %r90;

$L__BB18_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB18_14;

	or.b64  	%rd46, %rd56, %rd10;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p7, %rd47, 0;
	@%p7 bra 	$L__BB18_13;

	div.u64 	%rd56, %rd56, %rd10;
	bra.uni 	$L__BB18_14;

$L__BB18_13:
	cvt.u32.u64 	%r91, %rd10;
	cvt.u32.u64 	%r92, %rd56;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd56, %r93;

$L__BB18_14:
	cvt.u32.u64 	%r94, %rd56;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r94, 0, %p8;
	cvt.s64.s32 	%rd48, %r2;
	mul.lo.s64 	%rd49, %rd48, %rd12;
	add.s64 	%rd26, %rd5, %rd49;
	mul.lo.s64 	%rd50, %rd48, %rd13;
	add.s64 	%rd27, %rd6, %rd50;
	mul.lo.s64 	%rd51, %rd48, %rd14;
	add.s64 	%rd28, %rd4, %rd51;
	setp.eq.s32 	%p9, %r44, 0;
	@%p9 bra 	$L__BB18_17;

	setp.ne.s32 	%p10, %r44, 1;
	@%p10 bra 	$L__BB18_18;

	ld.global.f64 	%fd3, [%rd27];
	ld.global.f64 	%fd4, [%rd26];
	sub.f64 	%fd5, %fd4, %fd3;
	ld.global.f64 	%fd6, [%rd27+8];
	ld.global.f64 	%fd7, [%rd26+8];
	sub.f64 	%fd8, %fd7, %fd6;
	ld.global.f64 	%fd9, [%rd27+16];
	ld.global.f64 	%fd10, [%rd26+16];
	sub.f64 	%fd11, %fd10, %fd9;
	ld.global.f64 	%fd12, [%rd27+24];
	ld.global.f64 	%fd13, [%rd26+24];
	sub.f64 	%fd14, %fd13, %fd12;
	ld.global.f64 	%fd15, [%rd27+32];
	ld.global.f64 	%fd16, [%rd26+32];
	sub.f64 	%fd17, %fd16, %fd15;
	ld.global.f64 	%fd18, [%rd27+40];
	ld.global.f64 	%fd19, [%rd26+40];
	sub.f64 	%fd20, %fd19, %fd18;
	ld.global.f64 	%fd21, [%rd27+48];
	ld.global.f64 	%fd22, [%rd26+48];
	sub.f64 	%fd23, %fd22, %fd21;
	ld.global.f64 	%fd24, [%rd27+56];
	ld.global.f64 	%fd25, [%rd26+56];
	sub.f64 	%fd26, %fd25, %fd24;
	ld.global.f64 	%fd27, [%rd27+64];
	ld.global.f64 	%fd28, [%rd26+64];
	sub.f64 	%fd29, %fd28, %fd27;
	ld.global.f64 	%fd30, [%rd27+72];
	ld.global.f64 	%fd31, [%rd26+72];
	sub.f64 	%fd32, %fd31, %fd30;
	ld.global.f64 	%fd33, [%rd27+80];
	ld.global.f64 	%fd34, [%rd26+80];
	sub.f64 	%fd35, %fd34, %fd33;
	ld.global.f64 	%fd36, [%rd27+88];
	ld.global.f64 	%fd37, [%rd26+88];
	sub.f64 	%fd38, %fd37, %fd36;
	div.rn.f64 	%fd39, %fd5, %fd2;
	div.rn.f64 	%fd40, %fd8, %fd2;
	div.rn.f64 	%fd41, %fd11, %fd2;
	div.rn.f64 	%fd42, %fd14, %fd2;
	div.rn.f64 	%fd43, %fd17, %fd2;
	div.rn.f64 	%fd44, %fd20, %fd2;
	div.rn.f64 	%fd45, %fd23, %fd2;
	div.rn.f64 	%fd46, %fd26, %fd2;
	div.rn.f64 	%fd47, %fd29, %fd2;
	div.rn.f64 	%fd48, %fd32, %fd2;
	div.rn.f64 	%fd49, %fd35, %fd2;
	div.rn.f64 	%fd50, %fd38, %fd2;
	mul.lo.s64 	%rd53, %rd48, %rd15;
	add.s64 	%rd54, %rd7, %rd53;
	ld.global.f64 	%fd51, [%rd54];
	sub.f64 	%fd52, %fd39, %fd51;
	ld.global.f64 	%fd53, [%rd54+8];
	sub.f64 	%fd54, %fd40, %fd53;
	ld.global.f64 	%fd55, [%rd54+16];
	sub.f64 	%fd56, %fd41, %fd55;
	ld.global.f64 	%fd57, [%rd54+24];
	sub.f64 	%fd58, %fd42, %fd57;
	ld.global.f64 	%fd59, [%rd54+32];
	sub.f64 	%fd60, %fd43, %fd59;
	ld.global.f64 	%fd61, [%rd54+40];
	sub.f64 	%fd62, %fd44, %fd61;
	ld.global.f64 	%fd63, [%rd54+48];
	sub.f64 	%fd64, %fd45, %fd63;
	ld.global.f64 	%fd65, [%rd54+56];
	sub.f64 	%fd66, %fd46, %fd65;
	ld.global.f64 	%fd67, [%rd54+64];
	sub.f64 	%fd68, %fd47, %fd67;
	ld.global.f64 	%fd69, [%rd54+72];
	sub.f64 	%fd70, %fd48, %fd69;
	ld.global.f64 	%fd71, [%rd54+80];
	sub.f64 	%fd72, %fd49, %fd71;
	ld.global.f64 	%fd73, [%rd54+88];
	sub.f64 	%fd74, %fd50, %fd73;
	div.rn.f64 	%fd75, %fd52, %fd2;
	div.rn.f64 	%fd76, %fd54, %fd2;
	div.rn.f64 	%fd77, %fd56, %fd2;
	div.rn.f64 	%fd78, %fd58, %fd2;
	div.rn.f64 	%fd79, %fd60, %fd2;
	div.rn.f64 	%fd80, %fd62, %fd2;
	div.rn.f64 	%fd81, %fd64, %fd2;
	div.rn.f64 	%fd82, %fd66, %fd2;
	div.rn.f64 	%fd83, %fd68, %fd2;
	div.rn.f64 	%fd84, %fd70, %fd2;
	div.rn.f64 	%fd85, %fd72, %fd2;
	div.rn.f64 	%fd86, %fd74, %fd2;
	st.global.f64 	[%rd28], %fd75;
	st.global.f64 	[%rd28+8], %fd76;
	st.global.f64 	[%rd28+16], %fd77;
	st.global.f64 	[%rd28+24], %fd78;
	st.global.f64 	[%rd28+32], %fd79;
	st.global.f64 	[%rd28+40], %fd80;
	st.global.f64 	[%rd28+48], %fd81;
	st.global.f64 	[%rd28+56], %fd82;
	st.global.f64 	[%rd28+64], %fd83;
	st.global.f64 	[%rd28+72], %fd84;
	st.global.f64 	[%rd28+80], %fd85;
	st.global.f64 	[%rd28+88], %fd86;
	st.global.f64 	[%rd54], %fd39;
	st.global.f64 	[%rd54+8], %fd40;
	st.global.f64 	[%rd54+16], %fd41;
	st.global.f64 	[%rd54+24], %fd42;
	st.global.f64 	[%rd54+32], %fd43;
	st.global.f64 	[%rd54+40], %fd44;
	st.global.f64 	[%rd54+48], %fd45;
	st.global.f64 	[%rd54+56], %fd46;
	st.global.f64 	[%rd54+64], %fd47;
	st.global.f64 	[%rd54+72], %fd48;
	st.global.f64 	[%rd54+80], %fd49;
	st.global.f64 	[%rd54+88], %fd50;
	ld.global.f64 	%fd87, [%rd26];
	ld.global.f64 	%fd88, [%rd26+8];
	ld.global.f64 	%fd89, [%rd26+16];
	ld.global.f64 	%fd90, [%rd26+24];
	ld.global.f64 	%fd91, [%rd26+32];
	ld.global.f64 	%fd92, [%rd26+40];
	ld.global.f64 	%fd93, [%rd26+48];
	ld.global.f64 	%fd94, [%rd26+56];
	ld.global.f64 	%fd95, [%rd26+64];
	ld.global.f64 	%fd96, [%rd26+72];
	ld.global.f64 	%fd97, [%rd26+80];
	ld.global.f64 	%fd98, [%rd26+88];
	st.global.f64 	[%rd27], %fd87;
	st.global.f64 	[%rd27+8], %fd88;
	st.global.f64 	[%rd27+16], %fd89;
	st.global.f64 	[%rd27+24], %fd90;
	st.global.f64 	[%rd27+32], %fd91;
	st.global.f64 	[%rd27+40], %fd92;
	st.global.f64 	[%rd27+48], %fd93;
	st.global.f64 	[%rd27+56], %fd94;
	st.global.f64 	[%rd27+64], %fd95;
	st.global.f64 	[%rd27+72], %fd96;
	st.global.f64 	[%rd27+80], %fd97;
	st.global.f64 	[%rd27+88], %fd98;
	bra.uni 	$L__BB18_18;

$L__BB18_17:
	ld.global.f64 	%fd99, [%rd26];
	ld.global.f64 	%fd100, [%rd27];
	sub.f64 	%fd101, %fd99, %fd100;
	ld.global.f64 	%fd102, [%rd27+8];
	ld.global.f64 	%fd103, [%rd26+8];
	sub.f64 	%fd104, %fd103, %fd102;
	ld.global.f64 	%fd105, [%rd27+16];
	ld.global.f64 	%fd106, [%rd26+16];
	sub.f64 	%fd107, %fd106, %fd105;
	ld.global.f64 	%fd108, [%rd27+24];
	ld.global.f64 	%fd109, [%rd26+24];
	sub.f64 	%fd110, %fd109, %fd108;
	ld.global.f64 	%fd111, [%rd27+32];
	ld.global.f64 	%fd112, [%rd26+32];
	sub.f64 	%fd113, %fd112, %fd111;
	ld.global.f64 	%fd114, [%rd27+40];
	ld.global.f64 	%fd115, [%rd26+40];
	sub.f64 	%fd116, %fd115, %fd114;
	ld.global.f64 	%fd117, [%rd27+48];
	ld.global.f64 	%fd118, [%rd26+48];
	sub.f64 	%fd119, %fd118, %fd117;
	ld.global.f64 	%fd120, [%rd27+56];
	ld.global.f64 	%fd121, [%rd26+56];
	sub.f64 	%fd122, %fd121, %fd120;
	ld.global.f64 	%fd123, [%rd27+64];
	ld.global.f64 	%fd124, [%rd26+64];
	sub.f64 	%fd125, %fd124, %fd123;
	ld.global.f64 	%fd126, [%rd27+72];
	ld.global.f64 	%fd127, [%rd26+72];
	sub.f64 	%fd128, %fd127, %fd126;
	ld.global.f64 	%fd129, [%rd27+80];
	ld.global.f64 	%fd130, [%rd26+80];
	sub.f64 	%fd131, %fd130, %fd129;
	ld.global.f64 	%fd132, [%rd27+88];
	ld.global.f64 	%fd133, [%rd26+88];
	sub.f64 	%fd134, %fd133, %fd132;
	div.rn.f64 	%fd135, %fd101, %fd1;
	div.rn.f64 	%fd136, %fd104, %fd1;
	div.rn.f64 	%fd137, %fd107, %fd1;
	div.rn.f64 	%fd138, %fd110, %fd1;
	div.rn.f64 	%fd139, %fd113, %fd1;
	div.rn.f64 	%fd140, %fd116, %fd1;
	div.rn.f64 	%fd141, %fd119, %fd1;
	div.rn.f64 	%fd142, %fd122, %fd1;
	div.rn.f64 	%fd143, %fd125, %fd1;
	div.rn.f64 	%fd144, %fd128, %fd1;
	div.rn.f64 	%fd145, %fd131, %fd1;
	div.rn.f64 	%fd146, %fd134, %fd1;
	st.global.f64 	[%rd28], %fd135;
	st.global.f64 	[%rd28+8], %fd136;
	st.global.f64 	[%rd28+16], %fd137;
	st.global.f64 	[%rd28+24], %fd138;
	st.global.f64 	[%rd28+32], %fd139;
	st.global.f64 	[%rd28+40], %fd140;
	st.global.f64 	[%rd28+48], %fd141;
	st.global.f64 	[%rd28+56], %fd142;
	st.global.f64 	[%rd28+64], %fd143;
	st.global.f64 	[%rd28+72], %fd144;
	st.global.f64 	[%rd28+80], %fd145;
	st.global.f64 	[%rd28+88], %fd146;
	ld.global.f64 	%fd147, [%rd26];
	ld.global.f64 	%fd148, [%rd26+8];
	ld.global.f64 	%fd149, [%rd26+16];
	ld.global.f64 	%fd150, [%rd26+24];
	ld.global.f64 	%fd151, [%rd26+32];
	ld.global.f64 	%fd152, [%rd26+40];
	ld.global.f64 	%fd153, [%rd26+48];
	ld.global.f64 	%fd154, [%rd26+56];
	ld.global.f64 	%fd155, [%rd26+64];
	ld.global.f64 	%fd156, [%rd26+72];
	ld.global.f64 	%fd157, [%rd26+80];
	ld.global.f64 	%fd158, [%rd26+88];
	st.global.f64 	[%rd27], %fd147;
	st.global.f64 	[%rd27+8], %fd148;
	st.global.f64 	[%rd27+16], %fd149;
	st.global.f64 	[%rd27+24], %fd150;
	st.global.f64 	[%rd27+32], %fd151;
	st.global.f64 	[%rd27+40], %fd152;
	st.global.f64 	[%rd27+48], %fd153;
	st.global.f64 	[%rd27+56], %fd154;
	st.global.f64 	[%rd27+64], %fd155;
	st.global.f64 	[%rd27+72], %fd156;
	st.global.f64 	[%rd27+80], %fd157;
	st.global.f64 	[%rd27+88], %fd158;

$L__BB18_18:
	add.s64 	%rd55, %rd55, %rd11;
	setp.lt.u64 	%p11, %rd55, %rd30;
	@%p11 bra 	$L__BB18_2;

$L__BB18_19:
	ret;

}
	// .globl	advection_y_cuda_kernel_backward
.visible .entry advection_y_cuda_kernel_backward(
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_4[56],
	.param .f64 advection_y_cuda_kernel_backward_param_5,
	.param .u32 advection_y_cuda_kernel_backward_param_6,
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 advection_y_cuda_kernel_backward_param_10[56],
	.param .f64 advection_y_cuda_kernel_backward_param_11,
	.param .u32 advection_y_cuda_kernel_backward_param_12
)
{
	.reg .pred 	%p<44>;
	.reg .b16 	%rs<71>;
	.reg .b32 	%r<164>;
	.reg .f64 	%fd<954>;
	.reg .b64 	%rd<289>;


	ld.param.v2.u32 	{%r81, %r82}, [advection_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r83, %r84}, [advection_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r89, %r90}, [advection_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r97, %r98}, [advection_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r105, %r106}, [advection_y_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r113, %r114}, [advection_y_cuda_kernel_backward_param_4+32];
	ld.param.f64 	%fd256, [advection_y_cuda_kernel_backward_param_5];
	ld.param.u32 	%r44, [advection_y_cuda_kernel_backward_param_6];
	ld.param.v2.u32 	{%r121, %r122}, [advection_y_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r129, %r130}, [advection_y_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r137, %r138}, [advection_y_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r145, %r146}, [advection_y_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd60, [advection_y_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd58, [advection_y_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd56, [advection_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd54, [advection_y_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd53, [advection_y_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd51, [advection_y_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd49, [advection_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd47, [advection_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd45, [advection_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [advection_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r149, %ntid.x;
	cvt.u64.u32 	%rd1, %r149;
	mov.u32 	%r150, %ctaid.x;
	mul.wide.u32 	%rd62, %r149, %r150;
	mov.u32 	%r151, %tid.x;
	cvt.u64.u32 	%rd63, %r151;
	add.s64 	%rd285, %rd62, %rd63;
	setp.ge.u64 	%p2, %rd285, %rd45;
	@%p2 bra 	$L__BB19_65;

	cvt.s64.s32 	%rd18, %r84;
	cvt.s64.s32 	%rd19, %r83;
	cvt.s64.s32 	%rd20, %r82;
	cvt.s64.s32 	%rd21, %r105;
	cvt.s64.s32 	%rd22, %r89;
	setp.ne.s32 	%p1, %r44, 0;
	mul.f64 	%fd1, %fd256, %fd256;
	cvt.s64.s32 	%rd23, %r121;
	mov.u32 	%r152, %nctaid.x;
	cvt.u64.u32 	%rd64, %r152;
	mul.lo.s64 	%rd24, %rd1, %rd64;
	cvt.s64.s32 	%rd25, %r137;
	cvt.s64.s32 	%rd26, %r129;
	cvt.s64.s32 	%rd27, %r97;
	cvt.s64.s32 	%rd28, %r145;
	cvt.s64.s32 	%rd29, %r113;
	not.pred 	%p13, %p1;

$L__BB19_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd286, %rd285;
	@%p3 bra 	$L__BB19_6;

	or.b64  	%rd65, %rd285, %rd18;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p4, %rd66, 0;
	@%p4 bra 	$L__BB19_5;

	div.u64 	%rd286, %rd285, %rd18;
	bra.uni 	$L__BB19_6;

$L__BB19_5:
	cvt.u32.u64 	%r153, %rd18;
	cvt.u32.u64 	%r154, %rd285;
	div.u32 	%r155, %r154, %r153;
	cvt.u64.u32 	%rd286, %r155;

$L__BB19_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB19_10;

	or.b64  	%rd67, %rd286, %rd19;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p6, %rd68, 0;
	@%p6 bra 	$L__BB19_9;

	div.u64 	%rd286, %rd286, %rd19;
	bra.uni 	$L__BB19_10;

$L__BB19_9:
	cvt.u32.u64 	%r156, %rd19;
	cvt.u32.u64 	%r157, %rd286;
	div.u32 	%r158, %r157, %r156;
	cvt.u64.u32 	%rd286, %r158;

$L__BB19_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB19_14;

	or.b64  	%rd69, %rd286, %rd20;
	and.b64  	%rd70, %rd69, -4294967296;
	setp.eq.s64 	%p8, %rd70, 0;
	@%p8 bra 	$L__BB19_13;

	div.u64 	%rd286, %rd286, %rd20;
	bra.uni 	$L__BB19_14;

$L__BB19_13:
	cvt.u32.u64 	%r159, %rd20;
	cvt.u32.u64 	%r160, %rd286;
	div.u32 	%r161, %r160, %r159;
	cvt.u64.u32 	%rd286, %r161;

$L__BB19_14:
	cvta.to.global.u64 	%rd282, %rd53;
	cvta.to.global.u64 	%rd281, %rd60;
	cvta.to.global.u64 	%rd280, %rd47;
	cvta.to.global.u64 	%rd279, %rd54;
	ld.param.u32 	%r163, [advection_y_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r162, %rd286;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r2, %r162, 0, %p9;
	setp.eq.s32 	%p10, %r163, 1;
	selp.u16 	%rs68, 1, 0, %p10;
	setp.eq.s32 	%p11, %r163, 0;
	selp.f64 	%fd893, %fd1, %fd893, %p11;
	selp.b16 	%rs70, %rs70, %rs68, %p11;
	and.b16  	%rs69, %rs70, 255;
	setp.eq.s16 	%p12, %rs69, 0;
	cvt.s64.s32 	%rd71, %r2;
	mul.lo.s64 	%rd72, %rd71, %rd23;
	add.s64 	%rd40, %rd279, %rd72;
	mul.lo.s64 	%rd73, %rd71, %rd22;
	add.s64 	%rd41, %rd280, %rd73;
	mul.lo.s64 	%rd74, %rd71, %rd28;
	add.s64 	%rd42, %rd281, %rd74;
	mul.lo.s64 	%rd75, %rd71, %rd29;
	add.s64 	%rd43, %rd282, %rd75;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB19_43;

	setp.eq.s64 	%p15, %rd54, 0;
	@%p15 bra 	$L__BB19_17;

	ld.global.f64 	%fd258, [%rd40];
	add.f64 	%fd905, %fd258, 0d0000000000000000;
	ld.global.f64 	%fd259, [%rd40+8];
	add.f64 	%fd904, %fd259, 0d0000000000000000;
	ld.global.f64 	%fd260, [%rd40+16];
	add.f64 	%fd903, %fd260, 0d0000000000000000;
	ld.global.f64 	%fd261, [%rd40+24];
	add.f64 	%fd902, %fd261, 0d0000000000000000;
	ld.global.f64 	%fd262, [%rd40+32];
	add.f64 	%fd901, %fd262, 0d0000000000000000;
	ld.global.f64 	%fd263, [%rd40+40];
	add.f64 	%fd900, %fd263, 0d0000000000000000;
	ld.global.f64 	%fd264, [%rd40+48];
	add.f64 	%fd899, %fd264, 0d0000000000000000;
	ld.global.f64 	%fd265, [%rd40+56];
	add.f64 	%fd898, %fd265, 0d0000000000000000;
	ld.global.f64 	%fd266, [%rd40+64];
	add.f64 	%fd897, %fd266, 0d0000000000000000;
	ld.global.f64 	%fd267, [%rd40+72];
	add.f64 	%fd896, %fd267, 0d0000000000000000;
	ld.global.f64 	%fd268, [%rd40+80];
	add.f64 	%fd895, %fd268, 0d0000000000000000;
	ld.global.f64 	%fd269, [%rd40+88];
	add.f64 	%fd894, %fd269, 0d0000000000000000;
	bra.uni 	$L__BB19_19;

$L__BB19_17:
	setp.eq.s64 	%p16, %rd47, 0;
	mov.f64 	%fd894, 0d0000000000000000;
	mov.f64 	%fd895, %fd894;
	mov.f64 	%fd896, %fd894;
	mov.f64 	%fd897, %fd894;
	mov.f64 	%fd898, %fd894;
	mov.f64 	%fd899, %fd894;
	mov.f64 	%fd900, %fd894;
	mov.f64 	%fd901, %fd894;
	mov.f64 	%fd902, %fd894;
	mov.f64 	%fd903, %fd894;
	mov.f64 	%fd904, %fd894;
	mov.f64 	%fd905, %fd894;
	@%p16 bra 	$L__BB19_19;

	ld.global.f64 	%fd282, [%rd41];
	add.f64 	%fd905, %fd282, 0d0000000000000000;
	ld.global.f64 	%fd283, [%rd41+8];
	add.f64 	%fd904, %fd283, 0d0000000000000000;
	ld.global.f64 	%fd284, [%rd41+16];
	add.f64 	%fd903, %fd284, 0d0000000000000000;
	ld.global.f64 	%fd285, [%rd41+24];
	add.f64 	%fd902, %fd285, 0d0000000000000000;
	ld.global.f64 	%fd286, [%rd41+32];
	add.f64 	%fd901, %fd286, 0d0000000000000000;
	ld.global.f64 	%fd287, [%rd41+40];
	add.f64 	%fd900, %fd287, 0d0000000000000000;
	ld.global.f64 	%fd288, [%rd41+48];
	add.f64 	%fd899, %fd288, 0d0000000000000000;
	ld.global.f64 	%fd289, [%rd41+56];
	add.f64 	%fd898, %fd289, 0d0000000000000000;
	ld.global.f64 	%fd290, [%rd41+64];
	add.f64 	%fd897, %fd290, 0d0000000000000000;
	ld.global.f64 	%fd291, [%rd41+72];
	add.f64 	%fd896, %fd291, 0d0000000000000000;
	ld.global.f64 	%fd292, [%rd41+80];
	add.f64 	%fd895, %fd292, 0d0000000000000000;
	ld.global.f64 	%fd293, [%rd41+88];
	add.f64 	%fd894, %fd293, 0d0000000000000000;

$L__BB19_19:
	setp.eq.s64 	%p17, %rd58, 0;
	@%p17 bra 	$L__BB19_21;

	mul.lo.s64 	%rd89, %rd71, %rd25;
	add.s64 	%rd76, %rd58, %rd89;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd76],%fd905; }

	// end inline asm
	add.s64 	%rd77, %rd76, 8;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd77],%fd904; }

	// end inline asm
	add.s64 	%rd78, %rd76, 16;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd78],%fd903; }

	// end inline asm
	add.s64 	%rd79, %rd76, 24;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd79],%fd902; }

	// end inline asm
	add.s64 	%rd80, %rd76, 32;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd80],%fd901; }

	// end inline asm
	add.s64 	%rd81, %rd76, 40;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd81],%fd900; }

	// end inline asm
	add.s64 	%rd82, %rd76, 48;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd82],%fd899; }

	// end inline asm
	add.s64 	%rd83, %rd76, 56;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd83],%fd898; }

	// end inline asm
	add.s64 	%rd84, %rd76, 64;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd84],%fd897; }

	// end inline asm
	add.s64 	%rd85, %rd76, 72;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd85],%fd896; }

	// end inline asm
	add.s64 	%rd86, %rd76, 80;
	// begin inline asm
	{ atom.add.f64 %fd314,[%rd86],%fd895; }

	// end inline asm
	add.s64 	%rd87, %rd76, 88;
	// begin inline asm
	{ atom.add.f64 %fd316,[%rd87],%fd894; }

	// end inline asm
	bra.uni 	$L__BB19_23;

$L__BB19_21:
	setp.eq.s64 	%p18, %rd51, 0;
	@%p18 bra 	$L__BB19_23;

	mul.lo.s64 	%rd103, %rd71, %rd21;
	add.s64 	%rd90, %rd51, %rd103;
	// begin inline asm
	{ atom.add.f64 %fd318,[%rd90],%fd905; }

	// end inline asm
	add.s64 	%rd91, %rd90, 8;
	// begin inline asm
	{ atom.add.f64 %fd320,[%rd91],%fd904; }

	// end inline asm
	add.s64 	%rd92, %rd90, 16;
	// begin inline asm
	{ atom.add.f64 %fd322,[%rd92],%fd903; }

	// end inline asm
	add.s64 	%rd93, %rd90, 24;
	// begin inline asm
	{ atom.add.f64 %fd324,[%rd93],%fd902; }

	// end inline asm
	add.s64 	%rd94, %rd90, 32;
	// begin inline asm
	{ atom.add.f64 %fd326,[%rd94],%fd901; }

	// end inline asm
	add.s64 	%rd95, %rd90, 40;
	// begin inline asm
	{ atom.add.f64 %fd328,[%rd95],%fd900; }

	// end inline asm
	add.s64 	%rd96, %rd90, 48;
	// begin inline asm
	{ atom.add.f64 %fd330,[%rd96],%fd899; }

	// end inline asm
	add.s64 	%rd97, %rd90, 56;
	// begin inline asm
	{ atom.add.f64 %fd332,[%rd97],%fd898; }

	// end inline asm
	add.s64 	%rd98, %rd90, 64;
	// begin inline asm
	{ atom.add.f64 %fd334,[%rd98],%fd897; }

	// end inline asm
	add.s64 	%rd99, %rd90, 72;
	// begin inline asm
	{ atom.add.f64 %fd336,[%rd99],%fd896; }

	// end inline asm
	add.s64 	%rd100, %rd90, 80;
	// begin inline asm
	{ atom.add.f64 %fd338,[%rd100],%fd895; }

	// end inline asm
	add.s64 	%rd101, %rd90, 88;
	// begin inline asm
	{ atom.add.f64 %fd340,[%rd101],%fd894; }

	// end inline asm

$L__BB19_23:
	setp.eq.s64 	%p19, %rd56, 0;
	@%p19 bra 	$L__BB19_25;

	cvta.to.global.u64 	%rd283, %rd56;
	mul.lo.s64 	%rd105, %rd71, %rd26;
	add.s64 	%rd106, %rd283, %rd105;
	ld.global.f64 	%fd342, [%rd106];
	add.f64 	%fd917, %fd342, 0d0000000000000000;
	ld.global.f64 	%fd343, [%rd106+8];
	add.f64 	%fd916, %fd343, 0d0000000000000000;
	ld.global.f64 	%fd344, [%rd106+16];
	add.f64 	%fd915, %fd344, 0d0000000000000000;
	ld.global.f64 	%fd345, [%rd106+24];
	add.f64 	%fd914, %fd345, 0d0000000000000000;
	ld.global.f64 	%fd346, [%rd106+32];
	add.f64 	%fd913, %fd346, 0d0000000000000000;
	ld.global.f64 	%fd347, [%rd106+40];
	add.f64 	%fd912, %fd347, 0d0000000000000000;
	ld.global.f64 	%fd348, [%rd106+48];
	add.f64 	%fd911, %fd348, 0d0000000000000000;
	ld.global.f64 	%fd349, [%rd106+56];
	add.f64 	%fd910, %fd349, 0d0000000000000000;
	ld.global.f64 	%fd350, [%rd106+64];
	add.f64 	%fd909, %fd350, 0d0000000000000000;
	ld.global.f64 	%fd351, [%rd106+72];
	add.f64 	%fd908, %fd351, 0d0000000000000000;
	ld.global.f64 	%fd352, [%rd106+80];
	add.f64 	%fd907, %fd352, 0d0000000000000000;
	ld.global.f64 	%fd353, [%rd106+88];
	add.f64 	%fd906, %fd353, 0d0000000000000000;
	bra.uni 	$L__BB19_27;

$L__BB19_25:
	setp.eq.s64 	%p20, %rd49, 0;
	mov.f64 	%fd906, 0d0000000000000000;
	mov.f64 	%fd907, %fd906;
	mov.f64 	%fd908, %fd906;
	mov.f64 	%fd909, %fd906;
	mov.f64 	%fd910, %fd906;
	mov.f64 	%fd911, %fd906;
	mov.f64 	%fd912, %fd906;
	mov.f64 	%fd913, %fd906;
	mov.f64 	%fd914, %fd906;
	mov.f64 	%fd915, %fd906;
	mov.f64 	%fd916, %fd906;
	mov.f64 	%fd917, %fd906;
	@%p20 bra 	$L__BB19_27;

	cvta.to.global.u64 	%rd284, %rd49;
	mul.lo.s64 	%rd108, %rd71, %rd27;
	add.s64 	%rd109, %rd284, %rd108;
	ld.global.f64 	%fd366, [%rd109];
	add.f64 	%fd917, %fd366, 0d0000000000000000;
	ld.global.f64 	%fd367, [%rd109+8];
	add.f64 	%fd916, %fd367, 0d0000000000000000;
	ld.global.f64 	%fd368, [%rd109+16];
	add.f64 	%fd915, %fd368, 0d0000000000000000;
	ld.global.f64 	%fd369, [%rd109+24];
	add.f64 	%fd914, %fd369, 0d0000000000000000;
	ld.global.f64 	%fd370, [%rd109+32];
	add.f64 	%fd913, %fd370, 0d0000000000000000;
	ld.global.f64 	%fd371, [%rd109+40];
	add.f64 	%fd912, %fd371, 0d0000000000000000;
	ld.global.f64 	%fd372, [%rd109+48];
	add.f64 	%fd911, %fd372, 0d0000000000000000;
	ld.global.f64 	%fd373, [%rd109+56];
	add.f64 	%fd910, %fd373, 0d0000000000000000;
	ld.global.f64 	%fd374, [%rd109+64];
	add.f64 	%fd909, %fd374, 0d0000000000000000;
	ld.global.f64 	%fd375, [%rd109+72];
	add.f64 	%fd908, %fd375, 0d0000000000000000;
	ld.global.f64 	%fd376, [%rd109+80];
	add.f64 	%fd907, %fd376, 0d0000000000000000;
	ld.global.f64 	%fd377, [%rd109+88];
	add.f64 	%fd906, %fd377, 0d0000000000000000;

$L__BB19_27:
	setp.eq.s64 	%p21, %rd60, 0;
	@%p21 bra 	$L__BB19_29;

	ld.global.f64 	%fd378, [%rd42];
	add.f64 	%fd929, %fd378, 0d0000000000000000;
	ld.global.f64 	%fd379, [%rd42+8];
	add.f64 	%fd928, %fd379, 0d0000000000000000;
	ld.global.f64 	%fd380, [%rd42+16];
	add.f64 	%fd927, %fd380, 0d0000000000000000;
	ld.global.f64 	%fd381, [%rd42+24];
	add.f64 	%fd926, %fd381, 0d0000000000000000;
	ld.global.f64 	%fd382, [%rd42+32];
	add.f64 	%fd925, %fd382, 0d0000000000000000;
	ld.global.f64 	%fd383, [%rd42+40];
	add.f64 	%fd924, %fd383, 0d0000000000000000;
	ld.global.f64 	%fd384, [%rd42+48];
	add.f64 	%fd923, %fd384, 0d0000000000000000;
	ld.global.f64 	%fd385, [%rd42+56];
	add.f64 	%fd922, %fd385, 0d0000000000000000;
	ld.global.f64 	%fd386, [%rd42+64];
	add.f64 	%fd921, %fd386, 0d0000000000000000;
	ld.global.f64 	%fd387, [%rd42+72];
	add.f64 	%fd920, %fd387, 0d0000000000000000;
	ld.global.f64 	%fd388, [%rd42+80];
	add.f64 	%fd919, %fd388, 0d0000000000000000;
	ld.global.f64 	%fd389, [%rd42+88];
	add.f64 	%fd918, %fd389, 0d0000000000000000;
	bra.uni 	$L__BB19_31;

$L__BB19_29:
	setp.eq.s64 	%p22, %rd53, 0;
	mov.f64 	%fd918, 0d0000000000000000;
	mov.f64 	%fd919, %fd918;
	mov.f64 	%fd920, %fd918;
	mov.f64 	%fd921, %fd918;
	mov.f64 	%fd922, %fd918;
	mov.f64 	%fd923, %fd918;
	mov.f64 	%fd924, %fd918;
	mov.f64 	%fd925, %fd918;
	mov.f64 	%fd926, %fd918;
	mov.f64 	%fd927, %fd918;
	mov.f64 	%fd928, %fd918;
	mov.f64 	%fd929, %fd918;
	@%p22 bra 	$L__BB19_31;

	ld.global.f64 	%fd402, [%rd43];
	add.f64 	%fd929, %fd402, 0d0000000000000000;
	ld.global.f64 	%fd403, [%rd43+8];
	add.f64 	%fd928, %fd403, 0d0000000000000000;
	ld.global.f64 	%fd404, [%rd43+16];
	add.f64 	%fd927, %fd404, 0d0000000000000000;
	ld.global.f64 	%fd405, [%rd43+24];
	add.f64 	%fd926, %fd405, 0d0000000000000000;
	ld.global.f64 	%fd406, [%rd43+32];
	add.f64 	%fd925, %fd406, 0d0000000000000000;
	ld.global.f64 	%fd407, [%rd43+40];
	add.f64 	%fd924, %fd407, 0d0000000000000000;
	ld.global.f64 	%fd408, [%rd43+48];
	add.f64 	%fd923, %fd408, 0d0000000000000000;
	ld.global.f64 	%fd409, [%rd43+56];
	add.f64 	%fd922, %fd409, 0d0000000000000000;
	ld.global.f64 	%fd410, [%rd43+64];
	add.f64 	%fd921, %fd410, 0d0000000000000000;
	ld.global.f64 	%fd411, [%rd43+72];
	add.f64 	%fd920, %fd411, 0d0000000000000000;
	ld.global.f64 	%fd412, [%rd43+80];
	add.f64 	%fd919, %fd412, 0d0000000000000000;
	ld.global.f64 	%fd413, [%rd43+88];
	add.f64 	%fd918, %fd413, 0d0000000000000000;

$L__BB19_31:
	div.rn.f64 	%fd414, %fd929, %fd256;
	add.f64 	%fd415, %fd414, 0d0000000000000000;
	mov.f64 	%fd416, 0d0000000000000000;
	div.rn.f64 	%fd417, %fd928, %fd256;
	add.f64 	%fd418, %fd417, 0d0000000000000000;
	div.rn.f64 	%fd419, %fd927, %fd256;
	add.f64 	%fd420, %fd419, 0d0000000000000000;
	div.rn.f64 	%fd421, %fd926, %fd256;
	add.f64 	%fd422, %fd421, 0d0000000000000000;
	div.rn.f64 	%fd423, %fd925, %fd256;
	add.f64 	%fd424, %fd423, 0d0000000000000000;
	div.rn.f64 	%fd425, %fd924, %fd256;
	add.f64 	%fd426, %fd425, 0d0000000000000000;
	div.rn.f64 	%fd427, %fd923, %fd256;
	add.f64 	%fd428, %fd427, 0d0000000000000000;
	div.rn.f64 	%fd429, %fd922, %fd256;
	add.f64 	%fd430, %fd429, 0d0000000000000000;
	div.rn.f64 	%fd431, %fd921, %fd256;
	add.f64 	%fd432, %fd431, 0d0000000000000000;
	div.rn.f64 	%fd433, %fd920, %fd256;
	add.f64 	%fd434, %fd433, 0d0000000000000000;
	div.rn.f64 	%fd435, %fd919, %fd256;
	add.f64 	%fd436, %fd435, 0d0000000000000000;
	div.rn.f64 	%fd437, %fd918, %fd256;
	add.f64 	%fd438, %fd437, 0d0000000000000000;
	add.f64 	%fd112, %fd917, %fd415;
	add.f64 	%fd113, %fd916, %fd418;
	add.f64 	%fd114, %fd915, %fd420;
	add.f64 	%fd115, %fd914, %fd422;
	add.f64 	%fd116, %fd913, %fd424;
	add.f64 	%fd117, %fd912, %fd426;
	add.f64 	%fd118, %fd911, %fd428;
	add.f64 	%fd119, %fd910, %fd430;
	add.f64 	%fd120, %fd909, %fd432;
	add.f64 	%fd121, %fd908, %fd434;
	add.f64 	%fd122, %fd907, %fd436;
	add.f64 	%fd123, %fd906, %fd438;
	sub.f64 	%fd124, %fd416, %fd415;
	sub.f64 	%fd125, %fd416, %fd418;
	sub.f64 	%fd126, %fd416, %fd420;
	sub.f64 	%fd127, %fd416, %fd422;
	sub.f64 	%fd128, %fd416, %fd424;
	sub.f64 	%fd129, %fd416, %fd426;
	sub.f64 	%fd130, %fd416, %fd428;
	sub.f64 	%fd131, %fd416, %fd430;
	sub.f64 	%fd132, %fd416, %fd432;
	sub.f64 	%fd133, %fd416, %fd434;
	sub.f64 	%fd134, %fd416, %fd436;
	sub.f64 	%fd135, %fd416, %fd438;
	@%p19 bra 	$L__BB19_33;

	mul.lo.s64 	%rd123, %rd71, %rd26;
	add.s64 	%rd110, %rd56, %rd123;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd110],%fd124; }

	// end inline asm
	add.s64 	%rd111, %rd110, 8;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd111],%fd125; }

	// end inline asm
	add.s64 	%rd112, %rd110, 16;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd112],%fd126; }

	// end inline asm
	add.s64 	%rd113, %rd110, 24;
	// begin inline asm
	{ atom.add.f64 %fd445,[%rd113],%fd127; }

	// end inline asm
	add.s64 	%rd114, %rd110, 32;
	// begin inline asm
	{ atom.add.f64 %fd447,[%rd114],%fd128; }

	// end inline asm
	add.s64 	%rd115, %rd110, 40;
	// begin inline asm
	{ atom.add.f64 %fd449,[%rd115],%fd129; }

	// end inline asm
	add.s64 	%rd116, %rd110, 48;
	// begin inline asm
	{ atom.add.f64 %fd451,[%rd116],%fd130; }

	// end inline asm
	add.s64 	%rd117, %rd110, 56;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd117],%fd131; }

	// end inline asm
	add.s64 	%rd118, %rd110, 64;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd118],%fd132; }

	// end inline asm
	add.s64 	%rd119, %rd110, 72;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd119],%fd133; }

	// end inline asm
	add.s64 	%rd120, %rd110, 80;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd120],%fd134; }

	// end inline asm
	add.s64 	%rd121, %rd110, 88;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd121],%fd135; }

	// end inline asm
	bra.uni 	$L__BB19_35;

$L__BB19_33:
	setp.eq.s64 	%p24, %rd49, 0;
	@%p24 bra 	$L__BB19_35;

	mul.lo.s64 	%rd137, %rd71, %rd27;
	add.s64 	%rd124, %rd49, %rd137;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd124],%fd124; }

	// end inline asm
	add.s64 	%rd125, %rd124, 8;
	// begin inline asm
	{ atom.add.f64 %fd465,[%rd125],%fd125; }

	// end inline asm
	add.s64 	%rd126, %rd124, 16;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd126],%fd126; }

	// end inline asm
	add.s64 	%rd127, %rd124, 24;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd127],%fd127; }

	// end inline asm
	add.s64 	%rd128, %rd124, 32;
	// begin inline asm
	{ atom.add.f64 %fd471,[%rd128],%fd128; }

	// end inline asm
	add.s64 	%rd129, %rd124, 40;
	// begin inline asm
	{ atom.add.f64 %fd473,[%rd129],%fd129; }

	// end inline asm
	add.s64 	%rd130, %rd124, 48;
	// begin inline asm
	{ atom.add.f64 %fd475,[%rd130],%fd130; }

	// end inline asm
	add.s64 	%rd131, %rd124, 56;
	// begin inline asm
	{ atom.add.f64 %fd477,[%rd131],%fd131; }

	// end inline asm
	add.s64 	%rd132, %rd124, 64;
	// begin inline asm
	{ atom.add.f64 %fd479,[%rd132],%fd132; }

	// end inline asm
	add.s64 	%rd133, %rd124, 72;
	// begin inline asm
	{ atom.add.f64 %fd481,[%rd133],%fd133; }

	// end inline asm
	add.s64 	%rd134, %rd124, 80;
	// begin inline asm
	{ atom.add.f64 %fd483,[%rd134],%fd134; }

	// end inline asm
	add.s64 	%rd135, %rd124, 88;
	// begin inline asm
	{ atom.add.f64 %fd485,[%rd135],%fd135; }

	// end inline asm

$L__BB19_35:
	setp.eq.s64 	%p41, %rd54, 0;
	div.rn.f64 	%fd487, %fd112, %fd256;
	mov.f64 	%fd488, 0d0000000000000000;
	div.rn.f64 	%fd489, %fd113, %fd256;
	div.rn.f64 	%fd490, %fd114, %fd256;
	add.f64 	%fd138, %fd490, 0d0000000000000000;
	div.rn.f64 	%fd491, %fd115, %fd256;
	add.f64 	%fd139, %fd491, 0d0000000000000000;
	div.rn.f64 	%fd492, %fd116, %fd256;
	add.f64 	%fd140, %fd492, 0d0000000000000000;
	div.rn.f64 	%fd493, %fd117, %fd256;
	add.f64 	%fd141, %fd493, 0d0000000000000000;
	div.rn.f64 	%fd494, %fd118, %fd256;
	add.f64 	%fd142, %fd494, 0d0000000000000000;
	div.rn.f64 	%fd495, %fd119, %fd256;
	add.f64 	%fd143, %fd495, 0d0000000000000000;
	div.rn.f64 	%fd496, %fd120, %fd256;
	add.f64 	%fd144, %fd496, 0d0000000000000000;
	div.rn.f64 	%fd497, %fd121, %fd256;
	add.f64 	%fd145, %fd497, 0d0000000000000000;
	div.rn.f64 	%fd498, %fd122, %fd256;
	add.f64 	%fd146, %fd498, 0d0000000000000000;
	div.rn.f64 	%fd499, %fd123, %fd256;
	add.f64 	%fd147, %fd499, 0d0000000000000000;
	sub.f64 	%fd150, %fd488, %fd138;
	sub.f64 	%fd151, %fd488, %fd139;
	sub.f64 	%fd152, %fd488, %fd140;
	sub.f64 	%fd153, %fd488, %fd141;
	sub.f64 	%fd154, %fd488, %fd142;
	sub.f64 	%fd155, %fd488, %fd143;
	sub.f64 	%fd156, %fd488, %fd144;
	sub.f64 	%fd157, %fd488, %fd145;
	sub.f64 	%fd158, %fd488, %fd146;
	sub.f64 	%fd159, %fd488, %fd147;
	@%p41 bra 	$L__BB19_37;

	mov.f64 	%fd829, 0d0000000000000000;
	add.f64 	%fd828, %fd489, 0d0000000000000000;
	sub.f64 	%fd827, %fd829, %fd828;
	add.f64 	%fd826, %fd487, 0d0000000000000000;
	sub.f64 	%fd825, %fd829, %fd826;
	add.s64 	%rd138, %rd54, %rd72;
	// begin inline asm
	{ atom.add.f64 %fd500,[%rd138],%fd825; }

	// end inline asm
	add.s64 	%rd139, %rd138, 8;
	// begin inline asm
	{ atom.add.f64 %fd502,[%rd139],%fd827; }

	// end inline asm
	add.s64 	%rd140, %rd138, 16;
	// begin inline asm
	{ atom.add.f64 %fd504,[%rd140],%fd150; }

	// end inline asm
	add.s64 	%rd141, %rd138, 24;
	// begin inline asm
	{ atom.add.f64 %fd506,[%rd141],%fd151; }

	// end inline asm
	add.s64 	%rd142, %rd138, 32;
	// begin inline asm
	{ atom.add.f64 %fd508,[%rd142],%fd152; }

	// end inline asm
	add.s64 	%rd143, %rd138, 40;
	// begin inline asm
	{ atom.add.f64 %fd510,[%rd143],%fd153; }

	// end inline asm
	add.s64 	%rd144, %rd138, 48;
	// begin inline asm
	{ atom.add.f64 %fd512,[%rd144],%fd154; }

	// end inline asm
	add.s64 	%rd145, %rd138, 56;
	// begin inline asm
	{ atom.add.f64 %fd514,[%rd145],%fd155; }

	// end inline asm
	add.s64 	%rd146, %rd138, 64;
	// begin inline asm
	{ atom.add.f64 %fd516,[%rd146],%fd156; }

	// end inline asm
	add.s64 	%rd147, %rd138, 72;
	// begin inline asm
	{ atom.add.f64 %fd518,[%rd147],%fd157; }

	// end inline asm
	add.s64 	%rd148, %rd138, 80;
	// begin inline asm
	{ atom.add.f64 %fd520,[%rd148],%fd158; }

	// end inline asm
	add.s64 	%rd149, %rd138, 88;
	// begin inline asm
	{ atom.add.f64 %fd522,[%rd149],%fd159; }

	// end inline asm
	bra.uni 	$L__BB19_39;

$L__BB19_37:
	setp.eq.s64 	%p26, %rd47, 0;
	@%p26 bra 	$L__BB19_39;

	mov.f64 	%fd858, 0d0000000000000000;
	add.f64 	%fd857, %fd489, 0d0000000000000000;
	sub.f64 	%fd856, %fd858, %fd857;
	add.f64 	%fd855, %fd487, 0d0000000000000000;
	sub.f64 	%fd854, %fd858, %fd855;
	add.s64 	%rd152, %rd47, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd524,[%rd152],%fd854; }

	// end inline asm
	add.s64 	%rd153, %rd152, 8;
	// begin inline asm
	{ atom.add.f64 %fd526,[%rd153],%fd856; }

	// end inline asm
	add.s64 	%rd154, %rd152, 16;
	// begin inline asm
	{ atom.add.f64 %fd528,[%rd154],%fd150; }

	// end inline asm
	add.s64 	%rd155, %rd152, 24;
	// begin inline asm
	{ atom.add.f64 %fd530,[%rd155],%fd151; }

	// end inline asm
	add.s64 	%rd156, %rd152, 32;
	// begin inline asm
	{ atom.add.f64 %fd532,[%rd156],%fd152; }

	// end inline asm
	add.s64 	%rd157, %rd152, 40;
	// begin inline asm
	{ atom.add.f64 %fd534,[%rd157],%fd153; }

	// end inline asm
	add.s64 	%rd158, %rd152, 48;
	// begin inline asm
	{ atom.add.f64 %fd536,[%rd158],%fd154; }

	// end inline asm
	add.s64 	%rd159, %rd152, 56;
	// begin inline asm
	{ atom.add.f64 %fd538,[%rd159],%fd155; }

	// end inline asm
	add.s64 	%rd160, %rd152, 64;
	// begin inline asm
	{ atom.add.f64 %fd540,[%rd160],%fd156; }

	// end inline asm
	add.s64 	%rd161, %rd152, 72;
	// begin inline asm
	{ atom.add.f64 %fd542,[%rd161],%fd157; }

	// end inline asm
	add.s64 	%rd162, %rd152, 80;
	// begin inline asm
	{ atom.add.f64 %fd544,[%rd162],%fd158; }

	// end inline asm
	add.s64 	%rd163, %rd152, 88;
	// begin inline asm
	{ atom.add.f64 %fd546,[%rd163],%fd159; }

	// end inline asm

$L__BB19_39:
	setp.eq.s64 	%p42, %rd58, 0;
	@%p42 bra 	$L__BB19_41;

	add.f64 	%fd841, %fd499, 0d0000000000000000;
	add.f64 	%fd840, %fd498, 0d0000000000000000;
	add.f64 	%fd839, %fd497, 0d0000000000000000;
	add.f64 	%fd838, %fd496, 0d0000000000000000;
	add.f64 	%fd837, %fd495, 0d0000000000000000;
	add.f64 	%fd836, %fd494, 0d0000000000000000;
	add.f64 	%fd835, %fd493, 0d0000000000000000;
	add.f64 	%fd834, %fd492, 0d0000000000000000;
	add.f64 	%fd833, %fd491, 0d0000000000000000;
	add.f64 	%fd832, %fd490, 0d0000000000000000;
	add.f64 	%fd831, %fd489, 0d0000000000000000;
	add.f64 	%fd830, %fd487, 0d0000000000000000;
	mul.lo.s64 	%rd179, %rd71, %rd25;
	add.s64 	%rd166, %rd58, %rd179;
	// begin inline asm
	{ atom.add.f64 %fd548,[%rd166],%fd830; }

	// end inline asm
	add.s64 	%rd167, %rd166, 8;
	// begin inline asm
	{ atom.add.f64 %fd550,[%rd167],%fd831; }

	// end inline asm
	add.s64 	%rd168, %rd166, 16;
	// begin inline asm
	{ atom.add.f64 %fd552,[%rd168],%fd832; }

	// end inline asm
	add.s64 	%rd169, %rd166, 24;
	// begin inline asm
	{ atom.add.f64 %fd554,[%rd169],%fd833; }

	// end inline asm
	add.s64 	%rd170, %rd166, 32;
	// begin inline asm
	{ atom.add.f64 %fd556,[%rd170],%fd834; }

	// end inline asm
	add.s64 	%rd171, %rd166, 40;
	// begin inline asm
	{ atom.add.f64 %fd558,[%rd171],%fd835; }

	// end inline asm
	add.s64 	%rd172, %rd166, 48;
	// begin inline asm
	{ atom.add.f64 %fd560,[%rd172],%fd836; }

	// end inline asm
	add.s64 	%rd173, %rd166, 56;
	// begin inline asm
	{ atom.add.f64 %fd562,[%rd173],%fd837; }

	// end inline asm
	add.s64 	%rd174, %rd166, 64;
	// begin inline asm
	{ atom.add.f64 %fd564,[%rd174],%fd838; }

	// end inline asm
	add.s64 	%rd175, %rd166, 72;
	// begin inline asm
	{ atom.add.f64 %fd566,[%rd175],%fd839; }

	// end inline asm
	add.s64 	%rd176, %rd166, 80;
	// begin inline asm
	{ atom.add.f64 %fd568,[%rd176],%fd840; }

	// end inline asm
	add.s64 	%rd177, %rd166, 88;
	// begin inline asm
	{ atom.add.f64 %fd570,[%rd177],%fd841; }

	// end inline asm
	bra.uni 	$L__BB19_43;

$L__BB19_41:
	setp.eq.s64 	%p28, %rd51, 0;
	@%p28 bra 	$L__BB19_43;

	add.f64 	%fd853, %fd499, 0d0000000000000000;
	add.f64 	%fd852, %fd498, 0d0000000000000000;
	add.f64 	%fd851, %fd497, 0d0000000000000000;
	add.f64 	%fd850, %fd496, 0d0000000000000000;
	add.f64 	%fd849, %fd495, 0d0000000000000000;
	add.f64 	%fd848, %fd494, 0d0000000000000000;
	add.f64 	%fd847, %fd493, 0d0000000000000000;
	add.f64 	%fd846, %fd492, 0d0000000000000000;
	add.f64 	%fd845, %fd491, 0d0000000000000000;
	add.f64 	%fd844, %fd490, 0d0000000000000000;
	add.f64 	%fd843, %fd489, 0d0000000000000000;
	add.f64 	%fd842, %fd487, 0d0000000000000000;
	mul.lo.s64 	%rd193, %rd71, %rd21;
	add.s64 	%rd180, %rd51, %rd193;
	// begin inline asm
	{ atom.add.f64 %fd572,[%rd180],%fd842; }

	// end inline asm
	add.s64 	%rd181, %rd180, 8;
	// begin inline asm
	{ atom.add.f64 %fd574,[%rd181],%fd843; }

	// end inline asm
	add.s64 	%rd182, %rd180, 16;
	// begin inline asm
	{ atom.add.f64 %fd576,[%rd182],%fd844; }

	// end inline asm
	add.s64 	%rd183, %rd180, 24;
	// begin inline asm
	{ atom.add.f64 %fd578,[%rd183],%fd845; }

	// end inline asm
	add.s64 	%rd184, %rd180, 32;
	// begin inline asm
	{ atom.add.f64 %fd580,[%rd184],%fd846; }

	// end inline asm
	add.s64 	%rd185, %rd180, 40;
	// begin inline asm
	{ atom.add.f64 %fd582,[%rd185],%fd847; }

	// end inline asm
	add.s64 	%rd186, %rd180, 48;
	// begin inline asm
	{ atom.add.f64 %fd584,[%rd186],%fd848; }

	// end inline asm
	add.s64 	%rd187, %rd180, 56;
	// begin inline asm
	{ atom.add.f64 %fd586,[%rd187],%fd849; }

	// end inline asm
	add.s64 	%rd188, %rd180, 64;
	// begin inline asm
	{ atom.add.f64 %fd588,[%rd188],%fd850; }

	// end inline asm
	add.s64 	%rd189, %rd180, 72;
	// begin inline asm
	{ atom.add.f64 %fd590,[%rd189],%fd851; }

	// end inline asm
	add.s64 	%rd190, %rd180, 80;
	// begin inline asm
	{ atom.add.f64 %fd592,[%rd190],%fd852; }

	// end inline asm
	add.s64 	%rd191, %rd180, 88;
	// begin inline asm
	{ atom.add.f64 %fd594,[%rd191],%fd853; }

	// end inline asm

$L__BB19_43:
	@%p1 bra 	$L__BB19_64;

	setp.eq.s64 	%p30, %rd54, 0;
	@%p30 bra 	$L__BB19_46;

	ld.global.f64 	%fd596, [%rd40];
	add.f64 	%fd941, %fd596, 0d0000000000000000;
	ld.global.f64 	%fd597, [%rd40+8];
	add.f64 	%fd940, %fd597, 0d0000000000000000;
	ld.global.f64 	%fd598, [%rd40+16];
	add.f64 	%fd939, %fd598, 0d0000000000000000;
	ld.global.f64 	%fd599, [%rd40+24];
	add.f64 	%fd938, %fd599, 0d0000000000000000;
	ld.global.f64 	%fd600, [%rd40+32];
	add.f64 	%fd937, %fd600, 0d0000000000000000;
	ld.global.f64 	%fd601, [%rd40+40];
	add.f64 	%fd936, %fd601, 0d0000000000000000;
	ld.global.f64 	%fd602, [%rd40+48];
	add.f64 	%fd935, %fd602, 0d0000000000000000;
	ld.global.f64 	%fd603, [%rd40+56];
	add.f64 	%fd934, %fd603, 0d0000000000000000;
	ld.global.f64 	%fd604, [%rd40+64];
	add.f64 	%fd933, %fd604, 0d0000000000000000;
	ld.global.f64 	%fd605, [%rd40+72];
	add.f64 	%fd932, %fd605, 0d0000000000000000;
	ld.global.f64 	%fd606, [%rd40+80];
	add.f64 	%fd931, %fd606, 0d0000000000000000;
	ld.global.f64 	%fd607, [%rd40+88];
	add.f64 	%fd930, %fd607, 0d0000000000000000;
	bra.uni 	$L__BB19_48;

$L__BB19_46:
	setp.eq.s64 	%p31, %rd47, 0;
	mov.f64 	%fd930, 0d0000000000000000;
	mov.f64 	%fd931, %fd930;
	mov.f64 	%fd932, %fd930;
	mov.f64 	%fd933, %fd930;
	mov.f64 	%fd934, %fd930;
	mov.f64 	%fd935, %fd930;
	mov.f64 	%fd936, %fd930;
	mov.f64 	%fd937, %fd930;
	mov.f64 	%fd938, %fd930;
	mov.f64 	%fd939, %fd930;
	mov.f64 	%fd940, %fd930;
	mov.f64 	%fd941, %fd930;
	@%p31 bra 	$L__BB19_48;

	ld.global.f64 	%fd620, [%rd41];
	add.f64 	%fd941, %fd620, 0d0000000000000000;
	ld.global.f64 	%fd621, [%rd41+8];
	add.f64 	%fd940, %fd621, 0d0000000000000000;
	ld.global.f64 	%fd622, [%rd41+16];
	add.f64 	%fd939, %fd622, 0d0000000000000000;
	ld.global.f64 	%fd623, [%rd41+24];
	add.f64 	%fd938, %fd623, 0d0000000000000000;
	ld.global.f64 	%fd624, [%rd41+32];
	add.f64 	%fd937, %fd624, 0d0000000000000000;
	ld.global.f64 	%fd625, [%rd41+40];
	add.f64 	%fd936, %fd625, 0d0000000000000000;
	ld.global.f64 	%fd626, [%rd41+48];
	add.f64 	%fd935, %fd626, 0d0000000000000000;
	ld.global.f64 	%fd627, [%rd41+56];
	add.f64 	%fd934, %fd627, 0d0000000000000000;
	ld.global.f64 	%fd628, [%rd41+64];
	add.f64 	%fd933, %fd628, 0d0000000000000000;
	ld.global.f64 	%fd629, [%rd41+72];
	add.f64 	%fd932, %fd629, 0d0000000000000000;
	ld.global.f64 	%fd630, [%rd41+80];
	add.f64 	%fd931, %fd630, 0d0000000000000000;
	ld.global.f64 	%fd631, [%rd41+88];
	add.f64 	%fd930, %fd631, 0d0000000000000000;

$L__BB19_48:
	setp.eq.s64 	%p32, %rd58, 0;
	@%p32 bra 	$L__BB19_50;

	mul.lo.s64 	%rd207, %rd71, %rd25;
	add.s64 	%rd194, %rd58, %rd207;
	// begin inline asm
	{ atom.add.f64 %fd632,[%rd194],%fd941; }

	// end inline asm
	add.s64 	%rd195, %rd194, 8;
	// begin inline asm
	{ atom.add.f64 %fd634,[%rd195],%fd940; }

	// end inline asm
	add.s64 	%rd196, %rd194, 16;
	// begin inline asm
	{ atom.add.f64 %fd636,[%rd196],%fd939; }

	// end inline asm
	add.s64 	%rd197, %rd194, 24;
	// begin inline asm
	{ atom.add.f64 %fd638,[%rd197],%fd938; }

	// end inline asm
	add.s64 	%rd198, %rd194, 32;
	// begin inline asm
	{ atom.add.f64 %fd640,[%rd198],%fd937; }

	// end inline asm
	add.s64 	%rd199, %rd194, 40;
	// begin inline asm
	{ atom.add.f64 %fd642,[%rd199],%fd936; }

	// end inline asm
	add.s64 	%rd200, %rd194, 48;
	// begin inline asm
	{ atom.add.f64 %fd644,[%rd200],%fd935; }

	// end inline asm
	add.s64 	%rd201, %rd194, 56;
	// begin inline asm
	{ atom.add.f64 %fd646,[%rd201],%fd934; }

	// end inline asm
	add.s64 	%rd202, %rd194, 64;
	// begin inline asm
	{ atom.add.f64 %fd648,[%rd202],%fd933; }

	// end inline asm
	add.s64 	%rd203, %rd194, 72;
	// begin inline asm
	{ atom.add.f64 %fd650,[%rd203],%fd932; }

	// end inline asm
	add.s64 	%rd204, %rd194, 80;
	// begin inline asm
	{ atom.add.f64 %fd652,[%rd204],%fd931; }

	// end inline asm
	add.s64 	%rd205, %rd194, 88;
	// begin inline asm
	{ atom.add.f64 %fd654,[%rd205],%fd930; }

	// end inline asm
	bra.uni 	$L__BB19_52;

$L__BB19_50:
	setp.eq.s64 	%p33, %rd51, 0;
	@%p33 bra 	$L__BB19_52;

	mul.lo.s64 	%rd221, %rd71, %rd21;
	add.s64 	%rd208, %rd51, %rd221;
	// begin inline asm
	{ atom.add.f64 %fd656,[%rd208],%fd941; }

	// end inline asm
	add.s64 	%rd209, %rd208, 8;
	// begin inline asm
	{ atom.add.f64 %fd658,[%rd209],%fd940; }

	// end inline asm
	add.s64 	%rd210, %rd208, 16;
	// begin inline asm
	{ atom.add.f64 %fd660,[%rd210],%fd939; }

	// end inline asm
	add.s64 	%rd211, %rd208, 24;
	// begin inline asm
	{ atom.add.f64 %fd662,[%rd211],%fd938; }

	// end inline asm
	add.s64 	%rd212, %rd208, 32;
	// begin inline asm
	{ atom.add.f64 %fd664,[%rd212],%fd937; }

	// end inline asm
	add.s64 	%rd213, %rd208, 40;
	// begin inline asm
	{ atom.add.f64 %fd666,[%rd213],%fd936; }

	// end inline asm
	add.s64 	%rd214, %rd208, 48;
	// begin inline asm
	{ atom.add.f64 %fd668,[%rd214],%fd935; }

	// end inline asm
	add.s64 	%rd215, %rd208, 56;
	// begin inline asm
	{ atom.add.f64 %fd670,[%rd215],%fd934; }

	// end inline asm
	add.s64 	%rd216, %rd208, 64;
	// begin inline asm
	{ atom.add.f64 %fd672,[%rd216],%fd933; }

	// end inline asm
	add.s64 	%rd217, %rd208, 72;
	// begin inline asm
	{ atom.add.f64 %fd674,[%rd217],%fd932; }

	// end inline asm
	add.s64 	%rd218, %rd208, 80;
	// begin inline asm
	{ atom.add.f64 %fd676,[%rd218],%fd931; }

	// end inline asm
	add.s64 	%rd219, %rd208, 88;
	// begin inline asm
	{ atom.add.f64 %fd678,[%rd219],%fd930; }

	// end inline asm

$L__BB19_52:
	setp.eq.s64 	%p34, %rd60, 0;
	@%p34 bra 	$L__BB19_54;

	ld.global.f64 	%fd680, [%rd42];
	add.f64 	%fd953, %fd680, 0d0000000000000000;
	ld.global.f64 	%fd681, [%rd42+8];
	add.f64 	%fd952, %fd681, 0d0000000000000000;
	ld.global.f64 	%fd682, [%rd42+16];
	add.f64 	%fd951, %fd682, 0d0000000000000000;
	ld.global.f64 	%fd683, [%rd42+24];
	add.f64 	%fd950, %fd683, 0d0000000000000000;
	ld.global.f64 	%fd684, [%rd42+32];
	add.f64 	%fd949, %fd684, 0d0000000000000000;
	ld.global.f64 	%fd685, [%rd42+40];
	add.f64 	%fd948, %fd685, 0d0000000000000000;
	ld.global.f64 	%fd686, [%rd42+48];
	add.f64 	%fd947, %fd686, 0d0000000000000000;
	ld.global.f64 	%fd687, [%rd42+56];
	add.f64 	%fd946, %fd687, 0d0000000000000000;
	ld.global.f64 	%fd688, [%rd42+64];
	add.f64 	%fd945, %fd688, 0d0000000000000000;
	ld.global.f64 	%fd689, [%rd42+72];
	add.f64 	%fd944, %fd689, 0d0000000000000000;
	ld.global.f64 	%fd690, [%rd42+80];
	add.f64 	%fd943, %fd690, 0d0000000000000000;
	ld.global.f64 	%fd691, [%rd42+88];
	add.f64 	%fd942, %fd691, 0d0000000000000000;
	bra.uni 	$L__BB19_56;

$L__BB19_54:
	setp.eq.s64 	%p35, %rd53, 0;
	mov.f64 	%fd942, 0d0000000000000000;
	mov.f64 	%fd943, %fd942;
	mov.f64 	%fd944, %fd942;
	mov.f64 	%fd945, %fd942;
	mov.f64 	%fd946, %fd942;
	mov.f64 	%fd947, %fd942;
	mov.f64 	%fd948, %fd942;
	mov.f64 	%fd949, %fd942;
	mov.f64 	%fd950, %fd942;
	mov.f64 	%fd951, %fd942;
	mov.f64 	%fd952, %fd942;
	mov.f64 	%fd953, %fd942;
	@%p35 bra 	$L__BB19_56;

	ld.global.f64 	%fd704, [%rd43];
	add.f64 	%fd953, %fd704, 0d0000000000000000;
	ld.global.f64 	%fd705, [%rd43+8];
	add.f64 	%fd952, %fd705, 0d0000000000000000;
	ld.global.f64 	%fd706, [%rd43+16];
	add.f64 	%fd951, %fd706, 0d0000000000000000;
	ld.global.f64 	%fd707, [%rd43+24];
	add.f64 	%fd950, %fd707, 0d0000000000000000;
	ld.global.f64 	%fd708, [%rd43+32];
	add.f64 	%fd949, %fd708, 0d0000000000000000;
	ld.global.f64 	%fd709, [%rd43+40];
	add.f64 	%fd948, %fd709, 0d0000000000000000;
	ld.global.f64 	%fd710, [%rd43+48];
	add.f64 	%fd947, %fd710, 0d0000000000000000;
	ld.global.f64 	%fd711, [%rd43+56];
	add.f64 	%fd946, %fd711, 0d0000000000000000;
	ld.global.f64 	%fd712, [%rd43+64];
	add.f64 	%fd945, %fd712, 0d0000000000000000;
	ld.global.f64 	%fd713, [%rd43+72];
	add.f64 	%fd944, %fd713, 0d0000000000000000;
	ld.global.f64 	%fd714, [%rd43+80];
	add.f64 	%fd943, %fd714, 0d0000000000000000;
	ld.global.f64 	%fd715, [%rd43+88];
	add.f64 	%fd942, %fd715, 0d0000000000000000;

$L__BB19_56:
	div.rn.f64 	%fd716, %fd953, %fd893;
	mov.f64 	%fd717, 0d0000000000000000;
	div.rn.f64 	%fd718, %fd952, %fd893;
	div.rn.f64 	%fd719, %fd951, %fd893;
	add.f64 	%fd234, %fd719, 0d0000000000000000;
	div.rn.f64 	%fd720, %fd950, %fd893;
	add.f64 	%fd235, %fd720, 0d0000000000000000;
	div.rn.f64 	%fd721, %fd949, %fd893;
	add.f64 	%fd236, %fd721, 0d0000000000000000;
	div.rn.f64 	%fd722, %fd948, %fd893;
	add.f64 	%fd237, %fd722, 0d0000000000000000;
	div.rn.f64 	%fd723, %fd947, %fd893;
	add.f64 	%fd238, %fd723, 0d0000000000000000;
	div.rn.f64 	%fd724, %fd946, %fd893;
	add.f64 	%fd239, %fd724, 0d0000000000000000;
	div.rn.f64 	%fd725, %fd945, %fd893;
	add.f64 	%fd240, %fd725, 0d0000000000000000;
	div.rn.f64 	%fd726, %fd944, %fd893;
	add.f64 	%fd241, %fd726, 0d0000000000000000;
	div.rn.f64 	%fd727, %fd943, %fd893;
	add.f64 	%fd242, %fd727, 0d0000000000000000;
	div.rn.f64 	%fd728, %fd942, %fd893;
	add.f64 	%fd243, %fd728, 0d0000000000000000;
	sub.f64 	%fd246, %fd717, %fd234;
	sub.f64 	%fd247, %fd717, %fd235;
	sub.f64 	%fd248, %fd717, %fd236;
	sub.f64 	%fd249, %fd717, %fd237;
	sub.f64 	%fd250, %fd717, %fd238;
	sub.f64 	%fd251, %fd717, %fd239;
	sub.f64 	%fd252, %fd717, %fd240;
	sub.f64 	%fd253, %fd717, %fd241;
	sub.f64 	%fd254, %fd717, %fd242;
	sub.f64 	%fd255, %fd717, %fd243;
	@%p30 bra 	$L__BB19_58;

	mov.f64 	%fd863, 0d0000000000000000;
	add.f64 	%fd862, %fd718, 0d0000000000000000;
	sub.f64 	%fd861, %fd863, %fd862;
	add.f64 	%fd860, %fd716, 0d0000000000000000;
	sub.f64 	%fd859, %fd863, %fd860;
	add.s64 	%rd222, %rd54, %rd72;
	// begin inline asm
	{ atom.add.f64 %fd729,[%rd222],%fd859; }

	// end inline asm
	add.s64 	%rd223, %rd222, 8;
	// begin inline asm
	{ atom.add.f64 %fd731,[%rd223],%fd861; }

	// end inline asm
	add.s64 	%rd224, %rd222, 16;
	// begin inline asm
	{ atom.add.f64 %fd733,[%rd224],%fd246; }

	// end inline asm
	add.s64 	%rd225, %rd222, 24;
	// begin inline asm
	{ atom.add.f64 %fd735,[%rd225],%fd247; }

	// end inline asm
	add.s64 	%rd226, %rd222, 32;
	// begin inline asm
	{ atom.add.f64 %fd737,[%rd226],%fd248; }

	// end inline asm
	add.s64 	%rd227, %rd222, 40;
	// begin inline asm
	{ atom.add.f64 %fd739,[%rd227],%fd249; }

	// end inline asm
	add.s64 	%rd228, %rd222, 48;
	// begin inline asm
	{ atom.add.f64 %fd741,[%rd228],%fd250; }

	// end inline asm
	add.s64 	%rd229, %rd222, 56;
	// begin inline asm
	{ atom.add.f64 %fd743,[%rd229],%fd251; }

	// end inline asm
	add.s64 	%rd230, %rd222, 64;
	// begin inline asm
	{ atom.add.f64 %fd745,[%rd230],%fd252; }

	// end inline asm
	add.s64 	%rd231, %rd222, 72;
	// begin inline asm
	{ atom.add.f64 %fd747,[%rd231],%fd253; }

	// end inline asm
	add.s64 	%rd232, %rd222, 80;
	// begin inline asm
	{ atom.add.f64 %fd749,[%rd232],%fd254; }

	// end inline asm
	add.s64 	%rd233, %rd222, 88;
	// begin inline asm
	{ atom.add.f64 %fd751,[%rd233],%fd255; }

	// end inline asm
	bra.uni 	$L__BB19_60;

$L__BB19_58:
	setp.eq.s64 	%p37, %rd47, 0;
	@%p37 bra 	$L__BB19_60;

	mov.f64 	%fd892, 0d0000000000000000;
	add.f64 	%fd891, %fd718, 0d0000000000000000;
	sub.f64 	%fd890, %fd892, %fd891;
	add.f64 	%fd889, %fd716, 0d0000000000000000;
	sub.f64 	%fd888, %fd892, %fd889;
	add.s64 	%rd236, %rd47, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd753,[%rd236],%fd888; }

	// end inline asm
	add.s64 	%rd237, %rd236, 8;
	// begin inline asm
	{ atom.add.f64 %fd755,[%rd237],%fd890; }

	// end inline asm
	add.s64 	%rd238, %rd236, 16;
	// begin inline asm
	{ atom.add.f64 %fd757,[%rd238],%fd246; }

	// end inline asm
	add.s64 	%rd239, %rd236, 24;
	// begin inline asm
	{ atom.add.f64 %fd759,[%rd239],%fd247; }

	// end inline asm
	add.s64 	%rd240, %rd236, 32;
	// begin inline asm
	{ atom.add.f64 %fd761,[%rd240],%fd248; }

	// end inline asm
	add.s64 	%rd241, %rd236, 40;
	// begin inline asm
	{ atom.add.f64 %fd763,[%rd241],%fd249; }

	// end inline asm
	add.s64 	%rd242, %rd236, 48;
	// begin inline asm
	{ atom.add.f64 %fd765,[%rd242],%fd250; }

	// end inline asm
	add.s64 	%rd243, %rd236, 56;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd243],%fd251; }

	// end inline asm
	add.s64 	%rd244, %rd236, 64;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd244],%fd252; }

	// end inline asm
	add.s64 	%rd245, %rd236, 72;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd245],%fd253; }

	// end inline asm
	add.s64 	%rd246, %rd236, 80;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd246],%fd254; }

	// end inline asm
	add.s64 	%rd247, %rd236, 88;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd247],%fd255; }

	// end inline asm

$L__BB19_60:
	setp.eq.s64 	%p43, %rd58, 0;
	@%p43 bra 	$L__BB19_62;

	add.f64 	%fd875, %fd728, 0d0000000000000000;
	add.f64 	%fd874, %fd727, 0d0000000000000000;
	add.f64 	%fd873, %fd726, 0d0000000000000000;
	add.f64 	%fd872, %fd725, 0d0000000000000000;
	add.f64 	%fd871, %fd724, 0d0000000000000000;
	add.f64 	%fd870, %fd723, 0d0000000000000000;
	add.f64 	%fd869, %fd722, 0d0000000000000000;
	add.f64 	%fd868, %fd721, 0d0000000000000000;
	add.f64 	%fd867, %fd720, 0d0000000000000000;
	add.f64 	%fd866, %fd719, 0d0000000000000000;
	add.f64 	%fd865, %fd718, 0d0000000000000000;
	add.f64 	%fd864, %fd716, 0d0000000000000000;
	mul.lo.s64 	%rd263, %rd71, %rd25;
	add.s64 	%rd250, %rd58, %rd263;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd250],%fd864; }

	// end inline asm
	add.s64 	%rd251, %rd250, 8;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd251],%fd865; }

	// end inline asm
	add.s64 	%rd252, %rd250, 16;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd252],%fd866; }

	// end inline asm
	add.s64 	%rd253, %rd250, 24;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd253],%fd867; }

	// end inline asm
	add.s64 	%rd254, %rd250, 32;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd254],%fd868; }

	// end inline asm
	add.s64 	%rd255, %rd250, 40;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd255],%fd869; }

	// end inline asm
	add.s64 	%rd256, %rd250, 48;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd256],%fd870; }

	// end inline asm
	add.s64 	%rd257, %rd250, 56;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd257],%fd871; }

	// end inline asm
	add.s64 	%rd258, %rd250, 64;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd258],%fd872; }

	// end inline asm
	add.s64 	%rd259, %rd250, 72;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd259],%fd873; }

	// end inline asm
	add.s64 	%rd260, %rd250, 80;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd260],%fd874; }

	// end inline asm
	add.s64 	%rd261, %rd250, 88;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd261],%fd875; }

	// end inline asm
	bra.uni 	$L__BB19_64;

$L__BB19_62:
	setp.eq.s64 	%p39, %rd51, 0;
	@%p39 bra 	$L__BB19_64;

	add.f64 	%fd887, %fd728, 0d0000000000000000;
	add.f64 	%fd886, %fd727, 0d0000000000000000;
	add.f64 	%fd885, %fd726, 0d0000000000000000;
	add.f64 	%fd884, %fd725, 0d0000000000000000;
	add.f64 	%fd883, %fd724, 0d0000000000000000;
	add.f64 	%fd882, %fd723, 0d0000000000000000;
	add.f64 	%fd881, %fd722, 0d0000000000000000;
	add.f64 	%fd880, %fd721, 0d0000000000000000;
	add.f64 	%fd879, %fd720, 0d0000000000000000;
	add.f64 	%fd878, %fd719, 0d0000000000000000;
	add.f64 	%fd877, %fd718, 0d0000000000000000;
	add.f64 	%fd876, %fd716, 0d0000000000000000;
	mul.lo.s64 	%rd277, %rd71, %rd21;
	add.s64 	%rd264, %rd51, %rd277;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd264],%fd876; }

	// end inline asm
	add.s64 	%rd265, %rd264, 8;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd265],%fd877; }

	// end inline asm
	add.s64 	%rd266, %rd264, 16;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd266],%fd878; }

	// end inline asm
	add.s64 	%rd267, %rd264, 24;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd267],%fd879; }

	// end inline asm
	add.s64 	%rd268, %rd264, 32;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd268],%fd880; }

	// end inline asm
	add.s64 	%rd269, %rd264, 40;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd269],%fd881; }

	// end inline asm
	add.s64 	%rd270, %rd264, 48;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd270],%fd882; }

	// end inline asm
	add.s64 	%rd271, %rd264, 56;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd271],%fd883; }

	// end inline asm
	add.s64 	%rd272, %rd264, 64;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd272],%fd884; }

	// end inline asm
	add.s64 	%rd273, %rd264, 72;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd273],%fd885; }

	// end inline asm
	add.s64 	%rd274, %rd264, 80;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd274],%fd886; }

	// end inline asm
	add.s64 	%rd275, %rd264, 88;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd275],%fd887; }

	// end inline asm

$L__BB19_64:
	ld.param.u64 	%rd278, [advection_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd285, %rd285, %rd24;
	setp.lt.u64 	%p40, %rd285, %rd278;
	@%p40 bra 	$L__BB19_2;

$L__BB19_65:
	ret;

}
	// .globl	clamp_search_direction_cuda_kernel_forward
.visible .entry clamp_search_direction_cuda_kernel_forward(
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_forward_param_6[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<126>;
	.reg .f64 	%fd<114>;
	.reg .b64 	%rd<77>;


	ld.param.v2.u32 	{%r61, %r62}, [clamp_search_direction_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r63, %r64}, [clamp_search_direction_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r69, %r70}, [clamp_search_direction_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r77, %r78}, [clamp_search_direction_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r85, %r86}, [clamp_search_direction_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r93, %r94}, [clamp_search_direction_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r101, %r102}, [clamp_search_direction_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r109, %r110}, [clamp_search_direction_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd43, [clamp_search_direction_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd41, [clamp_search_direction_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd39, [clamp_search_direction_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd37, [clamp_search_direction_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd35, [clamp_search_direction_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd33, [clamp_search_direction_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd32, [clamp_search_direction_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [clamp_search_direction_cuda_kernel_forward_param_0+16];
	mov.u32 	%r113, %ntid.x;
	cvt.u64.u32 	%rd1, %r113;
	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd45, %r113, %r114;
	mov.u32 	%r115, %tid.x;
	cvt.u64.u32 	%rd46, %r115;
	add.s64 	%rd73, %rd45, %rd46;
	setp.ge.u64 	%p1, %rd73, %rd32;
	@%p1 bra 	$L__BB20_17;

	cvta.to.global.u64 	%rd4, %rd43;
	cvta.to.global.u64 	%rd5, %rd41;
	cvta.to.global.u64 	%rd6, %rd39;
	cvta.to.global.u64 	%rd7, %rd37;
	cvta.to.global.u64 	%rd8, %rd35;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r64;
	cvt.s64.s32 	%rd11, %r63;
	cvt.s64.s32 	%rd12, %r62;
	cvt.s64.s32 	%rd13, %r77;
	cvt.s64.s32 	%rd14, %r109;
	cvt.s64.s32 	%rd15, %r85;
	cvt.s64.s32 	%rd16, %r93;
	cvt.s64.s32 	%rd17, %r101;
	cvt.s64.s32 	%rd18, %r69;
	mov.u32 	%r116, %nctaid.x;
	cvt.u64.u32 	%rd47, %r116;
	mul.lo.s64 	%rd19, %rd1, %rd47;

$L__BB20_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd74, %rd73;
	@%p2 bra 	$L__BB20_6;

	or.b64  	%rd48, %rd73, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p3, %rd49, 0;
	@%p3 bra 	$L__BB20_5;

	div.u64 	%rd74, %rd73, %rd10;
	bra.uni 	$L__BB20_6;

$L__BB20_5:
	cvt.u32.u64 	%r117, %rd10;
	cvt.u32.u64 	%r118, %rd73;
	div.u32 	%r119, %r118, %r117;
	cvt.u64.u32 	%rd74, %r119;

$L__BB20_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB20_10;

	or.b64  	%rd50, %rd74, %rd11;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB20_9;

	div.u64 	%rd74, %rd74, %rd11;
	bra.uni 	$L__BB20_10;

$L__BB20_9:
	cvt.u32.u64 	%r120, %rd11;
	cvt.u32.u64 	%r121, %rd74;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd74, %r122;

$L__BB20_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB20_14;

	or.b64  	%rd52, %rd74, %rd12;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p7, %rd53, 0;
	@%p7 bra 	$L__BB20_13;

	div.u64 	%rd74, %rd74, %rd12;
	bra.uni 	$L__BB20_14;

$L__BB20_13:
	cvt.u32.u64 	%r123, %rd12;
	cvt.u32.u64 	%r124, %rd74;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd74, %r125;

$L__BB20_14:
	cvt.s64.s32 	%rd54, %rd74;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd30, %rd54, 0, %p8;
	mul.lo.s64 	%rd55, %rd30, %rd13;
	add.s64 	%rd56, %rd8, %rd55;
	ld.global.s32 	%rd57, [%rd56];
	mul.lo.s64 	%rd58, %rd57, %rd14;
	add.s64 	%rd59, %rd4, %rd58;
	ld.global.s32 	%rd60, [%rd56+4];
	mul.lo.s64 	%rd61, %rd60, %rd14;
	add.s64 	%rd62, %rd4, %rd61;
	mul.lo.s64 	%rd63, %rd30, %rd15;
	add.s64 	%rd64, %rd7, %rd63;
	ld.global.s32 	%rd65, [%rd64];
	mul.lo.s64 	%rd66, %rd65, %rd16;
	add.s64 	%rd67, %rd6, %rd66;
	mul.lo.s64 	%rd68, %rd65, %rd17;
	add.s64 	%rd69, %rd5, %rd68;
	ld.global.f64 	%fd3, [%rd69];
	ld.global.f64 	%fd4, [%rd67];
	add.f64 	%fd5, %fd4, %fd3;
	ld.global.f64 	%fd6, [%rd69+8];
	add.f64 	%fd7, %fd4, %fd6;
	ld.global.f64 	%fd8, [%rd69+16];
	add.f64 	%fd9, %fd4, %fd8;
	ld.global.f64 	%fd10, [%rd69+24];
	ld.global.f64 	%fd11, [%rd67+24];
	add.f64 	%fd12, %fd11, %fd10;
	ld.global.f64 	%fd13, [%rd69+32];
	add.f64 	%fd14, %fd11, %fd13;
	ld.global.f64 	%fd15, [%rd69+40];
	add.f64 	%fd16, %fd11, %fd15;
	ld.global.f64 	%fd17, [%rd69+48];
	ld.global.f64 	%fd18, [%rd67+48];
	add.f64 	%fd19, %fd18, %fd17;
	ld.global.f64 	%fd20, [%rd69+56];
	add.f64 	%fd21, %fd18, %fd20;
	ld.global.f64 	%fd22, [%rd69+64];
	add.f64 	%fd23, %fd18, %fd22;
	ld.global.f64 	%fd24, [%rd69+72];
	ld.global.f64 	%fd25, [%rd67+72];
	add.f64 	%fd26, %fd25, %fd24;
	ld.global.f64 	%fd27, [%rd69+80];
	add.f64 	%fd28, %fd25, %fd27;
	ld.global.f64 	%fd29, [%rd69+88];
	add.f64 	%fd30, %fd25, %fd29;
	ld.global.f64 	%fd31, [%rd59];
	mov.f64 	%fd32, 0d3FF0000000000000;
	sub.f64 	%fd33, %fd32, %fd31;
	ld.global.f64 	%fd34, [%rd59+8];
	sub.f64 	%fd35, %fd33, %fd34;
	ld.global.f64 	%fd36, [%rd59+16];
	sub.f64 	%fd37, %fd35, %fd36;
	ld.global.f64 	%fd38, [%rd67+8];
	ld.global.f64 	%fd39, [%rd67+16];
	mul.f64 	%fd40, %fd31, %fd11;
	ld.global.f64 	%fd41, [%rd67+32];
	mul.f64 	%fd42, %fd31, %fd41;
	ld.global.f64 	%fd43, [%rd67+40];
	mul.f64 	%fd44, %fd31, %fd43;
	fma.rn.f64 	%fd45, %fd37, %fd4, %fd40;
	fma.rn.f64 	%fd46, %fd37, %fd38, %fd42;
	fma.rn.f64 	%fd47, %fd37, %fd39, %fd44;
	ld.global.f64 	%fd48, [%rd67+56];
	ld.global.f64 	%fd49, [%rd67+64];
	fma.rn.f64 	%fd50, %fd34, %fd18, %fd45;
	fma.rn.f64 	%fd51, %fd34, %fd48, %fd46;
	fma.rn.f64 	%fd52, %fd34, %fd49, %fd47;
	ld.global.f64 	%fd53, [%rd67+80];
	ld.global.f64 	%fd54, [%rd67+88];
	fma.rn.f64 	%fd55, %fd36, %fd25, %fd50;
	fma.rn.f64 	%fd56, %fd36, %fd53, %fd51;
	fma.rn.f64 	%fd57, %fd36, %fd54, %fd52;
	ld.global.f64 	%fd58, [%rd62];
	sub.f64 	%fd59, %fd32, %fd58;
	ld.global.f64 	%fd60, [%rd62+8];
	sub.f64 	%fd61, %fd59, %fd60;
	ld.global.f64 	%fd62, [%rd62+16];
	sub.f64 	%fd63, %fd61, %fd62;
	mul.f64 	%fd64, %fd58, %fd11;
	mul.f64 	%fd65, %fd58, %fd41;
	mul.f64 	%fd66, %fd58, %fd43;
	fma.rn.f64 	%fd67, %fd63, %fd4, %fd64;
	fma.rn.f64 	%fd68, %fd63, %fd38, %fd65;
	fma.rn.f64 	%fd69, %fd63, %fd39, %fd66;
	fma.rn.f64 	%fd70, %fd60, %fd18, %fd67;
	fma.rn.f64 	%fd71, %fd60, %fd48, %fd68;
	fma.rn.f64 	%fd72, %fd60, %fd49, %fd69;
	fma.rn.f64 	%fd73, %fd62, %fd25, %fd70;
	fma.rn.f64 	%fd74, %fd62, %fd53, %fd71;
	fma.rn.f64 	%fd75, %fd62, %fd54, %fd72;
	sub.f64 	%fd76, %fd55, %fd73;
	sub.f64 	%fd77, %fd56, %fd74;
	sub.f64 	%fd78, %fd57, %fd75;
	mul.f64 	%fd79, %fd31, %fd12;
	mul.f64 	%fd80, %fd31, %fd14;
	mul.f64 	%fd81, %fd31, %fd16;
	fma.rn.f64 	%fd82, %fd37, %fd5, %fd79;
	fma.rn.f64 	%fd83, %fd37, %fd7, %fd80;
	fma.rn.f64 	%fd84, %fd37, %fd9, %fd81;
	fma.rn.f64 	%fd85, %fd34, %fd19, %fd82;
	fma.rn.f64 	%fd86, %fd34, %fd21, %fd83;
	fma.rn.f64 	%fd87, %fd34, %fd23, %fd84;
	fma.rn.f64 	%fd88, %fd36, %fd26, %fd85;
	fma.rn.f64 	%fd89, %fd36, %fd28, %fd86;
	fma.rn.f64 	%fd90, %fd36, %fd30, %fd87;
	mul.f64 	%fd91, %fd58, %fd12;
	mul.f64 	%fd92, %fd58, %fd14;
	mul.f64 	%fd93, %fd58, %fd16;
	fma.rn.f64 	%fd94, %fd63, %fd5, %fd91;
	fma.rn.f64 	%fd95, %fd63, %fd7, %fd92;
	fma.rn.f64 	%fd96, %fd63, %fd9, %fd93;
	fma.rn.f64 	%fd97, %fd60, %fd19, %fd94;
	fma.rn.f64 	%fd98, %fd60, %fd21, %fd95;
	fma.rn.f64 	%fd99, %fd60, %fd23, %fd96;
	fma.rn.f64 	%fd100, %fd62, %fd26, %fd97;
	fma.rn.f64 	%fd101, %fd62, %fd28, %fd98;
	fma.rn.f64 	%fd102, %fd62, %fd30, %fd99;
	sub.f64 	%fd103, %fd88, %fd100;
	sub.f64 	%fd104, %fd89, %fd101;
	sub.f64 	%fd105, %fd90, %fd102;
	mul.f64 	%fd106, %fd77, %fd77;
	fma.rn.f64 	%fd107, %fd76, %fd76, %fd106;
	fma.rn.f64 	%fd108, %fd78, %fd78, %fd107;
	sqrt.rn.f64 	%fd109, %fd108;
	mul.f64 	%fd110, %fd104, %fd104;
	fma.rn.f64 	%fd111, %fd103, %fd103, %fd110;
	fma.rn.f64 	%fd112, %fd105, %fd105, %fd111;
	sqrt.rn.f64 	%fd1, %fd112;
	add.f64 	%fd2, %fd109, %fd109;
	setp.leu.f64 	%p9, %fd1, %fd2;
	@%p9 bra 	$L__BB20_16;

	mul.lo.s64 	%rd70, %rd30, %rd18;
	add.s64 	%rd71, %rd9, %rd70;
	div.rn.f64 	%fd113, %fd2, %fd1;
	st.global.f64 	[%rd71], %fd113;

$L__BB20_16:
	ld.param.u64 	%rd72, [clamp_search_direction_cuda_kernel_forward_param_0+24];
	add.s64 	%rd73, %rd73, %rd19;
	setp.lt.u64 	%p10, %rd73, %rd72;
	@%p10 bra 	$L__BB20_2;

$L__BB20_17:
	ret;

}
	// .globl	clamp_search_direction_cuda_kernel_backward
.visible .entry clamp_search_direction_cuda_kernel_backward(
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 clamp_search_direction_cuda_kernel_backward_param_12[56]
)
{
	.reg .pred 	%p<67>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<194>;
	.reg .f64 	%fd<1544>;
	.reg .b64 	%rd<719>;


	ld.param.v2.u32 	{%r97, %r98}, [clamp_search_direction_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r99, %r100}, [clamp_search_direction_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r105, %r106}, [clamp_search_direction_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r113, %r114}, [clamp_search_direction_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r121, %r122}, [clamp_search_direction_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r129, %r130}, [clamp_search_direction_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r137, %r138}, [clamp_search_direction_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r145, %r146}, [clamp_search_direction_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r153, %r154}, [clamp_search_direction_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r161, %r162}, [clamp_search_direction_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r169, %r170}, [clamp_search_direction_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r177, %r178}, [clamp_search_direction_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd71, [clamp_search_direction_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd69, [clamp_search_direction_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd67, [clamp_search_direction_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd65, [clamp_search_direction_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd64, [clamp_search_direction_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd63, [clamp_search_direction_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd62, [clamp_search_direction_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd61, [clamp_search_direction_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd60, [clamp_search_direction_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd59, [clamp_search_direction_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd57, [clamp_search_direction_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd55, [clamp_search_direction_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd54, [clamp_search_direction_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd52, [clamp_search_direction_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [clamp_search_direction_cuda_kernel_backward_param_0+16];
	mov.u32 	%r181, %ntid.x;
	cvt.u64.u32 	%rd1, %r181;
	mov.u32 	%r182, %ctaid.x;
	mul.wide.u32 	%rd73, %r181, %r182;
	mov.u32 	%r183, %tid.x;
	cvt.u64.u32 	%rd74, %r183;
	add.s64 	%rd715, %rd73, %rd74;
	setp.ge.u64 	%p1, %rd715, %rd52;
	@%p1 bra 	$L__BB21_129;

	cvta.to.global.u64 	%rd12, %rd65;
	cvta.to.global.u64 	%rd13, %rd63;
	cvta.to.global.u64 	%rd14, %rd61;
	cvta.to.global.u64 	%rd15, %rd59;
	cvta.to.global.u64 	%rd16, %rd57;
	cvta.to.global.u64 	%rd17, %rd55;
	cvta.to.global.u64 	%rd18, %rd54;
	cvt.s64.s32 	%rd19, %r100;
	cvt.s64.s32 	%rd20, %r99;
	cvt.s64.s32 	%rd21, %r98;
	cvt.s64.s32 	%rd22, %r113;
	cvt.s64.s32 	%rd23, %r145;
	cvt.s64.s32 	%rd24, %r121;
	cvt.s64.s32 	%rd25, %r129;
	cvt.s64.s32 	%rd26, %r137;
	cvt.s64.s32 	%rd27, %r153;
	cvt.s64.s32 	%rd28, %r105;
	cvt.s64.s32 	%rd29, %r169;
	cvt.s64.s32 	%rd30, %r161;
	cvt.s64.s32 	%rd31, %r177;
	mov.u32 	%r184, %nctaid.x;
	cvt.u64.u32 	%rd75, %r184;
	mul.lo.s64 	%rd32, %rd1, %rd75;

$L__BB21_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd716, %rd715;
	@%p2 bra 	$L__BB21_6;

	or.b64  	%rd76, %rd715, %rd19;
	and.b64  	%rd77, %rd76, -4294967296;
	setp.eq.s64 	%p3, %rd77, 0;
	@%p3 bra 	$L__BB21_5;

	div.u64 	%rd716, %rd715, %rd19;
	bra.uni 	$L__BB21_6;

$L__BB21_5:
	cvt.u32.u64 	%r185, %rd19;
	cvt.u32.u64 	%r186, %rd715;
	div.u32 	%r187, %r186, %r185;
	cvt.u64.u32 	%rd716, %r187;

$L__BB21_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB21_10;

	or.b64  	%rd78, %rd716, %rd20;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p5, %rd79, 0;
	@%p5 bra 	$L__BB21_9;

	div.u64 	%rd716, %rd716, %rd20;
	bra.uni 	$L__BB21_10;

$L__BB21_9:
	cvt.u32.u64 	%r188, %rd20;
	cvt.u32.u64 	%r189, %rd716;
	div.u32 	%r190, %r189, %r188;
	cvt.u64.u32 	%rd716, %r190;

$L__BB21_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB21_14;

	or.b64  	%rd80, %rd716, %rd21;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p7, %rd81, 0;
	@%p7 bra 	$L__BB21_13;

	div.u64 	%rd716, %rd716, %rd21;
	bra.uni 	$L__BB21_14;

$L__BB21_13:
	cvt.u32.u64 	%r191, %rd21;
	cvt.u32.u64 	%r192, %rd716;
	div.u32 	%r193, %r192, %r191;
	cvt.u64.u32 	%rd716, %r193;

$L__BB21_14:
	cvt.s64.s32 	%rd710, %r145;
	cvt.s64.s32 	%rd82, %rd716;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd43, %rd82, 0, %p8;
	mul.lo.s64 	%rd83, %rd43, %rd22;
	add.s64 	%rd84, %rd17, %rd83;
	ld.global.s32 	%rd44, [%rd84];
	mul.lo.s64 	%rd45, %rd44, %rd710;
	add.s64 	%rd85, %rd13, %rd45;
	ld.global.s32 	%rd46, [%rd84+4];
	mul.lo.s64 	%rd47, %rd46, %rd710;
	add.s64 	%rd86, %rd13, %rd47;
	mul.lo.s64 	%rd87, %rd43, %rd24;
	add.s64 	%rd88, %rd16, %rd87;
	ld.global.s32 	%rd48, [%rd88];
	mul.lo.s64 	%rd49, %rd48, %rd25;
	add.s64 	%rd89, %rd15, %rd49;
	mul.lo.s64 	%rd50, %rd48, %rd26;
	add.s64 	%rd90, %rd14, %rd50;
	ld.global.f64 	%fd132, [%rd90];
	ld.global.f64 	%fd1, [%rd89];
	add.f64 	%fd2, %fd1, %fd132;
	ld.global.f64 	%fd133, [%rd90+8];
	add.f64 	%fd3, %fd1, %fd133;
	ld.global.f64 	%fd134, [%rd90+16];
	add.f64 	%fd4, %fd1, %fd134;
	ld.global.f64 	%fd135, [%rd90+24];
	ld.global.f64 	%fd5, [%rd89+24];
	add.f64 	%fd6, %fd5, %fd135;
	ld.global.f64 	%fd136, [%rd90+32];
	add.f64 	%fd7, %fd5, %fd136;
	ld.global.f64 	%fd137, [%rd90+40];
	add.f64 	%fd8, %fd5, %fd137;
	ld.global.f64 	%fd138, [%rd90+48];
	ld.global.f64 	%fd9, [%rd89+48];
	add.f64 	%fd10, %fd9, %fd138;
	ld.global.f64 	%fd139, [%rd90+56];
	add.f64 	%fd11, %fd9, %fd139;
	ld.global.f64 	%fd140, [%rd90+64];
	add.f64 	%fd12, %fd9, %fd140;
	ld.global.f64 	%fd141, [%rd90+72];
	ld.global.f64 	%fd13, [%rd89+72];
	add.f64 	%fd14, %fd13, %fd141;
	ld.global.f64 	%fd142, [%rd90+80];
	add.f64 	%fd15, %fd13, %fd142;
	ld.global.f64 	%fd143, [%rd90+88];
	add.f64 	%fd16, %fd13, %fd143;
	ld.global.f64 	%fd17, [%rd85];
	mov.f64 	%fd144, 0d3FF0000000000000;
	sub.f64 	%fd145, %fd144, %fd17;
	ld.global.f64 	%fd18, [%rd85+8];
	sub.f64 	%fd146, %fd145, %fd18;
	ld.global.f64 	%fd19, [%rd85+16];
	sub.f64 	%fd20, %fd146, %fd19;
	ld.global.f64 	%fd21, [%rd89+8];
	ld.global.f64 	%fd22, [%rd89+16];
	mul.f64 	%fd147, %fd17, %fd5;
	ld.global.f64 	%fd23, [%rd89+32];
	mul.f64 	%fd148, %fd17, %fd23;
	ld.global.f64 	%fd24, [%rd89+40];
	mul.f64 	%fd149, %fd17, %fd24;
	fma.rn.f64 	%fd150, %fd20, %fd1, %fd147;
	fma.rn.f64 	%fd151, %fd20, %fd21, %fd148;
	fma.rn.f64 	%fd152, %fd20, %fd22, %fd149;
	ld.global.f64 	%fd25, [%rd89+56];
	ld.global.f64 	%fd26, [%rd89+64];
	fma.rn.f64 	%fd153, %fd18, %fd9, %fd150;
	fma.rn.f64 	%fd154, %fd18, %fd25, %fd151;
	fma.rn.f64 	%fd155, %fd18, %fd26, %fd152;
	ld.global.f64 	%fd27, [%rd89+80];
	ld.global.f64 	%fd28, [%rd89+88];
	fma.rn.f64 	%fd156, %fd19, %fd13, %fd153;
	fma.rn.f64 	%fd157, %fd19, %fd27, %fd154;
	fma.rn.f64 	%fd158, %fd19, %fd28, %fd155;
	ld.global.f64 	%fd29, [%rd86];
	sub.f64 	%fd159, %fd144, %fd29;
	ld.global.f64 	%fd30, [%rd86+8];
	sub.f64 	%fd160, %fd159, %fd30;
	ld.global.f64 	%fd31, [%rd86+16];
	sub.f64 	%fd32, %fd160, %fd31;
	mul.f64 	%fd161, %fd29, %fd5;
	mul.f64 	%fd162, %fd29, %fd23;
	mul.f64 	%fd163, %fd29, %fd24;
	fma.rn.f64 	%fd164, %fd32, %fd1, %fd161;
	fma.rn.f64 	%fd165, %fd32, %fd21, %fd162;
	fma.rn.f64 	%fd166, %fd32, %fd22, %fd163;
	fma.rn.f64 	%fd167, %fd30, %fd9, %fd164;
	fma.rn.f64 	%fd168, %fd30, %fd25, %fd165;
	fma.rn.f64 	%fd169, %fd30, %fd26, %fd166;
	fma.rn.f64 	%fd170, %fd31, %fd13, %fd167;
	fma.rn.f64 	%fd171, %fd31, %fd27, %fd168;
	fma.rn.f64 	%fd172, %fd31, %fd28, %fd169;
	sub.f64 	%fd33, %fd156, %fd170;
	sub.f64 	%fd34, %fd157, %fd171;
	sub.f64 	%fd35, %fd158, %fd172;
	mul.f64 	%fd173, %fd17, %fd6;
	mul.f64 	%fd174, %fd17, %fd7;
	mul.f64 	%fd175, %fd17, %fd8;
	fma.rn.f64 	%fd176, %fd20, %fd2, %fd173;
	fma.rn.f64 	%fd177, %fd20, %fd3, %fd174;
	fma.rn.f64 	%fd178, %fd20, %fd4, %fd175;
	fma.rn.f64 	%fd179, %fd18, %fd10, %fd176;
	fma.rn.f64 	%fd180, %fd18, %fd11, %fd177;
	fma.rn.f64 	%fd181, %fd18, %fd12, %fd178;
	fma.rn.f64 	%fd182, %fd19, %fd14, %fd179;
	fma.rn.f64 	%fd183, %fd19, %fd15, %fd180;
	fma.rn.f64 	%fd184, %fd19, %fd16, %fd181;
	mul.f64 	%fd185, %fd29, %fd6;
	mul.f64 	%fd186, %fd29, %fd7;
	mul.f64 	%fd187, %fd29, %fd8;
	fma.rn.f64 	%fd188, %fd32, %fd2, %fd185;
	fma.rn.f64 	%fd189, %fd32, %fd3, %fd186;
	fma.rn.f64 	%fd190, %fd32, %fd4, %fd187;
	fma.rn.f64 	%fd191, %fd30, %fd10, %fd188;
	fma.rn.f64 	%fd192, %fd30, %fd11, %fd189;
	fma.rn.f64 	%fd193, %fd30, %fd12, %fd190;
	fma.rn.f64 	%fd194, %fd31, %fd14, %fd191;
	fma.rn.f64 	%fd195, %fd31, %fd15, %fd192;
	fma.rn.f64 	%fd196, %fd31, %fd16, %fd193;
	sub.f64 	%fd36, %fd182, %fd194;
	sub.f64 	%fd37, %fd183, %fd195;
	sub.f64 	%fd38, %fd184, %fd196;
	mul.f64 	%fd197, %fd34, %fd34;
	fma.rn.f64 	%fd198, %fd33, %fd33, %fd197;
	fma.rn.f64 	%fd199, %fd35, %fd35, %fd198;
	sqrt.rn.f64 	%fd39, %fd199;
	mul.f64 	%fd200, %fd37, %fd37;
	fma.rn.f64 	%fd201, %fd36, %fd36, %fd200;
	fma.rn.f64 	%fd202, %fd38, %fd38, %fd201;
	sqrt.rn.f64 	%fd40, %fd202;
	add.f64 	%fd41, %fd39, %fd39;
	setp.leu.f64 	%p9, %fd40, %fd41;
	mov.f64 	%fd1538, 0d0000000000000000;
	mov.f64 	%fd1536, %fd1538;
	mov.f64 	%fd1537, %fd1538;
	@%p9 bra 	$L__BB21_20;

	ld.param.u64 	%rd713, [clamp_search_direction_cuda_kernel_backward_param_7];
	setp.eq.s64 	%p10, %rd713, 0;
	@%p10 bra 	$L__BB21_17;

	mul.lo.s64 	%rd91, %rd43, %rd27;
	add.s64 	%rd92, %rd12, %rd91;
	ld.global.f64 	%fd203, [%rd92];
	add.f64 	%fd1535, %fd203, 0d0000000000000000;
	bra.uni 	$L__BB21_19;

$L__BB21_17:
	ld.param.u64 	%rd714, [clamp_search_direction_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p11, %rd714, 0;
	mov.f64 	%fd1535, 0d0000000000000000;
	@%p11 bra 	$L__BB21_19;

	mul.lo.s64 	%rd93, %rd43, %rd28;
	add.s64 	%rd94, %rd18, %rd93;
	ld.global.f64 	%fd205, [%rd94];
	add.f64 	%fd1535, %fd205, 0d0000000000000000;

$L__BB21_19:
	add.f64 	%fd1534, %fd39, %fd39;
	div.rn.f64 	%fd206, %fd1535, %fd40;
	add.f64 	%fd207, %fd206, 0d0000000000000000;
	mov.f64 	%fd208, 0d0000000000000000;
	div.rn.f64 	%fd209, %fd1534, %fd40;
	mul.f64 	%fd210, %fd209, %fd1535;
	div.rn.f64 	%fd211, %fd210, %fd40;
	sub.f64 	%fd1537, %fd208, %fd211;
	fma.rn.f64 	%fd1536, %fd207, 0d4000000000000000, 0d0000000000000000;

$L__BB21_20:
	setp.leu.f64 	%p12, %fd40, 0d0000000000000000;
	mov.f64 	%fd1539, %fd1538;
	mov.f64 	%fd1540, %fd1538;
	@%p12 bra 	$L__BB21_22;

	div.rn.f64 	%fd215, %fd36, %fd40;
	div.rn.f64 	%fd216, %fd37, %fd40;
	div.rn.f64 	%fd217, %fd38, %fd40;
	fma.rn.f64 	%fd1540, %fd215, %fd1537, 0d0000000000000000;
	fma.rn.f64 	%fd1539, %fd216, %fd1537, 0d0000000000000000;
	fma.rn.f64 	%fd1538, %fd217, %fd1537, 0d0000000000000000;

$L__BB21_22:
	setp.leu.f64 	%p13, %fd39, 0d0000000000000000;
	mov.f64 	%fd220, 0d0000000000000000;
	mov.f64 	%fd1541, %fd220;
	mov.f64 	%fd1542, %fd220;
	mov.f64 	%fd1543, %fd220;
	@%p13 bra 	$L__BB21_24;

	add.f64 	%fd221, %fd1536, 0d0000000000000000;
	div.rn.f64 	%fd222, %fd33, %fd39;
	div.rn.f64 	%fd223, %fd34, %fd39;
	div.rn.f64 	%fd224, %fd35, %fd39;
	fma.rn.f64 	%fd1543, %fd222, %fd221, 0d0000000000000000;
	fma.rn.f64 	%fd1542, %fd223, %fd221, 0d0000000000000000;
	fma.rn.f64 	%fd1541, %fd224, %fd221, 0d0000000000000000;

$L__BB21_24:
	mov.f64 	%fd1533, 0d3FF0000000000000;
	sub.f64 	%fd1532, %fd1533, %fd17;
	sub.f64 	%fd1531, %fd1532, %fd18;
	sub.f64 	%fd1530, %fd1531, %fd19;
	sub.f64 	%fd1529, %fd1533, %fd29;
	sub.f64 	%fd1528, %fd1529, %fd30;
	sub.f64 	%fd1527, %fd1528, %fd31;
	sub.f64 	%fd226, %fd220, %fd1539;
	add.f64 	%fd227, %fd226, 0d0000000000000000;
	sub.f64 	%fd228, %fd220, %fd1540;
	add.f64 	%fd229, %fd228, 0d0000000000000000;
	sub.f64 	%fd230, %fd220, %fd1538;
	add.f64 	%fd231, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd232, %fd31, %fd229, 0d0000000000000000;
	fma.rn.f64 	%fd233, %fd31, %fd227, 0d0000000000000000;
	fma.rn.f64 	%fd234, %fd31, %fd231, 0d0000000000000000;
	mul.f64 	%fd235, %fd15, %fd227;
	fma.rn.f64 	%fd236, %fd14, %fd229, %fd235;
	fma.rn.f64 	%fd237, %fd16, %fd231, %fd236;
	fma.rn.f64 	%fd238, %fd30, %fd229, 0d0000000000000000;
	fma.rn.f64 	%fd239, %fd30, %fd227, 0d0000000000000000;
	fma.rn.f64 	%fd240, %fd30, %fd231, 0d0000000000000000;
	mul.f64 	%fd241, %fd11, %fd227;
	fma.rn.f64 	%fd242, %fd10, %fd229, %fd241;
	fma.rn.f64 	%fd243, %fd12, %fd231, %fd242;
	fma.rn.f64 	%fd244, %fd29, %fd229, 0d0000000000000000;
	fma.rn.f64 	%fd245, %fd29, %fd227, 0d0000000000000000;
	fma.rn.f64 	%fd246, %fd29, %fd231, 0d0000000000000000;
	add.f64 	%fd247, %fd237, 0d0000000000000000;
	mul.f64 	%fd248, %fd7, %fd227;
	fma.rn.f64 	%fd249, %fd6, %fd229, %fd248;
	fma.rn.f64 	%fd250, %fd8, %fd231, %fd249;
	fma.rn.f64 	%fd251, %fd1527, %fd229, 0d0000000000000000;
	fma.rn.f64 	%fd252, %fd1527, %fd227, 0d0000000000000000;
	fma.rn.f64 	%fd253, %fd1527, %fd231, 0d0000000000000000;
	add.f64 	%fd254, %fd243, 0d0000000000000000;
	add.f64 	%fd255, %fd250, 0d0000000000000000;
	mul.f64 	%fd256, %fd3, %fd227;
	fma.rn.f64 	%fd257, %fd2, %fd229, %fd256;
	fma.rn.f64 	%fd258, %fd4, %fd231, %fd257;
	add.f64 	%fd259, %fd258, 0d0000000000000000;
	sub.f64 	%fd260, %fd220, %fd259;
	add.f64 	%fd261, %fd1540, 0d0000000000000000;
	fma.rn.f64 	%fd262, %fd19, %fd261, %fd232;
	add.f64 	%fd263, %fd1539, 0d0000000000000000;
	fma.rn.f64 	%fd264, %fd19, %fd263, %fd233;
	add.f64 	%fd265, %fd1538, 0d0000000000000000;
	fma.rn.f64 	%fd266, %fd19, %fd265, %fd234;
	mul.f64 	%fd267, %fd15, %fd263;
	fma.rn.f64 	%fd268, %fd14, %fd261, %fd267;
	fma.rn.f64 	%fd269, %fd16, %fd265, %fd268;
	fma.rn.f64 	%fd61, %fd18, %fd261, %fd238;
	fma.rn.f64 	%fd62, %fd18, %fd263, %fd239;
	fma.rn.f64 	%fd63, %fd18, %fd265, %fd240;
	add.f64 	%fd270, %fd255, %fd260;
	mul.f64 	%fd271, %fd11, %fd263;
	fma.rn.f64 	%fd272, %fd10, %fd261, %fd271;
	fma.rn.f64 	%fd273, %fd12, %fd265, %fd272;
	fma.rn.f64 	%fd64, %fd17, %fd261, %fd244;
	fma.rn.f64 	%fd65, %fd17, %fd263, %fd245;
	fma.rn.f64 	%fd66, %fd17, %fd265, %fd246;
	add.f64 	%fd274, %fd269, 0d0000000000000000;
	mul.f64 	%fd275, %fd7, %fd263;
	fma.rn.f64 	%fd276, %fd6, %fd261, %fd275;
	fma.rn.f64 	%fd277, %fd8, %fd265, %fd276;
	fma.rn.f64 	%fd67, %fd1530, %fd261, %fd251;
	fma.rn.f64 	%fd68, %fd1530, %fd263, %fd252;
	fma.rn.f64 	%fd69, %fd1530, %fd265, %fd253;
	add.f64 	%fd278, %fd273, 0d0000000000000000;
	add.f64 	%fd279, %fd277, 0d0000000000000000;
	mul.f64 	%fd280, %fd3, %fd263;
	fma.rn.f64 	%fd281, %fd2, %fd261, %fd280;
	fma.rn.f64 	%fd282, %fd4, %fd265, %fd281;
	add.f64 	%fd283, %fd282, 0d0000000000000000;
	sub.f64 	%fd284, %fd220, %fd283;
	add.f64 	%fd285, %fd279, %fd284;
	sub.f64 	%fd286, %fd220, %fd1542;
	add.f64 	%fd287, %fd286, 0d0000000000000000;
	sub.f64 	%fd288, %fd220, %fd1543;
	add.f64 	%fd289, %fd288, 0d0000000000000000;
	sub.f64 	%fd290, %fd220, %fd1541;
	add.f64 	%fd291, %fd290, 0d0000000000000000;
	fma.rn.f64 	%fd292, %fd31, %fd289, 0d0000000000000000;
	fma.rn.f64 	%fd293, %fd31, %fd287, 0d0000000000000000;
	fma.rn.f64 	%fd294, %fd31, %fd291, 0d0000000000000000;
	add.f64 	%fd295, %fd247, %fd260;
	mul.f64 	%fd296, %fd27, %fd287;
	fma.rn.f64 	%fd297, %fd13, %fd289, %fd296;
	fma.rn.f64 	%fd298, %fd28, %fd291, %fd297;
	add.f64 	%fd299, %fd298, 0d0000000000000000;
	fma.rn.f64 	%fd300, %fd30, %fd289, 0d0000000000000000;
	fma.rn.f64 	%fd301, %fd30, %fd287, 0d0000000000000000;
	fma.rn.f64 	%fd302, %fd30, %fd291, 0d0000000000000000;
	add.f64 	%fd303, %fd254, %fd260;
	add.f64 	%fd304, %fd295, %fd299;
	mul.f64 	%fd305, %fd25, %fd287;
	fma.rn.f64 	%fd306, %fd9, %fd289, %fd305;
	fma.rn.f64 	%fd307, %fd26, %fd291, %fd306;
	add.f64 	%fd308, %fd307, 0d0000000000000000;
	fma.rn.f64 	%fd309, %fd29, %fd289, 0d0000000000000000;
	fma.rn.f64 	%fd310, %fd29, %fd287, 0d0000000000000000;
	fma.rn.f64 	%fd311, %fd29, %fd291, 0d0000000000000000;
	add.f64 	%fd312, %fd303, %fd308;
	mul.f64 	%fd313, %fd23, %fd287;
	fma.rn.f64 	%fd314, %fd5, %fd289, %fd313;
	fma.rn.f64 	%fd315, %fd24, %fd291, %fd314;
	add.f64 	%fd316, %fd315, 0d0000000000000000;
	fma.rn.f64 	%fd317, %fd1527, %fd289, 0d0000000000000000;
	fma.rn.f64 	%fd318, %fd1527, %fd287, 0d0000000000000000;
	fma.rn.f64 	%fd319, %fd1527, %fd291, 0d0000000000000000;
	add.f64 	%fd320, %fd270, %fd316;
	mul.f64 	%fd321, %fd21, %fd287;
	fma.rn.f64 	%fd322, %fd1, %fd289, %fd321;
	fma.rn.f64 	%fd323, %fd22, %fd291, %fd322;
	add.f64 	%fd324, %fd323, 0d0000000000000000;
	sub.f64 	%fd325, %fd220, %fd324;
	add.f64 	%fd326, %fd1543, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd19, %fd326, %fd292;
	add.f64 	%fd327, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd71, %fd19, %fd327, %fd293;
	add.f64 	%fd328, %fd1541, 0d0000000000000000;
	fma.rn.f64 	%fd72, %fd19, %fd328, %fd294;
	add.f64 	%fd329, %fd274, %fd284;
	mul.f64 	%fd330, %fd27, %fd327;
	fma.rn.f64 	%fd331, %fd13, %fd326, %fd330;
	fma.rn.f64 	%fd332, %fd28, %fd328, %fd331;
	add.f64 	%fd333, %fd332, 0d0000000000000000;
	fma.rn.f64 	%fd73, %fd18, %fd326, %fd300;
	fma.rn.f64 	%fd74, %fd18, %fd327, %fd301;
	fma.rn.f64 	%fd75, %fd18, %fd328, %fd302;
	add.f64 	%fd334, %fd278, %fd284;
	mul.f64 	%fd335, %fd25, %fd327;
	fma.rn.f64 	%fd336, %fd9, %fd326, %fd335;
	fma.rn.f64 	%fd337, %fd26, %fd328, %fd336;
	add.f64 	%fd338, %fd337, 0d0000000000000000;
	fma.rn.f64 	%fd76, %fd17, %fd326, %fd309;
	fma.rn.f64 	%fd77, %fd17, %fd327, %fd310;
	fma.rn.f64 	%fd78, %fd17, %fd328, %fd311;
	mul.f64 	%fd339, %fd23, %fd327;
	fma.rn.f64 	%fd340, %fd5, %fd326, %fd339;
	fma.rn.f64 	%fd341, %fd24, %fd328, %fd340;
	add.f64 	%fd342, %fd341, 0d0000000000000000;
	fma.rn.f64 	%fd79, %fd1530, %fd326, %fd317;
	fma.rn.f64 	%fd80, %fd1530, %fd327, %fd318;
	fma.rn.f64 	%fd81, %fd1530, %fd328, %fd319;
	add.f64 	%fd82, %fd304, %fd325;
	add.f64 	%fd83, %fd312, %fd325;
	add.f64 	%fd84, %fd320, %fd325;
	add.f64 	%fd343, %fd329, %fd333;
	add.f64 	%fd344, %fd334, %fd338;
	add.f64 	%fd345, %fd285, %fd342;
	mul.f64 	%fd346, %fd21, %fd327;
	fma.rn.f64 	%fd347, %fd1, %fd326, %fd346;
	fma.rn.f64 	%fd348, %fd22, %fd328, %fd347;
	add.f64 	%fd349, %fd348, 0d0000000000000000;
	sub.f64 	%fd350, %fd220, %fd349;
	add.f64 	%fd85, %fd343, %fd350;
	add.f64 	%fd86, %fd344, %fd350;
	add.f64 	%fd87, %fd345, %fd350;
	add.f64 	%fd88, %fd262, 0d0000000000000000;
	add.f64 	%fd89, %fd264, 0d0000000000000000;
	add.f64 	%fd90, %fd266, 0d0000000000000000;
	setp.eq.s64 	%p14, %rd69, 0;
	@%p14 bra 	$L__BB21_26;

	mul.lo.s64 	%rd107, %rd48, %rd29;
	add.s64 	%rd95, %rd69, %rd107;
	mov.f64 	%fd372, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd351,[%rd95],%fd372; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd353,[%rd96],%fd372; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd355,[%rd97],%fd372; }

	// end inline asm
	add.s64 	%rd98, %rd95, 24;
	// begin inline asm
	{ atom.add.f64 %fd357,[%rd98],%fd372; }

	// end inline asm
	add.s64 	%rd99, %rd95, 32;
	// begin inline asm
	{ atom.add.f64 %fd359,[%rd99],%fd372; }

	// end inline asm
	add.s64 	%rd100, %rd95, 40;
	// begin inline asm
	{ atom.add.f64 %fd361,[%rd100],%fd372; }

	// end inline asm
	add.s64 	%rd101, %rd95, 48;
	// begin inline asm
	{ atom.add.f64 %fd363,[%rd101],%fd372; }

	// end inline asm
	add.s64 	%rd102, %rd95, 56;
	// begin inline asm
	{ atom.add.f64 %fd365,[%rd102],%fd372; }

	// end inline asm
	add.s64 	%rd103, %rd95, 64;
	// begin inline asm
	{ atom.add.f64 %fd367,[%rd103],%fd372; }

	// end inline asm
	add.s64 	%rd104, %rd95, 72;
	// begin inline asm
	{ atom.add.f64 %fd369,[%rd104],%fd372; }

	// end inline asm
	add.s64 	%rd105, %rd95, 80;
	// begin inline asm
	{ atom.add.f64 %fd371,[%rd105],%fd372; }

	// end inline asm
	add.s64 	%rd106, %rd95, 88;
	// begin inline asm
	{ atom.add.f64 %fd373,[%rd106],%fd90; }

	// end inline asm
	bra.uni 	$L__BB21_28;

$L__BB21_26:
	setp.eq.s64 	%p15, %rd62, 0;
	@%p15 bra 	$L__BB21_28;

	add.s64 	%rd108, %rd62, %rd50;
	mov.f64 	%fd396, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd375,[%rd108],%fd396; }

	// end inline asm
	add.s64 	%rd109, %rd108, 8;
	// begin inline asm
	{ atom.add.f64 %fd377,[%rd109],%fd396; }

	// end inline asm
	add.s64 	%rd110, %rd108, 16;
	// begin inline asm
	{ atom.add.f64 %fd379,[%rd110],%fd396; }

	// end inline asm
	add.s64 	%rd111, %rd108, 24;
	// begin inline asm
	{ atom.add.f64 %fd381,[%rd111],%fd396; }

	// end inline asm
	add.s64 	%rd112, %rd108, 32;
	// begin inline asm
	{ atom.add.f64 %fd383,[%rd112],%fd396; }

	// end inline asm
	add.s64 	%rd113, %rd108, 40;
	// begin inline asm
	{ atom.add.f64 %fd385,[%rd113],%fd396; }

	// end inline asm
	add.s64 	%rd114, %rd108, 48;
	// begin inline asm
	{ atom.add.f64 %fd387,[%rd114],%fd396; }

	// end inline asm
	add.s64 	%rd115, %rd108, 56;
	// begin inline asm
	{ atom.add.f64 %fd389,[%rd115],%fd396; }

	// end inline asm
	add.s64 	%rd116, %rd108, 64;
	// begin inline asm
	{ atom.add.f64 %fd391,[%rd116],%fd396; }

	// end inline asm
	add.s64 	%rd117, %rd108, 72;
	// begin inline asm
	{ atom.add.f64 %fd393,[%rd117],%fd396; }

	// end inline asm
	add.s64 	%rd118, %rd108, 80;
	// begin inline asm
	{ atom.add.f64 %fd395,[%rd118],%fd396; }

	// end inline asm
	add.s64 	%rd119, %rd108, 88;
	// begin inline asm
	{ atom.add.f64 %fd397,[%rd119],%fd90; }

	// end inline asm

$L__BB21_28:
	add.f64 	%fd91, %fd90, %fd70;
	@%p14 bra 	$L__BB21_30;

	mul.lo.s64 	%rd132, %rd48, %rd29;
	add.s64 	%rd120, %rd69, %rd132;
	mov.f64 	%fd422, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd399,[%rd120],%fd422; }

	// end inline asm
	add.s64 	%rd121, %rd120, 8;
	// begin inline asm
	{ atom.add.f64 %fd401,[%rd121],%fd422; }

	// end inline asm
	add.s64 	%rd122, %rd120, 16;
	// begin inline asm
	{ atom.add.f64 %fd403,[%rd122],%fd422; }

	// end inline asm
	add.s64 	%rd123, %rd120, 24;
	// begin inline asm
	{ atom.add.f64 %fd405,[%rd123],%fd422; }

	// end inline asm
	add.s64 	%rd124, %rd120, 32;
	// begin inline asm
	{ atom.add.f64 %fd407,[%rd124],%fd422; }

	// end inline asm
	add.s64 	%rd125, %rd120, 40;
	// begin inline asm
	{ atom.add.f64 %fd409,[%rd125],%fd422; }

	// end inline asm
	add.s64 	%rd126, %rd120, 48;
	// begin inline asm
	{ atom.add.f64 %fd411,[%rd126],%fd422; }

	// end inline asm
	add.s64 	%rd127, %rd120, 56;
	// begin inline asm
	{ atom.add.f64 %fd413,[%rd127],%fd422; }

	// end inline asm
	add.s64 	%rd128, %rd120, 64;
	// begin inline asm
	{ atom.add.f64 %fd415,[%rd128],%fd422; }

	// end inline asm
	add.s64 	%rd129, %rd120, 72;
	// begin inline asm
	{ atom.add.f64 %fd417,[%rd129],%fd422; }

	// end inline asm
	add.s64 	%rd130, %rd120, 80;
	// begin inline asm
	{ atom.add.f64 %fd419,[%rd130],%fd89; }

	// end inline asm
	add.s64 	%rd131, %rd120, 88;
	// begin inline asm
	{ atom.add.f64 %fd421,[%rd131],%fd422; }

	// end inline asm
	bra.uni 	$L__BB21_32;

$L__BB21_30:
	setp.eq.s64 	%p17, %rd62, 0;
	@%p17 bra 	$L__BB21_32;

	add.s64 	%rd133, %rd62, %rd50;
	mov.f64 	%fd446, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd423,[%rd133],%fd446; }

	// end inline asm
	add.s64 	%rd134, %rd133, 8;
	// begin inline asm
	{ atom.add.f64 %fd425,[%rd134],%fd446; }

	// end inline asm
	add.s64 	%rd135, %rd133, 16;
	// begin inline asm
	{ atom.add.f64 %fd427,[%rd135],%fd446; }

	// end inline asm
	add.s64 	%rd136, %rd133, 24;
	// begin inline asm
	{ atom.add.f64 %fd429,[%rd136],%fd446; }

	// end inline asm
	add.s64 	%rd137, %rd133, 32;
	// begin inline asm
	{ atom.add.f64 %fd431,[%rd137],%fd446; }

	// end inline asm
	add.s64 	%rd138, %rd133, 40;
	// begin inline asm
	{ atom.add.f64 %fd433,[%rd138],%fd446; }

	// end inline asm
	add.s64 	%rd139, %rd133, 48;
	// begin inline asm
	{ atom.add.f64 %fd435,[%rd139],%fd446; }

	// end inline asm
	add.s64 	%rd140, %rd133, 56;
	// begin inline asm
	{ atom.add.f64 %fd437,[%rd140],%fd446; }

	// end inline asm
	add.s64 	%rd141, %rd133, 64;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd141],%fd446; }

	// end inline asm
	add.s64 	%rd142, %rd133, 72;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd142],%fd446; }

	// end inline asm
	add.s64 	%rd143, %rd133, 80;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd143],%fd89; }

	// end inline asm
	add.s64 	%rd144, %rd133, 88;
	// begin inline asm
	{ atom.add.f64 %fd445,[%rd144],%fd446; }

	// end inline asm

$L__BB21_32:
	add.f64 	%fd92, %fd89, %fd91;
	@%p14 bra 	$L__BB21_34;

	mul.lo.s64 	%rd157, %rd48, %rd29;
	add.s64 	%rd145, %rd69, %rd157;
	mov.f64 	%fd470, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd447,[%rd145],%fd470; }

	// end inline asm
	add.s64 	%rd146, %rd145, 8;
	// begin inline asm
	{ atom.add.f64 %fd449,[%rd146],%fd470; }

	// end inline asm
	add.s64 	%rd147, %rd145, 16;
	// begin inline asm
	{ atom.add.f64 %fd451,[%rd147],%fd470; }

	// end inline asm
	add.s64 	%rd148, %rd145, 24;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd148],%fd470; }

	// end inline asm
	add.s64 	%rd149, %rd145, 32;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd149],%fd470; }

	// end inline asm
	add.s64 	%rd150, %rd145, 40;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd150],%fd470; }

	// end inline asm
	add.s64 	%rd151, %rd145, 48;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd151],%fd470; }

	// end inline asm
	add.s64 	%rd152, %rd145, 56;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd152],%fd470; }

	// end inline asm
	add.s64 	%rd153, %rd145, 64;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd153],%fd470; }

	// end inline asm
	add.s64 	%rd154, %rd145, 72;
	// begin inline asm
	{ atom.add.f64 %fd465,[%rd154],%fd88; }

	// end inline asm
	add.s64 	%rd155, %rd145, 80;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd155],%fd470; }

	// end inline asm
	add.s64 	%rd156, %rd145, 88;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd156],%fd470; }

	// end inline asm
	bra.uni 	$L__BB21_36;

$L__BB21_34:
	setp.eq.s64 	%p19, %rd62, 0;
	@%p19 bra 	$L__BB21_36;

	add.s64 	%rd158, %rd62, %rd50;
	mov.f64 	%fd494, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd471,[%rd158],%fd494; }

	// end inline asm
	add.s64 	%rd159, %rd158, 8;
	// begin inline asm
	{ atom.add.f64 %fd473,[%rd159],%fd494; }

	// end inline asm
	add.s64 	%rd160, %rd158, 16;
	// begin inline asm
	{ atom.add.f64 %fd475,[%rd160],%fd494; }

	// end inline asm
	add.s64 	%rd161, %rd158, 24;
	// begin inline asm
	{ atom.add.f64 %fd477,[%rd161],%fd494; }

	// end inline asm
	add.s64 	%rd162, %rd158, 32;
	// begin inline asm
	{ atom.add.f64 %fd479,[%rd162],%fd494; }

	// end inline asm
	add.s64 	%rd163, %rd158, 40;
	// begin inline asm
	{ atom.add.f64 %fd481,[%rd163],%fd494; }

	// end inline asm
	add.s64 	%rd164, %rd158, 48;
	// begin inline asm
	{ atom.add.f64 %fd483,[%rd164],%fd494; }

	// end inline asm
	add.s64 	%rd165, %rd158, 56;
	// begin inline asm
	{ atom.add.f64 %fd485,[%rd165],%fd494; }

	// end inline asm
	add.s64 	%rd166, %rd158, 64;
	// begin inline asm
	{ atom.add.f64 %fd487,[%rd166],%fd494; }

	// end inline asm
	add.s64 	%rd167, %rd158, 72;
	// begin inline asm
	{ atom.add.f64 %fd489,[%rd167],%fd88; }

	// end inline asm
	add.s64 	%rd168, %rd158, 80;
	// begin inline asm
	{ atom.add.f64 %fd491,[%rd168],%fd494; }

	// end inline asm
	add.s64 	%rd169, %rd158, 88;
	// begin inline asm
	{ atom.add.f64 %fd493,[%rd169],%fd494; }

	// end inline asm

$L__BB21_36:
	add.f64 	%fd93, %fd88, %fd92;
	add.f64 	%fd94, %fd61, 0d0000000000000000;
	add.f64 	%fd95, %fd62, 0d0000000000000000;
	add.f64 	%fd96, %fd63, 0d0000000000000000;
	@%p14 bra 	$L__BB21_38;

	mul.lo.s64 	%rd182, %rd48, %rd29;
	add.s64 	%rd170, %rd69, %rd182;
	mov.f64 	%fd518, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd495,[%rd170],%fd518; }

	// end inline asm
	add.s64 	%rd171, %rd170, 8;
	// begin inline asm
	{ atom.add.f64 %fd497,[%rd171],%fd518; }

	// end inline asm
	add.s64 	%rd172, %rd170, 16;
	// begin inline asm
	{ atom.add.f64 %fd499,[%rd172],%fd518; }

	// end inline asm
	add.s64 	%rd173, %rd170, 24;
	// begin inline asm
	{ atom.add.f64 %fd501,[%rd173],%fd518; }

	// end inline asm
	add.s64 	%rd174, %rd170, 32;
	// begin inline asm
	{ atom.add.f64 %fd503,[%rd174],%fd518; }

	// end inline asm
	add.s64 	%rd175, %rd170, 40;
	// begin inline asm
	{ atom.add.f64 %fd505,[%rd175],%fd518; }

	// end inline asm
	add.s64 	%rd176, %rd170, 48;
	// begin inline asm
	{ atom.add.f64 %fd507,[%rd176],%fd518; }

	// end inline asm
	add.s64 	%rd177, %rd170, 56;
	// begin inline asm
	{ atom.add.f64 %fd509,[%rd177],%fd518; }

	// end inline asm
	add.s64 	%rd178, %rd170, 64;
	// begin inline asm
	{ atom.add.f64 %fd511,[%rd178],%fd96; }

	// end inline asm
	add.s64 	%rd179, %rd170, 72;
	// begin inline asm
	{ atom.add.f64 %fd513,[%rd179],%fd518; }

	// end inline asm
	add.s64 	%rd180, %rd170, 80;
	// begin inline asm
	{ atom.add.f64 %fd515,[%rd180],%fd518; }

	// end inline asm
	add.s64 	%rd181, %rd170, 88;
	// begin inline asm
	{ atom.add.f64 %fd517,[%rd181],%fd518; }

	// end inline asm
	bra.uni 	$L__BB21_40;

$L__BB21_38:
	setp.eq.s64 	%p21, %rd62, 0;
	@%p21 bra 	$L__BB21_40;

	add.s64 	%rd183, %rd62, %rd50;
	mov.f64 	%fd542, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd519,[%rd183],%fd542; }

	// end inline asm
	add.s64 	%rd184, %rd183, 8;
	// begin inline asm
	{ atom.add.f64 %fd521,[%rd184],%fd542; }

	// end inline asm
	add.s64 	%rd185, %rd183, 16;
	// begin inline asm
	{ atom.add.f64 %fd523,[%rd185],%fd542; }

	// end inline asm
	add.s64 	%rd186, %rd183, 24;
	// begin inline asm
	{ atom.add.f64 %fd525,[%rd186],%fd542; }

	// end inline asm
	add.s64 	%rd187, %rd183, 32;
	// begin inline asm
	{ atom.add.f64 %fd527,[%rd187],%fd542; }

	// end inline asm
	add.s64 	%rd188, %rd183, 40;
	// begin inline asm
	{ atom.add.f64 %fd529,[%rd188],%fd542; }

	// end inline asm
	add.s64 	%rd189, %rd183, 48;
	// begin inline asm
	{ atom.add.f64 %fd531,[%rd189],%fd542; }

	// end inline asm
	add.s64 	%rd190, %rd183, 56;
	// begin inline asm
	{ atom.add.f64 %fd533,[%rd190],%fd542; }

	// end inline asm
	add.s64 	%rd191, %rd183, 64;
	// begin inline asm
	{ atom.add.f64 %fd535,[%rd191],%fd96; }

	// end inline asm
	add.s64 	%rd192, %rd183, 72;
	// begin inline asm
	{ atom.add.f64 %fd537,[%rd192],%fd542; }

	// end inline asm
	add.s64 	%rd193, %rd183, 80;
	// begin inline asm
	{ atom.add.f64 %fd539,[%rd193],%fd542; }

	// end inline asm
	add.s64 	%rd194, %rd183, 88;
	// begin inline asm
	{ atom.add.f64 %fd541,[%rd194],%fd542; }

	// end inline asm

$L__BB21_40:
	add.f64 	%fd97, %fd96, %fd73;
	@%p14 bra 	$L__BB21_42;

	mul.lo.s64 	%rd207, %rd48, %rd29;
	add.s64 	%rd195, %rd69, %rd207;
	mov.f64 	%fd566, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd543,[%rd195],%fd566; }

	// end inline asm
	add.s64 	%rd196, %rd195, 8;
	// begin inline asm
	{ atom.add.f64 %fd545,[%rd196],%fd566; }

	// end inline asm
	add.s64 	%rd197, %rd195, 16;
	// begin inline asm
	{ atom.add.f64 %fd547,[%rd197],%fd566; }

	// end inline asm
	add.s64 	%rd198, %rd195, 24;
	// begin inline asm
	{ atom.add.f64 %fd549,[%rd198],%fd566; }

	// end inline asm
	add.s64 	%rd199, %rd195, 32;
	// begin inline asm
	{ atom.add.f64 %fd551,[%rd199],%fd566; }

	// end inline asm
	add.s64 	%rd200, %rd195, 40;
	// begin inline asm
	{ atom.add.f64 %fd553,[%rd200],%fd566; }

	// end inline asm
	add.s64 	%rd201, %rd195, 48;
	// begin inline asm
	{ atom.add.f64 %fd555,[%rd201],%fd566; }

	// end inline asm
	add.s64 	%rd202, %rd195, 56;
	// begin inline asm
	{ atom.add.f64 %fd557,[%rd202],%fd95; }

	// end inline asm
	add.s64 	%rd203, %rd195, 64;
	// begin inline asm
	{ atom.add.f64 %fd559,[%rd203],%fd566; }

	// end inline asm
	add.s64 	%rd204, %rd195, 72;
	// begin inline asm
	{ atom.add.f64 %fd561,[%rd204],%fd566; }

	// end inline asm
	add.s64 	%rd205, %rd195, 80;
	// begin inline asm
	{ atom.add.f64 %fd563,[%rd205],%fd566; }

	// end inline asm
	add.s64 	%rd206, %rd195, 88;
	// begin inline asm
	{ atom.add.f64 %fd565,[%rd206],%fd566; }

	// end inline asm
	bra.uni 	$L__BB21_44;

$L__BB21_42:
	setp.eq.s64 	%p23, %rd62, 0;
	@%p23 bra 	$L__BB21_44;

	add.s64 	%rd208, %rd62, %rd50;
	mov.f64 	%fd590, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd567,[%rd208],%fd590; }

	// end inline asm
	add.s64 	%rd209, %rd208, 8;
	// begin inline asm
	{ atom.add.f64 %fd569,[%rd209],%fd590; }

	// end inline asm
	add.s64 	%rd210, %rd208, 16;
	// begin inline asm
	{ atom.add.f64 %fd571,[%rd210],%fd590; }

	// end inline asm
	add.s64 	%rd211, %rd208, 24;
	// begin inline asm
	{ atom.add.f64 %fd573,[%rd211],%fd590; }

	// end inline asm
	add.s64 	%rd212, %rd208, 32;
	// begin inline asm
	{ atom.add.f64 %fd575,[%rd212],%fd590; }

	// end inline asm
	add.s64 	%rd213, %rd208, 40;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd213],%fd590; }

	// end inline asm
	add.s64 	%rd214, %rd208, 48;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd214],%fd590; }

	// end inline asm
	add.s64 	%rd215, %rd208, 56;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd215],%fd95; }

	// end inline asm
	add.s64 	%rd216, %rd208, 64;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd216],%fd590; }

	// end inline asm
	add.s64 	%rd217, %rd208, 72;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd217],%fd590; }

	// end inline asm
	add.s64 	%rd218, %rd208, 80;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd218],%fd590; }

	// end inline asm
	add.s64 	%rd219, %rd208, 88;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd219],%fd590; }

	// end inline asm

$L__BB21_44:
	add.f64 	%fd98, %fd95, %fd97;
	@%p14 bra 	$L__BB21_46;

	mul.lo.s64 	%rd232, %rd48, %rd29;
	add.s64 	%rd220, %rd69, %rd232;
	mov.f64 	%fd614, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd220],%fd614; }

	// end inline asm
	add.s64 	%rd221, %rd220, 8;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd221],%fd614; }

	// end inline asm
	add.s64 	%rd222, %rd220, 16;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd222],%fd614; }

	// end inline asm
	add.s64 	%rd223, %rd220, 24;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd223],%fd614; }

	// end inline asm
	add.s64 	%rd224, %rd220, 32;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd224],%fd614; }

	// end inline asm
	add.s64 	%rd225, %rd220, 40;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd225],%fd614; }

	// end inline asm
	add.s64 	%rd226, %rd220, 48;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd226],%fd94; }

	// end inline asm
	add.s64 	%rd227, %rd220, 56;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd227],%fd614; }

	// end inline asm
	add.s64 	%rd228, %rd220, 64;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd228],%fd614; }

	// end inline asm
	add.s64 	%rd229, %rd220, 72;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd229],%fd614; }

	// end inline asm
	add.s64 	%rd230, %rd220, 80;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd230],%fd614; }

	// end inline asm
	add.s64 	%rd231, %rd220, 88;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd231],%fd614; }

	// end inline asm
	bra.uni 	$L__BB21_48;

$L__BB21_46:
	setp.eq.s64 	%p25, %rd62, 0;
	@%p25 bra 	$L__BB21_48;

	add.s64 	%rd233, %rd62, %rd50;
	mov.f64 	%fd638, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd233],%fd638; }

	// end inline asm
	add.s64 	%rd234, %rd233, 8;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd234],%fd638; }

	// end inline asm
	add.s64 	%rd235, %rd233, 16;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd235],%fd638; }

	// end inline asm
	add.s64 	%rd236, %rd233, 24;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd236],%fd638; }

	// end inline asm
	add.s64 	%rd237, %rd233, 32;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd237],%fd638; }

	// end inline asm
	add.s64 	%rd238, %rd233, 40;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd238],%fd638; }

	// end inline asm
	add.s64 	%rd239, %rd233, 48;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd239],%fd94; }

	// end inline asm
	add.s64 	%rd240, %rd233, 56;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd240],%fd638; }

	// end inline asm
	add.s64 	%rd241, %rd233, 64;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd241],%fd638; }

	// end inline asm
	add.s64 	%rd242, %rd233, 72;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd242],%fd638; }

	// end inline asm
	add.s64 	%rd243, %rd233, 80;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd243],%fd638; }

	// end inline asm
	add.s64 	%rd244, %rd233, 88;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd244],%fd638; }

	// end inline asm

$L__BB21_48:
	add.f64 	%fd99, %fd94, %fd98;
	add.f64 	%fd100, %fd64, 0d0000000000000000;
	add.f64 	%fd101, %fd65, 0d0000000000000000;
	add.f64 	%fd102, %fd66, 0d0000000000000000;
	@%p14 bra 	$L__BB21_50;

	mul.lo.s64 	%rd257, %rd48, %rd29;
	add.s64 	%rd245, %rd69, %rd257;
	mov.f64 	%fd662, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd245],%fd662; }

	// end inline asm
	add.s64 	%rd246, %rd245, 8;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd246],%fd662; }

	// end inline asm
	add.s64 	%rd247, %rd245, 16;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd247],%fd662; }

	// end inline asm
	add.s64 	%rd248, %rd245, 24;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd248],%fd662; }

	// end inline asm
	add.s64 	%rd249, %rd245, 32;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd249],%fd662; }

	// end inline asm
	add.s64 	%rd250, %rd245, 40;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd250],%fd102; }

	// end inline asm
	add.s64 	%rd251, %rd245, 48;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd251],%fd662; }

	// end inline asm
	add.s64 	%rd252, %rd245, 56;
	// begin inline asm
	{ atom.add.f64 %fd653,[%rd252],%fd662; }

	// end inline asm
	add.s64 	%rd253, %rd245, 64;
	// begin inline asm
	{ atom.add.f64 %fd655,[%rd253],%fd662; }

	// end inline asm
	add.s64 	%rd254, %rd245, 72;
	// begin inline asm
	{ atom.add.f64 %fd657,[%rd254],%fd662; }

	// end inline asm
	add.s64 	%rd255, %rd245, 80;
	// begin inline asm
	{ atom.add.f64 %fd659,[%rd255],%fd662; }

	// end inline asm
	add.s64 	%rd256, %rd245, 88;
	// begin inline asm
	{ atom.add.f64 %fd661,[%rd256],%fd662; }

	// end inline asm
	bra.uni 	$L__BB21_52;

$L__BB21_50:
	setp.eq.s64 	%p27, %rd62, 0;
	@%p27 bra 	$L__BB21_52;

	add.s64 	%rd258, %rd62, %rd50;
	mov.f64 	%fd686, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd663,[%rd258],%fd686; }

	// end inline asm
	add.s64 	%rd259, %rd258, 8;
	// begin inline asm
	{ atom.add.f64 %fd665,[%rd259],%fd686; }

	// end inline asm
	add.s64 	%rd260, %rd258, 16;
	// begin inline asm
	{ atom.add.f64 %fd667,[%rd260],%fd686; }

	// end inline asm
	add.s64 	%rd261, %rd258, 24;
	// begin inline asm
	{ atom.add.f64 %fd669,[%rd261],%fd686; }

	// end inline asm
	add.s64 	%rd262, %rd258, 32;
	// begin inline asm
	{ atom.add.f64 %fd671,[%rd262],%fd686; }

	// end inline asm
	add.s64 	%rd263, %rd258, 40;
	// begin inline asm
	{ atom.add.f64 %fd673,[%rd263],%fd102; }

	// end inline asm
	add.s64 	%rd264, %rd258, 48;
	// begin inline asm
	{ atom.add.f64 %fd675,[%rd264],%fd686; }

	// end inline asm
	add.s64 	%rd265, %rd258, 56;
	// begin inline asm
	{ atom.add.f64 %fd677,[%rd265],%fd686; }

	// end inline asm
	add.s64 	%rd266, %rd258, 64;
	// begin inline asm
	{ atom.add.f64 %fd679,[%rd266],%fd686; }

	// end inline asm
	add.s64 	%rd267, %rd258, 72;
	// begin inline asm
	{ atom.add.f64 %fd681,[%rd267],%fd686; }

	// end inline asm
	add.s64 	%rd268, %rd258, 80;
	// begin inline asm
	{ atom.add.f64 %fd683,[%rd268],%fd686; }

	// end inline asm
	add.s64 	%rd269, %rd258, 88;
	// begin inline asm
	{ atom.add.f64 %fd685,[%rd269],%fd686; }

	// end inline asm

$L__BB21_52:
	add.f64 	%fd103, %fd102, %fd76;
	@%p14 bra 	$L__BB21_54;

	mul.lo.s64 	%rd282, %rd48, %rd29;
	add.s64 	%rd270, %rd69, %rd282;
	mov.f64 	%fd710, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd687,[%rd270],%fd710; }

	// end inline asm
	add.s64 	%rd271, %rd270, 8;
	// begin inline asm
	{ atom.add.f64 %fd689,[%rd271],%fd710; }

	// end inline asm
	add.s64 	%rd272, %rd270, 16;
	// begin inline asm
	{ atom.add.f64 %fd691,[%rd272],%fd710; }

	// end inline asm
	add.s64 	%rd273, %rd270, 24;
	// begin inline asm
	{ atom.add.f64 %fd693,[%rd273],%fd710; }

	// end inline asm
	add.s64 	%rd274, %rd270, 32;
	// begin inline asm
	{ atom.add.f64 %fd695,[%rd274],%fd101; }

	// end inline asm
	add.s64 	%rd275, %rd270, 40;
	// begin inline asm
	{ atom.add.f64 %fd697,[%rd275],%fd710; }

	// end inline asm
	add.s64 	%rd276, %rd270, 48;
	// begin inline asm
	{ atom.add.f64 %fd699,[%rd276],%fd710; }

	// end inline asm
	add.s64 	%rd277, %rd270, 56;
	// begin inline asm
	{ atom.add.f64 %fd701,[%rd277],%fd710; }

	// end inline asm
	add.s64 	%rd278, %rd270, 64;
	// begin inline asm
	{ atom.add.f64 %fd703,[%rd278],%fd710; }

	// end inline asm
	add.s64 	%rd279, %rd270, 72;
	// begin inline asm
	{ atom.add.f64 %fd705,[%rd279],%fd710; }

	// end inline asm
	add.s64 	%rd280, %rd270, 80;
	// begin inline asm
	{ atom.add.f64 %fd707,[%rd280],%fd710; }

	// end inline asm
	add.s64 	%rd281, %rd270, 88;
	// begin inline asm
	{ atom.add.f64 %fd709,[%rd281],%fd710; }

	// end inline asm
	bra.uni 	$L__BB21_56;

$L__BB21_54:
	setp.eq.s64 	%p29, %rd62, 0;
	@%p29 bra 	$L__BB21_56;

	add.s64 	%rd283, %rd62, %rd50;
	mov.f64 	%fd734, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd711,[%rd283],%fd734; }

	// end inline asm
	add.s64 	%rd284, %rd283, 8;
	// begin inline asm
	{ atom.add.f64 %fd713,[%rd284],%fd734; }

	// end inline asm
	add.s64 	%rd285, %rd283, 16;
	// begin inline asm
	{ atom.add.f64 %fd715,[%rd285],%fd734; }

	// end inline asm
	add.s64 	%rd286, %rd283, 24;
	// begin inline asm
	{ atom.add.f64 %fd717,[%rd286],%fd734; }

	// end inline asm
	add.s64 	%rd287, %rd283, 32;
	// begin inline asm
	{ atom.add.f64 %fd719,[%rd287],%fd101; }

	// end inline asm
	add.s64 	%rd288, %rd283, 40;
	// begin inline asm
	{ atom.add.f64 %fd721,[%rd288],%fd734; }

	// end inline asm
	add.s64 	%rd289, %rd283, 48;
	// begin inline asm
	{ atom.add.f64 %fd723,[%rd289],%fd734; }

	// end inline asm
	add.s64 	%rd290, %rd283, 56;
	// begin inline asm
	{ atom.add.f64 %fd725,[%rd290],%fd734; }

	// end inline asm
	add.s64 	%rd291, %rd283, 64;
	// begin inline asm
	{ atom.add.f64 %fd727,[%rd291],%fd734; }

	// end inline asm
	add.s64 	%rd292, %rd283, 72;
	// begin inline asm
	{ atom.add.f64 %fd729,[%rd292],%fd734; }

	// end inline asm
	add.s64 	%rd293, %rd283, 80;
	// begin inline asm
	{ atom.add.f64 %fd731,[%rd293],%fd734; }

	// end inline asm
	add.s64 	%rd294, %rd283, 88;
	// begin inline asm
	{ atom.add.f64 %fd733,[%rd294],%fd734; }

	// end inline asm

$L__BB21_56:
	add.f64 	%fd104, %fd101, %fd103;
	@%p14 bra 	$L__BB21_58;

	mul.lo.s64 	%rd307, %rd48, %rd29;
	add.s64 	%rd295, %rd69, %rd307;
	mov.f64 	%fd758, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd735,[%rd295],%fd758; }

	// end inline asm
	add.s64 	%rd296, %rd295, 8;
	// begin inline asm
	{ atom.add.f64 %fd737,[%rd296],%fd758; }

	// end inline asm
	add.s64 	%rd297, %rd295, 16;
	// begin inline asm
	{ atom.add.f64 %fd739,[%rd297],%fd758; }

	// end inline asm
	add.s64 	%rd298, %rd295, 24;
	// begin inline asm
	{ atom.add.f64 %fd741,[%rd298],%fd100; }

	// end inline asm
	add.s64 	%rd299, %rd295, 32;
	// begin inline asm
	{ atom.add.f64 %fd743,[%rd299],%fd758; }

	// end inline asm
	add.s64 	%rd300, %rd295, 40;
	// begin inline asm
	{ atom.add.f64 %fd745,[%rd300],%fd758; }

	// end inline asm
	add.s64 	%rd301, %rd295, 48;
	// begin inline asm
	{ atom.add.f64 %fd747,[%rd301],%fd758; }

	// end inline asm
	add.s64 	%rd302, %rd295, 56;
	// begin inline asm
	{ atom.add.f64 %fd749,[%rd302],%fd758; }

	// end inline asm
	add.s64 	%rd303, %rd295, 64;
	// begin inline asm
	{ atom.add.f64 %fd751,[%rd303],%fd758; }

	// end inline asm
	add.s64 	%rd304, %rd295, 72;
	// begin inline asm
	{ atom.add.f64 %fd753,[%rd304],%fd758; }

	// end inline asm
	add.s64 	%rd305, %rd295, 80;
	// begin inline asm
	{ atom.add.f64 %fd755,[%rd305],%fd758; }

	// end inline asm
	add.s64 	%rd306, %rd295, 88;
	// begin inline asm
	{ atom.add.f64 %fd757,[%rd306],%fd758; }

	// end inline asm
	bra.uni 	$L__BB21_60;

$L__BB21_58:
	setp.eq.s64 	%p31, %rd62, 0;
	@%p31 bra 	$L__BB21_60;

	add.s64 	%rd308, %rd62, %rd50;
	mov.f64 	%fd782, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd759,[%rd308],%fd782; }

	// end inline asm
	add.s64 	%rd309, %rd308, 8;
	// begin inline asm
	{ atom.add.f64 %fd761,[%rd309],%fd782; }

	// end inline asm
	add.s64 	%rd310, %rd308, 16;
	// begin inline asm
	{ atom.add.f64 %fd763,[%rd310],%fd782; }

	// end inline asm
	add.s64 	%rd311, %rd308, 24;
	// begin inline asm
	{ atom.add.f64 %fd765,[%rd311],%fd100; }

	// end inline asm
	add.s64 	%rd312, %rd308, 32;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd312],%fd782; }

	// end inline asm
	add.s64 	%rd313, %rd308, 40;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd313],%fd782; }

	// end inline asm
	add.s64 	%rd314, %rd308, 48;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd314],%fd782; }

	// end inline asm
	add.s64 	%rd315, %rd308, 56;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd315],%fd782; }

	// end inline asm
	add.s64 	%rd316, %rd308, 64;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd316],%fd782; }

	// end inline asm
	add.s64 	%rd317, %rd308, 72;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd317],%fd782; }

	// end inline asm
	add.s64 	%rd318, %rd308, 80;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd318],%fd782; }

	// end inline asm
	add.s64 	%rd319, %rd308, 88;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd319],%fd782; }

	// end inline asm

$L__BB21_60:
	add.f64 	%fd105, %fd100, %fd104;
	add.f64 	%fd106, %fd67, 0d0000000000000000;
	add.f64 	%fd107, %fd68, 0d0000000000000000;
	add.f64 	%fd108, %fd69, 0d0000000000000000;
	@%p14 bra 	$L__BB21_62;

	mul.lo.s64 	%rd332, %rd48, %rd29;
	add.s64 	%rd320, %rd69, %rd332;
	mov.f64 	%fd806, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd320],%fd806; }

	// end inline asm
	add.s64 	%rd321, %rd320, 8;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd321],%fd806; }

	// end inline asm
	add.s64 	%rd322, %rd320, 16;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd322],%fd108; }

	// end inline asm
	add.s64 	%rd323, %rd320, 24;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd323],%fd806; }

	// end inline asm
	add.s64 	%rd324, %rd320, 32;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd324],%fd806; }

	// end inline asm
	add.s64 	%rd325, %rd320, 40;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd325],%fd806; }

	// end inline asm
	add.s64 	%rd326, %rd320, 48;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd326],%fd806; }

	// end inline asm
	add.s64 	%rd327, %rd320, 56;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd327],%fd806; }

	// end inline asm
	add.s64 	%rd328, %rd320, 64;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd328],%fd806; }

	// end inline asm
	add.s64 	%rd329, %rd320, 72;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd329],%fd806; }

	// end inline asm
	add.s64 	%rd330, %rd320, 80;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd330],%fd806; }

	// end inline asm
	add.s64 	%rd331, %rd320, 88;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd331],%fd806; }

	// end inline asm
	bra.uni 	$L__BB21_64;

$L__BB21_62:
	setp.eq.s64 	%p33, %rd62, 0;
	@%p33 bra 	$L__BB21_64;

	add.s64 	%rd333, %rd62, %rd50;
	mov.f64 	%fd830, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd333],%fd830; }

	// end inline asm
	add.s64 	%rd334, %rd333, 8;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd334],%fd830; }

	// end inline asm
	add.s64 	%rd335, %rd333, 16;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd335],%fd108; }

	// end inline asm
	add.s64 	%rd336, %rd333, 24;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd336],%fd830; }

	// end inline asm
	add.s64 	%rd337, %rd333, 32;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd337],%fd830; }

	// end inline asm
	add.s64 	%rd338, %rd333, 40;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd338],%fd830; }

	// end inline asm
	add.s64 	%rd339, %rd333, 48;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd339],%fd830; }

	// end inline asm
	add.s64 	%rd340, %rd333, 56;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd340],%fd830; }

	// end inline asm
	add.s64 	%rd341, %rd333, 64;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd341],%fd830; }

	// end inline asm
	add.s64 	%rd342, %rd333, 72;
	// begin inline asm
	{ atom.add.f64 %fd825,[%rd342],%fd830; }

	// end inline asm
	add.s64 	%rd343, %rd333, 80;
	// begin inline asm
	{ atom.add.f64 %fd827,[%rd343],%fd830; }

	// end inline asm
	add.s64 	%rd344, %rd333, 88;
	// begin inline asm
	{ atom.add.f64 %fd829,[%rd344],%fd830; }

	// end inline asm

$L__BB21_64:
	add.f64 	%fd109, %fd108, %fd79;
	@%p14 bra 	$L__BB21_66;

	mul.lo.s64 	%rd357, %rd48, %rd29;
	add.s64 	%rd345, %rd69, %rd357;
	mov.f64 	%fd854, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd831,[%rd345],%fd854; }

	// end inline asm
	add.s64 	%rd346, %rd345, 8;
	// begin inline asm
	{ atom.add.f64 %fd833,[%rd346],%fd107; }

	// end inline asm
	add.s64 	%rd347, %rd345, 16;
	// begin inline asm
	{ atom.add.f64 %fd835,[%rd347],%fd854; }

	// end inline asm
	add.s64 	%rd348, %rd345, 24;
	// begin inline asm
	{ atom.add.f64 %fd837,[%rd348],%fd854; }

	// end inline asm
	add.s64 	%rd349, %rd345, 32;
	// begin inline asm
	{ atom.add.f64 %fd839,[%rd349],%fd854; }

	// end inline asm
	add.s64 	%rd350, %rd345, 40;
	// begin inline asm
	{ atom.add.f64 %fd841,[%rd350],%fd854; }

	// end inline asm
	add.s64 	%rd351, %rd345, 48;
	// begin inline asm
	{ atom.add.f64 %fd843,[%rd351],%fd854; }

	// end inline asm
	add.s64 	%rd352, %rd345, 56;
	// begin inline asm
	{ atom.add.f64 %fd845,[%rd352],%fd854; }

	// end inline asm
	add.s64 	%rd353, %rd345, 64;
	// begin inline asm
	{ atom.add.f64 %fd847,[%rd353],%fd854; }

	// end inline asm
	add.s64 	%rd354, %rd345, 72;
	// begin inline asm
	{ atom.add.f64 %fd849,[%rd354],%fd854; }

	// end inline asm
	add.s64 	%rd355, %rd345, 80;
	// begin inline asm
	{ atom.add.f64 %fd851,[%rd355],%fd854; }

	// end inline asm
	add.s64 	%rd356, %rd345, 88;
	// begin inline asm
	{ atom.add.f64 %fd853,[%rd356],%fd854; }

	// end inline asm
	bra.uni 	$L__BB21_68;

$L__BB21_66:
	setp.eq.s64 	%p35, %rd62, 0;
	@%p35 bra 	$L__BB21_68;

	add.s64 	%rd358, %rd62, %rd50;
	mov.f64 	%fd878, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd855,[%rd358],%fd878; }

	// end inline asm
	add.s64 	%rd359, %rd358, 8;
	// begin inline asm
	{ atom.add.f64 %fd857,[%rd359],%fd107; }

	// end inline asm
	add.s64 	%rd360, %rd358, 16;
	// begin inline asm
	{ atom.add.f64 %fd859,[%rd360],%fd878; }

	// end inline asm
	add.s64 	%rd361, %rd358, 24;
	// begin inline asm
	{ atom.add.f64 %fd861,[%rd361],%fd878; }

	// end inline asm
	add.s64 	%rd362, %rd358, 32;
	// begin inline asm
	{ atom.add.f64 %fd863,[%rd362],%fd878; }

	// end inline asm
	add.s64 	%rd363, %rd358, 40;
	// begin inline asm
	{ atom.add.f64 %fd865,[%rd363],%fd878; }

	// end inline asm
	add.s64 	%rd364, %rd358, 48;
	// begin inline asm
	{ atom.add.f64 %fd867,[%rd364],%fd878; }

	// end inline asm
	add.s64 	%rd365, %rd358, 56;
	// begin inline asm
	{ atom.add.f64 %fd869,[%rd365],%fd878; }

	// end inline asm
	add.s64 	%rd366, %rd358, 64;
	// begin inline asm
	{ atom.add.f64 %fd871,[%rd366],%fd878; }

	// end inline asm
	add.s64 	%rd367, %rd358, 72;
	// begin inline asm
	{ atom.add.f64 %fd873,[%rd367],%fd878; }

	// end inline asm
	add.s64 	%rd368, %rd358, 80;
	// begin inline asm
	{ atom.add.f64 %fd875,[%rd368],%fd878; }

	// end inline asm
	add.s64 	%rd369, %rd358, 88;
	// begin inline asm
	{ atom.add.f64 %fd877,[%rd369],%fd878; }

	// end inline asm

$L__BB21_68:
	add.f64 	%fd110, %fd107, %fd109;
	@%p14 bra 	$L__BB21_70;

	mul.lo.s64 	%rd382, %rd48, %rd29;
	add.s64 	%rd370, %rd69, %rd382;
	// begin inline asm
	{ atom.add.f64 %fd879,[%rd370],%fd106; }

	// end inline asm
	add.s64 	%rd371, %rd370, 8;
	mov.f64 	%fd902, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd881,[%rd371],%fd902; }

	// end inline asm
	add.s64 	%rd372, %rd370, 16;
	// begin inline asm
	{ atom.add.f64 %fd883,[%rd372],%fd902; }

	// end inline asm
	add.s64 	%rd373, %rd370, 24;
	// begin inline asm
	{ atom.add.f64 %fd885,[%rd373],%fd902; }

	// end inline asm
	add.s64 	%rd374, %rd370, 32;
	// begin inline asm
	{ atom.add.f64 %fd887,[%rd374],%fd902; }

	// end inline asm
	add.s64 	%rd375, %rd370, 40;
	// begin inline asm
	{ atom.add.f64 %fd889,[%rd375],%fd902; }

	// end inline asm
	add.s64 	%rd376, %rd370, 48;
	// begin inline asm
	{ atom.add.f64 %fd891,[%rd376],%fd902; }

	// end inline asm
	add.s64 	%rd377, %rd370, 56;
	// begin inline asm
	{ atom.add.f64 %fd893,[%rd377],%fd902; }

	// end inline asm
	add.s64 	%rd378, %rd370, 64;
	// begin inline asm
	{ atom.add.f64 %fd895,[%rd378],%fd902; }

	// end inline asm
	add.s64 	%rd379, %rd370, 72;
	// begin inline asm
	{ atom.add.f64 %fd897,[%rd379],%fd902; }

	// end inline asm
	add.s64 	%rd380, %rd370, 80;
	// begin inline asm
	{ atom.add.f64 %fd899,[%rd380],%fd902; }

	// end inline asm
	add.s64 	%rd381, %rd370, 88;
	// begin inline asm
	{ atom.add.f64 %fd901,[%rd381],%fd902; }

	// end inline asm
	bra.uni 	$L__BB21_72;

$L__BB21_70:
	setp.eq.s64 	%p37, %rd62, 0;
	@%p37 bra 	$L__BB21_72;

	add.s64 	%rd383, %rd62, %rd50;
	// begin inline asm
	{ atom.add.f64 %fd903,[%rd383],%fd106; }

	// end inline asm
	add.s64 	%rd384, %rd383, 8;
	mov.f64 	%fd926, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd905,[%rd384],%fd926; }

	// end inline asm
	add.s64 	%rd385, %rd383, 16;
	// begin inline asm
	{ atom.add.f64 %fd907,[%rd385],%fd926; }

	// end inline asm
	add.s64 	%rd386, %rd383, 24;
	// begin inline asm
	{ atom.add.f64 %fd909,[%rd386],%fd926; }

	// end inline asm
	add.s64 	%rd387, %rd383, 32;
	// begin inline asm
	{ atom.add.f64 %fd911,[%rd387],%fd926; }

	// end inline asm
	add.s64 	%rd388, %rd383, 40;
	// begin inline asm
	{ atom.add.f64 %fd913,[%rd388],%fd926; }

	// end inline asm
	add.s64 	%rd389, %rd383, 48;
	// begin inline asm
	{ atom.add.f64 %fd915,[%rd389],%fd926; }

	// end inline asm
	add.s64 	%rd390, %rd383, 56;
	// begin inline asm
	{ atom.add.f64 %fd917,[%rd390],%fd926; }

	// end inline asm
	add.s64 	%rd391, %rd383, 64;
	// begin inline asm
	{ atom.add.f64 %fd919,[%rd391],%fd926; }

	// end inline asm
	add.s64 	%rd392, %rd383, 72;
	// begin inline asm
	{ atom.add.f64 %fd921,[%rd392],%fd926; }

	// end inline asm
	add.s64 	%rd393, %rd383, 80;
	// begin inline asm
	{ atom.add.f64 %fd923,[%rd393],%fd926; }

	// end inline asm
	add.s64 	%rd394, %rd383, 88;
	// begin inline asm
	{ atom.add.f64 %fd925,[%rd394],%fd926; }

	// end inline asm

$L__BB21_72:
	setp.eq.s64 	%p38, %rd67, 0;
	add.f64 	%fd111, %fd106, %fd110;
	add.f64 	%fd112, %fd93, 0d0000000000000000;
	add.f64 	%fd113, %fd71, 0d0000000000000000;
	add.f64 	%fd114, %fd72, 0d0000000000000000;
	@%p38 bra 	$L__BB21_74;

	mul.lo.s64 	%rd407, %rd48, %rd30;
	add.s64 	%rd395, %rd67, %rd407;
	mov.f64 	%fd948, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd927,[%rd395],%fd948; }

	// end inline asm
	add.s64 	%rd396, %rd395, 8;
	// begin inline asm
	{ atom.add.f64 %fd929,[%rd396],%fd948; }

	// end inline asm
	add.s64 	%rd397, %rd395, 16;
	// begin inline asm
	{ atom.add.f64 %fd931,[%rd397],%fd948; }

	// end inline asm
	add.s64 	%rd398, %rd395, 24;
	// begin inline asm
	{ atom.add.f64 %fd933,[%rd398],%fd948; }

	// end inline asm
	add.s64 	%rd399, %rd395, 32;
	// begin inline asm
	{ atom.add.f64 %fd935,[%rd399],%fd948; }

	// end inline asm
	add.s64 	%rd400, %rd395, 40;
	// begin inline asm
	{ atom.add.f64 %fd937,[%rd400],%fd948; }

	// end inline asm
	add.s64 	%rd401, %rd395, 48;
	// begin inline asm
	{ atom.add.f64 %fd939,[%rd401],%fd948; }

	// end inline asm
	add.s64 	%rd402, %rd395, 56;
	// begin inline asm
	{ atom.add.f64 %fd941,[%rd402],%fd948; }

	// end inline asm
	add.s64 	%rd403, %rd395, 64;
	// begin inline asm
	{ atom.add.f64 %fd943,[%rd403],%fd948; }

	// end inline asm
	add.s64 	%rd404, %rd395, 72;
	// begin inline asm
	{ atom.add.f64 %fd945,[%rd404],%fd948; }

	// end inline asm
	add.s64 	%rd405, %rd395, 80;
	// begin inline asm
	{ atom.add.f64 %fd947,[%rd405],%fd948; }

	// end inline asm
	add.s64 	%rd406, %rd395, 88;
	// begin inline asm
	{ atom.add.f64 %fd949,[%rd406],%fd114; }

	// end inline asm
	bra.uni 	$L__BB21_76;

$L__BB21_74:
	setp.eq.s64 	%p39, %rd60, 0;
	@%p39 bra 	$L__BB21_76;

	add.s64 	%rd408, %rd60, %rd49;
	mov.f64 	%fd972, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd951,[%rd408],%fd972; }

	// end inline asm
	add.s64 	%rd409, %rd408, 8;
	// begin inline asm
	{ atom.add.f64 %fd953,[%rd409],%fd972; }

	// end inline asm
	add.s64 	%rd410, %rd408, 16;
	// begin inline asm
	{ atom.add.f64 %fd955,[%rd410],%fd972; }

	// end inline asm
	add.s64 	%rd411, %rd408, 24;
	// begin inline asm
	{ atom.add.f64 %fd957,[%rd411],%fd972; }

	// end inline asm
	add.s64 	%rd412, %rd408, 32;
	// begin inline asm
	{ atom.add.f64 %fd959,[%rd412],%fd972; }

	// end inline asm
	add.s64 	%rd413, %rd408, 40;
	// begin inline asm
	{ atom.add.f64 %fd961,[%rd413],%fd972; }

	// end inline asm
	add.s64 	%rd414, %rd408, 48;
	// begin inline asm
	{ atom.add.f64 %fd963,[%rd414],%fd972; }

	// end inline asm
	add.s64 	%rd415, %rd408, 56;
	// begin inline asm
	{ atom.add.f64 %fd965,[%rd415],%fd972; }

	// end inline asm
	add.s64 	%rd416, %rd408, 64;
	// begin inline asm
	{ atom.add.f64 %fd967,[%rd416],%fd972; }

	// end inline asm
	add.s64 	%rd417, %rd408, 72;
	// begin inline asm
	{ atom.add.f64 %fd969,[%rd417],%fd972; }

	// end inline asm
	add.s64 	%rd418, %rd408, 80;
	// begin inline asm
	{ atom.add.f64 %fd971,[%rd418],%fd972; }

	// end inline asm
	add.s64 	%rd419, %rd408, 88;
	// begin inline asm
	{ atom.add.f64 %fd973,[%rd419],%fd114; }

	// end inline asm

$L__BB21_76:
	@%p38 bra 	$L__BB21_78;

	mul.lo.s64 	%rd432, %rd48, %rd30;
	add.s64 	%rd420, %rd67, %rd432;
	mov.f64 	%fd998, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd975,[%rd420],%fd998; }

	// end inline asm
	add.s64 	%rd421, %rd420, 8;
	// begin inline asm
	{ atom.add.f64 %fd977,[%rd421],%fd998; }

	// end inline asm
	add.s64 	%rd422, %rd420, 16;
	// begin inline asm
	{ atom.add.f64 %fd979,[%rd422],%fd998; }

	// end inline asm
	add.s64 	%rd423, %rd420, 24;
	// begin inline asm
	{ atom.add.f64 %fd981,[%rd423],%fd998; }

	// end inline asm
	add.s64 	%rd424, %rd420, 32;
	// begin inline asm
	{ atom.add.f64 %fd983,[%rd424],%fd998; }

	// end inline asm
	add.s64 	%rd425, %rd420, 40;
	// begin inline asm
	{ atom.add.f64 %fd985,[%rd425],%fd998; }

	// end inline asm
	add.s64 	%rd426, %rd420, 48;
	// begin inline asm
	{ atom.add.f64 %fd987,[%rd426],%fd998; }

	// end inline asm
	add.s64 	%rd427, %rd420, 56;
	// begin inline asm
	{ atom.add.f64 %fd989,[%rd427],%fd998; }

	// end inline asm
	add.s64 	%rd428, %rd420, 64;
	// begin inline asm
	{ atom.add.f64 %fd991,[%rd428],%fd998; }

	// end inline asm
	add.s64 	%rd429, %rd420, 72;
	// begin inline asm
	{ atom.add.f64 %fd993,[%rd429],%fd998; }

	// end inline asm
	add.s64 	%rd430, %rd420, 80;
	// begin inline asm
	{ atom.add.f64 %fd995,[%rd430],%fd113; }

	// end inline asm
	add.s64 	%rd431, %rd420, 88;
	// begin inline asm
	{ atom.add.f64 %fd997,[%rd431],%fd998; }

	// end inline asm
	bra.uni 	$L__BB21_80;

$L__BB21_78:
	setp.eq.s64 	%p41, %rd60, 0;
	@%p41 bra 	$L__BB21_80;

	add.s64 	%rd433, %rd60, %rd49;
	mov.f64 	%fd1022, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd999,[%rd433],%fd1022; }

	// end inline asm
	add.s64 	%rd434, %rd433, 8;
	// begin inline asm
	{ atom.add.f64 %fd1001,[%rd434],%fd1022; }

	// end inline asm
	add.s64 	%rd435, %rd433, 16;
	// begin inline asm
	{ atom.add.f64 %fd1003,[%rd435],%fd1022; }

	// end inline asm
	add.s64 	%rd436, %rd433, 24;
	// begin inline asm
	{ atom.add.f64 %fd1005,[%rd436],%fd1022; }

	// end inline asm
	add.s64 	%rd437, %rd433, 32;
	// begin inline asm
	{ atom.add.f64 %fd1007,[%rd437],%fd1022; }

	// end inline asm
	add.s64 	%rd438, %rd433, 40;
	// begin inline asm
	{ atom.add.f64 %fd1009,[%rd438],%fd1022; }

	// end inline asm
	add.s64 	%rd439, %rd433, 48;
	// begin inline asm
	{ atom.add.f64 %fd1011,[%rd439],%fd1022; }

	// end inline asm
	add.s64 	%rd440, %rd433, 56;
	// begin inline asm
	{ atom.add.f64 %fd1013,[%rd440],%fd1022; }

	// end inline asm
	add.s64 	%rd441, %rd433, 64;
	// begin inline asm
	{ atom.add.f64 %fd1015,[%rd441],%fd1022; }

	// end inline asm
	add.s64 	%rd442, %rd433, 72;
	// begin inline asm
	{ atom.add.f64 %fd1017,[%rd442],%fd1022; }

	// end inline asm
	add.s64 	%rd443, %rd433, 80;
	// begin inline asm
	{ atom.add.f64 %fd1019,[%rd443],%fd113; }

	// end inline asm
	add.s64 	%rd444, %rd433, 88;
	// begin inline asm
	{ atom.add.f64 %fd1021,[%rd444],%fd1022; }

	// end inline asm

$L__BB21_80:
	@%p38 bra 	$L__BB21_82;

	mul.lo.s64 	%rd457, %rd48, %rd30;
	add.s64 	%rd445, %rd67, %rd457;
	mov.f64 	%fd1046, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1023,[%rd445],%fd1046; }

	// end inline asm
	add.s64 	%rd446, %rd445, 8;
	// begin inline asm
	{ atom.add.f64 %fd1025,[%rd446],%fd1046; }

	// end inline asm
	add.s64 	%rd447, %rd445, 16;
	// begin inline asm
	{ atom.add.f64 %fd1027,[%rd447],%fd1046; }

	// end inline asm
	add.s64 	%rd448, %rd445, 24;
	// begin inline asm
	{ atom.add.f64 %fd1029,[%rd448],%fd1046; }

	// end inline asm
	add.s64 	%rd449, %rd445, 32;
	// begin inline asm
	{ atom.add.f64 %fd1031,[%rd449],%fd1046; }

	// end inline asm
	add.s64 	%rd450, %rd445, 40;
	// begin inline asm
	{ atom.add.f64 %fd1033,[%rd450],%fd1046; }

	// end inline asm
	add.s64 	%rd451, %rd445, 48;
	// begin inline asm
	{ atom.add.f64 %fd1035,[%rd451],%fd1046; }

	// end inline asm
	add.s64 	%rd452, %rd445, 56;
	// begin inline asm
	{ atom.add.f64 %fd1037,[%rd452],%fd1046; }

	// end inline asm
	add.s64 	%rd453, %rd445, 64;
	// begin inline asm
	{ atom.add.f64 %fd1039,[%rd453],%fd1046; }

	// end inline asm
	add.s64 	%rd454, %rd445, 72;
	// begin inline asm
	{ atom.add.f64 %fd1041,[%rd454],%fd112; }

	// end inline asm
	add.s64 	%rd455, %rd445, 80;
	// begin inline asm
	{ atom.add.f64 %fd1043,[%rd455],%fd1046; }

	// end inline asm
	add.s64 	%rd456, %rd445, 88;
	// begin inline asm
	{ atom.add.f64 %fd1045,[%rd456],%fd1046; }

	// end inline asm
	bra.uni 	$L__BB21_84;

$L__BB21_82:
	setp.eq.s64 	%p43, %rd60, 0;
	@%p43 bra 	$L__BB21_84;

	add.s64 	%rd458, %rd60, %rd49;
	mov.f64 	%fd1070, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1047,[%rd458],%fd1070; }

	// end inline asm
	add.s64 	%rd459, %rd458, 8;
	// begin inline asm
	{ atom.add.f64 %fd1049,[%rd459],%fd1070; }

	// end inline asm
	add.s64 	%rd460, %rd458, 16;
	// begin inline asm
	{ atom.add.f64 %fd1051,[%rd460],%fd1070; }

	// end inline asm
	add.s64 	%rd461, %rd458, 24;
	// begin inline asm
	{ atom.add.f64 %fd1053,[%rd461],%fd1070; }

	// end inline asm
	add.s64 	%rd462, %rd458, 32;
	// begin inline asm
	{ atom.add.f64 %fd1055,[%rd462],%fd1070; }

	// end inline asm
	add.s64 	%rd463, %rd458, 40;
	// begin inline asm
	{ atom.add.f64 %fd1057,[%rd463],%fd1070; }

	// end inline asm
	add.s64 	%rd464, %rd458, 48;
	// begin inline asm
	{ atom.add.f64 %fd1059,[%rd464],%fd1070; }

	// end inline asm
	add.s64 	%rd465, %rd458, 56;
	// begin inline asm
	{ atom.add.f64 %fd1061,[%rd465],%fd1070; }

	// end inline asm
	add.s64 	%rd466, %rd458, 64;
	// begin inline asm
	{ atom.add.f64 %fd1063,[%rd466],%fd1070; }

	// end inline asm
	add.s64 	%rd467, %rd458, 72;
	// begin inline asm
	{ atom.add.f64 %fd1065,[%rd467],%fd112; }

	// end inline asm
	add.s64 	%rd468, %rd458, 80;
	// begin inline asm
	{ atom.add.f64 %fd1067,[%rd468],%fd1070; }

	// end inline asm
	add.s64 	%rd469, %rd458, 88;
	// begin inline asm
	{ atom.add.f64 %fd1069,[%rd469],%fd1070; }

	// end inline asm

$L__BB21_84:
	add.f64 	%fd115, %fd99, 0d0000000000000000;
	add.f64 	%fd116, %fd74, 0d0000000000000000;
	add.f64 	%fd117, %fd75, 0d0000000000000000;
	@%p38 bra 	$L__BB21_86;

	mul.lo.s64 	%rd482, %rd48, %rd30;
	add.s64 	%rd470, %rd67, %rd482;
	mov.f64 	%fd1094, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1071,[%rd470],%fd1094; }

	// end inline asm
	add.s64 	%rd471, %rd470, 8;
	// begin inline asm
	{ atom.add.f64 %fd1073,[%rd471],%fd1094; }

	// end inline asm
	add.s64 	%rd472, %rd470, 16;
	// begin inline asm
	{ atom.add.f64 %fd1075,[%rd472],%fd1094; }

	// end inline asm
	add.s64 	%rd473, %rd470, 24;
	// begin inline asm
	{ atom.add.f64 %fd1077,[%rd473],%fd1094; }

	// end inline asm
	add.s64 	%rd474, %rd470, 32;
	// begin inline asm
	{ atom.add.f64 %fd1079,[%rd474],%fd1094; }

	// end inline asm
	add.s64 	%rd475, %rd470, 40;
	// begin inline asm
	{ atom.add.f64 %fd1081,[%rd475],%fd1094; }

	// end inline asm
	add.s64 	%rd476, %rd470, 48;
	// begin inline asm
	{ atom.add.f64 %fd1083,[%rd476],%fd1094; }

	// end inline asm
	add.s64 	%rd477, %rd470, 56;
	// begin inline asm
	{ atom.add.f64 %fd1085,[%rd477],%fd1094; }

	// end inline asm
	add.s64 	%rd478, %rd470, 64;
	// begin inline asm
	{ atom.add.f64 %fd1087,[%rd478],%fd117; }

	// end inline asm
	add.s64 	%rd479, %rd470, 72;
	// begin inline asm
	{ atom.add.f64 %fd1089,[%rd479],%fd1094; }

	// end inline asm
	add.s64 	%rd480, %rd470, 80;
	// begin inline asm
	{ atom.add.f64 %fd1091,[%rd480],%fd1094; }

	// end inline asm
	add.s64 	%rd481, %rd470, 88;
	// begin inline asm
	{ atom.add.f64 %fd1093,[%rd481],%fd1094; }

	// end inline asm
	bra.uni 	$L__BB21_88;

$L__BB21_86:
	setp.eq.s64 	%p45, %rd60, 0;
	@%p45 bra 	$L__BB21_88;

	add.s64 	%rd483, %rd60, %rd49;
	mov.f64 	%fd1118, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1095,[%rd483],%fd1118; }

	// end inline asm
	add.s64 	%rd484, %rd483, 8;
	// begin inline asm
	{ atom.add.f64 %fd1097,[%rd484],%fd1118; }

	// end inline asm
	add.s64 	%rd485, %rd483, 16;
	// begin inline asm
	{ atom.add.f64 %fd1099,[%rd485],%fd1118; }

	// end inline asm
	add.s64 	%rd486, %rd483, 24;
	// begin inline asm
	{ atom.add.f64 %fd1101,[%rd486],%fd1118; }

	// end inline asm
	add.s64 	%rd487, %rd483, 32;
	// begin inline asm
	{ atom.add.f64 %fd1103,[%rd487],%fd1118; }

	// end inline asm
	add.s64 	%rd488, %rd483, 40;
	// begin inline asm
	{ atom.add.f64 %fd1105,[%rd488],%fd1118; }

	// end inline asm
	add.s64 	%rd489, %rd483, 48;
	// begin inline asm
	{ atom.add.f64 %fd1107,[%rd489],%fd1118; }

	// end inline asm
	add.s64 	%rd490, %rd483, 56;
	// begin inline asm
	{ atom.add.f64 %fd1109,[%rd490],%fd1118; }

	// end inline asm
	add.s64 	%rd491, %rd483, 64;
	// begin inline asm
	{ atom.add.f64 %fd1111,[%rd491],%fd117; }

	// end inline asm
	add.s64 	%rd492, %rd483, 72;
	// begin inline asm
	{ atom.add.f64 %fd1113,[%rd492],%fd1118; }

	// end inline asm
	add.s64 	%rd493, %rd483, 80;
	// begin inline asm
	{ atom.add.f64 %fd1115,[%rd493],%fd1118; }

	// end inline asm
	add.s64 	%rd494, %rd483, 88;
	// begin inline asm
	{ atom.add.f64 %fd1117,[%rd494],%fd1118; }

	// end inline asm

$L__BB21_88:
	@%p38 bra 	$L__BB21_90;

	mul.lo.s64 	%rd507, %rd48, %rd30;
	add.s64 	%rd495, %rd67, %rd507;
	mov.f64 	%fd1142, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1119,[%rd495],%fd1142; }

	// end inline asm
	add.s64 	%rd496, %rd495, 8;
	// begin inline asm
	{ atom.add.f64 %fd1121,[%rd496],%fd1142; }

	// end inline asm
	add.s64 	%rd497, %rd495, 16;
	// begin inline asm
	{ atom.add.f64 %fd1123,[%rd497],%fd1142; }

	// end inline asm
	add.s64 	%rd498, %rd495, 24;
	// begin inline asm
	{ atom.add.f64 %fd1125,[%rd498],%fd1142; }

	// end inline asm
	add.s64 	%rd499, %rd495, 32;
	// begin inline asm
	{ atom.add.f64 %fd1127,[%rd499],%fd1142; }

	// end inline asm
	add.s64 	%rd500, %rd495, 40;
	// begin inline asm
	{ atom.add.f64 %fd1129,[%rd500],%fd1142; }

	// end inline asm
	add.s64 	%rd501, %rd495, 48;
	// begin inline asm
	{ atom.add.f64 %fd1131,[%rd501],%fd1142; }

	// end inline asm
	add.s64 	%rd502, %rd495, 56;
	// begin inline asm
	{ atom.add.f64 %fd1133,[%rd502],%fd116; }

	// end inline asm
	add.s64 	%rd503, %rd495, 64;
	// begin inline asm
	{ atom.add.f64 %fd1135,[%rd503],%fd1142; }

	// end inline asm
	add.s64 	%rd504, %rd495, 72;
	// begin inline asm
	{ atom.add.f64 %fd1137,[%rd504],%fd1142; }

	// end inline asm
	add.s64 	%rd505, %rd495, 80;
	// begin inline asm
	{ atom.add.f64 %fd1139,[%rd505],%fd1142; }

	// end inline asm
	add.s64 	%rd506, %rd495, 88;
	// begin inline asm
	{ atom.add.f64 %fd1141,[%rd506],%fd1142; }

	// end inline asm
	bra.uni 	$L__BB21_92;

$L__BB21_90:
	setp.eq.s64 	%p47, %rd60, 0;
	@%p47 bra 	$L__BB21_92;

	add.s64 	%rd508, %rd60, %rd49;
	mov.f64 	%fd1166, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1143,[%rd508],%fd1166; }

	// end inline asm
	add.s64 	%rd509, %rd508, 8;
	// begin inline asm
	{ atom.add.f64 %fd1145,[%rd509],%fd1166; }

	// end inline asm
	add.s64 	%rd510, %rd508, 16;
	// begin inline asm
	{ atom.add.f64 %fd1147,[%rd510],%fd1166; }

	// end inline asm
	add.s64 	%rd511, %rd508, 24;
	// begin inline asm
	{ atom.add.f64 %fd1149,[%rd511],%fd1166; }

	// end inline asm
	add.s64 	%rd512, %rd508, 32;
	// begin inline asm
	{ atom.add.f64 %fd1151,[%rd512],%fd1166; }

	// end inline asm
	add.s64 	%rd513, %rd508, 40;
	// begin inline asm
	{ atom.add.f64 %fd1153,[%rd513],%fd1166; }

	// end inline asm
	add.s64 	%rd514, %rd508, 48;
	// begin inline asm
	{ atom.add.f64 %fd1155,[%rd514],%fd1166; }

	// end inline asm
	add.s64 	%rd515, %rd508, 56;
	// begin inline asm
	{ atom.add.f64 %fd1157,[%rd515],%fd116; }

	// end inline asm
	add.s64 	%rd516, %rd508, 64;
	// begin inline asm
	{ atom.add.f64 %fd1159,[%rd516],%fd1166; }

	// end inline asm
	add.s64 	%rd517, %rd508, 72;
	// begin inline asm
	{ atom.add.f64 %fd1161,[%rd517],%fd1166; }

	// end inline asm
	add.s64 	%rd518, %rd508, 80;
	// begin inline asm
	{ atom.add.f64 %fd1163,[%rd518],%fd1166; }

	// end inline asm
	add.s64 	%rd519, %rd508, 88;
	// begin inline asm
	{ atom.add.f64 %fd1165,[%rd519],%fd1166; }

	// end inline asm

$L__BB21_92:
	@%p38 bra 	$L__BB21_94;

	mul.lo.s64 	%rd532, %rd48, %rd30;
	add.s64 	%rd520, %rd67, %rd532;
	mov.f64 	%fd1190, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1167,[%rd520],%fd1190; }

	// end inline asm
	add.s64 	%rd521, %rd520, 8;
	// begin inline asm
	{ atom.add.f64 %fd1169,[%rd521],%fd1190; }

	// end inline asm
	add.s64 	%rd522, %rd520, 16;
	// begin inline asm
	{ atom.add.f64 %fd1171,[%rd522],%fd1190; }

	// end inline asm
	add.s64 	%rd523, %rd520, 24;
	// begin inline asm
	{ atom.add.f64 %fd1173,[%rd523],%fd1190; }

	// end inline asm
	add.s64 	%rd524, %rd520, 32;
	// begin inline asm
	{ atom.add.f64 %fd1175,[%rd524],%fd1190; }

	// end inline asm
	add.s64 	%rd525, %rd520, 40;
	// begin inline asm
	{ atom.add.f64 %fd1177,[%rd525],%fd1190; }

	// end inline asm
	add.s64 	%rd526, %rd520, 48;
	// begin inline asm
	{ atom.add.f64 %fd1179,[%rd526],%fd115; }

	// end inline asm
	add.s64 	%rd527, %rd520, 56;
	// begin inline asm
	{ atom.add.f64 %fd1181,[%rd527],%fd1190; }

	// end inline asm
	add.s64 	%rd528, %rd520, 64;
	// begin inline asm
	{ atom.add.f64 %fd1183,[%rd528],%fd1190; }

	// end inline asm
	add.s64 	%rd529, %rd520, 72;
	// begin inline asm
	{ atom.add.f64 %fd1185,[%rd529],%fd1190; }

	// end inline asm
	add.s64 	%rd530, %rd520, 80;
	// begin inline asm
	{ atom.add.f64 %fd1187,[%rd530],%fd1190; }

	// end inline asm
	add.s64 	%rd531, %rd520, 88;
	// begin inline asm
	{ atom.add.f64 %fd1189,[%rd531],%fd1190; }

	// end inline asm
	bra.uni 	$L__BB21_96;

$L__BB21_94:
	setp.eq.s64 	%p49, %rd60, 0;
	@%p49 bra 	$L__BB21_96;

	add.s64 	%rd533, %rd60, %rd49;
	mov.f64 	%fd1214, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1191,[%rd533],%fd1214; }

	// end inline asm
	add.s64 	%rd534, %rd533, 8;
	// begin inline asm
	{ atom.add.f64 %fd1193,[%rd534],%fd1214; }

	// end inline asm
	add.s64 	%rd535, %rd533, 16;
	// begin inline asm
	{ atom.add.f64 %fd1195,[%rd535],%fd1214; }

	// end inline asm
	add.s64 	%rd536, %rd533, 24;
	// begin inline asm
	{ atom.add.f64 %fd1197,[%rd536],%fd1214; }

	// end inline asm
	add.s64 	%rd537, %rd533, 32;
	// begin inline asm
	{ atom.add.f64 %fd1199,[%rd537],%fd1214; }

	// end inline asm
	add.s64 	%rd538, %rd533, 40;
	// begin inline asm
	{ atom.add.f64 %fd1201,[%rd538],%fd1214; }

	// end inline asm
	add.s64 	%rd539, %rd533, 48;
	// begin inline asm
	{ atom.add.f64 %fd1203,[%rd539],%fd115; }

	// end inline asm
	add.s64 	%rd540, %rd533, 56;
	// begin inline asm
	{ atom.add.f64 %fd1205,[%rd540],%fd1214; }

	// end inline asm
	add.s64 	%rd541, %rd533, 64;
	// begin inline asm
	{ atom.add.f64 %fd1207,[%rd541],%fd1214; }

	// end inline asm
	add.s64 	%rd542, %rd533, 72;
	// begin inline asm
	{ atom.add.f64 %fd1209,[%rd542],%fd1214; }

	// end inline asm
	add.s64 	%rd543, %rd533, 80;
	// begin inline asm
	{ atom.add.f64 %fd1211,[%rd543],%fd1214; }

	// end inline asm
	add.s64 	%rd544, %rd533, 88;
	// begin inline asm
	{ atom.add.f64 %fd1213,[%rd544],%fd1214; }

	// end inline asm

$L__BB21_96:
	add.f64 	%fd118, %fd105, 0d0000000000000000;
	add.f64 	%fd119, %fd77, 0d0000000000000000;
	add.f64 	%fd120, %fd78, 0d0000000000000000;
	@%p38 bra 	$L__BB21_98;

	mul.lo.s64 	%rd557, %rd48, %rd30;
	add.s64 	%rd545, %rd67, %rd557;
	mov.f64 	%fd1238, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1215,[%rd545],%fd1238; }

	// end inline asm
	add.s64 	%rd546, %rd545, 8;
	// begin inline asm
	{ atom.add.f64 %fd1217,[%rd546],%fd1238; }

	// end inline asm
	add.s64 	%rd547, %rd545, 16;
	// begin inline asm
	{ atom.add.f64 %fd1219,[%rd547],%fd1238; }

	// end inline asm
	add.s64 	%rd548, %rd545, 24;
	// begin inline asm
	{ atom.add.f64 %fd1221,[%rd548],%fd1238; }

	// end inline asm
	add.s64 	%rd549, %rd545, 32;
	// begin inline asm
	{ atom.add.f64 %fd1223,[%rd549],%fd1238; }

	// end inline asm
	add.s64 	%rd550, %rd545, 40;
	// begin inline asm
	{ atom.add.f64 %fd1225,[%rd550],%fd120; }

	// end inline asm
	add.s64 	%rd551, %rd545, 48;
	// begin inline asm
	{ atom.add.f64 %fd1227,[%rd551],%fd1238; }

	// end inline asm
	add.s64 	%rd552, %rd545, 56;
	// begin inline asm
	{ atom.add.f64 %fd1229,[%rd552],%fd1238; }

	// end inline asm
	add.s64 	%rd553, %rd545, 64;
	// begin inline asm
	{ atom.add.f64 %fd1231,[%rd553],%fd1238; }

	// end inline asm
	add.s64 	%rd554, %rd545, 72;
	// begin inline asm
	{ atom.add.f64 %fd1233,[%rd554],%fd1238; }

	// end inline asm
	add.s64 	%rd555, %rd545, 80;
	// begin inline asm
	{ atom.add.f64 %fd1235,[%rd555],%fd1238; }

	// end inline asm
	add.s64 	%rd556, %rd545, 88;
	// begin inline asm
	{ atom.add.f64 %fd1237,[%rd556],%fd1238; }

	// end inline asm
	bra.uni 	$L__BB21_100;

$L__BB21_98:
	setp.eq.s64 	%p51, %rd60, 0;
	@%p51 bra 	$L__BB21_100;

	add.s64 	%rd558, %rd60, %rd49;
	mov.f64 	%fd1262, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1239,[%rd558],%fd1262; }

	// end inline asm
	add.s64 	%rd559, %rd558, 8;
	// begin inline asm
	{ atom.add.f64 %fd1241,[%rd559],%fd1262; }

	// end inline asm
	add.s64 	%rd560, %rd558, 16;
	// begin inline asm
	{ atom.add.f64 %fd1243,[%rd560],%fd1262; }

	// end inline asm
	add.s64 	%rd561, %rd558, 24;
	// begin inline asm
	{ atom.add.f64 %fd1245,[%rd561],%fd1262; }

	// end inline asm
	add.s64 	%rd562, %rd558, 32;
	// begin inline asm
	{ atom.add.f64 %fd1247,[%rd562],%fd1262; }

	// end inline asm
	add.s64 	%rd563, %rd558, 40;
	// begin inline asm
	{ atom.add.f64 %fd1249,[%rd563],%fd120; }

	// end inline asm
	add.s64 	%rd564, %rd558, 48;
	// begin inline asm
	{ atom.add.f64 %fd1251,[%rd564],%fd1262; }

	// end inline asm
	add.s64 	%rd565, %rd558, 56;
	// begin inline asm
	{ atom.add.f64 %fd1253,[%rd565],%fd1262; }

	// end inline asm
	add.s64 	%rd566, %rd558, 64;
	// begin inline asm
	{ atom.add.f64 %fd1255,[%rd566],%fd1262; }

	// end inline asm
	add.s64 	%rd567, %rd558, 72;
	// begin inline asm
	{ atom.add.f64 %fd1257,[%rd567],%fd1262; }

	// end inline asm
	add.s64 	%rd568, %rd558, 80;
	// begin inline asm
	{ atom.add.f64 %fd1259,[%rd568],%fd1262; }

	// end inline asm
	add.s64 	%rd569, %rd558, 88;
	// begin inline asm
	{ atom.add.f64 %fd1261,[%rd569],%fd1262; }

	// end inline asm

$L__BB21_100:
	@%p38 bra 	$L__BB21_102;

	mul.lo.s64 	%rd582, %rd48, %rd30;
	add.s64 	%rd570, %rd67, %rd582;
	mov.f64 	%fd1286, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1263,[%rd570],%fd1286; }

	// end inline asm
	add.s64 	%rd571, %rd570, 8;
	// begin inline asm
	{ atom.add.f64 %fd1265,[%rd571],%fd1286; }

	// end inline asm
	add.s64 	%rd572, %rd570, 16;
	// begin inline asm
	{ atom.add.f64 %fd1267,[%rd572],%fd1286; }

	// end inline asm
	add.s64 	%rd573, %rd570, 24;
	// begin inline asm
	{ atom.add.f64 %fd1269,[%rd573],%fd1286; }

	// end inline asm
	add.s64 	%rd574, %rd570, 32;
	// begin inline asm
	{ atom.add.f64 %fd1271,[%rd574],%fd119; }

	// end inline asm
	add.s64 	%rd575, %rd570, 40;
	// begin inline asm
	{ atom.add.f64 %fd1273,[%rd575],%fd1286; }

	// end inline asm
	add.s64 	%rd576, %rd570, 48;
	// begin inline asm
	{ atom.add.f64 %fd1275,[%rd576],%fd1286; }

	// end inline asm
	add.s64 	%rd577, %rd570, 56;
	// begin inline asm
	{ atom.add.f64 %fd1277,[%rd577],%fd1286; }

	// end inline asm
	add.s64 	%rd578, %rd570, 64;
	// begin inline asm
	{ atom.add.f64 %fd1279,[%rd578],%fd1286; }

	// end inline asm
	add.s64 	%rd579, %rd570, 72;
	// begin inline asm
	{ atom.add.f64 %fd1281,[%rd579],%fd1286; }

	// end inline asm
	add.s64 	%rd580, %rd570, 80;
	// begin inline asm
	{ atom.add.f64 %fd1283,[%rd580],%fd1286; }

	// end inline asm
	add.s64 	%rd581, %rd570, 88;
	// begin inline asm
	{ atom.add.f64 %fd1285,[%rd581],%fd1286; }

	// end inline asm
	bra.uni 	$L__BB21_104;

$L__BB21_102:
	setp.eq.s64 	%p53, %rd60, 0;
	@%p53 bra 	$L__BB21_104;

	add.s64 	%rd583, %rd60, %rd49;
	mov.f64 	%fd1310, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1287,[%rd583],%fd1310; }

	// end inline asm
	add.s64 	%rd584, %rd583, 8;
	// begin inline asm
	{ atom.add.f64 %fd1289,[%rd584],%fd1310; }

	// end inline asm
	add.s64 	%rd585, %rd583, 16;
	// begin inline asm
	{ atom.add.f64 %fd1291,[%rd585],%fd1310; }

	// end inline asm
	add.s64 	%rd586, %rd583, 24;
	// begin inline asm
	{ atom.add.f64 %fd1293,[%rd586],%fd1310; }

	// end inline asm
	add.s64 	%rd587, %rd583, 32;
	// begin inline asm
	{ atom.add.f64 %fd1295,[%rd587],%fd119; }

	// end inline asm
	add.s64 	%rd588, %rd583, 40;
	// begin inline asm
	{ atom.add.f64 %fd1297,[%rd588],%fd1310; }

	// end inline asm
	add.s64 	%rd589, %rd583, 48;
	// begin inline asm
	{ atom.add.f64 %fd1299,[%rd589],%fd1310; }

	// end inline asm
	add.s64 	%rd590, %rd583, 56;
	// begin inline asm
	{ atom.add.f64 %fd1301,[%rd590],%fd1310; }

	// end inline asm
	add.s64 	%rd591, %rd583, 64;
	// begin inline asm
	{ atom.add.f64 %fd1303,[%rd591],%fd1310; }

	// end inline asm
	add.s64 	%rd592, %rd583, 72;
	// begin inline asm
	{ atom.add.f64 %fd1305,[%rd592],%fd1310; }

	// end inline asm
	add.s64 	%rd593, %rd583, 80;
	// begin inline asm
	{ atom.add.f64 %fd1307,[%rd593],%fd1310; }

	// end inline asm
	add.s64 	%rd594, %rd583, 88;
	// begin inline asm
	{ atom.add.f64 %fd1309,[%rd594],%fd1310; }

	// end inline asm

$L__BB21_104:
	@%p38 bra 	$L__BB21_106;

	mul.lo.s64 	%rd607, %rd48, %rd30;
	add.s64 	%rd595, %rd67, %rd607;
	mov.f64 	%fd1334, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1311,[%rd595],%fd1334; }

	// end inline asm
	add.s64 	%rd596, %rd595, 8;
	// begin inline asm
	{ atom.add.f64 %fd1313,[%rd596],%fd1334; }

	// end inline asm
	add.s64 	%rd597, %rd595, 16;
	// begin inline asm
	{ atom.add.f64 %fd1315,[%rd597],%fd1334; }

	// end inline asm
	add.s64 	%rd598, %rd595, 24;
	// begin inline asm
	{ atom.add.f64 %fd1317,[%rd598],%fd118; }

	// end inline asm
	add.s64 	%rd599, %rd595, 32;
	// begin inline asm
	{ atom.add.f64 %fd1319,[%rd599],%fd1334; }

	// end inline asm
	add.s64 	%rd600, %rd595, 40;
	// begin inline asm
	{ atom.add.f64 %fd1321,[%rd600],%fd1334; }

	// end inline asm
	add.s64 	%rd601, %rd595, 48;
	// begin inline asm
	{ atom.add.f64 %fd1323,[%rd601],%fd1334; }

	// end inline asm
	add.s64 	%rd602, %rd595, 56;
	// begin inline asm
	{ atom.add.f64 %fd1325,[%rd602],%fd1334; }

	// end inline asm
	add.s64 	%rd603, %rd595, 64;
	// begin inline asm
	{ atom.add.f64 %fd1327,[%rd603],%fd1334; }

	// end inline asm
	add.s64 	%rd604, %rd595, 72;
	// begin inline asm
	{ atom.add.f64 %fd1329,[%rd604],%fd1334; }

	// end inline asm
	add.s64 	%rd605, %rd595, 80;
	// begin inline asm
	{ atom.add.f64 %fd1331,[%rd605],%fd1334; }

	// end inline asm
	add.s64 	%rd606, %rd595, 88;
	// begin inline asm
	{ atom.add.f64 %fd1333,[%rd606],%fd1334; }

	// end inline asm
	bra.uni 	$L__BB21_108;

$L__BB21_106:
	setp.eq.s64 	%p55, %rd60, 0;
	@%p55 bra 	$L__BB21_108;

	add.s64 	%rd608, %rd60, %rd49;
	mov.f64 	%fd1358, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1335,[%rd608],%fd1358; }

	// end inline asm
	add.s64 	%rd609, %rd608, 8;
	// begin inline asm
	{ atom.add.f64 %fd1337,[%rd609],%fd1358; }

	// end inline asm
	add.s64 	%rd610, %rd608, 16;
	// begin inline asm
	{ atom.add.f64 %fd1339,[%rd610],%fd1358; }

	// end inline asm
	add.s64 	%rd611, %rd608, 24;
	// begin inline asm
	{ atom.add.f64 %fd1341,[%rd611],%fd118; }

	// end inline asm
	add.s64 	%rd612, %rd608, 32;
	// begin inline asm
	{ atom.add.f64 %fd1343,[%rd612],%fd1358; }

	// end inline asm
	add.s64 	%rd613, %rd608, 40;
	// begin inline asm
	{ atom.add.f64 %fd1345,[%rd613],%fd1358; }

	// end inline asm
	add.s64 	%rd614, %rd608, 48;
	// begin inline asm
	{ atom.add.f64 %fd1347,[%rd614],%fd1358; }

	// end inline asm
	add.s64 	%rd615, %rd608, 56;
	// begin inline asm
	{ atom.add.f64 %fd1349,[%rd615],%fd1358; }

	// end inline asm
	add.s64 	%rd616, %rd608, 64;
	// begin inline asm
	{ atom.add.f64 %fd1351,[%rd616],%fd1358; }

	// end inline asm
	add.s64 	%rd617, %rd608, 72;
	// begin inline asm
	{ atom.add.f64 %fd1353,[%rd617],%fd1358; }

	// end inline asm
	add.s64 	%rd618, %rd608, 80;
	// begin inline asm
	{ atom.add.f64 %fd1355,[%rd618],%fd1358; }

	// end inline asm
	add.s64 	%rd619, %rd608, 88;
	// begin inline asm
	{ atom.add.f64 %fd1357,[%rd619],%fd1358; }

	// end inline asm

$L__BB21_108:
	add.f64 	%fd121, %fd111, 0d0000000000000000;
	add.f64 	%fd122, %fd80, 0d0000000000000000;
	add.f64 	%fd123, %fd81, 0d0000000000000000;
	@%p38 bra 	$L__BB21_110;

	mul.lo.s64 	%rd632, %rd48, %rd30;
	add.s64 	%rd620, %rd67, %rd632;
	mov.f64 	%fd1382, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1359,[%rd620],%fd1382; }

	// end inline asm
	add.s64 	%rd621, %rd620, 8;
	// begin inline asm
	{ atom.add.f64 %fd1361,[%rd621],%fd1382; }

	// end inline asm
	add.s64 	%rd622, %rd620, 16;
	// begin inline asm
	{ atom.add.f64 %fd1363,[%rd622],%fd123; }

	// end inline asm
	add.s64 	%rd623, %rd620, 24;
	// begin inline asm
	{ atom.add.f64 %fd1365,[%rd623],%fd1382; }

	// end inline asm
	add.s64 	%rd624, %rd620, 32;
	// begin inline asm
	{ atom.add.f64 %fd1367,[%rd624],%fd1382; }

	// end inline asm
	add.s64 	%rd625, %rd620, 40;
	// begin inline asm
	{ atom.add.f64 %fd1369,[%rd625],%fd1382; }

	// end inline asm
	add.s64 	%rd626, %rd620, 48;
	// begin inline asm
	{ atom.add.f64 %fd1371,[%rd626],%fd1382; }

	// end inline asm
	add.s64 	%rd627, %rd620, 56;
	// begin inline asm
	{ atom.add.f64 %fd1373,[%rd627],%fd1382; }

	// end inline asm
	add.s64 	%rd628, %rd620, 64;
	// begin inline asm
	{ atom.add.f64 %fd1375,[%rd628],%fd1382; }

	// end inline asm
	add.s64 	%rd629, %rd620, 72;
	// begin inline asm
	{ atom.add.f64 %fd1377,[%rd629],%fd1382; }

	// end inline asm
	add.s64 	%rd630, %rd620, 80;
	// begin inline asm
	{ atom.add.f64 %fd1379,[%rd630],%fd1382; }

	// end inline asm
	add.s64 	%rd631, %rd620, 88;
	// begin inline asm
	{ atom.add.f64 %fd1381,[%rd631],%fd1382; }

	// end inline asm
	bra.uni 	$L__BB21_112;

$L__BB21_110:
	setp.eq.s64 	%p57, %rd60, 0;
	@%p57 bra 	$L__BB21_112;

	add.s64 	%rd633, %rd60, %rd49;
	mov.f64 	%fd1406, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1383,[%rd633],%fd1406; }

	// end inline asm
	add.s64 	%rd634, %rd633, 8;
	// begin inline asm
	{ atom.add.f64 %fd1385,[%rd634],%fd1406; }

	// end inline asm
	add.s64 	%rd635, %rd633, 16;
	// begin inline asm
	{ atom.add.f64 %fd1387,[%rd635],%fd123; }

	// end inline asm
	add.s64 	%rd636, %rd633, 24;
	// begin inline asm
	{ atom.add.f64 %fd1389,[%rd636],%fd1406; }

	// end inline asm
	add.s64 	%rd637, %rd633, 32;
	// begin inline asm
	{ atom.add.f64 %fd1391,[%rd637],%fd1406; }

	// end inline asm
	add.s64 	%rd638, %rd633, 40;
	// begin inline asm
	{ atom.add.f64 %fd1393,[%rd638],%fd1406; }

	// end inline asm
	add.s64 	%rd639, %rd633, 48;
	// begin inline asm
	{ atom.add.f64 %fd1395,[%rd639],%fd1406; }

	// end inline asm
	add.s64 	%rd640, %rd633, 56;
	// begin inline asm
	{ atom.add.f64 %fd1397,[%rd640],%fd1406; }

	// end inline asm
	add.s64 	%rd641, %rd633, 64;
	// begin inline asm
	{ atom.add.f64 %fd1399,[%rd641],%fd1406; }

	// end inline asm
	add.s64 	%rd642, %rd633, 72;
	// begin inline asm
	{ atom.add.f64 %fd1401,[%rd642],%fd1406; }

	// end inline asm
	add.s64 	%rd643, %rd633, 80;
	// begin inline asm
	{ atom.add.f64 %fd1403,[%rd643],%fd1406; }

	// end inline asm
	add.s64 	%rd644, %rd633, 88;
	// begin inline asm
	{ atom.add.f64 %fd1405,[%rd644],%fd1406; }

	// end inline asm

$L__BB21_112:
	@%p38 bra 	$L__BB21_114;

	mul.lo.s64 	%rd657, %rd48, %rd30;
	add.s64 	%rd645, %rd67, %rd657;
	mov.f64 	%fd1430, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1407,[%rd645],%fd1430; }

	// end inline asm
	add.s64 	%rd646, %rd645, 8;
	// begin inline asm
	{ atom.add.f64 %fd1409,[%rd646],%fd122; }

	// end inline asm
	add.s64 	%rd647, %rd645, 16;
	// begin inline asm
	{ atom.add.f64 %fd1411,[%rd647],%fd1430; }

	// end inline asm
	add.s64 	%rd648, %rd645, 24;
	// begin inline asm
	{ atom.add.f64 %fd1413,[%rd648],%fd1430; }

	// end inline asm
	add.s64 	%rd649, %rd645, 32;
	// begin inline asm
	{ atom.add.f64 %fd1415,[%rd649],%fd1430; }

	// end inline asm
	add.s64 	%rd650, %rd645, 40;
	// begin inline asm
	{ atom.add.f64 %fd1417,[%rd650],%fd1430; }

	// end inline asm
	add.s64 	%rd651, %rd645, 48;
	// begin inline asm
	{ atom.add.f64 %fd1419,[%rd651],%fd1430; }

	// end inline asm
	add.s64 	%rd652, %rd645, 56;
	// begin inline asm
	{ atom.add.f64 %fd1421,[%rd652],%fd1430; }

	// end inline asm
	add.s64 	%rd653, %rd645, 64;
	// begin inline asm
	{ atom.add.f64 %fd1423,[%rd653],%fd1430; }

	// end inline asm
	add.s64 	%rd654, %rd645, 72;
	// begin inline asm
	{ atom.add.f64 %fd1425,[%rd654],%fd1430; }

	// end inline asm
	add.s64 	%rd655, %rd645, 80;
	// begin inline asm
	{ atom.add.f64 %fd1427,[%rd655],%fd1430; }

	// end inline asm
	add.s64 	%rd656, %rd645, 88;
	// begin inline asm
	{ atom.add.f64 %fd1429,[%rd656],%fd1430; }

	// end inline asm
	bra.uni 	$L__BB21_116;

$L__BB21_114:
	setp.eq.s64 	%p59, %rd60, 0;
	@%p59 bra 	$L__BB21_116;

	add.s64 	%rd658, %rd60, %rd49;
	mov.f64 	%fd1454, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1431,[%rd658],%fd1454; }

	// end inline asm
	add.s64 	%rd659, %rd658, 8;
	// begin inline asm
	{ atom.add.f64 %fd1433,[%rd659],%fd122; }

	// end inline asm
	add.s64 	%rd660, %rd658, 16;
	// begin inline asm
	{ atom.add.f64 %fd1435,[%rd660],%fd1454; }

	// end inline asm
	add.s64 	%rd661, %rd658, 24;
	// begin inline asm
	{ atom.add.f64 %fd1437,[%rd661],%fd1454; }

	// end inline asm
	add.s64 	%rd662, %rd658, 32;
	// begin inline asm
	{ atom.add.f64 %fd1439,[%rd662],%fd1454; }

	// end inline asm
	add.s64 	%rd663, %rd658, 40;
	// begin inline asm
	{ atom.add.f64 %fd1441,[%rd663],%fd1454; }

	// end inline asm
	add.s64 	%rd664, %rd658, 48;
	// begin inline asm
	{ atom.add.f64 %fd1443,[%rd664],%fd1454; }

	// end inline asm
	add.s64 	%rd665, %rd658, 56;
	// begin inline asm
	{ atom.add.f64 %fd1445,[%rd665],%fd1454; }

	// end inline asm
	add.s64 	%rd666, %rd658, 64;
	// begin inline asm
	{ atom.add.f64 %fd1447,[%rd666],%fd1454; }

	// end inline asm
	add.s64 	%rd667, %rd658, 72;
	// begin inline asm
	{ atom.add.f64 %fd1449,[%rd667],%fd1454; }

	// end inline asm
	add.s64 	%rd668, %rd658, 80;
	// begin inline asm
	{ atom.add.f64 %fd1451,[%rd668],%fd1454; }

	// end inline asm
	add.s64 	%rd669, %rd658, 88;
	// begin inline asm
	{ atom.add.f64 %fd1453,[%rd669],%fd1454; }

	// end inline asm

$L__BB21_116:
	@%p38 bra 	$L__BB21_118;

	mul.lo.s64 	%rd682, %rd48, %rd30;
	add.s64 	%rd670, %rd67, %rd682;
	// begin inline asm
	{ atom.add.f64 %fd1455,[%rd670],%fd121; }

	// end inline asm
	add.s64 	%rd671, %rd670, 8;
	mov.f64 	%fd1478, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1457,[%rd671],%fd1478; }

	// end inline asm
	add.s64 	%rd672, %rd670, 16;
	// begin inline asm
	{ atom.add.f64 %fd1459,[%rd672],%fd1478; }

	// end inline asm
	add.s64 	%rd673, %rd670, 24;
	// begin inline asm
	{ atom.add.f64 %fd1461,[%rd673],%fd1478; }

	// end inline asm
	add.s64 	%rd674, %rd670, 32;
	// begin inline asm
	{ atom.add.f64 %fd1463,[%rd674],%fd1478; }

	// end inline asm
	add.s64 	%rd675, %rd670, 40;
	// begin inline asm
	{ atom.add.f64 %fd1465,[%rd675],%fd1478; }

	// end inline asm
	add.s64 	%rd676, %rd670, 48;
	// begin inline asm
	{ atom.add.f64 %fd1467,[%rd676],%fd1478; }

	// end inline asm
	add.s64 	%rd677, %rd670, 56;
	// begin inline asm
	{ atom.add.f64 %fd1469,[%rd677],%fd1478; }

	// end inline asm
	add.s64 	%rd678, %rd670, 64;
	// begin inline asm
	{ atom.add.f64 %fd1471,[%rd678],%fd1478; }

	// end inline asm
	add.s64 	%rd679, %rd670, 72;
	// begin inline asm
	{ atom.add.f64 %fd1473,[%rd679],%fd1478; }

	// end inline asm
	add.s64 	%rd680, %rd670, 80;
	// begin inline asm
	{ atom.add.f64 %fd1475,[%rd680],%fd1478; }

	// end inline asm
	add.s64 	%rd681, %rd670, 88;
	// begin inline asm
	{ atom.add.f64 %fd1477,[%rd681],%fd1478; }

	// end inline asm
	bra.uni 	$L__BB21_120;

$L__BB21_118:
	setp.eq.s64 	%p61, %rd60, 0;
	@%p61 bra 	$L__BB21_120;

	add.s64 	%rd683, %rd60, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd1479,[%rd683],%fd121; }

	// end inline asm
	add.s64 	%rd684, %rd683, 8;
	mov.f64 	%fd1502, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1481,[%rd684],%fd1502; }

	// end inline asm
	add.s64 	%rd685, %rd683, 16;
	// begin inline asm
	{ atom.add.f64 %fd1483,[%rd685],%fd1502; }

	// end inline asm
	add.s64 	%rd686, %rd683, 24;
	// begin inline asm
	{ atom.add.f64 %fd1485,[%rd686],%fd1502; }

	// end inline asm
	add.s64 	%rd687, %rd683, 32;
	// begin inline asm
	{ atom.add.f64 %fd1487,[%rd687],%fd1502; }

	// end inline asm
	add.s64 	%rd688, %rd683, 40;
	// begin inline asm
	{ atom.add.f64 %fd1489,[%rd688],%fd1502; }

	// end inline asm
	add.s64 	%rd689, %rd683, 48;
	// begin inline asm
	{ atom.add.f64 %fd1491,[%rd689],%fd1502; }

	// end inline asm
	add.s64 	%rd690, %rd683, 56;
	// begin inline asm
	{ atom.add.f64 %fd1493,[%rd690],%fd1502; }

	// end inline asm
	add.s64 	%rd691, %rd683, 64;
	// begin inline asm
	{ atom.add.f64 %fd1495,[%rd691],%fd1502; }

	// end inline asm
	add.s64 	%rd692, %rd683, 72;
	// begin inline asm
	{ atom.add.f64 %fd1497,[%rd692],%fd1502; }

	// end inline asm
	add.s64 	%rd693, %rd683, 80;
	// begin inline asm
	{ atom.add.f64 %fd1499,[%rd693],%fd1502; }

	// end inline asm
	add.s64 	%rd694, %rd683, 88;
	// begin inline asm
	{ atom.add.f64 %fd1501,[%rd694],%fd1502; }

	// end inline asm

$L__BB21_120:
	setp.eq.s64 	%p62, %rd71, 0;
	add.f64 	%fd124, %fd84, 0d0000000000000000;
	add.f64 	%fd125, %fd83, 0d0000000000000000;
	add.f64 	%fd126, %fd82, 0d0000000000000000;
	@%p62 bra 	$L__BB21_122;

	mul.lo.s64 	%rd698, %rd46, %rd31;
	add.s64 	%rd695, %rd71, %rd698;
	// begin inline asm
	{ atom.add.f64 %fd1503,[%rd695],%fd124; }

	// end inline asm
	add.s64 	%rd696, %rd695, 8;
	// begin inline asm
	{ atom.add.f64 %fd1505,[%rd696],%fd125; }

	// end inline asm
	add.s64 	%rd697, %rd695, 16;
	// begin inline asm
	{ atom.add.f64 %fd1507,[%rd697],%fd126; }

	// end inline asm
	bra.uni 	$L__BB21_124;

$L__BB21_122:
	setp.eq.s64 	%p63, %rd64, 0;
	@%p63 bra 	$L__BB21_124;

	mul.lo.s64 	%rd712, %rd46, %rd710;
	add.s64 	%rd699, %rd64, %rd712;
	// begin inline asm
	{ atom.add.f64 %fd1509,[%rd699],%fd124; }

	// end inline asm
	add.s64 	%rd700, %rd699, 8;
	// begin inline asm
	{ atom.add.f64 %fd1511,[%rd700],%fd125; }

	// end inline asm
	add.s64 	%rd701, %rd699, 16;
	// begin inline asm
	{ atom.add.f64 %fd1513,[%rd701],%fd126; }

	// end inline asm

$L__BB21_124:
	add.f64 	%fd127, %fd87, 0d0000000000000000;
	add.f64 	%fd128, %fd86, 0d0000000000000000;
	add.f64 	%fd129, %fd85, 0d0000000000000000;
	@%p62 bra 	$L__BB21_126;

	mul.lo.s64 	%rd705, %rd44, %rd31;
	add.s64 	%rd702, %rd71, %rd705;
	// begin inline asm
	{ atom.add.f64 %fd1515,[%rd702],%fd127; }

	// end inline asm
	add.s64 	%rd703, %rd702, 8;
	// begin inline asm
	{ atom.add.f64 %fd1517,[%rd703],%fd128; }

	// end inline asm
	add.s64 	%rd704, %rd702, 16;
	// begin inline asm
	{ atom.add.f64 %fd1519,[%rd704],%fd129; }

	// end inline asm
	bra.uni 	$L__BB21_128;

$L__BB21_126:
	setp.eq.s64 	%p65, %rd64, 0;
	@%p65 bra 	$L__BB21_128;

	mul.lo.s64 	%rd711, %rd44, %rd710;
	add.s64 	%rd706, %rd64, %rd711;
	// begin inline asm
	{ atom.add.f64 %fd1521,[%rd706],%fd127; }

	// end inline asm
	add.s64 	%rd707, %rd706, 8;
	// begin inline asm
	{ atom.add.f64 %fd1523,[%rd707],%fd128; }

	// end inline asm
	add.s64 	%rd708, %rd706, 16;
	// begin inline asm
	{ atom.add.f64 %fd1525,[%rd708],%fd129; }

	// end inline asm

$L__BB21_128:
	ld.param.u64 	%rd709, [clamp_search_direction_cuda_kernel_backward_param_0+24];
	add.s64 	%rd715, %rd715, %rd32;
	setp.lt.u64 	%p66, %rd715, %rd709;
	@%p66 bra 	$L__BB21_2;

$L__BB21_129:
	ret;

}
	// .globl	sys_to_x_soft_cuda_kernel_forward
.visible .entry sys_to_x_soft_cuda_kernel_forward(
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_forward_param_2[56],
	.param .u32 sys_to_x_soft_cuda_kernel_forward_param_3,
	.param .u32 sys_to_x_soft_cuda_kernel_forward_param_4
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<82>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<83>;


	ld.param.v2.u32 	{%r28, %r29}, [sys_to_x_soft_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r30, %r31}, [sys_to_x_soft_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r36, %r37}, [sys_to_x_soft_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r44, %r45}, [sys_to_x_soft_cuda_kernel_forward_param_2+32];
	ld.param.u32 	%r26, [sys_to_x_soft_cuda_kernel_forward_param_3];
	ld.param.u32 	%r27, [sys_to_x_soft_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd39, [sys_to_x_soft_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [sys_to_x_soft_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd36, [sys_to_x_soft_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [sys_to_x_soft_cuda_kernel_forward_param_0+16];
	mov.u32 	%r48, %ntid.x;
	cvt.u64.u32 	%rd1, %r48;
	mov.u32 	%r49, %ctaid.x;
	mul.wide.u32 	%rd41, %r48, %r49;
	mov.u32 	%r50, %tid.x;
	cvt.u64.u32 	%rd42, %r50;
	add.s64 	%rd74, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd74, %rd36;
	@%p1 bra 	$L__BB22_31;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvt.s64.s32 	%rd6, %r31;
	cvt.s64.s32 	%rd7, %r30;
	cvt.s64.s32 	%rd8, %r29;
	shl.b32 	%r2, %r27, 2;
	cvt.s64.s32 	%rd9, %r36;
	cvt.s64.s32 	%rd10, %r44;
	mov.u32 	%r51, %nctaid.x;
	cvt.u64.u32 	%rd43, %r51;
	mul.lo.s64 	%rd11, %rd1, %rd43;
	setp.gt.s32 	%p2, %r7, 3;
	@%p2 bra 	$L__BB22_18;
	bra.uni 	$L__BB22_2;

$L__BB22_18:
	cvt.u32.u64 	%r69, %rd6;
	cvt.u32.u64 	%r72, %rd7;

$L__BB22_19:
	or.b64  	%rd62, %rd74, %rd6;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB22_21;

	div.u64 	%rd81, %rd74, %rd6;
	bra.uni 	$L__BB22_22;

$L__BB22_21:
	cvt.u32.u64 	%r70, %rd74;
	div.u32 	%r71, %r70, %r69;
	cvt.u64.u32 	%rd81, %r71;

$L__BB22_22:
	setp.lt.s32 	%p14, %r7, 3;
	@%p14 bra 	$L__BB22_26;

	or.b64  	%rd64, %rd81, %rd7;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB22_25;

	div.u64 	%rd81, %rd81, %rd7;
	bra.uni 	$L__BB22_26;

$L__BB22_25:
	cvt.u32.u64 	%r73, %rd81;
	div.u32 	%r74, %r73, %r72;
	cvt.u64.u32 	%rd81, %r74;

$L__BB22_26:
	setp.lt.s32 	%p16, %r7, 2;
	@%p16 bra 	$L__BB22_30;

	or.b64  	%rd66, %rd81, %rd8;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p17, %rd67, 0;
	@%p17 bra 	$L__BB22_29;

	div.u64 	%rd81, %rd81, %rd8;
	bra.uni 	$L__BB22_30;

$L__BB22_29:
	cvt.u32.u64 	%r75, %rd8;
	cvt.u32.u64 	%r76, %rd81;
	div.u32 	%r77, %r76, %r75;
	cvt.u64.u32 	%rd81, %r77;

$L__BB22_30:
	cvt.u32.u64 	%r78, %rd81;
	setp.gt.s32 	%p18, %r7, 0;
	selp.b32 	%r79, %r78, 0, %p18;
	add.s32 	%r80, %r79, %r2;
	cvt.s64.s32 	%rd68, %r80;
	mul.lo.s64 	%rd69, %rd68, %rd9;
	add.s64 	%rd70, %rd5, %rd69;
	add.s32 	%r81, %r79, %r26;
	ld.global.f64 	%fd7, [%rd70];
	ld.global.f64 	%fd8, [%rd70+8];
	ld.global.f64 	%fd9, [%rd70+16];
	cvt.s64.s32 	%rd71, %r81;
	mul.lo.s64 	%rd72, %rd71, %rd10;
	add.s64 	%rd73, %rd4, %rd72;
	st.global.f64 	[%rd73], %fd7;
	st.global.f64 	[%rd73+8], %fd8;
	st.global.f64 	[%rd73+16], %fd9;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p19, %rd74, %rd36;
	@%p19 bra 	$L__BB22_19;
	bra.uni 	$L__BB22_31;

$L__BB22_2:
	setp.gt.s32 	%p3, %r7, 2;
	@%p3 bra 	$L__BB22_9;
	bra.uni 	$L__BB22_3;

$L__BB22_9:
	cvt.u32.u64 	%r59, %rd7;
	cvt.u32.u64 	%r62, %rd8;

$L__BB22_10:
	or.b64  	%rd52, %rd74, %rd7;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p8, %rd53, 0;
	@%p8 bra 	$L__BB22_12;

	div.u64 	%rd78, %rd74, %rd7;
	bra.uni 	$L__BB22_13;

$L__BB22_12:
	cvt.u32.u64 	%r60, %rd74;
	div.u32 	%r61, %r60, %r59;
	cvt.u64.u32 	%rd78, %r61;

$L__BB22_13:
	setp.lt.s32 	%p9, %r7, 2;
	@%p9 bra 	$L__BB22_17;

	or.b64  	%rd54, %rd78, %rd8;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p10, %rd55, 0;
	@%p10 bra 	$L__BB22_16;

	div.u64 	%rd78, %rd78, %rd8;
	bra.uni 	$L__BB22_17;

$L__BB22_16:
	cvt.u32.u64 	%r63, %rd78;
	div.u32 	%r64, %r63, %r62;
	cvt.u64.u32 	%rd78, %r64;

$L__BB22_17:
	cvt.u32.u64 	%r65, %rd78;
	setp.gt.s32 	%p11, %r7, 0;
	selp.b32 	%r66, %r65, 0, %p11;
	add.s32 	%r67, %r66, %r2;
	cvt.s64.s32 	%rd56, %r67;
	mul.lo.s64 	%rd57, %rd56, %rd9;
	add.s64 	%rd58, %rd5, %rd57;
	add.s32 	%r68, %r66, %r26;
	ld.global.f64 	%fd4, [%rd58];
	ld.global.f64 	%fd5, [%rd58+8];
	ld.global.f64 	%fd6, [%rd58+16];
	cvt.s64.s32 	%rd59, %r68;
	mul.lo.s64 	%rd60, %rd59, %rd10;
	add.s64 	%rd61, %rd4, %rd60;
	st.global.f64 	[%rd61], %fd4;
	st.global.f64 	[%rd61+8], %fd5;
	st.global.f64 	[%rd61+16], %fd6;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p12, %rd74, %rd36;
	@%p12 bra 	$L__BB22_10;
	bra.uni 	$L__BB22_31;

$L__BB22_3:
	cvt.u32.u64 	%r52, %rd8;

$L__BB22_4:
	setp.lt.s32 	%p4, %r7, 2;
	mov.u64 	%rd75, %rd74;
	@%p4 bra 	$L__BB22_8;

	or.b64  	%rd44, %rd74, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB22_7;

	div.u64 	%rd75, %rd74, %rd8;
	bra.uni 	$L__BB22_8;

$L__BB22_7:
	cvt.u32.u64 	%r53, %rd74;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd75, %r54;

$L__BB22_8:
	cvt.u32.u64 	%r55, %rd75;
	setp.gt.s32 	%p6, %r7, 0;
	selp.b32 	%r56, %r55, 0, %p6;
	add.s32 	%r57, %r56, %r2;
	cvt.s64.s32 	%rd46, %r57;
	mul.lo.s64 	%rd47, %rd46, %rd9;
	add.s64 	%rd48, %rd5, %rd47;
	add.s32 	%r58, %r56, %r26;
	ld.global.f64 	%fd1, [%rd48];
	ld.global.f64 	%fd2, [%rd48+8];
	ld.global.f64 	%fd3, [%rd48+16];
	cvt.s64.s32 	%rd49, %r58;
	mul.lo.s64 	%rd50, %rd49, %rd10;
	add.s64 	%rd51, %rd4, %rd50;
	st.global.f64 	[%rd51], %fd1;
	st.global.f64 	[%rd51+8], %fd2;
	st.global.f64 	[%rd51+16], %fd3;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p7, %rd74, %rd36;
	@%p7 bra 	$L__BB22_4;

$L__BB22_31:
	ret;

}
	// .globl	sys_to_x_soft_cuda_kernel_backward
.visible .entry sys_to_x_soft_cuda_kernel_backward(
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_2[56],
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_3,
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_4,
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 sys_to_x_soft_cuda_kernel_backward_param_6[56],
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_7,
	.param .u32 sys_to_x_soft_cuda_kernel_backward_param_8
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<99>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r48, %r49}, [sys_to_x_soft_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r50, %r51}, [sys_to_x_soft_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r56, %r57}, [sys_to_x_soft_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r64, %r65}, [sys_to_x_soft_cuda_kernel_backward_param_2+32];
	ld.param.u32 	%r28, [sys_to_x_soft_cuda_kernel_backward_param_3];
	ld.param.u32 	%r29, [sys_to_x_soft_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r72, %r73}, [sys_to_x_soft_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r80, %r81}, [sys_to_x_soft_cuda_kernel_backward_param_6+32];
	ld.param.u64 	%rd36, [sys_to_x_soft_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd34, [sys_to_x_soft_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd33, [sys_to_x_soft_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [sys_to_x_soft_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [sys_to_x_soft_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r9, [sys_to_x_soft_cuda_kernel_backward_param_0+16];
	mov.u32 	%r84, %ntid.x;
	cvt.u64.u32 	%rd1, %r84;
	mov.u32 	%r85, %ctaid.x;
	mul.wide.u32 	%rd38, %r84, %r85;
	mov.u32 	%r86, %tid.x;
	cvt.u64.u32 	%rd39, %r86;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB23_23;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r51;
	cvt.s64.s32 	%rd11, %r50;
	cvt.s64.s32 	%rd12, %r49;
	shl.b32 	%r2, %r29, 2;
	cvt.s64.s32 	%rd13, %r80;
	cvt.s64.s32 	%rd14, %r64;
	mov.u32 	%r87, %nctaid.x;
	cvt.u64.u32 	%rd40, %r87;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r56;

$L__BB23_2:
	setp.lt.s32 	%p2, %r9, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB23_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB23_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB23_6;

$L__BB23_5:
	cvt.u32.u64 	%r88, %rd10;
	cvt.u32.u64 	%r89, %rd63;
	div.u32 	%r90, %r89, %r88;
	cvt.u64.u32 	%rd64, %r90;

$L__BB23_6:
	setp.lt.s32 	%p4, %r9, 3;
	@%p4 bra 	$L__BB23_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB23_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB23_10;

$L__BB23_9:
	cvt.u32.u64 	%r91, %rd11;
	cvt.u32.u64 	%r92, %rd64;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd64, %r93;

$L__BB23_10:
	setp.lt.s32 	%p6, %r9, 2;
	@%p6 bra 	$L__BB23_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB23_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB23_14;

$L__BB23_13:
	cvt.u32.u64 	%r94, %rd12;
	cvt.u32.u64 	%r95, %rd64;
	div.u32 	%r96, %r95, %r94;
	cvt.u64.u32 	%rd64, %r96;

$L__BB23_14:
	cvt.u32.u64 	%r97, %rd64;
	setp.gt.s32 	%p8, %r9, 0;
	selp.b32 	%r98, %r97, 0, %p8;
	add.s32 	%r3, %r98, %r2;
	add.s32 	%r4, %r98, %r28;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB23_16;

	cvt.s64.s32 	%rd47, %r4;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB23_18;

$L__BB23_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB23_18;

	cvt.s64.s32 	%rd50, %r4;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB23_18:
	setp.eq.s64 	%p11, %rd34, 0;
	@%p11 bra 	$L__BB23_20;

	cvt.s64.s32 	%rd56, %r3;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd34, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB23_22;

$L__BB23_20:
	setp.eq.s64 	%p12, %rd31, 0;
	@%p12 bra 	$L__BB23_22;

	cvt.s64.s32 	%rd61, %r3;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd31, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB23_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB23_2;

$L__BB23_23:
	ret;

}
	// .globl	init_affine_diag_hess_inds_kernel_cuda_kernel_forward
.visible .entry init_affine_diag_hess_inds_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_1[184]
)
{
	.local .align 8 .b8 	__local_depot24[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<122>;
	.reg .b32 	%r<107>;
	.reg .b64 	%rd<172>;


	mov.u64 	%SPL, __local_depot24;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r40, %r41}, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r42, %r43}, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0+8];
	mov.b64 	%rd26, init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_1;
	ld.param.u64 	%rd25, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r39, [init_affine_diag_hess_inds_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r44, %ntid.x;
	cvt.u64.u32 	%rd1, %r44;
	mov.u32 	%r45, %ctaid.x;
	mul.wide.u32 	%rd27, %r44, %r45;
	mov.u32 	%r46, %tid.x;
	cvt.u64.u32 	%rd28, %r46;
	add.s64 	%rd168, %rd27, %rd28;
	setp.ge.u64 	%p1, %rd168, %rd25;
	@%p1 bra 	$L__BB24_63;

	cvt.s64.s32 	%rd4, %r43;
	cvt.s64.s32 	%rd5, %r42;
	cvt.s64.s32 	%rd6, %r41;
	ld.param.v2.u32 	{%r47, %r48}, [%rd26+176];
	ld.param.u32 	%r6, [%rd26+172];
	ld.param.u64 	%rd30, [%rd26];
	cvta.to.global.u64 	%rd7, %rd30;
	ld.param.s32 	%rd8, [%rd26+32];
	ld.param.u64 	%rd31, [%rd26+56];
	cvta.to.global.u64 	%rd9, %rd31;
	ld.param.s32 	%rd10, [%rd26+88];
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd32, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd32;
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd12, %SPL, 0;

$L__BB24_2:
	setp.lt.s32 	%p2, %r39, 4;
	mov.u64 	%rd169, %rd168;
	@%p2 bra 	$L__BB24_6;

	or.b64  	%rd34, %rd168, %rd4;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p3, %rd35, 0;
	@%p3 bra 	$L__BB24_5;

	div.u64 	%rd169, %rd168, %rd4;
	bra.uni 	$L__BB24_6;

$L__BB24_5:
	cvt.u32.u64 	%r50, %rd4;
	cvt.u32.u64 	%r51, %rd168;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd169, %r52;

$L__BB24_6:
	setp.lt.s32 	%p4, %r39, 3;
	@%p4 bra 	$L__BB24_10;

	or.b64  	%rd36, %rd169, %rd5;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p5, %rd37, 0;
	@%p5 bra 	$L__BB24_9;

	div.u64 	%rd169, %rd169, %rd5;
	bra.uni 	$L__BB24_10;

$L__BB24_9:
	cvt.u32.u64 	%r53, %rd5;
	cvt.u32.u64 	%r54, %rd169;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd169, %r55;

$L__BB24_10:
	setp.lt.s32 	%p6, %r39, 2;
	@%p6 bra 	$L__BB24_14;

	or.b64  	%rd38, %rd169, %rd6;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p7, %rd39, 0;
	@%p7 bra 	$L__BB24_13;

	div.u64 	%rd169, %rd169, %rd6;
	bra.uni 	$L__BB24_14;

$L__BB24_13:
	cvt.u32.u64 	%r56, %rd6;
	cvt.u32.u64 	%r57, %rd169;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd169, %r58;

$L__BB24_14:
	cvt.u32.u64 	%r59, %rd169;
	setp.gt.s32 	%p8, %r39, 0;
	selp.b32 	%r7, %r59, 0, %p8;
	shl.b32 	%r8, %r7, 4;
	shl.b32 	%r9, %r7, 2;
	setp.le.s32 	%p9, %r47, %r9;
	setp.le.s32 	%p10, %r48, %r9;
	setp.le.s32 	%p11, %r6, %r8;
	or.pred  	%p12, %p9, %p10;
	or.b32  	%r60, %r9, %r8;
	setp.lt.s32 	%p13, %r60, 0;
	or.pred  	%p14, %p13, %p12;
	or.pred  	%p15, %p11, %p14;
	@%p15 bra 	$L__BB24_16;
	bra.uni 	$L__BB24_15;

$L__BB24_16:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd45, $str;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r61, [retval0+0];
	} // callseq 0
	bra.uni 	$L__BB24_17;

$L__BB24_15:
	cvt.s64.s32 	%rd40, %r8;
	mul.lo.s64 	%rd41, %rd8, %rd40;
	add.s64 	%rd42, %rd7, %rd41;
	st.global.u32 	[%rd42], %r9;
	mul.lo.s64 	%rd43, %rd10, %rd40;
	add.s64 	%rd44, %rd9, %rd43;
	st.global.u32 	[%rd44], %r9;

$L__BB24_17:
	mul.lo.s32 	%r10, %r7, -12;
	add.s32 	%r11, %r8, 1;
	add.s32 	%r12, %r11, %r10;
	setp.le.s32 	%p17, %r48, %r12;
	setp.le.s32 	%p18, %r6, %r11;
	or.pred  	%p19, %p9, %p17;
	or.b32  	%r13, %r12, %r9;
	or.b32  	%r62, %r13, %r11;
	setp.lt.s32 	%p20, %r62, 0;
	or.pred  	%p21, %p20, %p19;
	or.pred  	%p22, %p18, %p21;
	@%p22 bra 	$L__BB24_19;
	bra.uni 	$L__BB24_18;

$L__BB24_19:
	st.local.v2.u32 	[%rd12], {%r9, %r12};
	add.s32 	%r92, %r8, 1;
	st.local.v2.u32 	[%rd12+8], {%r92, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r63, [retval0+0];
	} // callseq 1
	bra.uni 	$L__BB24_20;

$L__BB24_18:
	cvt.s64.s32 	%rd48, %r11;
	mul.lo.s64 	%rd49, %rd8, %rd48;
	add.s64 	%rd50, %rd7, %rd49;
	st.global.u32 	[%rd50], %r9;
	mul.lo.s64 	%rd51, %rd10, %rd48;
	add.s64 	%rd52, %rd9, %rd51;
	st.global.u32 	[%rd52], %r12;

$L__BB24_20:
	add.s32 	%r14, %r8, 2;
	add.s32 	%r15, %r14, %r10;
	setp.le.s32 	%p24, %r48, %r15;
	setp.le.s32 	%p25, %r6, %r14;
	or.pred  	%p26, %p9, %p24;
	or.b32  	%r16, %r15, %r9;
	or.b32  	%r64, %r16, %r14;
	setp.lt.s32 	%p27, %r64, 0;
	or.pred  	%p28, %p27, %p26;
	or.pred  	%p29, %p25, %p28;
	@%p29 bra 	$L__BB24_22;
	bra.uni 	$L__BB24_21;

$L__BB24_22:
	st.local.v2.u32 	[%rd12], {%r9, %r15};
	add.s32 	%r93, %r8, 2;
	st.local.v2.u32 	[%rd12+8], {%r93, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r65, [retval0+0];
	} // callseq 2
	bra.uni 	$L__BB24_23;

$L__BB24_21:
	cvt.s64.s32 	%rd56, %r14;
	mul.lo.s64 	%rd57, %rd8, %rd56;
	add.s64 	%rd58, %rd7, %rd57;
	st.global.u32 	[%rd58], %r9;
	mul.lo.s64 	%rd59, %rd10, %rd56;
	add.s64 	%rd60, %rd9, %rd59;
	st.global.u32 	[%rd60], %r15;

$L__BB24_23:
	add.s32 	%r17, %r8, 3;
	add.s32 	%r18, %r17, %r10;
	setp.le.s32 	%p31, %r48, %r18;
	setp.le.s32 	%p32, %r6, %r17;
	or.pred  	%p33, %p9, %p31;
	or.b32  	%r19, %r18, %r9;
	or.b32  	%r66, %r19, %r17;
	setp.lt.s32 	%p34, %r66, 0;
	or.pred  	%p35, %p34, %p33;
	or.pred  	%p36, %p32, %p35;
	@%p36 bra 	$L__BB24_25;
	bra.uni 	$L__BB24_24;

$L__BB24_25:
	st.local.v2.u32 	[%rd12], {%r9, %r18};
	add.s32 	%r94, %r8, 3;
	st.local.v2.u32 	[%rd12+8], {%r94, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd69, $str;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r67, [retval0+0];
	} // callseq 3
	bra.uni 	$L__BB24_26;

$L__BB24_24:
	cvt.s64.s32 	%rd64, %r17;
	mul.lo.s64 	%rd65, %rd8, %rd64;
	add.s64 	%rd66, %rd7, %rd65;
	st.global.u32 	[%rd66], %r9;
	mul.lo.s64 	%rd67, %rd10, %rd64;
	add.s64 	%rd68, %rd9, %rd67;
	st.global.u32 	[%rd68], %r18;

$L__BB24_26:
	add.s32 	%r20, %r8, 4;
	setp.le.s32 	%p38, %r6, %r20;
	setp.le.s32 	%p39, %r47, %r12;
	or.pred  	%p40, %p39, %p10;
	or.b32  	%r68, %r13, %r20;
	setp.lt.s32 	%p41, %r68, 0;
	or.pred  	%p42, %p41, %p40;
	or.pred  	%p43, %p38, %p42;
	@%p43 bra 	$L__BB24_28;
	bra.uni 	$L__BB24_27;

$L__BB24_28:
	st.local.v2.u32 	[%rd12], {%r12, %r9};
	add.s32 	%r95, %r8, 4;
	st.local.v2.u32 	[%rd12+8], {%r95, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd77, $str;
	cvta.global.u64 	%rd78, %rd77;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r69, [retval0+0];
	} // callseq 4
	bra.uni 	$L__BB24_29;

$L__BB24_27:
	cvt.s64.s32 	%rd72, %r20;
	mul.lo.s64 	%rd73, %rd8, %rd72;
	add.s64 	%rd74, %rd7, %rd73;
	st.global.u32 	[%rd74], %r12;
	mul.lo.s64 	%rd75, %rd10, %rd72;
	add.s64 	%rd76, %rd9, %rd75;
	st.global.u32 	[%rd76], %r9;

$L__BB24_29:
	add.s32 	%r21, %r8, 5;
	setp.le.s32 	%p45, %r6, %r21;
	or.pred  	%p47, %p39, %p17;
	or.b32  	%r70, %r21, %r12;
	setp.lt.s32 	%p48, %r70, 0;
	or.pred  	%p49, %p48, %p47;
	or.pred  	%p50, %p45, %p49;
	@%p50 bra 	$L__BB24_31;
	bra.uni 	$L__BB24_30;

$L__BB24_31:
	st.local.v2.u32 	[%rd12], {%r12, %r12};
	add.s32 	%r96, %r8, 5;
	st.local.v2.u32 	[%rd12+8], {%r96, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd85, $str;
	cvta.global.u64 	%rd86, %rd85;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r71, [retval0+0];
	} // callseq 5
	bra.uni 	$L__BB24_32;

$L__BB24_30:
	cvt.s64.s32 	%rd80, %r21;
	mul.lo.s64 	%rd81, %rd8, %rd80;
	add.s64 	%rd82, %rd7, %rd81;
	st.global.u32 	[%rd82], %r12;
	mul.lo.s64 	%rd83, %rd10, %rd80;
	add.s64 	%rd84, %rd9, %rd83;
	st.global.u32 	[%rd84], %r12;

$L__BB24_32:
	add.s32 	%r22, %r8, 6;
	setp.le.s32 	%p52, %r6, %r22;
	or.pred  	%p54, %p39, %p24;
	or.b32  	%r23, %r12, %r15;
	or.b32  	%r72, %r23, %r22;
	setp.lt.s32 	%p55, %r72, 0;
	or.pred  	%p56, %p55, %p54;
	or.pred  	%p57, %p52, %p56;
	@%p57 bra 	$L__BB24_34;
	bra.uni 	$L__BB24_33;

$L__BB24_34:
	st.local.v2.u32 	[%rd12], {%r12, %r15};
	add.s32 	%r97, %r8, 6;
	st.local.v2.u32 	[%rd12+8], {%r97, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd93, $str;
	cvta.global.u64 	%rd94, %rd93;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r73, [retval0+0];
	} // callseq 6
	bra.uni 	$L__BB24_35;

$L__BB24_33:
	cvt.s64.s32 	%rd88, %r22;
	mul.lo.s64 	%rd89, %rd8, %rd88;
	add.s64 	%rd90, %rd7, %rd89;
	st.global.u32 	[%rd90], %r12;
	mul.lo.s64 	%rd91, %rd10, %rd88;
	add.s64 	%rd92, %rd9, %rd91;
	st.global.u32 	[%rd92], %r15;

$L__BB24_35:
	add.s32 	%r24, %r8, 7;
	setp.le.s32 	%p59, %r6, %r24;
	or.pred  	%p61, %p39, %p31;
	or.b32  	%r25, %r12, %r18;
	or.b32  	%r74, %r25, %r24;
	setp.lt.s32 	%p62, %r74, 0;
	or.pred  	%p63, %p62, %p61;
	or.pred  	%p64, %p59, %p63;
	@%p64 bra 	$L__BB24_37;
	bra.uni 	$L__BB24_36;

$L__BB24_37:
	st.local.v2.u32 	[%rd12], {%r12, %r18};
	add.s32 	%r98, %r8, 7;
	st.local.v2.u32 	[%rd12+8], {%r98, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd101, $str;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r75, [retval0+0];
	} // callseq 7
	bra.uni 	$L__BB24_38;

$L__BB24_36:
	cvt.s64.s32 	%rd96, %r24;
	mul.lo.s64 	%rd97, %rd8, %rd96;
	add.s64 	%rd98, %rd7, %rd97;
	st.global.u32 	[%rd98], %r12;
	mul.lo.s64 	%rd99, %rd10, %rd96;
	add.s64 	%rd100, %rd9, %rd99;
	st.global.u32 	[%rd100], %r18;

$L__BB24_38:
	add.s32 	%r26, %r8, 8;
	setp.le.s32 	%p66, %r6, %r26;
	setp.le.s32 	%p67, %r47, %r15;
	or.pred  	%p68, %p67, %p10;
	or.b32  	%r76, %r16, %r26;
	setp.lt.s32 	%p69, %r76, 0;
	or.pred  	%p70, %p69, %p68;
	or.pred  	%p71, %p66, %p70;
	@%p71 bra 	$L__BB24_40;
	bra.uni 	$L__BB24_39;

$L__BB24_40:
	st.local.v2.u32 	[%rd12], {%r15, %r9};
	add.s32 	%r99, %r8, 8;
	st.local.v2.u32 	[%rd12+8], {%r99, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd109, $str;
	cvta.global.u64 	%rd110, %rd109;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r77, [retval0+0];
	} // callseq 8
	bra.uni 	$L__BB24_41;

$L__BB24_39:
	cvt.s64.s32 	%rd104, %r26;
	mul.lo.s64 	%rd105, %rd8, %rd104;
	add.s64 	%rd106, %rd7, %rd105;
	st.global.u32 	[%rd106], %r15;
	mul.lo.s64 	%rd107, %rd10, %rd104;
	add.s64 	%rd108, %rd9, %rd107;
	st.global.u32 	[%rd108], %r9;

$L__BB24_41:
	add.s32 	%r27, %r8, 9;
	setp.le.s32 	%p73, %r6, %r27;
	or.pred  	%p75, %p67, %p17;
	or.b32  	%r78, %r23, %r27;
	setp.lt.s32 	%p76, %r78, 0;
	or.pred  	%p77, %p76, %p75;
	or.pred  	%p78, %p73, %p77;
	@%p78 bra 	$L__BB24_43;
	bra.uni 	$L__BB24_42;

$L__BB24_43:
	st.local.v2.u32 	[%rd12], {%r15, %r12};
	add.s32 	%r100, %r8, 9;
	st.local.v2.u32 	[%rd12+8], {%r100, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd117, $str;
	cvta.global.u64 	%rd118, %rd117;
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd118;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r79, [retval0+0];
	} // callseq 9
	bra.uni 	$L__BB24_44;

$L__BB24_42:
	cvt.s64.s32 	%rd112, %r27;
	mul.lo.s64 	%rd113, %rd8, %rd112;
	add.s64 	%rd114, %rd7, %rd113;
	st.global.u32 	[%rd114], %r15;
	mul.lo.s64 	%rd115, %rd10, %rd112;
	add.s64 	%rd116, %rd9, %rd115;
	st.global.u32 	[%rd116], %r12;

$L__BB24_44:
	add.s32 	%r28, %r8, 10;
	setp.le.s32 	%p80, %r6, %r28;
	or.pred  	%p82, %p67, %p24;
	or.b32  	%r80, %r28, %r15;
	setp.lt.s32 	%p83, %r80, 0;
	or.pred  	%p84, %p83, %p82;
	or.pred  	%p85, %p80, %p84;
	@%p85 bra 	$L__BB24_46;
	bra.uni 	$L__BB24_45;

$L__BB24_46:
	st.local.v2.u32 	[%rd12], {%r15, %r15};
	add.s32 	%r101, %r8, 10;
	st.local.v2.u32 	[%rd12+8], {%r101, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd125, $str;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 10, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r81, [retval0+0];
	} // callseq 10
	bra.uni 	$L__BB24_47;

$L__BB24_45:
	cvt.s64.s32 	%rd120, %r28;
	mul.lo.s64 	%rd121, %rd8, %rd120;
	add.s64 	%rd122, %rd7, %rd121;
	st.global.u32 	[%rd122], %r15;
	mul.lo.s64 	%rd123, %rd10, %rd120;
	add.s64 	%rd124, %rd9, %rd123;
	st.global.u32 	[%rd124], %r15;

$L__BB24_47:
	add.s32 	%r29, %r8, 11;
	setp.le.s32 	%p87, %r6, %r29;
	or.pred  	%p89, %p67, %p31;
	or.b32  	%r30, %r15, %r18;
	or.b32  	%r82, %r30, %r29;
	setp.lt.s32 	%p90, %r82, 0;
	or.pred  	%p91, %p90, %p89;
	or.pred  	%p92, %p87, %p91;
	@%p92 bra 	$L__BB24_49;
	bra.uni 	$L__BB24_48;

$L__BB24_49:
	st.local.v2.u32 	[%rd12], {%r15, %r18};
	add.s32 	%r102, %r8, 11;
	st.local.v2.u32 	[%rd12+8], {%r102, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd133, $str;
	cvta.global.u64 	%rd134, %rd133;
	{ // callseq 11, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd134;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r83, [retval0+0];
	} // callseq 11
	bra.uni 	$L__BB24_50;

$L__BB24_48:
	cvt.s64.s32 	%rd128, %r29;
	mul.lo.s64 	%rd129, %rd8, %rd128;
	add.s64 	%rd130, %rd7, %rd129;
	st.global.u32 	[%rd130], %r15;
	mul.lo.s64 	%rd131, %rd10, %rd128;
	add.s64 	%rd132, %rd9, %rd131;
	st.global.u32 	[%rd132], %r18;

$L__BB24_50:
	add.s32 	%r31, %r8, 12;
	setp.le.s32 	%p94, %r6, %r31;
	setp.le.s32 	%p95, %r47, %r18;
	or.pred  	%p96, %p95, %p10;
	or.b32  	%r84, %r19, %r31;
	setp.lt.s32 	%p97, %r84, 0;
	or.pred  	%p98, %p97, %p96;
	or.pred  	%p99, %p94, %p98;
	@%p99 bra 	$L__BB24_52;
	bra.uni 	$L__BB24_51;

$L__BB24_52:
	st.local.v2.u32 	[%rd12], {%r18, %r9};
	add.s32 	%r103, %r8, 12;
	st.local.v2.u32 	[%rd12+8], {%r103, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd141, $str;
	cvta.global.u64 	%rd142, %rd141;
	{ // callseq 12, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r85, [retval0+0];
	} // callseq 12
	bra.uni 	$L__BB24_53;

$L__BB24_51:
	cvt.s64.s32 	%rd136, %r31;
	mul.lo.s64 	%rd137, %rd8, %rd136;
	add.s64 	%rd138, %rd7, %rd137;
	st.global.u32 	[%rd138], %r18;
	mul.lo.s64 	%rd139, %rd10, %rd136;
	add.s64 	%rd140, %rd9, %rd139;
	st.global.u32 	[%rd140], %r9;

$L__BB24_53:
	add.s32 	%r32, %r8, 13;
	setp.le.s32 	%p101, %r6, %r32;
	or.pred  	%p103, %p95, %p17;
	or.b32  	%r86, %r25, %r32;
	setp.lt.s32 	%p104, %r86, 0;
	or.pred  	%p105, %p104, %p103;
	or.pred  	%p106, %p101, %p105;
	@%p106 bra 	$L__BB24_55;
	bra.uni 	$L__BB24_54;

$L__BB24_55:
	st.local.v2.u32 	[%rd12], {%r18, %r12};
	add.s32 	%r104, %r8, 13;
	st.local.v2.u32 	[%rd12+8], {%r104, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd149, $str;
	cvta.global.u64 	%rd150, %rd149;
	{ // callseq 13, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd150;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r87, [retval0+0];
	} // callseq 13
	bra.uni 	$L__BB24_56;

$L__BB24_54:
	cvt.s64.s32 	%rd144, %r32;
	mul.lo.s64 	%rd145, %rd8, %rd144;
	add.s64 	%rd146, %rd7, %rd145;
	st.global.u32 	[%rd146], %r18;
	mul.lo.s64 	%rd147, %rd10, %rd144;
	add.s64 	%rd148, %rd9, %rd147;
	st.global.u32 	[%rd148], %r12;

$L__BB24_56:
	add.s32 	%r33, %r8, 14;
	setp.le.s32 	%p108, %r6, %r33;
	or.pred  	%p110, %p95, %p24;
	or.b32  	%r88, %r30, %r33;
	setp.lt.s32 	%p111, %r88, 0;
	or.pred  	%p112, %p111, %p110;
	or.pred  	%p113, %p108, %p112;
	@%p113 bra 	$L__BB24_58;
	bra.uni 	$L__BB24_57;

$L__BB24_58:
	st.local.v2.u32 	[%rd12], {%r18, %r15};
	add.s32 	%r105, %r8, 14;
	st.local.v2.u32 	[%rd12+8], {%r105, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd157, $str;
	cvta.global.u64 	%rd158, %rd157;
	{ // callseq 14, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd158;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r89, [retval0+0];
	} // callseq 14
	bra.uni 	$L__BB24_59;

$L__BB24_57:
	cvt.s64.s32 	%rd152, %r33;
	mul.lo.s64 	%rd153, %rd8, %rd152;
	add.s64 	%rd154, %rd7, %rd153;
	st.global.u32 	[%rd154], %r18;
	mul.lo.s64 	%rd155, %rd10, %rd152;
	add.s64 	%rd156, %rd9, %rd155;
	st.global.u32 	[%rd156], %r15;

$L__BB24_59:
	add.s32 	%r34, %r8, 15;
	setp.le.s32 	%p115, %r6, %r34;
	or.pred  	%p117, %p95, %p31;
	or.b32  	%r90, %r34, %r18;
	setp.lt.s32 	%p118, %r90, 0;
	or.pred  	%p119, %p118, %p117;
	or.pred  	%p120, %p115, %p119;
	@%p120 bra 	$L__BB24_61;
	bra.uni 	$L__BB24_60;

$L__BB24_61:
	st.local.v2.u32 	[%rd12], {%r18, %r18};
	add.s32 	%r106, %r8, 15;
	st.local.v2.u32 	[%rd12+8], {%r106, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd165, $str;
	cvta.global.u64 	%rd166, %rd165;
	{ // callseq 15, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd166;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r91, [retval0+0];
	} // callseq 15
	bra.uni 	$L__BB24_62;

$L__BB24_60:
	cvt.s64.s32 	%rd160, %r34;
	mul.lo.s64 	%rd161, %rd8, %rd160;
	add.s64 	%rd162, %rd7, %rd161;
	st.global.u32 	[%rd162], %r18;
	mul.lo.s64 	%rd163, %rd10, %rd160;
	add.s64 	%rd164, %rd9, %rd163;
	st.global.u32 	[%rd164], %r18;

$L__BB24_62:
	add.s64 	%rd168, %rd168, %rd11;
	setp.lt.u64 	%p121, %rd168, %rd25;
	@%p121 bra 	$L__BB24_2;

$L__BB24_63:
	ret;

}
	// .globl	init_affine_diag_hess_inds_kernel_cuda_kernel_backward
.visible .entry init_affine_diag_hess_inds_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_1[184],
	.param .align 8 .b8 init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_2[184]
)
{
	.local .align 8 .b8 	__local_depot25[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<138>;
	.reg .b32 	%r<138>;
	.reg .b64 	%rd<220>;


	mov.u64 	%SPL, __local_depot25;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r40, %r41}, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r42, %r43}, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0+8];
	mov.b64 	%rd26, init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_1;
	ld.param.u64 	%rd25, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r39, [init_affine_diag_hess_inds_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r44, %ntid.x;
	cvt.u64.u32 	%rd1, %r44;
	mov.u32 	%r45, %ctaid.x;
	mul.wide.u32 	%rd27, %r44, %r45;
	mov.u32 	%r46, %tid.x;
	cvt.u64.u32 	%rd28, %r46;
	add.s64 	%rd216, %rd27, %rd28;
	setp.ge.u64 	%p17, %rd216, %rd25;
	@%p17 bra 	$L__BB25_95;

	cvt.s64.s32 	%rd4, %r43;
	cvt.s64.s32 	%rd5, %r42;
	cvt.s64.s32 	%rd6, %r41;
	ld.param.v2.u32 	{%r47, %r48}, [%rd26+176];
	ld.param.u32 	%r6, [%rd26+172];
	ld.param.u64 	%rd30, [%rd26];
	cvta.to.global.u64 	%rd7, %rd30;
	ld.param.s32 	%rd8, [%rd26+32];
	ld.param.u64 	%rd31, [%rd26+56];
	cvta.to.global.u64 	%rd9, %rd31;
	ld.param.s32 	%rd10, [%rd26+88];
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd32, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd32;
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd12, %SPL, 0;

$L__BB25_2:
	setp.lt.s32 	%p18, %r39, 4;
	mov.u64 	%rd217, %rd216;
	@%p18 bra 	$L__BB25_6;

	or.b64  	%rd34, %rd216, %rd4;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p19, %rd35, 0;
	@%p19 bra 	$L__BB25_5;

	div.u64 	%rd217, %rd216, %rd4;
	bra.uni 	$L__BB25_6;

$L__BB25_5:
	cvt.u32.u64 	%r50, %rd4;
	cvt.u32.u64 	%r51, %rd216;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd217, %r52;

$L__BB25_6:
	setp.lt.s32 	%p20, %r39, 3;
	@%p20 bra 	$L__BB25_10;

	or.b64  	%rd36, %rd217, %rd5;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p21, %rd37, 0;
	@%p21 bra 	$L__BB25_9;

	div.u64 	%rd217, %rd217, %rd5;
	bra.uni 	$L__BB25_10;

$L__BB25_9:
	cvt.u32.u64 	%r53, %rd5;
	cvt.u32.u64 	%r54, %rd217;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd217, %r55;

$L__BB25_10:
	setp.lt.s32 	%p22, %r39, 2;
	@%p22 bra 	$L__BB25_14;

	or.b64  	%rd38, %rd217, %rd6;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p23, %rd39, 0;
	@%p23 bra 	$L__BB25_13;

	div.u64 	%rd217, %rd217, %rd6;
	bra.uni 	$L__BB25_14;

$L__BB25_13:
	cvt.u32.u64 	%r56, %rd6;
	cvt.u32.u64 	%r57, %rd217;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd217, %r58;

$L__BB25_14:
	cvt.u32.u64 	%r59, %rd217;
	setp.gt.s32 	%p24, %r39, 0;
	selp.b32 	%r7, %r59, 0, %p24;
	shl.b32 	%r8, %r7, 4;
	shl.b32 	%r9, %r7, 2;
	setp.le.s32 	%p25, %r47, %r9;
	setp.le.s32 	%p26, %r48, %r9;
	setp.le.s32 	%p27, %r6, %r8;
	or.pred  	%p28, %p25, %p26;
	or.b32  	%r60, %r9, %r8;
	setp.lt.s32 	%p29, %r60, 0;
	or.pred  	%p30, %p29, %p28;
	or.pred  	%p1, %p27, %p30;
	@%p1 bra 	$L__BB25_16;
	bra.uni 	$L__BB25_15;

$L__BB25_16:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd45, $str;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 16, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r61, [retval0+0];
	} // callseq 16
	bra.uni 	$L__BB25_17;

$L__BB25_15:
	cvt.s64.s32 	%rd40, %r8;
	mul.lo.s64 	%rd41, %rd8, %rd40;
	add.s64 	%rd42, %rd7, %rd41;
	st.global.u32 	[%rd42], %r9;
	mul.lo.s64 	%rd43, %rd10, %rd40;
	add.s64 	%rd44, %rd9, %rd43;
	st.global.u32 	[%rd44], %r9;

$L__BB25_17:
	mul.lo.s32 	%r10, %r7, -12;
	add.s32 	%r11, %r8, 1;
	add.s32 	%r12, %r11, %r10;
	setp.le.s32 	%p32, %r48, %r12;
	setp.le.s32 	%p33, %r6, %r11;
	or.pred  	%p34, %p25, %p32;
	or.b32  	%r13, %r12, %r9;
	or.b32  	%r62, %r13, %r11;
	setp.lt.s32 	%p35, %r62, 0;
	or.pred  	%p36, %p35, %p34;
	or.pred  	%p2, %p33, %p36;
	@%p2 bra 	$L__BB25_19;
	bra.uni 	$L__BB25_18;

$L__BB25_19:
	st.local.v2.u32 	[%rd12], {%r9, %r12};
	add.s32 	%r108, %r8, 1;
	st.local.v2.u32 	[%rd12+8], {%r108, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 17, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r63, [retval0+0];
	} // callseq 17
	bra.uni 	$L__BB25_20;

$L__BB25_18:
	cvt.s64.s32 	%rd48, %r11;
	mul.lo.s64 	%rd49, %rd8, %rd48;
	add.s64 	%rd50, %rd7, %rd49;
	st.global.u32 	[%rd50], %r9;
	mul.lo.s64 	%rd51, %rd10, %rd48;
	add.s64 	%rd52, %rd9, %rd51;
	st.global.u32 	[%rd52], %r12;

$L__BB25_20:
	add.s32 	%r14, %r8, 2;
	add.s32 	%r15, %r14, %r10;
	setp.le.s32 	%p38, %r48, %r15;
	setp.le.s32 	%p39, %r6, %r14;
	or.pred  	%p40, %p25, %p38;
	or.b32  	%r16, %r15, %r9;
	or.b32  	%r64, %r16, %r14;
	setp.lt.s32 	%p41, %r64, 0;
	or.pred  	%p42, %p41, %p40;
	or.pred  	%p3, %p39, %p42;
	@%p3 bra 	$L__BB25_22;
	bra.uni 	$L__BB25_21;

$L__BB25_22:
	st.local.v2.u32 	[%rd12], {%r9, %r15};
	add.s32 	%r109, %r8, 2;
	st.local.v2.u32 	[%rd12+8], {%r109, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 18, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r65, [retval0+0];
	} // callseq 18
	bra.uni 	$L__BB25_23;

$L__BB25_21:
	cvt.s64.s32 	%rd56, %r14;
	mul.lo.s64 	%rd57, %rd8, %rd56;
	add.s64 	%rd58, %rd7, %rd57;
	st.global.u32 	[%rd58], %r9;
	mul.lo.s64 	%rd59, %rd10, %rd56;
	add.s64 	%rd60, %rd9, %rd59;
	st.global.u32 	[%rd60], %r15;

$L__BB25_23:
	add.s32 	%r17, %r8, 3;
	add.s32 	%r18, %r17, %r10;
	setp.le.s32 	%p44, %r48, %r18;
	setp.le.s32 	%p45, %r6, %r17;
	or.pred  	%p46, %p25, %p44;
	or.b32  	%r19, %r18, %r9;
	or.b32  	%r66, %r19, %r17;
	setp.lt.s32 	%p47, %r66, 0;
	or.pred  	%p48, %p47, %p46;
	or.pred  	%p4, %p45, %p48;
	@%p4 bra 	$L__BB25_25;
	bra.uni 	$L__BB25_24;

$L__BB25_25:
	st.local.v2.u32 	[%rd12], {%r9, %r18};
	add.s32 	%r110, %r8, 3;
	st.local.v2.u32 	[%rd12+8], {%r110, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd69, $str;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 19, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r67, [retval0+0];
	} // callseq 19
	bra.uni 	$L__BB25_26;

$L__BB25_24:
	cvt.s64.s32 	%rd64, %r17;
	mul.lo.s64 	%rd65, %rd8, %rd64;
	add.s64 	%rd66, %rd7, %rd65;
	st.global.u32 	[%rd66], %r9;
	mul.lo.s64 	%rd67, %rd10, %rd64;
	add.s64 	%rd68, %rd9, %rd67;
	st.global.u32 	[%rd68], %r18;

$L__BB25_26:
	add.s32 	%r20, %r8, 4;
	setp.le.s32 	%p50, %r6, %r20;
	setp.le.s32 	%p51, %r47, %r12;
	or.pred  	%p52, %p51, %p26;
	or.b32  	%r68, %r13, %r20;
	setp.lt.s32 	%p53, %r68, 0;
	or.pred  	%p54, %p53, %p52;
	or.pred  	%p5, %p50, %p54;
	@%p5 bra 	$L__BB25_28;
	bra.uni 	$L__BB25_27;

$L__BB25_28:
	st.local.v2.u32 	[%rd12], {%r12, %r9};
	add.s32 	%r111, %r8, 4;
	st.local.v2.u32 	[%rd12+8], {%r111, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd77, $str;
	cvta.global.u64 	%rd78, %rd77;
	{ // callseq 20, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r69, [retval0+0];
	} // callseq 20
	bra.uni 	$L__BB25_29;

$L__BB25_27:
	cvt.s64.s32 	%rd72, %r20;
	mul.lo.s64 	%rd73, %rd8, %rd72;
	add.s64 	%rd74, %rd7, %rd73;
	st.global.u32 	[%rd74], %r12;
	mul.lo.s64 	%rd75, %rd10, %rd72;
	add.s64 	%rd76, %rd9, %rd75;
	st.global.u32 	[%rd76], %r9;

$L__BB25_29:
	add.s32 	%r21, %r8, 5;
	setp.le.s32 	%p56, %r6, %r21;
	or.pred  	%p58, %p51, %p32;
	or.b32  	%r70, %r12, %r21;
	setp.lt.s32 	%p59, %r70, 0;
	or.pred  	%p60, %p59, %p58;
	or.pred  	%p6, %p56, %p60;
	@%p6 bra 	$L__BB25_31;
	bra.uni 	$L__BB25_30;

$L__BB25_31:
	st.local.v2.u32 	[%rd12], {%r12, %r12};
	add.s32 	%r112, %r8, 5;
	st.local.v2.u32 	[%rd12+8], {%r112, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd85, $str;
	cvta.global.u64 	%rd86, %rd85;
	{ // callseq 21, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r71, [retval0+0];
	} // callseq 21
	bra.uni 	$L__BB25_32;

$L__BB25_30:
	cvt.s64.s32 	%rd80, %r21;
	mul.lo.s64 	%rd81, %rd8, %rd80;
	add.s64 	%rd82, %rd7, %rd81;
	st.global.u32 	[%rd82], %r12;
	mul.lo.s64 	%rd83, %rd10, %rd80;
	add.s64 	%rd84, %rd9, %rd83;
	st.global.u32 	[%rd84], %r12;

$L__BB25_32:
	add.s32 	%r22, %r8, 6;
	setp.le.s32 	%p62, %r6, %r22;
	or.pred  	%p64, %p51, %p38;
	or.b32  	%r23, %r12, %r15;
	or.b32  	%r72, %r23, %r22;
	setp.lt.s32 	%p65, %r72, 0;
	or.pred  	%p66, %p65, %p64;
	or.pred  	%p7, %p62, %p66;
	@%p7 bra 	$L__BB25_34;
	bra.uni 	$L__BB25_33;

$L__BB25_34:
	st.local.v2.u32 	[%rd12], {%r12, %r15};
	add.s32 	%r113, %r8, 6;
	st.local.v2.u32 	[%rd12+8], {%r113, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd93, $str;
	cvta.global.u64 	%rd94, %rd93;
	{ // callseq 22, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd94;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r73, [retval0+0];
	} // callseq 22
	bra.uni 	$L__BB25_35;

$L__BB25_33:
	cvt.s64.s32 	%rd88, %r22;
	mul.lo.s64 	%rd89, %rd8, %rd88;
	add.s64 	%rd90, %rd7, %rd89;
	st.global.u32 	[%rd90], %r12;
	mul.lo.s64 	%rd91, %rd10, %rd88;
	add.s64 	%rd92, %rd9, %rd91;
	st.global.u32 	[%rd92], %r15;

$L__BB25_35:
	add.s32 	%r24, %r8, 7;
	setp.le.s32 	%p68, %r6, %r24;
	or.pred  	%p70, %p51, %p44;
	or.b32  	%r25, %r12, %r18;
	or.b32  	%r74, %r25, %r24;
	setp.lt.s32 	%p71, %r74, 0;
	or.pred  	%p72, %p71, %p70;
	or.pred  	%p8, %p68, %p72;
	@%p8 bra 	$L__BB25_37;
	bra.uni 	$L__BB25_36;

$L__BB25_37:
	st.local.v2.u32 	[%rd12], {%r12, %r18};
	add.s32 	%r114, %r8, 7;
	st.local.v2.u32 	[%rd12+8], {%r114, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd101, $str;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 23, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r75, [retval0+0];
	} // callseq 23
	bra.uni 	$L__BB25_38;

$L__BB25_36:
	cvt.s64.s32 	%rd96, %r24;
	mul.lo.s64 	%rd97, %rd8, %rd96;
	add.s64 	%rd98, %rd7, %rd97;
	st.global.u32 	[%rd98], %r12;
	mul.lo.s64 	%rd99, %rd10, %rd96;
	add.s64 	%rd100, %rd9, %rd99;
	st.global.u32 	[%rd100], %r18;

$L__BB25_38:
	add.s32 	%r26, %r8, 8;
	setp.le.s32 	%p74, %r6, %r26;
	setp.le.s32 	%p75, %r47, %r15;
	or.pred  	%p76, %p75, %p26;
	or.b32  	%r76, %r16, %r26;
	setp.lt.s32 	%p77, %r76, 0;
	or.pred  	%p78, %p77, %p76;
	or.pred  	%p9, %p74, %p78;
	@%p9 bra 	$L__BB25_40;
	bra.uni 	$L__BB25_39;

$L__BB25_40:
	st.local.v2.u32 	[%rd12], {%r15, %r9};
	add.s32 	%r115, %r8, 8;
	st.local.v2.u32 	[%rd12+8], {%r115, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd109, $str;
	cvta.global.u64 	%rd110, %rd109;
	{ // callseq 24, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r77, [retval0+0];
	} // callseq 24
	bra.uni 	$L__BB25_41;

$L__BB25_39:
	cvt.s64.s32 	%rd104, %r26;
	mul.lo.s64 	%rd105, %rd8, %rd104;
	add.s64 	%rd106, %rd7, %rd105;
	st.global.u32 	[%rd106], %r15;
	mul.lo.s64 	%rd107, %rd10, %rd104;
	add.s64 	%rd108, %rd9, %rd107;
	st.global.u32 	[%rd108], %r9;

$L__BB25_41:
	add.s32 	%r27, %r8, 9;
	setp.le.s32 	%p80, %r6, %r27;
	or.pred  	%p82, %p75, %p32;
	or.b32  	%r78, %r23, %r27;
	setp.lt.s32 	%p83, %r78, 0;
	or.pred  	%p84, %p83, %p82;
	or.pred  	%p10, %p80, %p84;
	@%p10 bra 	$L__BB25_43;
	bra.uni 	$L__BB25_42;

$L__BB25_43:
	st.local.v2.u32 	[%rd12], {%r15, %r12};
	add.s32 	%r116, %r8, 9;
	st.local.v2.u32 	[%rd12+8], {%r116, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd117, $str;
	cvta.global.u64 	%rd118, %rd117;
	{ // callseq 25, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd118;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r79, [retval0+0];
	} // callseq 25
	bra.uni 	$L__BB25_44;

$L__BB25_42:
	cvt.s64.s32 	%rd112, %r27;
	mul.lo.s64 	%rd113, %rd8, %rd112;
	add.s64 	%rd114, %rd7, %rd113;
	st.global.u32 	[%rd114], %r15;
	mul.lo.s64 	%rd115, %rd10, %rd112;
	add.s64 	%rd116, %rd9, %rd115;
	st.global.u32 	[%rd116], %r12;

$L__BB25_44:
	add.s32 	%r28, %r8, 10;
	setp.le.s32 	%p86, %r6, %r28;
	or.pred  	%p88, %p75, %p38;
	or.b32  	%r80, %r15, %r28;
	setp.lt.s32 	%p89, %r80, 0;
	or.pred  	%p90, %p89, %p88;
	or.pred  	%p11, %p86, %p90;
	@%p11 bra 	$L__BB25_46;
	bra.uni 	$L__BB25_45;

$L__BB25_46:
	st.local.v2.u32 	[%rd12], {%r15, %r15};
	add.s32 	%r117, %r8, 10;
	st.local.v2.u32 	[%rd12+8], {%r117, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd125, $str;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 26, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r81, [retval0+0];
	} // callseq 26
	bra.uni 	$L__BB25_47;

$L__BB25_45:
	cvt.s64.s32 	%rd120, %r28;
	mul.lo.s64 	%rd121, %rd8, %rd120;
	add.s64 	%rd122, %rd7, %rd121;
	st.global.u32 	[%rd122], %r15;
	mul.lo.s64 	%rd123, %rd10, %rd120;
	add.s64 	%rd124, %rd9, %rd123;
	st.global.u32 	[%rd124], %r15;

$L__BB25_47:
	add.s32 	%r29, %r8, 11;
	setp.le.s32 	%p92, %r6, %r29;
	or.pred  	%p94, %p75, %p44;
	or.b32  	%r30, %r15, %r18;
	or.b32  	%r82, %r30, %r29;
	setp.lt.s32 	%p95, %r82, 0;
	or.pred  	%p96, %p95, %p94;
	or.pred  	%p12, %p92, %p96;
	@%p12 bra 	$L__BB25_49;
	bra.uni 	$L__BB25_48;

$L__BB25_49:
	st.local.v2.u32 	[%rd12], {%r15, %r18};
	add.s32 	%r118, %r8, 11;
	st.local.v2.u32 	[%rd12+8], {%r118, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd133, $str;
	cvta.global.u64 	%rd134, %rd133;
	{ // callseq 27, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd134;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r83, [retval0+0];
	} // callseq 27
	bra.uni 	$L__BB25_50;

$L__BB25_48:
	cvt.s64.s32 	%rd128, %r29;
	mul.lo.s64 	%rd129, %rd8, %rd128;
	add.s64 	%rd130, %rd7, %rd129;
	st.global.u32 	[%rd130], %r15;
	mul.lo.s64 	%rd131, %rd10, %rd128;
	add.s64 	%rd132, %rd9, %rd131;
	st.global.u32 	[%rd132], %r18;

$L__BB25_50:
	add.s32 	%r31, %r8, 12;
	setp.le.s32 	%p98, %r6, %r31;
	setp.le.s32 	%p99, %r47, %r18;
	or.pred  	%p100, %p99, %p26;
	or.b32  	%r84, %r19, %r31;
	setp.lt.s32 	%p101, %r84, 0;
	or.pred  	%p102, %p101, %p100;
	or.pred  	%p13, %p98, %p102;
	@%p13 bra 	$L__BB25_52;
	bra.uni 	$L__BB25_51;

$L__BB25_52:
	st.local.v2.u32 	[%rd12], {%r18, %r9};
	add.s32 	%r119, %r8, 12;
	st.local.v2.u32 	[%rd12+8], {%r119, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd141, $str;
	cvta.global.u64 	%rd142, %rd141;
	{ // callseq 28, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r85, [retval0+0];
	} // callseq 28
	bra.uni 	$L__BB25_53;

$L__BB25_51:
	cvt.s64.s32 	%rd136, %r31;
	mul.lo.s64 	%rd137, %rd8, %rd136;
	add.s64 	%rd138, %rd7, %rd137;
	st.global.u32 	[%rd138], %r18;
	mul.lo.s64 	%rd139, %rd10, %rd136;
	add.s64 	%rd140, %rd9, %rd139;
	st.global.u32 	[%rd140], %r9;

$L__BB25_53:
	add.s32 	%r32, %r8, 13;
	setp.le.s32 	%p104, %r6, %r32;
	or.pred  	%p106, %p99, %p32;
	or.b32  	%r86, %r25, %r32;
	setp.lt.s32 	%p107, %r86, 0;
	or.pred  	%p108, %p107, %p106;
	or.pred  	%p14, %p104, %p108;
	@%p14 bra 	$L__BB25_55;
	bra.uni 	$L__BB25_54;

$L__BB25_55:
	st.local.v2.u32 	[%rd12], {%r18, %r12};
	add.s32 	%r120, %r8, 13;
	st.local.v2.u32 	[%rd12+8], {%r120, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd149, $str;
	cvta.global.u64 	%rd150, %rd149;
	{ // callseq 29, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd150;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r87, [retval0+0];
	} // callseq 29
	bra.uni 	$L__BB25_56;

$L__BB25_54:
	cvt.s64.s32 	%rd144, %r32;
	mul.lo.s64 	%rd145, %rd8, %rd144;
	add.s64 	%rd146, %rd7, %rd145;
	st.global.u32 	[%rd146], %r18;
	mul.lo.s64 	%rd147, %rd10, %rd144;
	add.s64 	%rd148, %rd9, %rd147;
	st.global.u32 	[%rd148], %r12;

$L__BB25_56:
	add.s32 	%r33, %r8, 14;
	setp.le.s32 	%p110, %r6, %r33;
	or.pred  	%p112, %p99, %p38;
	or.b32  	%r88, %r30, %r33;
	setp.lt.s32 	%p113, %r88, 0;
	or.pred  	%p114, %p113, %p112;
	or.pred  	%p15, %p110, %p114;
	@%p15 bra 	$L__BB25_58;
	bra.uni 	$L__BB25_57;

$L__BB25_58:
	st.local.v2.u32 	[%rd12], {%r18, %r15};
	add.s32 	%r121, %r8, 14;
	st.local.v2.u32 	[%rd12+8], {%r121, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd157, $str;
	cvta.global.u64 	%rd158, %rd157;
	{ // callseq 30, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd158;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r89, [retval0+0];
	} // callseq 30
	bra.uni 	$L__BB25_59;

$L__BB25_57:
	cvt.s64.s32 	%rd152, %r33;
	mul.lo.s64 	%rd153, %rd8, %rd152;
	add.s64 	%rd154, %rd7, %rd153;
	st.global.u32 	[%rd154], %r18;
	mul.lo.s64 	%rd155, %rd10, %rd152;
	add.s64 	%rd156, %rd9, %rd155;
	st.global.u32 	[%rd156], %r15;

$L__BB25_59:
	add.s32 	%r34, %r8, 15;
	setp.le.s32 	%p116, %r6, %r34;
	or.pred  	%p118, %p99, %p44;
	or.b32  	%r90, %r18, %r34;
	setp.lt.s32 	%p119, %r90, 0;
	or.pred  	%p120, %p119, %p118;
	or.pred  	%p16, %p116, %p120;
	@%p16 bra 	$L__BB25_61;
	bra.uni 	$L__BB25_60;

$L__BB25_61:
	st.local.v2.u32 	[%rd12], {%r18, %r18};
	add.s32 	%r122, %r8, 15;
	st.local.v2.u32 	[%rd12+8], {%r122, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd165, $str;
	cvta.global.u64 	%rd166, %rd165;
	{ // callseq 31, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd166;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r91, [retval0+0];
	} // callseq 31
	bra.uni 	$L__BB25_62;

$L__BB25_60:
	cvt.s64.s32 	%rd160, %r34;
	mul.lo.s64 	%rd161, %rd8, %rd160;
	add.s64 	%rd162, %rd7, %rd161;
	st.global.u32 	[%rd162], %r18;
	mul.lo.s64 	%rd163, %rd10, %rd160;
	add.s64 	%rd164, %rd9, %rd163;
	st.global.u32 	[%rd164], %r18;

$L__BB25_62:
	not.pred 	%p121, %p16;
	@%p121 bra 	$L__BB25_64;

	st.local.v2.u32 	[%rd12], {%r18, %r18};
	add.s32 	%r123, %r8, 15;
	st.local.v2.u32 	[%rd12+8], {%r123, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd168, $str;
	cvta.global.u64 	%rd169, %rd168;
	{ // callseq 32, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd169;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r92, [retval0+0];
	} // callseq 32

$L__BB25_64:
	not.pred 	%p122, %p15;
	@%p122 bra 	$L__BB25_66;

	st.local.v2.u32 	[%rd12], {%r18, %r15};
	add.s32 	%r124, %r8, 14;
	st.local.v2.u32 	[%rd12+8], {%r124, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd171, $str;
	cvta.global.u64 	%rd172, %rd171;
	{ // callseq 33, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd172;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r93, [retval0+0];
	} // callseq 33

$L__BB25_66:
	not.pred 	%p123, %p14;
	@%p123 bra 	$L__BB25_68;

	st.local.v2.u32 	[%rd12], {%r18, %r12};
	add.s32 	%r125, %r8, 13;
	st.local.v2.u32 	[%rd12+8], {%r125, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd174, $str;
	cvta.global.u64 	%rd175, %rd174;
	{ // callseq 34, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd175;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r94, [retval0+0];
	} // callseq 34

$L__BB25_68:
	not.pred 	%p124, %p13;
	@%p124 bra 	$L__BB25_70;

	st.local.v2.u32 	[%rd12], {%r18, %r9};
	add.s32 	%r126, %r8, 12;
	st.local.v2.u32 	[%rd12+8], {%r126, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd177, $str;
	cvta.global.u64 	%rd178, %rd177;
	{ // callseq 35, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd178;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r95, [retval0+0];
	} // callseq 35

$L__BB25_70:
	not.pred 	%p125, %p12;
	@%p125 bra 	$L__BB25_72;

	st.local.v2.u32 	[%rd12], {%r15, %r18};
	add.s32 	%r127, %r8, 11;
	st.local.v2.u32 	[%rd12+8], {%r127, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd180, $str;
	cvta.global.u64 	%rd181, %rd180;
	{ // callseq 36, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd181;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r96, [retval0+0];
	} // callseq 36

$L__BB25_72:
	not.pred 	%p126, %p11;
	@%p126 bra 	$L__BB25_74;

	st.local.v2.u32 	[%rd12], {%r15, %r15};
	add.s32 	%r128, %r8, 10;
	st.local.v2.u32 	[%rd12+8], {%r128, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd183, $str;
	cvta.global.u64 	%rd184, %rd183;
	{ // callseq 37, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd184;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r97, [retval0+0];
	} // callseq 37

$L__BB25_74:
	not.pred 	%p127, %p10;
	@%p127 bra 	$L__BB25_76;

	st.local.v2.u32 	[%rd12], {%r15, %r12};
	add.s32 	%r129, %r8, 9;
	st.local.v2.u32 	[%rd12+8], {%r129, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd186, $str;
	cvta.global.u64 	%rd187, %rd186;
	{ // callseq 38, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd187;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r98, [retval0+0];
	} // callseq 38

$L__BB25_76:
	not.pred 	%p128, %p9;
	@%p128 bra 	$L__BB25_78;

	st.local.v2.u32 	[%rd12], {%r15, %r9};
	add.s32 	%r130, %r8, 8;
	st.local.v2.u32 	[%rd12+8], {%r130, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd189, $str;
	cvta.global.u64 	%rd190, %rd189;
	{ // callseq 39, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd190;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r99, [retval0+0];
	} // callseq 39

$L__BB25_78:
	not.pred 	%p129, %p8;
	@%p129 bra 	$L__BB25_80;

	st.local.v2.u32 	[%rd12], {%r12, %r18};
	add.s32 	%r131, %r8, 7;
	st.local.v2.u32 	[%rd12+8], {%r131, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd192, $str;
	cvta.global.u64 	%rd193, %rd192;
	{ // callseq 40, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd193;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r100, [retval0+0];
	} // callseq 40

$L__BB25_80:
	not.pred 	%p130, %p7;
	@%p130 bra 	$L__BB25_82;

	st.local.v2.u32 	[%rd12], {%r12, %r15};
	add.s32 	%r132, %r8, 6;
	st.local.v2.u32 	[%rd12+8], {%r132, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd195, $str;
	cvta.global.u64 	%rd196, %rd195;
	{ // callseq 41, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd196;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r101, [retval0+0];
	} // callseq 41

$L__BB25_82:
	not.pred 	%p131, %p6;
	@%p131 bra 	$L__BB25_84;

	st.local.v2.u32 	[%rd12], {%r12, %r12};
	add.s32 	%r133, %r8, 5;
	st.local.v2.u32 	[%rd12+8], {%r133, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd198, $str;
	cvta.global.u64 	%rd199, %rd198;
	{ // callseq 42, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd199;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r102, [retval0+0];
	} // callseq 42

$L__BB25_84:
	not.pred 	%p132, %p5;
	@%p132 bra 	$L__BB25_86;

	st.local.v2.u32 	[%rd12], {%r12, %r9};
	add.s32 	%r134, %r8, 4;
	st.local.v2.u32 	[%rd12+8], {%r134, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd201, $str;
	cvta.global.u64 	%rd202, %rd201;
	{ // callseq 43, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd202;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r103, [retval0+0];
	} // callseq 43

$L__BB25_86:
	not.pred 	%p133, %p4;
	@%p133 bra 	$L__BB25_88;

	st.local.v2.u32 	[%rd12], {%r9, %r18};
	add.s32 	%r135, %r8, 3;
	st.local.v2.u32 	[%rd12+8], {%r135, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd204, $str;
	cvta.global.u64 	%rd205, %rd204;
	{ // callseq 44, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd205;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r104, [retval0+0];
	} // callseq 44

$L__BB25_88:
	not.pred 	%p134, %p3;
	@%p134 bra 	$L__BB25_90;

	st.local.v2.u32 	[%rd12], {%r9, %r15};
	add.s32 	%r136, %r8, 2;
	st.local.v2.u32 	[%rd12+8], {%r136, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd207, $str;
	cvta.global.u64 	%rd208, %rd207;
	{ // callseq 45, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd208;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r105, [retval0+0];
	} // callseq 45

$L__BB25_90:
	not.pred 	%p135, %p2;
	@%p135 bra 	$L__BB25_92;

	st.local.v2.u32 	[%rd12], {%r9, %r12};
	add.s32 	%r137, %r8, 1;
	st.local.v2.u32 	[%rd12+8], {%r137, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd210, $str;
	cvta.global.u64 	%rd211, %rd210;
	{ // callseq 46, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd211;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r106, [retval0+0];
	} // callseq 46

$L__BB25_92:
	not.pred 	%p136, %p1;
	@%p136 bra 	$L__BB25_94;

	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r47};
	st.local.v2.u32 	[%rd12+16], {%r48, %r6};
	mov.u64 	%rd213, $str;
	cvta.global.u64 	%rd214, %rd213;
	{ // callseq 47, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd214;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd33;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r107, [retval0+0];
	} // callseq 47

$L__BB25_94:
	add.s64 	%rd216, %rd216, %rd11;
	setp.lt.u64 	%p137, %rd216, %rd25;
	@%p137 bra 	$L__BB25_2;

$L__BB25_95:
	ret;

}
	// .globl	initialize_soft_tilde_x_cuda_kernel_forward
.visible .entry initialize_soft_tilde_x_cuda_kernel_forward(
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_forward_param_3[56],
	.param .f64 initialize_soft_tilde_x_cuda_kernel_forward_param_4,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_forward_param_5,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_forward_param_6
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<91>;
	.reg .f64 	%fd<26>;
	.reg .b64 	%rd<79>;


	ld.param.v2.u32 	{%r38, %r39}, [initialize_soft_tilde_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r40, %r41}, [initialize_soft_tilde_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r46, %r47}, [initialize_soft_tilde_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r54, %r55}, [initialize_soft_tilde_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r62, %r63}, [initialize_soft_tilde_x_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [initialize_soft_tilde_x_cuda_kernel_forward_param_4];
	ld.param.u32 	%r36, [initialize_soft_tilde_x_cuda_kernel_forward_param_5];
	ld.param.u32 	%r37, [initialize_soft_tilde_x_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd42, [initialize_soft_tilde_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd40, [initialize_soft_tilde_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd38, [initialize_soft_tilde_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd37, [initialize_soft_tilde_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r8, [initialize_soft_tilde_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r66, %ntid.x;
	cvt.u64.u32 	%rd1, %r66;
	mov.u32 	%r67, %ctaid.x;
	mul.wide.u32 	%rd44, %r66, %r67;
	mov.u32 	%r68, %tid.x;
	cvt.u64.u32 	%rd45, %r68;
	add.s64 	%rd72, %rd44, %rd45;
	setp.ge.u64 	%p1, %rd72, %rd37;
	@%p1 bra 	$L__BB26_33;

	cvta.to.global.u64 	%rd4, %rd40;
	cvta.to.global.u64 	%rd5, %rd38;
	cvta.to.global.u64 	%rd6, %rd42;
	cvt.s64.s32 	%rd7, %r41;
	cvt.s64.s32 	%rd8, %r40;
	cvt.s64.s32 	%rd9, %r39;
	cvt.s64.s32 	%rd10, %r54;
	cvt.s64.s32 	%rd11, %r46;
	cvt.s64.s32 	%rd12, %r62;
	setp.gt.s32 	%p2, %r8, 3;
	@%p2 bra 	$L__BB26_16;
	bra.uni 	$L__BB26_2;

$L__BB26_16:
	cvt.u32.u64 	%r78, %rd7;
	cvt.u32.u64 	%r81, %rd8;
	cvt.u32.u64 	%r84, %rd9;

$L__BB26_17:
	or.b64  	%rd58, %rd72, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p11, %rd59, 0;
	@%p11 bra 	$L__BB26_19;

	div.u64 	%rd77, %rd72, %rd7;
	bra.uni 	$L__BB26_20;

$L__BB26_19:
	cvt.u32.u64 	%r79, %rd72;
	div.u32 	%r80, %r79, %r78;
	cvt.u64.u32 	%rd77, %r80;

$L__BB26_20:
	setp.lt.s32 	%p12, %r8, 3;
	@%p12 bra 	$L__BB26_24;

	or.b64  	%rd60, %rd77, %rd8;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p13, %rd61, 0;
	@%p13 bra 	$L__BB26_23;

	div.u64 	%rd77, %rd77, %rd8;
	bra.uni 	$L__BB26_24;

$L__BB26_23:
	cvt.u32.u64 	%r82, %rd77;
	div.u32 	%r83, %r82, %r81;
	cvt.u64.u32 	%rd77, %r83;

$L__BB26_24:
	setp.lt.s32 	%p14, %r8, 2;
	@%p14 bra 	$L__BB26_28;

	or.b64  	%rd62, %rd77, %rd9;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p15, %rd63, 0;
	@%p15 bra 	$L__BB26_27;

	div.u64 	%rd77, %rd77, %rd9;
	bra.uni 	$L__BB26_28;

$L__BB26_27:
	cvt.u32.u64 	%r85, %rd77;
	div.u32 	%r86, %r85, %r84;
	cvt.u64.u32 	%rd77, %r86;

$L__BB26_28:
	cvt.u32.u64 	%r87, %rd77;
	setp.gt.s32 	%p16, %r8, 0;
	selp.b32 	%r88, %r87, 0, %p16;
	add.s32 	%r3, %r88, %r36;
	cvt.s64.s32 	%rd64, %r3;
	mul.lo.s64 	%rd65, %rd64, %rd10;
	add.s64 	%rd34, %rd4, %rd65;
	cvt.s64.s32 	%rd66, %r88;
	mul.lo.s64 	%rd67, %rd66, %rd11;
	add.s64 	%rd35, %rd5, %rd67;
	setp.eq.s32 	%p17, %r37, 0;
	@%p17 bra 	$L__BB26_31;

	setp.ne.s32 	%p18, %r37, 1;
	@%p18 bra 	$L__BB26_32;

	mul.lo.s64 	%rd69, %rd64, %rd12;
	add.s64 	%rd70, %rd6, %rd69;
	ld.global.f64 	%fd14, [%rd70];
	ld.global.f64 	%fd15, [%rd70+8];
	ld.global.f64 	%fd16, [%rd70+16];
	ld.global.f64 	%fd17, [%rd34];
	fma.rn.f64 	%fd18, %fd14, %fd1, %fd17;
	ld.global.f64 	%fd19, [%rd34+8];
	fma.rn.f64 	%fd20, %fd15, %fd1, %fd19;
	ld.global.f64 	%fd21, [%rd34+16];
	fma.rn.f64 	%fd22, %fd16, %fd1, %fd21;
	st.global.f64 	[%rd35], %fd18;
	st.global.f64 	[%rd35+8], %fd20;
	st.global.f64 	[%rd35+16], %fd22;
	bra.uni 	$L__BB26_32;

$L__BB26_31:
	ld.global.f64 	%fd23, [%rd34];
	ld.global.f64 	%fd24, [%rd34+8];
	ld.global.f64 	%fd25, [%rd34+16];
	st.global.f64 	[%rd35], %fd23;
	st.global.f64 	[%rd35+8], %fd24;
	st.global.f64 	[%rd35+16], %fd25;

$L__BB26_32:
	mov.u32 	%r90, %nctaid.x;
	mul.wide.u32 	%rd71, %r66, %r90;
	add.s64 	%rd72, %rd72, %rd71;
	setp.lt.u64 	%p19, %rd72, %rd37;
	@%p19 bra 	$L__BB26_17;
	bra.uni 	$L__BB26_33;

$L__BB26_2:
	mov.u32 	%r69, %nctaid.x;
	cvt.u64.u32 	%rd46, %r69;
	mul.lo.s64 	%rd13, %rd1, %rd46;
	cvt.u32.u64 	%r70, %rd8;
	cvt.u32.u64 	%r73, %rd9;

$L__BB26_3:
	setp.lt.s32 	%p3, %r8, 3;
	mov.u64 	%rd73, %rd72;
	@%p3 bra 	$L__BB26_7;

	or.b64  	%rd47, %rd72, %rd8;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB26_6;

	div.u64 	%rd73, %rd72, %rd8;
	bra.uni 	$L__BB26_7;

$L__BB26_6:
	cvt.u32.u64 	%r71, %rd72;
	div.u32 	%r72, %r71, %r70;
	cvt.u64.u32 	%rd73, %r72;

$L__BB26_7:
	setp.lt.s32 	%p5, %r8, 2;
	@%p5 bra 	$L__BB26_11;

	or.b64  	%rd49, %rd73, %rd9;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p6, %rd50, 0;
	@%p6 bra 	$L__BB26_10;

	div.u64 	%rd73, %rd73, %rd9;
	bra.uni 	$L__BB26_11;

$L__BB26_10:
	cvt.u32.u64 	%r74, %rd73;
	div.u32 	%r75, %r74, %r73;
	cvt.u64.u32 	%rd73, %r75;

$L__BB26_11:
	cvt.u32.u64 	%r76, %rd73;
	setp.gt.s32 	%p7, %r8, 0;
	selp.b32 	%r77, %r76, 0, %p7;
	add.s32 	%r2, %r77, %r36;
	cvt.s64.s32 	%rd51, %r2;
	mul.lo.s64 	%rd52, %rd51, %rd10;
	add.s64 	%rd21, %rd4, %rd52;
	cvt.s64.s32 	%rd53, %r77;
	mul.lo.s64 	%rd54, %rd53, %rd11;
	add.s64 	%rd22, %rd5, %rd54;
	setp.eq.s32 	%p8, %r37, 0;
	@%p8 bra 	$L__BB26_14;

	setp.ne.s32 	%p9, %r37, 1;
	@%p9 bra 	$L__BB26_15;

	mul.lo.s64 	%rd56, %rd51, %rd12;
	add.s64 	%rd57, %rd6, %rd56;
	ld.global.f64 	%fd2, [%rd57];
	ld.global.f64 	%fd3, [%rd57+8];
	ld.global.f64 	%fd4, [%rd57+16];
	ld.global.f64 	%fd5, [%rd21];
	fma.rn.f64 	%fd6, %fd2, %fd1, %fd5;
	ld.global.f64 	%fd7, [%rd21+8];
	fma.rn.f64 	%fd8, %fd3, %fd1, %fd7;
	ld.global.f64 	%fd9, [%rd21+16];
	fma.rn.f64 	%fd10, %fd4, %fd1, %fd9;
	st.global.f64 	[%rd22], %fd6;
	st.global.f64 	[%rd22+8], %fd8;
	st.global.f64 	[%rd22+16], %fd10;
	bra.uni 	$L__BB26_15;

$L__BB26_14:
	ld.global.f64 	%fd11, [%rd21];
	ld.global.f64 	%fd12, [%rd21+8];
	ld.global.f64 	%fd13, [%rd21+16];
	st.global.f64 	[%rd22], %fd11;
	st.global.f64 	[%rd22+8], %fd12;
	st.global.f64 	[%rd22+16], %fd13;

$L__BB26_15:
	add.s64 	%rd72, %rd72, %rd13;
	setp.lt.u64 	%p10, %rd72, %rd37;
	@%p10 bra 	$L__BB26_3;

$L__BB26_33:
	ret;

}
	// .globl	initialize_soft_tilde_x_cuda_kernel_backward
.visible .entry initialize_soft_tilde_x_cuda_kernel_backward(
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_3[56],
	.param .f64 initialize_soft_tilde_x_cuda_kernel_backward_param_4,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_5,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_6,
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 initialize_soft_tilde_x_cuda_kernel_backward_param_9[56],
	.param .f64 initialize_soft_tilde_x_cuda_kernel_backward_param_10,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_11,
	.param .u32 initialize_soft_tilde_x_cuda_kernel_backward_param_12
)
{
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<55>;
	.reg .b32 	%r<133>;
	.reg .f64 	%fd<87>;
	.reg .b64 	%rd<96>;


	ld.param.v2.u32 	{%r64, %r65}, [initialize_soft_tilde_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r66, %r67}, [initialize_soft_tilde_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r72, %r73}, [initialize_soft_tilde_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r80, %r81}, [initialize_soft_tilde_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r88, %r89}, [initialize_soft_tilde_x_cuda_kernel_backward_param_3+32];
	ld.param.u32 	%r36, [initialize_soft_tilde_x_cuda_kernel_backward_param_6];
	ld.param.v2.u32 	{%r96, %r97}, [initialize_soft_tilde_x_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r104, %r105}, [initialize_soft_tilde_x_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r112, %r113}, [initialize_soft_tilde_x_cuda_kernel_backward_param_9+32];
	ld.param.u64 	%rd46, [initialize_soft_tilde_x_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd44, [initialize_soft_tilde_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd42, [initialize_soft_tilde_x_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd41, [initialize_soft_tilde_x_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd39, [initialize_soft_tilde_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd37, [initialize_soft_tilde_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd35, [initialize_soft_tilde_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [initialize_soft_tilde_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r116, %ntid.x;
	cvt.u64.u32 	%rd1, %r116;
	mov.u32 	%r117, %ctaid.x;
	mul.wide.u32 	%rd48, %r116, %r117;
	mov.u32 	%r118, %tid.x;
	cvt.u64.u32 	%rd49, %r118;
	add.s64 	%rd92, %rd48, %rd49;
	setp.ge.u64 	%p2, %rd92, %rd35;
	@%p2 bra 	$L__BB27_37;

	cvt.s64.s32 	%rd12, %r67;
	cvt.s64.s32 	%rd13, %r66;
	cvt.s64.s32 	%rd14, %r65;
	setp.ne.s32 	%p1, %r36, 0;
	cvt.s64.s32 	%rd15, %r80;
	cvt.s64.s32 	%rd16, %r88;
	cvt.s64.s32 	%rd17, %r96;
	mov.u32 	%r119, %nctaid.x;
	cvt.u64.u32 	%rd50, %r119;
	mul.lo.s64 	%rd18, %rd1, %rd50;
	cvt.s64.s32 	%rd19, %r72;
	cvt.s64.s32 	%rd20, %r104;
	cvt.s64.s32 	%rd21, %r112;
	not.pred 	%p13, %p1;

$L__BB27_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd93, %rd92;
	@%p3 bra 	$L__BB27_6;

	or.b64  	%rd51, %rd92, %rd12;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.eq.s64 	%p4, %rd52, 0;
	@%p4 bra 	$L__BB27_5;

	div.u64 	%rd93, %rd92, %rd12;
	bra.uni 	$L__BB27_6;

$L__BB27_5:
	cvt.u32.u64 	%r120, %rd12;
	cvt.u32.u64 	%r121, %rd92;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd93, %r122;

$L__BB27_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB27_10;

	or.b64  	%rd53, %rd93, %rd13;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB27_9;

	div.u64 	%rd93, %rd93, %rd13;
	bra.uni 	$L__BB27_10;

$L__BB27_9:
	cvt.u32.u64 	%r123, %rd13;
	cvt.u32.u64 	%r124, %rd93;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd93, %r125;

$L__BB27_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB27_14;

	or.b64  	%rd55, %rd93, %rd14;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p8, %rd56, 0;
	@%p8 bra 	$L__BB27_13;

	div.u64 	%rd93, %rd93, %rd14;
	bra.uni 	$L__BB27_14;

$L__BB27_13:
	cvt.u32.u64 	%r126, %rd14;
	cvt.u32.u64 	%r127, %rd93;
	div.u32 	%r128, %r127, %r126;
	cvt.u64.u32 	%rd93, %r128;

$L__BB27_14:
	cvta.to.global.u64 	%rd91, %rd37;
	cvta.to.global.u64 	%rd90, %rd42;
	ld.param.u32 	%r132, [initialize_soft_tilde_x_cuda_kernel_backward_param_5];
	ld.param.u32 	%r131, [initialize_soft_tilde_x_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r129, %rd93;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r130, %r129, 0, %p9;
	add.s32 	%r2, %r130, %r132;
	setp.eq.s32 	%p10, %r131, 1;
	selp.u16 	%rs52, 1, 0, %p10;
	setp.eq.s32 	%p11, %r131, 0;
	selp.b16 	%rs54, %rs54, %rs52, %p11;
	and.b16  	%rs53, %rs54, 255;
	setp.eq.s16 	%p12, %rs53, 0;
	cvt.s64.s32 	%rd57, %r130;
	mul.lo.s64 	%rd58, %rd57, %rd17;
	add.s64 	%rd32, %rd90, %rd58;
	mul.lo.s64 	%rd59, %rd57, %rd19;
	add.s64 	%rd33, %rd91, %rd59;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB27_27;

	setp.eq.s64 	%p15, %rd42, 0;
	@%p15 bra 	$L__BB27_17;

	ld.global.f64 	%fd26, [%rd32];
	add.f64 	%fd83, %fd26, 0d0000000000000000;
	ld.global.f64 	%fd27, [%rd32+8];
	add.f64 	%fd82, %fd27, 0d0000000000000000;
	ld.global.f64 	%fd28, [%rd32+16];
	add.f64 	%fd81, %fd28, 0d0000000000000000;
	bra.uni 	$L__BB27_19;

$L__BB27_17:
	setp.eq.s64 	%p16, %rd37, 0;
	mov.f64 	%fd81, 0d0000000000000000;
	mov.f64 	%fd82, %fd81;
	mov.f64 	%fd83, %fd81;
	@%p16 bra 	$L__BB27_19;

	ld.global.f64 	%fd32, [%rd33];
	add.f64 	%fd83, %fd32, 0d0000000000000000;
	ld.global.f64 	%fd33, [%rd33+8];
	add.f64 	%fd82, %fd33, 0d0000000000000000;
	ld.global.f64 	%fd34, [%rd33+16];
	add.f64 	%fd81, %fd34, 0d0000000000000000;

$L__BB27_19:
	ld.param.f64 	%fd80, [initialize_soft_tilde_x_cuda_kernel_backward_param_4];
	add.f64 	%fd10, %fd83, 0d0000000000000000;
	fma.rn.f64 	%fd11, %fd10, %fd80, 0d0000000000000000;
	add.f64 	%fd12, %fd82, 0d0000000000000000;
	fma.rn.f64 	%fd13, %fd12, %fd80, 0d0000000000000000;
	add.f64 	%fd14, %fd81, 0d0000000000000000;
	fma.rn.f64 	%fd15, %fd14, %fd80, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd46, 0;
	@%p17 bra 	$L__BB27_21;

	cvt.s64.s32 	%rd63, %r2;
	mul.lo.s64 	%rd64, %rd63, %rd21;
	add.s64 	%rd60, %rd46, %rd64;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd60],%fd11; }

	// end inline asm
	add.s64 	%rd61, %rd60, 8;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd61],%fd13; }

	// end inline asm
	add.s64 	%rd62, %rd60, 16;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd62],%fd15; }

	// end inline asm
	bra.uni 	$L__BB27_23;

$L__BB27_21:
	setp.eq.s64 	%p18, %rd41, 0;
	@%p18 bra 	$L__BB27_23;

	cvt.s64.s32 	%rd68, %r2;
	mul.lo.s64 	%rd69, %rd68, %rd16;
	add.s64 	%rd65, %rd41, %rd69;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd65],%fd11; }

	// end inline asm
	add.s64 	%rd66, %rd65, 8;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd66],%fd13; }

	// end inline asm
	add.s64 	%rd67, %rd65, 16;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd67],%fd15; }

	// end inline asm

$L__BB27_23:
	setp.eq.s64 	%p19, %rd44, 0;
	@%p19 bra 	$L__BB27_25;

	cvt.s64.s32 	%rd73, %r2;
	mul.lo.s64 	%rd74, %rd73, %rd20;
	add.s64 	%rd70, %rd44, %rd74;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd70],%fd10; }

	// end inline asm
	add.s64 	%rd71, %rd70, 8;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd71],%fd12; }

	// end inline asm
	add.s64 	%rd72, %rd70, 16;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd72],%fd14; }

	// end inline asm
	bra.uni 	$L__BB27_27;

$L__BB27_25:
	setp.eq.s64 	%p20, %rd39, 0;
	@%p20 bra 	$L__BB27_27;

	cvt.s64.s32 	%rd78, %r2;
	mul.lo.s64 	%rd79, %rd78, %rd15;
	add.s64 	%rd75, %rd39, %rd79;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd75],%fd10; }

	// end inline asm
	add.s64 	%rd76, %rd75, 8;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd76],%fd12; }

	// end inline asm
	add.s64 	%rd77, %rd75, 16;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd77],%fd14; }

	// end inline asm

$L__BB27_27:
	@%p1 bra 	$L__BB27_36;

	setp.eq.s64 	%p22, %rd42, 0;
	@%p22 bra 	$L__BB27_30;

	ld.global.f64 	%fd59, [%rd32];
	add.f64 	%fd86, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd32+8];
	add.f64 	%fd85, %fd60, 0d0000000000000000;
	ld.global.f64 	%fd61, [%rd32+16];
	add.f64 	%fd84, %fd61, 0d0000000000000000;
	bra.uni 	$L__BB27_32;

$L__BB27_30:
	setp.eq.s64 	%p23, %rd37, 0;
	mov.f64 	%fd84, 0d0000000000000000;
	mov.f64 	%fd85, %fd84;
	mov.f64 	%fd86, %fd84;
	@%p23 bra 	$L__BB27_32;

	ld.global.f64 	%fd65, [%rd33];
	add.f64 	%fd86, %fd65, 0d0000000000000000;
	ld.global.f64 	%fd66, [%rd33+8];
	add.f64 	%fd85, %fd66, 0d0000000000000000;
	ld.global.f64 	%fd67, [%rd33+16];
	add.f64 	%fd84, %fd67, 0d0000000000000000;

$L__BB27_32:
	setp.eq.s64 	%p24, %rd44, 0;
	@%p24 bra 	$L__BB27_34;

	cvt.s64.s32 	%rd83, %r2;
	mul.lo.s64 	%rd84, %rd83, %rd20;
	add.s64 	%rd80, %rd44, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd80],%fd86; }

	// end inline asm
	add.s64 	%rd81, %rd80, 8;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd81],%fd85; }

	// end inline asm
	add.s64 	%rd82, %rd80, 16;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd82],%fd84; }

	// end inline asm
	bra.uni 	$L__BB27_36;

$L__BB27_34:
	setp.eq.s64 	%p25, %rd39, 0;
	@%p25 bra 	$L__BB27_36;

	cvt.s64.s32 	%rd88, %r2;
	mul.lo.s64 	%rd89, %rd88, %rd15;
	add.s64 	%rd85, %rd39, %rd89;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd85],%fd86; }

	// end inline asm
	add.s64 	%rd86, %rd85, 8;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd86],%fd85; }

	// end inline asm
	add.s64 	%rd87, %rd85, 16;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd87],%fd84; }

	// end inline asm

$L__BB27_36:
	add.s64 	%rd92, %rd92, %rd18;
	setp.lt.u64 	%p26, %rd92, %rd35;
	@%p26 bra 	$L__BB27_2;

$L__BB27_37:
	ret;

}
	// .globl	negate_arr_vec3d_cuda_kernel_forward
.visible .entry negate_arr_vec3d_cuda_kernel_forward(
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r16, %r17}, [negate_arr_vec3d_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [negate_arr_vec3d_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [negate_arr_vec3d_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd35, [negate_arr_vec3d_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [negate_arr_vec3d_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [negate_arr_vec3d_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd37, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd38, %r30;
	add.s64 	%rd64, %rd37, %rd38;
	setp.ge.u64 	%p1, %rd64, %rd34;
	@%p1 bra 	$L__BB28_31;

	cvta.to.global.u64 	%rd4, %rd35;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd39, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd39;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB28_18;
	bra.uni 	$L__BB28_2;

$L__BB28_18:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r47, %rd7;

$L__BB28_19:
	or.b64  	%rd54, %rd64, %rd5;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p13, %rd55, 0;
	@%p13 bra 	$L__BB28_21;

	div.u64 	%rd71, %rd64, %rd5;
	bra.uni 	$L__BB28_22;

$L__BB28_21:
	cvt.u32.u64 	%r42, %rd64;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd71, %r43;

$L__BB28_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB28_26;

	or.b64  	%rd56, %rd71, %rd6;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB28_25;

	div.u64 	%rd71, %rd71, %rd6;
	bra.uni 	$L__BB28_26;

$L__BB28_25:
	cvt.u32.u64 	%r45, %rd71;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd71, %r46;

$L__BB28_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB28_30;

	or.b64  	%rd58, %rd71, %rd7;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB28_29;

	div.u64 	%rd71, %rd71, %rd7;
	bra.uni 	$L__BB28_30;

$L__BB28_29:
	cvt.u32.u64 	%r48, %rd71;
	div.u32 	%r49, %r48, %r47;
	cvt.u64.u32 	%rd71, %r49;

$L__BB28_30:
	cvt.s64.s32 	%rd60, %rd71;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd61, %rd60, 0, %p18;
	mul.lo.s64 	%rd62, %rd61, %rd8;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.f64 	%fd13, [%rd63];
	neg.f64 	%fd14, %fd13;
	ld.global.f64 	%fd15, [%rd63+8];
	neg.f64 	%fd16, %fd15;
	ld.global.f64 	%fd17, [%rd63+16];
	neg.f64 	%fd18, %fd17;
	st.global.f64 	[%rd63], %fd14;
	st.global.f64 	[%rd63+8], %fd16;
	st.global.f64 	[%rd63+16], %fd18;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p19, %rd64, %rd34;
	@%p19 bra 	$L__BB28_19;
	bra.uni 	$L__BB28_31;

$L__BB28_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB28_9;
	bra.uni 	$L__BB28_3;

$L__BB28_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r38, %rd7;

$L__BB28_10:
	or.b64  	%rd46, %rd64, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p8, %rd47, 0;
	@%p8 bra 	$L__BB28_12;

	div.u64 	%rd68, %rd64, %rd6;
	bra.uni 	$L__BB28_13;

$L__BB28_12:
	cvt.u32.u64 	%r36, %rd64;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd68, %r37;

$L__BB28_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB28_17;

	or.b64  	%rd48, %rd68, %rd7;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB28_16;

	div.u64 	%rd68, %rd68, %rd7;
	bra.uni 	$L__BB28_17;

$L__BB28_16:
	cvt.u32.u64 	%r39, %rd68;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd68, %r40;

$L__BB28_17:
	cvt.s64.s32 	%rd50, %rd68;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd51, %rd50, 0, %p11;
	mul.lo.s64 	%rd52, %rd51, %rd8;
	add.s64 	%rd53, %rd4, %rd52;
	ld.global.f64 	%fd7, [%rd53];
	neg.f64 	%fd8, %fd7;
	ld.global.f64 	%fd9, [%rd53+8];
	neg.f64 	%fd10, %fd9;
	ld.global.f64 	%fd11, [%rd53+16];
	neg.f64 	%fd12, %fd11;
	st.global.f64 	[%rd53], %fd8;
	st.global.f64 	[%rd53+8], %fd10;
	st.global.f64 	[%rd53+16], %fd12;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p12, %rd64, %rd34;
	@%p12 bra 	$L__BB28_10;
	bra.uni 	$L__BB28_31;

$L__BB28_3:
	cvt.u32.u64 	%r32, %rd7;

$L__BB28_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd65, %rd64;
	@%p4 bra 	$L__BB28_8;

	or.b64  	%rd40, %rd64, %rd7;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB28_7;

	div.u64 	%rd65, %rd64, %rd7;
	bra.uni 	$L__BB28_8;

$L__BB28_7:
	cvt.u32.u64 	%r33, %rd64;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd65, %r34;

$L__BB28_8:
	cvt.s64.s32 	%rd42, %rd65;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd43, %rd42, 0, %p6;
	mul.lo.s64 	%rd44, %rd43, %rd8;
	add.s64 	%rd45, %rd4, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	neg.f64 	%fd2, %fd1;
	ld.global.f64 	%fd3, [%rd45+8];
	neg.f64 	%fd4, %fd3;
	ld.global.f64 	%fd5, [%rd45+16];
	neg.f64 	%fd6, %fd5;
	st.global.f64 	[%rd45], %fd2;
	st.global.f64 	[%rd45+8], %fd4;
	st.global.f64 	[%rd45+16], %fd6;
	add.s64 	%rd64, %rd64, %rd9;
	setp.lt.u64 	%p7, %rd64, %rd34;
	@%p7 bra 	$L__BB28_4;

$L__BB28_31:
	ret;

}
	// .globl	negate_arr_vec3d_cuda_kernel_backward
.visible .entry negate_arr_vec3d_cuda_kernel_backward(
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 negate_arr_vec3d_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<38>;
	.reg .b64 	%rd<59>;


	ld.param.v2.u32 	{%r26, %r27}, [negate_arr_vec3d_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [negate_arr_vec3d_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [negate_arr_vec3d_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [negate_arr_vec3d_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd26, [negate_arr_vec3d_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd25, [negate_arr_vec3d_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd23, [negate_arr_vec3d_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [negate_arr_vec3d_cuda_kernel_backward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd55, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd55, %rd23;
	@%p1 bra 	$L__BB29_23;

	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r42;
	cvt.s64.s32 	%rd10, %r34;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB29_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd56, %rd55;
	@%p2 bra 	$L__BB29_6;

	or.b64  	%rd31, %rd55, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB29_5;

	div.u64 	%rd56, %rd55, %rd6;
	bra.uni 	$L__BB29_6;

$L__BB29_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd55;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd56, %r52;

$L__BB29_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB29_10;

	or.b64  	%rd33, %rd56, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB29_9;

	div.u64 	%rd56, %rd56, %rd7;
	bra.uni 	$L__BB29_10;

$L__BB29_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd56;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd56, %r55;

$L__BB29_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB29_14;

	or.b64  	%rd35, %rd56, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB29_13;

	div.u64 	%rd56, %rd56, %rd8;
	bra.uni 	$L__BB29_14;

$L__BB29_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd56;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd56, %r58;

$L__BB29_14:
	cvt.u32.u64 	%r59, %rd56;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r59, 0, %p8;
	setp.eq.s64 	%p9, %rd26, 0;
	@%p9 bra 	$L__BB29_16;

	cvta.to.global.u64 	%rd37, %rd26;
	cvt.s64.s32 	%rd38, %r2;
	mul.lo.s64 	%rd39, %rd38, %rd9;
	add.s64 	%rd40, %rd37, %rd39;
	ld.global.f64 	%fd13, [%rd40];
	add.f64 	%fd37, %fd13, 0d0000000000000000;
	ld.global.f64 	%fd14, [%rd40+8];
	add.f64 	%fd36, %fd14, 0d0000000000000000;
	ld.global.f64 	%fd15, [%rd40+16];
	add.f64 	%fd35, %fd15, 0d0000000000000000;
	bra.uni 	$L__BB29_18;

$L__BB29_16:
	setp.eq.s64 	%p10, %rd25, 0;
	mov.f64 	%fd35, 0d0000000000000000;
	mov.f64 	%fd36, %fd35;
	mov.f64 	%fd37, %fd35;
	@%p10 bra 	$L__BB29_18;

	cvta.to.global.u64 	%rd41, %rd25;
	cvt.s64.s32 	%rd42, %r2;
	mul.lo.s64 	%rd43, %rd42, %rd10;
	add.s64 	%rd44, %rd41, %rd43;
	ld.global.f64 	%fd19, [%rd44];
	add.f64 	%fd37, %fd19, 0d0000000000000000;
	ld.global.f64 	%fd20, [%rd44+8];
	add.f64 	%fd36, %fd20, 0d0000000000000000;
	ld.global.f64 	%fd21, [%rd44+16];
	add.f64 	%fd35, %fd21, 0d0000000000000000;

$L__BB29_18:
	mov.f64 	%fd22, 0d0000000000000000;
	sub.f64 	%fd10, %fd22, %fd37;
	sub.f64 	%fd11, %fd22, %fd36;
	sub.f64 	%fd12, %fd22, %fd35;
	@%p9 bra 	$L__BB29_20;

	cvt.s64.s32 	%rd48, %r2;
	mul.lo.s64 	%rd49, %rd48, %rd9;
	add.s64 	%rd45, %rd26, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd45],%fd10; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd46],%fd11; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd47],%fd12; }

	// end inline asm
	bra.uni 	$L__BB29_22;

$L__BB29_20:
	setp.eq.s64 	%p12, %rd25, 0;
	@%p12 bra 	$L__BB29_22;

	cvt.s64.s32 	%rd53, %r2;
	mul.lo.s64 	%rd54, %rd53, %rd10;
	add.s64 	%rd50, %rd25, %rd54;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd50],%fd10; }

	// end inline asm
	add.s64 	%rd51, %rd50, 8;
	// begin inline asm
	{ atom.add.f64 %fd31,[%rd51],%fd11; }

	// end inline asm
	add.s64 	%rd52, %rd50, 16;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd52],%fd12; }

	// end inline asm

$L__BB29_22:
	add.s64 	%rd55, %rd55, %rd11;
	setp.lt.u64 	%p13, %rd55, %rd23;
	@%p13 bra 	$L__BB29_2;

$L__BB29_23:
	ret;

}
	// .globl	sys_to_x_affine_cuda_kernel_forward
.visible .entry sys_to_x_affine_cuda_kernel_forward(
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_forward_param_4[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<108>;
	.reg .f64 	%fd<63>;
	.reg .b64 	%rd<104>;


	ld.param.v2.u32 	{%r43, %r44}, [sys_to_x_affine_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [sys_to_x_affine_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [sys_to_x_affine_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [sys_to_x_affine_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [sys_to_x_affine_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [sys_to_x_affine_cuda_kernel_forward_param_4+32];
	ld.param.u64 	%rd42, [sys_to_x_affine_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd40, [sys_to_x_affine_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [sys_to_x_affine_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd36, [sys_to_x_affine_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd35, [sys_to_x_affine_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [sys_to_x_affine_cuda_kernel_forward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd44, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd45, %r81;
	add.s64 	%rd97, %rd44, %rd45;
	setp.ge.u64 	%p1, %rd97, %rd35;
	@%p1 bra 	$L__BB30_25;

	cvta.to.global.u64 	%rd4, %rd42;
	cvta.to.global.u64 	%rd5, %rd40;
	cvta.to.global.u64 	%rd6, %rd38;
	cvta.to.global.u64 	%rd7, %rd36;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r75;
	cvt.s64.s32 	%rd12, %r51;
	cvt.s64.s32 	%rd13, %r67;
	cvt.s64.s32 	%rd14, %r59;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd46, %r82;
	mul.lo.s64 	%rd15, %rd1, %rd46;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB30_12;
	bra.uni 	$L__BB30_2;

$L__BB30_12:
	cvt.u32.u64 	%r94, %rd8;
	cvt.u32.u64 	%r97, %rd9;
	cvt.u32.u64 	%r100, %rd10;

$L__BB30_13:
	or.b64  	%rd71, %rd97, %rd8;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p9, %rd72, 0;
	@%p9 bra 	$L__BB30_15;

	div.u64 	%rd102, %rd97, %rd8;
	bra.uni 	$L__BB30_16;

$L__BB30_15:
	cvt.u32.u64 	%r95, %rd97;
	div.u32 	%r96, %r95, %r94;
	cvt.u64.u32 	%rd102, %r96;

$L__BB30_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB30_20;

	or.b64  	%rd73, %rd102, %rd9;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p11, %rd74, 0;
	@%p11 bra 	$L__BB30_19;

	div.u64 	%rd102, %rd102, %rd9;
	bra.uni 	$L__BB30_20;

$L__BB30_19:
	cvt.u32.u64 	%r98, %rd102;
	div.u32 	%r99, %r98, %r97;
	cvt.u64.u32 	%rd102, %r99;

$L__BB30_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB30_24;

	or.b64  	%rd75, %rd102, %rd10;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p13, %rd76, 0;
	@%p13 bra 	$L__BB30_23;

	div.u64 	%rd102, %rd102, %rd10;
	bra.uni 	$L__BB30_24;

$L__BB30_23:
	cvt.u32.u64 	%r101, %rd102;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd102, %r102;

$L__BB30_24:
	cvt.s64.s32 	%rd77, %rd102;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd78, %rd77, 0, %p14;
	mul.lo.s64 	%rd79, %rd78, %rd11;
	add.s64 	%rd80, %rd4, %rd79;
	ld.global.u32 	%r103, [%rd80];
	shl.b32 	%r104, %r103, 2;
	cvt.s64.s32 	%rd81, %r104;
	mul.lo.s64 	%rd82, %rd81, %rd12;
	add.s64 	%rd83, %rd7, %rd82;
	or.b32  	%r105, %r104, 1;
	cvt.s64.s32 	%rd84, %r105;
	mul.lo.s64 	%rd85, %rd84, %rd12;
	add.s64 	%rd86, %rd7, %rd85;
	or.b32  	%r106, %r104, 2;
	cvt.s64.s32 	%rd87, %r106;
	mul.lo.s64 	%rd88, %rd87, %rd12;
	add.s64 	%rd89, %rd7, %rd88;
	or.b32  	%r107, %r104, 3;
	cvt.s64.s32 	%rd90, %r107;
	mul.lo.s64 	%rd91, %rd90, %rd12;
	add.s64 	%rd92, %rd7, %rd91;
	mul.lo.s64 	%rd93, %rd78, %rd13;
	add.s64 	%rd94, %rd5, %rd93;
	ld.global.f64 	%fd32, [%rd94];
	mov.f64 	%fd33, 0d3FF0000000000000;
	sub.f64 	%fd34, %fd33, %fd32;
	ld.global.f64 	%fd35, [%rd94+8];
	sub.f64 	%fd36, %fd34, %fd35;
	ld.global.f64 	%fd37, [%rd94+16];
	sub.f64 	%fd38, %fd36, %fd37;
	ld.global.f64 	%fd39, [%rd83];
	mul.f64 	%fd40, %fd39, %fd38;
	ld.global.f64 	%fd41, [%rd83+8];
	mul.f64 	%fd42, %fd41, %fd38;
	ld.global.f64 	%fd43, [%rd83+16];
	mul.f64 	%fd44, %fd43, %fd38;
	ld.global.f64 	%fd45, [%rd86];
	ld.global.f64 	%fd46, [%rd86+8];
	ld.global.f64 	%fd47, [%rd86+16];
	fma.rn.f64 	%fd48, %fd45, %fd32, %fd40;
	fma.rn.f64 	%fd49, %fd46, %fd32, %fd42;
	fma.rn.f64 	%fd50, %fd47, %fd32, %fd44;
	ld.global.f64 	%fd51, [%rd89];
	ld.global.f64 	%fd52, [%rd89+8];
	ld.global.f64 	%fd53, [%rd89+16];
	fma.rn.f64 	%fd54, %fd51, %fd35, %fd48;
	fma.rn.f64 	%fd55, %fd52, %fd35, %fd49;
	fma.rn.f64 	%fd56, %fd53, %fd35, %fd50;
	ld.global.f64 	%fd57, [%rd92];
	ld.global.f64 	%fd58, [%rd92+8];
	ld.global.f64 	%fd59, [%rd92+16];
	fma.rn.f64 	%fd60, %fd57, %fd37, %fd54;
	fma.rn.f64 	%fd61, %fd58, %fd37, %fd55;
	fma.rn.f64 	%fd62, %fd59, %fd37, %fd56;
	mul.lo.s64 	%rd95, %rd78, %rd14;
	add.s64 	%rd96, %rd6, %rd95;
	st.global.f64 	[%rd96], %fd60;
	st.global.f64 	[%rd96+8], %fd61;
	st.global.f64 	[%rd96+16], %fd62;
	add.s64 	%rd97, %rd97, %rd15;
	setp.lt.u64 	%p15, %rd97, %rd35;
	@%p15 bra 	$L__BB30_13;
	bra.uni 	$L__BB30_25;

$L__BB30_2:
	cvt.u32.u64 	%r83, %rd9;
	cvt.u32.u64 	%r86, %rd10;

$L__BB30_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd98, %rd97;
	@%p3 bra 	$L__BB30_7;

	or.b64  	%rd47, %rd97, %rd9;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB30_6;

	div.u64 	%rd98, %rd97, %rd9;
	bra.uni 	$L__BB30_7;

$L__BB30_6:
	cvt.u32.u64 	%r84, %rd97;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd98, %r85;

$L__BB30_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB30_11;

	or.b64  	%rd49, %rd98, %rd10;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p6, %rd50, 0;
	@%p6 bra 	$L__BB30_10;

	div.u64 	%rd98, %rd98, %rd10;
	bra.uni 	$L__BB30_11;

$L__BB30_10:
	cvt.u32.u64 	%r87, %rd98;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd98, %r88;

$L__BB30_11:
	cvt.s64.s32 	%rd51, %rd98;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd52, %rd51, 0, %p7;
	mul.lo.s64 	%rd53, %rd52, %rd11;
	add.s64 	%rd54, %rd4, %rd53;
	ld.global.u32 	%r89, [%rd54];
	shl.b32 	%r90, %r89, 2;
	cvt.s64.s32 	%rd55, %r90;
	mul.lo.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd57, %rd7, %rd56;
	or.b32  	%r91, %r90, 1;
	cvt.s64.s32 	%rd58, %r91;
	mul.lo.s64 	%rd59, %rd58, %rd12;
	add.s64 	%rd60, %rd7, %rd59;
	or.b32  	%r92, %r90, 2;
	cvt.s64.s32 	%rd61, %r92;
	mul.lo.s64 	%rd62, %rd61, %rd12;
	add.s64 	%rd63, %rd7, %rd62;
	or.b32  	%r93, %r90, 3;
	cvt.s64.s32 	%rd64, %r93;
	mul.lo.s64 	%rd65, %rd64, %rd12;
	add.s64 	%rd66, %rd7, %rd65;
	mul.lo.s64 	%rd67, %rd52, %rd13;
	add.s64 	%rd68, %rd5, %rd67;
	ld.global.f64 	%fd1, [%rd68];
	mov.f64 	%fd2, 0d3FF0000000000000;
	sub.f64 	%fd3, %fd2, %fd1;
	ld.global.f64 	%fd4, [%rd68+8];
	sub.f64 	%fd5, %fd3, %fd4;
	ld.global.f64 	%fd6, [%rd68+16];
	sub.f64 	%fd7, %fd5, %fd6;
	ld.global.f64 	%fd8, [%rd57];
	mul.f64 	%fd9, %fd8, %fd7;
	ld.global.f64 	%fd10, [%rd57+8];
	mul.f64 	%fd11, %fd10, %fd7;
	ld.global.f64 	%fd12, [%rd57+16];
	mul.f64 	%fd13, %fd12, %fd7;
	ld.global.f64 	%fd14, [%rd60];
	ld.global.f64 	%fd15, [%rd60+8];
	ld.global.f64 	%fd16, [%rd60+16];
	fma.rn.f64 	%fd17, %fd14, %fd1, %fd9;
	fma.rn.f64 	%fd18, %fd15, %fd1, %fd11;
	fma.rn.f64 	%fd19, %fd16, %fd1, %fd13;
	ld.global.f64 	%fd20, [%rd63];
	ld.global.f64 	%fd21, [%rd63+8];
	ld.global.f64 	%fd22, [%rd63+16];
	fma.rn.f64 	%fd23, %fd20, %fd4, %fd17;
	fma.rn.f64 	%fd24, %fd21, %fd4, %fd18;
	fma.rn.f64 	%fd25, %fd22, %fd4, %fd19;
	ld.global.f64 	%fd26, [%rd66];
	ld.global.f64 	%fd27, [%rd66+8];
	ld.global.f64 	%fd28, [%rd66+16];
	fma.rn.f64 	%fd29, %fd26, %fd6, %fd23;
	fma.rn.f64 	%fd30, %fd27, %fd6, %fd24;
	fma.rn.f64 	%fd31, %fd28, %fd6, %fd25;
	mul.lo.s64 	%rd69, %rd52, %rd14;
	add.s64 	%rd70, %rd6, %rd69;
	st.global.f64 	[%rd70], %fd29;
	st.global.f64 	[%rd70+8], %fd30;
	st.global.f64 	[%rd70+16], %fd31;
	add.s64 	%rd97, %rd97, %rd15;
	setp.lt.u64 	%p8, %rd97, %rd35;
	@%p8 bra 	$L__BB30_3;

$L__BB30_25:
	ret;

}
	// .globl	sys_to_x_affine_cuda_kernel_backward
.visible .entry sys_to_x_affine_cuda_kernel_backward(
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 sys_to_x_affine_cuda_kernel_backward_param_8[56]
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<57>;
	.reg .b32 	%r<145>;
	.reg .f64 	%fd<140>;
	.reg .b64 	%rd<128>;


	ld.param.v2.u32 	{%r70, %r71}, [sys_to_x_affine_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [sys_to_x_affine_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [sys_to_x_affine_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [sys_to_x_affine_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [sys_to_x_affine_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r102, %r103}, [sys_to_x_affine_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r110, %r111}, [sys_to_x_affine_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r118, %r119}, [sys_to_x_affine_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r126, %r127}, [sys_to_x_affine_cuda_kernel_backward_param_7+32];
	ld.param.u64 	%rd57, [sys_to_x_affine_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd55, [sys_to_x_affine_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd53, [sys_to_x_affine_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd51, [sys_to_x_affine_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd50, [sys_to_x_affine_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd49, [sys_to_x_affine_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd48, [sys_to_x_affine_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd46, [sys_to_x_affine_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd45, [sys_to_x_affine_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd44, [sys_to_x_affine_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [sys_to_x_affine_cuda_kernel_backward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd59, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd60, %r132;
	add.s64 	%rd124, %rd59, %rd60;
	setp.ge.u64 	%p1, %rd124, %rd44;
	@%p1 bra 	$L__BB31_39;

	cvta.to.global.u64 	%rd10, %rd55;
	cvta.to.global.u64 	%rd11, %rd51;
	cvta.to.global.u64 	%rd12, %rd49;
	cvta.to.global.u64 	%rd13, %rd48;
	cvta.to.global.u64 	%rd14, %rd45;
	cvt.s64.s32 	%rd15, %r73;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r71;
	cvt.s64.s32 	%rd18, %r102;
	cvt.s64.s32 	%rd19, %r78;
	cvt.s64.s32 	%rd20, %r94;
	cvt.s64.s32 	%rd21, %r118;
	cvt.s64.s32 	%rd22, %r86;
	cvt.s64.s32 	%rd23, %r126;
	cvt.s64.s32 	%rd24, %r110;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd61, %r133;
	mul.lo.s64 	%rd25, %rd1, %rd61;

$L__BB31_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd125, %rd124;
	@%p2 bra 	$L__BB31_6;

	or.b64  	%rd62, %rd124, %rd15;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p3, %rd63, 0;
	@%p3 bra 	$L__BB31_5;

	div.u64 	%rd125, %rd124, %rd15;
	bra.uni 	$L__BB31_6;

$L__BB31_5:
	cvt.u32.u64 	%r134, %rd15;
	cvt.u32.u64 	%r135, %rd124;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd125, %r136;

$L__BB31_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB31_10;

	or.b64  	%rd64, %rd125, %rd16;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p5, %rd65, 0;
	@%p5 bra 	$L__BB31_9;

	div.u64 	%rd125, %rd125, %rd16;
	bra.uni 	$L__BB31_10;

$L__BB31_9:
	cvt.u32.u64 	%r137, %rd16;
	cvt.u32.u64 	%r138, %rd125;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd125, %r139;

$L__BB31_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB31_14;

	or.b64  	%rd66, %rd125, %rd17;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p7, %rd67, 0;
	@%p7 bra 	$L__BB31_13;

	div.u64 	%rd125, %rd125, %rd17;
	bra.uni 	$L__BB31_14;

$L__BB31_13:
	cvt.u32.u64 	%r140, %rd17;
	cvt.u32.u64 	%r141, %rd125;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd125, %r142;

$L__BB31_14:
	ld.param.u64 	%rd122, [sys_to_x_affine_cuda_kernel_backward_param_6];
	cvt.s64.s32 	%rd68, %rd125;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd36, %rd68, 0, %p8;
	mul.lo.s64 	%rd69, %rd36, %rd18;
	add.s64 	%rd70, %rd11, %rd69;
	ld.global.u32 	%r143, [%rd70];
	shl.b32 	%r144, %r143, 2;
	cvt.s64.s32 	%rd37, %r144;
	mul.lo.s64 	%rd38, %rd37, %rd19;
	add.s64 	%rd71, %rd14, %rd38;
	ld.global.f64 	%fd1, [%rd71];
	ld.global.f64 	%fd2, [%rd71+8];
	ld.global.f64 	%fd3, [%rd71+16];
	or.b64  	%rd72, %rd37, 1;
	mul.lo.s64 	%rd39, %rd72, %rd19;
	add.s64 	%rd73, %rd14, %rd39;
	ld.global.f64 	%fd4, [%rd73];
	ld.global.f64 	%fd5, [%rd73+8];
	ld.global.f64 	%fd6, [%rd73+16];
	or.b64  	%rd74, %rd37, 2;
	mul.lo.s64 	%rd40, %rd74, %rd19;
	add.s64 	%rd75, %rd14, %rd40;
	ld.global.f64 	%fd7, [%rd75];
	ld.global.f64 	%fd8, [%rd75+8];
	ld.global.f64 	%fd9, [%rd75+16];
	or.b64  	%rd76, %rd37, 3;
	mul.lo.s64 	%rd41, %rd76, %rd19;
	add.s64 	%rd77, %rd14, %rd41;
	ld.global.f64 	%fd10, [%rd77];
	ld.global.f64 	%fd11, [%rd77+8];
	ld.global.f64 	%fd12, [%rd77+16];
	mul.lo.s64 	%rd42, %rd36, %rd20;
	add.s64 	%rd78, %rd12, %rd42;
	ld.global.f64 	%fd13, [%rd78];
	ld.global.f64 	%fd14, [%rd78+8];
	ld.global.f64 	%fd15, [%rd78+16];
	setp.eq.s64 	%p9, %rd122, 0;
	@%p9 bra 	$L__BB31_16;

	mul.lo.s64 	%rd79, %rd36, %rd21;
	add.s64 	%rd80, %rd10, %rd79;
	ld.global.f64 	%fd40, [%rd80];
	add.f64 	%fd139, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd80+8];
	add.f64 	%fd138, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd80+16];
	add.f64 	%fd137, %fd42, 0d0000000000000000;
	bra.uni 	$L__BB31_18;

$L__BB31_16:
	ld.param.u64 	%rd123, [sys_to_x_affine_cuda_kernel_backward_param_2+8];
	setp.eq.s64 	%p10, %rd123, 0;
	mov.f64 	%fd137, 0d0000000000000000;
	mov.f64 	%fd138, %fd137;
	mov.f64 	%fd139, %fd137;
	@%p10 bra 	$L__BB31_18;

	mul.lo.s64 	%rd81, %rd36, %rd22;
	add.s64 	%rd82, %rd13, %rd81;
	ld.global.f64 	%fd46, [%rd82];
	add.f64 	%fd139, %fd46, 0d0000000000000000;
	ld.global.f64 	%fd47, [%rd82+8];
	add.f64 	%fd138, %fd47, 0d0000000000000000;
	ld.global.f64 	%fd48, [%rd82+16];
	add.f64 	%fd137, %fd48, 0d0000000000000000;

$L__BB31_18:
	mov.f64 	%fd49, 0d3FF0000000000000;
	sub.f64 	%fd50, %fd49, %fd13;
	sub.f64 	%fd51, %fd50, %fd14;
	sub.f64 	%fd52, %fd51, %fd15;
	add.f64 	%fd53, %fd139, 0d0000000000000000;
	mov.f64 	%fd54, 0d0000000000000000;
	fma.rn.f64 	%fd25, %fd15, %fd53, 0d0000000000000000;
	add.f64 	%fd55, %fd138, 0d0000000000000000;
	fma.rn.f64 	%fd26, %fd15, %fd55, 0d0000000000000000;
	add.f64 	%fd56, %fd137, 0d0000000000000000;
	fma.rn.f64 	%fd27, %fd15, %fd56, 0d0000000000000000;
	mul.f64 	%fd57, %fd11, %fd55;
	fma.rn.f64 	%fd58, %fd10, %fd53, %fd57;
	fma.rn.f64 	%fd59, %fd12, %fd56, %fd58;
	fma.rn.f64 	%fd28, %fd14, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd29, %fd14, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd30, %fd14, %fd56, 0d0000000000000000;
	mul.f64 	%fd60, %fd8, %fd55;
	fma.rn.f64 	%fd61, %fd7, %fd53, %fd60;
	fma.rn.f64 	%fd62, %fd9, %fd56, %fd61;
	fma.rn.f64 	%fd31, %fd13, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd32, %fd13, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd33, %fd13, %fd56, 0d0000000000000000;
	mul.f64 	%fd63, %fd5, %fd55;
	fma.rn.f64 	%fd64, %fd4, %fd53, %fd63;
	fma.rn.f64 	%fd65, %fd6, %fd56, %fd64;
	add.f64 	%fd66, %fd59, 0d0000000000000000;
	fma.rn.f64 	%fd34, %fd52, %fd53, 0d0000000000000000;
	fma.rn.f64 	%fd35, %fd52, %fd55, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd52, %fd56, 0d0000000000000000;
	add.f64 	%fd67, %fd62, 0d0000000000000000;
	add.f64 	%fd68, %fd65, 0d0000000000000000;
	mul.f64 	%fd69, %fd2, %fd55;
	fma.rn.f64 	%fd70, %fd1, %fd53, %fd69;
	fma.rn.f64 	%fd71, %fd3, %fd56, %fd70;
	add.f64 	%fd72, %fd71, 0d0000000000000000;
	sub.f64 	%fd73, %fd54, %fd72;
	add.f64 	%fd74, %fd66, %fd73;
	add.f64 	%fd75, %fd67, %fd73;
	add.f64 	%fd76, %fd68, %fd73;
	add.f64 	%fd37, %fd76, 0d0000000000000000;
	add.f64 	%fd38, %fd75, 0d0000000000000000;
	add.f64 	%fd39, %fd74, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd57, 0;
	@%p11 bra 	$L__BB31_20;

	mul.lo.s64 	%rd86, %rd36, %rd23;
	add.s64 	%rd83, %rd57, %rd86;
	// begin inline asm
	{ atom.add.f64 %fd77,[%rd83],%fd37; }

	// end inline asm
	add.s64 	%rd84, %rd83, 8;
	// begin inline asm
	{ atom.add.f64 %fd79,[%rd84],%fd38; }

	// end inline asm
	add.s64 	%rd85, %rd83, 16;
	// begin inline asm
	{ atom.add.f64 %fd81,[%rd85],%fd39; }

	// end inline asm
	bra.uni 	$L__BB31_22;

$L__BB31_20:
	setp.eq.s64 	%p12, %rd50, 0;
	@%p12 bra 	$L__BB31_22;

	add.s64 	%rd87, %rd50, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd83,[%rd87],%fd37; }

	// end inline asm
	add.s64 	%rd88, %rd87, 8;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd88],%fd38; }

	// end inline asm
	add.s64 	%rd89, %rd87, 16;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd89],%fd39; }

	// end inline asm

$L__BB31_22:
	setp.eq.s64 	%p13, %rd53, 0;
	@%p13 bra 	$L__BB31_24;

	add.s64 	%rd93, %rd37, 3;
	mul.lo.s64 	%rd94, %rd93, %rd24;
	add.s64 	%rd90, %rd53, %rd94;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd90],%fd25; }

	// end inline asm
	add.s64 	%rd91, %rd90, 8;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd91],%fd26; }

	// end inline asm
	add.s64 	%rd92, %rd90, 16;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd92],%fd27; }

	// end inline asm
	bra.uni 	$L__BB31_26;

$L__BB31_24:
	setp.eq.s64 	%p14, %rd46, 0;
	@%p14 bra 	$L__BB31_26;

	add.s64 	%rd95, %rd46, %rd41;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd95],%fd25; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd96],%fd26; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd97],%fd27; }

	// end inline asm

$L__BB31_26:
	@%p13 bra 	$L__BB31_28;

	add.s64 	%rd101, %rd37, 2;
	mul.lo.s64 	%rd102, %rd101, %rd24;
	add.s64 	%rd98, %rd53, %rd102;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd98],%fd28; }

	// end inline asm
	add.s64 	%rd99, %rd98, 8;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd99],%fd29; }

	// end inline asm
	add.s64 	%rd100, %rd98, 16;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd100],%fd30; }

	// end inline asm
	bra.uni 	$L__BB31_30;

$L__BB31_28:
	setp.eq.s64 	%p16, %rd46, 0;
	@%p16 bra 	$L__BB31_30;

	add.s64 	%rd103, %rd46, %rd40;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd103],%fd28; }

	// end inline asm
	add.s64 	%rd104, %rd103, 8;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd104],%fd29; }

	// end inline asm
	add.s64 	%rd105, %rd103, 16;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd105],%fd30; }

	// end inline asm

$L__BB31_30:
	@%p13 bra 	$L__BB31_32;

	add.s64 	%rd109, %rd37, 1;
	mul.lo.s64 	%rd110, %rd109, %rd24;
	add.s64 	%rd106, %rd53, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd106],%fd31; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd107],%fd32; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd108],%fd33; }

	// end inline asm
	bra.uni 	$L__BB31_34;

$L__BB31_32:
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB31_34;

	add.s64 	%rd111, %rd46, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd111],%fd31; }

	// end inline asm
	add.s64 	%rd112, %rd111, 8;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd112],%fd32; }

	// end inline asm
	add.s64 	%rd113, %rd111, 16;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd113],%fd33; }

	// end inline asm

$L__BB31_34:
	@%p13 bra 	$L__BB31_36;

	mul.lo.s64 	%rd117, %rd37, %rd24;
	add.s64 	%rd114, %rd53, %rd117;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd114],%fd34; }

	// end inline asm
	add.s64 	%rd115, %rd114, 8;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd115],%fd35; }

	// end inline asm
	add.s64 	%rd116, %rd114, 16;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd116],%fd36; }

	// end inline asm
	bra.uni 	$L__BB31_38;

$L__BB31_36:
	setp.eq.s64 	%p20, %rd46, 0;
	@%p20 bra 	$L__BB31_38;

	add.s64 	%rd118, %rd46, %rd38;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd118],%fd34; }

	// end inline asm
	add.s64 	%rd119, %rd118, 8;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd119],%fd35; }

	// end inline asm
	add.s64 	%rd120, %rd118, 16;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd120],%fd36; }

	// end inline asm

$L__BB31_38:
	ld.param.u64 	%rd121, [sys_to_x_affine_cuda_kernel_backward_param_0+24];
	add.s64 	%rd124, %rd124, %rd25;
	setp.lt.u64 	%p21, %rd124, %rd121;
	@%p21 bra 	$L__BB31_2;

$L__BB31_39:
	ret;

}
	// .globl	affine_to_sys_grad_cuda_kernel_forward
.visible .entry affine_to_sys_grad_cuda_kernel_forward(
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_forward_param_2[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<76>;
	.reg .f64 	%fd<25>;
	.reg .b64 	%rd<86>;


	ld.param.v2.u32 	{%r25, %r26}, [affine_to_sys_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [affine_to_sys_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [affine_to_sys_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [affine_to_sys_grad_cuda_kernel_forward_param_2+32];
	ld.param.u64 	%rd34, [affine_to_sys_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd32, [affine_to_sys_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd31, [affine_to_sys_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [affine_to_sys_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd36, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd37, %r47;
	add.s64 	%rd79, %rd36, %rd37;
	setp.ge.u64 	%p1, %rd79, %rd31;
	@%p1 bra 	$L__BB32_25;

	cvta.to.global.u64 	%rd4, %rd34;
	cvta.to.global.u64 	%rd5, %rd32;
	cvt.s64.s32 	%rd6, %r28;
	cvt.s64.s32 	%rd7, %r27;
	cvt.s64.s32 	%rd8, %r26;
	cvt.s64.s32 	%rd9, %r33;
	cvt.s64.s32 	%rd10, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd38, %r48;
	mul.lo.s64 	%rd11, %rd1, %rd38;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB32_12;
	bra.uni 	$L__BB32_2;

$L__BB32_12:
	cvt.u32.u64 	%r61, %rd6;
	cvt.u32.u64 	%r64, %rd7;
	cvt.u32.u64 	%r67, %rd8;

$L__BB32_13:
	or.b64  	%rd58, %rd79, %rd6;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p9, %rd59, 0;
	@%p9 bra 	$L__BB32_15;

	div.u64 	%rd84, %rd79, %rd6;
	bra.uni 	$L__BB32_16;

$L__BB32_15:
	cvt.u32.u64 	%r62, %rd79;
	div.u32 	%r63, %r62, %r61;
	cvt.u64.u32 	%rd84, %r63;

$L__BB32_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB32_20;

	or.b64  	%rd60, %rd84, %rd7;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p11, %rd61, 0;
	@%p11 bra 	$L__BB32_19;

	div.u64 	%rd84, %rd84, %rd7;
	bra.uni 	$L__BB32_20;

$L__BB32_19:
	cvt.u32.u64 	%r65, %rd84;
	div.u32 	%r66, %r65, %r64;
	cvt.u64.u32 	%rd84, %r66;

$L__BB32_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB32_24;

	or.b64  	%rd62, %rd84, %rd8;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB32_23;

	div.u64 	%rd84, %rd84, %rd8;
	bra.uni 	$L__BB32_24;

$L__BB32_23:
	cvt.u32.u64 	%r68, %rd84;
	div.u32 	%r69, %r68, %r67;
	cvt.u64.u32 	%rd84, %r69;

$L__BB32_24:
	cvt.u32.u64 	%r70, %rd84;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b32 	%r71, %r70, 0, %p14;
	cvt.s64.s32 	%rd64, %r71;
	mul.lo.s64 	%rd65, %rd64, %rd9;
	add.s64 	%rd66, %rd5, %rd65;
	ld.global.f64 	%fd13, [%rd66];
	ld.global.f64 	%fd14, [%rd66+8];
	ld.global.f64 	%fd15, [%rd66+16];
	ld.global.f64 	%fd16, [%rd66+24];
	ld.global.f64 	%fd17, [%rd66+32];
	ld.global.f64 	%fd18, [%rd66+40];
	ld.global.f64 	%fd19, [%rd66+48];
	ld.global.f64 	%fd20, [%rd66+56];
	ld.global.f64 	%fd21, [%rd66+64];
	ld.global.f64 	%fd22, [%rd66+72];
	ld.global.f64 	%fd23, [%rd66+80];
	ld.global.f64 	%fd24, [%rd66+88];
	shl.b32 	%r72, %r71, 2;
	cvt.s64.s32 	%rd67, %r72;
	mul.lo.s64 	%rd68, %rd67, %rd10;
	add.s64 	%rd69, %rd4, %rd68;
	st.global.f64 	[%rd69], %fd13;
	st.global.f64 	[%rd69+8], %fd14;
	st.global.f64 	[%rd69+16], %fd15;
	or.b32  	%r73, %r72, 1;
	cvt.s64.s32 	%rd70, %r73;
	mul.lo.s64 	%rd71, %rd70, %rd10;
	add.s64 	%rd72, %rd4, %rd71;
	st.global.f64 	[%rd72], %fd16;
	st.global.f64 	[%rd72+8], %fd17;
	st.global.f64 	[%rd72+16], %fd18;
	or.b32  	%r74, %r72, 2;
	cvt.s64.s32 	%rd73, %r74;
	mul.lo.s64 	%rd74, %rd73, %rd10;
	add.s64 	%rd75, %rd4, %rd74;
	st.global.f64 	[%rd75], %fd19;
	st.global.f64 	[%rd75+8], %fd20;
	st.global.f64 	[%rd75+16], %fd21;
	or.b32  	%r75, %r72, 3;
	cvt.s64.s32 	%rd76, %r75;
	mul.lo.s64 	%rd77, %rd76, %rd10;
	add.s64 	%rd78, %rd4, %rd77;
	st.global.f64 	[%rd78], %fd22;
	st.global.f64 	[%rd78+8], %fd23;
	st.global.f64 	[%rd78+16], %fd24;
	add.s64 	%rd79, %rd79, %rd11;
	setp.lt.u64 	%p15, %rd79, %rd31;
	@%p15 bra 	$L__BB32_13;
	bra.uni 	$L__BB32_25;

$L__BB32_2:
	cvt.u32.u64 	%r49, %rd7;
	cvt.u32.u64 	%r52, %rd8;

$L__BB32_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd80, %rd79;
	@%p3 bra 	$L__BB32_7;

	or.b64  	%rd39, %rd79, %rd7;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p4, %rd40, 0;
	@%p4 bra 	$L__BB32_6;

	div.u64 	%rd80, %rd79, %rd7;
	bra.uni 	$L__BB32_7;

$L__BB32_6:
	cvt.u32.u64 	%r50, %rd79;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd80, %r51;

$L__BB32_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB32_11;

	or.b64  	%rd41, %rd80, %rd8;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p6, %rd42, 0;
	@%p6 bra 	$L__BB32_10;

	div.u64 	%rd80, %rd80, %rd8;
	bra.uni 	$L__BB32_11;

$L__BB32_10:
	cvt.u32.u64 	%r53, %rd80;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd80, %r54;

$L__BB32_11:
	cvt.u32.u64 	%r55, %rd80;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b32 	%r56, %r55, 0, %p7;
	cvt.s64.s32 	%rd43, %r56;
	mul.lo.s64 	%rd44, %rd43, %rd9;
	add.s64 	%rd45, %rd5, %rd44;
	ld.global.f64 	%fd1, [%rd45];
	ld.global.f64 	%fd2, [%rd45+8];
	ld.global.f64 	%fd3, [%rd45+16];
	ld.global.f64 	%fd4, [%rd45+24];
	ld.global.f64 	%fd5, [%rd45+32];
	ld.global.f64 	%fd6, [%rd45+40];
	ld.global.f64 	%fd7, [%rd45+48];
	ld.global.f64 	%fd8, [%rd45+56];
	ld.global.f64 	%fd9, [%rd45+64];
	ld.global.f64 	%fd10, [%rd45+72];
	ld.global.f64 	%fd11, [%rd45+80];
	ld.global.f64 	%fd12, [%rd45+88];
	shl.b32 	%r57, %r56, 2;
	cvt.s64.s32 	%rd46, %r57;
	mul.lo.s64 	%rd47, %rd46, %rd10;
	add.s64 	%rd48, %rd4, %rd47;
	st.global.f64 	[%rd48], %fd1;
	st.global.f64 	[%rd48+8], %fd2;
	st.global.f64 	[%rd48+16], %fd3;
	or.b32  	%r58, %r57, 1;
	cvt.s64.s32 	%rd49, %r58;
	mul.lo.s64 	%rd50, %rd49, %rd10;
	add.s64 	%rd51, %rd4, %rd50;
	st.global.f64 	[%rd51], %fd4;
	st.global.f64 	[%rd51+8], %fd5;
	st.global.f64 	[%rd51+16], %fd6;
	or.b32  	%r59, %r57, 2;
	cvt.s64.s32 	%rd52, %r59;
	mul.lo.s64 	%rd53, %rd52, %rd10;
	add.s64 	%rd54, %rd4, %rd53;
	st.global.f64 	[%rd54], %fd7;
	st.global.f64 	[%rd54+8], %fd8;
	st.global.f64 	[%rd54+16], %fd9;
	or.b32  	%r60, %r57, 3;
	cvt.s64.s32 	%rd55, %r60;
	mul.lo.s64 	%rd56, %rd55, %rd10;
	add.s64 	%rd57, %rd4, %rd56;
	st.global.f64 	[%rd57], %fd10;
	st.global.f64 	[%rd57+8], %fd11;
	st.global.f64 	[%rd57+16], %fd12;
	add.s64 	%rd79, %rd79, %rd11;
	setp.lt.u64 	%p8, %rd79, %rd31;
	@%p8 bra 	$L__BB32_3;

$L__BB32_25:
	ret;

}
	// .globl	affine_to_sys_grad_cuda_kernel_backward
.visible .entry affine_to_sys_grad_cuda_kernel_backward(
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 affine_to_sys_grad_cuda_kernel_backward_param_4[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<101>;
	.reg .f64 	%fd<145>;
	.reg .b64 	%rd<103>;


	ld.param.v2.u32 	{%r45, %r46}, [affine_to_sys_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r47, %r48}, [affine_to_sys_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r53, %r54}, [affine_to_sys_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r61, %r62}, [affine_to_sys_grad_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r69, %r70}, [affine_to_sys_grad_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r77, %r78}, [affine_to_sys_grad_cuda_kernel_backward_param_4+32];
	ld.param.u64 	%rd36, [affine_to_sys_grad_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd34, [affine_to_sys_grad_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd33, [affine_to_sys_grad_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [affine_to_sys_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [affine_to_sys_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r8, [affine_to_sys_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r81, %ntid.x;
	cvt.u64.u32 	%rd1, %r81;
	mov.u32 	%r82, %ctaid.x;
	mul.wide.u32 	%rd38, %r81, %r82;
	mov.u32 	%r83, %tid.x;
	cvt.u64.u32 	%rd39, %r83;
	add.s64 	%rd99, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd99, %rd29;
	@%p1 bra 	$L__BB33_35;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r48;
	cvt.s64.s32 	%rd11, %r47;
	cvt.s64.s32 	%rd12, %r46;
	cvt.s64.s32 	%rd13, %r77;
	cvt.s64.s32 	%rd14, %r61;
	mov.u32 	%r84, %nctaid.x;
	cvt.u64.u32 	%rd40, %r84;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r69;
	cvt.s64.s32 	%rd17, %r53;

$L__BB33_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd100, %rd99;
	@%p2 bra 	$L__BB33_6;

	or.b64  	%rd41, %rd99, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB33_5;

	div.u64 	%rd100, %rd99, %rd10;
	bra.uni 	$L__BB33_6;

$L__BB33_5:
	cvt.u32.u64 	%r85, %rd10;
	cvt.u32.u64 	%r86, %rd99;
	div.u32 	%r87, %r86, %r85;
	cvt.u64.u32 	%rd100, %r87;

$L__BB33_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB33_10;

	or.b64  	%rd43, %rd100, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB33_9;

	div.u64 	%rd100, %rd100, %rd11;
	bra.uni 	$L__BB33_10;

$L__BB33_9:
	cvt.u32.u64 	%r88, %rd11;
	cvt.u32.u64 	%r89, %rd100;
	div.u32 	%r90, %r89, %r88;
	cvt.u64.u32 	%rd100, %r90;

$L__BB33_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB33_14;

	or.b64  	%rd45, %rd100, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB33_13;

	div.u64 	%rd100, %rd100, %rd12;
	bra.uni 	$L__BB33_14;

$L__BB33_13:
	cvt.u32.u64 	%r91, %rd12;
	cvt.u32.u64 	%r92, %rd100;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd100, %r93;

$L__BB33_14:
	cvt.u32.u64 	%r94, %rd100;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r94, 0, %p8;
	shl.b32 	%r3, %r2, 2;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB33_16;

	add.s32 	%r95, %r3, 3;
	cvt.s64.s32 	%rd47, %r95;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd49, [%rd49];
	add.f64 	%fd135, %fd49, 0d0000000000000000;
	ld.global.f64 	%fd50, [%rd49+8];
	add.f64 	%fd134, %fd50, 0d0000000000000000;
	ld.global.f64 	%fd51, [%rd49+16];
	add.f64 	%fd133, %fd51, 0d0000000000000000;
	bra.uni 	$L__BB33_18;

$L__BB33_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd133, 0d0000000000000000;
	mov.f64 	%fd134, %fd133;
	mov.f64 	%fd135, %fd133;
	@%p10 bra 	$L__BB33_18;

	add.s32 	%r96, %r3, 3;
	cvt.s64.s32 	%rd50, %r96;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd55, [%rd52];
	add.f64 	%fd135, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd52+8];
	add.f64 	%fd134, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd52+16];
	add.f64 	%fd133, %fd57, 0d0000000000000000;

$L__BB33_18:
	add.f64 	%fd10, %fd135, 0d0000000000000000;
	add.f64 	%fd11, %fd134, 0d0000000000000000;
	add.f64 	%fd12, %fd133, 0d0000000000000000;
	@%p9 bra 	$L__BB33_20;

	add.s32 	%r97, %r3, 2;
	cvt.s64.s32 	%rd53, %r97;
	mul.lo.s64 	%rd54, %rd53, %rd13;
	add.s64 	%rd55, %rd8, %rd54;
	ld.global.f64 	%fd58, [%rd55];
	add.f64 	%fd138, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd55+8];
	add.f64 	%fd137, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd55+16];
	add.f64 	%fd136, %fd60, 0d0000000000000000;
	bra.uni 	$L__BB33_22;

$L__BB33_20:
	setp.eq.s64 	%p12, %rd33, 0;
	mov.f64 	%fd136, 0d0000000000000000;
	mov.f64 	%fd137, %fd136;
	mov.f64 	%fd138, %fd136;
	@%p12 bra 	$L__BB33_22;

	add.s32 	%r98, %r3, 2;
	cvt.s64.s32 	%rd56, %r98;
	mul.lo.s64 	%rd57, %rd56, %rd14;
	add.s64 	%rd58, %rd9, %rd57;
	ld.global.f64 	%fd64, [%rd58];
	add.f64 	%fd138, %fd64, 0d0000000000000000;
	ld.global.f64 	%fd65, [%rd58+8];
	add.f64 	%fd137, %fd65, 0d0000000000000000;
	ld.global.f64 	%fd66, [%rd58+16];
	add.f64 	%fd136, %fd66, 0d0000000000000000;

$L__BB33_22:
	add.f64 	%fd22, %fd138, 0d0000000000000000;
	add.f64 	%fd23, %fd137, 0d0000000000000000;
	add.f64 	%fd24, %fd136, 0d0000000000000000;
	@%p9 bra 	$L__BB33_24;

	add.s32 	%r99, %r3, 1;
	cvt.s64.s32 	%rd59, %r99;
	mul.lo.s64 	%rd60, %rd59, %rd13;
	add.s64 	%rd61, %rd8, %rd60;
	ld.global.f64 	%fd67, [%rd61];
	add.f64 	%fd141, %fd67, 0d0000000000000000;
	ld.global.f64 	%fd68, [%rd61+8];
	add.f64 	%fd140, %fd68, 0d0000000000000000;
	ld.global.f64 	%fd69, [%rd61+16];
	add.f64 	%fd139, %fd69, 0d0000000000000000;
	bra.uni 	$L__BB33_26;

$L__BB33_24:
	setp.eq.s64 	%p14, %rd33, 0;
	mov.f64 	%fd139, 0d0000000000000000;
	mov.f64 	%fd140, %fd139;
	mov.f64 	%fd141, %fd139;
	@%p14 bra 	$L__BB33_26;

	add.s32 	%r100, %r3, 1;
	cvt.s64.s32 	%rd62, %r100;
	mul.lo.s64 	%rd63, %rd62, %rd14;
	add.s64 	%rd64, %rd9, %rd63;
	ld.global.f64 	%fd73, [%rd64];
	add.f64 	%fd141, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd64+8];
	add.f64 	%fd140, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd64+16];
	add.f64 	%fd139, %fd75, 0d0000000000000000;

$L__BB33_26:
	add.f64 	%fd34, %fd141, 0d0000000000000000;
	add.f64 	%fd35, %fd140, 0d0000000000000000;
	add.f64 	%fd36, %fd139, 0d0000000000000000;
	@%p9 bra 	$L__BB33_28;

	cvt.s64.s32 	%rd65, %r3;
	mul.lo.s64 	%rd66, %rd65, %rd13;
	add.s64 	%rd67, %rd8, %rd66;
	ld.global.f64 	%fd76, [%rd67];
	add.f64 	%fd144, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd67+8];
	add.f64 	%fd143, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd67+16];
	add.f64 	%fd142, %fd78, 0d0000000000000000;
	bra.uni 	$L__BB33_30;

$L__BB33_28:
	setp.eq.s64 	%p16, %rd33, 0;
	mov.f64 	%fd142, 0d0000000000000000;
	mov.f64 	%fd143, %fd142;
	mov.f64 	%fd144, %fd142;
	@%p16 bra 	$L__BB33_30;

	cvt.s64.s32 	%rd68, %r3;
	mul.lo.s64 	%rd69, %rd68, %rd14;
	add.s64 	%rd70, %rd9, %rd69;
	ld.global.f64 	%fd82, [%rd70];
	add.f64 	%fd144, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd70+8];
	add.f64 	%fd143, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd70+16];
	add.f64 	%fd142, %fd84, 0d0000000000000000;

$L__BB33_30:
	add.f64 	%fd46, %fd144, 0d0000000000000000;
	add.f64 	%fd47, %fd143, 0d0000000000000000;
	add.f64 	%fd48, %fd142, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd34, 0;
	@%p17 bra 	$L__BB33_32;

	cvt.s64.s32 	%rd83, %r2;
	mul.lo.s64 	%rd84, %rd83, %rd16;
	add.s64 	%rd71, %rd34, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd71],%fd46; }

	// end inline asm
	add.s64 	%rd72, %rd71, 8;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd72],%fd47; }

	// end inline asm
	add.s64 	%rd73, %rd71, 16;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd73],%fd48; }

	// end inline asm
	add.s64 	%rd74, %rd71, 24;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd74],%fd34; }

	// end inline asm
	add.s64 	%rd75, %rd71, 32;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd75],%fd35; }

	// end inline asm
	add.s64 	%rd76, %rd71, 40;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd76],%fd36; }

	// end inline asm
	add.s64 	%rd77, %rd71, 48;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd77],%fd22; }

	// end inline asm
	add.s64 	%rd78, %rd71, 56;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd78],%fd23; }

	// end inline asm
	add.s64 	%rd79, %rd71, 64;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd79],%fd24; }

	// end inline asm
	add.s64 	%rd80, %rd71, 72;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd80],%fd10; }

	// end inline asm
	add.s64 	%rd81, %rd71, 80;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd81],%fd11; }

	// end inline asm
	add.s64 	%rd82, %rd71, 88;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd82],%fd12; }

	// end inline asm
	bra.uni 	$L__BB33_34;

$L__BB33_32:
	setp.eq.s64 	%p18, %rd31, 0;
	@%p18 bra 	$L__BB33_34;

	cvt.s64.s32 	%rd97, %r2;
	mul.lo.s64 	%rd98, %rd97, %rd17;
	add.s64 	%rd85, %rd31, %rd98;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd85],%fd46; }

	// end inline asm
	add.s64 	%rd86, %rd85, 8;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd86],%fd47; }

	// end inline asm
	add.s64 	%rd87, %rd85, 16;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd87],%fd48; }

	// end inline asm
	add.s64 	%rd88, %rd85, 24;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd88],%fd34; }

	// end inline asm
	add.s64 	%rd89, %rd85, 32;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd89],%fd35; }

	// end inline asm
	add.s64 	%rd90, %rd85, 40;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd90],%fd36; }

	// end inline asm
	add.s64 	%rd91, %rd85, 48;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd91],%fd22; }

	// end inline asm
	add.s64 	%rd92, %rd85, 56;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd92],%fd23; }

	// end inline asm
	add.s64 	%rd93, %rd85, 64;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd93],%fd24; }

	// end inline asm
	add.s64 	%rd94, %rd85, 72;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd94],%fd10; }

	// end inline asm
	add.s64 	%rd95, %rd85, 80;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd95],%fd11; }

	// end inline asm
	add.s64 	%rd96, %rd85, 88;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd96],%fd12; }

	// end inline asm

$L__BB33_34:
	add.s64 	%rd99, %rd99, %rd15;
	setp.lt.u64 	%p19, %rd99, %rd29;
	@%p19 bra 	$L__BB33_2;

$L__BB33_35:
	ret;

}
	// .globl	initialize_tilde_y_cuda_kernel_forward
.visible .entry initialize_tilde_y_cuda_kernel_forward(
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_forward_param_3[56],
	.param .f64 initialize_tilde_y_cuda_kernel_forward_param_4,
	.param .u32 initialize_tilde_y_cuda_kernel_forward_param_5
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<79>;
	.reg .f64 	%fd<50>;
	.reg .b64 	%rd<51>;


	ld.param.v2.u32 	{%r36, %r37}, [initialize_tilde_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r38, %r39}, [initialize_tilde_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r44, %r45}, [initialize_tilde_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r52, %r53}, [initialize_tilde_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r60, %r61}, [initialize_tilde_y_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [initialize_tilde_y_cuda_kernel_forward_param_4];
	ld.param.u32 	%r35, [initialize_tilde_y_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd30, [initialize_tilde_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd28, [initialize_tilde_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd26, [initialize_tilde_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd25, [initialize_tilde_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [initialize_tilde_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r64, %ntid.x;
	mov.u32 	%r65, %ctaid.x;
	mul.wide.u32 	%rd32, %r64, %r65;
	mov.u32 	%r66, %tid.x;
	cvt.u64.u32 	%rd33, %r66;
	add.s64 	%rd47, %rd32, %rd33;
	setp.ge.u64 	%p1, %rd47, %rd25;
	@%p1 bra 	$L__BB34_19;

	cvta.to.global.u64 	%rd3, %rd28;
	cvta.to.global.u64 	%rd4, %rd26;
	cvta.to.global.u64 	%rd5, %rd30;
	cvt.s64.s32 	%rd6, %r39;
	cvt.s64.s32 	%rd7, %r38;
	cvt.s64.s32 	%rd8, %r37;
	cvt.s64.s32 	%rd9, %r52;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r60;

$L__BB34_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd48, %rd47;
	@%p2 bra 	$L__BB34_6;

	or.b64  	%rd34, %rd47, %rd6;
	and.b64  	%rd35, %rd34, -4294967296;
	setp.eq.s64 	%p3, %rd35, 0;
	@%p3 bra 	$L__BB34_5;

	div.u64 	%rd48, %rd47, %rd6;
	bra.uni 	$L__BB34_6;

$L__BB34_5:
	cvt.u32.u64 	%r67, %rd6;
	cvt.u32.u64 	%r68, %rd47;
	div.u32 	%r69, %r68, %r67;
	cvt.u64.u32 	%rd48, %r69;

$L__BB34_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB34_10;

	or.b64  	%rd36, %rd48, %rd7;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p5, %rd37, 0;
	@%p5 bra 	$L__BB34_9;

	div.u64 	%rd48, %rd48, %rd7;
	bra.uni 	$L__BB34_10;

$L__BB34_9:
	cvt.u32.u64 	%r70, %rd7;
	cvt.u32.u64 	%r71, %rd48;
	div.u32 	%r72, %r71, %r70;
	cvt.u64.u32 	%rd48, %r72;

$L__BB34_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB34_14;

	or.b64  	%rd38, %rd48, %rd8;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p7, %rd39, 0;
	@%p7 bra 	$L__BB34_13;

	div.u64 	%rd48, %rd48, %rd8;
	bra.uni 	$L__BB34_14;

$L__BB34_13:
	cvt.u32.u64 	%r73, %rd8;
	cvt.u32.u64 	%r74, %rd48;
	div.u32 	%r75, %r74, %r73;
	cvt.u64.u32 	%rd48, %r75;

$L__BB34_14:
	cvt.u32.u64 	%r76, %rd48;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r76, 0, %p8;
	cvt.s64.s32 	%rd40, %r2;
	mul.lo.s64 	%rd41, %rd40, %rd9;
	add.s64 	%rd22, %rd3, %rd41;
	mul.lo.s64 	%rd42, %rd40, %rd10;
	add.s64 	%rd23, %rd4, %rd42;
	setp.eq.s32 	%p9, %r35, 0;
	@%p9 bra 	$L__BB34_17;

	setp.ne.s32 	%p10, %r35, 1;
	@%p10 bra 	$L__BB34_18;

	mul.lo.s64 	%rd44, %rd40, %rd11;
	add.s64 	%rd45, %rd5, %rd44;
	ld.global.f64 	%fd2, [%rd45];
	ld.global.f64 	%fd3, [%rd45+8];
	ld.global.f64 	%fd4, [%rd45+16];
	ld.global.f64 	%fd5, [%rd45+24];
	ld.global.f64 	%fd6, [%rd45+32];
	ld.global.f64 	%fd7, [%rd45+40];
	ld.global.f64 	%fd8, [%rd45+48];
	ld.global.f64 	%fd9, [%rd45+56];
	ld.global.f64 	%fd10, [%rd45+64];
	ld.global.f64 	%fd11, [%rd45+72];
	ld.global.f64 	%fd12, [%rd45+80];
	ld.global.f64 	%fd13, [%rd45+88];
	ld.global.f64 	%fd14, [%rd22];
	fma.rn.f64 	%fd15, %fd2, %fd1, %fd14;
	ld.global.f64 	%fd16, [%rd22+8];
	fma.rn.f64 	%fd17, %fd3, %fd1, %fd16;
	ld.global.f64 	%fd18, [%rd22+16];
	fma.rn.f64 	%fd19, %fd4, %fd1, %fd18;
	ld.global.f64 	%fd20, [%rd22+24];
	fma.rn.f64 	%fd21, %fd5, %fd1, %fd20;
	ld.global.f64 	%fd22, [%rd22+32];
	fma.rn.f64 	%fd23, %fd6, %fd1, %fd22;
	ld.global.f64 	%fd24, [%rd22+40];
	fma.rn.f64 	%fd25, %fd7, %fd1, %fd24;
	ld.global.f64 	%fd26, [%rd22+48];
	fma.rn.f64 	%fd27, %fd8, %fd1, %fd26;
	ld.global.f64 	%fd28, [%rd22+56];
	fma.rn.f64 	%fd29, %fd9, %fd1, %fd28;
	ld.global.f64 	%fd30, [%rd22+64];
	fma.rn.f64 	%fd31, %fd10, %fd1, %fd30;
	ld.global.f64 	%fd32, [%rd22+72];
	fma.rn.f64 	%fd33, %fd11, %fd1, %fd32;
	ld.global.f64 	%fd34, [%rd22+80];
	fma.rn.f64 	%fd35, %fd12, %fd1, %fd34;
	ld.global.f64 	%fd36, [%rd22+88];
	fma.rn.f64 	%fd37, %fd13, %fd1, %fd36;
	st.global.f64 	[%rd23], %fd15;
	st.global.f64 	[%rd23+8], %fd17;
	st.global.f64 	[%rd23+16], %fd19;
	st.global.f64 	[%rd23+24], %fd21;
	st.global.f64 	[%rd23+32], %fd23;
	st.global.f64 	[%rd23+40], %fd25;
	st.global.f64 	[%rd23+48], %fd27;
	st.global.f64 	[%rd23+56], %fd29;
	st.global.f64 	[%rd23+64], %fd31;
	st.global.f64 	[%rd23+72], %fd33;
	st.global.f64 	[%rd23+80], %fd35;
	st.global.f64 	[%rd23+88], %fd37;
	bra.uni 	$L__BB34_18;

$L__BB34_17:
	ld.global.f64 	%fd38, [%rd22];
	ld.global.f64 	%fd39, [%rd22+8];
	ld.global.f64 	%fd40, [%rd22+16];
	ld.global.f64 	%fd41, [%rd22+24];
	ld.global.f64 	%fd42, [%rd22+32];
	ld.global.f64 	%fd43, [%rd22+40];
	ld.global.f64 	%fd44, [%rd22+48];
	ld.global.f64 	%fd45, [%rd22+56];
	ld.global.f64 	%fd46, [%rd22+64];
	ld.global.f64 	%fd47, [%rd22+72];
	ld.global.f64 	%fd48, [%rd22+80];
	ld.global.f64 	%fd49, [%rd22+88];
	st.global.f64 	[%rd23], %fd38;
	st.global.f64 	[%rd23+8], %fd39;
	st.global.f64 	[%rd23+16], %fd40;
	st.global.f64 	[%rd23+24], %fd41;
	st.global.f64 	[%rd23+32], %fd42;
	st.global.f64 	[%rd23+40], %fd43;
	st.global.f64 	[%rd23+48], %fd44;
	st.global.f64 	[%rd23+56], %fd45;
	st.global.f64 	[%rd23+64], %fd46;
	st.global.f64 	[%rd23+72], %fd47;
	st.global.f64 	[%rd23+80], %fd48;
	st.global.f64 	[%rd23+88], %fd49;

$L__BB34_18:
	mov.u32 	%r78, %nctaid.x;
	mul.wide.u32 	%rd46, %r64, %r78;
	add.s64 	%rd47, %rd47, %rd46;
	setp.lt.u64 	%p11, %rd47, %rd25;
	@%p11 bra 	$L__BB34_2;

$L__BB34_19:
	ret;

}
	// .globl	initialize_tilde_y_cuda_kernel_backward
.visible .entry initialize_tilde_y_cuda_kernel_backward(
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_3[56],
	.param .f64 initialize_tilde_y_cuda_kernel_backward_param_4,
	.param .u32 initialize_tilde_y_cuda_kernel_backward_param_5,
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_tilde_y_cuda_kernel_backward_param_8[56],
	.param .f64 initialize_tilde_y_cuda_kernel_backward_param_9,
	.param .u32 initialize_tilde_y_cuda_kernel_backward_param_10
)
{
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<55>;
	.reg .b32 	%r<130>;
	.reg .f64 	%fd<363>;
	.reg .b64 	%rd<151>;


	ld.param.v2.u32 	{%r63, %r64}, [initialize_tilde_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r65, %r66}, [initialize_tilde_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r71, %r72}, [initialize_tilde_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r79, %r80}, [initialize_tilde_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r87, %r88}, [initialize_tilde_y_cuda_kernel_backward_param_3+32];
	ld.param.u32 	%r35, [initialize_tilde_y_cuda_kernel_backward_param_5];
	ld.param.v2.u32 	{%r95, %r96}, [initialize_tilde_y_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r103, %r104}, [initialize_tilde_y_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r111, %r112}, [initialize_tilde_y_cuda_kernel_backward_param_8+32];
	ld.param.u64 	%rd46, [initialize_tilde_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd44, [initialize_tilde_y_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd42, [initialize_tilde_y_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd41, [initialize_tilde_y_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd39, [initialize_tilde_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd37, [initialize_tilde_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd35, [initialize_tilde_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [initialize_tilde_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r115, %ntid.x;
	cvt.u64.u32 	%rd1, %r115;
	mov.u32 	%r116, %ctaid.x;
	mul.wide.u32 	%rd48, %r115, %r116;
	mov.u32 	%r117, %tid.x;
	cvt.u64.u32 	%rd49, %r117;
	add.s64 	%rd147, %rd48, %rd49;
	setp.ge.u64 	%p2, %rd147, %rd35;
	@%p2 bra 	$L__BB35_37;

	cvt.s64.s32 	%rd12, %r66;
	cvt.s64.s32 	%rd13, %r65;
	cvt.s64.s32 	%rd14, %r64;
	setp.ne.s32 	%p1, %r35, 0;
	cvt.s64.s32 	%rd15, %r79;
	cvt.s64.s32 	%rd16, %r87;
	cvt.s64.s32 	%rd17, %r95;
	mov.u32 	%r118, %nctaid.x;
	cvt.u64.u32 	%rd50, %r118;
	mul.lo.s64 	%rd18, %rd1, %rd50;
	cvt.s64.s32 	%rd19, %r71;
	cvt.s64.s32 	%rd20, %r103;
	cvt.s64.s32 	%rd21, %r111;
	not.pred 	%p13, %p1;

$L__BB35_2:
	setp.lt.s32 	%p3, %r7, 4;
	mov.u64 	%rd148, %rd147;
	@%p3 bra 	$L__BB35_6;

	or.b64  	%rd51, %rd147, %rd12;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.eq.s64 	%p4, %rd52, 0;
	@%p4 bra 	$L__BB35_5;

	div.u64 	%rd148, %rd147, %rd12;
	bra.uni 	$L__BB35_6;

$L__BB35_5:
	cvt.u32.u64 	%r119, %rd12;
	cvt.u32.u64 	%r120, %rd147;
	div.u32 	%r121, %r120, %r119;
	cvt.u64.u32 	%rd148, %r121;

$L__BB35_6:
	setp.lt.s32 	%p5, %r7, 3;
	@%p5 bra 	$L__BB35_10;

	or.b64  	%rd53, %rd148, %rd13;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p6, %rd54, 0;
	@%p6 bra 	$L__BB35_9;

	div.u64 	%rd148, %rd148, %rd13;
	bra.uni 	$L__BB35_10;

$L__BB35_9:
	cvt.u32.u64 	%r122, %rd13;
	cvt.u32.u64 	%r123, %rd148;
	div.u32 	%r124, %r123, %r122;
	cvt.u64.u32 	%rd148, %r124;

$L__BB35_10:
	setp.lt.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB35_14;

	or.b64  	%rd55, %rd148, %rd14;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p8, %rd56, 0;
	@%p8 bra 	$L__BB35_13;

	div.u64 	%rd148, %rd148, %rd14;
	bra.uni 	$L__BB35_14;

$L__BB35_13:
	cvt.u32.u64 	%r125, %rd14;
	cvt.u32.u64 	%r126, %rd148;
	div.u32 	%r127, %r126, %r125;
	cvt.u64.u32 	%rd148, %r127;

$L__BB35_14:
	cvta.to.global.u64 	%rd146, %rd37;
	cvta.to.global.u64 	%rd145, %rd42;
	ld.param.u32 	%r129, [initialize_tilde_y_cuda_kernel_backward_param_5];
	cvt.u32.u64 	%r128, %rd148;
	setp.gt.s32 	%p9, %r7, 0;
	selp.b32 	%r2, %r128, 0, %p9;
	setp.eq.s32 	%p10, %r129, 1;
	selp.u16 	%rs52, 1, 0, %p10;
	setp.eq.s32 	%p11, %r129, 0;
	selp.b16 	%rs54, %rs54, %rs52, %p11;
	and.b16  	%rs53, %rs54, 255;
	setp.eq.s16 	%p12, %rs53, 0;
	cvt.s64.s32 	%rd57, %r2;
	mul.lo.s64 	%rd58, %rd57, %rd17;
	add.s64 	%rd32, %rd145, %rd58;
	mul.lo.s64 	%rd59, %rd57, %rd19;
	add.s64 	%rd33, %rd146, %rd59;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB35_27;

	setp.eq.s64 	%p15, %rd42, 0;
	@%p15 bra 	$L__BB35_17;

	ld.global.f64 	%fd98, [%rd32];
	add.f64 	%fd350, %fd98, 0d0000000000000000;
	ld.global.f64 	%fd99, [%rd32+8];
	add.f64 	%fd349, %fd99, 0d0000000000000000;
	ld.global.f64 	%fd100, [%rd32+16];
	add.f64 	%fd348, %fd100, 0d0000000000000000;
	ld.global.f64 	%fd101, [%rd32+24];
	add.f64 	%fd347, %fd101, 0d0000000000000000;
	ld.global.f64 	%fd102, [%rd32+32];
	add.f64 	%fd346, %fd102, 0d0000000000000000;
	ld.global.f64 	%fd103, [%rd32+40];
	add.f64 	%fd345, %fd103, 0d0000000000000000;
	ld.global.f64 	%fd104, [%rd32+48];
	add.f64 	%fd344, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd32+56];
	add.f64 	%fd343, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd32+64];
	add.f64 	%fd342, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd32+72];
	add.f64 	%fd341, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd32+80];
	add.f64 	%fd340, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd32+88];
	add.f64 	%fd339, %fd109, 0d0000000000000000;
	bra.uni 	$L__BB35_19;

$L__BB35_17:
	setp.eq.s64 	%p16, %rd37, 0;
	mov.f64 	%fd339, 0d0000000000000000;
	mov.f64 	%fd340, %fd339;
	mov.f64 	%fd341, %fd339;
	mov.f64 	%fd342, %fd339;
	mov.f64 	%fd343, %fd339;
	mov.f64 	%fd344, %fd339;
	mov.f64 	%fd345, %fd339;
	mov.f64 	%fd346, %fd339;
	mov.f64 	%fd347, %fd339;
	mov.f64 	%fd348, %fd339;
	mov.f64 	%fd349, %fd339;
	mov.f64 	%fd350, %fd339;
	@%p16 bra 	$L__BB35_19;

	ld.global.f64 	%fd122, [%rd33];
	add.f64 	%fd350, %fd122, 0d0000000000000000;
	ld.global.f64 	%fd123, [%rd33+8];
	add.f64 	%fd349, %fd123, 0d0000000000000000;
	ld.global.f64 	%fd124, [%rd33+16];
	add.f64 	%fd348, %fd124, 0d0000000000000000;
	ld.global.f64 	%fd125, [%rd33+24];
	add.f64 	%fd347, %fd125, 0d0000000000000000;
	ld.global.f64 	%fd126, [%rd33+32];
	add.f64 	%fd346, %fd126, 0d0000000000000000;
	ld.global.f64 	%fd127, [%rd33+40];
	add.f64 	%fd345, %fd127, 0d0000000000000000;
	ld.global.f64 	%fd128, [%rd33+48];
	add.f64 	%fd344, %fd128, 0d0000000000000000;
	ld.global.f64 	%fd129, [%rd33+56];
	add.f64 	%fd343, %fd129, 0d0000000000000000;
	ld.global.f64 	%fd130, [%rd33+64];
	add.f64 	%fd342, %fd130, 0d0000000000000000;
	ld.global.f64 	%fd131, [%rd33+72];
	add.f64 	%fd341, %fd131, 0d0000000000000000;
	ld.global.f64 	%fd132, [%rd33+80];
	add.f64 	%fd340, %fd132, 0d0000000000000000;
	ld.global.f64 	%fd133, [%rd33+88];
	add.f64 	%fd339, %fd133, 0d0000000000000000;

$L__BB35_19:
	ld.param.f64 	%fd326, [initialize_tilde_y_cuda_kernel_backward_param_4];
	add.f64 	%fd37, %fd350, 0d0000000000000000;
	fma.rn.f64 	%fd38, %fd37, %fd326, 0d0000000000000000;
	add.f64 	%fd39, %fd349, 0d0000000000000000;
	fma.rn.f64 	%fd40, %fd39, %fd326, 0d0000000000000000;
	add.f64 	%fd41, %fd348, 0d0000000000000000;
	fma.rn.f64 	%fd42, %fd41, %fd326, 0d0000000000000000;
	add.f64 	%fd43, %fd347, 0d0000000000000000;
	fma.rn.f64 	%fd44, %fd43, %fd326, 0d0000000000000000;
	add.f64 	%fd45, %fd346, 0d0000000000000000;
	fma.rn.f64 	%fd46, %fd45, %fd326, 0d0000000000000000;
	add.f64 	%fd47, %fd345, 0d0000000000000000;
	fma.rn.f64 	%fd48, %fd47, %fd326, 0d0000000000000000;
	add.f64 	%fd49, %fd344, 0d0000000000000000;
	fma.rn.f64 	%fd50, %fd49, %fd326, 0d0000000000000000;
	add.f64 	%fd51, %fd343, 0d0000000000000000;
	fma.rn.f64 	%fd52, %fd51, %fd326, 0d0000000000000000;
	add.f64 	%fd53, %fd342, 0d0000000000000000;
	fma.rn.f64 	%fd54, %fd53, %fd326, 0d0000000000000000;
	add.f64 	%fd55, %fd341, 0d0000000000000000;
	fma.rn.f64 	%fd56, %fd55, %fd326, 0d0000000000000000;
	add.f64 	%fd57, %fd340, 0d0000000000000000;
	fma.rn.f64 	%fd58, %fd57, %fd326, 0d0000000000000000;
	add.f64 	%fd59, %fd339, 0d0000000000000000;
	fma.rn.f64 	%fd60, %fd59, %fd326, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd46, 0;
	@%p17 bra 	$L__BB35_21;

	mul.lo.s64 	%rd73, %rd57, %rd21;
	add.s64 	%rd60, %rd46, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd134,[%rd60],%fd38; }

	// end inline asm
	add.s64 	%rd61, %rd60, 8;
	// begin inline asm
	{ atom.add.f64 %fd136,[%rd61],%fd40; }

	// end inline asm
	add.s64 	%rd62, %rd60, 16;
	// begin inline asm
	{ atom.add.f64 %fd138,[%rd62],%fd42; }

	// end inline asm
	add.s64 	%rd63, %rd60, 24;
	// begin inline asm
	{ atom.add.f64 %fd140,[%rd63],%fd44; }

	// end inline asm
	add.s64 	%rd64, %rd60, 32;
	// begin inline asm
	{ atom.add.f64 %fd142,[%rd64],%fd46; }

	// end inline asm
	add.s64 	%rd65, %rd60, 40;
	// begin inline asm
	{ atom.add.f64 %fd144,[%rd65],%fd48; }

	// end inline asm
	add.s64 	%rd66, %rd60, 48;
	// begin inline asm
	{ atom.add.f64 %fd146,[%rd66],%fd50; }

	// end inline asm
	add.s64 	%rd67, %rd60, 56;
	// begin inline asm
	{ atom.add.f64 %fd148,[%rd67],%fd52; }

	// end inline asm
	add.s64 	%rd68, %rd60, 64;
	// begin inline asm
	{ atom.add.f64 %fd150,[%rd68],%fd54; }

	// end inline asm
	add.s64 	%rd69, %rd60, 72;
	// begin inline asm
	{ atom.add.f64 %fd152,[%rd69],%fd56; }

	// end inline asm
	add.s64 	%rd70, %rd60, 80;
	// begin inline asm
	{ atom.add.f64 %fd154,[%rd70],%fd58; }

	// end inline asm
	add.s64 	%rd71, %rd60, 88;
	// begin inline asm
	{ atom.add.f64 %fd156,[%rd71],%fd60; }

	// end inline asm
	bra.uni 	$L__BB35_23;

$L__BB35_21:
	setp.eq.s64 	%p18, %rd41, 0;
	@%p18 bra 	$L__BB35_23;

	mul.lo.s64 	%rd87, %rd57, %rd16;
	add.s64 	%rd74, %rd41, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd158,[%rd74],%fd38; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd160,[%rd75],%fd40; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd162,[%rd76],%fd42; }

	// end inline asm
	add.s64 	%rd77, %rd74, 24;
	// begin inline asm
	{ atom.add.f64 %fd164,[%rd77],%fd44; }

	// end inline asm
	add.s64 	%rd78, %rd74, 32;
	// begin inline asm
	{ atom.add.f64 %fd166,[%rd78],%fd46; }

	// end inline asm
	add.s64 	%rd79, %rd74, 40;
	// begin inline asm
	{ atom.add.f64 %fd168,[%rd79],%fd48; }

	// end inline asm
	add.s64 	%rd80, %rd74, 48;
	// begin inline asm
	{ atom.add.f64 %fd170,[%rd80],%fd50; }

	// end inline asm
	add.s64 	%rd81, %rd74, 56;
	// begin inline asm
	{ atom.add.f64 %fd172,[%rd81],%fd52; }

	// end inline asm
	add.s64 	%rd82, %rd74, 64;
	// begin inline asm
	{ atom.add.f64 %fd174,[%rd82],%fd54; }

	// end inline asm
	add.s64 	%rd83, %rd74, 72;
	// begin inline asm
	{ atom.add.f64 %fd176,[%rd83],%fd56; }

	// end inline asm
	add.s64 	%rd84, %rd74, 80;
	// begin inline asm
	{ atom.add.f64 %fd178,[%rd84],%fd58; }

	// end inline asm
	add.s64 	%rd85, %rd74, 88;
	// begin inline asm
	{ atom.add.f64 %fd180,[%rd85],%fd60; }

	// end inline asm

$L__BB35_23:
	setp.eq.s64 	%p19, %rd44, 0;
	@%p19 bra 	$L__BB35_25;

	add.f64 	%fd325, %fd339, 0d0000000000000000;
	add.f64 	%fd324, %fd340, 0d0000000000000000;
	add.f64 	%fd323, %fd341, 0d0000000000000000;
	add.f64 	%fd322, %fd342, 0d0000000000000000;
	add.f64 	%fd321, %fd343, 0d0000000000000000;
	add.f64 	%fd320, %fd344, 0d0000000000000000;
	add.f64 	%fd319, %fd345, 0d0000000000000000;
	add.f64 	%fd318, %fd346, 0d0000000000000000;
	add.f64 	%fd317, %fd347, 0d0000000000000000;
	add.f64 	%fd316, %fd348, 0d0000000000000000;
	add.f64 	%fd315, %fd349, 0d0000000000000000;
	add.f64 	%fd314, %fd350, 0d0000000000000000;
	mul.lo.s64 	%rd101, %rd57, %rd20;
	add.s64 	%rd88, %rd44, %rd101;
	// begin inline asm
	{ atom.add.f64 %fd182,[%rd88],%fd314; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	// begin inline asm
	{ atom.add.f64 %fd184,[%rd89],%fd315; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd186,[%rd90],%fd316; }

	// end inline asm
	add.s64 	%rd91, %rd88, 24;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd91],%fd317; }

	// end inline asm
	add.s64 	%rd92, %rd88, 32;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd92],%fd318; }

	// end inline asm
	add.s64 	%rd93, %rd88, 40;
	// begin inline asm
	{ atom.add.f64 %fd192,[%rd93],%fd319; }

	// end inline asm
	add.s64 	%rd94, %rd88, 48;
	// begin inline asm
	{ atom.add.f64 %fd194,[%rd94],%fd320; }

	// end inline asm
	add.s64 	%rd95, %rd88, 56;
	// begin inline asm
	{ atom.add.f64 %fd196,[%rd95],%fd321; }

	// end inline asm
	add.s64 	%rd96, %rd88, 64;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd96],%fd322; }

	// end inline asm
	add.s64 	%rd97, %rd88, 72;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd97],%fd323; }

	// end inline asm
	add.s64 	%rd98, %rd88, 80;
	// begin inline asm
	{ atom.add.f64 %fd202,[%rd98],%fd324; }

	// end inline asm
	add.s64 	%rd99, %rd88, 88;
	// begin inline asm
	{ atom.add.f64 %fd204,[%rd99],%fd325; }

	// end inline asm
	bra.uni 	$L__BB35_27;

$L__BB35_25:
	setp.eq.s64 	%p20, %rd39, 0;
	@%p20 bra 	$L__BB35_27;

	add.f64 	%fd338, %fd339, 0d0000000000000000;
	add.f64 	%fd337, %fd340, 0d0000000000000000;
	add.f64 	%fd336, %fd341, 0d0000000000000000;
	add.f64 	%fd335, %fd342, 0d0000000000000000;
	add.f64 	%fd334, %fd343, 0d0000000000000000;
	add.f64 	%fd333, %fd344, 0d0000000000000000;
	add.f64 	%fd332, %fd345, 0d0000000000000000;
	add.f64 	%fd331, %fd346, 0d0000000000000000;
	add.f64 	%fd330, %fd347, 0d0000000000000000;
	add.f64 	%fd329, %fd348, 0d0000000000000000;
	add.f64 	%fd328, %fd349, 0d0000000000000000;
	add.f64 	%fd327, %fd350, 0d0000000000000000;
	mul.lo.s64 	%rd115, %rd57, %rd15;
	add.s64 	%rd102, %rd39, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd206,[%rd102],%fd327; }

	// end inline asm
	add.s64 	%rd103, %rd102, 8;
	// begin inline asm
	{ atom.add.f64 %fd208,[%rd103],%fd328; }

	// end inline asm
	add.s64 	%rd104, %rd102, 16;
	// begin inline asm
	{ atom.add.f64 %fd210,[%rd104],%fd329; }

	// end inline asm
	add.s64 	%rd105, %rd102, 24;
	// begin inline asm
	{ atom.add.f64 %fd212,[%rd105],%fd330; }

	// end inline asm
	add.s64 	%rd106, %rd102, 32;
	// begin inline asm
	{ atom.add.f64 %fd214,[%rd106],%fd331; }

	// end inline asm
	add.s64 	%rd107, %rd102, 40;
	// begin inline asm
	{ atom.add.f64 %fd216,[%rd107],%fd332; }

	// end inline asm
	add.s64 	%rd108, %rd102, 48;
	// begin inline asm
	{ atom.add.f64 %fd218,[%rd108],%fd333; }

	// end inline asm
	add.s64 	%rd109, %rd102, 56;
	// begin inline asm
	{ atom.add.f64 %fd220,[%rd109],%fd334; }

	// end inline asm
	add.s64 	%rd110, %rd102, 64;
	// begin inline asm
	{ atom.add.f64 %fd222,[%rd110],%fd335; }

	// end inline asm
	add.s64 	%rd111, %rd102, 72;
	// begin inline asm
	{ atom.add.f64 %fd224,[%rd111],%fd336; }

	// end inline asm
	add.s64 	%rd112, %rd102, 80;
	// begin inline asm
	{ atom.add.f64 %fd226,[%rd112],%fd337; }

	// end inline asm
	add.s64 	%rd113, %rd102, 88;
	// begin inline asm
	{ atom.add.f64 %fd228,[%rd113],%fd338; }

	// end inline asm

$L__BB35_27:
	@%p1 bra 	$L__BB35_36;

	setp.eq.s64 	%p22, %rd42, 0;
	@%p22 bra 	$L__BB35_30;

	ld.global.f64 	%fd230, [%rd32];
	add.f64 	%fd362, %fd230, 0d0000000000000000;
	ld.global.f64 	%fd231, [%rd32+8];
	add.f64 	%fd361, %fd231, 0d0000000000000000;
	ld.global.f64 	%fd232, [%rd32+16];
	add.f64 	%fd360, %fd232, 0d0000000000000000;
	ld.global.f64 	%fd233, [%rd32+24];
	add.f64 	%fd359, %fd233, 0d0000000000000000;
	ld.global.f64 	%fd234, [%rd32+32];
	add.f64 	%fd358, %fd234, 0d0000000000000000;
	ld.global.f64 	%fd235, [%rd32+40];
	add.f64 	%fd357, %fd235, 0d0000000000000000;
	ld.global.f64 	%fd236, [%rd32+48];
	add.f64 	%fd356, %fd236, 0d0000000000000000;
	ld.global.f64 	%fd237, [%rd32+56];
	add.f64 	%fd355, %fd237, 0d0000000000000000;
	ld.global.f64 	%fd238, [%rd32+64];
	add.f64 	%fd354, %fd238, 0d0000000000000000;
	ld.global.f64 	%fd239, [%rd32+72];
	add.f64 	%fd353, %fd239, 0d0000000000000000;
	ld.global.f64 	%fd240, [%rd32+80];
	add.f64 	%fd352, %fd240, 0d0000000000000000;
	ld.global.f64 	%fd241, [%rd32+88];
	add.f64 	%fd351, %fd241, 0d0000000000000000;
	bra.uni 	$L__BB35_32;

$L__BB35_30:
	setp.eq.s64 	%p23, %rd37, 0;
	mov.f64 	%fd351, 0d0000000000000000;
	mov.f64 	%fd352, %fd351;
	mov.f64 	%fd353, %fd351;
	mov.f64 	%fd354, %fd351;
	mov.f64 	%fd355, %fd351;
	mov.f64 	%fd356, %fd351;
	mov.f64 	%fd357, %fd351;
	mov.f64 	%fd358, %fd351;
	mov.f64 	%fd359, %fd351;
	mov.f64 	%fd360, %fd351;
	mov.f64 	%fd361, %fd351;
	mov.f64 	%fd362, %fd351;
	@%p23 bra 	$L__BB35_32;

	ld.global.f64 	%fd254, [%rd33];
	add.f64 	%fd362, %fd254, 0d0000000000000000;
	ld.global.f64 	%fd255, [%rd33+8];
	add.f64 	%fd361, %fd255, 0d0000000000000000;
	ld.global.f64 	%fd256, [%rd33+16];
	add.f64 	%fd360, %fd256, 0d0000000000000000;
	ld.global.f64 	%fd257, [%rd33+24];
	add.f64 	%fd359, %fd257, 0d0000000000000000;
	ld.global.f64 	%fd258, [%rd33+32];
	add.f64 	%fd358, %fd258, 0d0000000000000000;
	ld.global.f64 	%fd259, [%rd33+40];
	add.f64 	%fd357, %fd259, 0d0000000000000000;
	ld.global.f64 	%fd260, [%rd33+48];
	add.f64 	%fd356, %fd260, 0d0000000000000000;
	ld.global.f64 	%fd261, [%rd33+56];
	add.f64 	%fd355, %fd261, 0d0000000000000000;
	ld.global.f64 	%fd262, [%rd33+64];
	add.f64 	%fd354, %fd262, 0d0000000000000000;
	ld.global.f64 	%fd263, [%rd33+72];
	add.f64 	%fd353, %fd263, 0d0000000000000000;
	ld.global.f64 	%fd264, [%rd33+80];
	add.f64 	%fd352, %fd264, 0d0000000000000000;
	ld.global.f64 	%fd265, [%rd33+88];
	add.f64 	%fd351, %fd265, 0d0000000000000000;

$L__BB35_32:
	setp.eq.s64 	%p24, %rd44, 0;
	@%p24 bra 	$L__BB35_34;

	mul.lo.s64 	%rd129, %rd57, %rd20;
	add.s64 	%rd116, %rd44, %rd129;
	// begin inline asm
	{ atom.add.f64 %fd266,[%rd116],%fd362; }

	// end inline asm
	add.s64 	%rd117, %rd116, 8;
	// begin inline asm
	{ atom.add.f64 %fd268,[%rd117],%fd361; }

	// end inline asm
	add.s64 	%rd118, %rd116, 16;
	// begin inline asm
	{ atom.add.f64 %fd270,[%rd118],%fd360; }

	// end inline asm
	add.s64 	%rd119, %rd116, 24;
	// begin inline asm
	{ atom.add.f64 %fd272,[%rd119],%fd359; }

	// end inline asm
	add.s64 	%rd120, %rd116, 32;
	// begin inline asm
	{ atom.add.f64 %fd274,[%rd120],%fd358; }

	// end inline asm
	add.s64 	%rd121, %rd116, 40;
	// begin inline asm
	{ atom.add.f64 %fd276,[%rd121],%fd357; }

	// end inline asm
	add.s64 	%rd122, %rd116, 48;
	// begin inline asm
	{ atom.add.f64 %fd278,[%rd122],%fd356; }

	// end inline asm
	add.s64 	%rd123, %rd116, 56;
	// begin inline asm
	{ atom.add.f64 %fd280,[%rd123],%fd355; }

	// end inline asm
	add.s64 	%rd124, %rd116, 64;
	// begin inline asm
	{ atom.add.f64 %fd282,[%rd124],%fd354; }

	// end inline asm
	add.s64 	%rd125, %rd116, 72;
	// begin inline asm
	{ atom.add.f64 %fd284,[%rd125],%fd353; }

	// end inline asm
	add.s64 	%rd126, %rd116, 80;
	// begin inline asm
	{ atom.add.f64 %fd286,[%rd126],%fd352; }

	// end inline asm
	add.s64 	%rd127, %rd116, 88;
	// begin inline asm
	{ atom.add.f64 %fd288,[%rd127],%fd351; }

	// end inline asm
	bra.uni 	$L__BB35_36;

$L__BB35_34:
	setp.eq.s64 	%p25, %rd39, 0;
	@%p25 bra 	$L__BB35_36;

	mul.lo.s64 	%rd143, %rd57, %rd15;
	add.s64 	%rd130, %rd39, %rd143;
	// begin inline asm
	{ atom.add.f64 %fd290,[%rd130],%fd362; }

	// end inline asm
	add.s64 	%rd131, %rd130, 8;
	// begin inline asm
	{ atom.add.f64 %fd292,[%rd131],%fd361; }

	// end inline asm
	add.s64 	%rd132, %rd130, 16;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd132],%fd360; }

	// end inline asm
	add.s64 	%rd133, %rd130, 24;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd133],%fd359; }

	// end inline asm
	add.s64 	%rd134, %rd130, 32;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd134],%fd358; }

	// end inline asm
	add.s64 	%rd135, %rd130, 40;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd135],%fd357; }

	// end inline asm
	add.s64 	%rd136, %rd130, 48;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd136],%fd356; }

	// end inline asm
	add.s64 	%rd137, %rd130, 56;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd137],%fd355; }

	// end inline asm
	add.s64 	%rd138, %rd130, 64;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd138],%fd354; }

	// end inline asm
	add.s64 	%rd139, %rd130, 72;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd139],%fd353; }

	// end inline asm
	add.s64 	%rd140, %rd130, 80;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd140],%fd352; }

	// end inline asm
	add.s64 	%rd141, %rd130, 88;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd141],%fd351; }

	// end inline asm

$L__BB35_36:
	ld.param.u64 	%rd144, [initialize_tilde_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd147, %rd147, %rd18;
	setp.lt.u64 	%p26, %rd147, %rd144;
	@%p26 bra 	$L__BB35_2;

$L__BB35_37:
	ret;

}
	// .globl	step_x_cuda_kernel_forward
.visible .entry step_x_cuda_kernel_forward(
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 step_x_cuda_kernel_forward_param_6[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<135>;
	.reg .f64 	%fd<31>;
	.reg .b64 	%rd<129>;


	ld.param.v2.u32 	{%r61, %r62}, [step_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r63, %r64}, [step_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r69, %r70}, [step_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r77, %r78}, [step_x_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r85, %r86}, [step_x_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r93, %r94}, [step_x_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r101, %r102}, [step_x_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r109, %r110}, [step_x_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd55, [step_x_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd53, [step_x_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd51, [step_x_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd49, [step_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd47, [step_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd45, [step_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd44, [step_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [step_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r113, %ntid.x;
	cvt.u64.u32 	%rd1, %r113;
	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd57, %r113, %r114;
	mov.u32 	%r115, %tid.x;
	cvt.u64.u32 	%rd58, %r115;
	add.s64 	%rd120, %rd57, %rd58;
	setp.ge.u64 	%p1, %rd120, %rd44;
	@%p1 bra 	$L__BB36_31;

	cvta.to.global.u64 	%rd4, %rd55;
	cvta.to.global.u64 	%rd5, %rd53;
	cvta.to.global.u64 	%rd6, %rd51;
	cvta.to.global.u64 	%rd7, %rd49;
	cvta.to.global.u64 	%rd8, %rd47;
	cvta.to.global.u64 	%rd9, %rd45;
	cvt.s64.s32 	%rd10, %r64;
	cvt.s64.s32 	%rd11, %r63;
	cvt.s64.s32 	%rd12, %r62;
	cvt.s64.s32 	%rd13, %r69;
	cvt.s64.s32 	%rd14, %r101;
	cvt.s64.s32 	%rd15, %r109;
	cvt.s64.s32 	%rd16, %r85;
	cvt.s64.s32 	%rd17, %r77;
	cvt.s64.s32 	%rd18, %r93;
	mov.u32 	%r116, %nctaid.x;
	cvt.u64.u32 	%rd59, %r116;
	mul.lo.s64 	%rd19, %rd1, %rd59;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB36_18;
	bra.uni 	$L__BB36_2;

$L__BB36_18:
	cvt.u32.u64 	%r126, %rd10;
	cvt.u32.u64 	%r129, %rd11;
	cvt.u32.u64 	%r132, %rd12;

$L__BB36_19:
	or.b64  	%rd98, %rd120, %rd10;
	and.b64  	%rd99, %rd98, -4294967296;
	setp.eq.s64 	%p13, %rd99, 0;
	@%p13 bra 	$L__BB36_21;

	div.u64 	%rd127, %rd120, %rd10;
	bra.uni 	$L__BB36_22;

$L__BB36_21:
	cvt.u32.u64 	%r127, %rd120;
	div.u32 	%r128, %r127, %r126;
	cvt.u64.u32 	%rd127, %r128;

$L__BB36_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB36_26;

	or.b64  	%rd100, %rd127, %rd11;
	and.b64  	%rd101, %rd100, -4294967296;
	setp.eq.s64 	%p15, %rd101, 0;
	@%p15 bra 	$L__BB36_25;

	div.u64 	%rd127, %rd127, %rd11;
	bra.uni 	$L__BB36_26;

$L__BB36_25:
	cvt.u32.u64 	%r130, %rd127;
	div.u32 	%r131, %r130, %r129;
	cvt.u64.u32 	%rd127, %r131;

$L__BB36_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB36_30;

	or.b64  	%rd102, %rd127, %rd12;
	and.b64  	%rd103, %rd102, -4294967296;
	setp.eq.s64 	%p17, %rd103, 0;
	@%p17 bra 	$L__BB36_29;

	div.u64 	%rd127, %rd127, %rd12;
	bra.uni 	$L__BB36_30;

$L__BB36_29:
	cvt.u32.u64 	%r133, %rd127;
	div.u32 	%r134, %r133, %r132;
	cvt.u64.u32 	%rd127, %r134;

$L__BB36_30:
	cvt.s64.s32 	%rd104, %rd127;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b64 	%rd105, %rd104, 0, %p18;
	mul.lo.s64 	%rd106, %rd105, %rd13;
	add.s64 	%rd107, %rd9, %rd106;
	mul.lo.s64 	%rd108, %rd105, %rd14;
	add.s64 	%rd109, %rd5, %rd108;
	ld.global.s32 	%rd110, [%rd109];
	mul.lo.s64 	%rd111, %rd110, %rd15;
	add.s64 	%rd112, %rd4, %rd111;
	ld.global.s32 	%rd113, [%rd112];
	mul.lo.s64 	%rd114, %rd113, %rd16;
	add.s64 	%rd115, %rd7, %rd114;
	mul.lo.s64 	%rd116, %rd105, %rd17;
	add.s64 	%rd117, %rd8, %rd116;
	ld.global.f64 	%fd21, [%rd117];
	ld.global.f64 	%fd22, [%rd115];
	ld.global.f64 	%fd23, [%rd117+8];
	ld.global.f64 	%fd24, [%rd117+16];
	ld.global.f64 	%fd25, [%rd107];
	fma.rn.f64 	%fd26, %fd22, %fd21, %fd25;
	ld.global.f64 	%fd27, [%rd107+8];
	fma.rn.f64 	%fd28, %fd22, %fd23, %fd27;
	ld.global.f64 	%fd29, [%rd107+16];
	fma.rn.f64 	%fd30, %fd22, %fd24, %fd29;
	mul.lo.s64 	%rd118, %rd105, %rd18;
	add.s64 	%rd119, %rd6, %rd118;
	st.global.f64 	[%rd119], %fd26;
	st.global.f64 	[%rd119+8], %fd28;
	st.global.f64 	[%rd119+16], %fd30;
	add.s64 	%rd120, %rd120, %rd19;
	setp.lt.u64 	%p19, %rd120, %rd44;
	@%p19 bra 	$L__BB36_19;
	bra.uni 	$L__BB36_31;

$L__BB36_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB36_9;
	bra.uni 	$L__BB36_3;

$L__BB36_9:
	cvt.u32.u64 	%r120, %rd11;
	cvt.u32.u64 	%r123, %rd12;

$L__BB36_10:
	or.b64  	%rd78, %rd120, %rd11;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p8, %rd79, 0;
	@%p8 bra 	$L__BB36_12;

	div.u64 	%rd124, %rd120, %rd11;
	bra.uni 	$L__BB36_13;

$L__BB36_12:
	cvt.u32.u64 	%r121, %rd120;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd124, %r122;

$L__BB36_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB36_17;

	or.b64  	%rd80, %rd124, %rd12;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p10, %rd81, 0;
	@%p10 bra 	$L__BB36_16;

	div.u64 	%rd124, %rd124, %rd12;
	bra.uni 	$L__BB36_17;

$L__BB36_16:
	cvt.u32.u64 	%r124, %rd124;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd124, %r125;

$L__BB36_17:
	cvt.s64.s32 	%rd82, %rd124;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b64 	%rd83, %rd82, 0, %p11;
	mul.lo.s64 	%rd84, %rd83, %rd13;
	add.s64 	%rd85, %rd9, %rd84;
	mul.lo.s64 	%rd86, %rd83, %rd14;
	add.s64 	%rd87, %rd5, %rd86;
	ld.global.s32 	%rd88, [%rd87];
	mul.lo.s64 	%rd89, %rd88, %rd15;
	add.s64 	%rd90, %rd4, %rd89;
	ld.global.s32 	%rd91, [%rd90];
	mul.lo.s64 	%rd92, %rd91, %rd16;
	add.s64 	%rd93, %rd7, %rd92;
	mul.lo.s64 	%rd94, %rd83, %rd17;
	add.s64 	%rd95, %rd8, %rd94;
	ld.global.f64 	%fd11, [%rd95];
	ld.global.f64 	%fd12, [%rd93];
	ld.global.f64 	%fd13, [%rd95+8];
	ld.global.f64 	%fd14, [%rd95+16];
	ld.global.f64 	%fd15, [%rd85];
	fma.rn.f64 	%fd16, %fd12, %fd11, %fd15;
	ld.global.f64 	%fd17, [%rd85+8];
	fma.rn.f64 	%fd18, %fd12, %fd13, %fd17;
	ld.global.f64 	%fd19, [%rd85+16];
	fma.rn.f64 	%fd20, %fd12, %fd14, %fd19;
	mul.lo.s64 	%rd96, %rd83, %rd18;
	add.s64 	%rd97, %rd6, %rd96;
	st.global.f64 	[%rd97], %fd16;
	st.global.f64 	[%rd97+8], %fd18;
	st.global.f64 	[%rd97+16], %fd20;
	add.s64 	%rd120, %rd120, %rd19;
	setp.lt.u64 	%p12, %rd120, %rd44;
	@%p12 bra 	$L__BB36_10;
	bra.uni 	$L__BB36_31;

$L__BB36_3:
	cvt.u32.u64 	%r117, %rd12;

$L__BB36_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd121, %rd120;
	@%p4 bra 	$L__BB36_8;

	or.b64  	%rd60, %rd120, %rd12;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p5, %rd61, 0;
	@%p5 bra 	$L__BB36_7;

	div.u64 	%rd121, %rd120, %rd12;
	bra.uni 	$L__BB36_8;

$L__BB36_7:
	cvt.u32.u64 	%r118, %rd120;
	div.u32 	%r119, %r118, %r117;
	cvt.u64.u32 	%rd121, %r119;

$L__BB36_8:
	cvt.s64.s32 	%rd62, %rd121;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b64 	%rd63, %rd62, 0, %p6;
	mul.lo.s64 	%rd64, %rd63, %rd13;
	add.s64 	%rd65, %rd9, %rd64;
	mul.lo.s64 	%rd66, %rd63, %rd14;
	add.s64 	%rd67, %rd5, %rd66;
	ld.global.s32 	%rd68, [%rd67];
	mul.lo.s64 	%rd69, %rd68, %rd15;
	add.s64 	%rd70, %rd4, %rd69;
	ld.global.s32 	%rd71, [%rd70];
	mul.lo.s64 	%rd72, %rd71, %rd16;
	add.s64 	%rd73, %rd7, %rd72;
	mul.lo.s64 	%rd74, %rd63, %rd17;
	add.s64 	%rd75, %rd8, %rd74;
	ld.global.f64 	%fd1, [%rd75];
	ld.global.f64 	%fd2, [%rd73];
	ld.global.f64 	%fd3, [%rd75+8];
	ld.global.f64 	%fd4, [%rd75+16];
	ld.global.f64 	%fd5, [%rd65];
	fma.rn.f64 	%fd6, %fd2, %fd1, %fd5;
	ld.global.f64 	%fd7, [%rd65+8];
	fma.rn.f64 	%fd8, %fd2, %fd3, %fd7;
	ld.global.f64 	%fd9, [%rd65+16];
	fma.rn.f64 	%fd10, %fd2, %fd4, %fd9;
	mul.lo.s64 	%rd76, %rd63, %rd18;
	add.s64 	%rd77, %rd6, %rd76;
	st.global.f64 	[%rd77], %fd6;
	st.global.f64 	[%rd77+8], %fd8;
	st.global.f64 	[%rd77+16], %fd10;
	add.s64 	%rd120, %rd120, %rd19;
	setp.lt.u64 	%p7, %rd120, %rd44;
	@%p7 bra 	$L__BB36_4;

$L__BB36_31:
	ret;

}
	// .globl	step_x_cuda_kernel_backward
.visible .entry step_x_cuda_kernel_backward(
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 step_x_cuda_kernel_backward_param_12[56]
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<194>;
	.reg .f64 	%fd<64>;
	.reg .b64 	%rd<116>;


	ld.param.v2.u32 	{%r97, %r98}, [step_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r99, %r100}, [step_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r105, %r106}, [step_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r113, %r114}, [step_x_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r121, %r122}, [step_x_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r129, %r130}, [step_x_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r137, %r138}, [step_x_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r145, %r146}, [step_x_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r153, %r154}, [step_x_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r161, %r162}, [step_x_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r169, %r170}, [step_x_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r177, %r178}, [step_x_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd66, [step_x_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd64, [step_x_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd62, [step_x_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd60, [step_x_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd58, [step_x_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd56, [step_x_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd55, [step_x_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd53, [step_x_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd52, [step_x_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd51, [step_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd50, [step_x_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd49, [step_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd47, [step_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [step_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r181, %ntid.x;
	cvt.u64.u32 	%rd1, %r181;
	mov.u32 	%r182, %ctaid.x;
	mul.wide.u32 	%rd68, %r181, %r182;
	mov.u32 	%r183, %tid.x;
	cvt.u64.u32 	%rd69, %r183;
	add.s64 	%rd112, %rd68, %rd69;
	setp.ge.u64 	%p1, %rd112, %rd47;
	@%p1 bra 	$L__BB37_31;

	cvta.to.global.u64 	%rd12, %rd66;
	cvta.to.global.u64 	%rd13, %rd58;
	cvta.to.global.u64 	%rd14, %rd56;
	cvta.to.global.u64 	%rd15, %rd55;
	cvta.to.global.u64 	%rd16, %rd52;
	cvta.to.global.u64 	%rd17, %rd50;
	cvt.s64.s32 	%rd18, %r100;
	cvt.s64.s32 	%rd19, %r99;
	cvt.s64.s32 	%rd20, %r98;
	cvt.s64.s32 	%rd21, %r137;
	cvt.s64.s32 	%rd22, %r145;
	cvt.s64.s32 	%rd23, %r121;
	cvt.s64.s32 	%rd24, %r113;
	cvt.s64.s32 	%rd25, %r177;
	cvt.s64.s32 	%rd26, %r129;
	cvt.s64.s32 	%rd27, %r161;
	cvt.s64.s32 	%rd28, %r169;
	mov.u32 	%r184, %nctaid.x;
	cvt.u64.u32 	%rd70, %r184;
	mul.lo.s64 	%rd29, %rd1, %rd70;
	cvt.s64.s32 	%rd30, %r153;
	cvt.s64.s32 	%rd31, %r105;

$L__BB37_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd113, %rd112;
	@%p2 bra 	$L__BB37_6;

	or.b64  	%rd71, %rd112, %rd18;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p3, %rd72, 0;
	@%p3 bra 	$L__BB37_5;

	div.u64 	%rd113, %rd112, %rd18;
	bra.uni 	$L__BB37_6;

$L__BB37_5:
	cvt.u32.u64 	%r185, %rd18;
	cvt.u32.u64 	%r186, %rd112;
	div.u32 	%r187, %r186, %r185;
	cvt.u64.u32 	%rd113, %r187;

$L__BB37_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB37_10;

	or.b64  	%rd73, %rd113, %rd19;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p5, %rd74, 0;
	@%p5 bra 	$L__BB37_9;

	div.u64 	%rd113, %rd113, %rd19;
	bra.uni 	$L__BB37_10;

$L__BB37_9:
	cvt.u32.u64 	%r188, %rd19;
	cvt.u32.u64 	%r189, %rd113;
	div.u32 	%r190, %r189, %r188;
	cvt.u64.u32 	%rd113, %r190;

$L__BB37_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB37_14;

	or.b64  	%rd75, %rd113, %rd20;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p7, %rd76, 0;
	@%p7 bra 	$L__BB37_13;

	div.u64 	%rd113, %rd113, %rd20;
	bra.uni 	$L__BB37_14;

$L__BB37_13:
	cvt.u32.u64 	%r191, %rd20;
	cvt.u32.u64 	%r192, %rd113;
	div.u32 	%r193, %r192, %r191;
	cvt.u64.u32 	%rd113, %r193;

$L__BB37_14:
	cvt.s64.s32 	%rd109, %r121;
	ld.param.u64 	%rd108, [step_x_cuda_kernel_backward_param_10];
	cvt.s64.s32 	%rd77, %rd113;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd42, %rd77, 0, %p8;
	mul.lo.s64 	%rd78, %rd42, %rd21;
	add.s64 	%rd79, %rd14, %rd78;
	ld.global.s32 	%rd80, [%rd79];
	mul.lo.s64 	%rd81, %rd80, %rd22;
	add.s64 	%rd82, %rd13, %rd81;
	ld.global.s32 	%rd43, [%rd82];
	mul.lo.s64 	%rd44, %rd43, %rd109;
	add.s64 	%rd83, %rd16, %rd44;
	mul.lo.s64 	%rd45, %rd42, %rd24;
	add.s64 	%rd84, %rd17, %rd45;
	ld.global.f64 	%fd1, [%rd83];
	ld.global.f64 	%fd2, [%rd84];
	ld.global.f64 	%fd3, [%rd84+8];
	ld.global.f64 	%fd4, [%rd84+16];
	setp.eq.s64 	%p9, %rd108, 0;
	@%p9 bra 	$L__BB37_16;

	mul.lo.s64 	%rd85, %rd42, %rd25;
	add.s64 	%rd86, %rd12, %rd85;
	ld.global.f64 	%fd21, [%rd86];
	add.f64 	%fd63, %fd21, 0d0000000000000000;
	ld.global.f64 	%fd22, [%rd86+8];
	add.f64 	%fd62, %fd22, 0d0000000000000000;
	ld.global.f64 	%fd23, [%rd86+16];
	add.f64 	%fd61, %fd23, 0d0000000000000000;
	bra.uni 	$L__BB37_18;

$L__BB37_16:
	ld.param.u64 	%rd110, [step_x_cuda_kernel_backward_param_4+8];
	setp.eq.s64 	%p10, %rd110, 0;
	mov.f64 	%fd61, 0d0000000000000000;
	mov.f64 	%fd62, %fd61;
	mov.f64 	%fd63, %fd61;
	@%p10 bra 	$L__BB37_18;

	mul.lo.s64 	%rd87, %rd42, %rd26;
	add.s64 	%rd88, %rd15, %rd87;
	ld.global.f64 	%fd27, [%rd88];
	add.f64 	%fd63, %fd27, 0d0000000000000000;
	ld.global.f64 	%fd28, [%rd88+8];
	add.f64 	%fd62, %fd28, 0d0000000000000000;
	ld.global.f64 	%fd29, [%rd88+16];
	add.f64 	%fd61, %fd29, 0d0000000000000000;

$L__BB37_18:
	add.f64 	%fd14, %fd63, 0d0000000000000000;
	fma.rn.f64 	%fd15, %fd1, %fd14, 0d0000000000000000;
	add.f64 	%fd16, %fd62, 0d0000000000000000;
	fma.rn.f64 	%fd17, %fd1, %fd16, 0d0000000000000000;
	add.f64 	%fd18, %fd61, 0d0000000000000000;
	fma.rn.f64 	%fd19, %fd1, %fd18, 0d0000000000000000;
	mul.f64 	%fd30, %fd3, %fd16;
	fma.rn.f64 	%fd31, %fd2, %fd14, %fd30;
	fma.rn.f64 	%fd32, %fd4, %fd18, %fd31;
	add.f64 	%fd20, %fd32, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd62, 0;
	@%p11 bra 	$L__BB37_20;

	mul.lo.s64 	%rd92, %rd42, %rd27;
	add.s64 	%rd89, %rd62, %rd92;
	// begin inline asm
	{ atom.add.f64 %fd33,[%rd89],%fd15; }

	// end inline asm
	add.s64 	%rd90, %rd89, 8;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd90],%fd17; }

	// end inline asm
	add.s64 	%rd91, %rd89, 16;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd91],%fd19; }

	// end inline asm
	bra.uni 	$L__BB37_22;

$L__BB37_20:
	setp.eq.s64 	%p12, %rd51, 0;
	@%p12 bra 	$L__BB37_22;

	add.s64 	%rd93, %rd51, %rd45;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd93],%fd15; }

	// end inline asm
	add.s64 	%rd94, %rd93, 8;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd94],%fd17; }

	// end inline asm
	add.s64 	%rd95, %rd93, 16;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd95],%fd19; }

	// end inline asm

$L__BB37_22:
	setp.eq.s64 	%p13, %rd64, 0;
	@%p13 bra 	$L__BB37_24;

	mul.lo.s64 	%rd97, %rd43, %rd28;
	add.s64 	%rd96, %rd64, %rd97;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd96],%fd20; }

	// end inline asm
	bra.uni 	$L__BB37_26;

$L__BB37_24:
	setp.eq.s64 	%p14, %rd53, 0;
	@%p14 bra 	$L__BB37_26;

	mul.lo.s64 	%rd111, %rd43, %rd109;
	add.s64 	%rd98, %rd53, %rd111;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd98],%fd20; }

	// end inline asm

$L__BB37_26:
	setp.eq.s64 	%p15, %rd60, 0;
	@%p15 bra 	$L__BB37_28;

	mul.lo.s64 	%rd102, %rd42, %rd30;
	add.s64 	%rd99, %rd60, %rd102;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd99],%fd14; }

	// end inline asm
	add.s64 	%rd100, %rd99, 8;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd100],%fd16; }

	// end inline asm
	add.s64 	%rd101, %rd99, 16;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd101],%fd18; }

	// end inline asm
	bra.uni 	$L__BB37_30;

$L__BB37_28:
	setp.eq.s64 	%p16, %rd49, 0;
	@%p16 bra 	$L__BB37_30;

	mul.lo.s64 	%rd106, %rd42, %rd31;
	add.s64 	%rd103, %rd49, %rd106;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd103],%fd14; }

	// end inline asm
	add.s64 	%rd104, %rd103, 8;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd104],%fd16; }

	// end inline asm
	add.s64 	%rd105, %rd103, 16;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd105],%fd18; }

	// end inline asm

$L__BB37_30:
	ld.param.u64 	%rd107, [step_x_cuda_kernel_backward_param_0+24];
	add.s64 	%rd112, %rd112, %rd29;
	setp.lt.u64 	%p17, %rd112, %rd107;
	@%p17 bra 	$L__BB37_2;

$L__BB37_31:
	ret;

}
	// .globl	soft_to_sys_grad_cuda_kernel_forward
.visible .entry soft_to_sys_grad_cuda_kernel_forward(
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_forward_param_2[56],
	.param .u32 soft_to_sys_grad_cuda_kernel_forward_param_3
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<77>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<83>;


	ld.param.v2.u32 	{%r26, %r27}, [soft_to_sys_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [soft_to_sys_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [soft_to_sys_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [soft_to_sys_grad_cuda_kernel_forward_param_2+32];
	ld.param.u32 	%r25, [soft_to_sys_grad_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd39, [soft_to_sys_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [soft_to_sys_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd36, [soft_to_sys_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [soft_to_sys_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd41, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd42, %r48;
	add.s64 	%rd74, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd74, %rd36;
	@%p1 bra 	$L__BB38_31;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r34;
	cvt.s64.s32 	%rd10, %r42;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd43, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd43;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB38_18;
	bra.uni 	$L__BB38_2;

$L__BB38_18:
	cvt.u32.u64 	%r65, %rd6;
	cvt.u32.u64 	%r68, %rd7;
	cvt.u32.u64 	%r71, %rd8;

$L__BB38_19:
	or.b64  	%rd62, %rd74, %rd6;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p13, %rd63, 0;
	@%p13 bra 	$L__BB38_21;

	div.u64 	%rd81, %rd74, %rd6;
	bra.uni 	$L__BB38_22;

$L__BB38_21:
	cvt.u32.u64 	%r66, %rd74;
	div.u32 	%r67, %r66, %r65;
	cvt.u64.u32 	%rd81, %r67;

$L__BB38_22:
	setp.lt.s32 	%p14, %r6, 3;
	@%p14 bra 	$L__BB38_26;

	or.b64  	%rd64, %rd81, %rd7;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB38_25;

	div.u64 	%rd81, %rd81, %rd7;
	bra.uni 	$L__BB38_26;

$L__BB38_25:
	cvt.u32.u64 	%r69, %rd81;
	div.u32 	%r70, %r69, %r68;
	cvt.u64.u32 	%rd81, %r70;

$L__BB38_26:
	setp.lt.s32 	%p16, %r6, 2;
	@%p16 bra 	$L__BB38_30;

	or.b64  	%rd66, %rd81, %rd8;
	and.b64  	%rd67, %rd66, -4294967296;
	setp.eq.s64 	%p17, %rd67, 0;
	@%p17 bra 	$L__BB38_29;

	div.u64 	%rd81, %rd81, %rd8;
	bra.uni 	$L__BB38_30;

$L__BB38_29:
	cvt.u32.u64 	%r72, %rd81;
	div.u32 	%r73, %r72, %r71;
	cvt.u64.u32 	%rd81, %r73;

$L__BB38_30:
	cvt.u32.u64 	%r74, %rd81;
	setp.gt.s32 	%p18, %r6, 0;
	selp.b32 	%r75, %r74, 0, %p18;
	cvt.s64.s32 	%rd68, %r75;
	mul.lo.s64 	%rd69, %rd68, %rd9;
	add.s64 	%rd70, %rd5, %rd69;
	add.s32 	%r76, %r75, %r25;
	ld.global.f64 	%fd7, [%rd70];
	ld.global.f64 	%fd8, [%rd70+8];
	ld.global.f64 	%fd9, [%rd70+16];
	cvt.s64.s32 	%rd71, %r76;
	mul.lo.s64 	%rd72, %rd71, %rd10;
	add.s64 	%rd73, %rd4, %rd72;
	st.global.f64 	[%rd73], %fd7;
	st.global.f64 	[%rd73+8], %fd8;
	st.global.f64 	[%rd73+16], %fd9;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p19, %rd74, %rd36;
	@%p19 bra 	$L__BB38_19;
	bra.uni 	$L__BB38_31;

$L__BB38_2:
	setp.gt.s32 	%p3, %r6, 2;
	@%p3 bra 	$L__BB38_9;
	bra.uni 	$L__BB38_3;

$L__BB38_9:
	cvt.u32.u64 	%r56, %rd7;
	cvt.u32.u64 	%r59, %rd8;

$L__BB38_10:
	or.b64  	%rd52, %rd74, %rd7;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p8, %rd53, 0;
	@%p8 bra 	$L__BB38_12;

	div.u64 	%rd78, %rd74, %rd7;
	bra.uni 	$L__BB38_13;

$L__BB38_12:
	cvt.u32.u64 	%r57, %rd74;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd78, %r58;

$L__BB38_13:
	setp.lt.s32 	%p9, %r6, 2;
	@%p9 bra 	$L__BB38_17;

	or.b64  	%rd54, %rd78, %rd8;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p10, %rd55, 0;
	@%p10 bra 	$L__BB38_16;

	div.u64 	%rd78, %rd78, %rd8;
	bra.uni 	$L__BB38_17;

$L__BB38_16:
	cvt.u32.u64 	%r60, %rd78;
	div.u32 	%r61, %r60, %r59;
	cvt.u64.u32 	%rd78, %r61;

$L__BB38_17:
	cvt.u32.u64 	%r62, %rd78;
	setp.gt.s32 	%p11, %r6, 0;
	selp.b32 	%r63, %r62, 0, %p11;
	cvt.s64.s32 	%rd56, %r63;
	mul.lo.s64 	%rd57, %rd56, %rd9;
	add.s64 	%rd58, %rd5, %rd57;
	add.s32 	%r64, %r63, %r25;
	ld.global.f64 	%fd4, [%rd58];
	ld.global.f64 	%fd5, [%rd58+8];
	ld.global.f64 	%fd6, [%rd58+16];
	cvt.s64.s32 	%rd59, %r64;
	mul.lo.s64 	%rd60, %rd59, %rd10;
	add.s64 	%rd61, %rd4, %rd60;
	st.global.f64 	[%rd61], %fd4;
	st.global.f64 	[%rd61+8], %fd5;
	st.global.f64 	[%rd61+16], %fd6;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p12, %rd74, %rd36;
	@%p12 bra 	$L__BB38_10;
	bra.uni 	$L__BB38_31;

$L__BB38_3:
	cvt.u32.u64 	%r50, %rd8;

$L__BB38_4:
	setp.lt.s32 	%p4, %r6, 2;
	mov.u64 	%rd75, %rd74;
	@%p4 bra 	$L__BB38_8;

	or.b64  	%rd44, %rd74, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p5, %rd45, 0;
	@%p5 bra 	$L__BB38_7;

	div.u64 	%rd75, %rd74, %rd8;
	bra.uni 	$L__BB38_8;

$L__BB38_7:
	cvt.u32.u64 	%r51, %rd74;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd75, %r52;

$L__BB38_8:
	cvt.u32.u64 	%r53, %rd75;
	setp.gt.s32 	%p6, %r6, 0;
	selp.b32 	%r54, %r53, 0, %p6;
	cvt.s64.s32 	%rd46, %r54;
	mul.lo.s64 	%rd47, %rd46, %rd9;
	add.s64 	%rd48, %rd5, %rd47;
	add.s32 	%r55, %r54, %r25;
	ld.global.f64 	%fd1, [%rd48];
	ld.global.f64 	%fd2, [%rd48+8];
	ld.global.f64 	%fd3, [%rd48+16];
	cvt.s64.s32 	%rd49, %r55;
	mul.lo.s64 	%rd50, %rd49, %rd10;
	add.s64 	%rd51, %rd4, %rd50;
	st.global.f64 	[%rd51], %fd1;
	st.global.f64 	[%rd51+8], %fd2;
	st.global.f64 	[%rd51+16], %fd3;
	add.s64 	%rd74, %rd74, %rd11;
	setp.lt.u64 	%p7, %rd74, %rd36;
	@%p7 bra 	$L__BB38_4;

$L__BB38_31:
	ret;

}
	// .globl	soft_to_sys_grad_cuda_kernel_backward
.visible .entry soft_to_sys_grad_cuda_kernel_backward(
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_2[56],
	.param .u32 soft_to_sys_grad_cuda_kernel_backward_param_3,
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 soft_to_sys_grad_cuda_kernel_backward_param_5[56],
	.param .u32 soft_to_sys_grad_cuda_kernel_backward_param_6
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<96>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r46, %r47}, [soft_to_sys_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [soft_to_sys_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [soft_to_sys_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r62, %r63}, [soft_to_sys_grad_cuda_kernel_backward_param_2+32];
	ld.param.u32 	%r27, [soft_to_sys_grad_cuda_kernel_backward_param_3];
	ld.param.v2.u32 	{%r70, %r71}, [soft_to_sys_grad_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r78, %r79}, [soft_to_sys_grad_cuda_kernel_backward_param_5+32];
	ld.param.u64 	%rd36, [soft_to_sys_grad_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd34, [soft_to_sys_grad_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd33, [soft_to_sys_grad_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [soft_to_sys_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [soft_to_sys_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r8, [soft_to_sys_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd38, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd39, %r84;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB39_23;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r49;
	cvt.s64.s32 	%rd11, %r48;
	cvt.s64.s32 	%rd12, %r47;
	cvt.s64.s32 	%rd13, %r78;
	cvt.s64.s32 	%rd14, %r62;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd40, %r85;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r70;
	cvt.s64.s32 	%rd17, %r54;

$L__BB39_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB39_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB39_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB39_6;

$L__BB39_5:
	cvt.u32.u64 	%r86, %rd10;
	cvt.u32.u64 	%r87, %rd63;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd64, %r88;

$L__BB39_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB39_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB39_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB39_10;

$L__BB39_9:
	cvt.u32.u64 	%r89, %rd11;
	cvt.u32.u64 	%r90, %rd64;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd64, %r91;

$L__BB39_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB39_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB39_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB39_14;

$L__BB39_13:
	cvt.u32.u64 	%r92, %rd12;
	cvt.u32.u64 	%r93, %rd64;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd64, %r94;

$L__BB39_14:
	cvt.u32.u64 	%r95, %rd64;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r95, 0, %p8;
	add.s32 	%r3, %r2, %r27;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB39_16;

	cvt.s64.s32 	%rd47, %r3;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB39_18;

$L__BB39_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB39_18;

	cvt.s64.s32 	%rd50, %r3;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB39_18:
	setp.eq.s64 	%p11, %rd34, 0;
	@%p11 bra 	$L__BB39_20;

	cvt.s64.s32 	%rd56, %r2;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd34, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB39_22;

$L__BB39_20:
	setp.eq.s64 	%p12, %rd31, 0;
	@%p12 bra 	$L__BB39_22;

	cvt.s64.s32 	%rd61, %r2;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd31, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB39_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB39_2;

$L__BB39_23:
	ret;

}
	// .globl	init_soft_diag_hess_inds_kernel_cuda_kernel_forward
.visible .entry init_soft_diag_hess_inds_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_1[184],
	.param .u32 init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_2
)
{
	.local .align 8 .b8 	__local_depot40[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b32 	%r<57>;
	.reg .b64 	%rd<91>;


	mov.u64 	%SPL, __local_depot40;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r20, %r21}, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r22, %r23}, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0+8];
	mov.b64 	%rd38, init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_1;
	ld.param.u32 	%r19, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd37, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r18, [init_soft_diag_hess_inds_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r24, %ntid.x;
	cvt.u64.u32 	%rd1, %r24;
	mov.u32 	%r25, %ctaid.x;
	mul.wide.u32 	%rd39, %r24, %r25;
	mov.u32 	%r26, %tid.x;
	cvt.u64.u32 	%rd40, %r26;
	add.s64 	%rd82, %rd39, %rd40;
	setp.ge.u64 	%p1, %rd82, %rd37;
	@%p1 bra 	$L__BB40_39;

	cvt.s64.s32 	%rd4, %r23;
	cvt.s64.s32 	%rd5, %r22;
	cvt.s64.s32 	%rd6, %r21;
	shl.b32 	%r2, %r19, 2;
	ld.param.v2.u32 	{%r27, %r28}, [%rd38+176];
	ld.param.u32 	%r7, [%rd38+172];
	mov.u32 	%r29, %nctaid.x;
	cvt.u64.u32 	%rd42, %r29;
	mul.lo.s64 	%rd7, %rd1, %rd42;
	ld.param.u64 	%rd43, [%rd38];
	cvta.to.global.u64 	%rd8, %rd43;
	ld.param.s32 	%rd9, [%rd38+32];
	ld.param.u64 	%rd44, [%rd38+56];
	cvta.to.global.u64 	%rd10, %rd44;
	ld.param.s32 	%rd11, [%rd38+88];
	add.u64 	%rd45, %SP, 0;
	add.u64 	%rd12, %SPL, 0;
	setp.gt.s32 	%p2, %r18, 3;
	@%p2 bra 	$L__BB40_24;
	bra.uni 	$L__BB40_2;

$L__BB40_24:
	or.b64  	%rd68, %rd82, %rd4;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p27, %rd69, 0;
	@%p27 bra 	$L__BB40_26;

	div.u64 	%rd89, %rd82, %rd4;
	bra.uni 	$L__BB40_27;

$L__BB40_26:
	cvt.u32.u64 	%r45, %rd4;
	cvt.u32.u64 	%r46, %rd82;
	div.u32 	%r47, %r46, %r45;
	cvt.u64.u32 	%rd89, %r47;

$L__BB40_27:
	setp.lt.s32 	%p28, %r18, 3;
	@%p28 bra 	$L__BB40_31;

	or.b64  	%rd70, %rd89, %rd5;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p29, %rd71, 0;
	@%p29 bra 	$L__BB40_30;

	div.u64 	%rd89, %rd89, %rd5;
	bra.uni 	$L__BB40_31;

$L__BB40_30:
	cvt.u32.u64 	%r48, %rd5;
	cvt.u32.u64 	%r49, %rd89;
	div.u32 	%r50, %r49, %r48;
	cvt.u64.u32 	%rd89, %r50;

$L__BB40_31:
	setp.lt.s32 	%p30, %r18, 2;
	@%p30 bra 	$L__BB40_35;

	or.b64  	%rd72, %rd89, %rd6;
	and.b64  	%rd73, %rd72, -4294967296;
	setp.eq.s64 	%p31, %rd73, 0;
	@%p31 bra 	$L__BB40_34;

	div.u64 	%rd89, %rd89, %rd6;
	bra.uni 	$L__BB40_35;

$L__BB40_34:
	cvt.u32.u64 	%r51, %rd6;
	cvt.u32.u64 	%r52, %rd89;
	div.u32 	%r53, %r52, %r51;
	cvt.u64.u32 	%rd89, %r53;

$L__BB40_35:
	cvt.u32.u64 	%r54, %rd89;
	setp.gt.s32 	%p32, %r18, 0;
	selp.b32 	%r12, %r54, 0, %p32;
	add.s32 	%r13, %r12, %r2;
	setp.le.s32 	%p33, %r27, %r13;
	setp.le.s32 	%p34, %r28, %r13;
	setp.le.s32 	%p35, %r7, %r12;
	or.pred  	%p36, %p33, %p34;
	or.b32  	%r55, %r13, %r12;
	setp.lt.s32 	%p37, %r55, 0;
	or.pred  	%p38, %p37, %p36;
	or.pred  	%p39, %p35, %p38;
	@%p39 bra 	$L__BB40_37;
	bra.uni 	$L__BB40_36;

$L__BB40_37:
	st.local.v2.u32 	[%rd12], {%r13, %r13};
	st.local.v2.u32 	[%rd12+8], {%r12, %r27};
	st.local.v2.u32 	[%rd12+16], {%r28, %r7};
	mov.u64 	%rd79, $str;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 50, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd80;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd45;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r56, [retval0+0];
	} // callseq 50
	bra.uni 	$L__BB40_38;

$L__BB40_36:
	cvt.s64.s32 	%rd74, %r12;
	mul.lo.s64 	%rd75, %rd9, %rd74;
	add.s64 	%rd76, %rd8, %rd75;
	st.global.u32 	[%rd76], %r13;
	mul.lo.s64 	%rd77, %rd11, %rd74;
	add.s64 	%rd78, %rd10, %rd77;
	st.global.u32 	[%rd78], %r13;

$L__BB40_38:
	add.s64 	%rd82, %rd82, %rd7;
	setp.lt.u64 	%p40, %rd82, %rd37;
	@%p40 bra 	$L__BB40_24;
	bra.uni 	$L__BB40_39;

$L__BB40_2:
	setp.gt.s32 	%p3, %r18, 2;
	@%p3 bra 	$L__BB40_12;
	bra.uni 	$L__BB40_3;

$L__BB40_12:
	cvt.u32.u64 	%r36, %rd5;
	cvt.u32.u64 	%r39, %rd6;

$L__BB40_13:
	or.b64  	%rd56, %rd82, %rd5;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p15, %rd57, 0;
	@%p15 bra 	$L__BB40_15;

	div.u64 	%rd86, %rd82, %rd5;
	bra.uni 	$L__BB40_16;

$L__BB40_15:
	cvt.u32.u64 	%r37, %rd82;
	div.u32 	%r38, %r37, %r36;
	cvt.u64.u32 	%rd86, %r38;

$L__BB40_16:
	setp.lt.s32 	%p16, %r18, 2;
	@%p16 bra 	$L__BB40_20;

	or.b64  	%rd58, %rd86, %rd6;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p17, %rd59, 0;
	@%p17 bra 	$L__BB40_19;

	div.u64 	%rd86, %rd86, %rd6;
	bra.uni 	$L__BB40_20;

$L__BB40_19:
	cvt.u32.u64 	%r40, %rd86;
	div.u32 	%r41, %r40, %r39;
	cvt.u64.u32 	%rd86, %r41;

$L__BB40_20:
	cvt.u32.u64 	%r42, %rd86;
	setp.gt.s32 	%p18, %r18, 0;
	selp.b32 	%r10, %r42, 0, %p18;
	add.s32 	%r11, %r10, %r2;
	setp.le.s32 	%p19, %r27, %r11;
	setp.le.s32 	%p20, %r28, %r11;
	setp.le.s32 	%p21, %r7, %r10;
	or.pred  	%p22, %p19, %p20;
	or.b32  	%r43, %r11, %r10;
	setp.lt.s32 	%p23, %r43, 0;
	or.pred  	%p24, %p23, %p22;
	or.pred  	%p25, %p21, %p24;
	@%p25 bra 	$L__BB40_22;
	bra.uni 	$L__BB40_21;

$L__BB40_22:
	st.local.v2.u32 	[%rd12], {%r11, %r11};
	st.local.v2.u32 	[%rd12+8], {%r10, %r27};
	st.local.v2.u32 	[%rd12+16], {%r28, %r7};
	mov.u64 	%rd65, $str;
	cvta.global.u64 	%rd66, %rd65;
	{ // callseq 49, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd66;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd45;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r44, [retval0+0];
	} // callseq 49
	bra.uni 	$L__BB40_23;

$L__BB40_21:
	cvt.s64.s32 	%rd60, %r10;
	mul.lo.s64 	%rd61, %rd9, %rd60;
	add.s64 	%rd62, %rd8, %rd61;
	st.global.u32 	[%rd62], %r11;
	mul.lo.s64 	%rd63, %rd11, %rd60;
	add.s64 	%rd64, %rd10, %rd63;
	st.global.u32 	[%rd64], %r11;

$L__BB40_23:
	add.s64 	%rd82, %rd82, %rd7;
	setp.lt.u64 	%p26, %rd82, %rd37;
	@%p26 bra 	$L__BB40_13;
	bra.uni 	$L__BB40_39;

$L__BB40_3:
	cvt.u32.u64 	%r30, %rd6;

$L__BB40_4:
	setp.lt.s32 	%p4, %r18, 2;
	mov.u64 	%rd83, %rd82;
	@%p4 bra 	$L__BB40_8;

	or.b64  	%rd46, %rd82, %rd6;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB40_7;

	div.u64 	%rd83, %rd82, %rd6;
	bra.uni 	$L__BB40_8;

$L__BB40_7:
	cvt.u32.u64 	%r31, %rd82;
	div.u32 	%r32, %r31, %r30;
	cvt.u64.u32 	%rd83, %r32;

$L__BB40_8:
	cvt.u32.u64 	%r33, %rd83;
	setp.gt.s32 	%p6, %r18, 0;
	selp.b32 	%r8, %r33, 0, %p6;
	add.s32 	%r9, %r8, %r2;
	setp.le.s32 	%p7, %r27, %r9;
	setp.le.s32 	%p8, %r28, %r9;
	setp.le.s32 	%p9, %r7, %r8;
	or.pred  	%p10, %p7, %p8;
	or.b32  	%r34, %r9, %r8;
	setp.lt.s32 	%p11, %r34, 0;
	or.pred  	%p12, %p11, %p10;
	or.pred  	%p13, %p9, %p12;
	@%p13 bra 	$L__BB40_10;
	bra.uni 	$L__BB40_9;

$L__BB40_10:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r27};
	st.local.v2.u32 	[%rd12+16], {%r28, %r7};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 48, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd45;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r35, [retval0+0];
	} // callseq 48
	bra.uni 	$L__BB40_11;

$L__BB40_9:
	cvt.s64.s32 	%rd48, %r8;
	mul.lo.s64 	%rd49, %rd9, %rd48;
	add.s64 	%rd50, %rd8, %rd49;
	st.global.u32 	[%rd50], %r9;
	mul.lo.s64 	%rd51, %rd11, %rd48;
	add.s64 	%rd52, %rd10, %rd51;
	st.global.u32 	[%rd52], %r9;

$L__BB40_11:
	add.s64 	%rd82, %rd82, %rd7;
	setp.lt.u64 	%p14, %rd82, %rd37;
	@%p14 bra 	$L__BB40_4;

$L__BB40_39:
	ret;

}
	// .globl	init_soft_diag_hess_inds_kernel_cuda_kernel_backward
.visible .entry init_soft_diag_hess_inds_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_1[184],
	.param .u32 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_2,
	.param .align 8 .b8 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_3[184],
	.param .u32 init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_4
)
{
	.local .align 8 .b8 	__local_depot41[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<32>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<80>;


	mov.u64 	%SPL, __local_depot41;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r18, %r19}, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r20, %r21}, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0+8];
	mov.b64 	%rd33, init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_1;
	ld.param.u32 	%r17, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd32, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r16, [init_soft_diag_hess_inds_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r22, %ntid.x;
	cvt.u64.u32 	%rd1, %r22;
	mov.u32 	%r23, %ctaid.x;
	mul.wide.u32 	%rd34, %r22, %r23;
	mov.u32 	%r24, %tid.x;
	cvt.u64.u32 	%rd35, %r24;
	add.s64 	%rd73, %rd34, %rd35;
	setp.ge.u64 	%p3, %rd73, %rd32;
	@%p3 bra 	$L__BB41_33;

	cvt.s64.s32 	%rd4, %r21;
	cvt.s64.s32 	%rd5, %r20;
	cvt.s64.s32 	%rd6, %r19;
	shl.b32 	%r2, %r17, 2;
	ld.param.v2.u32 	{%r25, %r26}, [%rd33+176];
	ld.param.u32 	%r7, [%rd33+172];
	ld.param.u64 	%rd37, [%rd33];
	cvta.to.global.u64 	%rd7, %rd37;
	ld.param.s32 	%rd8, [%rd33+32];
	ld.param.u64 	%rd38, [%rd33+56];
	cvta.to.global.u64 	%rd9, %rd38;
	ld.param.s32 	%rd10, [%rd33+88];
	mov.u32 	%r27, %nctaid.x;
	cvt.u64.u32 	%rd39, %r27;
	mul.lo.s64 	%rd11, %rd1, %rd39;
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd12, %SPL, 0;
	setp.gt.s32 	%p4, %r16, 3;
	@%p4 bra 	$L__BB41_16;
	bra.uni 	$L__BB41_2;

$L__BB41_16:
	or.b64  	%rd56, %rd73, %rd4;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p18, %rd57, 0;
	@%p18 bra 	$L__BB41_18;

	div.u64 	%rd78, %rd73, %rd4;
	bra.uni 	$L__BB41_19;

$L__BB41_18:
	cvt.u32.u64 	%r38, %rd4;
	cvt.u32.u64 	%r39, %rd73;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd78, %r40;

$L__BB41_19:
	setp.lt.s32 	%p19, %r16, 3;
	@%p19 bra 	$L__BB41_23;

	or.b64  	%rd58, %rd78, %rd5;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p20, %rd59, 0;
	@%p20 bra 	$L__BB41_22;

	div.u64 	%rd78, %rd78, %rd5;
	bra.uni 	$L__BB41_23;

$L__BB41_22:
	cvt.u32.u64 	%r41, %rd5;
	cvt.u32.u64 	%r42, %rd78;
	div.u32 	%r43, %r42, %r41;
	cvt.u64.u32 	%rd78, %r43;

$L__BB41_23:
	setp.lt.s32 	%p21, %r16, 2;
	@%p21 bra 	$L__BB41_27;

	or.b64  	%rd60, %rd78, %rd6;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p22, %rd61, 0;
	@%p22 bra 	$L__BB41_26;

	div.u64 	%rd78, %rd78, %rd6;
	bra.uni 	$L__BB41_27;

$L__BB41_26:
	cvt.u32.u64 	%r44, %rd6;
	cvt.u32.u64 	%r45, %rd78;
	div.u32 	%r46, %r45, %r44;
	cvt.u64.u32 	%rd78, %r46;

$L__BB41_27:
	cvt.u32.u64 	%r47, %rd78;
	setp.gt.s32 	%p23, %r16, 0;
	selp.b32 	%r10, %r47, 0, %p23;
	add.s32 	%r11, %r10, %r2;
	setp.le.s32 	%p24, %r25, %r11;
	setp.le.s32 	%p25, %r26, %r11;
	setp.le.s32 	%p26, %r7, %r10;
	or.pred  	%p27, %p24, %p25;
	or.b32  	%r48, %r11, %r10;
	setp.lt.s32 	%p28, %r48, 0;
	or.pred  	%p29, %p28, %p27;
	or.pred  	%p2, %p26, %p29;
	@%p2 bra 	$L__BB41_29;
	bra.uni 	$L__BB41_28;

$L__BB41_29:
	st.local.v2.u32 	[%rd12], {%r11, %r11};
	st.local.v2.u32 	[%rd12+8], {%r10, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd67, $str;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 53, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r49, [retval0+0];
	} // callseq 53
	bra.uni 	$L__BB41_30;

$L__BB41_28:
	cvt.s64.s32 	%rd62, %r10;
	mul.lo.s64 	%rd63, %rd8, %rd62;
	add.s64 	%rd64, %rd7, %rd63;
	st.global.u32 	[%rd64], %r11;
	mul.lo.s64 	%rd65, %rd10, %rd62;
	add.s64 	%rd66, %rd9, %rd65;
	st.global.u32 	[%rd66], %r11;

$L__BB41_30:
	not.pred 	%p30, %p2;
	@%p30 bra 	$L__BB41_32;

	st.local.v2.u32 	[%rd12], {%r11, %r11};
	st.local.v2.u32 	[%rd12+8], {%r10, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd70, $str;
	cvta.global.u64 	%rd71, %rd70;
	{ // callseq 54, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd71;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r50, [retval0+0];
	} // callseq 54

$L__BB41_32:
	add.s64 	%rd73, %rd73, %rd11;
	setp.lt.u64 	%p31, %rd73, %rd32;
	@%p31 bra 	$L__BB41_16;
	bra.uni 	$L__BB41_33;

$L__BB41_2:
	setp.lt.s32 	%p5, %r16, 3;
	mov.u64 	%rd74, %rd73;
	@%p5 bra 	$L__BB41_6;

	or.b64  	%rd41, %rd73, %rd5;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p6, %rd42, 0;
	@%p6 bra 	$L__BB41_5;

	div.u64 	%rd74, %rd73, %rd5;
	bra.uni 	$L__BB41_6;

$L__BB41_5:
	cvt.u32.u64 	%r28, %rd5;
	cvt.u32.u64 	%r29, %rd73;
	div.u32 	%r30, %r29, %r28;
	cvt.u64.u32 	%rd74, %r30;

$L__BB41_6:
	setp.lt.s32 	%p7, %r16, 2;
	@%p7 bra 	$L__BB41_10;

	or.b64  	%rd43, %rd74, %rd6;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p8, %rd44, 0;
	@%p8 bra 	$L__BB41_9;

	div.u64 	%rd74, %rd74, %rd6;
	bra.uni 	$L__BB41_10;

$L__BB41_9:
	cvt.u32.u64 	%r31, %rd6;
	cvt.u32.u64 	%r32, %rd74;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd74, %r33;

$L__BB41_10:
	cvt.u32.u64 	%r34, %rd74;
	setp.gt.s32 	%p9, %r16, 0;
	selp.b32 	%r8, %r34, 0, %p9;
	add.s32 	%r9, %r8, %r2;
	setp.le.s32 	%p10, %r25, %r9;
	setp.le.s32 	%p11, %r26, %r9;
	setp.le.s32 	%p12, %r7, %r8;
	or.pred  	%p13, %p10, %p11;
	or.b32  	%r35, %r9, %r8;
	setp.lt.s32 	%p14, %r35, 0;
	or.pred  	%p15, %p14, %p13;
	or.pred  	%p1, %p12, %p15;
	@%p1 bra 	$L__BB41_12;
	bra.uni 	$L__BB41_11;

$L__BB41_12:
	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd50, $str;
	cvta.global.u64 	%rd51, %rd50;
	{ // callseq 51, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd51;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r36, [retval0+0];
	} // callseq 51
	bra.uni 	$L__BB41_13;

$L__BB41_11:
	cvt.s64.s32 	%rd45, %r8;
	mul.lo.s64 	%rd46, %rd8, %rd45;
	add.s64 	%rd47, %rd7, %rd46;
	st.global.u32 	[%rd47], %r9;
	mul.lo.s64 	%rd48, %rd10, %rd45;
	add.s64 	%rd49, %rd9, %rd48;
	st.global.u32 	[%rd49], %r9;

$L__BB41_13:
	not.pred 	%p16, %p1;
	@%p16 bra 	$L__BB41_15;

	st.local.v2.u32 	[%rd12], {%r9, %r9};
	st.local.v2.u32 	[%rd12+8], {%r8, %r25};
	st.local.v2.u32 	[%rd12+16], {%r26, %r7};
	mov.u64 	%rd53, $str;
	cvta.global.u64 	%rd54, %rd53;
	{ // callseq 52, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r37, [retval0+0];
	} // callseq 52

$L__BB41_15:
	add.s64 	%rd73, %rd73, %rd11;
	setp.lt.u64 	%p17, %rd73, %rd32;
	@%p17 bra 	$L__BB41_2;

$L__BB41_33:
	ret;

}
	// .globl	init_affine_mass_matrix_kernel_cuda_kernel_forward
.visible .entry init_affine_mass_matrix_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0[32],
	.param .u32 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_1,
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_forward_param_11[56]
)
{
	.reg .pred 	%p<217>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<409>;
	.reg .f64 	%fd<861>;
	.reg .b64 	%rd<119>;


	ld.param.v2.u32 	{%r118, %r119}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r120, %r121}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r126, %r127}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r134, %r135}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r142, %r143}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r150, %r151}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r158, %r159}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r166, %r167}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r174, %r175}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r182, %r183}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r190, %r191}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r198, %r199}, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_11+32];
	ld.param.u64 	%rd58, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd56, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd54, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd52, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd50, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd46, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd44, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd42, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd41, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r26, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r202, %ntid.x;
	cvt.u64.u32 	%rd1, %r202;
	mov.u32 	%r203, %ctaid.x;
	mul.wide.u32 	%rd62, %r202, %r203;
	mov.u32 	%r204, %tid.x;
	cvt.u64.u32 	%rd63, %r204;
	add.s64 	%rd115, %rd62, %rd63;
	setp.ge.u64 	%p9, %rd115, %rd41;
	@%p9 bra 	$L__BB42_134;

	cvta.to.global.u64 	%rd5, %rd58;
	cvta.to.global.u64 	%rd6, %rd56;
	cvta.to.global.u64 	%rd7, %rd54;
	cvta.to.global.u64 	%rd8, %rd52;
	cvta.to.global.u64 	%rd9, %rd50;
	cvta.to.global.u64 	%rd10, %rd48;
	cvta.to.global.u64 	%rd11, %rd46;
	cvta.to.global.u64 	%rd12, %rd44;
	cvta.to.global.u64 	%rd13, %rd42;
	cvt.s64.s32 	%rd14, %r121;
	cvt.s64.s32 	%rd15, %r120;
	cvt.s64.s32 	%rd16, %r119;
	cvt.s64.s32 	%rd17, %r142;
	cvt.s64.s32 	%rd18, %r150;
	mov.u32 	%r205, %nctaid.x;
	cvt.u64.u32 	%rd64, %r205;
	mul.lo.s64 	%rd19, %rd1, %rd64;
	cvt.s64.s32 	%rd20, %r158;
	cvt.s64.s32 	%rd21, %r166;
	cvt.s64.s32 	%rd22, %r190;
	cvt.s64.s32 	%rd23, %r174;
	cvt.s64.s32 	%rd24, %r182;
	cvt.s64.s32 	%rd25, %r134;
	cvt.s64.s32 	%rd26, %r126;
	cvt.s64.s32 	%rd27, %r198;

$L__BB42_2:
	setp.lt.s32 	%p10, %r26, 4;
	mov.u64 	%rd116, %rd115;
	@%p10 bra 	$L__BB42_6;

	or.b64  	%rd65, %rd115, %rd14;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p11, %rd66, 0;
	@%p11 bra 	$L__BB42_5;

	div.u64 	%rd116, %rd115, %rd14;
	bra.uni 	$L__BB42_6;

$L__BB42_5:
	cvt.u32.u64 	%r206, %rd14;
	cvt.u32.u64 	%r207, %rd115;
	div.u32 	%r208, %r207, %r206;
	cvt.u64.u32 	%rd116, %r208;

$L__BB42_6:
	setp.lt.s32 	%p12, %r26, 3;
	@%p12 bra 	$L__BB42_10;

	or.b64  	%rd67, %rd116, %rd15;
	and.b64  	%rd68, %rd67, -4294967296;
	setp.eq.s64 	%p13, %rd68, 0;
	@%p13 bra 	$L__BB42_9;

	div.u64 	%rd116, %rd116, %rd15;
	bra.uni 	$L__BB42_10;

$L__BB42_9:
	cvt.u32.u64 	%r209, %rd15;
	cvt.u32.u64 	%r210, %rd116;
	div.u32 	%r211, %r210, %r209;
	cvt.u64.u32 	%rd116, %r211;

$L__BB42_10:
	setp.lt.s32 	%p14, %r26, 2;
	@%p14 bra 	$L__BB42_14;

	or.b64  	%rd69, %rd116, %rd16;
	and.b64  	%rd70, %rd69, -4294967296;
	setp.eq.s64 	%p15, %rd70, 0;
	@%p15 bra 	$L__BB42_13;

	div.u64 	%rd116, %rd116, %rd16;
	bra.uni 	$L__BB42_14;

$L__BB42_13:
	cvt.u32.u64 	%r212, %rd16;
	cvt.u32.u64 	%r213, %rd116;
	div.u32 	%r214, %r213, %r212;
	cvt.u64.u32 	%rd116, %r214;

$L__BB42_14:
	ld.param.u32 	%r387, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_1];
	cvt.u32.u64 	%r215, %rd116;
	setp.gt.s32 	%p16, %r26, 0;
	selp.b32 	%r2, %r215, 0, %p16;
	setp.ge.s32 	%p17, %r2, %r387;
	@%p17 bra 	$L__BB42_133;

	cvt.s64.s32 	%rd38, %r2;
	mul.lo.s64 	%rd71, %rd38, %rd17;
	add.s64 	%rd72, %rd11, %rd71;
	ld.global.s32 	%rd39, [%rd72];
	mul.lo.s64 	%rd73, %rd39, %rd18;
	add.s64 	%rd74, %rd10, %rd73;
	ld.global.u32 	%r216, [%rd74];
	setp.eq.s32 	%p18, %r216, 0;
	@%p18 bra 	$L__BB42_133;

	mul.lo.s64 	%rd75, %rd39, %rd20;
	add.s64 	%rd76, %rd9, %rd75;
	mul.lo.s64 	%rd77, %rd39, %rd22;
	add.s64 	%rd78, %rd5, %rd77;
	ld.global.f64 	%fd1, [%rd78];
	ld.global.f64 	%fd2, [%rd78+8];
	ld.global.f64 	%fd3, [%rd78+16];
	mul.lo.s64 	%rd79, %rd39, %rd23;
	add.s64 	%rd80, %rd7, %rd79;
	ld.global.f64 	%fd4, [%rd80];
	mul.lo.s64 	%rd81, %rd39, %rd24;
	add.s64 	%rd82, %rd6, %rd81;
	ld.global.f64 	%fd5, [%rd82];
	mul.lo.s64 	%rd83, %rd38, %rd25;
	add.s64 	%rd84, %rd12, %rd83;
	ld.global.s32 	%rd85, [%rd84];
	mul.lo.s64 	%rd86, %rd85, %rd26;
	add.s64 	%rd87, %rd13, %rd86;
	ld.global.s32 	%rd88, [%rd84+4];
	mul.lo.s64 	%rd89, %rd88, %rd26;
	add.s64 	%rd90, %rd13, %rd89;
	ld.global.s32 	%rd91, [%rd84+8];
	mul.lo.s64 	%rd92, %rd91, %rd26;
	add.s64 	%rd93, %rd13, %rd92;
	ld.global.f64 	%fd6, [%rd87];
	ld.global.f64 	%fd7, [%rd90];
	sub.f64 	%fd229, %fd7, %fd6;
	ld.global.f64 	%fd8, [%rd87+8];
	ld.global.f64 	%fd9, [%rd90+8];
	sub.f64 	%fd230, %fd9, %fd8;
	ld.global.f64 	%fd10, [%rd87+16];
	ld.global.f64 	%fd11, [%rd90+16];
	sub.f64 	%fd231, %fd11, %fd10;
	ld.global.f64 	%fd12, [%rd93];
	sub.f64 	%fd232, %fd12, %fd6;
	ld.global.f64 	%fd13, [%rd93+8];
	sub.f64 	%fd233, %fd13, %fd8;
	ld.global.f64 	%fd14, [%rd93+16];
	sub.f64 	%fd234, %fd14, %fd10;
	mul.f64 	%fd235, %fd230, %fd234;
	mul.f64 	%fd236, %fd231, %fd233;
	sub.f64 	%fd842, %fd235, %fd236;
	mul.f64 	%fd237, %fd231, %fd232;
	mul.f64 	%fd238, %fd229, %fd234;
	sub.f64 	%fd843, %fd237, %fd238;
	mul.f64 	%fd239, %fd229, %fd233;
	mul.f64 	%fd240, %fd230, %fd232;
	sub.f64 	%fd844, %fd239, %fd240;
	mul.f64 	%fd241, %fd843, %fd843;
	fma.rn.f64 	%fd242, %fd842, %fd842, %fd241;
	fma.rn.f64 	%fd243, %fd844, %fd844, %fd242;
	sqrt.rn.f64 	%fd244, %fd243;
	ld.global.u32 	%r3, [%rd76];
	setp.eq.s32 	%p19, %r3, 0;
	mov.f64 	%fd860, 0d0000000000000000;
	mov.f64 	%fd856, %fd860;
	mov.f64 	%fd852, %fd860;
	mov.f64 	%fd848, %fd860;
	mov.f64 	%fd855, %fd860;
	mov.f64 	%fd851, %fd860;
	mov.f64 	%fd847, %fd860;
	mov.f64 	%fd850, %fd860;
	mov.f64 	%fd846, %fd860;
	mov.f64 	%fd845, %fd860;
	@%p19 bra 	$L__BB42_130;

	mul.f64 	%fd796, %fd244, 0d3FE0000000000000;
	mul.lo.s64 	%rd94, %rd39, %rd21;
	add.s64 	%rd95, %rd8, %rd94;
	ld.global.u32 	%r217, [%rd95];
	setp.eq.s32 	%p20, %r217, 0;
	neg.f64 	%fd245, %fd842;
	neg.f64 	%fd246, %fd843;
	neg.f64 	%fd247, %fd844;
	selp.f64 	%fd248, 0d0000000000000000, %fd245, %p20;
	selp.f64 	%fd249, 0d0000000000000000, %fd246, %p20;
	selp.f64 	%fd250, 0d0000000000000000, %fd247, %p20;
	sub.f64 	%fd251, %fd6, %fd1;
	div.rn.f64 	%fd252, %fd251, 0d4008000000000000;
	mov.f64 	%fd253, 0d4008000000000000;
	sub.f64 	%fd254, %fd7, %fd1;
	div.rn.f64 	%fd255, %fd254, 0d4008000000000000;
	add.f64 	%fd256, %fd252, %fd255;
	sub.f64 	%fd257, %fd12, %fd1;
	div.rn.f64 	%fd258, %fd257, 0d4008000000000000;
	add.f64 	%fd22, %fd256, %fd258;
	mul.f64 	%fd259, %fd251, 0d4008000000000000;
	div.rn.f64 	%fd260, %fd259, 0d4014000000000000;
	div.rn.f64 	%fd261, %fd254, 0d4014000000000000;
	add.f64 	%fd262, %fd260, %fd261;
	div.rn.f64 	%fd263, %fd257, 0d4014000000000000;
	add.f64 	%fd23, %fd262, %fd263;
	div.rn.f64 	%fd264, %fd251, 0d4014000000000000;
	add.f64 	%fd265, %fd264, %fd261;
	mul.f64 	%fd266, %fd257, 0d4008000000000000;
	div.rn.f64 	%fd267, %fd266, 0d4014000000000000;
	add.f64 	%fd24, %fd265, %fd267;
	mul.f64 	%fd268, %fd254, 0d4008000000000000;
	div.rn.f64 	%fd269, %fd268, 0d4014000000000000;
	add.f64 	%fd270, %fd264, %fd269;
	add.f64 	%fd25, %fd263, %fd270;
	sub.f64 	%fd271, %fd8, %fd2;
	div.rn.f64 	%fd272, %fd271, 0d4008000000000000;
	sub.f64 	%fd273, %fd9, %fd2;
	div.rn.f64 	%fd274, %fd273, 0d4008000000000000;
	add.f64 	%fd275, %fd272, %fd274;
	sub.f64 	%fd276, %fd13, %fd2;
	div.rn.f64 	%fd277, %fd276, 0d4008000000000000;
	add.f64 	%fd26, %fd275, %fd277;
	mul.f64 	%fd278, %fd271, 0d4008000000000000;
	div.rn.f64 	%fd279, %fd278, 0d4014000000000000;
	div.rn.f64 	%fd280, %fd273, 0d4014000000000000;
	add.f64 	%fd281, %fd279, %fd280;
	div.rn.f64 	%fd282, %fd276, 0d4014000000000000;
	add.f64 	%fd27, %fd281, %fd282;
	div.rn.f64 	%fd283, %fd271, 0d4014000000000000;
	add.f64 	%fd284, %fd283, %fd280;
	mul.f64 	%fd285, %fd276, 0d4008000000000000;
	div.rn.f64 	%fd286, %fd285, 0d4014000000000000;
	add.f64 	%fd28, %fd284, %fd286;
	mul.f64 	%fd287, %fd273, 0d4008000000000000;
	div.rn.f64 	%fd288, %fd287, 0d4014000000000000;
	add.f64 	%fd289, %fd283, %fd288;
	add.f64 	%fd29, %fd282, %fd289;
	sub.f64 	%fd290, %fd10, %fd3;
	div.rn.f64 	%fd291, %fd290, 0d4008000000000000;
	sub.f64 	%fd292, %fd11, %fd3;
	div.rn.f64 	%fd293, %fd292, 0d4008000000000000;
	add.f64 	%fd294, %fd291, %fd293;
	sub.f64 	%fd295, %fd14, %fd3;
	div.rn.f64 	%fd296, %fd295, 0d4008000000000000;
	add.f64 	%fd30, %fd294, %fd296;
	mul.f64 	%fd297, %fd290, 0d4008000000000000;
	div.rn.f64 	%fd298, %fd297, 0d4014000000000000;
	div.rn.f64 	%fd299, %fd292, 0d4014000000000000;
	add.f64 	%fd300, %fd298, %fd299;
	div.rn.f64 	%fd301, %fd295, 0d4014000000000000;
	add.f64 	%fd31, %fd300, %fd301;
	div.rn.f64 	%fd302, %fd290, 0d4014000000000000;
	add.f64 	%fd303, %fd302, %fd299;
	mul.f64 	%fd304, %fd295, 0d4008000000000000;
	div.rn.f64 	%fd305, %fd304, 0d4014000000000000;
	add.f64 	%fd32, %fd303, %fd305;
	mul.f64 	%fd306, %fd292, 0d4008000000000000;
	div.rn.f64 	%fd307, %fd306, 0d4014000000000000;
	add.f64 	%fd308, %fd302, %fd307;
	add.f64 	%fd33, %fd301, %fd308;
	selp.f64 	%fd309, %fd844, %fd250, %p20;
	selp.f64 	%fd310, %fd843, %fd249, %p20;
	selp.f64 	%fd311, %fd842, %fd248, %p20;
	add.f64 	%fd312, %fd796, %fd796;
	div.rn.f64 	%fd842, %fd311, %fd312;
	div.rn.f64 	%fd843, %fd310, %fd312;
	div.rn.f64 	%fd844, %fd309, %fd312;
	mul.f64 	%fd37, %fd5, %fd796;
	mov.f64 	%fd313, 0d3FF0000000000000;
	sub.f64 	%fd314, %fd313, %fd22;
	sub.f64 	%fd315, %fd314, %fd26;
	sub.f64 	%fd38, %fd315, %fd30;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r4}, %fd38;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r5}, %fd253;
	}
	and.b32  	%r6, %r5, 2146435072;
	setp.eq.s32 	%p21, %r6, 1073741824;
	abs.f64 	%fd39, %fd38;
	{ // callseq 55, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd39;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd810, [retval0+0];
	} // callseq 55
	setp.lt.s32 	%p22, %r4, 0;
	and.pred  	%p1, %p22, %p21;
	not.pred 	%p23, %p1;
	@%p23 bra 	$L__BB42_19;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r218}, %fd810;
	}
	xor.b32  	%r219, %r218, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r220, %temp}, %fd810;
	}
	mov.b64 	%fd810, {%r220, %r219};

$L__BB42_19:
	setp.eq.f64 	%p24, %fd38, 0d0000000000000000;
	@%p24 bra 	$L__BB42_23;
	bra.uni 	$L__BB42_20;

$L__BB42_23:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r394}, %fd38;
	}
	selp.b32 	%r221, %r394, 0, %p21;
	mov.u32 	%r222, 0;
	or.b32  	%r223, %r221, 2146435072;
	setp.lt.s32 	%p28, %r5, 0;
	selp.b32 	%r224, %r223, %r221, %p28;
	mov.b64 	%fd810, {%r222, %r224};
	bra.uni 	$L__BB42_24;

$L__BB42_20:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r391}, %fd38;
	}
	setp.gt.s32 	%p25, %r391, -1;
	@%p25 bra 	$L__BB42_24;

	mov.f64 	%fd316, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd317, %fd316;
	setp.eq.f64 	%p26, %fd317, 0d4008000000000000;
	@%p26 bra 	$L__BB42_24;

	mov.f64 	%fd810, 0dFFF8000000000000;

$L__BB42_24:
	add.f64 	%fd319, %fd38, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r225}, %fd319;
	}
	and.b32  	%r226, %r225, 2146435072;
	setp.ne.s32 	%p29, %r226, 2146435072;
	@%p29 bra 	$L__BB42_31;

	abs.f64 	%fd800, %fd38;
	setp.gtu.f64 	%p30, %fd800, 0d7FF0000000000000;
	@%p30 bra 	$L__BB42_30;
	bra.uni 	$L__BB42_26;

$L__BB42_30:
	mov.f64 	%fd321, 0d4008000000000000;
	add.rn.f64 	%fd810, %fd38, %fd321;
	bra.uni 	$L__BB42_31;

$L__BB42_26:
	mov.f64 	%fd320, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r227, %temp}, %fd320;
	}
	and.b32  	%r7, %r5, 2147483647;
	setp.eq.s32 	%p31, %r7, 2146435072;
	setp.eq.s32 	%p32, %r227, 0;
	and.pred  	%p33, %p31, %p32;
	@%p33 bra 	$L__BB42_29;
	bra.uni 	$L__BB42_27;

$L__BB42_29:
	abs.f64 	%fd801, %fd38;
	setp.gt.f64 	%p40, %fd801, 0d3FF0000000000000;
	selp.b32 	%r234, 2146435072, 0, %p40;
	mov.u32 	%r235, 0;
	xor.b32  	%r236, %r234, 2146435072;
	setp.lt.s32 	%p41, %r5, 0;
	selp.b32 	%r237, %r236, %r234, %p41;
	setp.eq.f64 	%p42, %fd38, 0dBFF0000000000000;
	selp.b32 	%r238, 1072693248, %r237, %p42;
	mov.b64 	%fd810, {%r235, %r238};
	bra.uni 	$L__BB42_31;

$L__BB42_27:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r392}, %fd38;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r228, %temp}, %fd38;
	}
	and.b32  	%r229, %r392, 2147483647;
	setp.ne.s32 	%p34, %r229, 2146435072;
	setp.ne.s32 	%p35, %r228, 0;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB42_31;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r393}, %fd38;
	}
	setp.lt.s32 	%p210, %r393, 0;
	and.pred  	%p209, %p210, %p21;
	setp.gt.s32 	%p37, %r5, -1;
	selp.b32 	%r230, 2146435072, 0, %p37;
	mov.u32 	%r231, 0;
	setp.ne.s32 	%p38, %r7, 1071644672;
	and.pred  	%p39, %p38, %p209;
	or.b32  	%r232, %r230, -2147483648;
	selp.b32 	%r233, %r232, %r230, %p39;
	mov.b64 	%fd810, {%r231, %r233};

$L__BB42_31:
	mul.f64 	%fd49, %fd37, 0dBFE2000000000000;
	mov.f64 	%fd322, 0d0000000000000000;
	sub.f64 	%fd323, %fd322, %fd810;
	div.rn.f64 	%fd324, %fd323, 0d4008000000000000;
	setp.eq.f64 	%p43, %fd38, 0d3FF0000000000000;
	selp.f64 	%fd50, 0dBFD5555555555555, %fd324, %p43;
	abs.f64 	%fd51, %fd22;
	{ // callseq 56, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd51;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd813, [retval0+0];
	} // callseq 56
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r8}, %fd22;
	}
	setp.lt.s32 	%p44, %r8, 0;
	and.pred  	%p2, %p44, %p21;
	not.pred 	%p46, %p2;
	@%p46 bra 	$L__BB42_33;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r239}, %fd813;
	}
	xor.b32  	%r240, %r239, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r241, %temp}, %fd813;
	}
	mov.b64 	%fd813, {%r241, %r240};

$L__BB42_33:
	setp.eq.f64 	%p47, %fd22, 0d0000000000000000;
	@%p47 bra 	$L__BB42_37;
	bra.uni 	$L__BB42_34;

$L__BB42_37:
	selp.b32 	%r242, %r8, 0, %p21;
	mov.u32 	%r243, 0;
	or.b32  	%r244, %r242, 2146435072;
	setp.lt.s32 	%p51, %r5, 0;
	selp.b32 	%r245, %r244, %r242, %p51;
	mov.b64 	%fd813, {%r243, %r245};
	bra.uni 	$L__BB42_38;

$L__BB42_34:
	setp.gt.s32 	%p48, %r8, -1;
	@%p48 bra 	$L__BB42_38;

	mov.f64 	%fd325, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd326, %fd325;
	setp.eq.f64 	%p49, %fd326, 0d4008000000000000;
	@%p49 bra 	$L__BB42_38;

	mov.f64 	%fd813, 0dFFF8000000000000;

$L__BB42_38:
	add.f64 	%fd328, %fd22, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r246}, %fd328;
	}
	and.b32  	%r247, %r246, 2146435072;
	setp.ne.s32 	%p52, %r247, 2146435072;
	@%p52 bra 	$L__BB42_45;

	setp.gtu.f64 	%p53, %fd51, 0d7FF0000000000000;
	@%p53 bra 	$L__BB42_44;
	bra.uni 	$L__BB42_40;

$L__BB42_44:
	mov.f64 	%fd330, 0d4008000000000000;
	add.rn.f64 	%fd813, %fd22, %fd330;
	bra.uni 	$L__BB42_45;

$L__BB42_40:
	mov.f64 	%fd329, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r248, %temp}, %fd329;
	}
	and.b32  	%r9, %r5, 2147483647;
	setp.eq.s32 	%p54, %r9, 2146435072;
	setp.eq.s32 	%p55, %r248, 0;
	and.pred  	%p56, %p54, %p55;
	@%p56 bra 	$L__BB42_43;
	bra.uni 	$L__BB42_41;

$L__BB42_43:
	setp.gt.f64 	%p63, %fd51, 0d3FF0000000000000;
	selp.b32 	%r255, 2146435072, 0, %p63;
	mov.u32 	%r256, 0;
	xor.b32  	%r257, %r255, 2146435072;
	setp.lt.s32 	%p64, %r5, 0;
	selp.b32 	%r258, %r257, %r255, %p64;
	setp.eq.f64 	%p65, %fd22, 0dBFF0000000000000;
	selp.b32 	%r259, 1072693248, %r258, %p65;
	mov.b64 	%fd813, {%r256, %r259};
	bra.uni 	$L__BB42_45;

$L__BB42_41:
	{
	.reg .b32 %temp;
	mov.b64 	{%r249, %temp}, %fd22;
	}
	and.b32  	%r250, %r8, 2147483647;
	setp.ne.s32 	%p57, %r250, 2146435072;
	setp.ne.s32 	%p58, %r249, 0;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB42_45;

	setp.gt.s32 	%p60, %r5, -1;
	selp.b32 	%r251, 2146435072, 0, %p60;
	mov.u32 	%r252, 0;
	setp.ne.s32 	%p61, %r9, 1071644672;
	and.pred  	%p62, %p61, %p2;
	or.b32  	%r253, %r251, -2147483648;
	selp.b32 	%r254, %r253, %r251, %p62;
	mov.b64 	%fd813, {%r252, %r254};

$L__BB42_45:
	mul.f64 	%fd331, %fd22, %fd22;
	mul.f64 	%fd332, %fd26, 0d3FE0000000000000;
	mov.f64 	%fd333, 0d3FE0000000000000;
	mul.f64 	%fd334, %fd30, 0d3FE0000000000000;
	mul.f64 	%fd335, %fd22, 0d3FE0000000000000;
	div.rn.f64 	%fd336, %fd813, 0d4008000000000000;
	setp.eq.f64 	%p66, %fd22, 0d3FF0000000000000;
	mov.f64 	%fd337, 0d3FF0000000000000;
	selp.f64 	%fd338, 0d3FD5555555555555, %fd336, %p66;
	mul.f64 	%fd339, %fd842, %fd338;
	mul.f64 	%fd340, %fd331, 0d3FE0000000000000;
	mul.f64 	%fd341, %fd26, %fd340;
	mul.f64 	%fd342, %fd842, %fd341;
	mul.f64 	%fd343, %fd340, %fd30;
	mul.f64 	%fd344, %fd842, %fd343;
	mul.f64 	%fd345, %fd22, %fd26;
	mul.f64 	%fd346, %fd26, %fd345;
	mul.f64 	%fd347, %fd842, %fd346;
	mul.f64 	%fd348, %fd345, %fd30;
	mul.f64 	%fd349, %fd842, %fd348;
	mul.f64 	%fd350, %fd22, %fd30;
	mul.f64 	%fd351, %fd30, %fd350;
	mul.f64 	%fd352, %fd842, %fd351;
	mul.f64 	%fd353, %fd842, %fd50;
	fma.rn.f64 	%fd61, %fd49, %fd353, 0d0000000000000000;
	mul.f64 	%fd354, %fd26, %fd26;
	div.rn.f64 	%fd355, %fd26, 0d4008000000000000;
	sub.f64 	%fd356, %fd333, %fd355;
	sub.f64 	%fd357, %fd356, %fd335;
	sub.f64 	%fd358, %fd357, %fd334;
	mul.f64 	%fd359, %fd354, %fd358;
	mul.f64 	%fd360, %fd843, %fd359;
	fma.rn.f64 	%fd62, %fd49, %fd360, 0d0000000000000000;
	fma.rn.f64 	%fd63, %fd49, %fd339, 0d0000000000000000;
	fma.rn.f64 	%fd64, %fd49, %fd344, 0d0000000000000000;
	fma.rn.f64 	%fd65, %fd49, %fd349, 0d0000000000000000;
	div.rn.f64 	%fd361, %fd22, 0d4008000000000000;
	sub.f64 	%fd362, %fd333, %fd361;
	sub.f64 	%fd363, %fd362, %fd332;
	sub.f64 	%fd364, %fd363, %fd334;
	mul.f64 	%fd365, %fd331, %fd364;
	mul.f64 	%fd366, %fd842, %fd365;
	fma.rn.f64 	%fd66, %fd49, %fd366, 0d0000000000000000;
	mul.f64 	%fd367, %fd30, %fd30;
	div.rn.f64 	%fd368, %fd30, 0d4008000000000000;
	sub.f64 	%fd369, %fd333, %fd368;
	sub.f64 	%fd370, %fd369, %fd335;
	sub.f64 	%fd371, %fd370, %fd332;
	mul.f64 	%fd372, %fd367, %fd371;
	mul.f64 	%fd373, %fd844, %fd372;
	fma.rn.f64 	%fd67, %fd49, %fd373, 0d0000000000000000;
	fma.rn.f64 	%fd68, %fd49, %fd342, 0d0000000000000000;
	fma.rn.f64 	%fd69, %fd49, %fd347, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd49, %fd352, 0d0000000000000000;
	mul.f64 	%fd71, %fd37, 0d3FE0AAAAA0000000;
	sub.f64 	%fd374, %fd337, %fd23;
	sub.f64 	%fd375, %fd374, %fd27;
	sub.f64 	%fd72, %fd375, %fd31;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r10}, %fd72;
	}
	abs.f64 	%fd73, %fd72;
	{ // callseq 57, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd73;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd816, [retval0+0];
	} // callseq 57
	setp.lt.s32 	%p67, %r10, 0;
	and.pred  	%p3, %p67, %p21;
	not.pred 	%p69, %p3;
	@%p69 bra 	$L__BB42_47;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r260}, %fd816;
	}
	xor.b32  	%r261, %r260, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r262, %temp}, %fd816;
	}
	mov.b64 	%fd816, {%r262, %r261};

$L__BB42_47:
	setp.eq.f64 	%p70, %fd72, 0d0000000000000000;
	@%p70 bra 	$L__BB42_51;
	bra.uni 	$L__BB42_48;

$L__BB42_51:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r390}, %fd72;
	}
	selp.b32 	%r263, %r390, 0, %p21;
	mov.u32 	%r264, 0;
	or.b32  	%r265, %r263, 2146435072;
	setp.lt.s32 	%p74, %r5, 0;
	selp.b32 	%r266, %r265, %r263, %p74;
	mov.b64 	%fd816, {%r264, %r266};
	bra.uni 	$L__BB42_52;

$L__BB42_48:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r386}, %fd72;
	}
	setp.gt.s32 	%p71, %r386, -1;
	@%p71 bra 	$L__BB42_52;

	mov.f64 	%fd376, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd377, %fd376;
	setp.eq.f64 	%p72, %fd377, 0d4008000000000000;
	@%p72 bra 	$L__BB42_52;

	mov.f64 	%fd816, 0dFFF8000000000000;

$L__BB42_52:
	add.f64 	%fd379, %fd72, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r267}, %fd379;
	}
	and.b32  	%r268, %r267, 2146435072;
	setp.ne.s32 	%p75, %r268, 2146435072;
	@%p75 bra 	$L__BB42_59;

	abs.f64 	%fd798, %fd72;
	setp.gtu.f64 	%p76, %fd798, 0d7FF0000000000000;
	@%p76 bra 	$L__BB42_58;
	bra.uni 	$L__BB42_54;

$L__BB42_58:
	mov.f64 	%fd381, 0d4008000000000000;
	add.rn.f64 	%fd816, %fd72, %fd381;
	bra.uni 	$L__BB42_59;

$L__BB42_54:
	mov.f64 	%fd380, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r269, %temp}, %fd380;
	}
	and.b32  	%r11, %r5, 2147483647;
	setp.eq.s32 	%p77, %r11, 2146435072;
	setp.eq.s32 	%p78, %r269, 0;
	and.pred  	%p79, %p77, %p78;
	@%p79 bra 	$L__BB42_57;
	bra.uni 	$L__BB42_55;

$L__BB42_57:
	abs.f64 	%fd799, %fd72;
	setp.gt.f64 	%p86, %fd799, 0d3FF0000000000000;
	selp.b32 	%r276, 2146435072, 0, %p86;
	mov.u32 	%r277, 0;
	xor.b32  	%r278, %r276, 2146435072;
	setp.lt.s32 	%p87, %r5, 0;
	selp.b32 	%r279, %r278, %r276, %p87;
	setp.eq.f64 	%p88, %fd72, 0dBFF0000000000000;
	selp.b32 	%r280, 1072693248, %r279, %p88;
	mov.b64 	%fd816, {%r277, %r280};
	bra.uni 	$L__BB42_59;

$L__BB42_55:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r388}, %fd72;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r270, %temp}, %fd72;
	}
	and.b32  	%r271, %r388, 2147483647;
	setp.ne.s32 	%p80, %r271, 2146435072;
	setp.ne.s32 	%p81, %r270, 0;
	or.pred  	%p82, %p80, %p81;
	@%p82 bra 	$L__BB42_59;

	and.b32  	%r395, %r5, 2147483647;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r389}, %fd72;
	}
	setp.lt.s32 	%p208, %r389, 0;
	and.pred  	%p207, %p208, %p21;
	setp.gt.s32 	%p83, %r5, -1;
	selp.b32 	%r272, 2146435072, 0, %p83;
	mov.u32 	%r273, 0;
	setp.ne.s32 	%p84, %r395, 1071644672;
	and.pred  	%p85, %p84, %p207;
	or.b32  	%r274, %r272, -2147483648;
	selp.b32 	%r275, %r274, %r272, %p85;
	mov.b64 	%fd816, {%r273, %r275};

$L__BB42_59:
	mov.f64 	%fd382, 0d0000000000000000;
	sub.f64 	%fd383, %fd382, %fd816;
	div.rn.f64 	%fd384, %fd383, 0d4008000000000000;
	setp.eq.f64 	%p89, %fd72, 0d3FF0000000000000;
	selp.f64 	%fd83, 0dBFD5555555555555, %fd384, %p89;
	abs.f64 	%fd84, %fd23;
	{ // callseq 58, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd84;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd819, [retval0+0];
	} // callseq 58
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r12}, %fd23;
	}
	setp.lt.s32 	%p90, %r12, 0;
	and.pred  	%p4, %p90, %p21;
	not.pred 	%p92, %p4;
	@%p92 bra 	$L__BB42_61;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r281}, %fd819;
	}
	xor.b32  	%r282, %r281, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r283, %temp}, %fd819;
	}
	mov.b64 	%fd819, {%r283, %r282};

$L__BB42_61:
	setp.eq.f64 	%p93, %fd23, 0d0000000000000000;
	@%p93 bra 	$L__BB42_65;
	bra.uni 	$L__BB42_62;

$L__BB42_65:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r399}, %fd23;
	}
	selp.b32 	%r284, %r399, 0, %p21;
	mov.u32 	%r285, 0;
	or.b32  	%r286, %r284, 2146435072;
	setp.lt.s32 	%p97, %r5, 0;
	selp.b32 	%r287, %r286, %r284, %p97;
	mov.b64 	%fd819, {%r285, %r287};
	bra.uni 	$L__BB42_66;

$L__BB42_62:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r396}, %fd23;
	}
	setp.gt.s32 	%p94, %r396, -1;
	@%p94 bra 	$L__BB42_66;

	mov.f64 	%fd385, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd386, %fd385;
	setp.eq.f64 	%p95, %fd386, 0d4008000000000000;
	@%p95 bra 	$L__BB42_66;

	mov.f64 	%fd819, 0dFFF8000000000000;

$L__BB42_66:
	add.f64 	%fd388, %fd23, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r288}, %fd388;
	}
	and.b32  	%r289, %r288, 2146435072;
	setp.ne.s32 	%p98, %r289, 2146435072;
	@%p98 bra 	$L__BB42_73;

	abs.f64 	%fd802, %fd23;
	setp.gtu.f64 	%p99, %fd802, 0d7FF0000000000000;
	@%p99 bra 	$L__BB42_72;
	bra.uni 	$L__BB42_68;

$L__BB42_72:
	mov.f64 	%fd390, 0d4008000000000000;
	add.rn.f64 	%fd819, %fd23, %fd390;
	bra.uni 	$L__BB42_73;

$L__BB42_68:
	mov.f64 	%fd389, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r290, %temp}, %fd389;
	}
	and.b32  	%r13, %r5, 2147483647;
	setp.eq.s32 	%p100, %r13, 2146435072;
	setp.eq.s32 	%p101, %r290, 0;
	and.pred  	%p102, %p100, %p101;
	@%p102 bra 	$L__BB42_71;
	bra.uni 	$L__BB42_69;

$L__BB42_71:
	abs.f64 	%fd803, %fd23;
	setp.gt.f64 	%p109, %fd803, 0d3FF0000000000000;
	selp.b32 	%r297, 2146435072, 0, %p109;
	mov.u32 	%r298, 0;
	xor.b32  	%r299, %r297, 2146435072;
	setp.lt.s32 	%p110, %r5, 0;
	selp.b32 	%r300, %r299, %r297, %p110;
	setp.eq.f64 	%p111, %fd23, 0dBFF0000000000000;
	selp.b32 	%r301, 1072693248, %r300, %p111;
	mov.b64 	%fd819, {%r298, %r301};
	bra.uni 	$L__BB42_73;

$L__BB42_69:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r397}, %fd23;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r291, %temp}, %fd23;
	}
	and.b32  	%r292, %r397, 2147483647;
	setp.ne.s32 	%p103, %r292, 2146435072;
	setp.ne.s32 	%p104, %r291, 0;
	or.pred  	%p105, %p103, %p104;
	@%p105 bra 	$L__BB42_73;

	and.b32  	%r400, %r5, 2147483647;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r398}, %fd23;
	}
	setp.lt.s32 	%p212, %r398, 0;
	and.pred  	%p211, %p212, %p21;
	setp.gt.s32 	%p106, %r5, -1;
	selp.b32 	%r293, 2146435072, 0, %p106;
	mov.u32 	%r294, 0;
	setp.ne.s32 	%p107, %r400, 1071644672;
	and.pred  	%p108, %p107, %p211;
	or.b32  	%r295, %r293, -2147483648;
	selp.b32 	%r296, %r295, %r293, %p108;
	mov.b64 	%fd819, {%r294, %r296};

$L__BB42_73:
	mul.f64 	%fd391, %fd23, %fd23;
	mul.f64 	%fd392, %fd27, 0d3FE0000000000000;
	mov.f64 	%fd393, 0d3FE0000000000000;
	mul.f64 	%fd394, %fd31, 0d3FE0000000000000;
	mul.f64 	%fd395, %fd23, 0d3FE0000000000000;
	div.rn.f64 	%fd396, %fd819, 0d4008000000000000;
	setp.eq.f64 	%p112, %fd23, 0d3FF0000000000000;
	mov.f64 	%fd397, 0d3FF0000000000000;
	selp.f64 	%fd398, 0d3FD5555555555555, %fd396, %p112;
	mul.f64 	%fd399, %fd842, %fd398;
	mul.f64 	%fd400, %fd391, 0d3FE0000000000000;
	mul.f64 	%fd401, %fd27, %fd400;
	mul.f64 	%fd402, %fd842, %fd401;
	mul.f64 	%fd403, %fd400, %fd31;
	mul.f64 	%fd404, %fd842, %fd403;
	mul.f64 	%fd405, %fd23, %fd27;
	mul.f64 	%fd406, %fd27, %fd405;
	mul.f64 	%fd407, %fd842, %fd406;
	mul.f64 	%fd408, %fd405, %fd31;
	mul.f64 	%fd409, %fd842, %fd408;
	mul.f64 	%fd410, %fd23, %fd31;
	mul.f64 	%fd411, %fd31, %fd410;
	mul.f64 	%fd412, %fd842, %fd411;
	mul.f64 	%fd413, %fd842, %fd83;
	fma.rn.f64 	%fd94, %fd71, %fd413, %fd61;
	div.rn.f64 	%fd414, %fd23, 0d4008000000000000;
	sub.f64 	%fd415, %fd393, %fd414;
	sub.f64 	%fd416, %fd415, %fd392;
	sub.f64 	%fd417, %fd416, %fd394;
	mul.f64 	%fd418, %fd391, %fd417;
	mul.f64 	%fd419, %fd842, %fd418;
	fma.rn.f64 	%fd95, %fd71, %fd419, %fd66;
	mul.f64 	%fd420, %fd27, %fd27;
	div.rn.f64 	%fd421, %fd27, 0d4008000000000000;
	sub.f64 	%fd422, %fd393, %fd421;
	sub.f64 	%fd423, %fd422, %fd395;
	sub.f64 	%fd424, %fd423, %fd394;
	mul.f64 	%fd425, %fd420, %fd424;
	mul.f64 	%fd426, %fd843, %fd425;
	fma.rn.f64 	%fd96, %fd71, %fd426, %fd62;
	mul.f64 	%fd427, %fd31, %fd31;
	div.rn.f64 	%fd428, %fd31, 0d4008000000000000;
	sub.f64 	%fd429, %fd393, %fd428;
	sub.f64 	%fd430, %fd429, %fd395;
	sub.f64 	%fd431, %fd430, %fd392;
	mul.f64 	%fd432, %fd427, %fd431;
	mul.f64 	%fd433, %fd844, %fd432;
	fma.rn.f64 	%fd97, %fd71, %fd433, %fd67;
	fma.rn.f64 	%fd98, %fd71, %fd399, %fd63;
	fma.rn.f64 	%fd99, %fd71, %fd402, %fd68;
	fma.rn.f64 	%fd100, %fd71, %fd404, %fd64;
	fma.rn.f64 	%fd101, %fd71, %fd407, %fd69;
	fma.rn.f64 	%fd102, %fd71, %fd409, %fd65;
	fma.rn.f64 	%fd103, %fd71, %fd412, %fd70;
	sub.f64 	%fd434, %fd397, %fd24;
	sub.f64 	%fd435, %fd434, %fd28;
	sub.f64 	%fd104, %fd435, %fd32;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r14}, %fd104;
	}
	abs.f64 	%fd105, %fd104;
	{ // callseq 59, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd105;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd822, [retval0+0];
	} // callseq 59
	setp.lt.s32 	%p113, %r14, 0;
	and.pred  	%p5, %p113, %p21;
	not.pred 	%p115, %p5;
	@%p115 bra 	$L__BB42_75;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r302}, %fd822;
	}
	xor.b32  	%r303, %r302, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r304, %temp}, %fd822;
	}
	mov.b64 	%fd822, {%r304, %r303};

$L__BB42_75:
	setp.eq.f64 	%p116, %fd104, 0d0000000000000000;
	@%p116 bra 	$L__BB42_79;
	bra.uni 	$L__BB42_76;

$L__BB42_79:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r404}, %fd104;
	}
	selp.b32 	%r305, %r404, 0, %p21;
	mov.u32 	%r306, 0;
	or.b32  	%r307, %r305, 2146435072;
	setp.lt.s32 	%p120, %r5, 0;
	selp.b32 	%r308, %r307, %r305, %p120;
	mov.b64 	%fd822, {%r306, %r308};
	bra.uni 	$L__BB42_80;

$L__BB42_76:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r401}, %fd104;
	}
	setp.gt.s32 	%p117, %r401, -1;
	@%p117 bra 	$L__BB42_80;

	mov.f64 	%fd436, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd437, %fd436;
	setp.eq.f64 	%p118, %fd437, 0d4008000000000000;
	@%p118 bra 	$L__BB42_80;

	mov.f64 	%fd822, 0dFFF8000000000000;

$L__BB42_80:
	add.f64 	%fd439, %fd104, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r309}, %fd439;
	}
	and.b32  	%r310, %r309, 2146435072;
	setp.ne.s32 	%p121, %r310, 2146435072;
	@%p121 bra 	$L__BB42_87;

	abs.f64 	%fd804, %fd104;
	setp.gtu.f64 	%p122, %fd804, 0d7FF0000000000000;
	@%p122 bra 	$L__BB42_86;
	bra.uni 	$L__BB42_82;

$L__BB42_86:
	mov.f64 	%fd441, 0d4008000000000000;
	add.rn.f64 	%fd822, %fd104, %fd441;
	bra.uni 	$L__BB42_87;

$L__BB42_82:
	mov.f64 	%fd440, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r311, %temp}, %fd440;
	}
	and.b32  	%r15, %r5, 2147483647;
	setp.eq.s32 	%p123, %r15, 2146435072;
	setp.eq.s32 	%p124, %r311, 0;
	and.pred  	%p125, %p123, %p124;
	@%p125 bra 	$L__BB42_85;
	bra.uni 	$L__BB42_83;

$L__BB42_85:
	abs.f64 	%fd805, %fd104;
	setp.gt.f64 	%p132, %fd805, 0d3FF0000000000000;
	selp.b32 	%r318, 2146435072, 0, %p132;
	mov.u32 	%r319, 0;
	xor.b32  	%r320, %r318, 2146435072;
	setp.lt.s32 	%p133, %r5, 0;
	selp.b32 	%r321, %r320, %r318, %p133;
	setp.eq.f64 	%p134, %fd104, 0dBFF0000000000000;
	selp.b32 	%r322, 1072693248, %r321, %p134;
	mov.b64 	%fd822, {%r319, %r322};
	bra.uni 	$L__BB42_87;

$L__BB42_83:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r402}, %fd104;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r312, %temp}, %fd104;
	}
	and.b32  	%r313, %r402, 2147483647;
	setp.ne.s32 	%p126, %r313, 2146435072;
	setp.ne.s32 	%p127, %r312, 0;
	or.pred  	%p128, %p126, %p127;
	@%p128 bra 	$L__BB42_87;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r403}, %fd104;
	}
	setp.lt.s32 	%p214, %r403, 0;
	and.pred  	%p213, %p214, %p21;
	setp.gt.s32 	%p129, %r5, -1;
	selp.b32 	%r314, 2146435072, 0, %p129;
	mov.u32 	%r315, 0;
	setp.ne.s32 	%p130, %r15, 1071644672;
	and.pred  	%p131, %p130, %p213;
	or.b32  	%r316, %r314, -2147483648;
	selp.b32 	%r317, %r316, %r314, %p131;
	mov.b64 	%fd822, {%r315, %r317};

$L__BB42_87:
	mov.f64 	%fd442, 0d0000000000000000;
	sub.f64 	%fd443, %fd442, %fd822;
	div.rn.f64 	%fd444, %fd443, 0d4008000000000000;
	setp.eq.f64 	%p135, %fd104, 0d3FF0000000000000;
	selp.f64 	%fd115, 0dBFD5555555555555, %fd444, %p135;
	abs.f64 	%fd116, %fd24;
	{ // callseq 60, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd116;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd825, [retval0+0];
	} // callseq 60
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r16}, %fd24;
	}
	setp.lt.s32 	%p136, %r16, 0;
	and.pred  	%p6, %p136, %p21;
	not.pred 	%p138, %p6;
	@%p138 bra 	$L__BB42_89;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r323}, %fd825;
	}
	xor.b32  	%r324, %r323, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r325, %temp}, %fd825;
	}
	mov.b64 	%fd825, {%r325, %r324};

$L__BB42_89:
	setp.eq.f64 	%p139, %fd24, 0d0000000000000000;
	@%p139 bra 	$L__BB42_93;
	bra.uni 	$L__BB42_90;

$L__BB42_93:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r408}, %fd24;
	}
	selp.b32 	%r326, %r408, 0, %p21;
	mov.u32 	%r327, 0;
	or.b32  	%r328, %r326, 2146435072;
	setp.lt.s32 	%p143, %r5, 0;
	selp.b32 	%r329, %r328, %r326, %p143;
	mov.b64 	%fd825, {%r327, %r329};
	bra.uni 	$L__BB42_94;

$L__BB42_90:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r405}, %fd24;
	}
	setp.gt.s32 	%p140, %r405, -1;
	@%p140 bra 	$L__BB42_94;

	mov.f64 	%fd445, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd446, %fd445;
	setp.eq.f64 	%p141, %fd446, 0d4008000000000000;
	@%p141 bra 	$L__BB42_94;

	mov.f64 	%fd825, 0dFFF8000000000000;

$L__BB42_94:
	add.f64 	%fd448, %fd24, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r330}, %fd448;
	}
	and.b32  	%r331, %r330, 2146435072;
	setp.ne.s32 	%p144, %r331, 2146435072;
	@%p144 bra 	$L__BB42_101;

	abs.f64 	%fd806, %fd24;
	setp.gtu.f64 	%p145, %fd806, 0d7FF0000000000000;
	@%p145 bra 	$L__BB42_100;
	bra.uni 	$L__BB42_96;

$L__BB42_100:
	mov.f64 	%fd450, 0d4008000000000000;
	add.rn.f64 	%fd825, %fd24, %fd450;
	bra.uni 	$L__BB42_101;

$L__BB42_96:
	mov.f64 	%fd449, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r332, %temp}, %fd449;
	}
	and.b32  	%r17, %r5, 2147483647;
	setp.eq.s32 	%p146, %r17, 2146435072;
	setp.eq.s32 	%p147, %r332, 0;
	and.pred  	%p148, %p146, %p147;
	@%p148 bra 	$L__BB42_99;
	bra.uni 	$L__BB42_97;

$L__BB42_99:
	abs.f64 	%fd807, %fd24;
	setp.gt.f64 	%p155, %fd807, 0d3FF0000000000000;
	selp.b32 	%r339, 2146435072, 0, %p155;
	mov.u32 	%r340, 0;
	xor.b32  	%r341, %r339, 2146435072;
	setp.lt.s32 	%p156, %r5, 0;
	selp.b32 	%r342, %r341, %r339, %p156;
	setp.eq.f64 	%p157, %fd24, 0dBFF0000000000000;
	selp.b32 	%r343, 1072693248, %r342, %p157;
	mov.b64 	%fd825, {%r340, %r343};
	bra.uni 	$L__BB42_101;

$L__BB42_97:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r406}, %fd24;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r333, %temp}, %fd24;
	}
	and.b32  	%r334, %r406, 2147483647;
	setp.ne.s32 	%p149, %r334, 2146435072;
	setp.ne.s32 	%p150, %r333, 0;
	or.pred  	%p151, %p149, %p150;
	@%p151 bra 	$L__BB42_101;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r407}, %fd24;
	}
	setp.lt.s32 	%p216, %r407, 0;
	and.pred  	%p215, %p216, %p21;
	setp.gt.s32 	%p152, %r5, -1;
	selp.b32 	%r335, 2146435072, 0, %p152;
	mov.u32 	%r336, 0;
	setp.ne.s32 	%p153, %r17, 1071644672;
	and.pred  	%p154, %p153, %p215;
	or.b32  	%r337, %r335, -2147483648;
	selp.b32 	%r338, %r337, %r335, %p154;
	mov.b64 	%fd825, {%r336, %r338};

$L__BB42_101:
	mul.f64 	%fd451, %fd24, %fd24;
	mul.f64 	%fd452, %fd28, 0d3FE0000000000000;
	mov.f64 	%fd453, 0d3FE0000000000000;
	mul.f64 	%fd454, %fd32, 0d3FE0000000000000;
	mul.f64 	%fd455, %fd24, 0d3FE0000000000000;
	div.rn.f64 	%fd456, %fd825, 0d4008000000000000;
	setp.eq.f64 	%p158, %fd24, 0d3FF0000000000000;
	mov.f64 	%fd457, 0d3FF0000000000000;
	selp.f64 	%fd458, 0d3FD5555555555555, %fd456, %p158;
	mul.f64 	%fd459, %fd842, %fd458;
	mul.f64 	%fd460, %fd451, 0d3FE0000000000000;
	mul.f64 	%fd461, %fd28, %fd460;
	mul.f64 	%fd462, %fd842, %fd461;
	mul.f64 	%fd463, %fd460, %fd32;
	mul.f64 	%fd464, %fd842, %fd463;
	mul.f64 	%fd465, %fd24, %fd28;
	mul.f64 	%fd466, %fd28, %fd465;
	mul.f64 	%fd467, %fd842, %fd466;
	mul.f64 	%fd468, %fd465, %fd32;
	mul.f64 	%fd469, %fd842, %fd468;
	mul.f64 	%fd470, %fd24, %fd32;
	mul.f64 	%fd471, %fd32, %fd470;
	mul.f64 	%fd472, %fd842, %fd471;
	mul.f64 	%fd473, %fd842, %fd115;
	fma.rn.f64 	%fd126, %fd71, %fd473, %fd94;
	div.rn.f64 	%fd474, %fd24, 0d4008000000000000;
	sub.f64 	%fd475, %fd453, %fd474;
	sub.f64 	%fd476, %fd475, %fd452;
	sub.f64 	%fd477, %fd476, %fd454;
	mul.f64 	%fd478, %fd451, %fd477;
	mul.f64 	%fd479, %fd842, %fd478;
	fma.rn.f64 	%fd127, %fd71, %fd479, %fd95;
	mul.f64 	%fd480, %fd28, %fd28;
	div.rn.f64 	%fd481, %fd28, 0d4008000000000000;
	sub.f64 	%fd482, %fd453, %fd481;
	sub.f64 	%fd483, %fd482, %fd455;
	sub.f64 	%fd484, %fd483, %fd454;
	mul.f64 	%fd485, %fd480, %fd484;
	mul.f64 	%fd486, %fd843, %fd485;
	fma.rn.f64 	%fd128, %fd71, %fd486, %fd96;
	mul.f64 	%fd487, %fd32, %fd32;
	div.rn.f64 	%fd488, %fd32, 0d4008000000000000;
	sub.f64 	%fd489, %fd453, %fd488;
	sub.f64 	%fd490, %fd489, %fd455;
	sub.f64 	%fd491, %fd490, %fd452;
	mul.f64 	%fd492, %fd487, %fd491;
	mul.f64 	%fd493, %fd844, %fd492;
	fma.rn.f64 	%fd129, %fd71, %fd493, %fd97;
	fma.rn.f64 	%fd130, %fd71, %fd459, %fd98;
	fma.rn.f64 	%fd131, %fd71, %fd462, %fd99;
	fma.rn.f64 	%fd132, %fd71, %fd464, %fd100;
	fma.rn.f64 	%fd133, %fd71, %fd467, %fd101;
	fma.rn.f64 	%fd134, %fd71, %fd469, %fd102;
	fma.rn.f64 	%fd135, %fd71, %fd472, %fd103;
	sub.f64 	%fd494, %fd457, %fd25;
	sub.f64 	%fd495, %fd494, %fd29;
	sub.f64 	%fd136, %fd495, %fd33;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r18}, %fd136;
	}
	abs.f64 	%fd137, %fd136;
	{ // callseq 61, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd137;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd828, [retval0+0];
	} // callseq 61
	setp.lt.s32 	%p159, %r18, 0;
	and.pred  	%p7, %p159, %p21;
	not.pred 	%p161, %p7;
	@%p161 bra 	$L__BB42_103;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r344}, %fd828;
	}
	xor.b32  	%r345, %r344, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r346, %temp}, %fd828;
	}
	mov.b64 	%fd828, {%r346, %r345};

$L__BB42_103:
	setp.eq.f64 	%p162, %fd136, 0d0000000000000000;
	@%p162 bra 	$L__BB42_107;
	bra.uni 	$L__BB42_104;

$L__BB42_107:
	selp.b32 	%r347, %r18, 0, %p21;
	mov.u32 	%r348, 0;
	or.b32  	%r349, %r347, 2146435072;
	setp.lt.s32 	%p166, %r5, 0;
	selp.b32 	%r350, %r349, %r347, %p166;
	mov.b64 	%fd828, {%r348, %r350};
	bra.uni 	$L__BB42_108;

$L__BB42_104:
	setp.gt.s32 	%p163, %r18, -1;
	@%p163 bra 	$L__BB42_108;

	mov.f64 	%fd496, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd497, %fd496;
	setp.eq.f64 	%p164, %fd497, 0d4008000000000000;
	@%p164 bra 	$L__BB42_108;

	mov.f64 	%fd828, 0dFFF8000000000000;

$L__BB42_108:
	add.f64 	%fd499, %fd136, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r351}, %fd499;
	}
	and.b32  	%r352, %r351, 2146435072;
	setp.ne.s32 	%p167, %r352, 2146435072;
	@%p167 bra 	$L__BB42_115;

	setp.gtu.f64 	%p168, %fd137, 0d7FF0000000000000;
	@%p168 bra 	$L__BB42_114;
	bra.uni 	$L__BB42_110;

$L__BB42_114:
	mov.f64 	%fd501, 0d4008000000000000;
	add.rn.f64 	%fd828, %fd136, %fd501;
	bra.uni 	$L__BB42_115;

$L__BB42_110:
	mov.f64 	%fd500, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r353, %temp}, %fd500;
	}
	and.b32  	%r19, %r5, 2147483647;
	setp.eq.s32 	%p169, %r19, 2146435072;
	setp.eq.s32 	%p170, %r353, 0;
	and.pred  	%p171, %p169, %p170;
	@%p171 bra 	$L__BB42_113;
	bra.uni 	$L__BB42_111;

$L__BB42_113:
	setp.gt.f64 	%p178, %fd137, 0d3FF0000000000000;
	selp.b32 	%r360, 2146435072, 0, %p178;
	mov.u32 	%r361, 0;
	xor.b32  	%r362, %r360, 2146435072;
	setp.lt.s32 	%p179, %r5, 0;
	selp.b32 	%r363, %r362, %r360, %p179;
	setp.eq.f64 	%p180, %fd136, 0dBFF0000000000000;
	selp.b32 	%r364, 1072693248, %r363, %p180;
	mov.b64 	%fd828, {%r361, %r364};
	bra.uni 	$L__BB42_115;

$L__BB42_111:
	{
	.reg .b32 %temp;
	mov.b64 	{%r354, %temp}, %fd136;
	}
	and.b32  	%r355, %r18, 2147483647;
	setp.ne.s32 	%p172, %r355, 2146435072;
	setp.ne.s32 	%p173, %r354, 0;
	or.pred  	%p174, %p172, %p173;
	@%p174 bra 	$L__BB42_115;

	setp.gt.s32 	%p175, %r5, -1;
	selp.b32 	%r356, 2146435072, 0, %p175;
	mov.u32 	%r357, 0;
	setp.ne.s32 	%p176, %r19, 1071644672;
	and.pred  	%p177, %p176, %p7;
	or.b32  	%r358, %r356, -2147483648;
	selp.b32 	%r359, %r358, %r356, %p177;
	mov.b64 	%fd828, {%r357, %r359};

$L__BB42_115:
	mov.f64 	%fd502, 0d0000000000000000;
	sub.f64 	%fd503, %fd502, %fd828;
	div.rn.f64 	%fd504, %fd503, 0d4008000000000000;
	setp.eq.f64 	%p181, %fd136, 0d3FF0000000000000;
	selp.f64 	%fd147, 0dBFD5555555555555, %fd504, %p181;
	abs.f64 	%fd148, %fd25;
	{ // callseq 62, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd148;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd831, [retval0+0];
	} // callseq 62
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r20}, %fd25;
	}
	setp.lt.s32 	%p182, %r20, 0;
	and.pred  	%p8, %p182, %p21;
	not.pred 	%p184, %p8;
	@%p184 bra 	$L__BB42_117;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r365}, %fd831;
	}
	xor.b32  	%r366, %r365, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r367, %temp}, %fd831;
	}
	mov.b64 	%fd831, {%r367, %r366};

$L__BB42_117:
	setp.eq.f64 	%p185, %fd25, 0d0000000000000000;
	@%p185 bra 	$L__BB42_121;
	bra.uni 	$L__BB42_118;

$L__BB42_121:
	selp.b32 	%r368, %r20, 0, %p21;
	mov.u32 	%r369, 0;
	or.b32  	%r370, %r368, 2146435072;
	setp.lt.s32 	%p189, %r5, 0;
	selp.b32 	%r371, %r370, %r368, %p189;
	mov.b64 	%fd831, {%r369, %r371};
	bra.uni 	$L__BB42_122;

$L__BB42_118:
	setp.gt.s32 	%p186, %r20, -1;
	@%p186 bra 	$L__BB42_122;

	mov.f64 	%fd505, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd506, %fd505;
	setp.eq.f64 	%p187, %fd506, 0d4008000000000000;
	@%p187 bra 	$L__BB42_122;

	mov.f64 	%fd831, 0dFFF8000000000000;

$L__BB42_122:
	add.f64 	%fd508, %fd25, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r372}, %fd508;
	}
	and.b32  	%r373, %r372, 2146435072;
	setp.ne.s32 	%p190, %r373, 2146435072;
	@%p190 bra 	$L__BB42_129;

	setp.gtu.f64 	%p191, %fd148, 0d7FF0000000000000;
	@%p191 bra 	$L__BB42_128;
	bra.uni 	$L__BB42_124;

$L__BB42_128:
	mov.f64 	%fd510, 0d4008000000000000;
	add.rn.f64 	%fd831, %fd25, %fd510;
	bra.uni 	$L__BB42_129;

$L__BB42_124:
	mov.f64 	%fd509, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r374, %temp}, %fd509;
	}
	and.b32  	%r21, %r5, 2147483647;
	setp.eq.s32 	%p192, %r21, 2146435072;
	setp.eq.s32 	%p193, %r374, 0;
	and.pred  	%p194, %p192, %p193;
	@%p194 bra 	$L__BB42_127;
	bra.uni 	$L__BB42_125;

$L__BB42_127:
	setp.gt.f64 	%p201, %fd148, 0d3FF0000000000000;
	selp.b32 	%r381, 2146435072, 0, %p201;
	mov.u32 	%r382, 0;
	xor.b32  	%r383, %r381, 2146435072;
	setp.lt.s32 	%p202, %r5, 0;
	selp.b32 	%r384, %r383, %r381, %p202;
	setp.eq.f64 	%p203, %fd25, 0dBFF0000000000000;
	selp.b32 	%r385, 1072693248, %r384, %p203;
	mov.b64 	%fd831, {%r382, %r385};
	bra.uni 	$L__BB42_129;

$L__BB42_125:
	{
	.reg .b32 %temp;
	mov.b64 	{%r375, %temp}, %fd25;
	}
	and.b32  	%r376, %r20, 2147483647;
	setp.ne.s32 	%p195, %r376, 2146435072;
	setp.ne.s32 	%p196, %r375, 0;
	or.pred  	%p197, %p195, %p196;
	@%p197 bra 	$L__BB42_129;

	setp.gt.s32 	%p198, %r5, -1;
	selp.b32 	%r377, 2146435072, 0, %p198;
	mov.u32 	%r378, 0;
	setp.ne.s32 	%p199, %r21, 1071644672;
	and.pred  	%p200, %p199, %p8;
	or.b32  	%r379, %r377, -2147483648;
	selp.b32 	%r380, %r379, %r377, %p200;
	mov.b64 	%fd831, {%r378, %r380};

$L__BB42_129:
	mul.f64 	%fd511, %fd25, %fd25;
	mul.f64 	%fd512, %fd29, 0d3FE0000000000000;
	mov.f64 	%fd513, 0d3FE0000000000000;
	mul.f64 	%fd514, %fd33, 0d3FE0000000000000;
	mul.f64 	%fd515, %fd25, 0d3FE0000000000000;
	div.rn.f64 	%fd516, %fd831, 0d4008000000000000;
	setp.eq.f64 	%p204, %fd25, 0d3FF0000000000000;
	selp.f64 	%fd517, 0d3FD5555555555555, %fd516, %p204;
	mul.f64 	%fd518, %fd842, %fd517;
	mul.f64 	%fd519, %fd511, 0d3FE0000000000000;
	mul.f64 	%fd520, %fd29, %fd519;
	mul.f64 	%fd521, %fd842, %fd520;
	mul.f64 	%fd522, %fd519, %fd33;
	mul.f64 	%fd523, %fd842, %fd522;
	mul.f64 	%fd524, %fd25, %fd29;
	mul.f64 	%fd525, %fd29, %fd524;
	mul.f64 	%fd526, %fd842, %fd525;
	mul.f64 	%fd527, %fd524, %fd33;
	mul.f64 	%fd528, %fd842, %fd527;
	mul.f64 	%fd529, %fd25, %fd33;
	mul.f64 	%fd530, %fd33, %fd529;
	mul.f64 	%fd531, %fd842, %fd530;
	mul.f64 	%fd532, %fd842, %fd147;
	fma.rn.f64 	%fd845, %fd71, %fd532, %fd126;
	div.rn.f64 	%fd533, %fd25, 0d4008000000000000;
	sub.f64 	%fd534, %fd513, %fd533;
	sub.f64 	%fd535, %fd534, %fd512;
	sub.f64 	%fd536, %fd535, %fd514;
	mul.f64 	%fd537, %fd511, %fd536;
	mul.f64 	%fd538, %fd842, %fd537;
	fma.rn.f64 	%fd846, %fd71, %fd538, %fd127;
	mul.f64 	%fd539, %fd29, %fd29;
	div.rn.f64 	%fd540, %fd29, 0d4008000000000000;
	sub.f64 	%fd541, %fd513, %fd540;
	sub.f64 	%fd542, %fd541, %fd515;
	sub.f64 	%fd543, %fd542, %fd514;
	mul.f64 	%fd544, %fd539, %fd543;
	mul.f64 	%fd545, %fd843, %fd544;
	fma.rn.f64 	%fd847, %fd71, %fd545, %fd128;
	mul.f64 	%fd546, %fd33, %fd33;
	div.rn.f64 	%fd547, %fd33, 0d4008000000000000;
	sub.f64 	%fd548, %fd513, %fd547;
	sub.f64 	%fd549, %fd548, %fd515;
	sub.f64 	%fd550, %fd549, %fd512;
	mul.f64 	%fd551, %fd546, %fd550;
	mul.f64 	%fd552, %fd844, %fd551;
	fma.rn.f64 	%fd848, %fd71, %fd552, %fd129;
	fma.rn.f64 	%fd850, %fd71, %fd518, %fd130;
	fma.rn.f64 	%fd851, %fd71, %fd521, %fd131;
	fma.rn.f64 	%fd852, %fd71, %fd523, %fd132;
	fma.rn.f64 	%fd855, %fd71, %fd526, %fd133;
	fma.rn.f64 	%fd856, %fd71, %fd528, %fd134;
	fma.rn.f64 	%fd860, %fd71, %fd531, %fd135;

$L__BB42_130:
	setp.ne.s32 	%p205, %r3, 0;
	mov.f64 	%fd849, %fd846;
	mov.f64 	%fd853, %fd847;
	mov.f64 	%fd854, %fd851;
	mov.f64 	%fd857, %fd848;
	mov.f64 	%fd858, %fd852;
	mov.f64 	%fd859, %fd856;
	@%p205 bra 	$L__BB42_132;

	mul.f64 	%fd797, %fd244, 0d3FE0000000000000;
	mul.f64 	%fd553, %fd4, %fd797;
	mul.f64 	%fd554, %fd843, %fd843;
	fma.rn.f64 	%fd555, %fd842, %fd842, %fd554;
	fma.rn.f64 	%fd556, %fd844, %fd844, %fd555;
	sqrt.rn.f64 	%fd557, %fd556;
	div.rn.f64 	%fd558, %fd842, %fd557;
	div.rn.f64 	%fd559, %fd843, %fd557;
	div.rn.f64 	%fd560, %fd844, %fd557;
	mul.f64 	%fd561, %fd4, %fd558;
	mul.f64 	%fd562, %fd4, %fd559;
	mul.f64 	%fd563, %fd4, %fd560;
	div.rn.f64 	%fd564, %fd561, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd565, %fd562, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd566, %fd563, 0d400BB67AE8584CAA;
	sub.f64 	%fd567, %fd6, %fd1;
	add.f64 	%fd568, %fd567, %fd567;
	div.rn.f64 	%fd569, %fd568, 0d4008000000000000;
	sub.f64 	%fd570, %fd7, %fd1;
	div.rn.f64 	%fd571, %fd570, 0d4018000000000000;
	add.f64 	%fd572, %fd569, %fd571;
	sub.f64 	%fd573, %fd12, %fd1;
	div.rn.f64 	%fd574, %fd573, 0d4018000000000000;
	add.f64 	%fd575, %fd572, %fd574;
	sub.f64 	%fd576, %fd575, %fd564;
	fma.rn.f64 	%fd577, %fd567, 0d4018000000000000, %fd571;
	add.f64 	%fd578, %fd573, %fd573;
	div.rn.f64 	%fd579, %fd578, 0d4008000000000000;
	add.f64 	%fd580, %fd577, %fd579;
	sub.f64 	%fd581, %fd580, %fd564;
	div.rn.f64 	%fd582, %fd567, 0d4018000000000000;
	add.f64 	%fd583, %fd570, %fd570;
	div.rn.f64 	%fd584, %fd583, 0d4008000000000000;
	add.f64 	%fd585, %fd582, %fd584;
	add.f64 	%fd586, %fd574, %fd585;
	sub.f64 	%fd587, %fd586, %fd564;
	add.f64 	%fd588, %fd564, %fd575;
	add.f64 	%fd589, %fd564, %fd580;
	add.f64 	%fd590, %fd564, %fd586;
	sub.f64 	%fd591, %fd8, %fd2;
	add.f64 	%fd592, %fd591, %fd591;
	div.rn.f64 	%fd593, %fd592, 0d4008000000000000;
	sub.f64 	%fd594, %fd9, %fd2;
	div.rn.f64 	%fd595, %fd594, 0d4018000000000000;
	add.f64 	%fd596, %fd593, %fd595;
	sub.f64 	%fd597, %fd13, %fd2;
	div.rn.f64 	%fd598, %fd597, 0d4018000000000000;
	add.f64 	%fd599, %fd596, %fd598;
	sub.f64 	%fd600, %fd599, %fd565;
	fma.rn.f64 	%fd601, %fd591, 0d4018000000000000, %fd595;
	add.f64 	%fd602, %fd597, %fd597;
	div.rn.f64 	%fd603, %fd602, 0d4008000000000000;
	add.f64 	%fd604, %fd601, %fd603;
	sub.f64 	%fd605, %fd604, %fd565;
	div.rn.f64 	%fd606, %fd591, 0d4018000000000000;
	add.f64 	%fd607, %fd594, %fd594;
	div.rn.f64 	%fd608, %fd607, 0d4008000000000000;
	add.f64 	%fd609, %fd606, %fd608;
	add.f64 	%fd610, %fd598, %fd609;
	sub.f64 	%fd611, %fd610, %fd565;
	add.f64 	%fd612, %fd565, %fd599;
	add.f64 	%fd613, %fd565, %fd604;
	add.f64 	%fd614, %fd565, %fd610;
	sub.f64 	%fd615, %fd10, %fd3;
	add.f64 	%fd616, %fd615, %fd615;
	div.rn.f64 	%fd617, %fd616, 0d4008000000000000;
	sub.f64 	%fd618, %fd11, %fd3;
	div.rn.f64 	%fd619, %fd618, 0d4018000000000000;
	add.f64 	%fd620, %fd617, %fd619;
	sub.f64 	%fd621, %fd14, %fd3;
	div.rn.f64 	%fd622, %fd621, 0d4018000000000000;
	add.f64 	%fd623, %fd620, %fd622;
	sub.f64 	%fd624, %fd623, %fd566;
	fma.rn.f64 	%fd625, %fd615, 0d4018000000000000, %fd619;
	add.f64 	%fd626, %fd621, %fd621;
	div.rn.f64 	%fd627, %fd626, 0d4008000000000000;
	add.f64 	%fd628, %fd625, %fd627;
	sub.f64 	%fd629, %fd628, %fd566;
	div.rn.f64 	%fd630, %fd615, 0d4018000000000000;
	add.f64 	%fd631, %fd618, %fd618;
	div.rn.f64 	%fd632, %fd631, 0d4008000000000000;
	add.f64 	%fd633, %fd630, %fd632;
	add.f64 	%fd634, %fd622, %fd633;
	sub.f64 	%fd635, %fd634, %fd566;
	add.f64 	%fd636, %fd566, %fd623;
	add.f64 	%fd637, %fd566, %fd628;
	add.f64 	%fd638, %fd566, %fd634;
	mul.f64 	%fd639, %fd5, %fd553;
	div.rn.f64 	%fd640, %fd639, 0d4018000000000000;
	mov.f64 	%fd641, 0d3FF0000000000000;
	sub.f64 	%fd642, %fd641, %fd576;
	sub.f64 	%fd643, %fd642, %fd600;
	sub.f64 	%fd644, %fd643, %fd624;
	mul.f64 	%fd645, %fd640, %fd644;
	fma.rn.f64 	%fd646, %fd644, %fd645, %fd845;
	mul.f64 	%fd647, %fd640, %fd576;
	fma.rn.f64 	%fd648, %fd647, %fd644, %fd846;
	mul.f64 	%fd649, %fd640, %fd600;
	fma.rn.f64 	%fd650, %fd649, %fd644, %fd847;
	mul.f64 	%fd651, %fd640, %fd624;
	fma.rn.f64 	%fd652, %fd651, %fd644, %fd848;
	fma.rn.f64 	%fd653, %fd576, %fd645, %fd846;
	fma.rn.f64 	%fd654, %fd576, %fd647, %fd850;
	fma.rn.f64 	%fd655, %fd576, %fd649, %fd851;
	fma.rn.f64 	%fd656, %fd576, %fd651, %fd852;
	fma.rn.f64 	%fd657, %fd600, %fd645, %fd847;
	fma.rn.f64 	%fd658, %fd647, %fd600, %fd851;
	fma.rn.f64 	%fd659, %fd600, %fd649, %fd855;
	fma.rn.f64 	%fd660, %fd600, %fd651, %fd856;
	fma.rn.f64 	%fd661, %fd624, %fd645, %fd848;
	fma.rn.f64 	%fd662, %fd647, %fd624, %fd852;
	fma.rn.f64 	%fd663, %fd649, %fd624, %fd856;
	fma.rn.f64 	%fd664, %fd624, %fd651, %fd860;
	sub.f64 	%fd665, %fd641, %fd581;
	sub.f64 	%fd666, %fd665, %fd605;
	sub.f64 	%fd667, %fd666, %fd629;
	mul.f64 	%fd668, %fd640, %fd667;
	fma.rn.f64 	%fd669, %fd667, %fd668, %fd646;
	mul.f64 	%fd670, %fd640, %fd581;
	fma.rn.f64 	%fd671, %fd670, %fd667, %fd648;
	mul.f64 	%fd672, %fd640, %fd605;
	fma.rn.f64 	%fd673, %fd672, %fd667, %fd650;
	mul.f64 	%fd674, %fd640, %fd629;
	fma.rn.f64 	%fd675, %fd674, %fd667, %fd652;
	fma.rn.f64 	%fd676, %fd581, %fd668, %fd653;
	fma.rn.f64 	%fd677, %fd581, %fd670, %fd654;
	fma.rn.f64 	%fd678, %fd581, %fd672, %fd655;
	fma.rn.f64 	%fd679, %fd581, %fd674, %fd656;
	fma.rn.f64 	%fd680, %fd605, %fd668, %fd657;
	fma.rn.f64 	%fd681, %fd670, %fd605, %fd658;
	fma.rn.f64 	%fd682, %fd605, %fd672, %fd659;
	fma.rn.f64 	%fd683, %fd605, %fd674, %fd660;
	fma.rn.f64 	%fd684, %fd629, %fd668, %fd661;
	fma.rn.f64 	%fd685, %fd670, %fd629, %fd662;
	fma.rn.f64 	%fd686, %fd672, %fd629, %fd663;
	fma.rn.f64 	%fd687, %fd629, %fd674, %fd664;
	sub.f64 	%fd688, %fd641, %fd587;
	sub.f64 	%fd689, %fd688, %fd611;
	sub.f64 	%fd690, %fd689, %fd635;
	mul.f64 	%fd691, %fd640, %fd690;
	fma.rn.f64 	%fd692, %fd690, %fd691, %fd669;
	mul.f64 	%fd693, %fd640, %fd587;
	fma.rn.f64 	%fd694, %fd693, %fd690, %fd671;
	mul.f64 	%fd695, %fd640, %fd611;
	fma.rn.f64 	%fd696, %fd695, %fd690, %fd673;
	mul.f64 	%fd697, %fd640, %fd635;
	fma.rn.f64 	%fd698, %fd697, %fd690, %fd675;
	fma.rn.f64 	%fd699, %fd587, %fd691, %fd676;
	fma.rn.f64 	%fd700, %fd587, %fd693, %fd677;
	fma.rn.f64 	%fd701, %fd587, %fd695, %fd678;
	fma.rn.f64 	%fd702, %fd587, %fd697, %fd679;
	fma.rn.f64 	%fd703, %fd611, %fd691, %fd680;
	fma.rn.f64 	%fd704, %fd693, %fd611, %fd681;
	fma.rn.f64 	%fd705, %fd611, %fd695, %fd682;
	fma.rn.f64 	%fd706, %fd611, %fd697, %fd683;
	fma.rn.f64 	%fd707, %fd635, %fd691, %fd684;
	fma.rn.f64 	%fd708, %fd693, %fd635, %fd685;
	fma.rn.f64 	%fd709, %fd695, %fd635, %fd686;
	fma.rn.f64 	%fd710, %fd635, %fd697, %fd687;
	sub.f64 	%fd711, %fd641, %fd588;
	sub.f64 	%fd712, %fd711, %fd612;
	sub.f64 	%fd713, %fd712, %fd636;
	mul.f64 	%fd714, %fd640, %fd713;
	fma.rn.f64 	%fd715, %fd713, %fd714, %fd692;
	mul.f64 	%fd716, %fd640, %fd588;
	fma.rn.f64 	%fd717, %fd716, %fd713, %fd694;
	mul.f64 	%fd718, %fd640, %fd612;
	fma.rn.f64 	%fd719, %fd718, %fd713, %fd696;
	mul.f64 	%fd720, %fd640, %fd636;
	fma.rn.f64 	%fd721, %fd720, %fd713, %fd698;
	fma.rn.f64 	%fd722, %fd588, %fd714, %fd699;
	fma.rn.f64 	%fd723, %fd588, %fd716, %fd700;
	fma.rn.f64 	%fd724, %fd588, %fd718, %fd701;
	fma.rn.f64 	%fd725, %fd588, %fd720, %fd702;
	fma.rn.f64 	%fd726, %fd612, %fd714, %fd703;
	fma.rn.f64 	%fd727, %fd716, %fd612, %fd704;
	fma.rn.f64 	%fd728, %fd612, %fd718, %fd705;
	fma.rn.f64 	%fd729, %fd612, %fd720, %fd706;
	fma.rn.f64 	%fd730, %fd636, %fd714, %fd707;
	fma.rn.f64 	%fd731, %fd716, %fd636, %fd708;
	fma.rn.f64 	%fd732, %fd718, %fd636, %fd709;
	fma.rn.f64 	%fd733, %fd636, %fd720, %fd710;
	sub.f64 	%fd734, %fd641, %fd589;
	sub.f64 	%fd735, %fd734, %fd613;
	sub.f64 	%fd736, %fd735, %fd637;
	mul.f64 	%fd737, %fd640, %fd736;
	fma.rn.f64 	%fd738, %fd736, %fd737, %fd715;
	mul.f64 	%fd739, %fd640, %fd589;
	fma.rn.f64 	%fd740, %fd739, %fd736, %fd717;
	mul.f64 	%fd741, %fd640, %fd613;
	fma.rn.f64 	%fd742, %fd741, %fd736, %fd719;
	mul.f64 	%fd743, %fd640, %fd637;
	fma.rn.f64 	%fd744, %fd743, %fd736, %fd721;
	fma.rn.f64 	%fd745, %fd589, %fd737, %fd722;
	fma.rn.f64 	%fd746, %fd589, %fd739, %fd723;
	fma.rn.f64 	%fd747, %fd589, %fd741, %fd724;
	fma.rn.f64 	%fd748, %fd589, %fd743, %fd725;
	fma.rn.f64 	%fd749, %fd613, %fd737, %fd726;
	fma.rn.f64 	%fd750, %fd739, %fd613, %fd727;
	fma.rn.f64 	%fd751, %fd613, %fd741, %fd728;
	fma.rn.f64 	%fd752, %fd613, %fd743, %fd729;
	fma.rn.f64 	%fd753, %fd637, %fd737, %fd730;
	fma.rn.f64 	%fd754, %fd739, %fd637, %fd731;
	fma.rn.f64 	%fd755, %fd741, %fd637, %fd732;
	fma.rn.f64 	%fd756, %fd637, %fd743, %fd733;
	sub.f64 	%fd757, %fd641, %fd590;
	sub.f64 	%fd758, %fd757, %fd614;
	sub.f64 	%fd759, %fd758, %fd638;
	mul.f64 	%fd760, %fd640, %fd759;
	fma.rn.f64 	%fd845, %fd759, %fd760, %fd738;
	mul.f64 	%fd761, %fd640, %fd590;
	fma.rn.f64 	%fd846, %fd761, %fd759, %fd740;
	mul.f64 	%fd762, %fd640, %fd614;
	fma.rn.f64 	%fd847, %fd762, %fd759, %fd742;
	mul.f64 	%fd763, %fd640, %fd638;
	fma.rn.f64 	%fd848, %fd763, %fd759, %fd744;
	fma.rn.f64 	%fd849, %fd590, %fd760, %fd745;
	fma.rn.f64 	%fd850, %fd590, %fd761, %fd746;
	fma.rn.f64 	%fd851, %fd590, %fd762, %fd747;
	fma.rn.f64 	%fd852, %fd590, %fd763, %fd748;
	fma.rn.f64 	%fd853, %fd614, %fd760, %fd749;
	fma.rn.f64 	%fd854, %fd761, %fd614, %fd750;
	fma.rn.f64 	%fd855, %fd614, %fd762, %fd751;
	fma.rn.f64 	%fd856, %fd614, %fd763, %fd752;
	fma.rn.f64 	%fd857, %fd638, %fd760, %fd753;
	fma.rn.f64 	%fd858, %fd761, %fd638, %fd754;
	fma.rn.f64 	%fd859, %fd762, %fd638, %fd755;
	fma.rn.f64 	%fd860, %fd638, %fd763, %fd756;

$L__BB42_132:
	ld.param.u64 	%rd113, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_11];
	mul.lo.s64 	%rd112, %rd39, %rd27;
	add.s64 	%rd96, %rd113, %rd112;
	// begin inline asm
	{ atom.add.f64 %fd764,[%rd96],%fd845; }

	// end inline asm
	add.s64 	%rd97, %rd96, 8;
	// begin inline asm
	{ atom.add.f64 %fd766,[%rd97],%fd846; }

	// end inline asm
	add.s64 	%rd98, %rd96, 16;
	// begin inline asm
	{ atom.add.f64 %fd768,[%rd98],%fd847; }

	// end inline asm
	add.s64 	%rd99, %rd96, 24;
	// begin inline asm
	{ atom.add.f64 %fd770,[%rd99],%fd848; }

	// end inline asm
	add.s64 	%rd100, %rd96, 32;
	// begin inline asm
	{ atom.add.f64 %fd772,[%rd100],%fd849; }

	// end inline asm
	add.s64 	%rd101, %rd96, 40;
	// begin inline asm
	{ atom.add.f64 %fd774,[%rd101],%fd850; }

	// end inline asm
	add.s64 	%rd102, %rd96, 48;
	// begin inline asm
	{ atom.add.f64 %fd776,[%rd102],%fd851; }

	// end inline asm
	add.s64 	%rd103, %rd96, 56;
	// begin inline asm
	{ atom.add.f64 %fd778,[%rd103],%fd852; }

	// end inline asm
	add.s64 	%rd104, %rd96, 64;
	// begin inline asm
	{ atom.add.f64 %fd780,[%rd104],%fd853; }

	// end inline asm
	add.s64 	%rd105, %rd96, 72;
	// begin inline asm
	{ atom.add.f64 %fd782,[%rd105],%fd854; }

	// end inline asm
	add.s64 	%rd106, %rd96, 80;
	// begin inline asm
	{ atom.add.f64 %fd784,[%rd106],%fd855; }

	// end inline asm
	add.s64 	%rd107, %rd96, 88;
	// begin inline asm
	{ atom.add.f64 %fd786,[%rd107],%fd856; }

	// end inline asm
	add.s64 	%rd108, %rd96, 96;
	// begin inline asm
	{ atom.add.f64 %fd788,[%rd108],%fd857; }

	// end inline asm
	add.s64 	%rd109, %rd96, 104;
	// begin inline asm
	{ atom.add.f64 %fd790,[%rd109],%fd858; }

	// end inline asm
	add.s64 	%rd110, %rd96, 112;
	// begin inline asm
	{ atom.add.f64 %fd792,[%rd110],%fd859; }

	// end inline asm
	add.s64 	%rd111, %rd96, 120;
	// begin inline asm
	{ atom.add.f64 %fd794,[%rd111],%fd860; }

	// end inline asm

$L__BB42_133:
	ld.param.u64 	%rd114, [init_affine_mass_matrix_kernel_cuda_kernel_forward_param_0+24];
	add.s64 	%rd115, %rd115, %rd19;
	setp.lt.u64 	%p206, %rd115, %rd114;
	@%p206 bra 	$L__BB42_2;

$L__BB42_134:
	ret;

}
	// .globl	init_affine_mass_matrix_kernel_cuda_kernel_backward
.visible .entry init_affine_mass_matrix_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0[32],
	.param .u32 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_1,
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_11[56],
	.param .u32 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_12,
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_21[56],
	.param .align 8 .b8 init_affine_mass_matrix_kernel_cuda_kernel_backward_param_22[56]
)
{
	.reg .pred 	%p<420>;
	.reg .b16 	%rs<121>;
	.reg .b32 	%r<707>;
	.reg .f64 	%fd<5660>;
	.reg .b64 	%rd<183>;


	ld.param.v2.u32 	{%r207, %r208}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r209, %r210}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r215, %r216}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r223, %r224}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r231, %r232}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r239, %r240}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r247, %r248}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r255, %r256}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r263, %r264}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r271, %r272}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r279, %r280}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r287, %r288}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r295, %r296}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r303, %r304}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r311, %r312}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r319, %r320}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_21+32];
	ld.param.v2.u32 	{%r327, %r328}, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_22+32];
	ld.param.u64 	%rd82, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_22];
	ld.param.u64 	%rd80, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_21];
	ld.param.u64 	%rd78, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd76, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd74, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd73, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_11+8];
	ld.param.u64 	%rd71, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd70, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd69, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd68, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd67, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8+8];
	ld.param.u64 	%rd66, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd64, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd62, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd60, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd58, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd56, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd55, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd54, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd53, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r70, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r331, %ntid.x;
	mov.u32 	%r332, %ctaid.x;
	mul.wide.u32 	%rd84, %r331, %r332;
	mov.u32 	%r333, %tid.x;
	cvt.u64.u32 	%rd85, %r333;
	add.s64 	%rd179, %rd84, %rd85;
	setp.ge.u64 	%p17, %rd179, %rd53;
	@%p17 bra 	$L__BB43_281;

	cvta.to.global.u64 	%rd13, %rd70;
	cvta.to.global.u64 	%rd14, %rd68;
	cvta.to.global.u64 	%rd15, %rd66;
	cvta.to.global.u64 	%rd16, %rd64;
	cvta.to.global.u64 	%rd17, %rd62;
	cvta.to.global.u64 	%rd18, %rd60;
	cvta.to.global.u64 	%rd19, %rd58;
	cvta.to.global.u64 	%rd20, %rd56;
	cvta.to.global.u64 	%rd21, %rd54;
	cvt.s64.s32 	%rd22, %r210;
	cvt.s64.s32 	%rd23, %r209;
	cvt.s64.s32 	%rd24, %r208;
	cvt.s64.s32 	%rd25, %r231;
	cvt.s64.s32 	%rd26, %r239;
	cvt.s64.s32 	%rd27, %r247;
	cvt.s64.s32 	%rd28, %r255;
	cvt.s64.s32 	%rd29, %r279;
	cvt.s64.s32 	%rd30, %r263;
	cvt.s64.s32 	%rd31, %r271;
	cvt.s64.s32 	%rd32, %r223;
	cvt.s64.s32 	%rd33, %r215;
	cvt.s64.s32 	%rd34, %r327;
	cvt.s64.s32 	%rd35, %r287;
	cvt.s64.s32 	%rd36, %r295;
	cvt.s64.s32 	%rd37, %r311;
	cvt.s64.s32 	%rd38, %r303;
	cvt.s64.s32 	%rd39, %r319;

$L__BB43_2:
	setp.lt.s32 	%p18, %r70, 4;
	mov.u64 	%rd180, %rd179;
	@%p18 bra 	$L__BB43_6;

	or.b64  	%rd86, %rd179, %rd22;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.eq.s64 	%p19, %rd87, 0;
	@%p19 bra 	$L__BB43_5;

	div.u64 	%rd180, %rd179, %rd22;
	bra.uni 	$L__BB43_6;

$L__BB43_5:
	cvt.u32.u64 	%r335, %rd22;
	cvt.u32.u64 	%r336, %rd179;
	div.u32 	%r337, %r336, %r335;
	cvt.u64.u32 	%rd180, %r337;

$L__BB43_6:
	setp.lt.s32 	%p20, %r70, 3;
	@%p20 bra 	$L__BB43_10;

	or.b64  	%rd88, %rd180, %rd23;
	and.b64  	%rd89, %rd88, -4294967296;
	setp.eq.s64 	%p21, %rd89, 0;
	@%p21 bra 	$L__BB43_9;

	div.u64 	%rd180, %rd180, %rd23;
	bra.uni 	$L__BB43_10;

$L__BB43_9:
	cvt.u32.u64 	%r338, %rd23;
	cvt.u32.u64 	%r339, %rd180;
	div.u32 	%r340, %r339, %r338;
	cvt.u64.u32 	%rd180, %r340;

$L__BB43_10:
	setp.lt.s32 	%p22, %r70, 2;
	@%p22 bra 	$L__BB43_14;

	or.b64  	%rd90, %rd180, %rd24;
	and.b64  	%rd91, %rd90, -4294967296;
	setp.eq.s64 	%p23, %rd91, 0;
	@%p23 bra 	$L__BB43_13;

	div.u64 	%rd180, %rd180, %rd24;
	bra.uni 	$L__BB43_14;

$L__BB43_13:
	cvt.u32.u64 	%r341, %rd24;
	cvt.u32.u64 	%r342, %rd180;
	div.u32 	%r343, %r342, %r341;
	cvt.u64.u32 	%rd180, %r343;

$L__BB43_14:
	ld.param.u32 	%r686, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_1];
	cvt.u32.u64 	%r344, %rd180;
	setp.gt.s32 	%p24, %r70, 0;
	selp.b32 	%r12, %r344, 0, %p24;
	setp.ge.s32 	%p25, %r12, %r686;
	mov.f64 	%fd5011, 0d0000000000000000;
	mov.f64 	%fd5012, 0d0000000000000000;
	mov.f64 	%fd5013, 0d0000000000000000;
	mov.f64 	%fd5015, 0d0000000000000000;
	mov.f64 	%fd5016, 0d0000000000000000;
	mov.f64 	%fd5017, 0d0000000000000000;
	mov.f64 	%fd5019, 0d0000000000000000;
	mov.f64 	%fd5020, 0d0000000000000000;
	mov.f64 	%fd5021, 0d0000000000000000;
	mov.f64 	%fd5022, 0d0000000000000000;
	mov.f64 	%fd5023, 0d0000000000000000;
	mov.f64 	%fd5024, 0d0000000000000000;
	mov.f64 	%fd5025, 0d0000000000000000;
	mov.f64 	%fd5026, 0d0000000000000000;
	mov.f64 	%fd5027, 0d0000000000000000;
	mov.f64 	%fd5028, 0d0000000000000000;
	mov.f64 	%fd5029, 0d0000000000000000;
	mov.f64 	%fd5030, 0d0000000000000000;
	mov.f64 	%fd5031, 0d0000000000000000;
	mov.f64 	%fd5032, 0d0000000000000000;
	mov.f64 	%fd5033, 0d0000000000000000;
	mov.f64 	%fd5034, 0d0000000000000000;
	mov.f64 	%fd5035, 0d0000000000000000;
	mov.f64 	%fd5036, 0d0000000000000000;
	mov.f64 	%fd5037, 0d0000000000000000;
	mov.f64 	%fd5038, 0d0000000000000000;
	mov.f64 	%fd5039, 0d0000000000000000;
	mov.f64 	%fd5040, 0d0000000000000000;
	mov.f64 	%fd5041, 0d0000000000000000;
	mov.f64 	%fd5042, 0d0000000000000000;
	@%p25 bra 	$L__BB43_132;

	cvt.s64.s32 	%rd50, %r12;
	mul.lo.s64 	%rd92, %rd50, %rd25;
	add.s64 	%rd93, %rd19, %rd92;
	ld.global.u32 	%r13, [%rd93];
	cvt.s64.s32 	%rd51, %r13;
	mul.lo.s64 	%rd94, %rd51, %rd26;
	add.s64 	%rd95, %rd18, %rd94;
	ld.global.u32 	%r14, [%rd95];
	setp.eq.s32 	%p26, %r14, 0;
	mov.u32 	%r706, 0;
	@%p26 bra 	$L__BB43_132;

	mul.lo.s64 	%rd96, %rd51, %rd27;
	add.s64 	%rd97, %rd17, %rd96;
	mul.lo.s64 	%rd98, %rd51, %rd28;
	add.s64 	%rd99, %rd16, %rd98;
	ld.global.u32 	%r697, [%rd99];
	mul.lo.s64 	%rd100, %rd51, %rd29;
	add.s64 	%rd101, %rd13, %rd100;
	ld.global.f64 	%fd557, [%rd101];
	ld.global.f64 	%fd558, [%rd101+8];
	ld.global.f64 	%fd559, [%rd101+16];
	mul.lo.s64 	%rd102, %rd51, %rd30;
	add.s64 	%rd103, %rd15, %rd102;
	ld.global.f64 	%fd5043, [%rd103];
	mul.lo.s64 	%rd104, %rd51, %rd31;
	add.s64 	%rd105, %rd14, %rd104;
	ld.global.f64 	%fd5044, [%rd105];
	mul.lo.s64 	%rd106, %rd50, %rd32;
	add.s64 	%rd107, %rd20, %rd106;
	ld.global.u32 	%r702, [%rd107];
	cvt.s64.s32 	%rd108, %r702;
	mul.lo.s64 	%rd109, %rd108, %rd33;
	add.s64 	%rd110, %rd21, %rd109;
	ld.global.u32 	%r703, [%rd107+4];
	cvt.s64.s32 	%rd111, %r703;
	mul.lo.s64 	%rd112, %rd111, %rd33;
	add.s64 	%rd113, %rd21, %rd112;
	ld.global.u32 	%r704, [%rd107+8];
	cvt.s64.s32 	%rd114, %r704;
	mul.lo.s64 	%rd115, %rd114, %rd33;
	add.s64 	%rd116, %rd21, %rd115;
	ld.global.f64 	%fd562, [%rd110];
	ld.global.f64 	%fd563, [%rd113];
	sub.f64 	%fd5040, %fd563, %fd562;
	ld.global.f64 	%fd565, [%rd110+8];
	ld.global.f64 	%fd566, [%rd113+8];
	sub.f64 	%fd5041, %fd566, %fd565;
	ld.global.f64 	%fd568, [%rd110+16];
	ld.global.f64 	%fd569, [%rd113+16];
	sub.f64 	%fd5042, %fd569, %fd568;
	ld.global.f64 	%fd571, [%rd116];
	sub.f64 	%fd5037, %fd571, %fd562;
	ld.global.f64 	%fd573, [%rd116+8];
	sub.f64 	%fd5038, %fd573, %fd565;
	ld.global.f64 	%fd575, [%rd116+16];
	sub.f64 	%fd5039, %fd575, %fd568;
	mul.f64 	%fd2107, %fd5041, %fd5039;
	mul.f64 	%fd2108, %fd5042, %fd5038;
	sub.f64 	%fd5034, %fd2107, %fd2108;
	mul.f64 	%fd2109, %fd5042, %fd5037;
	mul.f64 	%fd2110, %fd5040, %fd5039;
	sub.f64 	%fd5035, %fd2109, %fd2110;
	mul.f64 	%fd2111, %fd5040, %fd5038;
	mul.f64 	%fd2112, %fd5041, %fd5037;
	sub.f64 	%fd5036, %fd2111, %fd2112;
	mul.f64 	%fd2113, %fd5035, %fd5035;
	fma.rn.f64 	%fd2114, %fd5034, %fd5034, %fd2113;
	fma.rn.f64 	%fd2115, %fd5036, %fd5036, %fd2114;
	sqrt.rn.f64 	%fd5045, %fd2115;
	mul.f64 	%fd5046, %fd5045, 0d3FE0000000000000;
	ld.global.u32 	%r698, [%rd97];
	setp.eq.s32 	%p27, %r698, 0;
	mov.f64 	%fd5033, 0d0000000000000000;
	mov.f64 	%fd5032, %fd5033;
	mov.f64 	%fd5031, %fd5033;
	mov.f64 	%fd5030, %fd5033;
	mov.f64 	%fd5029, %fd5033;
	mov.f64 	%fd5028, %fd5033;
	mov.f64 	%fd5027, %fd5033;
	mov.f64 	%fd5026, %fd5033;
	mov.f64 	%fd5025, %fd5033;
	mov.f64 	%fd5024, %fd5033;
	mov.f64 	%fd5023, %fd5033;
	mov.f64 	%fd5022, %fd5033;
	mov.f64 	%fd5021, %fd5033;
	mov.f64 	%fd5020, %fd5033;
	mov.f64 	%fd5019, %fd5033;
	mov.f64 	%fd5015, %fd5034;
	mov.f64 	%fd5016, %fd5035;
	mov.f64 	%fd5017, %fd5036;
	@%p27 bra 	$L__BB43_130;

	neg.f64 	%fd2116, %fd5034;
	setp.eq.s32 	%p28, %r697, 0;
	neg.f64 	%fd2117, %fd5035;
	neg.f64 	%fd2118, %fd5036;
	selp.f64 	%fd2119, 0d0000000000000000, %fd2116, %p28;
	selp.f64 	%fd2120, 0d0000000000000000, %fd2117, %p28;
	selp.f64 	%fd2121, 0d0000000000000000, %fd2118, %p28;
	sub.f64 	%fd2122, %fd562, %fd557;
	div.rn.f64 	%fd2123, %fd2122, 0d4008000000000000;
	mov.f64 	%fd2124, 0d4008000000000000;
	sub.f64 	%fd2125, %fd563, %fd557;
	div.rn.f64 	%fd2126, %fd2125, 0d4008000000000000;
	add.f64 	%fd2127, %fd2123, %fd2126;
	sub.f64 	%fd2128, %fd571, %fd557;
	div.rn.f64 	%fd2129, %fd2128, 0d4008000000000000;
	add.f64 	%fd5053, %fd2127, %fd2129;
	mul.f64 	%fd2130, %fd2122, 0d4008000000000000;
	div.rn.f64 	%fd2131, %fd2130, 0d4014000000000000;
	div.rn.f64 	%fd2132, %fd2125, 0d4014000000000000;
	add.f64 	%fd2133, %fd2131, %fd2132;
	div.rn.f64 	%fd2134, %fd2128, 0d4014000000000000;
	add.f64 	%fd5118, %fd2133, %fd2134;
	div.rn.f64 	%fd2135, %fd2122, 0d4014000000000000;
	add.f64 	%fd2136, %fd2135, %fd2132;
	mul.f64 	%fd2137, %fd2128, 0d4008000000000000;
	div.rn.f64 	%fd2138, %fd2137, 0d4014000000000000;
	add.f64 	%fd5183, %fd2136, %fd2138;
	mul.f64 	%fd2139, %fd2125, 0d4008000000000000;
	div.rn.f64 	%fd2140, %fd2139, 0d4014000000000000;
	add.f64 	%fd2141, %fd2135, %fd2140;
	add.f64 	%fd5247, %fd2134, %fd2141;
	sub.f64 	%fd2142, %fd565, %fd558;
	div.rn.f64 	%fd2143, %fd2142, 0d4008000000000000;
	sub.f64 	%fd2144, %fd566, %fd558;
	div.rn.f64 	%fd2145, %fd2144, 0d4008000000000000;
	add.f64 	%fd2146, %fd2143, %fd2145;
	sub.f64 	%fd2147, %fd573, %fd558;
	div.rn.f64 	%fd2148, %fd2147, 0d4008000000000000;
	add.f64 	%fd5060, %fd2146, %fd2148;
	mul.f64 	%fd2149, %fd2142, 0d4008000000000000;
	div.rn.f64 	%fd2150, %fd2149, 0d4014000000000000;
	div.rn.f64 	%fd2151, %fd2144, 0d4014000000000000;
	add.f64 	%fd2152, %fd2150, %fd2151;
	div.rn.f64 	%fd2153, %fd2147, 0d4014000000000000;
	add.f64 	%fd5125, %fd2152, %fd2153;
	div.rn.f64 	%fd2154, %fd2142, 0d4014000000000000;
	add.f64 	%fd2155, %fd2154, %fd2151;
	mul.f64 	%fd2156, %fd2147, 0d4008000000000000;
	div.rn.f64 	%fd2157, %fd2156, 0d4014000000000000;
	add.f64 	%fd5190, %fd2155, %fd2157;
	mul.f64 	%fd2158, %fd2144, 0d4008000000000000;
	div.rn.f64 	%fd2159, %fd2158, 0d4014000000000000;
	add.f64 	%fd2160, %fd2154, %fd2159;
	add.f64 	%fd5254, %fd2153, %fd2160;
	sub.f64 	%fd2161, %fd568, %fd559;
	div.rn.f64 	%fd2162, %fd2161, 0d4008000000000000;
	sub.f64 	%fd2163, %fd569, %fd559;
	div.rn.f64 	%fd2164, %fd2163, 0d4008000000000000;
	add.f64 	%fd2165, %fd2162, %fd2164;
	sub.f64 	%fd2166, %fd575, %fd559;
	div.rn.f64 	%fd2167, %fd2166, 0d4008000000000000;
	add.f64 	%fd5067, %fd2165, %fd2167;
	mul.f64 	%fd2168, %fd2161, 0d4008000000000000;
	div.rn.f64 	%fd2169, %fd2168, 0d4014000000000000;
	div.rn.f64 	%fd2170, %fd2163, 0d4014000000000000;
	add.f64 	%fd2171, %fd2169, %fd2170;
	div.rn.f64 	%fd2172, %fd2166, 0d4014000000000000;
	add.f64 	%fd5132, %fd2171, %fd2172;
	div.rn.f64 	%fd2173, %fd2161, 0d4014000000000000;
	add.f64 	%fd2174, %fd2173, %fd2170;
	mul.f64 	%fd2175, %fd2166, 0d4008000000000000;
	div.rn.f64 	%fd2176, %fd2175, 0d4014000000000000;
	add.f64 	%fd5197, %fd2174, %fd2176;
	mul.f64 	%fd2177, %fd2163, 0d4008000000000000;
	div.rn.f64 	%fd2178, %fd2177, 0d4014000000000000;
	add.f64 	%fd2179, %fd2173, %fd2178;
	add.f64 	%fd5261, %fd2172, %fd2179;
	add.f64 	%fd5047, %fd5046, %fd5046;
	selp.f64 	%fd5021, %fd5036, %fd2121, %p28;
	selp.f64 	%fd5020, %fd5035, %fd2120, %p28;
	selp.f64 	%fd5019, %fd5034, %fd2119, %p28;
	div.rn.f64 	%fd5051, %fd5019, %fd5047;
	div.rn.f64 	%fd5065, %fd5020, %fd5047;
	div.rn.f64 	%fd5072, %fd5021, %fd5047;
	mov.f64 	%fd2180, 0d3FF0000000000000;
	sub.f64 	%fd2181, %fd2180, %fd5053;
	sub.f64 	%fd2182, %fd2181, %fd5060;
	sub.f64 	%fd5049, %fd2182, %fd5067;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r20}, %fd5049;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r21}, %fd2124;
	}
	and.b32  	%r22, %r21, 2146435072;
	setp.eq.s32 	%p29, %r22, 1073741824;
	abs.f64 	%fd605, %fd5049;
	{ // callseq 63, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd605;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4709, [retval0+0];
	} // callseq 63
	setp.lt.s32 	%p30, %r20, 0;
	and.pred  	%p1, %p30, %p29;
	not.pred 	%p31, %p1;
	@%p31 bra 	$L__BB43_19;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r346}, %fd4709;
	}
	xor.b32  	%r347, %r346, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r348, %temp}, %fd4709;
	}
	mov.b64 	%fd4709, {%r348, %r347};

$L__BB43_19:
	setp.eq.f64 	%p32, %fd5049, 0d0000000000000000;
	@%p32 bra 	$L__BB43_23;
	bra.uni 	$L__BB43_20;

$L__BB43_23:
	selp.b32 	%r349, %r20, 0, %p29;
	mov.u32 	%r350, 0;
	or.b32  	%r351, %r349, 2146435072;
	setp.lt.s32 	%p36, %r21, 0;
	selp.b32 	%r352, %r351, %r349, %p36;
	mov.b64 	%fd4709, {%r350, %r352};
	bra.uni 	$L__BB43_24;

$L__BB43_20:
	setp.gt.s32 	%p33, %r20, -1;
	@%p33 bra 	$L__BB43_24;

	mov.f64 	%fd2183, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2184, %fd2183;
	setp.eq.f64 	%p34, %fd2184, 0d4008000000000000;
	@%p34 bra 	$L__BB43_24;

	mov.f64 	%fd4709, 0dFFF8000000000000;

$L__BB43_24:
	add.f64 	%fd2186, %fd5049, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r353}, %fd2186;
	}
	and.b32  	%r354, %r353, 2146435072;
	setp.ne.s32 	%p37, %r354, 2146435072;
	@%p37 bra 	$L__BB43_31;

	setp.gtu.f64 	%p38, %fd605, 0d7FF0000000000000;
	@%p38 bra 	$L__BB43_30;
	bra.uni 	$L__BB43_26;

$L__BB43_30:
	mov.f64 	%fd2188, 0d4008000000000000;
	add.rn.f64 	%fd4709, %fd5049, %fd2188;
	bra.uni 	$L__BB43_31;

$L__BB43_26:
	mov.f64 	%fd2187, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r355, %temp}, %fd2187;
	}
	and.b32  	%r23, %r21, 2147483647;
	setp.eq.s32 	%p39, %r23, 2146435072;
	setp.eq.s32 	%p40, %r355, 0;
	and.pred  	%p41, %p39, %p40;
	@%p41 bra 	$L__BB43_29;
	bra.uni 	$L__BB43_27;

$L__BB43_29:
	setp.gt.f64 	%p48, %fd605, 0d3FF0000000000000;
	selp.b32 	%r362, 2146435072, 0, %p48;
	mov.u32 	%r363, 0;
	xor.b32  	%r364, %r362, 2146435072;
	setp.lt.s32 	%p49, %r21, 0;
	selp.b32 	%r365, %r364, %r362, %p49;
	setp.eq.f64 	%p50, %fd5049, 0dBFF0000000000000;
	selp.b32 	%r366, 1072693248, %r365, %p50;
	mov.b64 	%fd4709, {%r363, %r366};
	bra.uni 	$L__BB43_31;

$L__BB43_27:
	{
	.reg .b32 %temp;
	mov.b64 	{%r356, %temp}, %fd5049;
	}
	and.b32  	%r357, %r20, 2147483647;
	setp.ne.s32 	%p42, %r357, 2146435072;
	setp.ne.s32 	%p43, %r356, 0;
	or.pred  	%p44, %p42, %p43;
	@%p44 bra 	$L__BB43_31;

	setp.gt.s32 	%p45, %r21, -1;
	selp.b32 	%r358, 2146435072, 0, %p45;
	mov.u32 	%r359, 0;
	setp.ne.s32 	%p46, %r23, 1071644672;
	and.pred  	%p47, %p46, %p1;
	or.b32  	%r360, %r358, -2147483648;
	selp.b32 	%r361, %r360, %r358, %p47;
	mov.b64 	%fd4709, {%r359, %r361};

$L__BB43_31:
	mov.f64 	%fd2189, 0d0000000000000000;
	sub.f64 	%fd2190, %fd2189, %fd4709;
	div.rn.f64 	%fd2191, %fd2190, 0d4008000000000000;
	setp.eq.f64 	%p51, %fd5049, 0d3FF0000000000000;
	selp.f64 	%fd5050, 0dBFD5555555555555, %fd2191, %p51;
	div.rn.f64 	%fd2192, %fd5053, 0d4008000000000000;
	mov.f64 	%fd2193, 0d3FE0000000000000;
	sub.f64 	%fd2194, %fd2193, %fd2192;
	mul.f64 	%fd2195, %fd5060, 0d3FE0000000000000;
	sub.f64 	%fd2196, %fd2194, %fd2195;
	mul.f64 	%fd2197, %fd5067, 0d3FE0000000000000;
	sub.f64 	%fd5056, %fd2196, %fd2197;
	mul.f64 	%fd5055, %fd5053, %fd5053;
	mul.f64 	%fd5057, %fd5055, %fd5056;
	div.rn.f64 	%fd2198, %fd5060, 0d4008000000000000;
	sub.f64 	%fd2199, %fd2193, %fd2198;
	mul.f64 	%fd2200, %fd5053, 0d3FE0000000000000;
	sub.f64 	%fd2201, %fd2199, %fd2200;
	sub.f64 	%fd5063, %fd2201, %fd2197;
	mul.f64 	%fd5062, %fd5060, %fd5060;
	mul.f64 	%fd5064, %fd5062, %fd5063;
	div.rn.f64 	%fd2202, %fd5067, 0d4008000000000000;
	sub.f64 	%fd2203, %fd2193, %fd2202;
	sub.f64 	%fd2204, %fd2203, %fd2200;
	sub.f64 	%fd5070, %fd2204, %fd2195;
	mul.f64 	%fd5069, %fd5067, %fd5067;
	mul.f64 	%fd5071, %fd5069, %fd5070;
	abs.f64 	%fd625, %fd5053;
	{ // callseq 64, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd625;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4712, [retval0+0];
	} // callseq 64
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r24}, %fd5053;
	}
	setp.lt.s32 	%p52, %r24, 0;
	and.pred  	%p2, %p52, %p29;
	not.pred 	%p54, %p2;
	@%p54 bra 	$L__BB43_33;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r367}, %fd4712;
	}
	xor.b32  	%r368, %r367, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r369, %temp}, %fd4712;
	}
	mov.b64 	%fd4712, {%r369, %r368};

$L__BB43_33:
	setp.eq.f64 	%p55, %fd5053, 0d0000000000000000;
	@%p55 bra 	$L__BB43_37;
	bra.uni 	$L__BB43_34;

$L__BB43_37:
	selp.b32 	%r370, %r24, 0, %p29;
	mov.u32 	%r371, 0;
	or.b32  	%r372, %r370, 2146435072;
	setp.lt.s32 	%p59, %r21, 0;
	selp.b32 	%r373, %r372, %r370, %p59;
	mov.b64 	%fd4712, {%r371, %r373};
	bra.uni 	$L__BB43_38;

$L__BB43_34:
	setp.gt.s32 	%p56, %r24, -1;
	@%p56 bra 	$L__BB43_38;

	mov.f64 	%fd2205, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2206, %fd2205;
	setp.eq.f64 	%p57, %fd2206, 0d4008000000000000;
	@%p57 bra 	$L__BB43_38;

	mov.f64 	%fd4712, 0dFFF8000000000000;

$L__BB43_38:
	add.f64 	%fd2208, %fd5053, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r374}, %fd2208;
	}
	and.b32  	%r375, %r374, 2146435072;
	setp.ne.s32 	%p60, %r375, 2146435072;
	@%p60 bra 	$L__BB43_45;

	setp.gtu.f64 	%p61, %fd625, 0d7FF0000000000000;
	@%p61 bra 	$L__BB43_44;
	bra.uni 	$L__BB43_40;

$L__BB43_44:
	mov.f64 	%fd2210, 0d4008000000000000;
	add.rn.f64 	%fd4712, %fd5053, %fd2210;
	bra.uni 	$L__BB43_45;

$L__BB43_40:
	mov.f64 	%fd2209, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r376, %temp}, %fd2209;
	}
	and.b32  	%r25, %r21, 2147483647;
	setp.eq.s32 	%p62, %r25, 2146435072;
	setp.eq.s32 	%p63, %r376, 0;
	and.pred  	%p64, %p62, %p63;
	@%p64 bra 	$L__BB43_43;
	bra.uni 	$L__BB43_41;

$L__BB43_43:
	setp.gt.f64 	%p71, %fd625, 0d3FF0000000000000;
	selp.b32 	%r383, 2146435072, 0, %p71;
	mov.u32 	%r384, 0;
	xor.b32  	%r385, %r383, 2146435072;
	setp.lt.s32 	%p72, %r21, 0;
	selp.b32 	%r386, %r385, %r383, %p72;
	setp.eq.f64 	%p73, %fd5053, 0dBFF0000000000000;
	selp.b32 	%r387, 1072693248, %r386, %p73;
	mov.b64 	%fd4712, {%r384, %r387};
	bra.uni 	$L__BB43_45;

$L__BB43_41:
	{
	.reg .b32 %temp;
	mov.b64 	{%r377, %temp}, %fd5053;
	}
	and.b32  	%r378, %r24, 2147483647;
	setp.ne.s32 	%p65, %r378, 2146435072;
	setp.ne.s32 	%p66, %r377, 0;
	or.pred  	%p67, %p65, %p66;
	@%p67 bra 	$L__BB43_45;

	setp.gt.s32 	%p68, %r21, -1;
	selp.b32 	%r379, 2146435072, 0, %p68;
	mov.u32 	%r380, 0;
	setp.ne.s32 	%p69, %r25, 1071644672;
	and.pred  	%p70, %p69, %p2;
	or.b32  	%r381, %r379, -2147483648;
	selp.b32 	%r382, %r381, %r379, %p70;
	mov.b64 	%fd4712, {%r380, %r382};

$L__BB43_45:
	div.rn.f64 	%fd2211, %fd4712, 0d4008000000000000;
	setp.eq.f64 	%p74, %fd5053, 0d3FF0000000000000;
	mov.f64 	%fd2212, 0d3FF0000000000000;
	selp.f64 	%fd5075, 0d3FD5555555555555, %fd2211, %p74;
	mul.f64 	%fd5080, %fd5055, 0d3FE0000000000000;
	mul.f64 	%fd5082, %fd5060, %fd5080;
	mul.f64 	%fd5089, %fd5080, %fd5067;
	mul.f64 	%fd5094, %fd5053, %fd5060;
	mul.f64 	%fd5096, %fd5060, %fd5094;
	mul.f64 	%fd5103, %fd5094, %fd5067;
	mul.f64 	%fd5108, %fd5053, %fd5067;
	mul.f64 	%fd5110, %fd5067, %fd5108;
	sub.f64 	%fd2213, %fd2212, %fd5118;
	sub.f64 	%fd2214, %fd2213, %fd5125;
	sub.f64 	%fd5114, %fd2214, %fd5132;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r26}, %fd5114;
	}
	abs.f64 	%fd645, %fd5114;
	{ // callseq 65, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd645;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4715, [retval0+0];
	} // callseq 65
	setp.lt.s32 	%p75, %r26, 0;
	and.pred  	%p3, %p75, %p29;
	not.pred 	%p77, %p3;
	@%p77 bra 	$L__BB43_47;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r388}, %fd4715;
	}
	xor.b32  	%r389, %r388, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r390, %temp}, %fd4715;
	}
	mov.b64 	%fd4715, {%r390, %r389};

$L__BB43_47:
	setp.eq.f64 	%p78, %fd5114, 0d0000000000000000;
	@%p78 bra 	$L__BB43_51;
	bra.uni 	$L__BB43_48;

$L__BB43_51:
	selp.b32 	%r391, %r26, 0, %p29;
	mov.u32 	%r392, 0;
	or.b32  	%r393, %r391, 2146435072;
	setp.lt.s32 	%p82, %r21, 0;
	selp.b32 	%r394, %r393, %r391, %p82;
	mov.b64 	%fd4715, {%r392, %r394};
	bra.uni 	$L__BB43_52;

$L__BB43_48:
	setp.gt.s32 	%p79, %r26, -1;
	@%p79 bra 	$L__BB43_52;

	mov.f64 	%fd2215, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2216, %fd2215;
	setp.eq.f64 	%p80, %fd2216, 0d4008000000000000;
	@%p80 bra 	$L__BB43_52;

	mov.f64 	%fd4715, 0dFFF8000000000000;

$L__BB43_52:
	add.f64 	%fd2218, %fd5114, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r395}, %fd2218;
	}
	and.b32  	%r396, %r395, 2146435072;
	setp.ne.s32 	%p83, %r396, 2146435072;
	@%p83 bra 	$L__BB43_59;

	setp.gtu.f64 	%p84, %fd645, 0d7FF0000000000000;
	@%p84 bra 	$L__BB43_58;
	bra.uni 	$L__BB43_54;

$L__BB43_58:
	mov.f64 	%fd2220, 0d4008000000000000;
	add.rn.f64 	%fd4715, %fd5114, %fd2220;
	bra.uni 	$L__BB43_59;

$L__BB43_54:
	mov.f64 	%fd2219, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r397, %temp}, %fd2219;
	}
	and.b32  	%r27, %r21, 2147483647;
	setp.eq.s32 	%p85, %r27, 2146435072;
	setp.eq.s32 	%p86, %r397, 0;
	and.pred  	%p87, %p85, %p86;
	@%p87 bra 	$L__BB43_57;
	bra.uni 	$L__BB43_55;

$L__BB43_57:
	setp.gt.f64 	%p94, %fd645, 0d3FF0000000000000;
	selp.b32 	%r404, 2146435072, 0, %p94;
	mov.u32 	%r405, 0;
	xor.b32  	%r406, %r404, 2146435072;
	setp.lt.s32 	%p95, %r21, 0;
	selp.b32 	%r407, %r406, %r404, %p95;
	setp.eq.f64 	%p96, %fd5114, 0dBFF0000000000000;
	selp.b32 	%r408, 1072693248, %r407, %p96;
	mov.b64 	%fd4715, {%r405, %r408};
	bra.uni 	$L__BB43_59;

$L__BB43_55:
	{
	.reg .b32 %temp;
	mov.b64 	{%r398, %temp}, %fd5114;
	}
	and.b32  	%r399, %r26, 2147483647;
	setp.ne.s32 	%p88, %r399, 2146435072;
	setp.ne.s32 	%p89, %r398, 0;
	or.pred  	%p90, %p88, %p89;
	@%p90 bra 	$L__BB43_59;

	setp.gt.s32 	%p91, %r21, -1;
	selp.b32 	%r400, 2146435072, 0, %p91;
	mov.u32 	%r401, 0;
	setp.ne.s32 	%p92, %r27, 1071644672;
	and.pred  	%p93, %p92, %p3;
	or.b32  	%r402, %r400, -2147483648;
	selp.b32 	%r403, %r402, %r400, %p93;
	mov.b64 	%fd4715, {%r401, %r403};

$L__BB43_59:
	mov.f64 	%fd2221, 0d0000000000000000;
	sub.f64 	%fd2222, %fd2221, %fd4715;
	div.rn.f64 	%fd2223, %fd2222, 0d4008000000000000;
	setp.eq.f64 	%p97, %fd5114, 0d3FF0000000000000;
	selp.f64 	%fd5115, 0dBFD5555555555555, %fd2223, %p97;
	div.rn.f64 	%fd2224, %fd5118, 0d4008000000000000;
	mov.f64 	%fd2225, 0d3FE0000000000000;
	sub.f64 	%fd2226, %fd2225, %fd2224;
	mul.f64 	%fd2227, %fd5125, 0d3FE0000000000000;
	sub.f64 	%fd2228, %fd2226, %fd2227;
	mul.f64 	%fd2229, %fd5132, 0d3FE0000000000000;
	sub.f64 	%fd5121, %fd2228, %fd2229;
	mul.f64 	%fd5120, %fd5118, %fd5118;
	mul.f64 	%fd5122, %fd5120, %fd5121;
	div.rn.f64 	%fd2230, %fd5125, 0d4008000000000000;
	sub.f64 	%fd2231, %fd2225, %fd2230;
	mul.f64 	%fd2232, %fd5118, 0d3FE0000000000000;
	sub.f64 	%fd2233, %fd2231, %fd2232;
	sub.f64 	%fd5128, %fd2233, %fd2229;
	mul.f64 	%fd5127, %fd5125, %fd5125;
	mul.f64 	%fd5129, %fd5127, %fd5128;
	div.rn.f64 	%fd2234, %fd5132, 0d4008000000000000;
	sub.f64 	%fd2235, %fd2225, %fd2234;
	sub.f64 	%fd2236, %fd2235, %fd2232;
	sub.f64 	%fd5135, %fd2236, %fd2227;
	mul.f64 	%fd5134, %fd5132, %fd5132;
	mul.f64 	%fd5136, %fd5134, %fd5135;
	abs.f64 	%fd665, %fd5118;
	{ // callseq 66, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd665;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4718, [retval0+0];
	} // callseq 66
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r28}, %fd5118;
	}
	setp.lt.s32 	%p98, %r28, 0;
	and.pred  	%p4, %p98, %p29;
	not.pred 	%p100, %p4;
	@%p100 bra 	$L__BB43_61;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r409}, %fd4718;
	}
	xor.b32  	%r410, %r409, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r411, %temp}, %fd4718;
	}
	mov.b64 	%fd4718, {%r411, %r410};

$L__BB43_61:
	setp.eq.f64 	%p101, %fd5118, 0d0000000000000000;
	@%p101 bra 	$L__BB43_65;
	bra.uni 	$L__BB43_62;

$L__BB43_65:
	selp.b32 	%r412, %r28, 0, %p29;
	mov.u32 	%r413, 0;
	or.b32  	%r414, %r412, 2146435072;
	setp.lt.s32 	%p105, %r21, 0;
	selp.b32 	%r415, %r414, %r412, %p105;
	mov.b64 	%fd4718, {%r413, %r415};
	bra.uni 	$L__BB43_66;

$L__BB43_62:
	setp.gt.s32 	%p102, %r28, -1;
	@%p102 bra 	$L__BB43_66;

	mov.f64 	%fd2237, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2238, %fd2237;
	setp.eq.f64 	%p103, %fd2238, 0d4008000000000000;
	@%p103 bra 	$L__BB43_66;

	mov.f64 	%fd4718, 0dFFF8000000000000;

$L__BB43_66:
	add.f64 	%fd2240, %fd5118, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r416}, %fd2240;
	}
	and.b32  	%r417, %r416, 2146435072;
	setp.ne.s32 	%p106, %r417, 2146435072;
	@%p106 bra 	$L__BB43_73;

	setp.gtu.f64 	%p107, %fd665, 0d7FF0000000000000;
	@%p107 bra 	$L__BB43_72;
	bra.uni 	$L__BB43_68;

$L__BB43_72:
	mov.f64 	%fd2242, 0d4008000000000000;
	add.rn.f64 	%fd4718, %fd5118, %fd2242;
	bra.uni 	$L__BB43_73;

$L__BB43_68:
	mov.f64 	%fd2241, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r418, %temp}, %fd2241;
	}
	and.b32  	%r29, %r21, 2147483647;
	setp.eq.s32 	%p108, %r29, 2146435072;
	setp.eq.s32 	%p109, %r418, 0;
	and.pred  	%p110, %p108, %p109;
	@%p110 bra 	$L__BB43_71;
	bra.uni 	$L__BB43_69;

$L__BB43_71:
	setp.gt.f64 	%p117, %fd665, 0d3FF0000000000000;
	selp.b32 	%r425, 2146435072, 0, %p117;
	mov.u32 	%r426, 0;
	xor.b32  	%r427, %r425, 2146435072;
	setp.lt.s32 	%p118, %r21, 0;
	selp.b32 	%r428, %r427, %r425, %p118;
	setp.eq.f64 	%p119, %fd5118, 0dBFF0000000000000;
	selp.b32 	%r429, 1072693248, %r428, %p119;
	mov.b64 	%fd4718, {%r426, %r429};
	bra.uni 	$L__BB43_73;

$L__BB43_69:
	{
	.reg .b32 %temp;
	mov.b64 	{%r419, %temp}, %fd5118;
	}
	and.b32  	%r420, %r28, 2147483647;
	setp.ne.s32 	%p111, %r420, 2146435072;
	setp.ne.s32 	%p112, %r419, 0;
	or.pred  	%p113, %p111, %p112;
	@%p113 bra 	$L__BB43_73;

	setp.gt.s32 	%p114, %r21, -1;
	selp.b32 	%r421, 2146435072, 0, %p114;
	mov.u32 	%r422, 0;
	setp.ne.s32 	%p115, %r29, 1071644672;
	and.pred  	%p116, %p115, %p4;
	or.b32  	%r423, %r421, -2147483648;
	selp.b32 	%r424, %r423, %r421, %p116;
	mov.b64 	%fd4718, {%r422, %r424};

$L__BB43_73:
	div.rn.f64 	%fd2243, %fd4718, 0d4008000000000000;
	setp.eq.f64 	%p120, %fd5118, 0d3FF0000000000000;
	mov.f64 	%fd2244, 0d3FF0000000000000;
	selp.f64 	%fd5140, 0d3FD5555555555555, %fd2243, %p120;
	mul.f64 	%fd5145, %fd5120, 0d3FE0000000000000;
	mul.f64 	%fd5147, %fd5125, %fd5145;
	mul.f64 	%fd5154, %fd5145, %fd5132;
	mul.f64 	%fd5159, %fd5118, %fd5125;
	mul.f64 	%fd5161, %fd5125, %fd5159;
	mul.f64 	%fd5168, %fd5159, %fd5132;
	mul.f64 	%fd5173, %fd5118, %fd5132;
	mul.f64 	%fd5175, %fd5132, %fd5173;
	sub.f64 	%fd2245, %fd2244, %fd5183;
	sub.f64 	%fd2246, %fd2245, %fd5190;
	sub.f64 	%fd5179, %fd2246, %fd5197;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r30}, %fd5179;
	}
	abs.f64 	%fd685, %fd5179;
	{ // callseq 67, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd685;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4721, [retval0+0];
	} // callseq 67
	setp.lt.s32 	%p121, %r30, 0;
	and.pred  	%p5, %p121, %p29;
	not.pred 	%p123, %p5;
	@%p123 bra 	$L__BB43_75;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r430}, %fd4721;
	}
	xor.b32  	%r431, %r430, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r432, %temp}, %fd4721;
	}
	mov.b64 	%fd4721, {%r432, %r431};

$L__BB43_75:
	setp.eq.f64 	%p124, %fd5179, 0d0000000000000000;
	@%p124 bra 	$L__BB43_79;
	bra.uni 	$L__BB43_76;

$L__BB43_79:
	selp.b32 	%r433, %r30, 0, %p29;
	mov.u32 	%r434, 0;
	or.b32  	%r435, %r433, 2146435072;
	setp.lt.s32 	%p128, %r21, 0;
	selp.b32 	%r436, %r435, %r433, %p128;
	mov.b64 	%fd4721, {%r434, %r436};
	bra.uni 	$L__BB43_80;

$L__BB43_76:
	setp.gt.s32 	%p125, %r30, -1;
	@%p125 bra 	$L__BB43_80;

	mov.f64 	%fd2247, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2248, %fd2247;
	setp.eq.f64 	%p126, %fd2248, 0d4008000000000000;
	@%p126 bra 	$L__BB43_80;

	mov.f64 	%fd4721, 0dFFF8000000000000;

$L__BB43_80:
	add.f64 	%fd2250, %fd5179, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r437}, %fd2250;
	}
	and.b32  	%r438, %r437, 2146435072;
	setp.ne.s32 	%p129, %r438, 2146435072;
	@%p129 bra 	$L__BB43_87;

	setp.gtu.f64 	%p130, %fd685, 0d7FF0000000000000;
	@%p130 bra 	$L__BB43_86;
	bra.uni 	$L__BB43_82;

$L__BB43_86:
	mov.f64 	%fd2252, 0d4008000000000000;
	add.rn.f64 	%fd4721, %fd5179, %fd2252;
	bra.uni 	$L__BB43_87;

$L__BB43_82:
	mov.f64 	%fd2251, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r439, %temp}, %fd2251;
	}
	and.b32  	%r31, %r21, 2147483647;
	setp.eq.s32 	%p131, %r31, 2146435072;
	setp.eq.s32 	%p132, %r439, 0;
	and.pred  	%p133, %p131, %p132;
	@%p133 bra 	$L__BB43_85;
	bra.uni 	$L__BB43_83;

$L__BB43_85:
	setp.gt.f64 	%p140, %fd685, 0d3FF0000000000000;
	selp.b32 	%r446, 2146435072, 0, %p140;
	mov.u32 	%r447, 0;
	xor.b32  	%r448, %r446, 2146435072;
	setp.lt.s32 	%p141, %r21, 0;
	selp.b32 	%r449, %r448, %r446, %p141;
	setp.eq.f64 	%p142, %fd5179, 0dBFF0000000000000;
	selp.b32 	%r450, 1072693248, %r449, %p142;
	mov.b64 	%fd4721, {%r447, %r450};
	bra.uni 	$L__BB43_87;

$L__BB43_83:
	{
	.reg .b32 %temp;
	mov.b64 	{%r440, %temp}, %fd5179;
	}
	and.b32  	%r441, %r30, 2147483647;
	setp.ne.s32 	%p134, %r441, 2146435072;
	setp.ne.s32 	%p135, %r440, 0;
	or.pred  	%p136, %p134, %p135;
	@%p136 bra 	$L__BB43_87;

	setp.gt.s32 	%p137, %r21, -1;
	selp.b32 	%r442, 2146435072, 0, %p137;
	mov.u32 	%r443, 0;
	setp.ne.s32 	%p138, %r31, 1071644672;
	and.pred  	%p139, %p138, %p5;
	or.b32  	%r444, %r442, -2147483648;
	selp.b32 	%r445, %r444, %r442, %p139;
	mov.b64 	%fd4721, {%r443, %r445};

$L__BB43_87:
	mov.f64 	%fd2253, 0d0000000000000000;
	sub.f64 	%fd2254, %fd2253, %fd4721;
	div.rn.f64 	%fd2255, %fd2254, 0d4008000000000000;
	setp.eq.f64 	%p143, %fd5179, 0d3FF0000000000000;
	selp.f64 	%fd5180, 0dBFD5555555555555, %fd2255, %p143;
	div.rn.f64 	%fd2256, %fd5183, 0d4008000000000000;
	mov.f64 	%fd2257, 0d3FE0000000000000;
	sub.f64 	%fd2258, %fd2257, %fd2256;
	mul.f64 	%fd2259, %fd5190, 0d3FE0000000000000;
	sub.f64 	%fd2260, %fd2258, %fd2259;
	mul.f64 	%fd2261, %fd5197, 0d3FE0000000000000;
	sub.f64 	%fd5186, %fd2260, %fd2261;
	mul.f64 	%fd5185, %fd5183, %fd5183;
	mul.f64 	%fd5187, %fd5185, %fd5186;
	div.rn.f64 	%fd2262, %fd5190, 0d4008000000000000;
	sub.f64 	%fd2263, %fd2257, %fd2262;
	mul.f64 	%fd2264, %fd5183, 0d3FE0000000000000;
	sub.f64 	%fd2265, %fd2263, %fd2264;
	sub.f64 	%fd5193, %fd2265, %fd2261;
	mul.f64 	%fd5192, %fd5190, %fd5190;
	mul.f64 	%fd5194, %fd5192, %fd5193;
	div.rn.f64 	%fd2266, %fd5197, 0d4008000000000000;
	sub.f64 	%fd2267, %fd2257, %fd2266;
	sub.f64 	%fd2268, %fd2267, %fd2264;
	sub.f64 	%fd5200, %fd2268, %fd2259;
	mul.f64 	%fd5199, %fd5197, %fd5197;
	mul.f64 	%fd5201, %fd5199, %fd5200;
	abs.f64 	%fd705, %fd5183;
	{ // callseq 68, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd705;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4724, [retval0+0];
	} // callseq 68
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r32}, %fd5183;
	}
	setp.lt.s32 	%p144, %r32, 0;
	and.pred  	%p6, %p144, %p29;
	not.pred 	%p146, %p6;
	@%p146 bra 	$L__BB43_89;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r451}, %fd4724;
	}
	xor.b32  	%r452, %r451, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r453, %temp}, %fd4724;
	}
	mov.b64 	%fd4724, {%r453, %r452};

$L__BB43_89:
	setp.eq.f64 	%p147, %fd5183, 0d0000000000000000;
	@%p147 bra 	$L__BB43_93;
	bra.uni 	$L__BB43_90;

$L__BB43_93:
	selp.b32 	%r454, %r32, 0, %p29;
	mov.u32 	%r455, 0;
	or.b32  	%r456, %r454, 2146435072;
	setp.lt.s32 	%p151, %r21, 0;
	selp.b32 	%r457, %r456, %r454, %p151;
	mov.b64 	%fd4724, {%r455, %r457};
	bra.uni 	$L__BB43_94;

$L__BB43_90:
	setp.gt.s32 	%p148, %r32, -1;
	@%p148 bra 	$L__BB43_94;

	mov.f64 	%fd2269, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2270, %fd2269;
	setp.eq.f64 	%p149, %fd2270, 0d4008000000000000;
	@%p149 bra 	$L__BB43_94;

	mov.f64 	%fd4724, 0dFFF8000000000000;

$L__BB43_94:
	add.f64 	%fd2272, %fd5183, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r458}, %fd2272;
	}
	and.b32  	%r459, %r458, 2146435072;
	setp.ne.s32 	%p152, %r459, 2146435072;
	@%p152 bra 	$L__BB43_101;

	setp.gtu.f64 	%p153, %fd705, 0d7FF0000000000000;
	@%p153 bra 	$L__BB43_100;
	bra.uni 	$L__BB43_96;

$L__BB43_100:
	mov.f64 	%fd2274, 0d4008000000000000;
	add.rn.f64 	%fd4724, %fd5183, %fd2274;
	bra.uni 	$L__BB43_101;

$L__BB43_96:
	mov.f64 	%fd2273, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r460, %temp}, %fd2273;
	}
	and.b32  	%r33, %r21, 2147483647;
	setp.eq.s32 	%p154, %r33, 2146435072;
	setp.eq.s32 	%p155, %r460, 0;
	and.pred  	%p156, %p154, %p155;
	@%p156 bra 	$L__BB43_99;
	bra.uni 	$L__BB43_97;

$L__BB43_99:
	setp.gt.f64 	%p163, %fd705, 0d3FF0000000000000;
	selp.b32 	%r467, 2146435072, 0, %p163;
	mov.u32 	%r468, 0;
	xor.b32  	%r469, %r467, 2146435072;
	setp.lt.s32 	%p164, %r21, 0;
	selp.b32 	%r470, %r469, %r467, %p164;
	setp.eq.f64 	%p165, %fd5183, 0dBFF0000000000000;
	selp.b32 	%r471, 1072693248, %r470, %p165;
	mov.b64 	%fd4724, {%r468, %r471};
	bra.uni 	$L__BB43_101;

$L__BB43_97:
	{
	.reg .b32 %temp;
	mov.b64 	{%r461, %temp}, %fd5183;
	}
	and.b32  	%r462, %r32, 2147483647;
	setp.ne.s32 	%p157, %r462, 2146435072;
	setp.ne.s32 	%p158, %r461, 0;
	or.pred  	%p159, %p157, %p158;
	@%p159 bra 	$L__BB43_101;

	setp.gt.s32 	%p160, %r21, -1;
	selp.b32 	%r463, 2146435072, 0, %p160;
	mov.u32 	%r464, 0;
	setp.ne.s32 	%p161, %r33, 1071644672;
	and.pred  	%p162, %p161, %p6;
	or.b32  	%r465, %r463, -2147483648;
	selp.b32 	%r466, %r465, %r463, %p162;
	mov.b64 	%fd4724, {%r464, %r466};

$L__BB43_101:
	div.rn.f64 	%fd2275, %fd4724, 0d4008000000000000;
	setp.eq.f64 	%p166, %fd5183, 0d3FF0000000000000;
	mov.f64 	%fd2276, 0d3FF0000000000000;
	selp.f64 	%fd5205, 0d3FD5555555555555, %fd2275, %p166;
	mul.f64 	%fd5210, %fd5185, 0d3FE0000000000000;
	mul.f64 	%fd5212, %fd5190, %fd5210;
	mul.f64 	%fd5219, %fd5210, %fd5197;
	mul.f64 	%fd5224, %fd5183, %fd5190;
	mul.f64 	%fd5226, %fd5190, %fd5224;
	mul.f64 	%fd5233, %fd5224, %fd5197;
	mul.f64 	%fd5238, %fd5183, %fd5197;
	mul.f64 	%fd5240, %fd5197, %fd5238;
	sub.f64 	%fd2277, %fd2276, %fd5247;
	sub.f64 	%fd2278, %fd2277, %fd5254;
	sub.f64 	%fd5243, %fd2278, %fd5261;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r34}, %fd5243;
	}
	abs.f64 	%fd725, %fd5243;
	{ // callseq 69, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd725;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4727, [retval0+0];
	} // callseq 69
	setp.lt.s32 	%p167, %r34, 0;
	and.pred  	%p7, %p167, %p29;
	not.pred 	%p169, %p7;
	@%p169 bra 	$L__BB43_103;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r472}, %fd4727;
	}
	xor.b32  	%r473, %r472, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r474, %temp}, %fd4727;
	}
	mov.b64 	%fd4727, {%r474, %r473};

$L__BB43_103:
	setp.eq.f64 	%p170, %fd5243, 0d0000000000000000;
	@%p170 bra 	$L__BB43_107;
	bra.uni 	$L__BB43_104;

$L__BB43_107:
	selp.b32 	%r475, %r34, 0, %p29;
	mov.u32 	%r476, 0;
	or.b32  	%r477, %r475, 2146435072;
	setp.lt.s32 	%p174, %r21, 0;
	selp.b32 	%r478, %r477, %r475, %p174;
	mov.b64 	%fd4727, {%r476, %r478};
	bra.uni 	$L__BB43_108;

$L__BB43_104:
	setp.gt.s32 	%p171, %r34, -1;
	@%p171 bra 	$L__BB43_108;

	mov.f64 	%fd2279, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2280, %fd2279;
	setp.eq.f64 	%p172, %fd2280, 0d4008000000000000;
	@%p172 bra 	$L__BB43_108;

	mov.f64 	%fd4727, 0dFFF8000000000000;

$L__BB43_108:
	add.f64 	%fd2282, %fd5243, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r479}, %fd2282;
	}
	and.b32  	%r480, %r479, 2146435072;
	setp.ne.s32 	%p175, %r480, 2146435072;
	@%p175 bra 	$L__BB43_115;

	setp.gtu.f64 	%p176, %fd725, 0d7FF0000000000000;
	@%p176 bra 	$L__BB43_114;
	bra.uni 	$L__BB43_110;

$L__BB43_114:
	mov.f64 	%fd2284, 0d4008000000000000;
	add.rn.f64 	%fd4727, %fd5243, %fd2284;
	bra.uni 	$L__BB43_115;

$L__BB43_110:
	mov.f64 	%fd2283, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r481, %temp}, %fd2283;
	}
	and.b32  	%r35, %r21, 2147483647;
	setp.eq.s32 	%p177, %r35, 2146435072;
	setp.eq.s32 	%p178, %r481, 0;
	and.pred  	%p179, %p177, %p178;
	@%p179 bra 	$L__BB43_113;
	bra.uni 	$L__BB43_111;

$L__BB43_113:
	setp.gt.f64 	%p186, %fd725, 0d3FF0000000000000;
	selp.b32 	%r488, 2146435072, 0, %p186;
	mov.u32 	%r489, 0;
	xor.b32  	%r490, %r488, 2146435072;
	setp.lt.s32 	%p187, %r21, 0;
	selp.b32 	%r491, %r490, %r488, %p187;
	setp.eq.f64 	%p188, %fd5243, 0dBFF0000000000000;
	selp.b32 	%r492, 1072693248, %r491, %p188;
	mov.b64 	%fd4727, {%r489, %r492};
	bra.uni 	$L__BB43_115;

$L__BB43_111:
	{
	.reg .b32 %temp;
	mov.b64 	{%r482, %temp}, %fd5243;
	}
	and.b32  	%r483, %r34, 2147483647;
	setp.ne.s32 	%p180, %r483, 2146435072;
	setp.ne.s32 	%p181, %r482, 0;
	or.pred  	%p182, %p180, %p181;
	@%p182 bra 	$L__BB43_115;

	setp.gt.s32 	%p183, %r21, -1;
	selp.b32 	%r484, 2146435072, 0, %p183;
	mov.u32 	%r485, 0;
	setp.ne.s32 	%p184, %r35, 1071644672;
	and.pred  	%p185, %p184, %p7;
	or.b32  	%r486, %r484, -2147483648;
	selp.b32 	%r487, %r486, %r484, %p185;
	mov.b64 	%fd4727, {%r485, %r487};

$L__BB43_115:
	mov.f64 	%fd2285, 0d0000000000000000;
	sub.f64 	%fd2286, %fd2285, %fd4727;
	div.rn.f64 	%fd2287, %fd2286, 0d4008000000000000;
	setp.eq.f64 	%p189, %fd5243, 0d3FF0000000000000;
	selp.f64 	%fd5244, 0dBFD5555555555555, %fd2287, %p189;
	div.rn.f64 	%fd2288, %fd5247, 0d4008000000000000;
	mov.f64 	%fd2289, 0d3FE0000000000000;
	sub.f64 	%fd2290, %fd2289, %fd2288;
	mul.f64 	%fd2291, %fd5254, 0d3FE0000000000000;
	sub.f64 	%fd2292, %fd2290, %fd2291;
	mul.f64 	%fd2293, %fd5261, 0d3FE0000000000000;
	sub.f64 	%fd5250, %fd2292, %fd2293;
	mul.f64 	%fd5249, %fd5247, %fd5247;
	mul.f64 	%fd5251, %fd5249, %fd5250;
	div.rn.f64 	%fd2294, %fd5254, 0d4008000000000000;
	sub.f64 	%fd2295, %fd2289, %fd2294;
	mul.f64 	%fd2296, %fd5247, 0d3FE0000000000000;
	sub.f64 	%fd2297, %fd2295, %fd2296;
	sub.f64 	%fd5257, %fd2297, %fd2293;
	mul.f64 	%fd5256, %fd5254, %fd5254;
	mul.f64 	%fd5258, %fd5256, %fd5257;
	div.rn.f64 	%fd2298, %fd5261, 0d4008000000000000;
	sub.f64 	%fd2299, %fd2289, %fd2298;
	sub.f64 	%fd2300, %fd2299, %fd2296;
	sub.f64 	%fd5264, %fd2300, %fd2291;
	mul.f64 	%fd5263, %fd5261, %fd5261;
	mul.f64 	%fd5265, %fd5263, %fd5264;
	abs.f64 	%fd745, %fd5247;
	{ // callseq 70, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd745;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4008000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd4730, [retval0+0];
	} // callseq 70
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r36}, %fd5247;
	}
	setp.lt.s32 	%p190, %r36, 0;
	and.pred  	%p8, %p190, %p29;
	not.pred 	%p192, %p8;
	@%p192 bra 	$L__BB43_117;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r493}, %fd4730;
	}
	xor.b32  	%r494, %r493, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r495, %temp}, %fd4730;
	}
	mov.b64 	%fd4730, {%r495, %r494};

$L__BB43_117:
	setp.eq.f64 	%p193, %fd5247, 0d0000000000000000;
	@%p193 bra 	$L__BB43_121;
	bra.uni 	$L__BB43_118;

$L__BB43_121:
	selp.b32 	%r496, %r36, 0, %p29;
	mov.u32 	%r497, 0;
	or.b32  	%r498, %r496, 2146435072;
	setp.lt.s32 	%p197, %r21, 0;
	selp.b32 	%r499, %r498, %r496, %p197;
	mov.b64 	%fd4730, {%r497, %r499};
	bra.uni 	$L__BB43_122;

$L__BB43_118:
	setp.gt.s32 	%p194, %r36, -1;
	@%p194 bra 	$L__BB43_122;

	mov.f64 	%fd2301, 0d4008000000000000;
	cvt.rzi.f64.f64 	%fd2302, %fd2301;
	setp.eq.f64 	%p195, %fd2302, 0d4008000000000000;
	@%p195 bra 	$L__BB43_122;

	mov.f64 	%fd4730, 0dFFF8000000000000;

$L__BB43_122:
	add.f64 	%fd2304, %fd5247, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r500}, %fd2304;
	}
	and.b32  	%r501, %r500, 2146435072;
	setp.ne.s32 	%p198, %r501, 2146435072;
	@%p198 bra 	$L__BB43_129;

	setp.gtu.f64 	%p199, %fd745, 0d7FF0000000000000;
	@%p199 bra 	$L__BB43_128;
	bra.uni 	$L__BB43_124;

$L__BB43_128:
	mov.f64 	%fd2306, 0d4008000000000000;
	add.rn.f64 	%fd4730, %fd5247, %fd2306;
	bra.uni 	$L__BB43_129;

$L__BB43_124:
	mov.f64 	%fd2305, 0d4008000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r502, %temp}, %fd2305;
	}
	and.b32  	%r37, %r21, 2147483647;
	setp.eq.s32 	%p200, %r37, 2146435072;
	setp.eq.s32 	%p201, %r502, 0;
	and.pred  	%p202, %p200, %p201;
	@%p202 bra 	$L__BB43_127;
	bra.uni 	$L__BB43_125;

$L__BB43_127:
	setp.gt.f64 	%p209, %fd745, 0d3FF0000000000000;
	selp.b32 	%r509, 2146435072, 0, %p209;
	mov.u32 	%r510, 0;
	xor.b32  	%r511, %r509, 2146435072;
	setp.lt.s32 	%p210, %r21, 0;
	selp.b32 	%r512, %r511, %r509, %p210;
	setp.eq.f64 	%p211, %fd5247, 0dBFF0000000000000;
	selp.b32 	%r513, 1072693248, %r512, %p211;
	mov.b64 	%fd4730, {%r510, %r513};
	bra.uni 	$L__BB43_129;

$L__BB43_125:
	{
	.reg .b32 %temp;
	mov.b64 	{%r503, %temp}, %fd5247;
	}
	and.b32  	%r504, %r36, 2147483647;
	setp.ne.s32 	%p203, %r504, 2146435072;
	setp.ne.s32 	%p204, %r503, 0;
	or.pred  	%p205, %p203, %p204;
	@%p205 bra 	$L__BB43_129;

	setp.gt.s32 	%p206, %r21, -1;
	selp.b32 	%r505, 2146435072, 0, %p206;
	mov.u32 	%r506, 0;
	setp.ne.s32 	%p207, %r37, 1071644672;
	and.pred  	%p208, %p207, %p8;
	or.b32  	%r507, %r505, -2147483648;
	selp.b32 	%r508, %r507, %r505, %p208;
	mov.b64 	%fd4730, {%r506, %r508};

$L__BB43_129:
	mul.f64 	%fd2307, %fd5044, %fd5046;
	mul.f64 	%fd5018, %fd2307, 0d3FE0AAAAA0000000;
	div.rn.f64 	%fd2308, %fd4730, 0d4008000000000000;
	setp.eq.f64 	%p212, %fd5247, 0d3FF0000000000000;
	selp.f64 	%fd5269, 0d3FD5555555555555, %fd2308, %p212;
	mul.f64 	%fd5271, %fd5051, %fd5269;
	mul.f64 	%fd5274, %fd5249, 0d3FE0000000000000;
	mul.f64 	%fd5276, %fd5254, %fd5274;
	mul.f64 	%fd5278, %fd5051, %fd5276;
	mul.f64 	%fd5283, %fd5274, %fd5261;
	mul.f64 	%fd5285, %fd5051, %fd5283;
	mul.f64 	%fd5288, %fd5247, %fd5254;
	mul.f64 	%fd5290, %fd5254, %fd5288;
	mul.f64 	%fd5292, %fd5051, %fd5290;
	mul.f64 	%fd5297, %fd5288, %fd5261;
	mul.f64 	%fd5299, %fd5051, %fd5297;
	mul.f64 	%fd5302, %fd5247, %fd5261;
	mul.f64 	%fd5304, %fd5261, %fd5302;
	mul.f64 	%fd5306, %fd5051, %fd5304;
	mul.f64 	%fd5207, %fd5051, %fd5205;
	mul.f64 	%fd5214, %fd5051, %fd5212;
	mul.f64 	%fd5221, %fd5051, %fd5219;
	mul.f64 	%fd5228, %fd5051, %fd5226;
	mul.f64 	%fd5235, %fd5051, %fd5233;
	mul.f64 	%fd5242, %fd5051, %fd5240;
	mul.f64 	%fd5142, %fd5051, %fd5140;
	mul.f64 	%fd5149, %fd5051, %fd5147;
	mul.f64 	%fd5156, %fd5051, %fd5154;
	mul.f64 	%fd5163, %fd5051, %fd5161;
	mul.f64 	%fd5170, %fd5051, %fd5168;
	mul.f64 	%fd5177, %fd5051, %fd5175;
	mul.f64 	%fd5077, %fd5051, %fd5075;
	mul.f64 	%fd5084, %fd5051, %fd5082;
	mul.f64 	%fd5091, %fd5051, %fd5089;
	mul.f64 	%fd5098, %fd5051, %fd5096;
	mul.f64 	%fd5105, %fd5051, %fd5103;
	mul.f64 	%fd5112, %fd5051, %fd5110;
	mul.f64 	%fd2309, %fd565, %fd569;
	mul.f64 	%fd2310, %fd568, %fd566;
	sub.f64 	%fd5031, %fd2309, %fd2310;
	mul.f64 	%fd2311, %fd562, %fd569;
	mul.f64 	%fd2312, %fd568, %fd563;
	sub.f64 	%fd5032, %fd2312, %fd2311;
	mul.f64 	%fd2313, %fd565, %fd563;
	mul.f64 	%fd2314, %fd562, %fd566;
	sub.f64 	%fd5033, %fd2314, %fd2313;
	mul.f64 	%fd5048, %fd2307, 0dBFE2000000000000;
	mul.f64 	%fd5073, %fd5072, %fd5071;
	mul.f64 	%fd5066, %fd5065, %fd5064;
	mul.f64 	%fd5059, %fd5051, %fd5057;
	mul.f64 	%fd5052, %fd5051, %fd5050;
	mul.f64 	%fd5138, %fd5072, %fd5136;
	mul.f64 	%fd5131, %fd5065, %fd5129;
	mul.f64 	%fd5124, %fd5051, %fd5122;
	mul.f64 	%fd5117, %fd5051, %fd5115;
	mul.f64 	%fd5203, %fd5072, %fd5201;
	mul.f64 	%fd5196, %fd5065, %fd5194;
	mul.f64 	%fd5189, %fd5051, %fd5187;
	mul.f64 	%fd5182, %fd5051, %fd5180;
	mul.f64 	%fd5267, %fd5072, %fd5265;
	mul.f64 	%fd5260, %fd5065, %fd5258;
	mul.f64 	%fd5253, %fd5051, %fd5251;
	mul.f64 	%fd5246, %fd5051, %fd5244;
	mov.f64 	%fd5030, %fd568;
	mov.f64 	%fd5029, %fd565;
	mov.f64 	%fd5028, %fd562;
	mov.f64 	%fd5027, %fd569;
	mov.f64 	%fd5026, %fd566;
	mov.f64 	%fd5025, %fd563;
	mov.f64 	%fd5024, %fd575;
	mov.f64 	%fd5023, %fd573;
	mov.f64 	%fd5022, %fd571;
	mov.f64 	%fd5015, %fd5051;
	mov.f64 	%fd5016, %fd5065;
	mov.f64 	%fd5017, %fd5072;

$L__BB43_130:
	setp.ne.s32 	%p213, %r698, 0;
	mov.u32 	%r699, %r13;
	mov.u32 	%r700, %r13;
	mov.u32 	%r701, %r13;
	mov.u32 	%r705, %r13;
	mov.u32 	%r706, %r14;
	@%p213 bra 	$L__BB43_132;

	mul.f64 	%fd5014, %fd5043, %fd5046;
	mul.f64 	%fd2318, %fd5016, %fd5016;
	fma.rn.f64 	%fd2319, %fd5015, %fd5015, %fd2318;
	fma.rn.f64 	%fd2320, %fd5017, %fd5017, %fd2319;
	sqrt.rn.f64 	%fd5307, %fd2320;
	div.rn.f64 	%fd5011, %fd5015, %fd5307;
	div.rn.f64 	%fd5012, %fd5016, %fd5307;
	div.rn.f64 	%fd5013, %fd5017, %fd5307;
	mul.f64 	%fd2321, %fd5043, %fd5011;
	mul.f64 	%fd2322, %fd5043, %fd5012;
	mul.f64 	%fd2323, %fd5043, %fd5013;
	div.rn.f64 	%fd2324, %fd2321, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd2325, %fd2322, 0d400BB67AE8584CAA;
	div.rn.f64 	%fd2326, %fd2323, 0d400BB67AE8584CAA;
	sub.f64 	%fd2327, %fd562, %fd557;
	add.f64 	%fd2328, %fd2327, %fd2327;
	div.rn.f64 	%fd2329, %fd2328, 0d4008000000000000;
	sub.f64 	%fd2330, %fd563, %fd557;
	div.rn.f64 	%fd2331, %fd2330, 0d4018000000000000;
	add.f64 	%fd2332, %fd2329, %fd2331;
	sub.f64 	%fd2333, %fd571, %fd557;
	div.rn.f64 	%fd2334, %fd2333, 0d4018000000000000;
	add.f64 	%fd2335, %fd2332, %fd2334;
	sub.f64 	%fd5347, %fd2335, %fd2324;
	fma.rn.f64 	%fd2336, %fd2327, 0d4018000000000000, %fd2331;
	add.f64 	%fd2337, %fd2333, %fd2333;
	div.rn.f64 	%fd2338, %fd2337, 0d4008000000000000;
	add.f64 	%fd2339, %fd2336, %fd2338;
	sub.f64 	%fd5395, %fd2339, %fd2324;
	div.rn.f64 	%fd2340, %fd2327, 0d4018000000000000;
	add.f64 	%fd2341, %fd2330, %fd2330;
	div.rn.f64 	%fd2342, %fd2341, 0d4008000000000000;
	add.f64 	%fd2343, %fd2340, %fd2342;
	add.f64 	%fd2344, %fd2334, %fd2343;
	sub.f64 	%fd5443, %fd2344, %fd2324;
	add.f64 	%fd5491, %fd2324, %fd2335;
	add.f64 	%fd5539, %fd2324, %fd2339;
	add.f64 	%fd5587, %fd2324, %fd2344;
	sub.f64 	%fd2345, %fd565, %fd558;
	add.f64 	%fd2346, %fd2345, %fd2345;
	div.rn.f64 	%fd2347, %fd2346, 0d4008000000000000;
	sub.f64 	%fd2348, %fd566, %fd558;
	div.rn.f64 	%fd2349, %fd2348, 0d4018000000000000;
	add.f64 	%fd2350, %fd2347, %fd2349;
	sub.f64 	%fd2351, %fd573, %fd558;
	div.rn.f64 	%fd2352, %fd2351, 0d4018000000000000;
	add.f64 	%fd2353, %fd2350, %fd2352;
	sub.f64 	%fd5350, %fd2353, %fd2325;
	fma.rn.f64 	%fd2354, %fd2345, 0d4018000000000000, %fd2349;
	add.f64 	%fd2355, %fd2351, %fd2351;
	div.rn.f64 	%fd2356, %fd2355, 0d4008000000000000;
	add.f64 	%fd2357, %fd2354, %fd2356;
	sub.f64 	%fd5398, %fd2357, %fd2325;
	div.rn.f64 	%fd2358, %fd2345, 0d4018000000000000;
	add.f64 	%fd2359, %fd2348, %fd2348;
	div.rn.f64 	%fd2360, %fd2359, 0d4008000000000000;
	add.f64 	%fd2361, %fd2358, %fd2360;
	add.f64 	%fd2362, %fd2352, %fd2361;
	sub.f64 	%fd5446, %fd2362, %fd2325;
	add.f64 	%fd5494, %fd2325, %fd2353;
	add.f64 	%fd5542, %fd2325, %fd2357;
	add.f64 	%fd5590, %fd2325, %fd2362;
	sub.f64 	%fd2363, %fd568, %fd559;
	add.f64 	%fd2364, %fd2363, %fd2363;
	div.rn.f64 	%fd2365, %fd2364, 0d4008000000000000;
	sub.f64 	%fd2366, %fd569, %fd559;
	div.rn.f64 	%fd2367, %fd2366, 0d4018000000000000;
	add.f64 	%fd2368, %fd2365, %fd2367;
	sub.f64 	%fd2369, %fd575, %fd559;
	div.rn.f64 	%fd2370, %fd2369, 0d4018000000000000;
	add.f64 	%fd2371, %fd2368, %fd2370;
	sub.f64 	%fd5355, %fd2371, %fd2326;
	fma.rn.f64 	%fd2372, %fd2363, 0d4018000000000000, %fd2367;
	add.f64 	%fd2373, %fd2369, %fd2369;
	div.rn.f64 	%fd2374, %fd2373, 0d4008000000000000;
	add.f64 	%fd2375, %fd2372, %fd2374;
	sub.f64 	%fd5403, %fd2375, %fd2326;
	div.rn.f64 	%fd2376, %fd2363, 0d4018000000000000;
	add.f64 	%fd2377, %fd2366, %fd2366;
	div.rn.f64 	%fd2378, %fd2377, 0d4008000000000000;
	add.f64 	%fd2379, %fd2376, %fd2378;
	add.f64 	%fd2380, %fd2370, %fd2379;
	sub.f64 	%fd5451, %fd2380, %fd2326;
	add.f64 	%fd5499, %fd2326, %fd2371;
	add.f64 	%fd5547, %fd2326, %fd2375;
	add.f64 	%fd5595, %fd2326, %fd2380;
	mul.f64 	%fd2381, %fd5044, %fd5014;
	div.rn.f64 	%fd5010, %fd2381, 0d4018000000000000;
	mov.f64 	%fd2382, 0d3FF0000000000000;
	sub.f64 	%fd2383, %fd2382, %fd5347;
	sub.f64 	%fd2384, %fd2383, %fd5350;
	sub.f64 	%fd5344, %fd2384, %fd5355;
	mul.f64 	%fd5345, %fd5010, %fd5344;
	mul.f64 	%fd5348, %fd5010, %fd5347;
	mul.f64 	%fd5351, %fd5010, %fd5350;
	mul.f64 	%fd5354, %fd5010, %fd5355;
	sub.f64 	%fd2385, %fd2382, %fd5395;
	sub.f64 	%fd2386, %fd2385, %fd5398;
	sub.f64 	%fd5392, %fd2386, %fd5403;
	mul.f64 	%fd5393, %fd5010, %fd5392;
	mul.f64 	%fd5396, %fd5010, %fd5395;
	mul.f64 	%fd5399, %fd5010, %fd5398;
	mul.f64 	%fd5402, %fd5010, %fd5403;
	sub.f64 	%fd2387, %fd2382, %fd5443;
	sub.f64 	%fd2388, %fd2387, %fd5446;
	sub.f64 	%fd5440, %fd2388, %fd5451;
	mul.f64 	%fd5441, %fd5010, %fd5440;
	mul.f64 	%fd5444, %fd5010, %fd5443;
	mul.f64 	%fd5447, %fd5010, %fd5446;
	mul.f64 	%fd5450, %fd5010, %fd5451;
	sub.f64 	%fd2389, %fd2382, %fd5491;
	sub.f64 	%fd2390, %fd2389, %fd5494;
	sub.f64 	%fd5488, %fd2390, %fd5499;
	mul.f64 	%fd5489, %fd5010, %fd5488;
	mul.f64 	%fd5492, %fd5010, %fd5491;
	mul.f64 	%fd5495, %fd5010, %fd5494;
	mul.f64 	%fd5498, %fd5010, %fd5499;
	sub.f64 	%fd2391, %fd2382, %fd5539;
	sub.f64 	%fd2392, %fd2391, %fd5542;
	sub.f64 	%fd5536, %fd2392, %fd5547;
	mul.f64 	%fd5537, %fd5010, %fd5536;
	mul.f64 	%fd5540, %fd5010, %fd5539;
	mul.f64 	%fd5543, %fd5010, %fd5542;
	mul.f64 	%fd5546, %fd5010, %fd5547;
	sub.f64 	%fd2393, %fd2382, %fd5587;
	sub.f64 	%fd2394, %fd2393, %fd5590;
	sub.f64 	%fd5584, %fd2394, %fd5595;
	mul.f64 	%fd5585, %fd5010, %fd5584;
	mul.f64 	%fd5588, %fd5010, %fd5587;
	mul.f64 	%fd5591, %fd5010, %fd5590;
	mul.f64 	%fd5594, %fd5010, %fd5595;
	mov.u32 	%r698, 0;
	mov.u32 	%r699, %r13;
	mov.u32 	%r700, %r13;
	mov.u32 	%r701, %r13;
	mov.u32 	%r705, %r13;
	mov.u32 	%r706, %r14;

$L__BB43_132:
	setp.eq.s32 	%p215, %r706, 0;
	or.pred  	%p216, %p215, %p25;
	@%p216 bra 	$L__BB43_280;

	setp.eq.s64 	%p217, %rd82, 0;
	@%p217 bra 	$L__BB43_135;

	cvta.to.global.u64 	%rd117, %rd82;
	cvt.s64.s32 	%rd118, %r705;
	mul.lo.s64 	%rd119, %rd118, %rd34;
	add.s64 	%rd120, %rd117, %rd119;
	ld.global.f64 	%fd2395, [%rd120];
	add.f64 	%fd5611, %fd2395, 0d0000000000000000;
	ld.global.f64 	%fd2396, [%rd120+8];
	add.f64 	%fd5610, %fd2396, 0d0000000000000000;
	ld.global.f64 	%fd2397, [%rd120+16];
	add.f64 	%fd5609, %fd2397, 0d0000000000000000;
	ld.global.f64 	%fd2398, [%rd120+24];
	add.f64 	%fd5608, %fd2398, 0d0000000000000000;
	ld.global.f64 	%fd2399, [%rd120+32];
	add.f64 	%fd5607, %fd2399, 0d0000000000000000;
	ld.global.f64 	%fd2400, [%rd120+40];
	add.f64 	%fd5606, %fd2400, 0d0000000000000000;
	ld.global.f64 	%fd2401, [%rd120+48];
	add.f64 	%fd5605, %fd2401, 0d0000000000000000;
	ld.global.f64 	%fd2402, [%rd120+56];
	add.f64 	%fd5604, %fd2402, 0d0000000000000000;
	ld.global.f64 	%fd2403, [%rd120+64];
	add.f64 	%fd5603, %fd2403, 0d0000000000000000;
	ld.global.f64 	%fd2404, [%rd120+72];
	add.f64 	%fd5602, %fd2404, 0d0000000000000000;
	ld.global.f64 	%fd2405, [%rd120+80];
	add.f64 	%fd5601, %fd2405, 0d0000000000000000;
	ld.global.f64 	%fd2406, [%rd120+88];
	add.f64 	%fd5600, %fd2406, 0d0000000000000000;
	ld.global.f64 	%fd2407, [%rd120+96];
	add.f64 	%fd5599, %fd2407, 0d0000000000000000;
	ld.global.f64 	%fd2408, [%rd120+104];
	add.f64 	%fd5598, %fd2408, 0d0000000000000000;
	ld.global.f64 	%fd2409, [%rd120+112];
	add.f64 	%fd5597, %fd2409, 0d0000000000000000;
	ld.global.f64 	%fd2410, [%rd120+120];
	add.f64 	%fd5596, %fd2410, 0d0000000000000000;
	bra.uni 	$L__BB43_137;

$L__BB43_135:
	setp.eq.s64 	%p218, %rd73, 0;
	mov.f64 	%fd5596, 0d0000000000000000;
	mov.f64 	%fd5597, %fd5596;
	mov.f64 	%fd5598, %fd5596;
	mov.f64 	%fd5599, %fd5596;
	mov.f64 	%fd5600, %fd5596;
	mov.f64 	%fd5601, %fd5596;
	mov.f64 	%fd5602, %fd5596;
	mov.f64 	%fd5603, %fd5596;
	mov.f64 	%fd5604, %fd5596;
	mov.f64 	%fd5605, %fd5596;
	mov.f64 	%fd5606, %fd5596;
	mov.f64 	%fd5607, %fd5596;
	mov.f64 	%fd5608, %fd5596;
	mov.f64 	%fd5609, %fd5596;
	mov.f64 	%fd5610, %fd5596;
	mov.f64 	%fd5611, %fd5596;
	@%p218 bra 	$L__BB43_137;

	cvta.to.global.u64 	%rd121, %rd73;
	cvt.s64.s32 	%rd122, %r705;
	mul.lo.s64 	%rd123, %rd122, %rd35;
	add.s64 	%rd124, %rd121, %rd123;
	ld.global.f64 	%fd2427, [%rd124];
	add.f64 	%fd5611, %fd2427, 0d0000000000000000;
	ld.global.f64 	%fd2428, [%rd124+8];
	add.f64 	%fd5610, %fd2428, 0d0000000000000000;
	ld.global.f64 	%fd2429, [%rd124+16];
	add.f64 	%fd5609, %fd2429, 0d0000000000000000;
	ld.global.f64 	%fd2430, [%rd124+24];
	add.f64 	%fd5608, %fd2430, 0d0000000000000000;
	ld.global.f64 	%fd2431, [%rd124+32];
	add.f64 	%fd5607, %fd2431, 0d0000000000000000;
	ld.global.f64 	%fd2432, [%rd124+40];
	add.f64 	%fd5606, %fd2432, 0d0000000000000000;
	ld.global.f64 	%fd2433, [%rd124+48];
	add.f64 	%fd5605, %fd2433, 0d0000000000000000;
	ld.global.f64 	%fd2434, [%rd124+56];
	add.f64 	%fd5604, %fd2434, 0d0000000000000000;
	ld.global.f64 	%fd2435, [%rd124+64];
	add.f64 	%fd5603, %fd2435, 0d0000000000000000;
	ld.global.f64 	%fd2436, [%rd124+72];
	add.f64 	%fd5602, %fd2436, 0d0000000000000000;
	ld.global.f64 	%fd2437, [%rd124+80];
	add.f64 	%fd5601, %fd2437, 0d0000000000000000;
	ld.global.f64 	%fd2438, [%rd124+88];
	add.f64 	%fd5600, %fd2438, 0d0000000000000000;
	ld.global.f64 	%fd2439, [%rd124+96];
	add.f64 	%fd5599, %fd2439, 0d0000000000000000;
	ld.global.f64 	%fd2440, [%rd124+104];
	add.f64 	%fd5598, %fd2440, 0d0000000000000000;
	ld.global.f64 	%fd2441, [%rd124+112];
	add.f64 	%fd5597, %fd2441, 0d0000000000000000;
	ld.global.f64 	%fd2442, [%rd124+120];
	add.f64 	%fd5596, %fd2442, 0d0000000000000000;

$L__BB43_137:
	add.f64 	%fd1782, %fd5596, 0d0000000000000000;
	setp.eq.s32 	%p219, %r698, 0;
	@%p219 bra 	$L__BB43_251;

	add.f64 	%fd2443, %fd5597, 0d0000000000000000;
	add.f64 	%fd1783, %fd5611, 0d0000000000000000;
	add.f64 	%fd1784, %fd2443, %fd5600;
	add.f64 	%fd2444, %fd5598, 0d0000000000000000;
	add.f64 	%fd1785, %fd2444, %fd5604;
	add.f64 	%fd2445, %fd5602, 0d0000000000000000;
	add.f64 	%fd1786, %fd2445, %fd5605;
	add.f64 	%fd2446, %fd5599, 0d0000000000000000;
	add.f64 	%fd1787, %fd2446, %fd5608;
	add.f64 	%fd2447, %fd5603, 0d0000000000000000;
	add.f64 	%fd1788, %fd2447, %fd5609;
	add.f64 	%fd2448, %fd5607, 0d0000000000000000;
	add.f64 	%fd1789, %fd2448, %fd5610;
	fma.rn.f64 	%fd2449, %fd5018, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2450, %fd5306, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2451, %fd5051, %fd2449, 0d0000000000000000;
	fma.rn.f64 	%fd2452, %fd5304, %fd2449, 0d0000000000000000;
	fma.rn.f64 	%fd2453, %fd5261, %fd2451, 0d0000000000000000;
	fma.rn.f64 	%fd2454, %fd5302, %fd2451, 0d0000000000000000;
	fma.rn.f64 	%fd2455, %fd5261, %fd2453, 0d0000000000000000;
	fma.rn.f64 	%fd2456, %fd5247, %fd2453, 0d0000000000000000;
	add.f64 	%fd2457, %fd2456, %fd2454;
	fma.rn.f64 	%fd2458, %fd5018, %fd1784, 0d0000000000000000;
	fma.rn.f64 	%fd2459, %fd5299, %fd1784, %fd2450;
	fma.rn.f64 	%fd2460, %fd5051, %fd2458, 0d0000000000000000;
	fma.rn.f64 	%fd2461, %fd5297, %fd2458, 0d0000000000000000;
	add.f64 	%fd2462, %fd2461, %fd2452;
	fma.rn.f64 	%fd2463, %fd5261, %fd2460, 0d0000000000000000;
	fma.rn.f64 	%fd2464, %fd5288, %fd2460, 0d0000000000000000;
	add.f64 	%fd2465, %fd2464, %fd2457;
	fma.rn.f64 	%fd2466, %fd5254, %fd2463, 0d0000000000000000;
	fma.rn.f64 	%fd2467, %fd5247, %fd2463, 0d0000000000000000;
	add.f64 	%fd2468, %fd2466, %fd2455;
	add.f64 	%fd1790, %fd5601, 0d0000000000000000;
	fma.rn.f64 	%fd2469, %fd5018, %fd1790, 0d0000000000000000;
	fma.rn.f64 	%fd2470, %fd5292, %fd1790, %fd2459;
	fma.rn.f64 	%fd2471, %fd5051, %fd2469, 0d0000000000000000;
	fma.rn.f64 	%fd2472, %fd5290, %fd2469, 0d0000000000000000;
	add.f64 	%fd2473, %fd2472, %fd2462;
	fma.rn.f64 	%fd2474, %fd5254, %fd2471, 0d0000000000000000;
	fma.rn.f64 	%fd2475, %fd5288, %fd2471, 0d0000000000000000;
	add.f64 	%fd2476, %fd2475, %fd2467;
	fma.rn.f64 	%fd2477, %fd5254, %fd2474, 0d0000000000000000;
	fma.rn.f64 	%fd2478, %fd5247, %fd2474, 0d0000000000000000;
	add.f64 	%fd2479, %fd2478, %fd2476;
	add.f64 	%fd2480, %fd2477, %fd2468;
	fma.rn.f64 	%fd2481, %fd5018, %fd1785, 0d0000000000000000;
	fma.rn.f64 	%fd2482, %fd5285, %fd1785, %fd2470;
	fma.rn.f64 	%fd2483, %fd5051, %fd2481, 0d0000000000000000;
	fma.rn.f64 	%fd2484, %fd5283, %fd2481, 0d0000000000000000;
	add.f64 	%fd2485, %fd2484, %fd2473;
	fma.rn.f64 	%fd2486, %fd5261, %fd2483, 0d0000000000000000;
	fma.rn.f64 	%fd2487, %fd5274, %fd2483, 0d0000000000000000;
	add.f64 	%fd1791, %fd2487, %fd2465;
	fma.rn.f64 	%fd2488, %fd2486, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2489, %fd5247, %fd2488, 0d0000000000000000;
	fma.rn.f64 	%fd2490, %fd5247, %fd2488, 0d0000000000000000;
	add.f64 	%fd2491, %fd2490, %fd2480;
	add.f64 	%fd2492, %fd2489, %fd2491;
	fma.rn.f64 	%fd2493, %fd5018, %fd1786, 0d0000000000000000;
	fma.rn.f64 	%fd2494, %fd5278, %fd1786, %fd2482;
	fma.rn.f64 	%fd2495, %fd5051, %fd2493, 0d0000000000000000;
	fma.rn.f64 	%fd2496, %fd5276, %fd2493, 0d0000000000000000;
	add.f64 	%fd2497, %fd2496, %fd2485;
	fma.rn.f64 	%fd2498, %fd5254, %fd2495, 0d0000000000000000;
	fma.rn.f64 	%fd2499, %fd5274, %fd2495, 0d0000000000000000;
	add.f64 	%fd1792, %fd2499, %fd2479;
	fma.rn.f64 	%fd2500, %fd2498, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2501, %fd5247, %fd2500, 0d0000000000000000;
	fma.rn.f64 	%fd2502, %fd5247, %fd2500, 0d0000000000000000;
	add.f64 	%fd2503, %fd2502, %fd2492;
	add.f64 	%fd1793, %fd2501, %fd2503;
	add.f64 	%fd1794, %fd5606, 0d0000000000000000;
	fma.rn.f64 	%fd2504, %fd5018, %fd1794, 0d0000000000000000;
	fma.rn.f64 	%fd1795, %fd5271, %fd1794, %fd2494;
	fma.rn.f64 	%fd1796, %fd5051, %fd2504, 0d0000000000000000;
	fma.rn.f64 	%fd2505, %fd5269, %fd2504, 0d0000000000000000;
	add.f64 	%fd1797, %fd2505, %fd2497;
	mov.f64 	%fd2506, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r48}, %fd2506;
	}
	and.b32  	%r49, %r48, 2146435072;
	setp.eq.s32 	%p220, %r49, 1062207488;
	abs.f64 	%fd1798, %fd5247;
	{ // callseq 71, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1798;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5614, [retval0+0];
	} // callseq 71
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r50}, %fd5247;
	}
	setp.lt.s32 	%p221, %r50, 0;
	and.pred  	%p9, %p221, %p220;
	not.pred 	%p222, %p9;
	@%p222 bra 	$L__BB43_140;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r515}, %fd5614;
	}
	xor.b32  	%r516, %r515, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r517, %temp}, %fd5614;
	}
	mov.b64 	%fd5614, {%r517, %r516};

$L__BB43_140:
	setp.eq.f64 	%p223, %fd5247, 0d0000000000000000;
	@%p223 bra 	$L__BB43_144;
	bra.uni 	$L__BB43_141;

$L__BB43_144:
	selp.b32 	%r518, %r50, 0, %p220;
	mov.u32 	%r519, 0;
	or.b32  	%r520, %r518, 2146435072;
	setp.lt.s32 	%p227, %r48, 0;
	selp.b32 	%r521, %r520, %r518, %p227;
	mov.b64 	%fd5614, {%r519, %r521};
	bra.uni 	$L__BB43_145;

$L__BB43_251:
	fma.rn.f64 	%fd3161, %fd5595, %fd1782, 0d0000000000000000;
	mov.f64 	%fd3162, 0d0000000000000000;
	fma.rn.f64 	%fd3163, %fd5594, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3164, %fd5595, %fd3161, 0d0000000000000000;
	fma.rn.f64 	%fd3165, %fd5010, %fd3161, 0d0000000000000000;
	add.f64 	%fd3166, %fd3163, %fd3165;
	add.f64 	%fd3167, %fd5597, 0d0000000000000000;
	fma.rn.f64 	%fd3168, %fd5595, %fd3167, 0d0000000000000000;
	fma.rn.f64 	%fd3169, %fd5591, %fd3167, 0d0000000000000000;
	add.f64 	%fd3170, %fd3169, %fd3166;
	fma.rn.f64 	%fd3171, %fd5590, %fd3168, %fd3164;
	fma.rn.f64 	%fd3172, %fd5010, %fd3168, 0d0000000000000000;
	add.f64 	%fd3173, %fd5598, 0d0000000000000000;
	fma.rn.f64 	%fd3174, %fd5595, %fd3173, 0d0000000000000000;
	fma.rn.f64 	%fd3175, %fd5588, %fd3173, 0d0000000000000000;
	add.f64 	%fd3176, %fd3175, %fd3170;
	fma.rn.f64 	%fd3177, %fd5587, %fd3174, %fd3171;
	fma.rn.f64 	%fd3178, %fd5010, %fd3174, 0d0000000000000000;
	add.f64 	%fd3179, %fd5599, 0d0000000000000000;
	fma.rn.f64 	%fd3180, %fd5595, %fd3179, 0d0000000000000000;
	fma.rn.f64 	%fd3181, %fd5585, %fd3179, 0d0000000000000000;
	add.f64 	%fd3182, %fd3181, %fd3176;
	fma.rn.f64 	%fd3183, %fd5584, %fd3180, %fd3177;
	fma.rn.f64 	%fd3184, %fd5010, %fd3180, 0d0000000000000000;
	add.f64 	%fd3185, %fd5600, 0d0000000000000000;
	fma.rn.f64 	%fd3186, %fd5590, %fd3185, 0d0000000000000000;
	fma.rn.f64 	%fd3187, %fd5594, %fd3185, 0d0000000000000000;
	add.f64 	%fd3188, %fd3172, %fd3187;
	fma.rn.f64 	%fd3189, %fd5595, %fd3186, %fd3183;
	fma.rn.f64 	%fd3190, %fd5010, %fd3186, 0d0000000000000000;
	add.f64 	%fd3191, %fd3182, %fd3190;
	add.f64 	%fd3192, %fd5601, 0d0000000000000000;
	fma.rn.f64 	%fd3193, %fd5590, %fd3192, 0d0000000000000000;
	fma.rn.f64 	%fd3194, %fd5591, %fd3192, 0d0000000000000000;
	add.f64 	%fd3195, %fd3188, %fd3194;
	fma.rn.f64 	%fd3196, %fd5590, %fd3193, %fd3189;
	fma.rn.f64 	%fd3197, %fd5010, %fd3193, 0d0000000000000000;
	add.f64 	%fd3198, %fd3195, %fd3197;
	add.f64 	%fd3199, %fd5602, 0d0000000000000000;
	fma.rn.f64 	%fd3200, %fd5590, %fd3199, 0d0000000000000000;
	fma.rn.f64 	%fd3201, %fd5588, %fd3199, 0d0000000000000000;
	add.f64 	%fd3202, %fd3201, %fd3198;
	fma.rn.f64 	%fd3203, %fd5587, %fd3200, %fd3196;
	fma.rn.f64 	%fd3204, %fd5010, %fd3200, 0d0000000000000000;
	add.f64 	%fd3205, %fd3178, %fd3204;
	add.f64 	%fd3206, %fd5603, 0d0000000000000000;
	fma.rn.f64 	%fd3207, %fd5590, %fd3206, 0d0000000000000000;
	fma.rn.f64 	%fd3208, %fd5585, %fd3206, 0d0000000000000000;
	add.f64 	%fd3209, %fd3208, %fd3202;
	fma.rn.f64 	%fd3210, %fd5584, %fd3207, %fd3203;
	fma.rn.f64 	%fd3211, %fd5010, %fd3207, 0d0000000000000000;
	add.f64 	%fd3212, %fd3184, %fd3211;
	add.f64 	%fd3213, %fd5604, 0d0000000000000000;
	fma.rn.f64 	%fd3214, %fd5587, %fd3213, 0d0000000000000000;
	fma.rn.f64 	%fd3215, %fd5594, %fd3213, 0d0000000000000000;
	add.f64 	%fd3216, %fd3215, %fd3205;
	fma.rn.f64 	%fd3217, %fd5595, %fd3214, %fd3210;
	fma.rn.f64 	%fd3218, %fd5010, %fd3214, 0d0000000000000000;
	add.f64 	%fd3219, %fd3191, %fd3218;
	add.f64 	%fd3220, %fd5605, 0d0000000000000000;
	fma.rn.f64 	%fd3221, %fd5587, %fd3220, 0d0000000000000000;
	fma.rn.f64 	%fd3222, %fd5591, %fd3220, 0d0000000000000000;
	add.f64 	%fd3223, %fd3222, %fd3216;
	fma.rn.f64 	%fd3224, %fd5590, %fd3221, %fd3217;
	fma.rn.f64 	%fd3225, %fd5010, %fd3221, 0d0000000000000000;
	add.f64 	%fd3226, %fd3209, %fd3225;
	add.f64 	%fd3227, %fd5606, 0d0000000000000000;
	fma.rn.f64 	%fd3228, %fd5587, %fd3227, 0d0000000000000000;
	fma.rn.f64 	%fd3229, %fd5588, %fd3227, 0d0000000000000000;
	add.f64 	%fd3230, %fd3229, %fd3223;
	fma.rn.f64 	%fd3231, %fd5587, %fd3228, %fd3224;
	fma.rn.f64 	%fd3232, %fd5010, %fd3228, 0d0000000000000000;
	add.f64 	%fd3233, %fd3230, %fd3232;
	add.f64 	%fd3234, %fd5607, 0d0000000000000000;
	fma.rn.f64 	%fd3235, %fd5587, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3236, %fd5585, %fd3234, 0d0000000000000000;
	add.f64 	%fd3237, %fd3236, %fd3233;
	fma.rn.f64 	%fd3238, %fd5584, %fd3235, %fd3231;
	fma.rn.f64 	%fd3239, %fd5010, %fd3235, 0d0000000000000000;
	add.f64 	%fd3240, %fd3212, %fd3239;
	add.f64 	%fd3241, %fd5608, 0d0000000000000000;
	fma.rn.f64 	%fd3242, %fd5584, %fd3241, 0d0000000000000000;
	fma.rn.f64 	%fd3243, %fd5594, %fd3241, 0d0000000000000000;
	add.f64 	%fd3244, %fd3243, %fd3240;
	fma.rn.f64 	%fd3245, %fd5595, %fd3242, %fd3238;
	fma.rn.f64 	%fd3246, %fd5010, %fd3242, 0d0000000000000000;
	add.f64 	%fd3247, %fd3219, %fd3246;
	add.f64 	%fd3248, %fd5609, 0d0000000000000000;
	fma.rn.f64 	%fd3249, %fd5584, %fd3248, 0d0000000000000000;
	fma.rn.f64 	%fd3250, %fd5591, %fd3248, 0d0000000000000000;
	add.f64 	%fd3251, %fd3250, %fd3244;
	fma.rn.f64 	%fd3252, %fd5590, %fd3249, %fd3245;
	fma.rn.f64 	%fd3253, %fd5010, %fd3249, 0d0000000000000000;
	add.f64 	%fd3254, %fd3226, %fd3253;
	add.f64 	%fd3255, %fd5610, 0d0000000000000000;
	fma.rn.f64 	%fd3256, %fd5584, %fd3255, 0d0000000000000000;
	fma.rn.f64 	%fd3257, %fd5588, %fd3255, 0d0000000000000000;
	add.f64 	%fd3258, %fd3257, %fd3251;
	fma.rn.f64 	%fd3259, %fd5587, %fd3256, %fd3252;
	fma.rn.f64 	%fd3260, %fd5010, %fd3256, 0d0000000000000000;
	add.f64 	%fd3261, %fd3237, %fd3260;
	add.f64 	%fd3262, %fd5611, 0d0000000000000000;
	fma.rn.f64 	%fd3263, %fd5584, %fd3262, 0d0000000000000000;
	fma.rn.f64 	%fd3264, %fd5585, %fd3262, 0d0000000000000000;
	add.f64 	%fd3265, %fd3264, %fd3258;
	fma.rn.f64 	%fd3266, %fd5584, %fd3263, %fd3259;
	fma.rn.f64 	%fd3267, %fd5010, %fd3263, 0d0000000000000000;
	add.f64 	%fd3268, %fd3267, %fd3265;
	add.f64 	%fd3269, %fd3268, 0d0000000000000000;
	add.f64 	%fd3270, %fd3261, 0d0000000000000000;
	add.f64 	%fd3271, %fd3254, 0d0000000000000000;
	add.f64 	%fd3272, %fd3247, 0d0000000000000000;
	sub.f64 	%fd3273, %fd3162, %fd3269;
	add.f64 	%fd3274, %fd3272, %fd3273;
	add.f64 	%fd3275, %fd3271, %fd3273;
	add.f64 	%fd3276, %fd3270, %fd3273;
	fma.rn.f64 	%fd3277, %fd5547, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3278, %fd5546, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3279, %fd5547, %fd3277, %fd3266;
	fma.rn.f64 	%fd3280, %fd5010, %fd3277, 0d0000000000000000;
	add.f64 	%fd3281, %fd3278, %fd3280;
	fma.rn.f64 	%fd3282, %fd5547, %fd3167, 0d0000000000000000;
	fma.rn.f64 	%fd3283, %fd5543, %fd3167, 0d0000000000000000;
	add.f64 	%fd3284, %fd3283, %fd3281;
	fma.rn.f64 	%fd3285, %fd5542, %fd3282, %fd3279;
	fma.rn.f64 	%fd3286, %fd5010, %fd3282, 0d0000000000000000;
	fma.rn.f64 	%fd3287, %fd5547, %fd3173, 0d0000000000000000;
	fma.rn.f64 	%fd3288, %fd5540, %fd3173, 0d0000000000000000;
	add.f64 	%fd3289, %fd3288, %fd3284;
	fma.rn.f64 	%fd3290, %fd5539, %fd3287, %fd3285;
	fma.rn.f64 	%fd3291, %fd5010, %fd3287, 0d0000000000000000;
	fma.rn.f64 	%fd3292, %fd5547, %fd3179, 0d0000000000000000;
	fma.rn.f64 	%fd3293, %fd5537, %fd3179, 0d0000000000000000;
	add.f64 	%fd3294, %fd3293, %fd3289;
	fma.rn.f64 	%fd3295, %fd5536, %fd3292, %fd3290;
	fma.rn.f64 	%fd3296, %fd5010, %fd3292, 0d0000000000000000;
	fma.rn.f64 	%fd3297, %fd5542, %fd3185, 0d0000000000000000;
	fma.rn.f64 	%fd3298, %fd5546, %fd3185, 0d0000000000000000;
	add.f64 	%fd3299, %fd3286, %fd3298;
	fma.rn.f64 	%fd3300, %fd5547, %fd3297, %fd3295;
	fma.rn.f64 	%fd3301, %fd5010, %fd3297, 0d0000000000000000;
	add.f64 	%fd3302, %fd3294, %fd3301;
	fma.rn.f64 	%fd3303, %fd5542, %fd3192, 0d0000000000000000;
	fma.rn.f64 	%fd3304, %fd5543, %fd3192, 0d0000000000000000;
	add.f64 	%fd3305, %fd3299, %fd3304;
	fma.rn.f64 	%fd3306, %fd5542, %fd3303, %fd3300;
	fma.rn.f64 	%fd3307, %fd5010, %fd3303, 0d0000000000000000;
	add.f64 	%fd3308, %fd3305, %fd3307;
	fma.rn.f64 	%fd3309, %fd5542, %fd3199, 0d0000000000000000;
	fma.rn.f64 	%fd3310, %fd5540, %fd3199, 0d0000000000000000;
	add.f64 	%fd3311, %fd3310, %fd3308;
	fma.rn.f64 	%fd3312, %fd5539, %fd3309, %fd3306;
	fma.rn.f64 	%fd3313, %fd5010, %fd3309, 0d0000000000000000;
	add.f64 	%fd3314, %fd3291, %fd3313;
	fma.rn.f64 	%fd3315, %fd5542, %fd3206, 0d0000000000000000;
	fma.rn.f64 	%fd3316, %fd5537, %fd3206, 0d0000000000000000;
	add.f64 	%fd3317, %fd3316, %fd3311;
	fma.rn.f64 	%fd3318, %fd5536, %fd3315, %fd3312;
	fma.rn.f64 	%fd3319, %fd5010, %fd3315, 0d0000000000000000;
	add.f64 	%fd3320, %fd3296, %fd3319;
	fma.rn.f64 	%fd3321, %fd5539, %fd3213, 0d0000000000000000;
	fma.rn.f64 	%fd3322, %fd5546, %fd3213, 0d0000000000000000;
	add.f64 	%fd3323, %fd3322, %fd3314;
	fma.rn.f64 	%fd3324, %fd5547, %fd3321, %fd3318;
	fma.rn.f64 	%fd3325, %fd5010, %fd3321, 0d0000000000000000;
	add.f64 	%fd3326, %fd3302, %fd3325;
	fma.rn.f64 	%fd3327, %fd5539, %fd3220, 0d0000000000000000;
	fma.rn.f64 	%fd3328, %fd5543, %fd3220, 0d0000000000000000;
	add.f64 	%fd3329, %fd3328, %fd3323;
	fma.rn.f64 	%fd3330, %fd5542, %fd3327, %fd3324;
	fma.rn.f64 	%fd3331, %fd5010, %fd3327, 0d0000000000000000;
	add.f64 	%fd3332, %fd3317, %fd3331;
	fma.rn.f64 	%fd3333, %fd5539, %fd3227, 0d0000000000000000;
	fma.rn.f64 	%fd3334, %fd5540, %fd3227, 0d0000000000000000;
	add.f64 	%fd3335, %fd3334, %fd3329;
	fma.rn.f64 	%fd3336, %fd5539, %fd3333, %fd3330;
	fma.rn.f64 	%fd3337, %fd5010, %fd3333, 0d0000000000000000;
	add.f64 	%fd3338, %fd3335, %fd3337;
	fma.rn.f64 	%fd3339, %fd5539, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3340, %fd5537, %fd3234, 0d0000000000000000;
	add.f64 	%fd3341, %fd3340, %fd3338;
	fma.rn.f64 	%fd3342, %fd5536, %fd3339, %fd3336;
	fma.rn.f64 	%fd3343, %fd5010, %fd3339, 0d0000000000000000;
	add.f64 	%fd3344, %fd3320, %fd3343;
	fma.rn.f64 	%fd3345, %fd5536, %fd3241, 0d0000000000000000;
	fma.rn.f64 	%fd3346, %fd5546, %fd3241, 0d0000000000000000;
	add.f64 	%fd3347, %fd3346, %fd3344;
	fma.rn.f64 	%fd3348, %fd5547, %fd3345, %fd3342;
	fma.rn.f64 	%fd3349, %fd5010, %fd3345, 0d0000000000000000;
	add.f64 	%fd3350, %fd3326, %fd3349;
	fma.rn.f64 	%fd3351, %fd5536, %fd3248, 0d0000000000000000;
	fma.rn.f64 	%fd3352, %fd5543, %fd3248, 0d0000000000000000;
	add.f64 	%fd3353, %fd3352, %fd3347;
	fma.rn.f64 	%fd3354, %fd5542, %fd3351, %fd3348;
	fma.rn.f64 	%fd3355, %fd5010, %fd3351, 0d0000000000000000;
	add.f64 	%fd3356, %fd3332, %fd3355;
	fma.rn.f64 	%fd3357, %fd5536, %fd3255, 0d0000000000000000;
	fma.rn.f64 	%fd3358, %fd5540, %fd3255, 0d0000000000000000;
	add.f64 	%fd3359, %fd3358, %fd3353;
	fma.rn.f64 	%fd3360, %fd5539, %fd3357, %fd3354;
	fma.rn.f64 	%fd3361, %fd5010, %fd3357, 0d0000000000000000;
	add.f64 	%fd3362, %fd3341, %fd3361;
	fma.rn.f64 	%fd3363, %fd5536, %fd3262, 0d0000000000000000;
	fma.rn.f64 	%fd3364, %fd5537, %fd3262, 0d0000000000000000;
	add.f64 	%fd3365, %fd3364, %fd3359;
	fma.rn.f64 	%fd3366, %fd5536, %fd3363, %fd3360;
	fma.rn.f64 	%fd3367, %fd5010, %fd3363, 0d0000000000000000;
	add.f64 	%fd3368, %fd3367, %fd3365;
	add.f64 	%fd3369, %fd3368, 0d0000000000000000;
	add.f64 	%fd3370, %fd3362, 0d0000000000000000;
	add.f64 	%fd3371, %fd3356, 0d0000000000000000;
	add.f64 	%fd3372, %fd3350, 0d0000000000000000;
	sub.f64 	%fd3373, %fd3162, %fd3369;
	add.f64 	%fd3374, %fd3372, %fd3373;
	add.f64 	%fd3375, %fd3371, %fd3373;
	add.f64 	%fd3376, %fd3370, %fd3373;
	fma.rn.f64 	%fd3377, %fd5499, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3378, %fd5498, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3379, %fd5499, %fd3377, %fd3366;
	fma.rn.f64 	%fd3380, %fd5010, %fd3377, 0d0000000000000000;
	add.f64 	%fd3381, %fd3378, %fd3380;
	fma.rn.f64 	%fd3382, %fd5499, %fd3167, 0d0000000000000000;
	fma.rn.f64 	%fd3383, %fd5495, %fd3167, 0d0000000000000000;
	add.f64 	%fd3384, %fd3383, %fd3381;
	fma.rn.f64 	%fd3385, %fd5494, %fd3382, %fd3379;
	fma.rn.f64 	%fd3386, %fd5010, %fd3382, 0d0000000000000000;
	fma.rn.f64 	%fd3387, %fd5499, %fd3173, 0d0000000000000000;
	fma.rn.f64 	%fd3388, %fd5492, %fd3173, 0d0000000000000000;
	add.f64 	%fd3389, %fd3388, %fd3384;
	fma.rn.f64 	%fd3390, %fd5491, %fd3387, %fd3385;
	fma.rn.f64 	%fd3391, %fd5010, %fd3387, 0d0000000000000000;
	fma.rn.f64 	%fd3392, %fd5499, %fd3179, 0d0000000000000000;
	fma.rn.f64 	%fd3393, %fd5489, %fd3179, 0d0000000000000000;
	add.f64 	%fd3394, %fd3393, %fd3389;
	fma.rn.f64 	%fd3395, %fd5488, %fd3392, %fd3390;
	fma.rn.f64 	%fd3396, %fd5010, %fd3392, 0d0000000000000000;
	fma.rn.f64 	%fd3397, %fd5494, %fd3185, 0d0000000000000000;
	fma.rn.f64 	%fd3398, %fd5498, %fd3185, 0d0000000000000000;
	add.f64 	%fd3399, %fd3386, %fd3398;
	fma.rn.f64 	%fd3400, %fd5499, %fd3397, %fd3395;
	fma.rn.f64 	%fd3401, %fd5010, %fd3397, 0d0000000000000000;
	add.f64 	%fd3402, %fd3394, %fd3401;
	fma.rn.f64 	%fd3403, %fd5494, %fd3192, 0d0000000000000000;
	fma.rn.f64 	%fd3404, %fd5495, %fd3192, 0d0000000000000000;
	add.f64 	%fd3405, %fd3399, %fd3404;
	fma.rn.f64 	%fd3406, %fd5494, %fd3403, %fd3400;
	fma.rn.f64 	%fd3407, %fd5010, %fd3403, 0d0000000000000000;
	add.f64 	%fd3408, %fd3405, %fd3407;
	fma.rn.f64 	%fd3409, %fd5494, %fd3199, 0d0000000000000000;
	fma.rn.f64 	%fd3410, %fd5492, %fd3199, 0d0000000000000000;
	add.f64 	%fd3411, %fd3410, %fd3408;
	fma.rn.f64 	%fd3412, %fd5491, %fd3409, %fd3406;
	fma.rn.f64 	%fd3413, %fd5010, %fd3409, 0d0000000000000000;
	add.f64 	%fd3414, %fd3391, %fd3413;
	fma.rn.f64 	%fd3415, %fd5494, %fd3206, 0d0000000000000000;
	fma.rn.f64 	%fd3416, %fd5489, %fd3206, 0d0000000000000000;
	add.f64 	%fd3417, %fd3416, %fd3411;
	fma.rn.f64 	%fd3418, %fd5488, %fd3415, %fd3412;
	fma.rn.f64 	%fd3419, %fd5010, %fd3415, 0d0000000000000000;
	add.f64 	%fd3420, %fd3396, %fd3419;
	fma.rn.f64 	%fd3421, %fd5491, %fd3213, 0d0000000000000000;
	fma.rn.f64 	%fd3422, %fd5498, %fd3213, 0d0000000000000000;
	add.f64 	%fd3423, %fd3422, %fd3414;
	fma.rn.f64 	%fd3424, %fd5499, %fd3421, %fd3418;
	fma.rn.f64 	%fd3425, %fd5010, %fd3421, 0d0000000000000000;
	add.f64 	%fd3426, %fd3402, %fd3425;
	fma.rn.f64 	%fd3427, %fd5491, %fd3220, 0d0000000000000000;
	fma.rn.f64 	%fd3428, %fd5495, %fd3220, 0d0000000000000000;
	add.f64 	%fd3429, %fd3428, %fd3423;
	fma.rn.f64 	%fd3430, %fd5494, %fd3427, %fd3424;
	fma.rn.f64 	%fd3431, %fd5010, %fd3427, 0d0000000000000000;
	add.f64 	%fd3432, %fd3417, %fd3431;
	fma.rn.f64 	%fd3433, %fd5491, %fd3227, 0d0000000000000000;
	fma.rn.f64 	%fd3434, %fd5492, %fd3227, 0d0000000000000000;
	add.f64 	%fd3435, %fd3434, %fd3429;
	fma.rn.f64 	%fd3436, %fd5491, %fd3433, %fd3430;
	fma.rn.f64 	%fd3437, %fd5010, %fd3433, 0d0000000000000000;
	add.f64 	%fd3438, %fd3435, %fd3437;
	fma.rn.f64 	%fd3439, %fd5491, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3440, %fd5489, %fd3234, 0d0000000000000000;
	add.f64 	%fd3441, %fd3440, %fd3438;
	fma.rn.f64 	%fd3442, %fd5488, %fd3439, %fd3436;
	fma.rn.f64 	%fd3443, %fd5010, %fd3439, 0d0000000000000000;
	add.f64 	%fd3444, %fd3420, %fd3443;
	fma.rn.f64 	%fd3445, %fd5488, %fd3241, 0d0000000000000000;
	fma.rn.f64 	%fd3446, %fd5498, %fd3241, 0d0000000000000000;
	add.f64 	%fd3447, %fd3446, %fd3444;
	fma.rn.f64 	%fd3448, %fd5499, %fd3445, %fd3442;
	fma.rn.f64 	%fd3449, %fd5010, %fd3445, 0d0000000000000000;
	add.f64 	%fd3450, %fd3426, %fd3449;
	fma.rn.f64 	%fd3451, %fd5488, %fd3248, 0d0000000000000000;
	fma.rn.f64 	%fd3452, %fd5495, %fd3248, 0d0000000000000000;
	add.f64 	%fd3453, %fd3452, %fd3447;
	fma.rn.f64 	%fd3454, %fd5494, %fd3451, %fd3448;
	fma.rn.f64 	%fd3455, %fd5010, %fd3451, 0d0000000000000000;
	add.f64 	%fd3456, %fd3432, %fd3455;
	fma.rn.f64 	%fd3457, %fd5488, %fd3255, 0d0000000000000000;
	fma.rn.f64 	%fd3458, %fd5492, %fd3255, 0d0000000000000000;
	add.f64 	%fd3459, %fd3458, %fd3453;
	fma.rn.f64 	%fd3460, %fd5491, %fd3457, %fd3454;
	fma.rn.f64 	%fd3461, %fd5010, %fd3457, 0d0000000000000000;
	add.f64 	%fd3462, %fd3441, %fd3461;
	fma.rn.f64 	%fd3463, %fd5488, %fd3262, 0d0000000000000000;
	fma.rn.f64 	%fd3464, %fd5489, %fd3262, 0d0000000000000000;
	add.f64 	%fd3465, %fd3464, %fd3459;
	fma.rn.f64 	%fd3466, %fd5488, %fd3463, %fd3460;
	fma.rn.f64 	%fd3467, %fd5010, %fd3463, 0d0000000000000000;
	add.f64 	%fd3468, %fd3467, %fd3465;
	add.f64 	%fd3469, %fd3468, 0d0000000000000000;
	add.f64 	%fd3470, %fd3462, 0d0000000000000000;
	add.f64 	%fd3471, %fd3456, 0d0000000000000000;
	add.f64 	%fd3472, %fd3450, 0d0000000000000000;
	sub.f64 	%fd3473, %fd3162, %fd3469;
	add.f64 	%fd3474, %fd3472, %fd3473;
	add.f64 	%fd3475, %fd3471, %fd3473;
	add.f64 	%fd3476, %fd3470, %fd3473;
	fma.rn.f64 	%fd3477, %fd5451, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3478, %fd5450, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3479, %fd5451, %fd3477, %fd3466;
	fma.rn.f64 	%fd3480, %fd5010, %fd3477, 0d0000000000000000;
	add.f64 	%fd3481, %fd3478, %fd3480;
	fma.rn.f64 	%fd3482, %fd5451, %fd3167, 0d0000000000000000;
	fma.rn.f64 	%fd3483, %fd5447, %fd3167, 0d0000000000000000;
	add.f64 	%fd3484, %fd3483, %fd3481;
	fma.rn.f64 	%fd3485, %fd5446, %fd3482, %fd3479;
	fma.rn.f64 	%fd3486, %fd5010, %fd3482, 0d0000000000000000;
	fma.rn.f64 	%fd3487, %fd5451, %fd3173, 0d0000000000000000;
	fma.rn.f64 	%fd3488, %fd5444, %fd3173, 0d0000000000000000;
	add.f64 	%fd3489, %fd3488, %fd3484;
	fma.rn.f64 	%fd3490, %fd5443, %fd3487, %fd3485;
	fma.rn.f64 	%fd3491, %fd5010, %fd3487, 0d0000000000000000;
	fma.rn.f64 	%fd3492, %fd5451, %fd3179, 0d0000000000000000;
	fma.rn.f64 	%fd3493, %fd5441, %fd3179, 0d0000000000000000;
	add.f64 	%fd3494, %fd3493, %fd3489;
	fma.rn.f64 	%fd3495, %fd5440, %fd3492, %fd3490;
	fma.rn.f64 	%fd3496, %fd5010, %fd3492, 0d0000000000000000;
	fma.rn.f64 	%fd3497, %fd5446, %fd3185, 0d0000000000000000;
	fma.rn.f64 	%fd3498, %fd5450, %fd3185, 0d0000000000000000;
	add.f64 	%fd3499, %fd3486, %fd3498;
	fma.rn.f64 	%fd3500, %fd5451, %fd3497, %fd3495;
	fma.rn.f64 	%fd3501, %fd5010, %fd3497, 0d0000000000000000;
	add.f64 	%fd3502, %fd3494, %fd3501;
	fma.rn.f64 	%fd3503, %fd5446, %fd3192, 0d0000000000000000;
	fma.rn.f64 	%fd3504, %fd5447, %fd3192, 0d0000000000000000;
	add.f64 	%fd3505, %fd3499, %fd3504;
	fma.rn.f64 	%fd3506, %fd5446, %fd3503, %fd3500;
	fma.rn.f64 	%fd3507, %fd5010, %fd3503, 0d0000000000000000;
	add.f64 	%fd3508, %fd3505, %fd3507;
	fma.rn.f64 	%fd3509, %fd5446, %fd3199, 0d0000000000000000;
	fma.rn.f64 	%fd3510, %fd5444, %fd3199, 0d0000000000000000;
	add.f64 	%fd3511, %fd3510, %fd3508;
	fma.rn.f64 	%fd3512, %fd5443, %fd3509, %fd3506;
	fma.rn.f64 	%fd3513, %fd5010, %fd3509, 0d0000000000000000;
	add.f64 	%fd3514, %fd3491, %fd3513;
	fma.rn.f64 	%fd3515, %fd5446, %fd3206, 0d0000000000000000;
	fma.rn.f64 	%fd3516, %fd5441, %fd3206, 0d0000000000000000;
	add.f64 	%fd3517, %fd3516, %fd3511;
	fma.rn.f64 	%fd3518, %fd5440, %fd3515, %fd3512;
	fma.rn.f64 	%fd3519, %fd5010, %fd3515, 0d0000000000000000;
	add.f64 	%fd3520, %fd3496, %fd3519;
	fma.rn.f64 	%fd3521, %fd5443, %fd3213, 0d0000000000000000;
	fma.rn.f64 	%fd3522, %fd5450, %fd3213, 0d0000000000000000;
	add.f64 	%fd3523, %fd3522, %fd3514;
	fma.rn.f64 	%fd3524, %fd5451, %fd3521, %fd3518;
	fma.rn.f64 	%fd3525, %fd5010, %fd3521, 0d0000000000000000;
	add.f64 	%fd3526, %fd3502, %fd3525;
	fma.rn.f64 	%fd3527, %fd5443, %fd3220, 0d0000000000000000;
	fma.rn.f64 	%fd3528, %fd5447, %fd3220, 0d0000000000000000;
	add.f64 	%fd3529, %fd3528, %fd3523;
	fma.rn.f64 	%fd3530, %fd5446, %fd3527, %fd3524;
	fma.rn.f64 	%fd3531, %fd5010, %fd3527, 0d0000000000000000;
	add.f64 	%fd3532, %fd3517, %fd3531;
	fma.rn.f64 	%fd3533, %fd5443, %fd3227, 0d0000000000000000;
	fma.rn.f64 	%fd3534, %fd5444, %fd3227, 0d0000000000000000;
	add.f64 	%fd3535, %fd3534, %fd3529;
	fma.rn.f64 	%fd3536, %fd5443, %fd3533, %fd3530;
	fma.rn.f64 	%fd3537, %fd5010, %fd3533, 0d0000000000000000;
	add.f64 	%fd3538, %fd3535, %fd3537;
	fma.rn.f64 	%fd3539, %fd5443, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3540, %fd5441, %fd3234, 0d0000000000000000;
	add.f64 	%fd3541, %fd3540, %fd3538;
	fma.rn.f64 	%fd3542, %fd5440, %fd3539, %fd3536;
	fma.rn.f64 	%fd3543, %fd5010, %fd3539, 0d0000000000000000;
	add.f64 	%fd3544, %fd3520, %fd3543;
	fma.rn.f64 	%fd3545, %fd5440, %fd3241, 0d0000000000000000;
	fma.rn.f64 	%fd3546, %fd5450, %fd3241, 0d0000000000000000;
	add.f64 	%fd3547, %fd3546, %fd3544;
	fma.rn.f64 	%fd3548, %fd5451, %fd3545, %fd3542;
	fma.rn.f64 	%fd3549, %fd5010, %fd3545, 0d0000000000000000;
	add.f64 	%fd3550, %fd3526, %fd3549;
	fma.rn.f64 	%fd3551, %fd5440, %fd3248, 0d0000000000000000;
	fma.rn.f64 	%fd3552, %fd5447, %fd3248, 0d0000000000000000;
	add.f64 	%fd3553, %fd3552, %fd3547;
	fma.rn.f64 	%fd3554, %fd5446, %fd3551, %fd3548;
	fma.rn.f64 	%fd3555, %fd5010, %fd3551, 0d0000000000000000;
	add.f64 	%fd3556, %fd3532, %fd3555;
	fma.rn.f64 	%fd3557, %fd5440, %fd3255, 0d0000000000000000;
	fma.rn.f64 	%fd3558, %fd5444, %fd3255, 0d0000000000000000;
	add.f64 	%fd3559, %fd3558, %fd3553;
	fma.rn.f64 	%fd3560, %fd5443, %fd3557, %fd3554;
	fma.rn.f64 	%fd3561, %fd5010, %fd3557, 0d0000000000000000;
	add.f64 	%fd3562, %fd3541, %fd3561;
	fma.rn.f64 	%fd3563, %fd5440, %fd3262, 0d0000000000000000;
	fma.rn.f64 	%fd3564, %fd5441, %fd3262, 0d0000000000000000;
	add.f64 	%fd3565, %fd3564, %fd3559;
	fma.rn.f64 	%fd3566, %fd5440, %fd3563, %fd3560;
	fma.rn.f64 	%fd3567, %fd5010, %fd3563, 0d0000000000000000;
	add.f64 	%fd3568, %fd3567, %fd3565;
	add.f64 	%fd3569, %fd3568, 0d0000000000000000;
	add.f64 	%fd3570, %fd3562, 0d0000000000000000;
	add.f64 	%fd3571, %fd3556, 0d0000000000000000;
	add.f64 	%fd3572, %fd3550, 0d0000000000000000;
	sub.f64 	%fd3573, %fd3162, %fd3569;
	add.f64 	%fd3574, %fd3572, %fd3573;
	add.f64 	%fd3575, %fd3571, %fd3573;
	add.f64 	%fd3576, %fd3570, %fd3573;
	fma.rn.f64 	%fd3577, %fd5403, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3578, %fd5402, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3579, %fd5403, %fd3577, %fd3566;
	fma.rn.f64 	%fd3580, %fd5010, %fd3577, 0d0000000000000000;
	add.f64 	%fd3581, %fd3578, %fd3580;
	fma.rn.f64 	%fd3582, %fd5403, %fd3167, 0d0000000000000000;
	fma.rn.f64 	%fd3583, %fd5399, %fd3167, 0d0000000000000000;
	add.f64 	%fd3584, %fd3583, %fd3581;
	fma.rn.f64 	%fd3585, %fd5398, %fd3582, %fd3579;
	fma.rn.f64 	%fd3586, %fd5010, %fd3582, 0d0000000000000000;
	fma.rn.f64 	%fd3587, %fd5403, %fd3173, 0d0000000000000000;
	fma.rn.f64 	%fd3588, %fd5396, %fd3173, 0d0000000000000000;
	add.f64 	%fd3589, %fd3588, %fd3584;
	fma.rn.f64 	%fd3590, %fd5395, %fd3587, %fd3585;
	fma.rn.f64 	%fd3591, %fd5010, %fd3587, 0d0000000000000000;
	fma.rn.f64 	%fd3592, %fd5403, %fd3179, 0d0000000000000000;
	fma.rn.f64 	%fd3593, %fd5393, %fd3179, 0d0000000000000000;
	add.f64 	%fd3594, %fd3593, %fd3589;
	fma.rn.f64 	%fd3595, %fd5392, %fd3592, %fd3590;
	fma.rn.f64 	%fd3596, %fd5010, %fd3592, 0d0000000000000000;
	fma.rn.f64 	%fd3597, %fd5398, %fd3185, 0d0000000000000000;
	fma.rn.f64 	%fd3598, %fd5402, %fd3185, 0d0000000000000000;
	add.f64 	%fd3599, %fd3586, %fd3598;
	fma.rn.f64 	%fd3600, %fd5403, %fd3597, %fd3595;
	fma.rn.f64 	%fd3601, %fd5010, %fd3597, 0d0000000000000000;
	add.f64 	%fd3602, %fd3594, %fd3601;
	fma.rn.f64 	%fd3603, %fd5398, %fd3192, 0d0000000000000000;
	fma.rn.f64 	%fd3604, %fd5399, %fd3192, 0d0000000000000000;
	add.f64 	%fd3605, %fd3599, %fd3604;
	fma.rn.f64 	%fd3606, %fd5398, %fd3603, %fd3600;
	fma.rn.f64 	%fd3607, %fd5010, %fd3603, 0d0000000000000000;
	add.f64 	%fd3608, %fd3605, %fd3607;
	fma.rn.f64 	%fd3609, %fd5398, %fd3199, 0d0000000000000000;
	fma.rn.f64 	%fd3610, %fd5396, %fd3199, 0d0000000000000000;
	add.f64 	%fd3611, %fd3610, %fd3608;
	fma.rn.f64 	%fd3612, %fd5395, %fd3609, %fd3606;
	fma.rn.f64 	%fd3613, %fd5010, %fd3609, 0d0000000000000000;
	add.f64 	%fd3614, %fd3591, %fd3613;
	fma.rn.f64 	%fd3615, %fd5398, %fd3206, 0d0000000000000000;
	fma.rn.f64 	%fd3616, %fd5393, %fd3206, 0d0000000000000000;
	add.f64 	%fd3617, %fd3616, %fd3611;
	fma.rn.f64 	%fd3618, %fd5392, %fd3615, %fd3612;
	fma.rn.f64 	%fd3619, %fd5010, %fd3615, 0d0000000000000000;
	add.f64 	%fd3620, %fd3596, %fd3619;
	fma.rn.f64 	%fd3621, %fd5395, %fd3213, 0d0000000000000000;
	fma.rn.f64 	%fd3622, %fd5402, %fd3213, 0d0000000000000000;
	add.f64 	%fd3623, %fd3622, %fd3614;
	fma.rn.f64 	%fd3624, %fd5403, %fd3621, %fd3618;
	fma.rn.f64 	%fd3625, %fd5010, %fd3621, 0d0000000000000000;
	add.f64 	%fd3626, %fd3602, %fd3625;
	fma.rn.f64 	%fd3627, %fd5395, %fd3220, 0d0000000000000000;
	fma.rn.f64 	%fd3628, %fd5399, %fd3220, 0d0000000000000000;
	add.f64 	%fd3629, %fd3628, %fd3623;
	fma.rn.f64 	%fd3630, %fd5398, %fd3627, %fd3624;
	fma.rn.f64 	%fd3631, %fd5010, %fd3627, 0d0000000000000000;
	add.f64 	%fd3632, %fd3617, %fd3631;
	fma.rn.f64 	%fd3633, %fd5395, %fd3227, 0d0000000000000000;
	fma.rn.f64 	%fd3634, %fd5396, %fd3227, 0d0000000000000000;
	add.f64 	%fd3635, %fd3634, %fd3629;
	fma.rn.f64 	%fd3636, %fd5395, %fd3633, %fd3630;
	fma.rn.f64 	%fd3637, %fd5010, %fd3633, 0d0000000000000000;
	add.f64 	%fd3638, %fd3635, %fd3637;
	fma.rn.f64 	%fd3639, %fd5395, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3640, %fd5393, %fd3234, 0d0000000000000000;
	add.f64 	%fd3641, %fd3640, %fd3638;
	fma.rn.f64 	%fd3642, %fd5392, %fd3639, %fd3636;
	fma.rn.f64 	%fd3643, %fd5010, %fd3639, 0d0000000000000000;
	add.f64 	%fd3644, %fd3620, %fd3643;
	fma.rn.f64 	%fd3645, %fd5392, %fd3241, 0d0000000000000000;
	fma.rn.f64 	%fd3646, %fd5402, %fd3241, 0d0000000000000000;
	add.f64 	%fd3647, %fd3646, %fd3644;
	fma.rn.f64 	%fd3648, %fd5403, %fd3645, %fd3642;
	fma.rn.f64 	%fd3649, %fd5010, %fd3645, 0d0000000000000000;
	add.f64 	%fd3650, %fd3626, %fd3649;
	fma.rn.f64 	%fd3651, %fd5392, %fd3248, 0d0000000000000000;
	fma.rn.f64 	%fd3652, %fd5399, %fd3248, 0d0000000000000000;
	add.f64 	%fd3653, %fd3652, %fd3647;
	fma.rn.f64 	%fd3654, %fd5398, %fd3651, %fd3648;
	fma.rn.f64 	%fd3655, %fd5010, %fd3651, 0d0000000000000000;
	add.f64 	%fd3656, %fd3632, %fd3655;
	fma.rn.f64 	%fd3657, %fd5392, %fd3255, 0d0000000000000000;
	fma.rn.f64 	%fd3658, %fd5396, %fd3255, 0d0000000000000000;
	add.f64 	%fd3659, %fd3658, %fd3653;
	fma.rn.f64 	%fd3660, %fd5395, %fd3657, %fd3654;
	fma.rn.f64 	%fd3661, %fd5010, %fd3657, 0d0000000000000000;
	add.f64 	%fd3662, %fd3641, %fd3661;
	fma.rn.f64 	%fd3663, %fd5392, %fd3262, 0d0000000000000000;
	fma.rn.f64 	%fd3664, %fd5393, %fd3262, 0d0000000000000000;
	add.f64 	%fd3665, %fd3664, %fd3659;
	fma.rn.f64 	%fd3666, %fd5392, %fd3663, %fd3660;
	fma.rn.f64 	%fd3667, %fd5010, %fd3663, 0d0000000000000000;
	add.f64 	%fd3668, %fd3667, %fd3665;
	add.f64 	%fd3669, %fd3668, 0d0000000000000000;
	add.f64 	%fd3670, %fd3662, 0d0000000000000000;
	add.f64 	%fd3671, %fd3656, 0d0000000000000000;
	add.f64 	%fd3672, %fd3650, 0d0000000000000000;
	sub.f64 	%fd3673, %fd3162, %fd3669;
	add.f64 	%fd3674, %fd3672, %fd3673;
	add.f64 	%fd3675, %fd3671, %fd3673;
	add.f64 	%fd3676, %fd3670, %fd3673;
	fma.rn.f64 	%fd3677, %fd5355, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3678, %fd5354, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd3679, %fd5355, %fd3677, %fd3666;
	fma.rn.f64 	%fd3680, %fd5010, %fd3677, 0d0000000000000000;
	add.f64 	%fd3681, %fd3678, %fd3680;
	fma.rn.f64 	%fd3682, %fd5355, %fd3167, 0d0000000000000000;
	fma.rn.f64 	%fd3683, %fd5351, %fd3167, 0d0000000000000000;
	add.f64 	%fd3684, %fd3683, %fd3681;
	fma.rn.f64 	%fd3685, %fd5350, %fd3682, %fd3679;
	fma.rn.f64 	%fd3686, %fd5010, %fd3682, 0d0000000000000000;
	fma.rn.f64 	%fd3687, %fd5355, %fd3173, 0d0000000000000000;
	fma.rn.f64 	%fd3688, %fd5348, %fd3173, 0d0000000000000000;
	add.f64 	%fd3689, %fd3688, %fd3684;
	fma.rn.f64 	%fd3690, %fd5347, %fd3687, %fd3685;
	fma.rn.f64 	%fd3691, %fd5010, %fd3687, 0d0000000000000000;
	fma.rn.f64 	%fd3692, %fd5355, %fd3179, 0d0000000000000000;
	fma.rn.f64 	%fd3693, %fd5345, %fd3179, 0d0000000000000000;
	add.f64 	%fd3694, %fd3693, %fd3689;
	fma.rn.f64 	%fd3695, %fd5344, %fd3692, %fd3690;
	fma.rn.f64 	%fd3696, %fd5010, %fd3692, 0d0000000000000000;
	fma.rn.f64 	%fd3697, %fd5350, %fd3185, 0d0000000000000000;
	fma.rn.f64 	%fd3698, %fd5354, %fd3185, 0d0000000000000000;
	add.f64 	%fd3699, %fd3686, %fd3698;
	fma.rn.f64 	%fd3700, %fd5355, %fd3697, %fd3695;
	fma.rn.f64 	%fd3701, %fd5010, %fd3697, 0d0000000000000000;
	add.f64 	%fd3702, %fd3694, %fd3701;
	fma.rn.f64 	%fd3703, %fd5350, %fd3192, 0d0000000000000000;
	fma.rn.f64 	%fd3704, %fd5351, %fd3192, 0d0000000000000000;
	add.f64 	%fd3705, %fd3699, %fd3704;
	fma.rn.f64 	%fd3706, %fd5350, %fd3703, %fd3700;
	fma.rn.f64 	%fd3707, %fd5010, %fd3703, 0d0000000000000000;
	add.f64 	%fd3708, %fd3705, %fd3707;
	fma.rn.f64 	%fd3709, %fd5350, %fd3199, 0d0000000000000000;
	fma.rn.f64 	%fd3710, %fd5348, %fd3199, 0d0000000000000000;
	add.f64 	%fd3711, %fd3710, %fd3708;
	fma.rn.f64 	%fd3712, %fd5347, %fd3709, %fd3706;
	fma.rn.f64 	%fd3713, %fd5010, %fd3709, 0d0000000000000000;
	add.f64 	%fd3714, %fd3691, %fd3713;
	fma.rn.f64 	%fd3715, %fd5350, %fd3206, 0d0000000000000000;
	fma.rn.f64 	%fd3716, %fd5345, %fd3206, 0d0000000000000000;
	add.f64 	%fd3717, %fd3716, %fd3711;
	fma.rn.f64 	%fd3718, %fd5344, %fd3715, %fd3712;
	fma.rn.f64 	%fd3719, %fd5010, %fd3715, 0d0000000000000000;
	add.f64 	%fd3720, %fd3696, %fd3719;
	fma.rn.f64 	%fd3721, %fd5347, %fd3213, 0d0000000000000000;
	fma.rn.f64 	%fd3722, %fd5354, %fd3213, 0d0000000000000000;
	add.f64 	%fd3723, %fd3722, %fd3714;
	fma.rn.f64 	%fd3724, %fd5355, %fd3721, %fd3718;
	fma.rn.f64 	%fd3725, %fd5010, %fd3721, 0d0000000000000000;
	add.f64 	%fd3726, %fd3702, %fd3725;
	fma.rn.f64 	%fd3727, %fd5347, %fd3220, 0d0000000000000000;
	fma.rn.f64 	%fd3728, %fd5351, %fd3220, 0d0000000000000000;
	add.f64 	%fd3729, %fd3728, %fd3723;
	fma.rn.f64 	%fd3730, %fd5350, %fd3727, %fd3724;
	fma.rn.f64 	%fd3731, %fd5010, %fd3727, 0d0000000000000000;
	add.f64 	%fd3732, %fd3717, %fd3731;
	fma.rn.f64 	%fd3733, %fd5347, %fd3227, 0d0000000000000000;
	fma.rn.f64 	%fd3734, %fd5348, %fd3227, 0d0000000000000000;
	add.f64 	%fd3735, %fd3734, %fd3729;
	fma.rn.f64 	%fd3736, %fd5347, %fd3733, %fd3730;
	fma.rn.f64 	%fd3737, %fd5010, %fd3733, 0d0000000000000000;
	add.f64 	%fd3738, %fd3735, %fd3737;
	fma.rn.f64 	%fd3739, %fd5347, %fd3234, 0d0000000000000000;
	fma.rn.f64 	%fd3740, %fd5345, %fd3234, 0d0000000000000000;
	add.f64 	%fd3741, %fd3740, %fd3738;
	fma.rn.f64 	%fd3742, %fd5344, %fd3739, %fd3736;
	fma.rn.f64 	%fd3743, %fd5010, %fd3739, 0d0000000000000000;
	add.f64 	%fd3744, %fd3720, %fd3743;
	fma.rn.f64 	%fd3745, %fd5344, %fd3241, 0d0000000000000000;
	fma.rn.f64 	%fd3746, %fd5354, %fd3241, 0d0000000000000000;
	add.f64 	%fd3747, %fd3746, %fd3744;
	fma.rn.f64 	%fd3748, %fd5355, %fd3745, %fd3742;
	fma.rn.f64 	%fd3749, %fd5010, %fd3745, 0d0000000000000000;
	add.f64 	%fd3750, %fd3726, %fd3749;
	fma.rn.f64 	%fd3751, %fd5344, %fd3248, 0d0000000000000000;
	fma.rn.f64 	%fd3752, %fd5351, %fd3248, 0d0000000000000000;
	add.f64 	%fd3753, %fd3752, %fd3747;
	fma.rn.f64 	%fd3754, %fd5350, %fd3751, %fd3748;
	fma.rn.f64 	%fd3755, %fd5010, %fd3751, 0d0000000000000000;
	add.f64 	%fd3756, %fd3732, %fd3755;
	fma.rn.f64 	%fd3757, %fd5344, %fd3255, 0d0000000000000000;
	fma.rn.f64 	%fd3758, %fd5348, %fd3255, 0d0000000000000000;
	add.f64 	%fd3759, %fd3758, %fd3753;
	fma.rn.f64 	%fd3760, %fd5347, %fd3757, %fd3754;
	fma.rn.f64 	%fd3761, %fd5010, %fd3757, 0d0000000000000000;
	add.f64 	%fd3762, %fd3741, %fd3761;
	fma.rn.f64 	%fd3763, %fd5344, %fd3262, 0d0000000000000000;
	fma.rn.f64 	%fd3764, %fd5345, %fd3262, 0d0000000000000000;
	add.f64 	%fd3765, %fd3764, %fd3759;
	fma.rn.f64 	%fd3766, %fd5344, %fd3763, %fd3760;
	fma.rn.f64 	%fd3767, %fd5010, %fd3763, 0d0000000000000000;
	add.f64 	%fd3768, %fd3767, %fd3765;
	add.f64 	%fd3769, %fd3768, 0d0000000000000000;
	add.f64 	%fd3770, %fd3762, 0d0000000000000000;
	add.f64 	%fd3771, %fd3756, 0d0000000000000000;
	add.f64 	%fd3772, %fd3750, 0d0000000000000000;
	sub.f64 	%fd3773, %fd3162, %fd3769;
	add.f64 	%fd3774, %fd3772, %fd3773;
	add.f64 	%fd3775, %fd3771, %fd3773;
	add.f64 	%fd3776, %fd3770, %fd3773;
	div.rn.f64 	%fd3777, %fd3766, 0d4018000000000000;
	add.f64 	%fd3779, %fd3274, 0d0000000000000000;
	div.rn.f64 	%fd3780, %fd3779, 0d4018000000000000;
	add.f64 	%fd3781, %fd3780, 0d0000000000000000;
	sub.f64 	%fd3782, %fd3162, %fd3781;
	div.rn.f64 	%fd3783, %fd3779, 0d4008000000000000;
	add.f64 	%fd3784, %fd3783, 0d0000000000000000;
	fma.rn.f64 	%fd3785, %fd3784, 0d4000000000000000, 0d0000000000000000;
	sub.f64 	%fd3786, %fd3782, %fd3785;
	sub.f64 	%fd3787, %fd3786, %fd3781;
	add.f64 	%fd3788, %fd3374, 0d0000000000000000;
	add.f64 	%fd3789, %fd3788, %fd3779;
	div.rn.f64 	%fd3790, %fd3788, 0d4008000000000000;
	add.f64 	%fd3791, %fd3790, 0d0000000000000000;
	fma.rn.f64 	%fd3792, %fd3791, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3793, %fd3781, %fd3792;
	sub.f64 	%fd3794, %fd3787, %fd3792;
	div.rn.f64 	%fd3795, %fd3788, 0d4018000000000000;
	add.f64 	%fd3796, %fd3795, 0d0000000000000000;
	add.f64 	%fd3797, %fd3785, %fd3796;
	sub.f64 	%fd3798, %fd3794, %fd3796;
	fma.rn.f64 	%fd3799, %fd3788, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3800, %fd3781, %fd3799;
	sub.f64 	%fd3801, %fd3798, %fd3799;
	add.f64 	%fd3802, %fd3474, 0d0000000000000000;
	add.f64 	%fd3803, %fd3802, %fd3789;
	div.rn.f64 	%fd3804, %fd3802, 0d4018000000000000;
	add.f64 	%fd3805, %fd3804, 0d0000000000000000;
	add.f64 	%fd3806, %fd3793, %fd3805;
	sub.f64 	%fd3807, %fd3801, %fd3805;
	add.f64 	%fd3808, %fd3797, %fd3805;
	sub.f64 	%fd3809, %fd3807, %fd3805;
	div.rn.f64 	%fd3810, %fd3802, 0d4008000000000000;
	add.f64 	%fd3811, %fd3810, 0d0000000000000000;
	fma.rn.f64 	%fd3812, %fd3811, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3813, %fd3800, %fd3812;
	sub.f64 	%fd3814, %fd3809, %fd3812;
	add.f64 	%fd3815, %fd3574, 0d0000000000000000;
	sub.f64 	%fd3816, %fd3162, %fd3815;
	add.f64 	%fd3817, %fd3816, %fd3803;
	div.rn.f64 	%fd3818, %fd3815, 0d4018000000000000;
	add.f64 	%fd3819, %fd3818, 0d0000000000000000;
	add.f64 	%fd3820, %fd3806, %fd3819;
	sub.f64 	%fd3821, %fd3814, %fd3819;
	div.rn.f64 	%fd3822, %fd3815, 0d4008000000000000;
	add.f64 	%fd3823, %fd3822, 0d0000000000000000;
	fma.rn.f64 	%fd3824, %fd3823, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3825, %fd3808, %fd3824;
	sub.f64 	%fd3826, %fd3821, %fd3824;
	add.f64 	%fd3827, %fd3813, %fd3819;
	sub.f64 	%fd3828, %fd3826, %fd3819;
	add.f64 	%fd3829, %fd3674, 0d0000000000000000;
	sub.f64 	%fd3830, %fd3162, %fd3829;
	add.f64 	%fd3831, %fd3830, %fd3817;
	div.rn.f64 	%fd3832, %fd3829, 0d4008000000000000;
	add.f64 	%fd3833, %fd3832, 0d0000000000000000;
	fma.rn.f64 	%fd3834, %fd3833, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3835, %fd3820, %fd3834;
	sub.f64 	%fd3836, %fd3828, %fd3834;
	div.rn.f64 	%fd3837, %fd3829, 0d4018000000000000;
	add.f64 	%fd3838, %fd3837, 0d0000000000000000;
	add.f64 	%fd3839, %fd3825, %fd3838;
	sub.f64 	%fd3840, %fd3836, %fd3838;
	fma.rn.f64 	%fd3841, %fd3829, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3842, %fd3827, %fd3841;
	sub.f64 	%fd3843, %fd3840, %fd3841;
	add.f64 	%fd3844, %fd3774, 0d0000000000000000;
	sub.f64 	%fd3845, %fd3162, %fd3844;
	add.f64 	%fd3846, %fd3831, %fd3845;
	div.rn.f64 	%fd3847, %fd3844, 0d4018000000000000;
	add.f64 	%fd3848, %fd3847, 0d0000000000000000;
	sub.f64 	%fd3849, %fd3843, %fd3848;
	sub.f64 	%fd3850, %fd3849, %fd3848;
	div.rn.f64 	%fd3851, %fd3844, 0d4008000000000000;
	add.f64 	%fd3852, %fd3851, 0d0000000000000000;
	fma.rn.f64 	%fd3853, %fd3852, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3854, %fd3275, 0d0000000000000000;
	div.rn.f64 	%fd3855, %fd3854, 0d4018000000000000;
	add.f64 	%fd3856, %fd3855, 0d0000000000000000;
	sub.f64 	%fd3857, %fd3162, %fd3856;
	div.rn.f64 	%fd3858, %fd3854, 0d4008000000000000;
	add.f64 	%fd3859, %fd3858, 0d0000000000000000;
	fma.rn.f64 	%fd3860, %fd3859, 0d4000000000000000, 0d0000000000000000;
	sub.f64 	%fd3861, %fd3857, %fd3860;
	sub.f64 	%fd3862, %fd3861, %fd3856;
	add.f64 	%fd3863, %fd3375, 0d0000000000000000;
	add.f64 	%fd3864, %fd3863, %fd3854;
	div.rn.f64 	%fd3865, %fd3863, 0d4008000000000000;
	add.f64 	%fd3866, %fd3865, 0d0000000000000000;
	fma.rn.f64 	%fd3867, %fd3866, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3868, %fd3856, %fd3867;
	sub.f64 	%fd3869, %fd3862, %fd3867;
	div.rn.f64 	%fd3870, %fd3863, 0d4018000000000000;
	add.f64 	%fd3871, %fd3870, 0d0000000000000000;
	add.f64 	%fd3872, %fd3860, %fd3871;
	sub.f64 	%fd3873, %fd3869, %fd3871;
	fma.rn.f64 	%fd3874, %fd3863, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3875, %fd3856, %fd3874;
	sub.f64 	%fd3876, %fd3873, %fd3874;
	add.f64 	%fd3877, %fd3475, 0d0000000000000000;
	add.f64 	%fd3878, %fd3877, %fd3864;
	div.rn.f64 	%fd3879, %fd3877, 0d4018000000000000;
	add.f64 	%fd3880, %fd3879, 0d0000000000000000;
	add.f64 	%fd3881, %fd3868, %fd3880;
	sub.f64 	%fd3882, %fd3876, %fd3880;
	add.f64 	%fd3883, %fd3872, %fd3880;
	sub.f64 	%fd3884, %fd3882, %fd3880;
	div.rn.f64 	%fd3885, %fd3877, 0d4008000000000000;
	add.f64 	%fd3886, %fd3885, 0d0000000000000000;
	fma.rn.f64 	%fd3887, %fd3886, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3888, %fd3875, %fd3887;
	sub.f64 	%fd3889, %fd3884, %fd3887;
	add.f64 	%fd3890, %fd3575, 0d0000000000000000;
	sub.f64 	%fd3891, %fd3162, %fd3890;
	add.f64 	%fd3892, %fd3891, %fd3878;
	div.rn.f64 	%fd3893, %fd3890, 0d4018000000000000;
	add.f64 	%fd3894, %fd3893, 0d0000000000000000;
	add.f64 	%fd3895, %fd3881, %fd3894;
	sub.f64 	%fd3896, %fd3889, %fd3894;
	div.rn.f64 	%fd3897, %fd3890, 0d4008000000000000;
	add.f64 	%fd3898, %fd3897, 0d0000000000000000;
	fma.rn.f64 	%fd3899, %fd3898, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3900, %fd3883, %fd3899;
	sub.f64 	%fd3901, %fd3896, %fd3899;
	add.f64 	%fd3902, %fd3888, %fd3894;
	sub.f64 	%fd3903, %fd3901, %fd3894;
	add.f64 	%fd3904, %fd3675, 0d0000000000000000;
	sub.f64 	%fd3905, %fd3162, %fd3904;
	add.f64 	%fd3906, %fd3905, %fd3892;
	div.rn.f64 	%fd3907, %fd3904, 0d4008000000000000;
	add.f64 	%fd3908, %fd3907, 0d0000000000000000;
	fma.rn.f64 	%fd3909, %fd3908, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3910, %fd3895, %fd3909;
	sub.f64 	%fd3911, %fd3903, %fd3909;
	div.rn.f64 	%fd3912, %fd3904, 0d4018000000000000;
	add.f64 	%fd3913, %fd3912, 0d0000000000000000;
	add.f64 	%fd3914, %fd3900, %fd3913;
	sub.f64 	%fd3915, %fd3911, %fd3913;
	fma.rn.f64 	%fd3916, %fd3904, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3917, %fd3902, %fd3916;
	sub.f64 	%fd3918, %fd3915, %fd3916;
	add.f64 	%fd3919, %fd3775, 0d0000000000000000;
	sub.f64 	%fd3920, %fd3162, %fd3919;
	add.f64 	%fd3921, %fd3920, %fd3906;
	div.rn.f64 	%fd3922, %fd3919, 0d4018000000000000;
	add.f64 	%fd3923, %fd3922, 0d0000000000000000;
	sub.f64 	%fd3924, %fd3918, %fd3923;
	sub.f64 	%fd3925, %fd3924, %fd3923;
	div.rn.f64 	%fd3926, %fd3919, 0d4008000000000000;
	add.f64 	%fd3927, %fd3926, 0d0000000000000000;
	fma.rn.f64 	%fd3928, %fd3927, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3929, %fd3276, 0d0000000000000000;
	div.rn.f64 	%fd3930, %fd3929, 0d4018000000000000;
	add.f64 	%fd3931, %fd3930, 0d0000000000000000;
	sub.f64 	%fd3932, %fd3162, %fd3931;
	div.rn.f64 	%fd3933, %fd3929, 0d4008000000000000;
	add.f64 	%fd3934, %fd3933, 0d0000000000000000;
	fma.rn.f64 	%fd3935, %fd3934, 0d4000000000000000, 0d0000000000000000;
	sub.f64 	%fd3936, %fd3932, %fd3935;
	sub.f64 	%fd3937, %fd3936, %fd3931;
	add.f64 	%fd3938, %fd3376, 0d0000000000000000;
	add.f64 	%fd3939, %fd3938, %fd3929;
	div.rn.f64 	%fd3940, %fd3938, 0d4008000000000000;
	add.f64 	%fd3941, %fd3940, 0d0000000000000000;
	fma.rn.f64 	%fd3942, %fd3941, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3943, %fd3931, %fd3942;
	sub.f64 	%fd3944, %fd3937, %fd3942;
	div.rn.f64 	%fd3945, %fd3938, 0d4018000000000000;
	add.f64 	%fd3946, %fd3945, 0d0000000000000000;
	add.f64 	%fd3947, %fd3935, %fd3946;
	sub.f64 	%fd3948, %fd3944, %fd3946;
	fma.rn.f64 	%fd3949, %fd3938, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3950, %fd3931, %fd3949;
	sub.f64 	%fd3951, %fd3948, %fd3949;
	add.f64 	%fd3952, %fd3476, 0d0000000000000000;
	add.f64 	%fd3953, %fd3952, %fd3939;
	div.rn.f64 	%fd3954, %fd3952, 0d4018000000000000;
	add.f64 	%fd3955, %fd3954, 0d0000000000000000;
	add.f64 	%fd3956, %fd3943, %fd3955;
	sub.f64 	%fd3957, %fd3951, %fd3955;
	add.f64 	%fd3958, %fd3947, %fd3955;
	sub.f64 	%fd3959, %fd3957, %fd3955;
	div.rn.f64 	%fd3960, %fd3952, 0d4008000000000000;
	add.f64 	%fd3961, %fd3960, 0d0000000000000000;
	fma.rn.f64 	%fd3962, %fd3961, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3963, %fd3950, %fd3962;
	sub.f64 	%fd3964, %fd3959, %fd3962;
	add.f64 	%fd3965, %fd3576, 0d0000000000000000;
	sub.f64 	%fd3966, %fd3162, %fd3965;
	add.f64 	%fd3967, %fd3966, %fd3953;
	div.rn.f64 	%fd3968, %fd3965, 0d4018000000000000;
	add.f64 	%fd3969, %fd3968, 0d0000000000000000;
	add.f64 	%fd3970, %fd3956, %fd3969;
	sub.f64 	%fd3971, %fd3964, %fd3969;
	div.rn.f64 	%fd3972, %fd3965, 0d4008000000000000;
	add.f64 	%fd3973, %fd3972, 0d0000000000000000;
	fma.rn.f64 	%fd3974, %fd3973, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3975, %fd3958, %fd3974;
	sub.f64 	%fd3976, %fd3971, %fd3974;
	add.f64 	%fd3977, %fd3963, %fd3969;
	sub.f64 	%fd3978, %fd3976, %fd3969;
	add.f64 	%fd3979, %fd3676, 0d0000000000000000;
	sub.f64 	%fd3980, %fd3162, %fd3979;
	add.f64 	%fd3981, %fd3980, %fd3967;
	div.rn.f64 	%fd3982, %fd3979, 0d4008000000000000;
	add.f64 	%fd3983, %fd3982, 0d0000000000000000;
	fma.rn.f64 	%fd3984, %fd3983, 0d4000000000000000, 0d0000000000000000;
	add.f64 	%fd3985, %fd3970, %fd3984;
	sub.f64 	%fd3986, %fd3978, %fd3984;
	div.rn.f64 	%fd3987, %fd3979, 0d4018000000000000;
	add.f64 	%fd3988, %fd3987, 0d0000000000000000;
	add.f64 	%fd3989, %fd3975, %fd3988;
	sub.f64 	%fd3990, %fd3986, %fd3988;
	fma.rn.f64 	%fd3991, %fd3979, 0d4018000000000000, 0d0000000000000000;
	add.f64 	%fd3992, %fd3977, %fd3991;
	sub.f64 	%fd3993, %fd3990, %fd3991;
	add.f64 	%fd3994, %fd3776, 0d0000000000000000;
	sub.f64 	%fd3995, %fd3162, %fd3994;
	add.f64 	%fd3996, %fd3995, %fd3981;
	div.rn.f64 	%fd3997, %fd3994, 0d4018000000000000;
	add.f64 	%fd3998, %fd3997, 0d0000000000000000;
	sub.f64 	%fd3999, %fd3993, %fd3998;
	sub.f64 	%fd4000, %fd3999, %fd3998;
	div.rn.f64 	%fd4001, %fd3994, 0d4008000000000000;
	add.f64 	%fd4002, %fd4001, 0d0000000000000000;
	fma.rn.f64 	%fd4003, %fd4002, 0d4000000000000000, 0d0000000000000000;
	div.rn.f64 	%fd4004, %fd3996, 0d400BB67AE8584CAA;
	add.f64 	%fd4005, %fd4004, 0d0000000000000000;
	div.rn.f64 	%fd4006, %fd3921, 0d400BB67AE8584CAA;
	add.f64 	%fd4007, %fd4006, 0d0000000000000000;
	div.rn.f64 	%fd4008, %fd3846, 0d400BB67AE8584CAA;
	add.f64 	%fd4009, %fd4008, 0d0000000000000000;
	add.f64 	%fd4010, %fd3835, %fd3848;
	add.f64 	%fd4011, %fd3842, %fd3853;
	add.f64 	%fd4012, %fd3914, %fd3923;
	add.f64 	%fd5641, %fd4010, 0d0000000000000000;
	add.f64 	%fd5647, %fd4011, 0d0000000000000000;
	add.f64 	%fd5643, %fd4012, 0d0000000000000000;
	fma.rn.f64 	%fd4013, %fd5043, %fd4005, 0d0000000000000000;
	fma.rn.f64 	%fd4014, %fd5043, %fd4007, 0d0000000000000000;
	fma.rn.f64 	%fd4015, %fd5043, %fd4009, 0d0000000000000000;
	add.f64 	%fd4016, %fd3910, %fd3923;
	add.f64 	%fd5640, %fd4016, 0d0000000000000000;
	mul.f64 	%fd4017, %fd5012, %fd4007;
	fma.rn.f64 	%fd4018, %fd5011, %fd4005, %fd4017;
	fma.rn.f64 	%fd1966, %fd5013, %fd4009, %fd4018;
	mul.f64 	%fd4019, %fd5016, %fd4014;
	fma.rn.f64 	%fd4020, %fd5015, %fd4013, %fd4019;
	fma.rn.f64 	%fd4021, %fd5017, %fd4015, %fd4020;
	mul.f64 	%fd4022, %fd5307, %fd5307;
	div.rn.f64 	%fd1967, %fd4021, %fd4022;
	div.rn.f64 	%fd4023, %fd4013, %fd5307;
	add.f64 	%fd5638, %fd4023, 0d0000000000000000;
	div.rn.f64 	%fd4024, %fd4014, %fd5307;
	add.f64 	%fd5637, %fd4024, 0d0000000000000000;
	div.rn.f64 	%fd4025, %fd4015, %fd5307;
	add.f64 	%fd5636, %fd4025, 0d0000000000000000;
	add.f64 	%fd1971, %fd3839, %fd3848;
	sub.f64 	%fd5650, %fd3850, %fd3853;
	add.f64 	%fd1973, %fd3917, %fd3928;
	sub.f64 	%fd5649, %fd3925, %fd3928;
	add.f64 	%fd5639, %fd3985, %fd3998;
	add.f64 	%fd5642, %fd3989, %fd3998;
	add.f64 	%fd5645, %fd3992, %fd4003;
	sub.f64 	%fd5648, %fd4000, %fd4003;
	setp.leu.f64 	%p405, %fd5307, 0d0000000000000000;
	@%p405 bra 	$L__BB43_253;

	sub.f64 	%fd4027, %fd3162, %fd1967;
	div.rn.f64 	%fd4028, %fd5015, %fd5307;
	div.rn.f64 	%fd4029, %fd5016, %fd5307;
	div.rn.f64 	%fd4030, %fd5017, %fd5307;
	fma.rn.f64 	%fd5638, %fd4028, %fd4027, %fd5638;
	fma.rn.f64 	%fd5637, %fd4029, %fd4027, %fd5637;
	fma.rn.f64 	%fd5636, %fd4030, %fd4027, %fd5636;

$L__BB43_253:
	add.f64 	%fd4150, %fd3777, 0d0000000000000000;
	fma.rn.f64 	%fd5656, %fd5014, %fd4150, 0d0000000000000000;
	fma.rn.f64 	%fd4148, %fd5044, %fd4150, 0d0000000000000000;
	add.f64 	%fd4031, %fd1966, 0d0000000000000000;
	fma.rn.f64 	%fd5651, %fd5046, %fd4148, %fd4031;
	fma.rn.f64 	%fd5652, %fd5043, %fd4148, 0d0000000000000000;
	add.f64 	%fd5657, %fd5638, 0d0000000000000000;
	add.f64 	%fd5658, %fd5637, 0d0000000000000000;
	add.f64 	%fd5659, %fd5636, 0d0000000000000000;
	add.f64 	%fd5646, %fd1973, 0d0000000000000000;
	add.f64 	%fd5644, %fd1971, 0d0000000000000000;
	bra.uni 	$L__BB43_254;

$L__BB43_141:
	setp.gt.s32 	%p224, %r50, -1;
	@%p224 bra 	$L__BB43_145;

	mov.f64 	%fd2507, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2508, %fd2507;
	setp.eq.f64 	%p225, %fd2508, 0d4000000000000000;
	@%p225 bra 	$L__BB43_145;

	mov.f64 	%fd5614, 0dFFF8000000000000;

$L__BB43_145:
	add.f64 	%fd2510, %fd5247, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r522}, %fd2510;
	}
	and.b32  	%r523, %r522, 2146435072;
	setp.ne.s32 	%p228, %r523, 2146435072;
	@%p228 bra 	$L__BB43_152;

	setp.gtu.f64 	%p229, %fd1798, 0d7FF0000000000000;
	@%p229 bra 	$L__BB43_151;
	bra.uni 	$L__BB43_147;

$L__BB43_151:
	mov.f64 	%fd2512, 0d4000000000000000;
	add.rn.f64 	%fd5614, %fd5247, %fd2512;
	bra.uni 	$L__BB43_152;

$L__BB43_147:
	mov.f64 	%fd2511, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r524, %temp}, %fd2511;
	}
	and.b32  	%r51, %r48, 2147483647;
	setp.eq.s32 	%p230, %r51, 2146435072;
	setp.eq.s32 	%p231, %r524, 0;
	and.pred  	%p232, %p230, %p231;
	@%p232 bra 	$L__BB43_150;
	bra.uni 	$L__BB43_148;

$L__BB43_150:
	setp.gt.f64 	%p239, %fd1798, 0d3FF0000000000000;
	selp.b32 	%r531, 2146435072, 0, %p239;
	mov.u32 	%r532, 0;
	xor.b32  	%r533, %r531, 2146435072;
	setp.lt.s32 	%p240, %r48, 0;
	selp.b32 	%r534, %r533, %r531, %p240;
	setp.eq.f64 	%p241, %fd5247, 0dBFF0000000000000;
	selp.b32 	%r535, 1072693248, %r534, %p241;
	mov.b64 	%fd5614, {%r532, %r535};
	bra.uni 	$L__BB43_152;

$L__BB43_148:
	{
	.reg .b32 %temp;
	mov.b64 	{%r525, %temp}, %fd5247;
	}
	and.b32  	%r526, %r50, 2147483647;
	setp.ne.s32 	%p233, %r526, 2146435072;
	setp.ne.s32 	%p234, %r525, 0;
	or.pred  	%p235, %p233, %p234;
	@%p235 bra 	$L__BB43_152;

	setp.gt.s32 	%p236, %r48, -1;
	selp.b32 	%r527, 2146435072, 0, %p236;
	mov.u32 	%r528, 0;
	setp.ne.s32 	%p237, %r51, 1071644672;
	and.pred  	%p238, %p237, %p9;
	or.b32  	%r529, %r527, -2147483648;
	selp.b32 	%r530, %r529, %r527, %p238;
	mov.b64 	%fd5614, {%r528, %r530};

$L__BB43_152:
	mul.f64 	%fd2513, %fd5614, 0d4008000000000000;
	setp.eq.f64 	%p242, %fd5247, 0d3FF0000000000000;
	selp.f64 	%fd2514, 0d4008000000000000, %fd2513, %p242;
	div.rn.f64 	%fd2515, %fd1796, 0d4008000000000000;
	add.f64 	%fd2516, %fd2515, 0d0000000000000000;
	mov.f64 	%fd2517, 0d0000000000000000;
	fma.rn.f64 	%fd2518, %fd2516, %fd2514, 0d0000000000000000;
	add.f64 	%fd2519, %fd1793, %fd2518;
	fma.rn.f64 	%fd2520, %fd5018, %fd1787, 0d0000000000000000;
	fma.rn.f64 	%fd2521, %fd5267, %fd1787, %fd1795;
	fma.rn.f64 	%fd2522, %fd5072, %fd2520, 0d0000000000000000;
	fma.rn.f64 	%fd1808, %fd5265, %fd2520, 0d0000000000000000;
	fma.rn.f64 	%fd2523, %fd5264, %fd2522, 0d0000000000000000;
	fma.rn.f64 	%fd2524, %fd5263, %fd2522, 0d0000000000000000;
	sub.f64 	%fd2525, %fd2517, %fd2524;
	fma.rn.f64 	%fd2526, %fd2525, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2527, %fd2526, %fd1792;
	add.f64 	%fd2528, %fd2526, %fd2519;
	div.rn.f64 	%fd2529, %fd2525, 0d4008000000000000;
	add.f64 	%fd2530, %fd2529, 0d0000000000000000;
	add.f64 	%fd2531, %fd2530, %fd1791;
	fma.rn.f64 	%fd2532, %fd5261, %fd2523, 0d0000000000000000;
	fma.rn.f64 	%fd2533, %fd5261, %fd2523, 0d0000000000000000;
	add.f64 	%fd2534, %fd2533, %fd2531;
	add.f64 	%fd2535, %fd2532, %fd2534;
	fma.rn.f64 	%fd2536, %fd5018, %fd1788, 0d0000000000000000;
	fma.rn.f64 	%fd2537, %fd5260, %fd1788, %fd2521;
	fma.rn.f64 	%fd2538, %fd5065, %fd2536, 0d0000000000000000;
	fma.rn.f64 	%fd1809, %fd5258, %fd2536, 0d0000000000000000;
	fma.rn.f64 	%fd2539, %fd5257, %fd2538, 0d0000000000000000;
	fma.rn.f64 	%fd2540, %fd5256, %fd2538, 0d0000000000000000;
	sub.f64 	%fd2541, %fd2517, %fd2540;
	fma.rn.f64 	%fd2542, %fd2541, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2543, %fd2542, %fd2535;
	add.f64 	%fd2544, %fd2542, %fd2528;
	div.rn.f64 	%fd2545, %fd2541, 0d4008000000000000;
	add.f64 	%fd2546, %fd2545, 0d0000000000000000;
	add.f64 	%fd2547, %fd2546, %fd2527;
	fma.rn.f64 	%fd2548, %fd5254, %fd2539, 0d0000000000000000;
	fma.rn.f64 	%fd2549, %fd5254, %fd2539, 0d0000000000000000;
	add.f64 	%fd2550, %fd2549, %fd2547;
	add.f64 	%fd2551, %fd2548, %fd2550;
	fma.rn.f64 	%fd2552, %fd5018, %fd1789, 0d0000000000000000;
	fma.rn.f64 	%fd2553, %fd5253, %fd1789, %fd2537;
	fma.rn.f64 	%fd2554, %fd5051, %fd2552, 0d0000000000000000;
	fma.rn.f64 	%fd2555, %fd5251, %fd2552, 0d0000000000000000;
	add.f64 	%fd2556, %fd2555, %fd1797;
	fma.rn.f64 	%fd2557, %fd5250, %fd2554, 0d0000000000000000;
	fma.rn.f64 	%fd2558, %fd5249, %fd2554, 0d0000000000000000;
	sub.f64 	%fd2559, %fd2517, %fd2558;
	fma.rn.f64 	%fd2560, %fd2559, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1810, %fd2560, %fd2543;
	add.f64 	%fd1811, %fd2560, %fd2551;
	div.rn.f64 	%fd2561, %fd2559, 0d4008000000000000;
	add.f64 	%fd2562, %fd2561, 0d0000000000000000;
	add.f64 	%fd2563, %fd2562, %fd2544;
	fma.rn.f64 	%fd2564, %fd5247, %fd2557, 0d0000000000000000;
	fma.rn.f64 	%fd2565, %fd5247, %fd2557, 0d0000000000000000;
	add.f64 	%fd2566, %fd2565, %fd2563;
	add.f64 	%fd1812, %fd2564, %fd2566;
	fma.rn.f64 	%fd2567, %fd5018, %fd1783, 0d0000000000000000;
	fma.rn.f64 	%fd1813, %fd5246, %fd1783, %fd2553;
	fma.rn.f64 	%fd1814, %fd5051, %fd2567, 0d0000000000000000;
	fma.rn.f64 	%fd2568, %fd5244, %fd2567, 0d0000000000000000;
	add.f64 	%fd1815, %fd2568, %fd2556;
	abs.f64 	%fd1816, %fd5243;
	{ // callseq 72, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1816;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5617, [retval0+0];
	} // callseq 72
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r52}, %fd5243;
	}
	setp.lt.s32 	%p243, %r52, 0;
	and.pred  	%p10, %p243, %p220;
	not.pred 	%p245, %p10;
	@%p245 bra 	$L__BB43_154;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r536}, %fd5617;
	}
	xor.b32  	%r537, %r536, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r538, %temp}, %fd5617;
	}
	mov.b64 	%fd5617, {%r538, %r537};

$L__BB43_154:
	setp.eq.f64 	%p246, %fd5243, 0d0000000000000000;
	@%p246 bra 	$L__BB43_158;
	bra.uni 	$L__BB43_155;

$L__BB43_158:
	selp.b32 	%r539, %r52, 0, %p220;
	mov.u32 	%r540, 0;
	or.b32  	%r541, %r539, 2146435072;
	setp.lt.s32 	%p250, %r48, 0;
	selp.b32 	%r542, %r541, %r539, %p250;
	mov.b64 	%fd5617, {%r540, %r542};
	bra.uni 	$L__BB43_159;

$L__BB43_155:
	setp.gt.s32 	%p247, %r52, -1;
	@%p247 bra 	$L__BB43_159;

	mov.f64 	%fd2569, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2570, %fd2569;
	setp.eq.f64 	%p248, %fd2570, 0d4000000000000000;
	@%p248 bra 	$L__BB43_159;

	mov.f64 	%fd5617, 0dFFF8000000000000;

$L__BB43_159:
	add.f64 	%fd2572, %fd5243, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r543}, %fd2572;
	}
	and.b32  	%r544, %r543, 2146435072;
	setp.ne.s32 	%p251, %r544, 2146435072;
	@%p251 bra 	$L__BB43_166;

	setp.gtu.f64 	%p252, %fd1816, 0d7FF0000000000000;
	@%p252 bra 	$L__BB43_165;
	bra.uni 	$L__BB43_161;

$L__BB43_165:
	mov.f64 	%fd2574, 0d4000000000000000;
	add.rn.f64 	%fd5617, %fd5243, %fd2574;
	bra.uni 	$L__BB43_166;

$L__BB43_161:
	mov.f64 	%fd2573, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r545, %temp}, %fd2573;
	}
	and.b32  	%r53, %r48, 2147483647;
	setp.eq.s32 	%p253, %r53, 2146435072;
	setp.eq.s32 	%p254, %r545, 0;
	and.pred  	%p255, %p253, %p254;
	@%p255 bra 	$L__BB43_164;
	bra.uni 	$L__BB43_162;

$L__BB43_164:
	setp.gt.f64 	%p262, %fd1816, 0d3FF0000000000000;
	selp.b32 	%r552, 2146435072, 0, %p262;
	mov.u32 	%r553, 0;
	xor.b32  	%r554, %r552, 2146435072;
	setp.lt.s32 	%p263, %r48, 0;
	selp.b32 	%r555, %r554, %r552, %p263;
	setp.eq.f64 	%p264, %fd5243, 0dBFF0000000000000;
	selp.b32 	%r556, 1072693248, %r555, %p264;
	mov.b64 	%fd5617, {%r553, %r556};
	bra.uni 	$L__BB43_166;

$L__BB43_162:
	{
	.reg .b32 %temp;
	mov.b64 	{%r546, %temp}, %fd5243;
	}
	and.b32  	%r547, %r52, 2147483647;
	setp.ne.s32 	%p256, %r547, 2146435072;
	setp.ne.s32 	%p257, %r546, 0;
	or.pred  	%p258, %p256, %p257;
	@%p258 bra 	$L__BB43_166;

	setp.gt.s32 	%p259, %r48, -1;
	selp.b32 	%r548, 2146435072, 0, %p259;
	mov.u32 	%r549, 0;
	setp.ne.s32 	%p260, %r53, 1071644672;
	and.pred  	%p261, %p260, %p10;
	or.b32  	%r550, %r548, -2147483648;
	selp.b32 	%r551, %r550, %r548, %p261;
	mov.b64 	%fd5617, {%r549, %r551};

$L__BB43_166:
	mul.f64 	%fd2575, %fd5617, 0d4008000000000000;
	setp.eq.f64 	%p265, %fd5243, 0d3FF0000000000000;
	selp.f64 	%fd2576, 0d4008000000000000, %fd2575, %p265;
	div.rn.f64 	%fd2577, %fd1814, 0d4008000000000000;
	add.f64 	%fd2578, %fd2577, 0d0000000000000000;
	mov.f64 	%fd2579, 0d0000000000000000;
	sub.f64 	%fd2580, %fd2579, %fd2578;
	fma.rn.f64 	%fd2581, %fd2580, %fd2576, 0d0000000000000000;
	sub.f64 	%fd2582, %fd2579, %fd2581;
	add.f64 	%fd2583, %fd1810, %fd2582;
	add.f64 	%fd2584, %fd1811, %fd2582;
	add.f64 	%fd2585, %fd1812, %fd2582;
	fma.rn.f64 	%fd2586, %fd1813, 0d3FE0AAAAA0000000, 0d0000000000000000;
	add.f64 	%fd1826, %fd2585, 0d0000000000000000;
	add.f64 	%fd1827, %fd2584, 0d0000000000000000;
	add.f64 	%fd1828, %fd2583, 0d0000000000000000;
	fma.rn.f64 	%fd1829, %fd5044, %fd2586, 0d0000000000000000;
	fma.rn.f64 	%fd1830, %fd5046, %fd2586, 0d0000000000000000;
	fma.rn.f64 	%fd2587, %fd5018, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2588, %fd5242, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2589, %fd5051, %fd2587, 0d0000000000000000;
	fma.rn.f64 	%fd2590, %fd5240, %fd2587, 0d0000000000000000;
	add.f64 	%fd2591, %fd2590, %fd1815;
	fma.rn.f64 	%fd2592, %fd5197, %fd2589, 0d0000000000000000;
	fma.rn.f64 	%fd2593, %fd5238, %fd2589, 0d0000000000000000;
	fma.rn.f64 	%fd2594, %fd5197, %fd2592, 0d0000000000000000;
	fma.rn.f64 	%fd2595, %fd5183, %fd2592, 0d0000000000000000;
	add.f64 	%fd2596, %fd2595, %fd2593;
	fma.rn.f64 	%fd2597, %fd5018, %fd1784, 0d0000000000000000;
	fma.rn.f64 	%fd2598, %fd5235, %fd1784, %fd2588;
	fma.rn.f64 	%fd2599, %fd5051, %fd2597, 0d0000000000000000;
	fma.rn.f64 	%fd2600, %fd5233, %fd2597, 0d0000000000000000;
	add.f64 	%fd2601, %fd2600, %fd2591;
	fma.rn.f64 	%fd2602, %fd5197, %fd2599, 0d0000000000000000;
	fma.rn.f64 	%fd2603, %fd5224, %fd2599, 0d0000000000000000;
	add.f64 	%fd2604, %fd2603, %fd2596;
	fma.rn.f64 	%fd2605, %fd5190, %fd2602, 0d0000000000000000;
	fma.rn.f64 	%fd2606, %fd5183, %fd2602, 0d0000000000000000;
	add.f64 	%fd2607, %fd2605, %fd2594;
	fma.rn.f64 	%fd2608, %fd5018, %fd1790, 0d0000000000000000;
	fma.rn.f64 	%fd2609, %fd5228, %fd1790, %fd2598;
	fma.rn.f64 	%fd2610, %fd5051, %fd2608, 0d0000000000000000;
	fma.rn.f64 	%fd2611, %fd5226, %fd2608, 0d0000000000000000;
	add.f64 	%fd2612, %fd2611, %fd2601;
	fma.rn.f64 	%fd2613, %fd5190, %fd2610, 0d0000000000000000;
	fma.rn.f64 	%fd2614, %fd5224, %fd2610, 0d0000000000000000;
	add.f64 	%fd2615, %fd2614, %fd2606;
	fma.rn.f64 	%fd2616, %fd5190, %fd2613, 0d0000000000000000;
	fma.rn.f64 	%fd2617, %fd5183, %fd2613, 0d0000000000000000;
	add.f64 	%fd2618, %fd2617, %fd2615;
	add.f64 	%fd2619, %fd2616, %fd2607;
	fma.rn.f64 	%fd2620, %fd5018, %fd1785, 0d0000000000000000;
	fma.rn.f64 	%fd2621, %fd5221, %fd1785, %fd2609;
	fma.rn.f64 	%fd2622, %fd5051, %fd2620, 0d0000000000000000;
	fma.rn.f64 	%fd2623, %fd5219, %fd2620, 0d0000000000000000;
	add.f64 	%fd2624, %fd2623, %fd2612;
	fma.rn.f64 	%fd2625, %fd5197, %fd2622, 0d0000000000000000;
	fma.rn.f64 	%fd2626, %fd5210, %fd2622, 0d0000000000000000;
	add.f64 	%fd1831, %fd2626, %fd2604;
	fma.rn.f64 	%fd2627, %fd2625, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2628, %fd5183, %fd2627, 0d0000000000000000;
	fma.rn.f64 	%fd2629, %fd5183, %fd2627, 0d0000000000000000;
	add.f64 	%fd2630, %fd2629, %fd2619;
	add.f64 	%fd2631, %fd2628, %fd2630;
	fma.rn.f64 	%fd2632, %fd5018, %fd1786, 0d0000000000000000;
	fma.rn.f64 	%fd2633, %fd5214, %fd1786, %fd2621;
	fma.rn.f64 	%fd2634, %fd5051, %fd2632, 0d0000000000000000;
	fma.rn.f64 	%fd2635, %fd5212, %fd2632, 0d0000000000000000;
	add.f64 	%fd2636, %fd2635, %fd2624;
	fma.rn.f64 	%fd2637, %fd5190, %fd2634, 0d0000000000000000;
	fma.rn.f64 	%fd2638, %fd5210, %fd2634, 0d0000000000000000;
	add.f64 	%fd1832, %fd2638, %fd2618;
	fma.rn.f64 	%fd2639, %fd2637, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2640, %fd5183, %fd2639, 0d0000000000000000;
	fma.rn.f64 	%fd2641, %fd5183, %fd2639, 0d0000000000000000;
	add.f64 	%fd2642, %fd2641, %fd2631;
	add.f64 	%fd1833, %fd2640, %fd2642;
	fma.rn.f64 	%fd2643, %fd5018, %fd1794, 0d0000000000000000;
	fma.rn.f64 	%fd1834, %fd5207, %fd1794, %fd2633;
	fma.rn.f64 	%fd1835, %fd5051, %fd2643, 0d0000000000000000;
	fma.rn.f64 	%fd2644, %fd5205, %fd2643, 0d0000000000000000;
	add.f64 	%fd1836, %fd2644, %fd2636;
	abs.f64 	%fd1837, %fd5183;
	{ // callseq 73, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1837;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5620, [retval0+0];
	} // callseq 73
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r54}, %fd5183;
	}
	setp.lt.s32 	%p266, %r54, 0;
	and.pred  	%p11, %p266, %p220;
	not.pred 	%p268, %p11;
	@%p268 bra 	$L__BB43_168;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r557}, %fd5620;
	}
	xor.b32  	%r558, %r557, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r559, %temp}, %fd5620;
	}
	mov.b64 	%fd5620, {%r559, %r558};

$L__BB43_168:
	setp.eq.f64 	%p269, %fd5183, 0d0000000000000000;
	@%p269 bra 	$L__BB43_172;
	bra.uni 	$L__BB43_169;

$L__BB43_172:
	selp.b32 	%r560, %r54, 0, %p220;
	mov.u32 	%r561, 0;
	or.b32  	%r562, %r560, 2146435072;
	setp.lt.s32 	%p273, %r48, 0;
	selp.b32 	%r563, %r562, %r560, %p273;
	mov.b64 	%fd5620, {%r561, %r563};
	bra.uni 	$L__BB43_173;

$L__BB43_169:
	setp.gt.s32 	%p270, %r54, -1;
	@%p270 bra 	$L__BB43_173;

	mov.f64 	%fd2645, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2646, %fd2645;
	setp.eq.f64 	%p271, %fd2646, 0d4000000000000000;
	@%p271 bra 	$L__BB43_173;

	mov.f64 	%fd5620, 0dFFF8000000000000;

$L__BB43_173:
	add.f64 	%fd2648, %fd5183, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r564}, %fd2648;
	}
	and.b32  	%r565, %r564, 2146435072;
	setp.ne.s32 	%p274, %r565, 2146435072;
	@%p274 bra 	$L__BB43_180;

	setp.gtu.f64 	%p275, %fd1837, 0d7FF0000000000000;
	@%p275 bra 	$L__BB43_179;
	bra.uni 	$L__BB43_175;

$L__BB43_179:
	mov.f64 	%fd2650, 0d4000000000000000;
	add.rn.f64 	%fd5620, %fd5183, %fd2650;
	bra.uni 	$L__BB43_180;

$L__BB43_175:
	mov.f64 	%fd2649, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r566, %temp}, %fd2649;
	}
	and.b32  	%r55, %r48, 2147483647;
	setp.eq.s32 	%p276, %r55, 2146435072;
	setp.eq.s32 	%p277, %r566, 0;
	and.pred  	%p278, %p276, %p277;
	@%p278 bra 	$L__BB43_178;
	bra.uni 	$L__BB43_176;

$L__BB43_178:
	setp.gt.f64 	%p285, %fd1837, 0d3FF0000000000000;
	selp.b32 	%r573, 2146435072, 0, %p285;
	mov.u32 	%r574, 0;
	xor.b32  	%r575, %r573, 2146435072;
	setp.lt.s32 	%p286, %r48, 0;
	selp.b32 	%r576, %r575, %r573, %p286;
	setp.eq.f64 	%p287, %fd5183, 0dBFF0000000000000;
	selp.b32 	%r577, 1072693248, %r576, %p287;
	mov.b64 	%fd5620, {%r574, %r577};
	bra.uni 	$L__BB43_180;

$L__BB43_176:
	{
	.reg .b32 %temp;
	mov.b64 	{%r567, %temp}, %fd5183;
	}
	and.b32  	%r568, %r54, 2147483647;
	setp.ne.s32 	%p279, %r568, 2146435072;
	setp.ne.s32 	%p280, %r567, 0;
	or.pred  	%p281, %p279, %p280;
	@%p281 bra 	$L__BB43_180;

	setp.gt.s32 	%p282, %r48, -1;
	selp.b32 	%r569, 2146435072, 0, %p282;
	mov.u32 	%r570, 0;
	setp.ne.s32 	%p283, %r55, 1071644672;
	and.pred  	%p284, %p283, %p11;
	or.b32  	%r571, %r569, -2147483648;
	selp.b32 	%r572, %r571, %r569, %p284;
	mov.b64 	%fd5620, {%r570, %r572};

$L__BB43_180:
	mul.f64 	%fd2651, %fd5620, 0d4008000000000000;
	setp.eq.f64 	%p288, %fd5183, 0d3FF0000000000000;
	selp.f64 	%fd2652, 0d4008000000000000, %fd2651, %p288;
	div.rn.f64 	%fd2653, %fd1835, 0d4008000000000000;
	add.f64 	%fd2654, %fd2653, 0d0000000000000000;
	mov.f64 	%fd2655, 0d0000000000000000;
	fma.rn.f64 	%fd2656, %fd2654, %fd2652, 0d0000000000000000;
	add.f64 	%fd2657, %fd1833, %fd2656;
	fma.rn.f64 	%fd2658, %fd5018, %fd1787, 0d0000000000000000;
	fma.rn.f64 	%fd2659, %fd5203, %fd1787, %fd1834;
	fma.rn.f64 	%fd2660, %fd5072, %fd2658, 0d0000000000000000;
	fma.rn.f64 	%fd2661, %fd5201, %fd2658, 0d0000000000000000;
	add.f64 	%fd1847, %fd2661, %fd1808;
	fma.rn.f64 	%fd2662, %fd5200, %fd2660, 0d0000000000000000;
	fma.rn.f64 	%fd2663, %fd5199, %fd2660, 0d0000000000000000;
	sub.f64 	%fd2664, %fd2655, %fd2663;
	fma.rn.f64 	%fd2665, %fd2664, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2666, %fd2665, %fd1832;
	add.f64 	%fd2667, %fd2665, %fd2657;
	div.rn.f64 	%fd2668, %fd2664, 0d4008000000000000;
	add.f64 	%fd2669, %fd2668, 0d0000000000000000;
	add.f64 	%fd2670, %fd2669, %fd1831;
	fma.rn.f64 	%fd2671, %fd5197, %fd2662, 0d0000000000000000;
	fma.rn.f64 	%fd2672, %fd5197, %fd2662, 0d0000000000000000;
	add.f64 	%fd2673, %fd2672, %fd2670;
	add.f64 	%fd2674, %fd2671, %fd2673;
	fma.rn.f64 	%fd2675, %fd5018, %fd1788, 0d0000000000000000;
	fma.rn.f64 	%fd2676, %fd5196, %fd1788, %fd2659;
	fma.rn.f64 	%fd2677, %fd5065, %fd2675, 0d0000000000000000;
	fma.rn.f64 	%fd2678, %fd5194, %fd2675, 0d0000000000000000;
	add.f64 	%fd1848, %fd2678, %fd1809;
	fma.rn.f64 	%fd2679, %fd5193, %fd2677, 0d0000000000000000;
	fma.rn.f64 	%fd2680, %fd5192, %fd2677, 0d0000000000000000;
	sub.f64 	%fd2681, %fd2655, %fd2680;
	fma.rn.f64 	%fd2682, %fd2681, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2683, %fd2682, %fd2674;
	add.f64 	%fd2684, %fd2682, %fd2667;
	div.rn.f64 	%fd2685, %fd2681, 0d4008000000000000;
	add.f64 	%fd2686, %fd2685, 0d0000000000000000;
	add.f64 	%fd2687, %fd2686, %fd2666;
	fma.rn.f64 	%fd2688, %fd5190, %fd2679, 0d0000000000000000;
	fma.rn.f64 	%fd2689, %fd5190, %fd2679, 0d0000000000000000;
	add.f64 	%fd2690, %fd2689, %fd2687;
	add.f64 	%fd2691, %fd2688, %fd2690;
	fma.rn.f64 	%fd2692, %fd5018, %fd1789, 0d0000000000000000;
	fma.rn.f64 	%fd2693, %fd5189, %fd1789, %fd2676;
	fma.rn.f64 	%fd2694, %fd5051, %fd2692, 0d0000000000000000;
	fma.rn.f64 	%fd2695, %fd5187, %fd2692, 0d0000000000000000;
	add.f64 	%fd2696, %fd2695, %fd1836;
	fma.rn.f64 	%fd2697, %fd5186, %fd2694, 0d0000000000000000;
	fma.rn.f64 	%fd2698, %fd5185, %fd2694, 0d0000000000000000;
	sub.f64 	%fd2699, %fd2655, %fd2698;
	fma.rn.f64 	%fd2700, %fd2699, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1849, %fd2700, %fd2683;
	add.f64 	%fd1850, %fd2700, %fd2691;
	div.rn.f64 	%fd2701, %fd2699, 0d4008000000000000;
	add.f64 	%fd2702, %fd2701, 0d0000000000000000;
	add.f64 	%fd2703, %fd2702, %fd2684;
	fma.rn.f64 	%fd2704, %fd5183, %fd2697, 0d0000000000000000;
	fma.rn.f64 	%fd2705, %fd5183, %fd2697, 0d0000000000000000;
	add.f64 	%fd2706, %fd2705, %fd2703;
	add.f64 	%fd1851, %fd2704, %fd2706;
	fma.rn.f64 	%fd2707, %fd5018, %fd1783, 0d0000000000000000;
	fma.rn.f64 	%fd1852, %fd5182, %fd1783, %fd2693;
	fma.rn.f64 	%fd1853, %fd5051, %fd2707, 0d0000000000000000;
	fma.rn.f64 	%fd2708, %fd5180, %fd2707, 0d0000000000000000;
	add.f64 	%fd1854, %fd2708, %fd2696;
	abs.f64 	%fd1855, %fd5179;
	{ // callseq 74, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1855;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5623, [retval0+0];
	} // callseq 74
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r56}, %fd5179;
	}
	setp.lt.s32 	%p289, %r56, 0;
	and.pred  	%p12, %p289, %p220;
	not.pred 	%p291, %p12;
	@%p291 bra 	$L__BB43_182;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r578}, %fd5623;
	}
	xor.b32  	%r579, %r578, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r580, %temp}, %fd5623;
	}
	mov.b64 	%fd5623, {%r580, %r579};

$L__BB43_182:
	setp.eq.f64 	%p292, %fd5179, 0d0000000000000000;
	@%p292 bra 	$L__BB43_186;
	bra.uni 	$L__BB43_183;

$L__BB43_186:
	selp.b32 	%r581, %r56, 0, %p220;
	mov.u32 	%r582, 0;
	or.b32  	%r583, %r581, 2146435072;
	setp.lt.s32 	%p296, %r48, 0;
	selp.b32 	%r584, %r583, %r581, %p296;
	mov.b64 	%fd5623, {%r582, %r584};
	bra.uni 	$L__BB43_187;

$L__BB43_183:
	setp.gt.s32 	%p293, %r56, -1;
	@%p293 bra 	$L__BB43_187;

	mov.f64 	%fd2709, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2710, %fd2709;
	setp.eq.f64 	%p294, %fd2710, 0d4000000000000000;
	@%p294 bra 	$L__BB43_187;

	mov.f64 	%fd5623, 0dFFF8000000000000;

$L__BB43_187:
	add.f64 	%fd2712, %fd5179, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r585}, %fd2712;
	}
	and.b32  	%r586, %r585, 2146435072;
	setp.ne.s32 	%p297, %r586, 2146435072;
	@%p297 bra 	$L__BB43_194;

	setp.gtu.f64 	%p298, %fd1855, 0d7FF0000000000000;
	@%p298 bra 	$L__BB43_193;
	bra.uni 	$L__BB43_189;

$L__BB43_193:
	mov.f64 	%fd2714, 0d4000000000000000;
	add.rn.f64 	%fd5623, %fd5179, %fd2714;
	bra.uni 	$L__BB43_194;

$L__BB43_189:
	mov.f64 	%fd2713, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r587, %temp}, %fd2713;
	}
	and.b32  	%r57, %r48, 2147483647;
	setp.eq.s32 	%p299, %r57, 2146435072;
	setp.eq.s32 	%p300, %r587, 0;
	and.pred  	%p301, %p299, %p300;
	@%p301 bra 	$L__BB43_192;
	bra.uni 	$L__BB43_190;

$L__BB43_192:
	setp.gt.f64 	%p308, %fd1855, 0d3FF0000000000000;
	selp.b32 	%r594, 2146435072, 0, %p308;
	mov.u32 	%r595, 0;
	xor.b32  	%r596, %r594, 2146435072;
	setp.lt.s32 	%p309, %r48, 0;
	selp.b32 	%r597, %r596, %r594, %p309;
	setp.eq.f64 	%p310, %fd5179, 0dBFF0000000000000;
	selp.b32 	%r598, 1072693248, %r597, %p310;
	mov.b64 	%fd5623, {%r595, %r598};
	bra.uni 	$L__BB43_194;

$L__BB43_190:
	{
	.reg .b32 %temp;
	mov.b64 	{%r588, %temp}, %fd5179;
	}
	and.b32  	%r589, %r56, 2147483647;
	setp.ne.s32 	%p302, %r589, 2146435072;
	setp.ne.s32 	%p303, %r588, 0;
	or.pred  	%p304, %p302, %p303;
	@%p304 bra 	$L__BB43_194;

	setp.gt.s32 	%p305, %r48, -1;
	selp.b32 	%r590, 2146435072, 0, %p305;
	mov.u32 	%r591, 0;
	setp.ne.s32 	%p306, %r57, 1071644672;
	and.pred  	%p307, %p306, %p12;
	or.b32  	%r592, %r590, -2147483648;
	selp.b32 	%r593, %r592, %r590, %p307;
	mov.b64 	%fd5623, {%r591, %r593};

$L__BB43_194:
	mul.f64 	%fd2715, %fd5623, 0d4008000000000000;
	setp.eq.f64 	%p311, %fd5179, 0d3FF0000000000000;
	selp.f64 	%fd2716, 0d4008000000000000, %fd2715, %p311;
	div.rn.f64 	%fd2717, %fd1853, 0d4008000000000000;
	add.f64 	%fd2718, %fd2717, 0d0000000000000000;
	mov.f64 	%fd2719, 0d0000000000000000;
	sub.f64 	%fd2720, %fd2719, %fd2718;
	fma.rn.f64 	%fd2721, %fd2720, %fd2716, 0d0000000000000000;
	sub.f64 	%fd2722, %fd2719, %fd2721;
	add.f64 	%fd2723, %fd1849, %fd2722;
	add.f64 	%fd2724, %fd1850, %fd2722;
	add.f64 	%fd2725, %fd1851, %fd2722;
	fma.rn.f64 	%fd2726, %fd1852, 0d3FE0AAAAA0000000, 0d0000000000000000;
	add.f64 	%fd1865, %fd2725, 0d0000000000000000;
	add.f64 	%fd1866, %fd2724, 0d0000000000000000;
	add.f64 	%fd1867, %fd2723, 0d0000000000000000;
	fma.rn.f64 	%fd1868, %fd5044, %fd2726, %fd1829;
	fma.rn.f64 	%fd1869, %fd5046, %fd2726, %fd1830;
	fma.rn.f64 	%fd2727, %fd5018, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2728, %fd5177, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2729, %fd5051, %fd2727, 0d0000000000000000;
	fma.rn.f64 	%fd2730, %fd5175, %fd2727, 0d0000000000000000;
	add.f64 	%fd2731, %fd2730, %fd1854;
	fma.rn.f64 	%fd2732, %fd5132, %fd2729, 0d0000000000000000;
	fma.rn.f64 	%fd2733, %fd5173, %fd2729, 0d0000000000000000;
	fma.rn.f64 	%fd2734, %fd5132, %fd2732, 0d0000000000000000;
	fma.rn.f64 	%fd2735, %fd5118, %fd2732, 0d0000000000000000;
	add.f64 	%fd2736, %fd2735, %fd2733;
	fma.rn.f64 	%fd2737, %fd5018, %fd1784, 0d0000000000000000;
	fma.rn.f64 	%fd2738, %fd5170, %fd1784, %fd2728;
	fma.rn.f64 	%fd2739, %fd5051, %fd2737, 0d0000000000000000;
	fma.rn.f64 	%fd2740, %fd5168, %fd2737, 0d0000000000000000;
	add.f64 	%fd2741, %fd2740, %fd2731;
	fma.rn.f64 	%fd2742, %fd5132, %fd2739, 0d0000000000000000;
	fma.rn.f64 	%fd2743, %fd5159, %fd2739, 0d0000000000000000;
	add.f64 	%fd2744, %fd2743, %fd2736;
	fma.rn.f64 	%fd2745, %fd5125, %fd2742, 0d0000000000000000;
	fma.rn.f64 	%fd2746, %fd5118, %fd2742, 0d0000000000000000;
	add.f64 	%fd2747, %fd2745, %fd2734;
	fma.rn.f64 	%fd2748, %fd5018, %fd1790, 0d0000000000000000;
	fma.rn.f64 	%fd2749, %fd5163, %fd1790, %fd2738;
	fma.rn.f64 	%fd2750, %fd5051, %fd2748, 0d0000000000000000;
	fma.rn.f64 	%fd2751, %fd5161, %fd2748, 0d0000000000000000;
	add.f64 	%fd2752, %fd2751, %fd2741;
	fma.rn.f64 	%fd2753, %fd5125, %fd2750, 0d0000000000000000;
	fma.rn.f64 	%fd2754, %fd5159, %fd2750, 0d0000000000000000;
	add.f64 	%fd2755, %fd2754, %fd2746;
	fma.rn.f64 	%fd2756, %fd5125, %fd2753, 0d0000000000000000;
	fma.rn.f64 	%fd2757, %fd5118, %fd2753, 0d0000000000000000;
	add.f64 	%fd2758, %fd2757, %fd2755;
	add.f64 	%fd2759, %fd2756, %fd2747;
	fma.rn.f64 	%fd2760, %fd5018, %fd1785, 0d0000000000000000;
	fma.rn.f64 	%fd2761, %fd5156, %fd1785, %fd2749;
	fma.rn.f64 	%fd2762, %fd5051, %fd2760, 0d0000000000000000;
	fma.rn.f64 	%fd2763, %fd5154, %fd2760, 0d0000000000000000;
	add.f64 	%fd2764, %fd2763, %fd2752;
	fma.rn.f64 	%fd2765, %fd5132, %fd2762, 0d0000000000000000;
	fma.rn.f64 	%fd2766, %fd5145, %fd2762, 0d0000000000000000;
	add.f64 	%fd1870, %fd2766, %fd2744;
	fma.rn.f64 	%fd2767, %fd2765, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2768, %fd5118, %fd2767, 0d0000000000000000;
	fma.rn.f64 	%fd2769, %fd5118, %fd2767, 0d0000000000000000;
	add.f64 	%fd2770, %fd2769, %fd2759;
	add.f64 	%fd2771, %fd2768, %fd2770;
	fma.rn.f64 	%fd2772, %fd5018, %fd1786, 0d0000000000000000;
	fma.rn.f64 	%fd2773, %fd5149, %fd1786, %fd2761;
	fma.rn.f64 	%fd2774, %fd5051, %fd2772, 0d0000000000000000;
	fma.rn.f64 	%fd2775, %fd5147, %fd2772, 0d0000000000000000;
	add.f64 	%fd2776, %fd2775, %fd2764;
	fma.rn.f64 	%fd2777, %fd5125, %fd2774, 0d0000000000000000;
	fma.rn.f64 	%fd2778, %fd5145, %fd2774, 0d0000000000000000;
	add.f64 	%fd1871, %fd2778, %fd2758;
	fma.rn.f64 	%fd2779, %fd2777, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2780, %fd5118, %fd2779, 0d0000000000000000;
	fma.rn.f64 	%fd2781, %fd5118, %fd2779, 0d0000000000000000;
	add.f64 	%fd2782, %fd2781, %fd2771;
	add.f64 	%fd1872, %fd2780, %fd2782;
	fma.rn.f64 	%fd2783, %fd5018, %fd1794, 0d0000000000000000;
	fma.rn.f64 	%fd1873, %fd5142, %fd1794, %fd2773;
	fma.rn.f64 	%fd1874, %fd5051, %fd2783, 0d0000000000000000;
	fma.rn.f64 	%fd2784, %fd5140, %fd2783, 0d0000000000000000;
	add.f64 	%fd1875, %fd2784, %fd2776;
	abs.f64 	%fd1876, %fd5118;
	{ // callseq 75, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1876;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5626, [retval0+0];
	} // callseq 75
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r58}, %fd5118;
	}
	setp.lt.s32 	%p312, %r58, 0;
	and.pred  	%p13, %p312, %p220;
	not.pred 	%p314, %p13;
	@%p314 bra 	$L__BB43_196;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r599}, %fd5626;
	}
	xor.b32  	%r600, %r599, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r601, %temp}, %fd5626;
	}
	mov.b64 	%fd5626, {%r601, %r600};

$L__BB43_196:
	setp.eq.f64 	%p315, %fd5118, 0d0000000000000000;
	@%p315 bra 	$L__BB43_200;
	bra.uni 	$L__BB43_197;

$L__BB43_200:
	selp.b32 	%r602, %r58, 0, %p220;
	mov.u32 	%r603, 0;
	or.b32  	%r604, %r602, 2146435072;
	setp.lt.s32 	%p319, %r48, 0;
	selp.b32 	%r605, %r604, %r602, %p319;
	mov.b64 	%fd5626, {%r603, %r605};
	bra.uni 	$L__BB43_201;

$L__BB43_197:
	setp.gt.s32 	%p316, %r58, -1;
	@%p316 bra 	$L__BB43_201;

	mov.f64 	%fd2785, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2786, %fd2785;
	setp.eq.f64 	%p317, %fd2786, 0d4000000000000000;
	@%p317 bra 	$L__BB43_201;

	mov.f64 	%fd5626, 0dFFF8000000000000;

$L__BB43_201:
	add.f64 	%fd2788, %fd5118, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r606}, %fd2788;
	}
	and.b32  	%r607, %r606, 2146435072;
	setp.ne.s32 	%p320, %r607, 2146435072;
	@%p320 bra 	$L__BB43_208;

	setp.gtu.f64 	%p321, %fd1876, 0d7FF0000000000000;
	@%p321 bra 	$L__BB43_207;
	bra.uni 	$L__BB43_203;

$L__BB43_207:
	mov.f64 	%fd2790, 0d4000000000000000;
	add.rn.f64 	%fd5626, %fd5118, %fd2790;
	bra.uni 	$L__BB43_208;

$L__BB43_203:
	mov.f64 	%fd2789, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r608, %temp}, %fd2789;
	}
	and.b32  	%r59, %r48, 2147483647;
	setp.eq.s32 	%p322, %r59, 2146435072;
	setp.eq.s32 	%p323, %r608, 0;
	and.pred  	%p324, %p322, %p323;
	@%p324 bra 	$L__BB43_206;
	bra.uni 	$L__BB43_204;

$L__BB43_206:
	setp.gt.f64 	%p331, %fd1876, 0d3FF0000000000000;
	selp.b32 	%r615, 2146435072, 0, %p331;
	mov.u32 	%r616, 0;
	xor.b32  	%r617, %r615, 2146435072;
	setp.lt.s32 	%p332, %r48, 0;
	selp.b32 	%r618, %r617, %r615, %p332;
	setp.eq.f64 	%p333, %fd5118, 0dBFF0000000000000;
	selp.b32 	%r619, 1072693248, %r618, %p333;
	mov.b64 	%fd5626, {%r616, %r619};
	bra.uni 	$L__BB43_208;

$L__BB43_204:
	{
	.reg .b32 %temp;
	mov.b64 	{%r609, %temp}, %fd5118;
	}
	and.b32  	%r610, %r58, 2147483647;
	setp.ne.s32 	%p325, %r610, 2146435072;
	setp.ne.s32 	%p326, %r609, 0;
	or.pred  	%p327, %p325, %p326;
	@%p327 bra 	$L__BB43_208;

	setp.gt.s32 	%p328, %r48, -1;
	selp.b32 	%r611, 2146435072, 0, %p328;
	mov.u32 	%r612, 0;
	setp.ne.s32 	%p329, %r59, 1071644672;
	and.pred  	%p330, %p329, %p13;
	or.b32  	%r613, %r611, -2147483648;
	selp.b32 	%r614, %r613, %r611, %p330;
	mov.b64 	%fd5626, {%r612, %r614};

$L__BB43_208:
	mul.f64 	%fd2791, %fd5626, 0d4008000000000000;
	setp.eq.f64 	%p334, %fd5118, 0d3FF0000000000000;
	selp.f64 	%fd2792, 0d4008000000000000, %fd2791, %p334;
	div.rn.f64 	%fd2793, %fd1874, 0d4008000000000000;
	add.f64 	%fd2794, %fd2793, 0d0000000000000000;
	mov.f64 	%fd2795, 0d0000000000000000;
	fma.rn.f64 	%fd2796, %fd2794, %fd2792, 0d0000000000000000;
	add.f64 	%fd2797, %fd1872, %fd2796;
	fma.rn.f64 	%fd2798, %fd5018, %fd1787, 0d0000000000000000;
	fma.rn.f64 	%fd2799, %fd5138, %fd1787, %fd1873;
	fma.rn.f64 	%fd2800, %fd5072, %fd2798, 0d0000000000000000;
	fma.rn.f64 	%fd2801, %fd5136, %fd2798, 0d0000000000000000;
	add.f64 	%fd1886, %fd2801, %fd1847;
	fma.rn.f64 	%fd2802, %fd5135, %fd2800, 0d0000000000000000;
	fma.rn.f64 	%fd2803, %fd5134, %fd2800, 0d0000000000000000;
	sub.f64 	%fd2804, %fd2795, %fd2803;
	fma.rn.f64 	%fd2805, %fd2804, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2806, %fd2805, %fd1871;
	add.f64 	%fd2807, %fd2805, %fd2797;
	div.rn.f64 	%fd2808, %fd2804, 0d4008000000000000;
	add.f64 	%fd2809, %fd2808, 0d0000000000000000;
	add.f64 	%fd2810, %fd2809, %fd1870;
	fma.rn.f64 	%fd2811, %fd5132, %fd2802, 0d0000000000000000;
	fma.rn.f64 	%fd2812, %fd5132, %fd2802, 0d0000000000000000;
	add.f64 	%fd2813, %fd2812, %fd2810;
	add.f64 	%fd2814, %fd2811, %fd2813;
	fma.rn.f64 	%fd2815, %fd5018, %fd1788, 0d0000000000000000;
	fma.rn.f64 	%fd2816, %fd5131, %fd1788, %fd2799;
	fma.rn.f64 	%fd2817, %fd5065, %fd2815, 0d0000000000000000;
	fma.rn.f64 	%fd2818, %fd5129, %fd2815, 0d0000000000000000;
	add.f64 	%fd1887, %fd2818, %fd1848;
	fma.rn.f64 	%fd2819, %fd5128, %fd2817, 0d0000000000000000;
	fma.rn.f64 	%fd2820, %fd5127, %fd2817, 0d0000000000000000;
	sub.f64 	%fd2821, %fd2795, %fd2820;
	fma.rn.f64 	%fd2822, %fd2821, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2823, %fd2822, %fd2814;
	add.f64 	%fd2824, %fd2822, %fd2807;
	div.rn.f64 	%fd2825, %fd2821, 0d4008000000000000;
	add.f64 	%fd2826, %fd2825, 0d0000000000000000;
	add.f64 	%fd2827, %fd2826, %fd2806;
	fma.rn.f64 	%fd2828, %fd5125, %fd2819, 0d0000000000000000;
	fma.rn.f64 	%fd2829, %fd5125, %fd2819, 0d0000000000000000;
	add.f64 	%fd2830, %fd2829, %fd2827;
	add.f64 	%fd2831, %fd2828, %fd2830;
	fma.rn.f64 	%fd2832, %fd5018, %fd1789, 0d0000000000000000;
	fma.rn.f64 	%fd2833, %fd5124, %fd1789, %fd2816;
	fma.rn.f64 	%fd2834, %fd5051, %fd2832, 0d0000000000000000;
	fma.rn.f64 	%fd2835, %fd5122, %fd2832, 0d0000000000000000;
	add.f64 	%fd2836, %fd2835, %fd1875;
	fma.rn.f64 	%fd2837, %fd5121, %fd2834, 0d0000000000000000;
	fma.rn.f64 	%fd2838, %fd5120, %fd2834, 0d0000000000000000;
	sub.f64 	%fd2839, %fd2795, %fd2838;
	fma.rn.f64 	%fd2840, %fd2839, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1888, %fd2840, %fd2823;
	add.f64 	%fd1889, %fd2840, %fd2831;
	div.rn.f64 	%fd2841, %fd2839, 0d4008000000000000;
	add.f64 	%fd2842, %fd2841, 0d0000000000000000;
	add.f64 	%fd2843, %fd2842, %fd2824;
	fma.rn.f64 	%fd2844, %fd5118, %fd2837, 0d0000000000000000;
	fma.rn.f64 	%fd2845, %fd5118, %fd2837, 0d0000000000000000;
	add.f64 	%fd2846, %fd2845, %fd2843;
	add.f64 	%fd1890, %fd2844, %fd2846;
	fma.rn.f64 	%fd2847, %fd5018, %fd1783, 0d0000000000000000;
	fma.rn.f64 	%fd1891, %fd5117, %fd1783, %fd2833;
	fma.rn.f64 	%fd1892, %fd5051, %fd2847, 0d0000000000000000;
	fma.rn.f64 	%fd2848, %fd5115, %fd2847, 0d0000000000000000;
	add.f64 	%fd1893, %fd2848, %fd2836;
	abs.f64 	%fd1894, %fd5114;
	{ // callseq 76, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1894;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5629, [retval0+0];
	} // callseq 76
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r60}, %fd5114;
	}
	setp.lt.s32 	%p335, %r60, 0;
	and.pred  	%p14, %p335, %p220;
	not.pred 	%p337, %p14;
	@%p337 bra 	$L__BB43_210;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r620}, %fd5629;
	}
	xor.b32  	%r621, %r620, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r622, %temp}, %fd5629;
	}
	mov.b64 	%fd5629, {%r622, %r621};

$L__BB43_210:
	setp.eq.f64 	%p338, %fd5114, 0d0000000000000000;
	@%p338 bra 	$L__BB43_214;
	bra.uni 	$L__BB43_211;

$L__BB43_214:
	selp.b32 	%r623, %r60, 0, %p220;
	mov.u32 	%r624, 0;
	or.b32  	%r625, %r623, 2146435072;
	setp.lt.s32 	%p342, %r48, 0;
	selp.b32 	%r626, %r625, %r623, %p342;
	mov.b64 	%fd5629, {%r624, %r626};
	bra.uni 	$L__BB43_215;

$L__BB43_211:
	setp.gt.s32 	%p339, %r60, -1;
	@%p339 bra 	$L__BB43_215;

	mov.f64 	%fd2849, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2850, %fd2849;
	setp.eq.f64 	%p340, %fd2850, 0d4000000000000000;
	@%p340 bra 	$L__BB43_215;

	mov.f64 	%fd5629, 0dFFF8000000000000;

$L__BB43_215:
	add.f64 	%fd2852, %fd5114, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r627}, %fd2852;
	}
	and.b32  	%r628, %r627, 2146435072;
	setp.ne.s32 	%p343, %r628, 2146435072;
	@%p343 bra 	$L__BB43_222;

	setp.gtu.f64 	%p344, %fd1894, 0d7FF0000000000000;
	@%p344 bra 	$L__BB43_221;
	bra.uni 	$L__BB43_217;

$L__BB43_221:
	mov.f64 	%fd2854, 0d4000000000000000;
	add.rn.f64 	%fd5629, %fd5114, %fd2854;
	bra.uni 	$L__BB43_222;

$L__BB43_217:
	mov.f64 	%fd2853, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r629, %temp}, %fd2853;
	}
	and.b32  	%r61, %r48, 2147483647;
	setp.eq.s32 	%p345, %r61, 2146435072;
	setp.eq.s32 	%p346, %r629, 0;
	and.pred  	%p347, %p345, %p346;
	@%p347 bra 	$L__BB43_220;
	bra.uni 	$L__BB43_218;

$L__BB43_220:
	setp.gt.f64 	%p354, %fd1894, 0d3FF0000000000000;
	selp.b32 	%r636, 2146435072, 0, %p354;
	mov.u32 	%r637, 0;
	xor.b32  	%r638, %r636, 2146435072;
	setp.lt.s32 	%p355, %r48, 0;
	selp.b32 	%r639, %r638, %r636, %p355;
	setp.eq.f64 	%p356, %fd5114, 0dBFF0000000000000;
	selp.b32 	%r640, 1072693248, %r639, %p356;
	mov.b64 	%fd5629, {%r637, %r640};
	bra.uni 	$L__BB43_222;

$L__BB43_218:
	{
	.reg .b32 %temp;
	mov.b64 	{%r630, %temp}, %fd5114;
	}
	and.b32  	%r631, %r60, 2147483647;
	setp.ne.s32 	%p348, %r631, 2146435072;
	setp.ne.s32 	%p349, %r630, 0;
	or.pred  	%p350, %p348, %p349;
	@%p350 bra 	$L__BB43_222;

	setp.gt.s32 	%p351, %r48, -1;
	selp.b32 	%r632, 2146435072, 0, %p351;
	mov.u32 	%r633, 0;
	setp.ne.s32 	%p352, %r61, 1071644672;
	and.pred  	%p353, %p352, %p14;
	or.b32  	%r634, %r632, -2147483648;
	selp.b32 	%r635, %r634, %r632, %p353;
	mov.b64 	%fd5629, {%r633, %r635};

$L__BB43_222:
	mul.f64 	%fd2855, %fd5629, 0d4008000000000000;
	setp.eq.f64 	%p357, %fd5114, 0d3FF0000000000000;
	selp.f64 	%fd2856, 0d4008000000000000, %fd2855, %p357;
	div.rn.f64 	%fd2857, %fd1892, 0d4008000000000000;
	add.f64 	%fd2858, %fd2857, 0d0000000000000000;
	mov.f64 	%fd2859, 0d0000000000000000;
	sub.f64 	%fd2860, %fd2859, %fd2858;
	fma.rn.f64 	%fd2861, %fd2860, %fd2856, 0d0000000000000000;
	sub.f64 	%fd2862, %fd2859, %fd2861;
	add.f64 	%fd2863, %fd1888, %fd2862;
	add.f64 	%fd2864, %fd1889, %fd2862;
	add.f64 	%fd2865, %fd1890, %fd2862;
	fma.rn.f64 	%fd2866, %fd1891, 0d3FE0AAAAA0000000, 0d0000000000000000;
	add.f64 	%fd1904, %fd2865, 0d0000000000000000;
	add.f64 	%fd1905, %fd2864, 0d0000000000000000;
	add.f64 	%fd1906, %fd2863, 0d0000000000000000;
	fma.rn.f64 	%fd1907, %fd5044, %fd2866, %fd1868;
	fma.rn.f64 	%fd1908, %fd5046, %fd2866, %fd1869;
	fma.rn.f64 	%fd2867, %fd5048, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2868, %fd5112, %fd1782, 0d0000000000000000;
	fma.rn.f64 	%fd2869, %fd5051, %fd2867, 0d0000000000000000;
	fma.rn.f64 	%fd2870, %fd5110, %fd2867, 0d0000000000000000;
	add.f64 	%fd2871, %fd2870, %fd1893;
	fma.rn.f64 	%fd2872, %fd5067, %fd2869, 0d0000000000000000;
	fma.rn.f64 	%fd2873, %fd5108, %fd2869, 0d0000000000000000;
	fma.rn.f64 	%fd2874, %fd5067, %fd2872, 0d0000000000000000;
	fma.rn.f64 	%fd2875, %fd5053, %fd2872, 0d0000000000000000;
	add.f64 	%fd2876, %fd2875, %fd2873;
	fma.rn.f64 	%fd2877, %fd5048, %fd1784, 0d0000000000000000;
	fma.rn.f64 	%fd2878, %fd5105, %fd1784, %fd2868;
	fma.rn.f64 	%fd2879, %fd5051, %fd2877, 0d0000000000000000;
	fma.rn.f64 	%fd2880, %fd5103, %fd2877, 0d0000000000000000;
	add.f64 	%fd2881, %fd2880, %fd2871;
	fma.rn.f64 	%fd2882, %fd5067, %fd2879, 0d0000000000000000;
	fma.rn.f64 	%fd2883, %fd5094, %fd2879, 0d0000000000000000;
	add.f64 	%fd2884, %fd2883, %fd2876;
	fma.rn.f64 	%fd2885, %fd5060, %fd2882, 0d0000000000000000;
	fma.rn.f64 	%fd2886, %fd5053, %fd2882, 0d0000000000000000;
	add.f64 	%fd2887, %fd2885, %fd2874;
	fma.rn.f64 	%fd2888, %fd5048, %fd1790, 0d0000000000000000;
	fma.rn.f64 	%fd2889, %fd5098, %fd1790, %fd2878;
	fma.rn.f64 	%fd2890, %fd5051, %fd2888, 0d0000000000000000;
	fma.rn.f64 	%fd2891, %fd5096, %fd2888, 0d0000000000000000;
	add.f64 	%fd2892, %fd2891, %fd2881;
	fma.rn.f64 	%fd2893, %fd5060, %fd2890, 0d0000000000000000;
	fma.rn.f64 	%fd2894, %fd5094, %fd2890, 0d0000000000000000;
	add.f64 	%fd2895, %fd2894, %fd2886;
	fma.rn.f64 	%fd2896, %fd5060, %fd2893, 0d0000000000000000;
	fma.rn.f64 	%fd2897, %fd5053, %fd2893, 0d0000000000000000;
	add.f64 	%fd2898, %fd2897, %fd2895;
	add.f64 	%fd2899, %fd2896, %fd2887;
	fma.rn.f64 	%fd2900, %fd5048, %fd1785, 0d0000000000000000;
	fma.rn.f64 	%fd2901, %fd5091, %fd1785, %fd2889;
	fma.rn.f64 	%fd2902, %fd5051, %fd2900, 0d0000000000000000;
	fma.rn.f64 	%fd2903, %fd5089, %fd2900, 0d0000000000000000;
	add.f64 	%fd2904, %fd2903, %fd2892;
	fma.rn.f64 	%fd2905, %fd5067, %fd2902, 0d0000000000000000;
	fma.rn.f64 	%fd2906, %fd5080, %fd2902, 0d0000000000000000;
	add.f64 	%fd1909, %fd2906, %fd2884;
	fma.rn.f64 	%fd2907, %fd2905, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2908, %fd5053, %fd2907, 0d0000000000000000;
	fma.rn.f64 	%fd2909, %fd5053, %fd2907, 0d0000000000000000;
	add.f64 	%fd2910, %fd2909, %fd2899;
	add.f64 	%fd2911, %fd2908, %fd2910;
	fma.rn.f64 	%fd2912, %fd5048, %fd1786, 0d0000000000000000;
	fma.rn.f64 	%fd2913, %fd5084, %fd1786, %fd2901;
	fma.rn.f64 	%fd2914, %fd5051, %fd2912, 0d0000000000000000;
	fma.rn.f64 	%fd2915, %fd5082, %fd2912, 0d0000000000000000;
	add.f64 	%fd2916, %fd2915, %fd2904;
	fma.rn.f64 	%fd2917, %fd5060, %fd2914, 0d0000000000000000;
	fma.rn.f64 	%fd2918, %fd5080, %fd2914, 0d0000000000000000;
	add.f64 	%fd1910, %fd2918, %fd2898;
	fma.rn.f64 	%fd2919, %fd2917, 0d3FE0000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2920, %fd5053, %fd2919, 0d0000000000000000;
	fma.rn.f64 	%fd2921, %fd5053, %fd2919, 0d0000000000000000;
	add.f64 	%fd2922, %fd2921, %fd2911;
	add.f64 	%fd1911, %fd2920, %fd2922;
	fma.rn.f64 	%fd2923, %fd5048, %fd1794, 0d0000000000000000;
	fma.rn.f64 	%fd1912, %fd5077, %fd1794, %fd2913;
	fma.rn.f64 	%fd1913, %fd5051, %fd2923, 0d0000000000000000;
	fma.rn.f64 	%fd2924, %fd5075, %fd2923, 0d0000000000000000;
	add.f64 	%fd1914, %fd2924, %fd2916;
	abs.f64 	%fd1915, %fd5053;
	{ // callseq 77, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1915;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5632, [retval0+0];
	} // callseq 77
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r62}, %fd5053;
	}
	setp.lt.s32 	%p358, %r62, 0;
	and.pred  	%p15, %p358, %p220;
	not.pred 	%p360, %p15;
	@%p360 bra 	$L__BB43_224;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r641}, %fd5632;
	}
	xor.b32  	%r642, %r641, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r643, %temp}, %fd5632;
	}
	mov.b64 	%fd5632, {%r643, %r642};

$L__BB43_224:
	setp.eq.f64 	%p361, %fd5053, 0d0000000000000000;
	@%p361 bra 	$L__BB43_228;
	bra.uni 	$L__BB43_225;

$L__BB43_228:
	selp.b32 	%r644, %r62, 0, %p220;
	mov.u32 	%r645, 0;
	or.b32  	%r646, %r644, 2146435072;
	setp.lt.s32 	%p365, %r48, 0;
	selp.b32 	%r647, %r646, %r644, %p365;
	mov.b64 	%fd5632, {%r645, %r647};
	bra.uni 	$L__BB43_229;

$L__BB43_225:
	setp.gt.s32 	%p362, %r62, -1;
	@%p362 bra 	$L__BB43_229;

	mov.f64 	%fd2925, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2926, %fd2925;
	setp.eq.f64 	%p363, %fd2926, 0d4000000000000000;
	@%p363 bra 	$L__BB43_229;

	mov.f64 	%fd5632, 0dFFF8000000000000;

$L__BB43_229:
	add.f64 	%fd2928, %fd5053, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r648}, %fd2928;
	}
	and.b32  	%r649, %r648, 2146435072;
	setp.ne.s32 	%p366, %r649, 2146435072;
	@%p366 bra 	$L__BB43_236;

	setp.gtu.f64 	%p367, %fd1915, 0d7FF0000000000000;
	@%p367 bra 	$L__BB43_235;
	bra.uni 	$L__BB43_231;

$L__BB43_235:
	mov.f64 	%fd2930, 0d4000000000000000;
	add.rn.f64 	%fd5632, %fd5053, %fd2930;
	bra.uni 	$L__BB43_236;

$L__BB43_231:
	mov.f64 	%fd2929, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r650, %temp}, %fd2929;
	}
	and.b32  	%r63, %r48, 2147483647;
	setp.eq.s32 	%p368, %r63, 2146435072;
	setp.eq.s32 	%p369, %r650, 0;
	and.pred  	%p370, %p368, %p369;
	@%p370 bra 	$L__BB43_234;
	bra.uni 	$L__BB43_232;

$L__BB43_234:
	setp.gt.f64 	%p377, %fd1915, 0d3FF0000000000000;
	selp.b32 	%r657, 2146435072, 0, %p377;
	mov.u32 	%r658, 0;
	xor.b32  	%r659, %r657, 2146435072;
	setp.lt.s32 	%p378, %r48, 0;
	selp.b32 	%r660, %r659, %r657, %p378;
	setp.eq.f64 	%p379, %fd5053, 0dBFF0000000000000;
	selp.b32 	%r661, 1072693248, %r660, %p379;
	mov.b64 	%fd5632, {%r658, %r661};
	bra.uni 	$L__BB43_236;

$L__BB43_232:
	{
	.reg .b32 %temp;
	mov.b64 	{%r651, %temp}, %fd5053;
	}
	and.b32  	%r652, %r62, 2147483647;
	setp.ne.s32 	%p371, %r652, 2146435072;
	setp.ne.s32 	%p372, %r651, 0;
	or.pred  	%p373, %p371, %p372;
	@%p373 bra 	$L__BB43_236;

	setp.gt.s32 	%p374, %r48, -1;
	selp.b32 	%r653, 2146435072, 0, %p374;
	mov.u32 	%r654, 0;
	setp.ne.s32 	%p375, %r63, 1071644672;
	and.pred  	%p376, %p375, %p15;
	or.b32  	%r655, %r653, -2147483648;
	selp.b32 	%r656, %r655, %r653, %p376;
	mov.b64 	%fd5632, {%r654, %r656};

$L__BB43_236:
	mul.f64 	%fd2931, %fd5632, 0d4008000000000000;
	setp.eq.f64 	%p380, %fd5053, 0d3FF0000000000000;
	selp.f64 	%fd2932, 0d4008000000000000, %fd2931, %p380;
	div.rn.f64 	%fd2933, %fd1913, 0d4008000000000000;
	add.f64 	%fd2934, %fd2933, 0d0000000000000000;
	mov.f64 	%fd2935, 0d0000000000000000;
	fma.rn.f64 	%fd2936, %fd2934, %fd2932, 0d0000000000000000;
	add.f64 	%fd2937, %fd1911, %fd2936;
	fma.rn.f64 	%fd2938, %fd5048, %fd1787, 0d0000000000000000;
	fma.rn.f64 	%fd2939, %fd5073, %fd1787, %fd1912;
	fma.rn.f64 	%fd2940, %fd5072, %fd2938, 0d0000000000000000;
	fma.rn.f64 	%fd2941, %fd5071, %fd2938, 0d0000000000000000;
	add.f64 	%fd1925, %fd2941, %fd1886;
	fma.rn.f64 	%fd2942, %fd5070, %fd2940, 0d0000000000000000;
	fma.rn.f64 	%fd2943, %fd5069, %fd2940, 0d0000000000000000;
	sub.f64 	%fd2944, %fd2935, %fd2943;
	fma.rn.f64 	%fd2945, %fd2944, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2946, %fd2945, %fd1910;
	add.f64 	%fd2947, %fd2945, %fd2937;
	div.rn.f64 	%fd2948, %fd2944, 0d4008000000000000;
	add.f64 	%fd2949, %fd2948, 0d0000000000000000;
	add.f64 	%fd2950, %fd2949, %fd1909;
	fma.rn.f64 	%fd2951, %fd5067, %fd2942, 0d0000000000000000;
	fma.rn.f64 	%fd2952, %fd5067, %fd2942, 0d0000000000000000;
	add.f64 	%fd2953, %fd2952, %fd2950;
	add.f64 	%fd2954, %fd2951, %fd2953;
	fma.rn.f64 	%fd2955, %fd5048, %fd1788, 0d0000000000000000;
	fma.rn.f64 	%fd2956, %fd5066, %fd1788, %fd2939;
	fma.rn.f64 	%fd2957, %fd5065, %fd2955, 0d0000000000000000;
	fma.rn.f64 	%fd2958, %fd5064, %fd2955, 0d0000000000000000;
	add.f64 	%fd1926, %fd2958, %fd1887;
	fma.rn.f64 	%fd2959, %fd5063, %fd2957, 0d0000000000000000;
	fma.rn.f64 	%fd2960, %fd5062, %fd2957, 0d0000000000000000;
	sub.f64 	%fd2961, %fd2935, %fd2960;
	fma.rn.f64 	%fd2962, %fd2961, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd2963, %fd2962, %fd2954;
	add.f64 	%fd2964, %fd2962, %fd2947;
	div.rn.f64 	%fd2965, %fd2961, 0d4008000000000000;
	add.f64 	%fd2966, %fd2965, 0d0000000000000000;
	add.f64 	%fd2967, %fd2966, %fd2946;
	fma.rn.f64 	%fd2968, %fd5060, %fd2959, 0d0000000000000000;
	fma.rn.f64 	%fd2969, %fd5060, %fd2959, 0d0000000000000000;
	add.f64 	%fd2970, %fd2969, %fd2967;
	add.f64 	%fd2971, %fd2968, %fd2970;
	fma.rn.f64 	%fd2972, %fd5048, %fd1789, 0d0000000000000000;
	fma.rn.f64 	%fd2973, %fd5059, %fd1789, %fd2956;
	fma.rn.f64 	%fd2974, %fd5051, %fd2972, 0d0000000000000000;
	fma.rn.f64 	%fd2975, %fd5057, %fd2972, 0d0000000000000000;
	add.f64 	%fd2976, %fd2975, %fd1914;
	fma.rn.f64 	%fd2977, %fd5056, %fd2974, 0d0000000000000000;
	fma.rn.f64 	%fd2978, %fd5055, %fd2974, 0d0000000000000000;
	sub.f64 	%fd2979, %fd2935, %fd2978;
	fma.rn.f64 	%fd2980, %fd2979, 0d3FE0000000000000, 0d0000000000000000;
	add.f64 	%fd1927, %fd2980, %fd2963;
	add.f64 	%fd1928, %fd2980, %fd2971;
	div.rn.f64 	%fd2981, %fd2979, 0d4008000000000000;
	add.f64 	%fd2982, %fd2981, 0d0000000000000000;
	add.f64 	%fd2983, %fd2982, %fd2964;
	fma.rn.f64 	%fd2984, %fd5053, %fd2977, 0d0000000000000000;
	fma.rn.f64 	%fd2985, %fd5053, %fd2977, 0d0000000000000000;
	add.f64 	%fd2986, %fd2985, %fd2983;
	add.f64 	%fd1929, %fd2984, %fd2986;
	fma.rn.f64 	%fd2987, %fd5048, %fd1783, 0d0000000000000000;
	fma.rn.f64 	%fd1930, %fd5052, %fd1783, %fd2973;
	fma.rn.f64 	%fd1931, %fd5051, %fd2987, 0d0000000000000000;
	fma.rn.f64 	%fd2988, %fd5050, %fd2987, 0d0000000000000000;
	add.f64 	%fd1932, %fd2988, %fd2976;
	abs.f64 	%fd1933, %fd5049;
	{ // callseq 78, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64 	[param0+0], %fd1933;
	.param .b64 param1;
	st.param.f64 	[param1+0], 0d4000000000000000;
	.param .b64 retval0;
	call.uni (retval0),
	__internal_accurate_pow,
	(
	param0,
	param1
	);
	ld.param.f64 	%fd5635, [retval0+0];
	} // callseq 78
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r64}, %fd5049;
	}
	setp.lt.s32 	%p381, %r64, 0;
	and.pred  	%p16, %p381, %p220;
	not.pred 	%p383, %p16;
	@%p383 bra 	$L__BB43_238;

	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r662}, %fd5635;
	}
	xor.b32  	%r663, %r662, -2147483648;
	{
	.reg .b32 %temp;
	mov.b64 	{%r664, %temp}, %fd5635;
	}
	mov.b64 	%fd5635, {%r664, %r663};

$L__BB43_238:
	setp.eq.f64 	%p384, %fd5049, 0d0000000000000000;
	@%p384 bra 	$L__BB43_242;
	bra.uni 	$L__BB43_239;

$L__BB43_242:
	selp.b32 	%r665, %r64, 0, %p220;
	mov.u32 	%r666, 0;
	or.b32  	%r667, %r665, 2146435072;
	setp.lt.s32 	%p388, %r48, 0;
	selp.b32 	%r668, %r667, %r665, %p388;
	mov.b64 	%fd5635, {%r666, %r668};
	bra.uni 	$L__BB43_243;

$L__BB43_239:
	setp.gt.s32 	%p385, %r64, -1;
	@%p385 bra 	$L__BB43_243;

	mov.f64 	%fd2989, 0d4000000000000000;
	cvt.rzi.f64.f64 	%fd2990, %fd2989;
	setp.eq.f64 	%p386, %fd2990, 0d4000000000000000;
	@%p386 bra 	$L__BB43_243;

	mov.f64 	%fd5635, 0dFFF8000000000000;

$L__BB43_243:
	add.f64 	%fd2992, %fd5049, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r669}, %fd2992;
	}
	and.b32  	%r670, %r669, 2146435072;
	setp.ne.s32 	%p389, %r670, 2146435072;
	@%p389 bra 	$L__BB43_250;

	setp.gtu.f64 	%p390, %fd1933, 0d7FF0000000000000;
	@%p390 bra 	$L__BB43_249;
	bra.uni 	$L__BB43_245;

$L__BB43_249:
	mov.f64 	%fd2994, 0d4000000000000000;
	add.rn.f64 	%fd5635, %fd5049, %fd2994;
	bra.uni 	$L__BB43_250;

$L__BB43_245:
	mov.f64 	%fd2993, 0d4000000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%r671, %temp}, %fd2993;
	}
	and.b32  	%r65, %r48, 2147483647;
	setp.eq.s32 	%p391, %r65, 2146435072;
	setp.eq.s32 	%p392, %r671, 0;
	and.pred  	%p393, %p391, %p392;
	@%p393 bra 	$L__BB43_248;
	bra.uni 	$L__BB43_246;

$L__BB43_248:
	setp.gt.f64 	%p400, %fd1933, 0d3FF0000000000000;
	selp.b32 	%r678, 2146435072, 0, %p400;
	mov.u32 	%r679, 0;
	xor.b32  	%r680, %r678, 2146435072;
	setp.lt.s32 	%p401, %r48, 0;
	selp.b32 	%r681, %r680, %r678, %p401;
	setp.eq.f64 	%p402, %fd5049, 0dBFF0000000000000;
	selp.b32 	%r682, 1072693248, %r681, %p402;
	mov.b64 	%fd5635, {%r679, %r682};
	bra.uni 	$L__BB43_250;

$L__BB43_246:
	{
	.reg .b32 %temp;
	mov.b64 	{%r672, %temp}, %fd5049;
	}
	and.b32  	%r673, %r64, 2147483647;
	setp.ne.s32 	%p394, %r673, 2146435072;
	setp.ne.s32 	%p395, %r672, 0;
	or.pred  	%p396, %p394, %p395;
	@%p396 bra 	$L__BB43_250;

	setp.gt.s32 	%p397, %r48, -1;
	selp.b32 	%r674, 2146435072, 0, %p397;
	mov.u32 	%r675, 0;
	setp.ne.s32 	%p398, %r65, 1071644672;
	and.pred  	%p399, %p398, %p16;
	or.b32  	%r676, %r674, -2147483648;
	selp.b32 	%r677, %r676, %r674, %p399;
	mov.b64 	%fd5635, {%r675, %r677};

$L__BB43_250:
	mul.f64 	%fd2996, %fd5635, 0d4008000000000000;
	setp.eq.f64 	%p403, %fd5049, 0d3FF0000000000000;
	selp.f64 	%fd2997, 0d4008000000000000, %fd2996, %p403;
	div.rn.f64 	%fd2998, %fd1931, 0d4008000000000000;
	add.f64 	%fd2999, %fd2998, 0d0000000000000000;
	mov.f64 	%fd5651, 0d0000000000000000;
	sub.f64 	%fd3000, %fd5651, %fd2999;
	fma.rn.f64 	%fd3001, %fd3000, %fd2997, 0d0000000000000000;
	sub.f64 	%fd3002, %fd5651, %fd3001;
	add.f64 	%fd3003, %fd1927, %fd3002;
	add.f64 	%fd3004, %fd1928, %fd3002;
	add.f64 	%fd3005, %fd1929, %fd3002;
	fma.rn.f64 	%fd3006, %fd1930, 0dBFE2000000000000, 0d0000000000000000;
	add.f64 	%fd3007, %fd3005, 0d0000000000000000;
	add.f64 	%fd3008, %fd3004, 0d0000000000000000;
	add.f64 	%fd3009, %fd3003, 0d0000000000000000;
	div.rn.f64 	%fd3010, %fd1828, 0d4014000000000000;
	add.f64 	%fd3011, %fd3010, 0d0000000000000000;
	sub.f64 	%fd3012, %fd5651, %fd3011;
	add.f64 	%fd3013, %fd3012, 0d0000000000000000;
	fma.rn.f64 	%fd3014, %fd3011, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3015, %fd5651, %fd3014;
	add.f64 	%fd3016, %fd3013, %fd3015;
	add.f64 	%fd3017, %fd3012, %fd3016;
	div.rn.f64 	%fd3018, %fd1867, 0d4014000000000000;
	add.f64 	%fd3019, %fd3018, 0d0000000000000000;
	fma.rn.f64 	%fd3020, %fd3019, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3021, %fd5651, %fd3020;
	add.f64 	%fd3022, %fd3021, %fd3017;
	add.f64 	%fd3023, %fd3011, %fd3020;
	sub.f64 	%fd3024, %fd5651, %fd3019;
	add.f64 	%fd3025, %fd3024, %fd3022;
	add.f64 	%fd3026, %fd3019, %fd3014;
	add.f64 	%fd3027, %fd3024, %fd3025;
	add.f64 	%fd3028, %fd3011, %fd3019;
	div.rn.f64 	%fd3029, %fd1906, 0d4014000000000000;
	add.f64 	%fd3030, %fd3029, 0d0000000000000000;
	sub.f64 	%fd3031, %fd5651, %fd3030;
	add.f64 	%fd3032, %fd3031, %fd3027;
	add.f64 	%fd3033, %fd3030, %fd3023;
	add.f64 	%fd3034, %fd3031, %fd3032;
	add.f64 	%fd3035, %fd3030, %fd3026;
	fma.rn.f64 	%fd3036, %fd3030, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3037, %fd5651, %fd3036;
	add.f64 	%fd3038, %fd3037, %fd3034;
	add.f64 	%fd3039, %fd3028, %fd3036;
	div.rn.f64 	%fd3040, %fd3009, 0d4008000000000000;
	add.f64 	%fd3041, %fd3040, 0d0000000000000000;
	sub.f64 	%fd3042, %fd5651, %fd3041;
	add.f64 	%fd3043, %fd3042, %fd3038;
	add.f64 	%fd3044, %fd3042, %fd3043;
	div.rn.f64 	%fd3045, %fd1827, 0d4014000000000000;
	add.f64 	%fd3046, %fd3045, 0d0000000000000000;
	sub.f64 	%fd3047, %fd5651, %fd3046;
	add.f64 	%fd3048, %fd3047, 0d0000000000000000;
	fma.rn.f64 	%fd3049, %fd3046, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3050, %fd5651, %fd3049;
	add.f64 	%fd3051, %fd3048, %fd3050;
	add.f64 	%fd3052, %fd3047, %fd3051;
	div.rn.f64 	%fd3053, %fd1866, 0d4014000000000000;
	add.f64 	%fd3054, %fd3053, 0d0000000000000000;
	fma.rn.f64 	%fd3055, %fd3054, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3056, %fd5651, %fd3055;
	add.f64 	%fd3057, %fd3056, %fd3052;
	add.f64 	%fd3058, %fd3046, %fd3055;
	sub.f64 	%fd3059, %fd5651, %fd3054;
	add.f64 	%fd3060, %fd3059, %fd3057;
	add.f64 	%fd3061, %fd3054, %fd3049;
	add.f64 	%fd3062, %fd3059, %fd3060;
	add.f64 	%fd3063, %fd3046, %fd3054;
	div.rn.f64 	%fd3064, %fd1905, 0d4014000000000000;
	add.f64 	%fd3065, %fd3064, 0d0000000000000000;
	sub.f64 	%fd3066, %fd5651, %fd3065;
	add.f64 	%fd3067, %fd3066, %fd3062;
	add.f64 	%fd3068, %fd3065, %fd3058;
	add.f64 	%fd3069, %fd3066, %fd3067;
	add.f64 	%fd3070, %fd3065, %fd3061;
	fma.rn.f64 	%fd3071, %fd3065, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3072, %fd5651, %fd3071;
	add.f64 	%fd3073, %fd3072, %fd3069;
	add.f64 	%fd3074, %fd3063, %fd3071;
	div.rn.f64 	%fd3075, %fd3008, 0d4008000000000000;
	add.f64 	%fd3076, %fd3075, 0d0000000000000000;
	sub.f64 	%fd3077, %fd5651, %fd3076;
	add.f64 	%fd3078, %fd3077, %fd3073;
	add.f64 	%fd3079, %fd3077, %fd3078;
	div.rn.f64 	%fd3080, %fd1826, 0d4014000000000000;
	add.f64 	%fd3081, %fd3080, 0d0000000000000000;
	sub.f64 	%fd3082, %fd5651, %fd3081;
	add.f64 	%fd3083, %fd3082, 0d0000000000000000;
	fma.rn.f64 	%fd3084, %fd3081, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3085, %fd5651, %fd3084;
	add.f64 	%fd3086, %fd3083, %fd3085;
	add.f64 	%fd3087, %fd3082, %fd3086;
	div.rn.f64 	%fd3088, %fd1865, 0d4014000000000000;
	add.f64 	%fd3089, %fd3088, 0d0000000000000000;
	fma.rn.f64 	%fd3090, %fd3089, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3091, %fd5651, %fd3090;
	add.f64 	%fd3092, %fd3091, %fd3087;
	add.f64 	%fd3093, %fd3081, %fd3090;
	sub.f64 	%fd3094, %fd5651, %fd3089;
	add.f64 	%fd3095, %fd3094, %fd3092;
	add.f64 	%fd3096, %fd3089, %fd3084;
	add.f64 	%fd3097, %fd3094, %fd3095;
	add.f64 	%fd3098, %fd3081, %fd3089;
	div.rn.f64 	%fd3099, %fd1904, 0d4014000000000000;
	add.f64 	%fd3100, %fd3099, 0d0000000000000000;
	sub.f64 	%fd3101, %fd5651, %fd3100;
	add.f64 	%fd3102, %fd3101, %fd3097;
	add.f64 	%fd3103, %fd3100, %fd3093;
	add.f64 	%fd3104, %fd3101, %fd3102;
	add.f64 	%fd3105, %fd3100, %fd3096;
	fma.rn.f64 	%fd3106, %fd3100, 0d4008000000000000, 0d0000000000000000;
	sub.f64 	%fd3107, %fd5651, %fd3106;
	add.f64 	%fd3108, %fd3107, %fd3104;
	add.f64 	%fd3109, %fd3098, %fd3106;
	div.rn.f64 	%fd3110, %fd3007, 0d4008000000000000;
	add.f64 	%fd3111, %fd3110, 0d0000000000000000;
	sub.f64 	%fd3112, %fd5651, %fd3111;
	add.f64 	%fd3113, %fd3112, %fd3108;
	add.f64 	%fd3114, %fd3112, %fd3113;
	mul.f64 	%fd3115, %fd5020, %fd1926;
	fma.rn.f64 	%fd3116, %fd5019, %fd1932, %fd3115;
	fma.rn.f64 	%fd3117, %fd5021, %fd1925, %fd3116;
	mul.f64 	%fd3118, %fd5047, %fd5047;
	div.rn.f64 	%fd3119, %fd3117, %fd3118;
	div.rn.f64 	%fd3120, %fd1932, %fd5047;
	add.f64 	%fd3121, %fd3120, 0d0000000000000000;
	div.rn.f64 	%fd3122, %fd1926, %fd5047;
	add.f64 	%fd3123, %fd3122, 0d0000000000000000;
	div.rn.f64 	%fd3124, %fd1925, %fd5047;
	add.f64 	%fd3125, %fd3124, 0d0000000000000000;
	fma.rn.f64 	%fd3126, %fd5044, %fd3006, %fd1907;
	fma.rn.f64 	%fd5656, %fd5046, %fd3006, %fd1908;
	add.f64 	%fd3127, %fd3041, %fd3033;
	add.f64 	%fd3128, %fd3041, %fd3035;
	add.f64 	%fd5650, %fd3042, %fd3044;
	add.f64 	%fd3129, %fd3041, %fd3039;
	add.f64 	%fd3130, %fd3076, %fd3068;
	add.f64 	%fd3131, %fd3076, %fd3070;
	add.f64 	%fd5649, %fd3077, %fd3079;
	add.f64 	%fd3132, %fd3076, %fd3074;
	add.f64 	%fd3133, %fd3111, %fd3103;
	add.f64 	%fd3134, %fd3111, %fd3105;
	add.f64 	%fd5648, %fd3112, %fd3114;
	add.f64 	%fd3135, %fd3111, %fd3109;
	sub.f64 	%fd3136, %fd5651, %fd3119;
	fma.rn.f64 	%fd5652, %fd3136, 0d4000000000000000, %fd3126;
	sub.f64 	%fd3137, %fd5651, %fd3121;
	sub.f64 	%fd3138, %fd5651, %fd3123;
	sub.f64 	%fd3139, %fd5651, %fd3125;
	setp.eq.s32 	%p404, %r697, 0;
	selp.f64 	%fd5657, %fd3121, %fd3137, %p404;
	selp.f64 	%fd5658, %fd3123, %fd3138, %p404;
	selp.f64 	%fd5659, %fd3125, %fd3139, %p404;
	fma.rn.f64 	%fd3140, %fd5022, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3141, %fd5023, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3142, %fd5024, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd5639, %fd5031, 0d0000000000000000, %fd3133;
	fma.rn.f64 	%fd5640, %fd5032, 0d0000000000000000, %fd3130;
	fma.rn.f64 	%fd5641, %fd5033, 0d0000000000000000, %fd3127;
	mul.f64 	%fd3143, %fd5026, %fd3142;
	mul.f64 	%fd3144, %fd3141, %fd5027;
	sub.f64 	%fd3145, %fd3143, %fd3144;
	mul.f64 	%fd3146, %fd3140, %fd5027;
	mul.f64 	%fd3147, %fd5025, %fd3142;
	sub.f64 	%fd3148, %fd3146, %fd3147;
	mul.f64 	%fd3149, %fd5025, %fd3141;
	mul.f64 	%fd3150, %fd3140, %fd5026;
	sub.f64 	%fd3151, %fd3149, %fd3150;
	add.f64 	%fd5645, %fd3145, %fd3135;
	add.f64 	%fd5646, %fd3148, %fd3132;
	add.f64 	%fd5647, %fd3151, %fd3129;
	mul.f64 	%fd3152, %fd3142, %fd5029;
	mul.f64 	%fd3153, %fd3141, %fd5030;
	sub.f64 	%fd3154, %fd3152, %fd3153;
	mul.f64 	%fd3155, %fd3140, %fd5030;
	mul.f64 	%fd3156, %fd3142, %fd5028;
	sub.f64 	%fd3157, %fd3155, %fd3156;
	mul.f64 	%fd3158, %fd3141, %fd5028;
	mul.f64 	%fd3159, %fd3140, %fd5029;
	sub.f64 	%fd3160, %fd3158, %fd3159;
	sub.f64 	%fd5642, %fd3134, %fd3154;
	sub.f64 	%fd5643, %fd3131, %fd3157;
	sub.f64 	%fd5644, %fd3128, %fd3160;

$L__BB43_254:
	fma.rn.f64 	%fd2010, %fd5652, 0d3FE0000000000000, 0d0000000000000000;
	setp.leu.f64 	%p406, %fd5045, 0d0000000000000000;
	@%p406 bra 	$L__BB43_256;

	div.rn.f64 	%fd4032, %fd5034, %fd5045;
	div.rn.f64 	%fd4033, %fd5035, %fd5045;
	div.rn.f64 	%fd4034, %fd5036, %fd5045;
	fma.rn.f64 	%fd5657, %fd4032, %fd2010, %fd5657;
	fma.rn.f64 	%fd5658, %fd4033, %fd2010, %fd5658;
	fma.rn.f64 	%fd5659, %fd4034, %fd2010, %fd5659;

$L__BB43_256:
	mul.f64 	%fd4035, %fd5039, %fd5658;
	mul.f64 	%fd4036, %fd5038, %fd5659;
	sub.f64 	%fd4037, %fd4036, %fd4035;
	mul.f64 	%fd4038, %fd5037, %fd5659;
	mul.f64 	%fd4039, %fd5039, %fd5657;
	sub.f64 	%fd4040, %fd4039, %fd4038;
	mul.f64 	%fd4041, %fd5038, %fd5657;
	mul.f64 	%fd4042, %fd5037, %fd5658;
	sub.f64 	%fd4043, %fd4042, %fd4041;
	add.f64 	%fd4044, %fd4037, 0d0000000000000000;
	add.f64 	%fd4045, %fd4040, 0d0000000000000000;
	add.f64 	%fd4046, %fd4043, 0d0000000000000000;
	mul.f64 	%fd4047, %fd5042, %fd5658;
	mul.f64 	%fd4048, %fd5041, %fd5659;
	sub.f64 	%fd4049, %fd4047, %fd4048;
	add.f64 	%fd4050, %fd4049, 0d0000000000000000;
	mul.f64 	%fd4051, %fd5040, %fd5659;
	mul.f64 	%fd4052, %fd5042, %fd5657;
	sub.f64 	%fd4053, %fd4051, %fd4052;
	add.f64 	%fd4054, %fd4053, 0d0000000000000000;
	mul.f64 	%fd4055, %fd5041, %fd5657;
	mul.f64 	%fd4056, %fd5040, %fd5658;
	sub.f64 	%fd4057, %fd4055, %fd4056;
	add.f64 	%fd4058, %fd4057, 0d0000000000000000;
	add.f64 	%fd2017, %fd5639, %fd4050;
	add.f64 	%fd2018, %fd5640, %fd4054;
	add.f64 	%fd2019, %fd5641, %fd4058;
	sub.f64 	%fd4059, %fd5645, %fd4050;
	sub.f64 	%fd4060, %fd5646, %fd4054;
	sub.f64 	%fd4061, %fd5647, %fd4058;
	add.f64 	%fd2020, %fd5642, %fd4044;
	add.f64 	%fd2021, %fd5643, %fd4045;
	add.f64 	%fd2022, %fd5644, %fd4046;
	sub.f64 	%fd2023, %fd4059, %fd4044;
	sub.f64 	%fd2024, %fd4060, %fd4045;
	sub.f64 	%fd2025, %fd4061, %fd4046;
	setp.eq.s64 	%p407, %rd74, 0;
	@%p407 bra 	$L__BB43_258;

	cvt.s64.s32 	%rd128, %r704;
	mul.lo.s64 	%rd129, %rd128, %rd36;
	add.s64 	%rd125, %rd74, %rd129;
	// begin inline asm
	{ atom.add.f64 %fd4062,[%rd125],%fd2017; }

	// end inline asm
	add.s64 	%rd126, %rd125, 8;
	// begin inline asm
	{ atom.add.f64 %fd4064,[%rd126],%fd2018; }

	// end inline asm
	add.s64 	%rd127, %rd125, 16;
	// begin inline asm
	{ atom.add.f64 %fd4066,[%rd127],%fd2019; }

	// end inline asm
	bra.uni 	$L__BB43_260;

$L__BB43_258:
	setp.eq.s64 	%p408, %rd55, 0;
	@%p408 bra 	$L__BB43_268;

	cvt.s64.s32 	%rd133, %r704;
	mul.lo.s64 	%rd134, %rd133, %rd33;
	add.s64 	%rd130, %rd55, %rd134;
	// begin inline asm
	{ atom.add.f64 %fd4068,[%rd130],%fd2017; }

	// end inline asm
	add.s64 	%rd131, %rd130, 8;
	// begin inline asm
	{ atom.add.f64 %fd4070,[%rd131],%fd2018; }

	// end inline asm
	add.s64 	%rd132, %rd130, 16;
	// begin inline asm
	{ atom.add.f64 %fd4072,[%rd132],%fd2019; }

	// end inline asm

$L__BB43_260:
	@%p407 bra 	$L__BB43_262;

	cvt.s64.s32 	%rd138, %r703;
	mul.lo.s64 	%rd139, %rd138, %rd36;
	add.s64 	%rd135, %rd74, %rd139;
	// begin inline asm
	{ atom.add.f64 %fd4074,[%rd135],%fd2020; }

	// end inline asm
	add.s64 	%rd136, %rd135, 8;
	// begin inline asm
	{ atom.add.f64 %fd4076,[%rd136],%fd2021; }

	// end inline asm
	add.s64 	%rd137, %rd135, 16;
	// begin inline asm
	{ atom.add.f64 %fd4078,[%rd137],%fd2022; }

	// end inline asm
	bra.uni 	$L__BB43_264;

$L__BB43_262:
	setp.eq.s64 	%p410, %rd55, 0;
	@%p410 bra 	$L__BB43_268;

	cvt.s64.s32 	%rd143, %r703;
	mul.lo.s64 	%rd144, %rd143, %rd33;
	add.s64 	%rd140, %rd55, %rd144;
	// begin inline asm
	{ atom.add.f64 %fd4080,[%rd140],%fd2020; }

	// end inline asm
	add.s64 	%rd141, %rd140, 8;
	// begin inline asm
	{ atom.add.f64 %fd4082,[%rd141],%fd2021; }

	// end inline asm
	add.s64 	%rd142, %rd140, 16;
	// begin inline asm
	{ atom.add.f64 %fd4084,[%rd142],%fd2022; }

	// end inline asm

$L__BB43_264:
	@%p407 bra 	$L__BB43_266;

	cvt.s64.s32 	%rd148, %r702;
	mul.lo.s64 	%rd149, %rd148, %rd36;
	add.s64 	%rd145, %rd74, %rd149;
	// begin inline asm
	{ atom.add.f64 %fd4086,[%rd145],%fd2023; }

	// end inline asm
	add.s64 	%rd146, %rd145, 8;
	// begin inline asm
	{ atom.add.f64 %fd4088,[%rd146],%fd2024; }

	// end inline asm
	add.s64 	%rd147, %rd145, 16;
	// begin inline asm
	{ atom.add.f64 %fd4090,[%rd147],%fd2025; }

	// end inline asm
	bra.uni 	$L__BB43_268;

$L__BB43_266:
	setp.eq.s64 	%p412, %rd55, 0;
	@%p412 bra 	$L__BB43_268;

	cvt.s64.s32 	%rd153, %r702;
	mul.lo.s64 	%rd154, %rd153, %rd33;
	add.s64 	%rd150, %rd55, %rd154;
	// begin inline asm
	{ atom.add.f64 %fd4092,[%rd150],%fd2023; }

	// end inline asm
	add.s64 	%rd151, %rd150, 8;
	// begin inline asm
	{ atom.add.f64 %fd4094,[%rd151],%fd2024; }

	// end inline asm
	add.s64 	%rd152, %rd150, 16;
	// begin inline asm
	{ atom.add.f64 %fd4096,[%rd152],%fd2025; }

	// end inline asm

$L__BB43_268:
	setp.eq.s64 	%p413, %rd78, 0;
	add.f64 	%fd2026, %fd5656, 0d0000000000000000;
	@%p413 bra 	$L__BB43_270;

	cvt.s64.s32 	%rd156, %r701;
	mul.lo.s64 	%rd157, %rd156, %rd37;
	add.s64 	%rd155, %rd78, %rd157;
	// begin inline asm
	{ atom.add.f64 %fd4098,[%rd155],%fd2026; }

	// end inline asm
	bra.uni 	$L__BB43_272;

$L__BB43_270:
	setp.eq.s64 	%p414, %rd69, 0;
	@%p414 bra 	$L__BB43_272;

	cvt.s64.s32 	%rd159, %r701;
	mul.lo.s64 	%rd160, %rd159, %rd31;
	add.s64 	%rd158, %rd69, %rd160;
	// begin inline asm
	{ atom.add.f64 %fd4100,[%rd158],%fd2026; }

	// end inline asm

$L__BB43_272:
	setp.eq.s64 	%p415, %rd76, 0;
	add.f64 	%fd2027, %fd5651, 0d0000000000000000;
	@%p415 bra 	$L__BB43_274;

	cvt.s64.s32 	%rd162, %r700;
	mul.lo.s64 	%rd163, %rd162, %rd38;
	add.s64 	%rd161, %rd76, %rd163;
	// begin inline asm
	{ atom.add.f64 %fd4102,[%rd161],%fd2027; }

	// end inline asm
	bra.uni 	$L__BB43_276;

$L__BB43_274:
	setp.eq.s64 	%p416, %rd67, 0;
	@%p416 bra 	$L__BB43_276;

	cvt.s64.s32 	%rd165, %r700;
	mul.lo.s64 	%rd166, %rd165, %rd30;
	add.s64 	%rd164, %rd67, %rd166;
	// begin inline asm
	{ atom.add.f64 %fd4104,[%rd164],%fd2027; }

	// end inline asm

$L__BB43_276:
	setp.eq.s64 	%p417, %rd80, 0;
	add.f64 	%fd2028, %fd5648, 0d0000000000000000;
	add.f64 	%fd2029, %fd5649, 0d0000000000000000;
	add.f64 	%fd2030, %fd5650, 0d0000000000000000;
	@%p417 bra 	$L__BB43_278;

	cvt.s64.s32 	%rd170, %r699;
	mul.lo.s64 	%rd171, %rd170, %rd39;
	add.s64 	%rd167, %rd80, %rd171;
	// begin inline asm
	{ atom.add.f64 %fd4106,[%rd167],%fd2028; }

	// end inline asm
	add.s64 	%rd168, %rd167, 8;
	// begin inline asm
	{ atom.add.f64 %fd4108,[%rd168],%fd2029; }

	// end inline asm
	add.s64 	%rd169, %rd167, 16;
	// begin inline asm
	{ atom.add.f64 %fd4110,[%rd169],%fd2030; }

	// end inline asm
	bra.uni 	$L__BB43_280;

$L__BB43_278:
	setp.eq.s64 	%p418, %rd71, 0;
	@%p418 bra 	$L__BB43_280;

	cvt.s64.s32 	%rd175, %r699;
	mul.lo.s64 	%rd176, %rd175, %rd29;
	add.s64 	%rd172, %rd71, %rd176;
	// begin inline asm
	{ atom.add.f64 %fd4112,[%rd172],%fd2028; }

	// end inline asm
	add.s64 	%rd173, %rd172, 8;
	// begin inline asm
	{ atom.add.f64 %fd4114,[%rd173],%fd2029; }

	// end inline asm
	add.s64 	%rd174, %rd172, 16;
	// begin inline asm
	{ atom.add.f64 %fd4116,[%rd174],%fd2030; }

	// end inline asm

$L__BB43_280:
	ld.param.u64 	%rd178, [init_affine_mass_matrix_kernel_cuda_kernel_backward_param_0+24];
	mov.u32 	%r685, %ntid.x;
	mov.u32 	%r684, %nctaid.x;
	mul.wide.u32 	%rd177, %r685, %r684;
	add.s64 	%rd179, %rd179, %rd177;
	setp.lt.u64 	%p419, %rd179, %rd178;
	@%p419 bra 	$L__BB43_2;

$L__BB43_281:
	ret;

}
.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<138>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r50}, %fd12;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r49, %temp}, %fd12;
	}
	shr.u32 	%r51, %r50, 20;
	setp.ne.s32 	%p1, %r51, 0;
	@%p1 bra 	$L__BB44_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r50}, %fd14;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r49, %temp}, %fd14;
	}
	shr.u32 	%r16, %r50, 20;
	add.s32 	%r51, %r16, -54;

$L__BB44_2:
	add.s32 	%r52, %r51, -1023;
	and.b32  	%r17, %r50, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd135, {%r49, %r18};
	setp.lt.u32 	%p2, %r18, 1073127583;
	@%p2 bra 	$L__BB44_4;

	{
	.reg .b32 %temp;
	mov.b64 	{%r19, %temp}, %fd135;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r20}, %fd135;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd135, {%r19, %r21};
	add.s32 	%r52, %r51, -1022;

$L__BB44_4:
	add.f64 	%fd15, %fd135, 0d3FF0000000000000;
	mov.f64 	%fd16, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd17, %fd15;
	neg.f64 	%fd18, %fd15;
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd16;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd17, %fd17;
	add.f64 	%fd22, %fd135, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp;
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r52, -2147483648;
	mov.u32 	%r26, -2147483648;
	mov.u32 	%r27, 1127219200;
	mov.b64 	%fd79, {%r25, %r27};
	mov.b64 	%fd80, {%r26, %r27};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r28}, %fd13;
	}
	shl.b32 	%r29, %r28, 1;
	setp.gt.u32 	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32 	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp;
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp;
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd16;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd16;
	{
	.reg .b32 %temp;
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd136, {%r14, %r34};
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	%f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32 	%p4, %f1, 0f4086232B;
	@%p4 bra 	$L__BB44_7;

	setp.lt.f64 	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64 	%fd136, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32 	%p6, %f1, 0f40874800;
	@%p6 bra 	$L__BB44_7;

	mov.f64 	%fd134, 0d4338000000000000;
	mov.f64 	%fd133, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
	{
	.reg .b32 %temp;
	mov.b64 	{%r48, %temp}, %fd132;
	}
	shr.u32 	%r36, %r48, 31;
	add.s32 	%r37, %r48, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r15, %r39;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r48, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd136, %fd130, %fd131;

$L__BB44_7:
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r45}, %fd136;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.eq.s32 	%p7, %r46, 2146435072;
	{
	.reg .b32 %temp;
	mov.b64 	{%r47, %temp}, %fd136;
	}
	setp.eq.s32 	%p8, %r47, 0;
	and.pred  	%p9, %p8, %p7;
	@%p9 bra 	$L__BB44_9;

	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;

$L__BB44_9:
	st.param.f64 	[func_retval0+0], %fd136;
	ret;

}

 
