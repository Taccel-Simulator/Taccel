//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34431801
// Cuda compilation tools, release 12.6, V12.6.20
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_86
.address_size 64

	// .globl	val_IPC_collisions_cuda_kernel_forward
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN67_INTERNAL_00000000_36_wp_warp_ipc_energy_barrier_energy_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;
.global .align 1 .b8 $str$1[54] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10};
.global .align 1 .b8 $str$2[92] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 105, 58, 32, 37, 100, 44, 32, 106, 58, 32, 37, 100, 44, 32, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 110, 95, 114, 111, 119, 115, 58, 32, 37, 100, 44, 32, 110, 95, 99, 111, 108, 115, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10};
.global .align 1 .b8 $str$3[10] = {112, 116, 32, 110, 97, 110, 33, 33, 33};
.global .align 1 .b8 $str$4[88] = {101, 101, 32, 110, 97, 110, 33, 33, 33, 44, 32, 101, 100, 103, 101, 95, 97, 114, 101, 97, 58, 32, 40, 37, 108, 102, 44, 32, 37, 108, 102, 41, 44, 32, 98, 97, 114, 114, 105, 101, 114, 32, 118, 97, 108, 58, 32, 37, 108, 102, 44, 32, 100, 115, 113, 58, 32, 37, 108, 102, 44, 32, 97, 99, 116, 105, 118, 101, 71, 97, 112, 50, 58, 32, 37, 108, 102, 44, 32, 120, 105, 58, 32, 37, 108, 102, 10};
.global .align 1 .b8 $str$5[4] = {37, 115, 10};
.global .align 1 .b8 $str$6[9] = {97, 100, 106, 58, 32, 37, 115, 10};

.visible .entry val_IPC_collisions_cuda_kernel_forward(
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_11[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_12[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_13[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_14[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_15[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_16[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_17[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_18[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_19[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_20[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_forward_param_21[56],
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_22,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_23,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_24,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_25,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_26,
	.param .f64 val_IPC_collisions_cuda_kernel_forward_param_27,
	.param .u32 val_IPC_collisions_cuda_kernel_forward_param_28
)
{
	.local .align 16 .b8 	__local_depot0[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<107>;
	.reg .b16 	%rs<169>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<550>;
	.reg .f64 	%fd<1136>;
	.reg .b64 	%rd<260>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r256, %r257}, [val_IPC_collisions_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r258, %r259}, [val_IPC_collisions_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r264, %r265}, [val_IPC_collisions_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r272, %r273}, [val_IPC_collisions_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r280, %r281}, [val_IPC_collisions_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r288, %r289}, [val_IPC_collisions_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r296, %r297}, [val_IPC_collisions_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r304, %r305}, [val_IPC_collisions_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r312, %r313}, [val_IPC_collisions_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r320, %r321}, [val_IPC_collisions_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r328, %r329}, [val_IPC_collisions_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r336, %r337}, [val_IPC_collisions_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r344, %r345}, [val_IPC_collisions_cuda_kernel_forward_param_11+32];
	ld.param.v2.u32 	{%r352, %r353}, [val_IPC_collisions_cuda_kernel_forward_param_12+32];
	ld.param.v2.u32 	{%r360, %r361}, [val_IPC_collisions_cuda_kernel_forward_param_13+32];
	ld.param.v2.u32 	{%r368, %r369}, [val_IPC_collisions_cuda_kernel_forward_param_14+32];
	ld.param.v2.u32 	{%r376, %r377}, [val_IPC_collisions_cuda_kernel_forward_param_15+32];
	ld.param.v2.u32 	{%r384, %r385}, [val_IPC_collisions_cuda_kernel_forward_param_16+32];
	ld.param.v2.u32 	{%r392, %r393}, [val_IPC_collisions_cuda_kernel_forward_param_17+32];
	ld.param.v2.u32 	{%r400, %r401}, [val_IPC_collisions_cuda_kernel_forward_param_18+32];
	ld.param.v2.u32 	{%r408, %r409}, [val_IPC_collisions_cuda_kernel_forward_param_19+32];
	ld.param.v2.u32 	{%r416, %r417}, [val_IPC_collisions_cuda_kernel_forward_param_20+32];
	ld.param.v2.u32 	{%r424, %r425}, [val_IPC_collisions_cuda_kernel_forward_param_21+32];
	ld.param.f64 	%fd192, [val_IPC_collisions_cuda_kernel_forward_param_22];
	ld.param.f64 	%fd193, [val_IPC_collisions_cuda_kernel_forward_param_23];
	ld.param.f64 	%fd194, [val_IPC_collisions_cuda_kernel_forward_param_24];
	ld.param.f64 	%fd195, [val_IPC_collisions_cuda_kernel_forward_param_25];
	ld.param.f64 	%fd196, [val_IPC_collisions_cuda_kernel_forward_param_26];
	ld.param.f64 	%fd197, [val_IPC_collisions_cuda_kernel_forward_param_27];
	ld.param.u32 	%r255, [val_IPC_collisions_cuda_kernel_forward_param_28];
	ld.param.u64 	%rd117, [val_IPC_collisions_cuda_kernel_forward_param_21];
	ld.param.u64 	%rd115, [val_IPC_collisions_cuda_kernel_forward_param_20];
	ld.param.u64 	%rd113, [val_IPC_collisions_cuda_kernel_forward_param_19];
	ld.param.u64 	%rd111, [val_IPC_collisions_cuda_kernel_forward_param_18];
	ld.param.u64 	%rd109, [val_IPC_collisions_cuda_kernel_forward_param_17];
	ld.param.u64 	%rd107, [val_IPC_collisions_cuda_kernel_forward_param_16];
	ld.param.u64 	%rd105, [val_IPC_collisions_cuda_kernel_forward_param_15];
	ld.param.u64 	%rd103, [val_IPC_collisions_cuda_kernel_forward_param_14];
	ld.param.u64 	%rd101, [val_IPC_collisions_cuda_kernel_forward_param_13];
	ld.param.u64 	%rd99, [val_IPC_collisions_cuda_kernel_forward_param_12];
	ld.param.u64 	%rd97, [val_IPC_collisions_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd95, [val_IPC_collisions_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd93, [val_IPC_collisions_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd91, [val_IPC_collisions_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd89, [val_IPC_collisions_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd87, [val_IPC_collisions_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd85, [val_IPC_collisions_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd83, [val_IPC_collisions_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd81, [val_IPC_collisions_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd79, [val_IPC_collisions_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd77, [val_IPC_collisions_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd76, [val_IPC_collisions_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r65, [val_IPC_collisions_cuda_kernel_forward_param_0+16];
	mov.u32 	%r428, %ntid.x;
	cvt.u64.u32 	%rd1, %r428;
	mov.u32 	%r429, %ctaid.x;
	mul.wide.u32 	%rd119, %r428, %r429;
	mov.u32 	%r430, %tid.x;
	cvt.u64.u32 	%rd120, %r430;
	add.s64 	%rd256, %rd119, %rd120;
	setp.ge.u64 	%p2, %rd256, %rd76;
	@%p2 bra 	$L__BB0_125;

	cvta.to.global.u64 	%rd4, %rd103;
	cvta.to.global.u64 	%rd5, %rd99;
	cvta.to.global.u64 	%rd6, %rd95;
	cvta.to.global.u64 	%rd7, %rd91;
	cvta.to.global.u64 	%rd8, %rd89;
	cvta.to.global.u64 	%rd9, %rd87;
	cvta.to.global.u64 	%rd10, %rd85;
	cvta.to.global.u64 	%rd11, %rd83;
	cvta.to.global.u64 	%rd12, %rd81;
	cvta.to.global.u64 	%rd13, %rd79;
	cvta.to.global.u64 	%rd15, %rd117;
	cvta.to.global.u64 	%rd16, %rd115;
	cvta.to.global.u64 	%rd17, %rd113;
	cvta.to.global.u64 	%rd18, %rd111;
	cvta.to.global.u64 	%rd19, %rd109;
	cvta.to.global.u64 	%rd20, %rd107;
	cvta.to.global.u64 	%rd21, %rd105;
	cvta.to.global.u64 	%rd22, %rd101;
	cvta.to.global.u64 	%rd23, %rd97;
	cvta.to.global.u64 	%rd24, %rd93;
	cvt.s64.s32 	%rd25, %r259;
	cvt.s64.s32 	%rd26, %r258;
	cvt.s64.s32 	%rd27, %r257;
	cvt.s64.s32 	%rd28, %r328;
	cvt.s64.s32 	%rd29, %r304;
	cvt.s64.s32 	%rd30, %r296;
	cvt.s64.s32 	%rd31, %r424;
	cvt.s64.s32 	%rd32, %r344;
	cvt.s64.s32 	%rd33, %r336;
	cvt.s64.s32 	%rd34, %r352;
	cvt.s64.s32 	%rd35, %r280;
	cvt.s64.s32 	%rd36, %r376;
	cvt.s64.s32 	%rd37, %r392;
	mov.u32 	%r431, %nctaid.x;
	cvt.u64.u32 	%rd121, %r431;
	mul.lo.s64 	%rd38, %rd1, %rd121;
	cvt.s64.s32 	%rd39, %r416;
	cvt.s64.s32 	%rd40, %r384;
	mul.f64 	%fd1, %fd192, %fd192;
	add.f64 	%fd2, %fd192, %fd192;
	cvt.s64.s32 	%rd41, %r400;
	mov.f64 	%fd198, 0d0000000000000000;
	sub.f64 	%fd3, %fd198, %fd194;
	cvt.s64.s32 	%rd42, %r360;
	cvt.s64.s32 	%rd43, %r408;
	cvt.s64.s32 	%rd44, %r312;
	cvt.s64.s32 	%rd45, %r368;
	cvt.s64.s32 	%rd46, %r320;
	cvt.s64.s32 	%rd47, %r272;
	cvt.s64.s32 	%rd48, %r288;
	cvt.s64.s32 	%rd49, %r264;
	mul.f64 	%fd4, %fd193, %fd197;
	mul.f64 	%fd5, %fd4, %fd4;
	div.rn.f64 	%fd6, %fd4, 0d4008000000000000;

$L__BB0_2:
	setp.lt.s32 	%p3, %r65, 4;
	mov.u64 	%rd257, %rd256;
	@%p3 bra 	$L__BB0_6;

	or.b64  	%rd124, %rd256, %rd25;
	and.b64  	%rd125, %rd124, -4294967296;
	setp.eq.s64 	%p4, %rd125, 0;
	@%p4 bra 	$L__BB0_5;

	div.u64 	%rd257, %rd256, %rd25;
	bra.uni 	$L__BB0_6;

$L__BB0_5:
	cvt.u32.u64 	%r432, %rd25;
	cvt.u32.u64 	%r433, %rd256;
	div.u32 	%r434, %r433, %r432;
	cvt.u64.u32 	%rd257, %r434;

$L__BB0_6:
	setp.lt.s32 	%p5, %r65, 3;
	@%p5 bra 	$L__BB0_10;

	or.b64  	%rd126, %rd257, %rd26;
	and.b64  	%rd127, %rd126, -4294967296;
	setp.eq.s64 	%p6, %rd127, 0;
	@%p6 bra 	$L__BB0_9;

	div.u64 	%rd257, %rd257, %rd26;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r435, %rd26;
	cvt.u32.u64 	%r436, %rd257;
	div.u32 	%r437, %r436, %r435;
	cvt.u64.u32 	%rd257, %r437;

$L__BB0_10:
	setp.lt.s32 	%p7, %r65, 2;
	@%p7 bra 	$L__BB0_14;

	or.b64  	%rd128, %rd257, %rd27;
	and.b64  	%rd129, %rd128, -4294967296;
	setp.eq.s64 	%p8, %rd129, 0;
	@%p8 bra 	$L__BB0_13;

	div.u64 	%rd257, %rd257, %rd27;
	bra.uni 	$L__BB0_14;

$L__BB0_13:
	cvt.u32.u64 	%r438, %rd27;
	cvt.u32.u64 	%r439, %rd257;
	div.u32 	%r440, %r439, %r438;
	cvt.u64.u32 	%rd257, %r440;

$L__BB0_14:
	cvt.s64.s32 	%rd130, %rd257;
	setp.gt.s32 	%p9, %r65, 0;
	selp.b64 	%rd62, %rd130, 0, %p9;
	mul.lo.s64 	%rd131, %rd62, %rd28;
	add.s64 	%rd132, %rd24, %rd131;
	ld.global.u32 	%r2, [%rd132];
	setp.lt.u32 	%p10, %r2, 2;
	mul.lo.s64 	%rd133, %rd62, %rd30;
	add.s64 	%rd63, %rd10, %rd133;
	@%p10 bra 	$L__BB0_80;
	bra.uni 	$L__BB0_15;

$L__BB0_80:
	mul.lo.s64 	%rd193, %rd62, %rd29;
	add.s64 	%rd194, %rd9, %rd193;
	ld.global.u32 	%r491, [%rd63];
	ld.global.u32 	%r492, [%rd194];
	setp.eq.s32 	%p59, %r2, 1;
	selp.b32 	%r493, %r491, %r492, %p59;
	selp.b32 	%r494, %r492, %r491, %p59;
	cvt.s64.s32 	%rd195, %r494;
	mul.lo.s64 	%rd196, %rd195, %rd31;
	add.s64 	%rd197, %rd15, %rd196;
	cvt.s64.s32 	%rd72, %r493;
	mul.lo.s64 	%rd198, %rd72, %rd32;
	add.s64 	%rd199, %rd23, %rd198;
	ld.global.s32 	%rd73, [%rd199];
	mul.lo.s64 	%rd200, %rd73, %rd33;
	add.s64 	%rd201, %rd6, %rd200;
	ld.global.s32 	%rd202, [%rd197];
	mul.lo.s64 	%rd203, %rd202, %rd33;
	add.s64 	%rd204, %rd6, %rd203;
	ld.global.s32 	%rd205, [%rd197+4];
	mul.lo.s64 	%rd206, %rd205, %rd33;
	add.s64 	%rd207, %rd6, %rd206;
	ld.global.s32 	%rd208, [%rd197+8];
	mul.lo.s64 	%rd209, %rd208, %rd33;
	add.s64 	%rd210, %rd6, %rd209;
	mul.lo.s64 	%rd211, %rd73, %rd34;
	add.s64 	%rd212, %rd5, %rd211;
	ld.global.f64 	%fd111, [%rd212];
	ld.global.f64 	%fd112, [%rd212+8];
	ld.global.f64 	%fd113, [%rd212+16];
	mul.lo.s64 	%rd213, %rd202, %rd34;
	add.s64 	%rd214, %rd5, %rd213;
	ld.global.f64 	%fd114, [%rd214];
	ld.global.f64 	%fd115, [%rd214+8];
	ld.global.f64 	%fd116, [%rd214+16];
	mul.lo.s64 	%rd215, %rd205, %rd34;
	add.s64 	%rd216, %rd5, %rd215;
	ld.global.f64 	%fd117, [%rd216];
	ld.global.f64 	%fd118, [%rd216+8];
	ld.global.f64 	%fd119, [%rd216+16];
	mul.lo.s64 	%rd217, %rd208, %rd34;
	add.s64 	%rd218, %rd5, %rd217;
	ld.global.f64 	%fd120, [%rd218];
	ld.global.f64 	%fd121, [%rd218+8];
	ld.global.f64 	%fd122, [%rd218+16];
	mul.lo.s64 	%rd219, %rd62, %rd35;
	add.s64 	%rd220, %rd12, %rd219;
	ld.global.f64 	%fd123, [%rd220];
	ld.global.f64 	%fd124, [%rd220+8];
	mul.lo.s64 	%rd221, %rd72, %rd36;
	add.s64 	%rd222, %rd21, %rd221;
	mul.lo.s64 	%rd223, %rd195, %rd37;
	add.s64 	%rd224, %rd19, %rd223;
	ld.global.f64 	%fd661, [%rd224];
	ld.global.f64 	%fd662, [%rd222];
	add.f64 	%fd125, %fd662, %fd661;
	ld.global.f64 	%fd126, [%rd207];
	ld.global.f64 	%fd127, [%rd204];
	sub.f64 	%fd663, %fd126, %fd127;
	ld.global.f64 	%fd128, [%rd207+8];
	ld.global.f64 	%fd129, [%rd204+8];
	sub.f64 	%fd664, %fd128, %fd129;
	ld.global.f64 	%fd130, [%rd207+16];
	ld.global.f64 	%fd131, [%rd204+16];
	sub.f64 	%fd665, %fd130, %fd131;
	ld.global.f64 	%fd132, [%rd210];
	sub.f64 	%fd666, %fd132, %fd127;
	ld.global.f64 	%fd133, [%rd210+8];
	sub.f64 	%fd667, %fd133, %fd129;
	ld.global.f64 	%fd134, [%rd210+16];
	sub.f64 	%fd668, %fd134, %fd131;
	mul.f64 	%fd669, %fd664, %fd668;
	mul.f64 	%fd670, %fd665, %fd667;
	sub.f64 	%fd135, %fd669, %fd670;
	mul.f64 	%fd671, %fd665, %fd666;
	mul.f64 	%fd672, %fd663, %fd668;
	sub.f64 	%fd136, %fd671, %fd672;
	mul.f64 	%fd673, %fd663, %fd667;
	mul.f64 	%fd674, %fd664, %fd666;
	sub.f64 	%fd137, %fd673, %fd674;
	mul.f64 	%fd675, %fd664, %fd137;
	mul.f64 	%fd676, %fd665, %fd136;
	sub.f64 	%fd677, %fd675, %fd676;
	mul.f64 	%fd678, %fd665, %fd135;
	mul.f64 	%fd679, %fd663, %fd137;
	sub.f64 	%fd680, %fd678, %fd679;
	mul.f64 	%fd681, %fd663, %fd136;
	mul.f64 	%fd682, %fd664, %fd135;
	sub.f64 	%fd683, %fd681, %fd682;
	mul.f64 	%fd684, %fd664, %fd664;
	fma.rn.f64 	%fd685, %fd663, %fd663, %fd684;
	fma.rn.f64 	%fd138, %fd665, %fd665, %fd685;
	mul.f64 	%fd686, %fd664, %fd680;
	fma.rn.f64 	%fd687, %fd663, %fd677, %fd686;
	fma.rn.f64 	%fd688, %fd665, %fd683, %fd687;
	mul.f64 	%fd689, %fd680, %fd680;
	fma.rn.f64 	%fd690, %fd677, %fd677, %fd689;
	fma.rn.f64 	%fd691, %fd683, %fd683, %fd690;
	ld.global.f64 	%fd139, [%rd201];
	sub.f64 	%fd140, %fd139, %fd127;
	ld.global.f64 	%fd141, [%rd201+8];
	sub.f64 	%fd142, %fd141, %fd129;
	ld.global.f64 	%fd143, [%rd201+16];
	sub.f64 	%fd144, %fd143, %fd131;
	mul.f64 	%fd692, %fd142, %fd664;
	fma.rn.f64 	%fd693, %fd140, %fd663, %fd692;
	fma.rn.f64 	%fd694, %fd144, %fd665, %fd693;
	mul.f64 	%fd695, %fd142, %fd680;
	fma.rn.f64 	%fd696, %fd140, %fd677, %fd695;
	fma.rn.f64 	%fd697, %fd144, %fd683, %fd696;
	div.rn.f64 	%fd698, %fd688, %fd138;
	mul.f64 	%fd699, %fd698, %fd698;
	mul.f64 	%fd700, %fd138, %fd699;
	sub.f64 	%fd701, %fd691, %fd700;
	mul.f64 	%fd702, %fd694, %fd698;
	sub.f64 	%fd703, %fd697, %fd702;
	div.rn.f64 	%fd704, %fd703, %fd701;
	mul.f64 	%fd705, %fd138, %fd698;
	mul.f64 	%fd706, %fd705, %fd704;
	sub.f64 	%fd707, %fd694, %fd706;
	div.rn.f64 	%fd145, %fd707, %fd138;
	setp.gt.f64 	%p60, %fd145, 0d0000000000000000;
	setp.lt.f64 	%p61, %fd145, 0d3FF0000000000000;
	setp.ge.f64 	%p62, %fd704, 0d0000000000000000;
	and.pred  	%p63, %p60, %p61;
	and.pred  	%p64, %p62, %p63;
	mov.u32 	%r541, 3;
	@%p64 bra 	$L__BB0_86;

	sub.f64 	%fd708, %fd132, %fd126;
	sub.f64 	%fd709, %fd133, %fd128;
	mul.f64 	%fd710, %fd709, %fd137;
	sub.f64 	%fd711, %fd134, %fd130;
	mul.f64 	%fd712, %fd711, %fd136;
	sub.f64 	%fd713, %fd710, %fd712;
	mul.f64 	%fd714, %fd711, %fd135;
	mul.f64 	%fd715, %fd708, %fd137;
	sub.f64 	%fd716, %fd714, %fd715;
	mul.f64 	%fd717, %fd708, %fd136;
	mul.f64 	%fd718, %fd709, %fd135;
	sub.f64 	%fd719, %fd717, %fd718;
	mul.f64 	%fd720, %fd709, %fd709;
	fma.rn.f64 	%fd721, %fd708, %fd708, %fd720;
	fma.rn.f64 	%fd722, %fd711, %fd711, %fd721;
	mul.f64 	%fd723, %fd709, %fd716;
	fma.rn.f64 	%fd724, %fd708, %fd713, %fd723;
	fma.rn.f64 	%fd725, %fd711, %fd719, %fd724;
	mul.f64 	%fd726, %fd716, %fd716;
	fma.rn.f64 	%fd727, %fd713, %fd713, %fd726;
	fma.rn.f64 	%fd728, %fd719, %fd719, %fd727;
	sub.f64 	%fd729, %fd139, %fd126;
	sub.f64 	%fd730, %fd141, %fd128;
	mul.f64 	%fd731, %fd730, %fd709;
	fma.rn.f64 	%fd732, %fd729, %fd708, %fd731;
	sub.f64 	%fd733, %fd143, %fd130;
	fma.rn.f64 	%fd734, %fd733, %fd711, %fd732;
	mul.f64 	%fd735, %fd730, %fd716;
	fma.rn.f64 	%fd736, %fd729, %fd713, %fd735;
	fma.rn.f64 	%fd737, %fd733, %fd719, %fd736;
	div.rn.f64 	%fd738, %fd725, %fd722;
	mul.f64 	%fd739, %fd738, %fd738;
	mul.f64 	%fd740, %fd722, %fd739;
	sub.f64 	%fd741, %fd728, %fd740;
	mul.f64 	%fd742, %fd734, %fd738;
	sub.f64 	%fd743, %fd737, %fd742;
	div.rn.f64 	%fd744, %fd743, %fd741;
	mul.f64 	%fd745, %fd722, %fd738;
	mul.f64 	%fd746, %fd745, %fd744;
	sub.f64 	%fd747, %fd734, %fd746;
	div.rn.f64 	%fd146, %fd747, %fd722;
	setp.gt.f64 	%p65, %fd146, 0d0000000000000000;
	setp.lt.f64 	%p66, %fd146, 0d3FF0000000000000;
	setp.ge.f64 	%p67, %fd744, 0d0000000000000000;
	and.pred  	%p68, %p65, %p66;
	and.pred  	%p69, %p67, %p68;
	mov.u32 	%r541, 4;
	@%p69 bra 	$L__BB0_86;

	sub.f64 	%fd748, %fd127, %fd132;
	sub.f64 	%fd749, %fd129, %fd133;
	mul.f64 	%fd750, %fd749, %fd137;
	sub.f64 	%fd751, %fd131, %fd134;
	mul.f64 	%fd752, %fd751, %fd136;
	sub.f64 	%fd753, %fd750, %fd752;
	mul.f64 	%fd754, %fd751, %fd135;
	mul.f64 	%fd755, %fd748, %fd137;
	sub.f64 	%fd756, %fd754, %fd755;
	mul.f64 	%fd757, %fd748, %fd136;
	mul.f64 	%fd758, %fd749, %fd135;
	sub.f64 	%fd759, %fd757, %fd758;
	mul.f64 	%fd760, %fd749, %fd749;
	fma.rn.f64 	%fd761, %fd748, %fd748, %fd760;
	fma.rn.f64 	%fd762, %fd751, %fd751, %fd761;
	mul.f64 	%fd763, %fd749, %fd756;
	fma.rn.f64 	%fd764, %fd748, %fd753, %fd763;
	fma.rn.f64 	%fd765, %fd751, %fd759, %fd764;
	mul.f64 	%fd766, %fd756, %fd756;
	fma.rn.f64 	%fd767, %fd753, %fd753, %fd766;
	fma.rn.f64 	%fd768, %fd759, %fd759, %fd767;
	sub.f64 	%fd769, %fd139, %fd132;
	sub.f64 	%fd770, %fd141, %fd133;
	mul.f64 	%fd771, %fd749, %fd770;
	fma.rn.f64 	%fd772, %fd748, %fd769, %fd771;
	sub.f64 	%fd773, %fd143, %fd134;
	fma.rn.f64 	%fd774, %fd751, %fd773, %fd772;
	mul.f64 	%fd775, %fd770, %fd756;
	fma.rn.f64 	%fd776, %fd769, %fd753, %fd775;
	fma.rn.f64 	%fd777, %fd773, %fd759, %fd776;
	div.rn.f64 	%fd778, %fd765, %fd762;
	mul.f64 	%fd779, %fd778, %fd778;
	mul.f64 	%fd780, %fd762, %fd779;
	sub.f64 	%fd781, %fd768, %fd780;
	mul.f64 	%fd782, %fd774, %fd778;
	sub.f64 	%fd783, %fd777, %fd782;
	div.rn.f64 	%fd784, %fd783, %fd781;
	mul.f64 	%fd785, %fd762, %fd778;
	mul.f64 	%fd786, %fd785, %fd784;
	sub.f64 	%fd787, %fd774, %fd786;
	div.rn.f64 	%fd147, %fd787, %fd762;
	setp.gt.f64 	%p70, %fd147, 0d0000000000000000;
	setp.lt.f64 	%p71, %fd147, 0d3FF0000000000000;
	setp.ge.f64 	%p72, %fd784, 0d0000000000000000;
	and.pred  	%p73, %p70, %p71;
	and.pred  	%p74, %p72, %p73;
	mov.u32 	%r541, 5;
	@%p74 bra 	$L__BB0_86;

	setp.le.f64 	%p75, %fd145, 0d0000000000000000;
	setp.ge.f64 	%p76, %fd147, 0d3FF0000000000000;
	and.pred  	%p77, %p75, %p76;
	mov.u32 	%r541, 0;
	@%p77 bra 	$L__BB0_86;

	setp.le.f64 	%p78, %fd146, 0d0000000000000000;
	setp.ge.f64 	%p79, %fd145, 0d3FF0000000000000;
	and.pred  	%p80, %p78, %p79;
	mov.u32 	%r541, 1;
	@%p80 bra 	$L__BB0_86;

	setp.le.f64 	%p81, %fd147, 0d0000000000000000;
	setp.ge.f64 	%p82, %fd146, 0d3FF0000000000000;
	and.pred  	%p83, %p81, %p82;
	selp.b32 	%r541, 2, 6, %p83;

$L__BB0_86:
	setp.eq.s32 	%p84, %r541, 0;
	@%p84 bra 	$L__BB0_98;

	setp.eq.s32 	%p85, %r541, 1;
	@%p85 bra 	$L__BB0_97;
	bra.uni 	$L__BB0_88;

$L__BB0_97:
	sub.f64 	%fd866, %fd139, %fd126;
	sub.f64 	%fd867, %fd141, %fd128;
	mul.f64 	%fd868, %fd867, %fd867;
	fma.rn.f64 	%fd869, %fd866, %fd866, %fd868;
	sub.f64 	%fd870, %fd143, %fd130;
	fma.rn.f64 	%fd1126, %fd870, %fd870, %fd869;
	bra.uni 	$L__BB0_99;

$L__BB0_15:
	mul.lo.s64 	%rd134, %rd62, %rd29;
	add.s64 	%rd135, %rd9, %rd134;
	mul.lo.s64 	%rd136, %rd62, %rd35;
	add.s64 	%rd137, %rd12, %rd136;
	ld.global.s32 	%rd64, [%rd63];
	mul.lo.s64 	%rd138, %rd64, %rd39;
	add.s64 	%rd139, %rd16, %rd138;
	ld.global.s32 	%rd65, [%rd135];
	mul.lo.s64 	%rd140, %rd65, %rd39;
	add.s64 	%rd141, %rd16, %rd140;
	mul.lo.s64 	%rd142, %rd64, %rd40;
	add.s64 	%rd143, %rd20, %rd142;
	mul.lo.s64 	%rd144, %rd65, %rd40;
	add.s64 	%rd145, %rd20, %rd144;
	ld.global.f64 	%fd200, [%rd145];
	ld.global.f64 	%fd201, [%rd143];
	add.f64 	%fd7, %fd201, %fd200;
	ld.global.s32 	%rd66, [%rd139];
	mul.lo.s64 	%rd146, %rd66, %rd33;
	add.s64 	%rd147, %rd6, %rd146;
	ld.global.s32 	%rd67, [%rd139+4];
	mul.lo.s64 	%rd148, %rd67, %rd33;
	add.s64 	%rd149, %rd6, %rd148;
	ld.global.s32 	%rd68, [%rd141];
	mul.lo.s64 	%rd150, %rd68, %rd33;
	add.s64 	%rd151, %rd6, %rd150;
	ld.global.s32 	%rd69, [%rd141+4];
	mul.lo.s64 	%rd152, %rd69, %rd33;
	add.s64 	%rd153, %rd6, %rd152;
	mul.lo.s64 	%rd154, %rd66, %rd34;
	add.s64 	%rd155, %rd5, %rd154;
	mul.lo.s64 	%rd156, %rd67, %rd34;
	add.s64 	%rd157, %rd5, %rd156;
	mul.lo.s64 	%rd158, %rd68, %rd34;
	add.s64 	%rd159, %rd5, %rd158;
	mul.lo.s64 	%rd160, %rd69, %rd34;
	add.s64 	%rd161, %rd5, %rd160;
	ld.global.f64 	%fd202, [%rd137];
	mov.f64 	%fd203, 0d3FF0000000000000;
	sub.f64 	%fd204, %fd203, %fd202;
	ld.global.f64 	%fd8, [%rd147];
	mul.f64 	%fd205, %fd8, %fd204;
	ld.global.f64 	%fd9, [%rd149];
	fma.rn.f64 	%fd206, %fd9, %fd202, %fd205;
	ld.global.f64 	%fd207, [%rd137+8];
	sub.f64 	%fd208, %fd203, %fd207;
	ld.global.f64 	%fd10, [%rd151];
	mul.f64 	%fd209, %fd10, %fd208;
	sub.f64 	%fd210, %fd206, %fd209;
	ld.global.f64 	%fd11, [%rd153];
	mul.f64 	%fd211, %fd11, %fd207;
	sub.f64 	%fd12, %fd210, %fd211;
	ld.global.f64 	%fd13, [%rd147+8];
	mul.f64 	%fd212, %fd13, %fd204;
	ld.global.f64 	%fd14, [%rd149+8];
	fma.rn.f64 	%fd213, %fd14, %fd202, %fd212;
	ld.global.f64 	%fd15, [%rd151+8];
	mul.f64 	%fd214, %fd15, %fd208;
	sub.f64 	%fd215, %fd213, %fd214;
	ld.global.f64 	%fd16, [%rd153+8];
	mul.f64 	%fd216, %fd16, %fd207;
	sub.f64 	%fd17, %fd215, %fd216;
	ld.global.f64 	%fd18, [%rd147+16];
	mul.f64 	%fd217, %fd18, %fd204;
	ld.global.f64 	%fd19, [%rd149+16];
	fma.rn.f64 	%fd218, %fd19, %fd202, %fd217;
	ld.global.f64 	%fd20, [%rd151+16];
	mul.f64 	%fd219, %fd20, %fd208;
	sub.f64 	%fd220, %fd218, %fd219;
	ld.global.f64 	%fd21, [%rd153+16];
	mul.f64 	%fd221, %fd21, %fd207;
	sub.f64 	%fd22, %fd220, %fd221;
	ld.global.f64 	%fd222, [%rd155];
	mul.f64 	%fd223, %fd222, %fd204;
	ld.global.f64 	%fd224, [%rd157];
	fma.rn.f64 	%fd225, %fd224, %fd202, %fd223;
	ld.global.f64 	%fd226, [%rd159];
	mul.f64 	%fd227, %fd226, %fd208;
	sub.f64 	%fd228, %fd225, %fd227;
	ld.global.f64 	%fd229, [%rd161];
	mul.f64 	%fd230, %fd229, %fd207;
	sub.f64 	%fd23, %fd228, %fd230;
	ld.global.f64 	%fd231, [%rd155+8];
	mul.f64 	%fd232, %fd231, %fd204;
	ld.global.f64 	%fd233, [%rd157+8];
	fma.rn.f64 	%fd234, %fd233, %fd202, %fd232;
	ld.global.f64 	%fd235, [%rd159+8];
	mul.f64 	%fd236, %fd235, %fd208;
	sub.f64 	%fd237, %fd234, %fd236;
	ld.global.f64 	%fd238, [%rd161+8];
	mul.f64 	%fd239, %fd238, %fd207;
	sub.f64 	%fd24, %fd237, %fd239;
	ld.global.f64 	%fd240, [%rd155+16];
	mul.f64 	%fd241, %fd240, %fd204;
	ld.global.f64 	%fd242, [%rd157+16];
	fma.rn.f64 	%fd243, %fd242, %fd202, %fd241;
	ld.global.f64 	%fd244, [%rd159+16];
	mul.f64 	%fd245, %fd244, %fd208;
	sub.f64 	%fd246, %fd243, %fd245;
	ld.global.f64 	%fd247, [%rd161+16];
	mul.f64 	%fd248, %fd247, %fd207;
	sub.f64 	%fd25, %fd246, %fd248;
	sub.f64 	%fd26, %fd9, %fd8;
	sub.f64 	%fd27, %fd14, %fd13;
	sub.f64 	%fd28, %fd19, %fd18;
	sub.f64 	%fd29, %fd11, %fd10;
	sub.f64 	%fd30, %fd16, %fd15;
	sub.f64 	%fd31, %fd21, %fd20;
	sub.f64 	%fd32, %fd8, %fd10;
	sub.f64 	%fd33, %fd13, %fd15;
	sub.f64 	%fd34, %fd18, %fd20;
	mul.f64 	%fd249, %fd27, %fd27;
	fma.rn.f64 	%fd250, %fd26, %fd26, %fd249;
	fma.rn.f64 	%fd35, %fd28, %fd28, %fd250;
	mul.f64 	%fd251, %fd27, %fd30;
	fma.rn.f64 	%fd252, %fd26, %fd29, %fd251;
	fma.rn.f64 	%fd36, %fd28, %fd31, %fd252;
	mul.f64 	%fd253, %fd30, %fd30;
	fma.rn.f64 	%fd254, %fd29, %fd29, %fd253;
	fma.rn.f64 	%fd37, %fd31, %fd31, %fd254;
	mul.f64 	%fd255, %fd27, %fd33;
	fma.rn.f64 	%fd256, %fd26, %fd32, %fd255;
	fma.rn.f64 	%fd38, %fd28, %fd34, %fd256;
	mul.f64 	%fd257, %fd33, %fd30;
	fma.rn.f64 	%fd258, %fd32, %fd29, %fd257;
	fma.rn.f64 	%fd39, %fd34, %fd31, %fd258;
	mul.f64 	%fd259, %fd35, %fd37;
	mul.f64 	%fd260, %fd36, %fd36;
	sub.f64 	%fd40, %fd259, %fd260;
	mul.f64 	%fd261, %fd36, %fd39;
	mul.f64 	%fd262, %fd38, %fd37;
	sub.f64 	%fd41, %fd261, %fd262;
	setp.le.f64 	%p11, %fd41, 0d0000000000000000;
	@%p11 bra 	$L__BB0_19;

	setp.ge.f64 	%p1, %fd41, %fd40;
	add.f64 	%fd42, %fd39, %fd36;
	@%p1 bra 	$L__BB0_18;

	sub.f64 	%fd1088, %fd18, %fd20;
	sub.f64 	%fd1087, %fd8, %fd10;
	sub.f64 	%fd1086, %fd13, %fd15;
	sub.f64 	%fd1085, %fd11, %fd10;
	sub.f64 	%fd1084, %fd9, %fd8;
	sub.f64 	%fd1083, %fd21, %fd20;
	sub.f64 	%fd1082, %fd14, %fd13;
	sub.f64 	%fd1081, %fd16, %fd15;
	sub.f64 	%fd1080, %fd19, %fd18;
	selp.f64 	%fd264, %fd37, %fd40, %p1;
	mul.f64 	%fd265, %fd35, %fd39;
	mul.f64 	%fd266, %fd38, %fd36;
	sub.f64 	%fd267, %fd265, %fd266;
	mul.f64 	%fd268, %fd1080, %fd1081;
	mul.f64 	%fd269, %fd1082, %fd1083;
	sub.f64 	%fd270, %fd269, %fd268;
	mul.f64 	%fd271, %fd1084, %fd1083;
	mul.f64 	%fd272, %fd1080, %fd1085;
	sub.f64 	%fd273, %fd272, %fd271;
	mul.f64 	%fd274, %fd1082, %fd1085;
	mul.f64 	%fd275, %fd1084, %fd1081;
	sub.f64 	%fd276, %fd275, %fd274;
	setp.gt.f64 	%p12, %fd267, 0d0000000000000000;
	setp.lt.f64 	%p13, %fd267, %fd264;
	mul.f64 	%fd277, %fd1086, %fd273;
	fma.rn.f64 	%fd278, %fd1087, %fd270, %fd277;
	fma.rn.f64 	%fd279, %fd1088, %fd276, %fd278;
	setp.eq.f64 	%p14, %fd279, 0d0000000000000000;
	mul.f64 	%fd280, %fd273, %fd273;
	fma.rn.f64 	%fd281, %fd270, %fd270, %fd280;
	fma.rn.f64 	%fd282, %fd276, %fd276, %fd281;
	mul.f64 	%fd283, %fd35, 0d3BC79CA100000000;
	mul.f64 	%fd284, %fd283, %fd37;
	setp.lt.f64 	%p15, %fd282, %fd284;
	or.pred  	%p16, %p14, %p15;
	and.pred  	%p17, %p12, %p13;
	and.pred  	%p18, %p16, %p17;
	mul.f64 	%fd285, %fd40, 0d3FE0000000000000;
	setp.lt.f64 	%p19, %fd41, %fd285;
	selp.b32 	%r443, 2, 5, %p19;
	selp.f64 	%fd286, %fd39, %fd42, %p19;
	selp.f64 	%fd1108, %fd37, %fd264, %p18;
	selp.b32 	%r526, %r443, 8, %p18;
	selp.f64 	%fd1107, %fd286, %fd267, %p18;

$L__BB0_18:
	selp.f64 	%fd1110, %fd37, %fd1108, %p1;
	selp.b32 	%r527, 5, %r526, %p1;
	selp.f64 	%fd1109, %fd42, %fd1107, %p1;

$L__BB0_19:
	selp.f64 	%fd51, %fd37, %fd1110, %p11;
	selp.b32 	%r528, 2, %r527, %p11;
	selp.f64 	%fd52, %fd39, %fd1109, %p11;
	setp.gtu.f64 	%p22, %fd52, 0d0000000000000000;
	@%p22 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_20;

$L__BB0_23:
	setp.ltu.f64 	%p25, %fd52, %fd51;
	@%p25 bra 	$L__BB0_27;

	mov.f64 	%fd288, 0d0000000000000000;
	sub.f64 	%fd289, %fd288, %fd38;
	add.f64 	%fd54, %fd289, %fd36;
	setp.le.f64 	%p26, %fd54, 0d0000000000000000;
	mov.u32 	%r528, 1;
	@%p26 bra 	$L__BB0_27;

	setp.ge.f64 	%p27, %fd54, %fd35;
	mov.u32 	%r528, 4;
	@%p27 bra 	$L__BB0_27;

	mov.u32 	%r528, 7;
	bra.uni 	$L__BB0_27;

$L__BB0_20:
	mov.f64 	%fd287, 0d0000000000000000;
	sub.f64 	%fd53, %fd287, %fd38;
	setp.le.f64 	%p23, %fd53, 0d0000000000000000;
	mov.u32 	%r528, 0;
	@%p23 bra 	$L__BB0_27;

	setp.ge.f64 	%p24, %fd53, %fd35;
	mov.u32 	%r528, 3;
	@%p24 bra 	$L__BB0_27;

	mov.u32 	%r528, 6;

$L__BB0_27:
	setp.eq.s32 	%p28, %r528, 0;
	@%p28 bra 	$L__BB0_43;

	setp.eq.s32 	%p29, %r528, 1;
	@%p29 bra 	$L__BB0_42;
	bra.uni 	$L__BB0_29;

$L__BB0_42:
	sub.f64 	%fd388, %fd8, %fd11;
	sub.f64 	%fd389, %fd13, %fd16;
	mul.f64 	%fd390, %fd389, %fd389;
	fma.rn.f64 	%fd391, %fd388, %fd388, %fd390;
	sub.f64 	%fd392, %fd18, %fd21;
	fma.rn.f64 	%fd1111, %fd392, %fd392, %fd391;
	bra.uni 	$L__BB0_44;

$L__BB0_98:
	sub.f64 	%fd1079, %fd143, %fd131;
	sub.f64 	%fd1078, %fd139, %fd127;
	sub.f64 	%fd1077, %fd141, %fd129;
	mul.f64 	%fd871, %fd1077, %fd1077;
	fma.rn.f64 	%fd872, %fd1078, %fd1078, %fd871;
	fma.rn.f64 	%fd1126, %fd1079, %fd1079, %fd872;
	bra.uni 	$L__BB0_99;

$L__BB0_43:
	sub.f64 	%fd1106, %fd18, %fd20;
	sub.f64 	%fd1105, %fd8, %fd10;
	sub.f64 	%fd1104, %fd13, %fd15;
	mul.f64 	%fd393, %fd1104, %fd1104;
	fma.rn.f64 	%fd394, %fd1105, %fd1105, %fd393;
	fma.rn.f64 	%fd1111, %fd1106, %fd1106, %fd394;
	bra.uni 	$L__BB0_44;

$L__BB0_88:
	setp.eq.s32 	%p86, %r541, 2;
	@%p86 bra 	$L__BB0_96;
	bra.uni 	$L__BB0_89;

$L__BB0_96:
	sub.f64 	%fd861, %fd139, %fd132;
	sub.f64 	%fd862, %fd141, %fd133;
	mul.f64 	%fd863, %fd862, %fd862;
	fma.rn.f64 	%fd864, %fd861, %fd861, %fd863;
	sub.f64 	%fd865, %fd143, %fd134;
	fma.rn.f64 	%fd1126, %fd865, %fd865, %fd864;
	bra.uni 	$L__BB0_99;

$L__BB0_29:
	setp.eq.s32 	%p30, %r528, 2;
	@%p30 bra 	$L__BB0_41;
	bra.uni 	$L__BB0_30;

$L__BB0_41:
	sub.f64 	%fd370, %fd10, %fd8;
	sub.f64 	%fd371, %fd21, %fd18;
	sub.f64 	%fd372, %fd15, %fd13;
	mul.f64 	%fd373, %fd372, %fd371;
	sub.f64 	%fd374, %fd16, %fd13;
	sub.f64 	%fd375, %fd20, %fd18;
	mul.f64 	%fd376, %fd375, %fd374;
	sub.f64 	%fd377, %fd373, %fd376;
	sub.f64 	%fd378, %fd11, %fd8;
	mul.f64 	%fd379, %fd375, %fd378;
	mul.f64 	%fd380, %fd370, %fd371;
	sub.f64 	%fd381, %fd379, %fd380;
	mul.f64 	%fd382, %fd370, %fd374;
	mul.f64 	%fd383, %fd372, %fd378;
	sub.f64 	%fd384, %fd382, %fd383;
	mul.f64 	%fd385, %fd381, %fd381;
	fma.rn.f64 	%fd386, %fd377, %fd377, %fd385;
	fma.rn.f64 	%fd387, %fd384, %fd384, %fd386;
	div.rn.f64 	%fd1111, %fd387, %fd37;
	bra.uni 	$L__BB0_44;

$L__BB0_89:
	setp.eq.s32 	%p87, %r541, 3;
	@%p87 bra 	$L__BB0_95;
	bra.uni 	$L__BB0_90;

$L__BB0_95:
	sub.f64 	%fd843, %fd127, %fd139;
	sub.f64 	%fd844, %fd130, %fd143;
	sub.f64 	%fd845, %fd129, %fd141;
	mul.f64 	%fd846, %fd845, %fd844;
	sub.f64 	%fd847, %fd128, %fd141;
	sub.f64 	%fd848, %fd131, %fd143;
	mul.f64 	%fd849, %fd848, %fd847;
	sub.f64 	%fd850, %fd846, %fd849;
	sub.f64 	%fd851, %fd126, %fd139;
	mul.f64 	%fd852, %fd848, %fd851;
	mul.f64 	%fd853, %fd843, %fd844;
	sub.f64 	%fd854, %fd852, %fd853;
	mul.f64 	%fd855, %fd843, %fd847;
	mul.f64 	%fd856, %fd845, %fd851;
	sub.f64 	%fd857, %fd855, %fd856;
	mul.f64 	%fd858, %fd854, %fd854;
	fma.rn.f64 	%fd859, %fd850, %fd850, %fd858;
	fma.rn.f64 	%fd860, %fd857, %fd857, %fd859;
	div.rn.f64 	%fd1126, %fd860, %fd138;
	bra.uni 	$L__BB0_99;

$L__BB0_30:
	setp.eq.s32 	%p31, %r528, 3;
	@%p31 bra 	$L__BB0_40;
	bra.uni 	$L__BB0_31;

$L__BB0_40:
	sub.f64 	%fd365, %fd9, %fd10;
	sub.f64 	%fd366, %fd14, %fd15;
	mul.f64 	%fd367, %fd366, %fd366;
	fma.rn.f64 	%fd368, %fd365, %fd365, %fd367;
	sub.f64 	%fd369, %fd19, %fd20;
	fma.rn.f64 	%fd1111, %fd369, %fd369, %fd368;
	bra.uni 	$L__BB0_44;

$L__BB0_90:
	setp.eq.s32 	%p88, %r541, 4;
	@%p88 bra 	$L__BB0_94;
	bra.uni 	$L__BB0_91;

$L__BB0_94:
	sub.f64 	%fd819, %fd126, %fd139;
	sub.f64 	%fd820, %fd134, %fd143;
	sub.f64 	%fd821, %fd128, %fd141;
	mul.f64 	%fd822, %fd821, %fd820;
	sub.f64 	%fd823, %fd133, %fd141;
	sub.f64 	%fd824, %fd130, %fd143;
	mul.f64 	%fd825, %fd824, %fd823;
	sub.f64 	%fd826, %fd822, %fd825;
	sub.f64 	%fd827, %fd132, %fd139;
	mul.f64 	%fd828, %fd824, %fd827;
	mul.f64 	%fd829, %fd819, %fd820;
	sub.f64 	%fd830, %fd828, %fd829;
	mul.f64 	%fd831, %fd819, %fd823;
	mul.f64 	%fd832, %fd821, %fd827;
	sub.f64 	%fd833, %fd831, %fd832;
	mul.f64 	%fd834, %fd830, %fd830;
	fma.rn.f64 	%fd835, %fd826, %fd826, %fd834;
	fma.rn.f64 	%fd836, %fd833, %fd833, %fd835;
	sub.f64 	%fd837, %fd132, %fd126;
	sub.f64 	%fd838, %fd133, %fd128;
	mul.f64 	%fd839, %fd838, %fd838;
	fma.rn.f64 	%fd840, %fd837, %fd837, %fd839;
	sub.f64 	%fd841, %fd134, %fd130;
	fma.rn.f64 	%fd842, %fd841, %fd841, %fd840;
	div.rn.f64 	%fd1126, %fd836, %fd842;
	bra.uni 	$L__BB0_99;

$L__BB0_31:
	setp.eq.s32 	%p32, %r528, 4;
	@%p32 bra 	$L__BB0_39;
	bra.uni 	$L__BB0_32;

$L__BB0_39:
	sub.f64 	%fd360, %fd9, %fd11;
	sub.f64 	%fd361, %fd14, %fd16;
	mul.f64 	%fd362, %fd361, %fd361;
	fma.rn.f64 	%fd363, %fd360, %fd360, %fd362;
	sub.f64 	%fd364, %fd19, %fd21;
	fma.rn.f64 	%fd1111, %fd364, %fd364, %fd363;
	bra.uni 	$L__BB0_44;

$L__BB0_91:
	setp.eq.s32 	%p89, %r541, 5;
	@%p89 bra 	$L__BB0_93;
	bra.uni 	$L__BB0_92;

$L__BB0_93:
	sub.f64 	%fd795, %fd132, %fd139;
	sub.f64 	%fd796, %fd131, %fd143;
	sub.f64 	%fd797, %fd133, %fd141;
	mul.f64 	%fd798, %fd796, %fd797;
	sub.f64 	%fd799, %fd129, %fd141;
	sub.f64 	%fd800, %fd134, %fd143;
	mul.f64 	%fd801, %fd799, %fd800;
	sub.f64 	%fd802, %fd798, %fd801;
	sub.f64 	%fd803, %fd127, %fd139;
	mul.f64 	%fd804, %fd803, %fd800;
	mul.f64 	%fd805, %fd796, %fd795;
	sub.f64 	%fd806, %fd804, %fd805;
	mul.f64 	%fd807, %fd799, %fd795;
	mul.f64 	%fd808, %fd803, %fd797;
	sub.f64 	%fd809, %fd807, %fd808;
	mul.f64 	%fd810, %fd806, %fd806;
	fma.rn.f64 	%fd811, %fd802, %fd802, %fd810;
	fma.rn.f64 	%fd812, %fd809, %fd809, %fd811;
	sub.f64 	%fd813, %fd127, %fd132;
	sub.f64 	%fd814, %fd129, %fd133;
	mul.f64 	%fd815, %fd814, %fd814;
	fma.rn.f64 	%fd816, %fd813, %fd813, %fd815;
	sub.f64 	%fd817, %fd131, %fd134;
	fma.rn.f64 	%fd818, %fd817, %fd817, %fd816;
	div.rn.f64 	%fd1126, %fd812, %fd818;
	bra.uni 	$L__BB0_99;

$L__BB0_32:
	setp.eq.s32 	%p33, %r528, 5;
	@%p33 bra 	$L__BB0_38;
	bra.uni 	$L__BB0_33;

$L__BB0_38:
	sub.f64 	%fd342, %fd10, %fd9;
	sub.f64 	%fd343, %fd21, %fd19;
	sub.f64 	%fd344, %fd15, %fd14;
	mul.f64 	%fd345, %fd344, %fd343;
	sub.f64 	%fd346, %fd16, %fd14;
	sub.f64 	%fd347, %fd20, %fd19;
	mul.f64 	%fd348, %fd347, %fd346;
	sub.f64 	%fd349, %fd345, %fd348;
	sub.f64 	%fd350, %fd11, %fd9;
	mul.f64 	%fd351, %fd347, %fd350;
	mul.f64 	%fd352, %fd342, %fd343;
	sub.f64 	%fd353, %fd351, %fd352;
	mul.f64 	%fd354, %fd342, %fd346;
	mul.f64 	%fd355, %fd344, %fd350;
	sub.f64 	%fd356, %fd354, %fd355;
	mul.f64 	%fd357, %fd353, %fd353;
	fma.rn.f64 	%fd358, %fd349, %fd349, %fd357;
	fma.rn.f64 	%fd359, %fd356, %fd356, %fd358;
	div.rn.f64 	%fd1111, %fd359, %fd37;
	bra.uni 	$L__BB0_44;

$L__BB0_92:
	sub.f64 	%fd1076, %fd143, %fd131;
	sub.f64 	%fd1075, %fd139, %fd127;
	sub.f64 	%fd1074, %fd141, %fd129;
	mul.f64 	%fd788, %fd1074, %fd136;
	fma.rn.f64 	%fd789, %fd1075, %fd135, %fd788;
	fma.rn.f64 	%fd790, %fd1076, %fd137, %fd789;
	mul.f64 	%fd791, %fd790, %fd790;
	mul.f64 	%fd792, %fd136, %fd136;
	fma.rn.f64 	%fd793, %fd135, %fd135, %fd792;
	fma.rn.f64 	%fd794, %fd137, %fd137, %fd793;
	div.rn.f64 	%fd1126, %fd791, %fd794;

$L__BB0_99:
	mul.f64 	%fd874, %fd125, %fd125;
	sub.f64 	%fd156, %fd1126, %fd874;
	mov.f64 	%fd875, 0d3FF0000000000000;
	sub.f64 	%fd876, %fd875, %fd123;
	sub.f64 	%fd877, %fd876, %fd124;
	mul.f64 	%fd878, %fd127, %fd877;
	mul.f64 	%fd879, %fd129, %fd877;
	mul.f64 	%fd880, %fd131, %fd877;
	sub.f64 	%fd881, %fd139, %fd878;
	sub.f64 	%fd882, %fd141, %fd879;
	sub.f64 	%fd883, %fd143, %fd880;
	mul.f64 	%fd884, %fd126, %fd123;
	sub.f64 	%fd885, %fd881, %fd884;
	mul.f64 	%fd886, %fd128, %fd123;
	sub.f64 	%fd887, %fd882, %fd886;
	mul.f64 	%fd888, %fd130, %fd123;
	sub.f64 	%fd889, %fd883, %fd888;
	mul.f64 	%fd890, %fd132, %fd124;
	sub.f64 	%fd157, %fd885, %fd890;
	mul.f64 	%fd891, %fd133, %fd124;
	sub.f64 	%fd158, %fd887, %fd891;
	mul.f64 	%fd892, %fd134, %fd124;
	sub.f64 	%fd159, %fd889, %fd892;
	mul.f64 	%fd893, %fd114, %fd877;
	mul.f64 	%fd894, %fd115, %fd877;
	mul.f64 	%fd895, %fd116, %fd877;
	sub.f64 	%fd896, %fd111, %fd893;
	sub.f64 	%fd897, %fd112, %fd894;
	sub.f64 	%fd898, %fd113, %fd895;
	mul.f64 	%fd899, %fd117, %fd123;
	sub.f64 	%fd900, %fd896, %fd899;
	mul.f64 	%fd901, %fd118, %fd123;
	sub.f64 	%fd902, %fd897, %fd901;
	mul.f64 	%fd903, %fd119, %fd123;
	sub.f64 	%fd904, %fd898, %fd903;
	mul.f64 	%fd905, %fd120, %fd124;
	sub.f64 	%fd160, %fd900, %fd905;
	mul.f64 	%fd906, %fd121, %fd124;
	sub.f64 	%fd161, %fd902, %fd906;
	mul.f64 	%fd907, %fd122, %fd124;
	sub.f64 	%fd162, %fd904, %fd907;
	fma.rn.f64 	%fd163, %fd2, %fd125, %fd1;
	mul.lo.s64 	%rd225, %rd72, %rd41;
	add.s64 	%rd74, %rd18, %rd225;
	ld.global.f64 	%fd164, [%rd74];
	setp.geu.f64 	%p90, %fd156, %fd163;
	@%p90 bra 	$L__BB0_108;

	div.rn.f64 	%fd1127, %fd156, %fd163;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r542}, %fd1127;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r543, %temp}, %fd1127;
	}
	setp.gt.s32 	%p91, %r542, 1048575;
	mov.u32 	%r544, -1023;
	@%p91 bra 	$L__BB0_102;

	mul.f64 	%fd1127, %fd1127, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r542}, %fd1127;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r543, %temp}, %fd1127;
	}
	mov.u32 	%r544, -1077;

$L__BB0_102:
	add.s32 	%r501, %r542, -1;
	setp.lt.u32 	%p92, %r501, 2146435071;
	@%p92 bra 	$L__BB0_104;
	bra.uni 	$L__BB0_103;

$L__BB0_104:
	shr.u32 	%r503, %r542, 20;
	add.s32 	%r545, %r544, %r503;
	and.b32  	%r504, %r542, -2146435073;
	or.b32  	%r505, %r504, 1072693248;
	mov.b64 	%fd1128, {%r543, %r505};
	setp.lt.s32 	%p94, %r505, 1073127583;
	@%p94 bra 	$L__BB0_106;

	{
	.reg .b32 %temp;
	mov.b64 	{%r506, %temp}, %fd1128;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r507}, %fd1128;
	}
	add.s32 	%r508, %r507, -1048576;
	mov.b64 	%fd1128, {%r506, %r508};
	add.s32 	%r545, %r545, 1;

$L__BB0_106:
	add.f64 	%fd910, %fd1128, 0d3FF0000000000000;
	mov.f64 	%fd911, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd912, %fd910;
	neg.f64 	%fd913, %fd910;
	fma.rn.f64 	%fd914, %fd913, %fd912, %fd911;
	fma.rn.f64 	%fd915, %fd914, %fd914, %fd914;
	fma.rn.f64 	%fd916, %fd915, %fd912, %fd912;
	add.f64 	%fd917, %fd1128, 0dBFF0000000000000;
	mul.f64 	%fd918, %fd917, %fd916;
	fma.rn.f64 	%fd919, %fd917, %fd916, %fd918;
	mul.f64 	%fd920, %fd919, %fd919;
	mov.f64 	%fd921, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd922, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd923, %fd922, %fd920, %fd921;
	mov.f64 	%fd924, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd925, %fd923, %fd920, %fd924;
	mov.f64 	%fd926, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd927, %fd925, %fd920, %fd926;
	mov.f64 	%fd928, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd929, %fd927, %fd920, %fd928;
	mov.f64 	%fd930, 0d3F624924923BE72D;
	fma.rn.f64 	%fd931, %fd929, %fd920, %fd930;
	mov.f64 	%fd932, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd933, %fd931, %fd920, %fd932;
	mov.f64 	%fd934, 0d3FB5555555555554;
	fma.rn.f64 	%fd935, %fd933, %fd920, %fd934;
	sub.f64 	%fd936, %fd917, %fd919;
	add.f64 	%fd937, %fd936, %fd936;
	neg.f64 	%fd938, %fd919;
	fma.rn.f64 	%fd939, %fd938, %fd917, %fd937;
	mul.f64 	%fd940, %fd916, %fd939;
	mul.f64 	%fd941, %fd920, %fd935;
	fma.rn.f64 	%fd942, %fd941, %fd919, %fd940;
	xor.b32  	%r509, %r545, -2147483648;
	mov.u32 	%r510, -2147483648;
	mov.u32 	%r511, 1127219200;
	mov.b64 	%fd943, {%r509, %r511};
	mov.b64 	%fd944, {%r510, %r511};
	sub.f64 	%fd945, %fd943, %fd944;
	mov.f64 	%fd946, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd947, %fd945, %fd946, %fd919;
	neg.f64 	%fd948, %fd945;
	fma.rn.f64 	%fd949, %fd948, %fd946, %fd947;
	sub.f64 	%fd950, %fd949, %fd919;
	sub.f64 	%fd951, %fd942, %fd950;
	mov.f64 	%fd952, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd953, %fd945, %fd952, %fd951;
	add.f64 	%fd1129, %fd947, %fd953;
	bra.uni 	$L__BB0_107;

$L__BB0_103:
	mov.f64 	%fd908, 0d7FF0000000000000;
	fma.rn.f64 	%fd909, %fd1127, %fd908, %fd908;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r502}, %fd1127;
	}
	mov.b32 	%f4, %r502;
	setp.eq.f32 	%p93, %f4, 0f00000000;
	selp.f64 	%fd1129, 0dFFF0000000000000, %fd909, %p93;

$L__BB0_107:
	sub.f64 	%fd954, %fd156, %fd163;
	div.rn.f64 	%fd955, %fd954, %fd163;
	mul.f64 	%fd956, %fd3, %fd955;
	mul.f64 	%fd957, %fd955, %fd956;
	mul.f64 	%fd1130, %fd957, %fd1129;

$L__BB0_108:
	setp.lt.f64 	%p95, %fd156, %fd163;
	selp.f64 	%fd958, %fd1130, 0d0000000000000000, %p95;
	mul.f64 	%fd959, %fd164, %fd195;
	mul.f64 	%fd960, %fd959, %fd192;
	mul.f64 	%fd961, %fd960, %fd958;
	mul.f64 	%fd962, %fd961, %fd196;
	setp.num.f64 	%p96, %fd962, %fd962;
	@%p96 bra 	$L__BB0_110;

	add.u64 	%rd251, %SP, 0;
	add.u64 	%rd250, %SP, 0;
	add.u64 	%rd249, %SPL, 0;
	mov.u64 	%rd226, $str$3;
	cvta.global.u64 	%rd227, %rd226;
	st.local.u64 	[%rd249], %rd227;
	mov.u64 	%rd228, $str$5;
	cvta.global.u64 	%rd229, %rd228;
	{ // callseq 253, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd229;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd250;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r512, [retval0+0];
	} // callseq 253

$L__BB0_110:
	setp.eq.s32 	%p97, %r255, 0;
	@%p97 bra 	$L__BB0_114;

	mul.lo.s64 	%rd231, %rd62, %rd44;
	add.s64 	%rd232, %rd8, %rd231;
	ld.global.s32 	%rd233, [%rd232];
	mul.lo.s64 	%rd234, %rd233, %rd45;
	add.s64 	%rd235, %rd4, %rd234;
	mul.lo.s64 	%rd236, %rd62, %rd46;
	add.s64 	%rd237, %rd7, %rd236;
	ld.global.s32 	%rd238, [%rd237];
	mul.lo.s64 	%rd239, %rd238, %rd45;
	add.s64 	%rd240, %rd4, %rd239;
	ld.global.f64 	%fd963, [%rd235];
	add.f64 	%fd964, %fd963, %fd963;
	ld.global.f64 	%fd965, [%rd240];
	mul.f64 	%fd966, %fd964, %fd965;
	add.f64 	%fd967, %fd963, %fd965;
	setp.neu.f64 	%p98, %fd967, 0d0000000000000000;
	mov.f64 	%fd968, 0d0000000000000000;
	div.rn.f64 	%fd969, %fd966, %fd967;
	selp.f64 	%fd970, %fd969, 0d0000000000000000, %p98;
	mul.lo.s64 	%rd241, %rd62, %rd47;
	add.s64 	%rd242, %rd13, %rd241;
	ld.global.f64 	%fd971, [%rd242];
	mul.f64 	%fd176, %fd971, %fd970;
	mul.lo.s64 	%rd243, %rd62, %rd48;
	add.s64 	%rd244, %rd11, %rd243;
	ld.global.f64 	%fd972, [%rd244];
	mul.f64 	%fd973, %fd972, %fd972;
	ld.global.f64 	%fd974, [%rd244+8];
	mul.f64 	%fd975, %fd972, %fd974;
	ld.global.f64 	%fd976, [%rd244+16];
	mul.f64 	%fd977, %fd972, %fd976;
	mul.f64 	%fd978, %fd974, %fd974;
	mul.f64 	%fd979, %fd974, %fd976;
	mul.f64 	%fd980, %fd976, %fd976;
	mov.f64 	%fd981, 0d3FF0000000000000;
	sub.f64 	%fd982, %fd981, %fd973;
	sub.f64 	%fd983, %fd968, %fd975;
	sub.f64 	%fd984, %fd968, %fd977;
	sub.f64 	%fd985, %fd981, %fd978;
	sub.f64 	%fd986, %fd968, %fd979;
	sub.f64 	%fd987, %fd981, %fd980;
	sub.f64 	%fd988, %fd157, %fd160;
	sub.f64 	%fd989, %fd158, %fd161;
	mul.f64 	%fd990, %fd983, %fd989;
	mul.f64 	%fd991, %fd985, %fd989;
	mul.f64 	%fd992, %fd986, %fd989;
	fma.rn.f64 	%fd993, %fd982, %fd988, %fd990;
	fma.rn.f64 	%fd994, %fd983, %fd988, %fd991;
	fma.rn.f64 	%fd995, %fd984, %fd988, %fd992;
	sub.f64 	%fd996, %fd159, %fd162;
	fma.rn.f64 	%fd997, %fd984, %fd996, %fd993;
	fma.rn.f64 	%fd998, %fd986, %fd996, %fd994;
	fma.rn.f64 	%fd999, %fd987, %fd996, %fd995;
	div.rn.f64 	%fd1000, %fd997, %fd193;
	div.rn.f64 	%fd1001, %fd998, %fd193;
	div.rn.f64 	%fd1002, %fd999, %fd193;
	mul.f64 	%fd1003, %fd1001, %fd1001;
	fma.rn.f64 	%fd1004, %fd1000, %fd1000, %fd1003;
	fma.rn.f64 	%fd1005, %fd1002, %fd1002, %fd1004;
	sqrt.rn.f64 	%fd1006, %fd1005;
	setp.ge.f64 	%p99, %fd1006, %fd197;
	mul.f64 	%fd1131, %fd1006, %fd193;
	@%p99 bra 	$L__BB0_113;

	mul.f64 	%fd1007, %fd1131, %fd1131;
	sub.f64 	%fd1009, %fd968, %fd1131;
	div.rn.f64 	%fd1010, %fd1009, 0d4008000000000000;
	add.f64 	%fd1011, %fd4, %fd1010;
	mul.f64 	%fd1012, %fd1007, %fd1011;
	div.rn.f64 	%fd1013, %fd1012, %fd5;
	add.f64 	%fd1131, %fd6, %fd1013;

$L__BB0_113:
	mul.lo.s64 	%rd246, %rd73, %rd49;
	add.s64 	%rd245, %rd77, %rd246;
	mul.f64 	%fd1016, %fd176, %fd196;
	mul.f64 	%fd1015, %fd1016, %fd1131;
	// begin inline asm
	{ atom.add.f64 %fd1014,[%rd245],%fd1015; }

	// end inline asm
	bra.uni 	$L__BB0_124;

$L__BB0_114:
	ld.global.f64 	%fd180, [%rd74];
	@%p90 bra 	$L__BB0_123;

	div.rn.f64 	%fd1132, %fd156, %fd163;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r546}, %fd1132;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r547, %temp}, %fd1132;
	}
	setp.gt.s32 	%p101, %r546, 1048575;
	mov.u32 	%r548, -1023;
	@%p101 bra 	$L__BB0_117;

	mul.f64 	%fd1132, %fd1132, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r546}, %fd1132;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r547, %temp}, %fd1132;
	}
	mov.u32 	%r548, -1077;

$L__BB0_117:
	add.s32 	%r515, %r546, -1;
	setp.lt.u32 	%p102, %r515, 2146435071;
	@%p102 bra 	$L__BB0_119;
	bra.uni 	$L__BB0_118;

$L__BB0_119:
	shr.u32 	%r517, %r546, 20;
	add.s32 	%r549, %r548, %r517;
	and.b32  	%r518, %r546, -2146435073;
	or.b32  	%r519, %r518, 1072693248;
	mov.b64 	%fd1133, {%r547, %r519};
	setp.lt.s32 	%p104, %r519, 1073127583;
	@%p104 bra 	$L__BB0_121;

	{
	.reg .b32 %temp;
	mov.b64 	{%r520, %temp}, %fd1133;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r521}, %fd1133;
	}
	add.s32 	%r522, %r521, -1048576;
	mov.b64 	%fd1133, {%r520, %r522};
	add.s32 	%r549, %r549, 1;

$L__BB0_121:
	add.f64 	%fd1020, %fd1133, 0d3FF0000000000000;
	mov.f64 	%fd1021, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1022, %fd1020;
	neg.f64 	%fd1023, %fd1020;
	fma.rn.f64 	%fd1024, %fd1023, %fd1022, %fd1021;
	fma.rn.f64 	%fd1025, %fd1024, %fd1024, %fd1024;
	fma.rn.f64 	%fd1026, %fd1025, %fd1022, %fd1022;
	add.f64 	%fd1027, %fd1133, 0dBFF0000000000000;
	mul.f64 	%fd1028, %fd1027, %fd1026;
	fma.rn.f64 	%fd1029, %fd1027, %fd1026, %fd1028;
	mul.f64 	%fd1030, %fd1029, %fd1029;
	mov.f64 	%fd1031, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1032, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1033, %fd1032, %fd1030, %fd1031;
	mov.f64 	%fd1034, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1035, %fd1033, %fd1030, %fd1034;
	mov.f64 	%fd1036, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1037, %fd1035, %fd1030, %fd1036;
	mov.f64 	%fd1038, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1039, %fd1037, %fd1030, %fd1038;
	mov.f64 	%fd1040, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1041, %fd1039, %fd1030, %fd1040;
	mov.f64 	%fd1042, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1043, %fd1041, %fd1030, %fd1042;
	mov.f64 	%fd1044, 0d3FB5555555555554;
	fma.rn.f64 	%fd1045, %fd1043, %fd1030, %fd1044;
	sub.f64 	%fd1046, %fd1027, %fd1029;
	add.f64 	%fd1047, %fd1046, %fd1046;
	neg.f64 	%fd1048, %fd1029;
	fma.rn.f64 	%fd1049, %fd1048, %fd1027, %fd1047;
	mul.f64 	%fd1050, %fd1026, %fd1049;
	mul.f64 	%fd1051, %fd1030, %fd1045;
	fma.rn.f64 	%fd1052, %fd1051, %fd1029, %fd1050;
	xor.b32  	%r523, %r549, -2147483648;
	mov.u32 	%r524, -2147483648;
	mov.u32 	%r525, 1127219200;
	mov.b64 	%fd1053, {%r523, %r525};
	mov.b64 	%fd1054, {%r524, %r525};
	sub.f64 	%fd1055, %fd1053, %fd1054;
	mov.f64 	%fd1056, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1057, %fd1055, %fd1056, %fd1029;
	neg.f64 	%fd1058, %fd1055;
	fma.rn.f64 	%fd1059, %fd1058, %fd1056, %fd1057;
	sub.f64 	%fd1060, %fd1059, %fd1029;
	sub.f64 	%fd1061, %fd1052, %fd1060;
	mov.f64 	%fd1062, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1063, %fd1055, %fd1062, %fd1061;
	add.f64 	%fd1134, %fd1057, %fd1063;
	bra.uni 	$L__BB0_122;

$L__BB0_118:
	mov.f64 	%fd1018, 0d7FF0000000000000;
	fma.rn.f64 	%fd1019, %fd1132, %fd1018, %fd1018;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r516}, %fd1132;
	}
	mov.b32 	%f5, %r516;
	setp.eq.f32 	%p103, %f5, 0f00000000;
	selp.f64 	%fd1134, 0dFFF0000000000000, %fd1019, %p103;

$L__BB0_122:
	sub.f64 	%fd1064, %fd156, %fd163;
	div.rn.f64 	%fd1065, %fd1064, %fd163;
	mul.f64 	%fd1066, %fd3, %fd1065;
	mul.f64 	%fd1067, %fd1065, %fd1066;
	mul.f64 	%fd1135, %fd1067, %fd1134;

$L__BB0_123:
	selp.f64 	%fd1070, %fd1135, 0d0000000000000000, %p95;
	mul.f64 	%fd1071, %fd180, %fd195;
	mul.f64 	%fd1072, %fd1071, %fd192;
	mul.f64 	%fd1073, %fd1072, %fd1070;
	mul.f64 	%fd1069, %fd1073, %fd196;
	mul.lo.s64 	%rd248, %rd73, %rd49;
	add.s64 	%rd247, %rd77, %rd248;
	// begin inline asm
	{ atom.add.f64 %fd1068,[%rd247],%fd1069; }

	// end inline asm
	bra.uni 	$L__BB0_124;

$L__BB0_33:
	setp.eq.s32 	%p34, %r528, 6;
	@%p34 bra 	$L__BB0_37;
	bra.uni 	$L__BB0_34;

$L__BB0_37:
	sub.f64 	%fd1103, %fd18, %fd20;
	sub.f64 	%fd1102, %fd8, %fd10;
	sub.f64 	%fd1101, %fd13, %fd15;
	sub.f64 	%fd327, %fd9, %fd10;
	sub.f64 	%fd328, %fd19, %fd20;
	mul.f64 	%fd329, %fd1101, %fd328;
	sub.f64 	%fd330, %fd14, %fd15;
	mul.f64 	%fd331, %fd330, %fd1103;
	sub.f64 	%fd332, %fd329, %fd331;
	mul.f64 	%fd333, %fd327, %fd1103;
	mul.f64 	%fd334, %fd1102, %fd328;
	sub.f64 	%fd335, %fd333, %fd334;
	mul.f64 	%fd336, %fd1102, %fd330;
	mul.f64 	%fd337, %fd327, %fd1101;
	sub.f64 	%fd338, %fd336, %fd337;
	mul.f64 	%fd339, %fd335, %fd335;
	fma.rn.f64 	%fd340, %fd332, %fd332, %fd339;
	fma.rn.f64 	%fd341, %fd338, %fd338, %fd340;
	div.rn.f64 	%fd1111, %fd341, %fd35;
	bra.uni 	$L__BB0_44;

$L__BB0_34:
	setp.eq.s32 	%p35, %r528, 7;
	@%p35 bra 	$L__BB0_36;
	bra.uni 	$L__BB0_35;

$L__BB0_36:
	sub.f64 	%fd309, %fd8, %fd11;
	sub.f64 	%fd310, %fd19, %fd21;
	sub.f64 	%fd311, %fd13, %fd16;
	mul.f64 	%fd312, %fd311, %fd310;
	sub.f64 	%fd313, %fd14, %fd16;
	sub.f64 	%fd314, %fd18, %fd21;
	mul.f64 	%fd315, %fd313, %fd314;
	sub.f64 	%fd316, %fd312, %fd315;
	sub.f64 	%fd317, %fd9, %fd11;
	mul.f64 	%fd318, %fd317, %fd314;
	mul.f64 	%fd319, %fd309, %fd310;
	sub.f64 	%fd320, %fd318, %fd319;
	mul.f64 	%fd321, %fd309, %fd313;
	mul.f64 	%fd322, %fd317, %fd311;
	sub.f64 	%fd323, %fd321, %fd322;
	mul.f64 	%fd324, %fd320, %fd320;
	fma.rn.f64 	%fd325, %fd316, %fd316, %fd324;
	fma.rn.f64 	%fd326, %fd323, %fd323, %fd325;
	div.rn.f64 	%fd1111, %fd326, %fd35;
	bra.uni 	$L__BB0_44;

$L__BB0_35:
	sub.f64 	%fd1094, %fd11, %fd10;
	sub.f64 	%fd1093, %fd9, %fd8;
	sub.f64 	%fd1092, %fd21, %fd20;
	sub.f64 	%fd1091, %fd14, %fd13;
	sub.f64 	%fd1090, %fd16, %fd15;
	sub.f64 	%fd1089, %fd19, %fd18;
	sub.f64 	%fd290, %fd10, %fd8;
	mul.f64 	%fd291, %fd1089, %fd1090;
	mul.f64 	%fd292, %fd1091, %fd1092;
	sub.f64 	%fd293, %fd292, %fd291;
	mul.f64 	%fd294, %fd1093, %fd1092;
	mul.f64 	%fd295, %fd1089, %fd1094;
	sub.f64 	%fd296, %fd295, %fd294;
	mul.f64 	%fd297, %fd1091, %fd1094;
	mul.f64 	%fd298, %fd1093, %fd1090;
	sub.f64 	%fd299, %fd298, %fd297;
	sub.f64 	%fd300, %fd15, %fd13;
	mul.f64 	%fd301, %fd300, %fd296;
	fma.rn.f64 	%fd302, %fd290, %fd293, %fd301;
	sub.f64 	%fd303, %fd20, %fd18;
	fma.rn.f64 	%fd304, %fd303, %fd299, %fd302;
	mul.f64 	%fd305, %fd304, %fd304;
	mul.f64 	%fd306, %fd296, %fd296;
	fma.rn.f64 	%fd307, %fd293, %fd293, %fd306;
	fma.rn.f64 	%fd308, %fd299, %fd299, %fd307;
	div.rn.f64 	%fd1111, %fd305, %fd308;

$L__BB0_44:
	sub.f64 	%fd1100, %fd11, %fd10;
	sub.f64 	%fd1099, %fd9, %fd8;
	sub.f64 	%fd1098, %fd21, %fd20;
	sub.f64 	%fd1097, %fd14, %fd13;
	sub.f64 	%fd1096, %fd16, %fd15;
	sub.f64 	%fd1095, %fd19, %fd18;
	mul.f64 	%fd396, %fd7, %fd7;
	sub.f64 	%fd65, %fd1111, %fd396;
	mul.lo.s64 	%rd162, %rd66, %rd42;
	add.s64 	%rd163, %rd22, %rd162;
	mul.lo.s64 	%rd164, %rd67, %rd42;
	add.s64 	%rd165, %rd22, %rd164;
	mul.lo.s64 	%rd166, %rd68, %rd42;
	add.s64 	%rd167, %rd22, %rd166;
	mul.lo.s64 	%rd168, %rd69, %rd42;
	add.s64 	%rd169, %rd22, %rd168;
	ld.global.f64 	%fd397, [%rd165];
	ld.global.f64 	%fd398, [%rd163];
	sub.f64 	%fd399, %fd397, %fd398;
	ld.global.f64 	%fd400, [%rd165+8];
	ld.global.f64 	%fd401, [%rd163+8];
	sub.f64 	%fd402, %fd400, %fd401;
	ld.global.f64 	%fd403, [%rd165+16];
	ld.global.f64 	%fd404, [%rd163+16];
	sub.f64 	%fd405, %fd403, %fd404;
	ld.global.f64 	%fd406, [%rd169];
	ld.global.f64 	%fd407, [%rd167];
	sub.f64 	%fd408, %fd406, %fd407;
	ld.global.f64 	%fd409, [%rd169+8];
	ld.global.f64 	%fd410, [%rd167+8];
	sub.f64 	%fd411, %fd409, %fd410;
	ld.global.f64 	%fd412, [%rd169+16];
	ld.global.f64 	%fd413, [%rd167+16];
	sub.f64 	%fd414, %fd412, %fd413;
	mul.f64 	%fd415, %fd402, %fd402;
	fma.rn.f64 	%fd416, %fd399, %fd399, %fd415;
	fma.rn.f64 	%fd417, %fd405, %fd405, %fd416;
	mul.f64 	%fd418, %fd417, 0d3F50624DE0000000;
	mul.f64 	%fd419, %fd411, %fd411;
	fma.rn.f64 	%fd420, %fd408, %fd408, %fd419;
	fma.rn.f64 	%fd421, %fd414, %fd414, %fd420;
	mul.f64 	%fd66, %fd418, %fd421;
	mul.f64 	%fd422, %fd1095, %fd1096;
	mul.f64 	%fd423, %fd1097, %fd1098;
	sub.f64 	%fd424, %fd423, %fd422;
	mul.f64 	%fd425, %fd1099, %fd1098;
	mul.f64 	%fd426, %fd1095, %fd1100;
	sub.f64 	%fd427, %fd426, %fd425;
	mul.f64 	%fd428, %fd1097, %fd1100;
	mul.f64 	%fd429, %fd1099, %fd1096;
	sub.f64 	%fd430, %fd429, %fd428;
	mul.f64 	%fd431, %fd427, %fd427;
	fma.rn.f64 	%fd432, %fd424, %fd424, %fd431;
	fma.rn.f64 	%fd67, %fd430, %fd430, %fd432;
	setp.geu.f64 	%p36, %fd67, %fd66;
	mov.f64 	%fd1112, 0d3FF0000000000000;
	@%p36 bra 	$L__BB0_46;

	div.rn.f64 	%fd433, %fd67, %fd66;
	mov.f64 	%fd434, 0d0000000000000000;
	sub.f64 	%fd435, %fd434, %fd433;
	add.f64 	%fd436, %fd435, 0d4000000000000000;
	mul.f64 	%fd1112, %fd433, %fd436;

$L__BB0_46:
	fma.rn.f64 	%fd70, %fd2, %fd7, %fd1;
	mul.lo.s64 	%rd170, %rd64, %rd43;
	add.s64 	%rd70, %rd17, %rd170;
	mul.lo.s64 	%rd171, %rd65, %rd43;
	add.s64 	%rd71, %rd17, %rd171;
	ld.global.f64 	%fd71, [%rd71];
	ld.global.f64 	%fd72, [%rd70];
	setp.geu.f64 	%p37, %fd65, %fd70;
	@%p37 bra 	$L__BB0_55;

	div.rn.f64 	%fd1113, %fd65, %fd70;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r529}, %fd1113;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r530, %temp}, %fd1113;
	}
	setp.gt.s32 	%p38, %r529, 1048575;
	mov.u32 	%r531, -1023;
	@%p38 bra 	$L__BB0_49;

	mul.f64 	%fd1113, %fd1113, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r529}, %fd1113;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r530, %temp}, %fd1113;
	}
	mov.u32 	%r531, -1077;

$L__BB0_49:
	add.s32 	%r452, %r529, -1;
	setp.lt.u32 	%p39, %r452, 2146435071;
	@%p39 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_50;

$L__BB0_51:
	shr.u32 	%r454, %r529, 20;
	add.s32 	%r532, %r531, %r454;
	and.b32  	%r455, %r529, -2146435073;
	or.b32  	%r456, %r455, 1072693248;
	mov.b64 	%fd1114, {%r530, %r456};
	setp.lt.s32 	%p41, %r456, 1073127583;
	@%p41 bra 	$L__BB0_53;

	{
	.reg .b32 %temp;
	mov.b64 	{%r457, %temp}, %fd1114;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r458}, %fd1114;
	}
	add.s32 	%r459, %r458, -1048576;
	mov.b64 	%fd1114, {%r457, %r459};
	add.s32 	%r532, %r532, 1;

$L__BB0_53:
	add.f64 	%fd440, %fd1114, 0d3FF0000000000000;
	mov.f64 	%fd441, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd442, %fd440;
	neg.f64 	%fd443, %fd440;
	fma.rn.f64 	%fd444, %fd443, %fd442, %fd441;
	fma.rn.f64 	%fd445, %fd444, %fd444, %fd444;
	fma.rn.f64 	%fd446, %fd445, %fd442, %fd442;
	add.f64 	%fd447, %fd1114, 0dBFF0000000000000;
	mul.f64 	%fd448, %fd447, %fd446;
	fma.rn.f64 	%fd449, %fd447, %fd446, %fd448;
	mul.f64 	%fd450, %fd449, %fd449;
	mov.f64 	%fd451, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd452, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd453, %fd452, %fd450, %fd451;
	mov.f64 	%fd454, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd455, %fd453, %fd450, %fd454;
	mov.f64 	%fd456, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd457, %fd455, %fd450, %fd456;
	mov.f64 	%fd458, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd459, %fd457, %fd450, %fd458;
	mov.f64 	%fd460, 0d3F624924923BE72D;
	fma.rn.f64 	%fd461, %fd459, %fd450, %fd460;
	mov.f64 	%fd462, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd463, %fd461, %fd450, %fd462;
	mov.f64 	%fd464, 0d3FB5555555555554;
	fma.rn.f64 	%fd465, %fd463, %fd450, %fd464;
	sub.f64 	%fd466, %fd447, %fd449;
	add.f64 	%fd467, %fd466, %fd466;
	neg.f64 	%fd468, %fd449;
	fma.rn.f64 	%fd469, %fd468, %fd447, %fd467;
	mul.f64 	%fd470, %fd446, %fd469;
	mul.f64 	%fd471, %fd450, %fd465;
	fma.rn.f64 	%fd472, %fd471, %fd449, %fd470;
	xor.b32  	%r460, %r532, -2147483648;
	mov.u32 	%r461, -2147483648;
	mov.u32 	%r462, 1127219200;
	mov.b64 	%fd473, {%r460, %r462};
	mov.b64 	%fd474, {%r461, %r462};
	sub.f64 	%fd475, %fd473, %fd474;
	mov.f64 	%fd476, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd477, %fd475, %fd476, %fd449;
	neg.f64 	%fd478, %fd475;
	fma.rn.f64 	%fd479, %fd478, %fd476, %fd477;
	sub.f64 	%fd480, %fd479, %fd449;
	sub.f64 	%fd481, %fd472, %fd480;
	mov.f64 	%fd482, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd483, %fd475, %fd482, %fd481;
	add.f64 	%fd1115, %fd477, %fd483;
	bra.uni 	$L__BB0_54;

$L__BB0_50:
	mov.f64 	%fd438, 0d7FF0000000000000;
	fma.rn.f64 	%fd439, %fd1113, %fd438, %fd438;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r453}, %fd1113;
	}
	mov.b32 	%f1, %r453;
	setp.eq.f32 	%p40, %f1, 0f00000000;
	selp.f64 	%fd1115, 0dFFF0000000000000, %fd439, %p40;

$L__BB0_54:
	sub.f64 	%fd484, %fd65, %fd70;
	div.rn.f64 	%fd485, %fd484, %fd70;
	mul.f64 	%fd486, %fd3, %fd485;
	mul.f64 	%fd487, %fd485, %fd486;
	mul.f64 	%fd1116, %fd487, %fd1115;

$L__BB0_55:
	setp.lt.f64 	%p42, %fd65, %fd70;
	selp.f64 	%fd488, %fd1116, 0d0000000000000000, %p42;
	add.f64 	%fd489, %fd72, %fd71;
	mul.f64 	%fd490, %fd489, %fd195;
	mul.f64 	%fd491, %fd490, %fd192;
	mul.f64 	%fd492, %fd1112, %fd491;
	mul.f64 	%fd493, %fd492, %fd488;
	mul.f64 	%fd494, %fd493, %fd196;
	setp.num.f64 	%p43, %fd494, %fd494;
	@%p43 bra 	$L__BB0_66;

	@%p37 bra 	$L__BB0_65;

	div.rn.f64 	%fd1117, %fd65, %fd70;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r533}, %fd1117;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r534, %temp}, %fd1117;
	}
	setp.gt.s32 	%p45, %r533, 1048575;
	mov.u32 	%r535, -1023;
	@%p45 bra 	$L__BB0_59;

	mul.f64 	%fd1117, %fd1117, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r533}, %fd1117;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r534, %temp}, %fd1117;
	}
	mov.u32 	%r535, -1077;

$L__BB0_59:
	add.s32 	%r465, %r533, -1;
	setp.lt.u32 	%p46, %r465, 2146435071;
	@%p46 bra 	$L__BB0_61;
	bra.uni 	$L__BB0_60;

$L__BB0_61:
	shr.u32 	%r467, %r533, 20;
	add.s32 	%r536, %r535, %r467;
	and.b32  	%r468, %r533, -2146435073;
	or.b32  	%r469, %r468, 1072693248;
	mov.b64 	%fd1118, {%r534, %r469};
	setp.lt.s32 	%p48, %r469, 1073127583;
	@%p48 bra 	$L__BB0_63;

	{
	.reg .b32 %temp;
	mov.b64 	{%r470, %temp}, %fd1118;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r471}, %fd1118;
	}
	add.s32 	%r472, %r471, -1048576;
	mov.b64 	%fd1118, {%r470, %r472};
	add.s32 	%r536, %r536, 1;

$L__BB0_63:
	add.f64 	%fd498, %fd1118, 0d3FF0000000000000;
	mov.f64 	%fd499, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd500, %fd498;
	neg.f64 	%fd501, %fd498;
	fma.rn.f64 	%fd502, %fd501, %fd500, %fd499;
	fma.rn.f64 	%fd503, %fd502, %fd502, %fd502;
	fma.rn.f64 	%fd504, %fd503, %fd500, %fd500;
	add.f64 	%fd505, %fd1118, 0dBFF0000000000000;
	mul.f64 	%fd506, %fd505, %fd504;
	fma.rn.f64 	%fd507, %fd505, %fd504, %fd506;
	mul.f64 	%fd508, %fd507, %fd507;
	mov.f64 	%fd509, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd510, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd511, %fd510, %fd508, %fd509;
	mov.f64 	%fd512, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd513, %fd511, %fd508, %fd512;
	mov.f64 	%fd514, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd515, %fd513, %fd508, %fd514;
	mov.f64 	%fd516, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd517, %fd515, %fd508, %fd516;
	mov.f64 	%fd518, 0d3F624924923BE72D;
	fma.rn.f64 	%fd519, %fd517, %fd508, %fd518;
	mov.f64 	%fd520, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd521, %fd519, %fd508, %fd520;
	mov.f64 	%fd522, 0d3FB5555555555554;
	fma.rn.f64 	%fd523, %fd521, %fd508, %fd522;
	sub.f64 	%fd524, %fd505, %fd507;
	add.f64 	%fd525, %fd524, %fd524;
	neg.f64 	%fd526, %fd507;
	fma.rn.f64 	%fd527, %fd526, %fd505, %fd525;
	mul.f64 	%fd528, %fd504, %fd527;
	mul.f64 	%fd529, %fd508, %fd523;
	fma.rn.f64 	%fd530, %fd529, %fd507, %fd528;
	xor.b32  	%r473, %r536, -2147483648;
	mov.u32 	%r474, -2147483648;
	mov.u32 	%r475, 1127219200;
	mov.b64 	%fd531, {%r473, %r475};
	mov.b64 	%fd532, {%r474, %r475};
	sub.f64 	%fd533, %fd531, %fd532;
	mov.f64 	%fd534, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd535, %fd533, %fd534, %fd507;
	neg.f64 	%fd536, %fd533;
	fma.rn.f64 	%fd537, %fd536, %fd534, %fd535;
	sub.f64 	%fd538, %fd537, %fd507;
	sub.f64 	%fd539, %fd530, %fd538;
	mov.f64 	%fd540, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd541, %fd533, %fd540, %fd539;
	add.f64 	%fd1119, %fd535, %fd541;
	bra.uni 	$L__BB0_64;

$L__BB0_60:
	mov.f64 	%fd496, 0d7FF0000000000000;
	fma.rn.f64 	%fd497, %fd1117, %fd496, %fd496;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r466}, %fd1117;
	}
	mov.b32 	%f2, %r466;
	setp.eq.f32 	%p47, %f2, 0f00000000;
	selp.f64 	%fd1119, 0dFFF0000000000000, %fd497, %p47;

$L__BB0_64:
	sub.f64 	%fd542, %fd65, %fd70;
	div.rn.f64 	%fd543, %fd542, %fd70;
	mul.f64 	%fd544, %fd3, %fd543;
	mul.f64 	%fd545, %fd543, %fd544;
	mul.f64 	%fd1120, %fd545, %fd1119;

$L__BB0_65:
	add.u64 	%rd255, %SP, 16;
	add.u64 	%rd254, %SP, 16;
	add.u64 	%rd253, %SPL, 16;
	st.local.v2.f64 	[%rd253], {%fd72, %fd71};
	selp.f64 	%fd546, %fd1120, 0d0000000000000000, %p42;
	st.local.v2.f64 	[%rd253+16], {%fd546, %fd65};
	st.local.v2.f64 	[%rd253+32], {%fd70, %fd7};
	mov.u64 	%rd172, $str$4;
	cvta.global.u64 	%rd173, %rd172;
	{ // callseq 252, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd173;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd254;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r476, [retval0+0];
	} // callseq 252

$L__BB0_66:
	setp.eq.s32 	%p50, %r255, 0;
	@%p50 bra 	$L__BB0_70;

	mul.lo.s64 	%rd175, %rd62, %rd44;
	add.s64 	%rd176, %rd8, %rd175;
	ld.global.s32 	%rd177, [%rd176];
	mul.lo.s64 	%rd178, %rd177, %rd45;
	add.s64 	%rd179, %rd4, %rd178;
	mul.lo.s64 	%rd180, %rd62, %rd46;
	add.s64 	%rd181, %rd7, %rd180;
	ld.global.s32 	%rd182, [%rd181];
	mul.lo.s64 	%rd183, %rd182, %rd45;
	add.s64 	%rd184, %rd4, %rd183;
	ld.global.f64 	%fd547, [%rd179];
	add.f64 	%fd548, %fd547, %fd547;
	ld.global.f64 	%fd549, [%rd184];
	mul.f64 	%fd550, %fd548, %fd549;
	add.f64 	%fd551, %fd547, %fd549;
	setp.neu.f64 	%p51, %fd551, 0d0000000000000000;
	mov.f64 	%fd552, 0d0000000000000000;
	div.rn.f64 	%fd553, %fd550, %fd551;
	selp.f64 	%fd554, %fd553, 0d0000000000000000, %p51;
	mul.lo.s64 	%rd185, %rd62, %rd47;
	add.s64 	%rd186, %rd13, %rd185;
	ld.global.f64 	%fd555, [%rd186];
	mul.f64 	%fd95, %fd555, %fd554;
	mul.lo.s64 	%rd187, %rd62, %rd48;
	add.s64 	%rd188, %rd11, %rd187;
	ld.global.f64 	%fd556, [%rd188];
	mul.f64 	%fd557, %fd556, %fd556;
	ld.global.f64 	%fd558, [%rd188+8];
	mul.f64 	%fd559, %fd556, %fd558;
	ld.global.f64 	%fd560, [%rd188+16];
	mul.f64 	%fd561, %fd556, %fd560;
	mul.f64 	%fd562, %fd558, %fd558;
	mul.f64 	%fd563, %fd558, %fd560;
	mul.f64 	%fd564, %fd560, %fd560;
	mov.f64 	%fd565, 0d3FF0000000000000;
	sub.f64 	%fd566, %fd565, %fd557;
	sub.f64 	%fd567, %fd552, %fd559;
	sub.f64 	%fd568, %fd552, %fd561;
	sub.f64 	%fd569, %fd565, %fd562;
	sub.f64 	%fd570, %fd552, %fd563;
	sub.f64 	%fd571, %fd565, %fd564;
	sub.f64 	%fd572, %fd12, %fd23;
	sub.f64 	%fd573, %fd17, %fd24;
	mul.f64 	%fd574, %fd567, %fd573;
	mul.f64 	%fd575, %fd569, %fd573;
	mul.f64 	%fd576, %fd570, %fd573;
	fma.rn.f64 	%fd577, %fd566, %fd572, %fd574;
	fma.rn.f64 	%fd578, %fd567, %fd572, %fd575;
	fma.rn.f64 	%fd579, %fd568, %fd572, %fd576;
	sub.f64 	%fd580, %fd22, %fd25;
	fma.rn.f64 	%fd581, %fd568, %fd580, %fd577;
	fma.rn.f64 	%fd582, %fd570, %fd580, %fd578;
	fma.rn.f64 	%fd583, %fd571, %fd580, %fd579;
	div.rn.f64 	%fd584, %fd581, %fd193;
	div.rn.f64 	%fd585, %fd582, %fd193;
	div.rn.f64 	%fd586, %fd583, %fd193;
	mul.f64 	%fd587, %fd585, %fd585;
	fma.rn.f64 	%fd588, %fd584, %fd584, %fd587;
	fma.rn.f64 	%fd589, %fd586, %fd586, %fd588;
	sqrt.rn.f64 	%fd590, %fd589;
	setp.ge.f64 	%p52, %fd590, %fd197;
	mul.f64 	%fd1121, %fd590, %fd193;
	@%p52 bra 	$L__BB0_69;

	mul.f64 	%fd591, %fd1121, %fd1121;
	sub.f64 	%fd593, %fd552, %fd1121;
	div.rn.f64 	%fd594, %fd593, 0d4008000000000000;
	add.f64 	%fd595, %fd4, %fd594;
	mul.f64 	%fd596, %fd591, %fd595;
	div.rn.f64 	%fd597, %fd596, %fd5;
	add.f64 	%fd1121, %fd6, %fd597;

$L__BB0_69:
	mul.lo.s64 	%rd190, %rd66, %rd49;
	add.s64 	%rd189, %rd77, %rd190;
	mul.f64 	%fd600, %fd95, %fd196;
	mul.f64 	%fd599, %fd600, %fd1121;
	// begin inline asm
	{ atom.add.f64 %fd598,[%rd189],%fd599; }

	// end inline asm
	bra.uni 	$L__BB0_124;

$L__BB0_70:
	ld.global.f64 	%fd602, [%rd71];
	ld.global.f64 	%fd603, [%rd70];
	add.f64 	%fd604, %fd603, %fd602;
	mul.f64 	%fd605, %fd604, %fd195;
	mul.f64 	%fd606, %fd605, %fd192;
	mul.f64 	%fd99, %fd1112, %fd606;
	@%p37 bra 	$L__BB0_79;

	div.rn.f64 	%fd1122, %fd65, %fd70;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r537}, %fd1122;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r538, %temp}, %fd1122;
	}
	setp.gt.s32 	%p54, %r537, 1048575;
	mov.u32 	%r539, -1023;
	@%p54 bra 	$L__BB0_73;

	mul.f64 	%fd1122, %fd1122, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r537}, %fd1122;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r538, %temp}, %fd1122;
	}
	mov.u32 	%r539, -1077;

$L__BB0_73:
	add.s32 	%r479, %r537, -1;
	setp.lt.u32 	%p55, %r479, 2146435071;
	@%p55 bra 	$L__BB0_75;
	bra.uni 	$L__BB0_74;

$L__BB0_75:
	shr.u32 	%r481, %r537, 20;
	add.s32 	%r540, %r539, %r481;
	and.b32  	%r482, %r537, -2146435073;
	or.b32  	%r483, %r482, 1072693248;
	mov.b64 	%fd1123, {%r538, %r483};
	setp.lt.s32 	%p57, %r483, 1073127583;
	@%p57 bra 	$L__BB0_77;

	{
	.reg .b32 %temp;
	mov.b64 	{%r484, %temp}, %fd1123;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r485}, %fd1123;
	}
	add.s32 	%r486, %r485, -1048576;
	mov.b64 	%fd1123, {%r484, %r486};
	add.s32 	%r540, %r540, 1;

$L__BB0_77:
	add.f64 	%fd609, %fd1123, 0d3FF0000000000000;
	mov.f64 	%fd610, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd611, %fd609;
	neg.f64 	%fd612, %fd609;
	fma.rn.f64 	%fd613, %fd612, %fd611, %fd610;
	fma.rn.f64 	%fd614, %fd613, %fd613, %fd613;
	fma.rn.f64 	%fd615, %fd614, %fd611, %fd611;
	add.f64 	%fd616, %fd1123, 0dBFF0000000000000;
	mul.f64 	%fd617, %fd616, %fd615;
	fma.rn.f64 	%fd618, %fd616, %fd615, %fd617;
	mul.f64 	%fd619, %fd618, %fd618;
	mov.f64 	%fd620, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd621, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd622, %fd621, %fd619, %fd620;
	mov.f64 	%fd623, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd624, %fd622, %fd619, %fd623;
	mov.f64 	%fd625, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd626, %fd624, %fd619, %fd625;
	mov.f64 	%fd627, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd628, %fd626, %fd619, %fd627;
	mov.f64 	%fd629, 0d3F624924923BE72D;
	fma.rn.f64 	%fd630, %fd628, %fd619, %fd629;
	mov.f64 	%fd631, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd632, %fd630, %fd619, %fd631;
	mov.f64 	%fd633, 0d3FB5555555555554;
	fma.rn.f64 	%fd634, %fd632, %fd619, %fd633;
	sub.f64 	%fd635, %fd616, %fd618;
	add.f64 	%fd636, %fd635, %fd635;
	neg.f64 	%fd637, %fd618;
	fma.rn.f64 	%fd638, %fd637, %fd616, %fd636;
	mul.f64 	%fd639, %fd615, %fd638;
	mul.f64 	%fd640, %fd619, %fd634;
	fma.rn.f64 	%fd641, %fd640, %fd618, %fd639;
	xor.b32  	%r487, %r540, -2147483648;
	mov.u32 	%r488, -2147483648;
	mov.u32 	%r489, 1127219200;
	mov.b64 	%fd642, {%r487, %r489};
	mov.b64 	%fd643, {%r488, %r489};
	sub.f64 	%fd644, %fd642, %fd643;
	mov.f64 	%fd645, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd646, %fd644, %fd645, %fd618;
	neg.f64 	%fd647, %fd644;
	fma.rn.f64 	%fd648, %fd647, %fd645, %fd646;
	sub.f64 	%fd649, %fd648, %fd618;
	sub.f64 	%fd650, %fd641, %fd649;
	mov.f64 	%fd651, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd652, %fd644, %fd651, %fd650;
	add.f64 	%fd1124, %fd646, %fd652;
	bra.uni 	$L__BB0_78;

$L__BB0_74:
	mov.f64 	%fd607, 0d7FF0000000000000;
	fma.rn.f64 	%fd608, %fd1122, %fd607, %fd607;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r480}, %fd1122;
	}
	mov.b32 	%f3, %r480;
	setp.eq.f32 	%p56, %f3, 0f00000000;
	selp.f64 	%fd1124, 0dFFF0000000000000, %fd608, %p56;

$L__BB0_78:
	sub.f64 	%fd653, %fd65, %fd70;
	div.rn.f64 	%fd654, %fd653, %fd70;
	mul.f64 	%fd655, %fd3, %fd654;
	mul.f64 	%fd656, %fd654, %fd655;
	mul.f64 	%fd1125, %fd656, %fd1124;

$L__BB0_79:
	selp.f64 	%fd659, %fd1125, 0d0000000000000000, %p42;
	mul.f64 	%fd660, %fd99, %fd659;
	mul.f64 	%fd658, %fd660, %fd196;
	mul.lo.s64 	%rd192, %rd66, %rd49;
	add.s64 	%rd191, %rd77, %rd192;
	// begin inline asm
	{ atom.add.f64 %fd657,[%rd191],%fd658; }

	// end inline asm

$L__BB0_124:
	ld.param.u64 	%rd252, [val_IPC_collisions_cuda_kernel_forward_param_0+24];
	add.s64 	%rd256, %rd256, %rd38;
	setp.lt.u64 	%p106, %rd256, %rd252;
	@%p106 bra 	$L__BB0_2;

$L__BB0_125:
	ret;

}
	// .globl	val_IPC_collisions_cuda_kernel_backward
.visible .entry val_IPC_collisions_cuda_kernel_backward(
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_21[56],
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_22,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_23,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_24,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_25,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_26,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_27,
	.param .u32 val_IPC_collisions_cuda_kernel_backward_param_28,
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_29[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_30[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_31[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_32[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_33[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_34[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_35[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_36[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_37[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_38[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_39[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_40[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_41[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_42[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_43[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_44[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_45[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_46[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_47[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_48[56],
	.param .align 8 .b8 val_IPC_collisions_cuda_kernel_backward_param_49[56],
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_50,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_51,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_52,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_53,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_54,
	.param .f64 val_IPC_collisions_cuda_kernel_backward_param_55,
	.param .u32 val_IPC_collisions_cuda_kernel_backward_param_56
)
{
	.local .align 16 .b8 	__local_depot1[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<310>;
	.reg .b16 	%rs<277>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<981>;
	.reg .f64 	%fd<4581>;
	.reg .b64 	%rd<634>;


	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r460, %r461}, [val_IPC_collisions_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r462, %r463}, [val_IPC_collisions_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r468, %r469}, [val_IPC_collisions_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r476, %r477}, [val_IPC_collisions_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r484, %r485}, [val_IPC_collisions_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r492, %r493}, [val_IPC_collisions_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r500, %r501}, [val_IPC_collisions_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r508, %r509}, [val_IPC_collisions_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r516, %r517}, [val_IPC_collisions_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r524, %r525}, [val_IPC_collisions_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r532, %r533}, [val_IPC_collisions_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r540, %r541}, [val_IPC_collisions_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r548, %r549}, [val_IPC_collisions_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r556, %r557}, [val_IPC_collisions_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r564, %r565}, [val_IPC_collisions_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r572, %r573}, [val_IPC_collisions_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r580, %r581}, [val_IPC_collisions_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r588, %r589}, [val_IPC_collisions_cuda_kernel_backward_param_16+32];
	ld.param.v2.u32 	{%r596, %r597}, [val_IPC_collisions_cuda_kernel_backward_param_17+32];
	ld.param.v2.u32 	{%r604, %r605}, [val_IPC_collisions_cuda_kernel_backward_param_18+32];
	ld.param.v2.u32 	{%r612, %r613}, [val_IPC_collisions_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r620, %r621}, [val_IPC_collisions_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r628, %r629}, [val_IPC_collisions_cuda_kernel_backward_param_21+32];
	ld.param.f64 	%fd962, [val_IPC_collisions_cuda_kernel_backward_param_22];
	ld.param.f64 	%fd963, [val_IPC_collisions_cuda_kernel_backward_param_23];
	ld.param.f64 	%fd964, [val_IPC_collisions_cuda_kernel_backward_param_24];
	ld.param.f64 	%fd965, [val_IPC_collisions_cuda_kernel_backward_param_25];
	ld.param.f64 	%fd966, [val_IPC_collisions_cuda_kernel_backward_param_26];
	ld.param.f64 	%fd967, [val_IPC_collisions_cuda_kernel_backward_param_27];
	ld.param.u32 	%r342, [val_IPC_collisions_cuda_kernel_backward_param_28];
	ld.param.v2.u32 	{%r636, %r637}, [val_IPC_collisions_cuda_kernel_backward_param_29+32];
	ld.param.v2.u32 	{%r644, %r645}, [val_IPC_collisions_cuda_kernel_backward_param_30+32];
	ld.param.v2.u32 	{%r652, %r653}, [val_IPC_collisions_cuda_kernel_backward_param_31+32];
	ld.param.v2.u32 	{%r660, %r661}, [val_IPC_collisions_cuda_kernel_backward_param_32+32];
	ld.param.v2.u32 	{%r668, %r669}, [val_IPC_collisions_cuda_kernel_backward_param_38+32];
	ld.param.v2.u32 	{%r676, %r677}, [val_IPC_collisions_cuda_kernel_backward_param_40+32];
	ld.param.v2.u32 	{%r684, %r685}, [val_IPC_collisions_cuda_kernel_backward_param_41+32];
	ld.param.v2.u32 	{%r692, %r693}, [val_IPC_collisions_cuda_kernel_backward_param_42+32];
	ld.param.v2.u32 	{%r700, %r701}, [val_IPC_collisions_cuda_kernel_backward_param_43+32];
	ld.param.v2.u32 	{%r708, %r709}, [val_IPC_collisions_cuda_kernel_backward_param_44+32];
	ld.param.v2.u32 	{%r716, %r717}, [val_IPC_collisions_cuda_kernel_backward_param_45+32];
	ld.param.v2.u32 	{%r724, %r725}, [val_IPC_collisions_cuda_kernel_backward_param_46+32];
	ld.param.v2.u32 	{%r732, %r733}, [val_IPC_collisions_cuda_kernel_backward_param_47+32];
	ld.param.u64 	%rd195, [val_IPC_collisions_cuda_kernel_backward_param_47];
	ld.param.u64 	%rd193, [val_IPC_collisions_cuda_kernel_backward_param_46];
	ld.param.u64 	%rd191, [val_IPC_collisions_cuda_kernel_backward_param_45];
	ld.param.u64 	%rd189, [val_IPC_collisions_cuda_kernel_backward_param_44];
	ld.param.u64 	%rd187, [val_IPC_collisions_cuda_kernel_backward_param_43];
	ld.param.u64 	%rd185, [val_IPC_collisions_cuda_kernel_backward_param_42];
	ld.param.u64 	%rd183, [val_IPC_collisions_cuda_kernel_backward_param_41];
	ld.param.u64 	%rd181, [val_IPC_collisions_cuda_kernel_backward_param_40];
	ld.param.u64 	%rd179, [val_IPC_collisions_cuda_kernel_backward_param_38];
	ld.param.u64 	%rd177, [val_IPC_collisions_cuda_kernel_backward_param_32];
	ld.param.u64 	%rd175, [val_IPC_collisions_cuda_kernel_backward_param_31];
	ld.param.u64 	%rd173, [val_IPC_collisions_cuda_kernel_backward_param_30];
	ld.param.u64 	%rd171, [val_IPC_collisions_cuda_kernel_backward_param_29];
	ld.param.u64 	%rd169, [val_IPC_collisions_cuda_kernel_backward_param_21];
	ld.param.u64 	%rd167, [val_IPC_collisions_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd166, [val_IPC_collisions_cuda_kernel_backward_param_19+8];
	ld.param.u64 	%rd165, [val_IPC_collisions_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd164, [val_IPC_collisions_cuda_kernel_backward_param_18+8];
	ld.param.u64 	%rd163, [val_IPC_collisions_cuda_kernel_backward_param_18];
	ld.param.u64 	%rd162, [val_IPC_collisions_cuda_kernel_backward_param_17+8];
	ld.param.u64 	%rd161, [val_IPC_collisions_cuda_kernel_backward_param_17];
	ld.param.u64 	%rd160, [val_IPC_collisions_cuda_kernel_backward_param_16+8];
	ld.param.u64 	%rd159, [val_IPC_collisions_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd158, [val_IPC_collisions_cuda_kernel_backward_param_15+8];
	ld.param.u64 	%rd157, [val_IPC_collisions_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd156, [val_IPC_collisions_cuda_kernel_backward_param_14+8];
	ld.param.u64 	%rd155, [val_IPC_collisions_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd154, [val_IPC_collisions_cuda_kernel_backward_param_13+8];
	ld.param.u64 	%rd153, [val_IPC_collisions_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd152, [val_IPC_collisions_cuda_kernel_backward_param_12+8];
	ld.param.u64 	%rd151, [val_IPC_collisions_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd149, [val_IPC_collisions_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd148, [val_IPC_collisions_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd147, [val_IPC_collisions_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd145, [val_IPC_collisions_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd143, [val_IPC_collisions_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd141, [val_IPC_collisions_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd139, [val_IPC_collisions_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd137, [val_IPC_collisions_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd136, [val_IPC_collisions_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd135, [val_IPC_collisions_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd134, [val_IPC_collisions_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd133, [val_IPC_collisions_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd132, [val_IPC_collisions_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd131, [val_IPC_collisions_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd130, [val_IPC_collisions_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd128, [val_IPC_collisions_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r152, [val_IPC_collisions_cuda_kernel_backward_param_0+16];
	mov.u32 	%r736, %ntid.x;
	mov.u32 	%r737, %ctaid.x;
	mul.wide.u32 	%rd197, %r736, %r737;
	mov.u32 	%r738, %tid.x;
	cvt.u64.u32 	%rd198, %r738;
	add.s64 	%rd630, %rd197, %rd198;
	setp.ge.u64 	%p6, %rd630, %rd128;
	@%p6 bra 	$L__BB1_415;

	cvta.to.global.u64 	%rd29, %rd155;
	cvta.to.global.u64 	%rd30, %rd151;
	cvta.to.global.u64 	%rd31, %rd147;
	cvta.to.global.u64 	%rd32, %rd143;
	cvta.to.global.u64 	%rd33, %rd141;
	cvta.to.global.u64 	%rd34, %rd139;
	cvta.to.global.u64 	%rd35, %rd137;
	cvta.to.global.u64 	%rd36, %rd135;
	cvta.to.global.u64 	%rd37, %rd133;
	cvta.to.global.u64 	%rd38, %rd131;
	cvta.to.global.u64 	%rd39, %rd169;
	cvta.to.global.u64 	%rd40, %rd167;
	cvta.to.global.u64 	%rd41, %rd165;
	cvta.to.global.u64 	%rd42, %rd163;
	cvta.to.global.u64 	%rd43, %rd161;
	cvta.to.global.u64 	%rd44, %rd159;
	cvta.to.global.u64 	%rd45, %rd157;
	cvta.to.global.u64 	%rd46, %rd153;
	cvta.to.global.u64 	%rd47, %rd149;
	cvta.to.global.u64 	%rd48, %rd145;
	cvt.s64.s32 	%rd49, %r463;
	cvt.s64.s32 	%rd50, %r462;
	cvt.s64.s32 	%rd51, %r461;
	cvt.s64.s32 	%rd52, %r532;
	cvt.s64.s32 	%rd53, %r508;
	cvt.s64.s32 	%rd54, %r500;
	cvt.s64.s32 	%rd55, %r628;
	cvt.s64.s32 	%rd56, %r548;
	cvt.s64.s32 	%rd57, %r540;
	cvt.s64.s32 	%rd58, %r556;
	cvt.s64.s32 	%rd59, %r484;
	cvt.s64.s32 	%rd60, %r580;
	cvt.s64.s32 	%rd61, %r596;
	cvt.s64.s32 	%rd62, %r620;
	cvt.s64.s32 	%rd63, %r588;
	mul.f64 	%fd1, %fd962, %fd962;
	add.f64 	%fd2, %fd962, %fd962;
	cvt.s64.s32 	%rd64, %r604;
	cvt.s64.s32 	%rd65, %r636;
	mov.f64 	%fd969, 0d0000000000000000;
	fma.rn.f64 	%fd3, %fd966, 0d0000000000000000, 0d0000000000000000;
	sub.f64 	%fd4, %fd969, %fd964;
	cvt.s64.s32 	%rd66, %r468;
	cvt.s64.s32 	%rd68, %r612;
	cvt.s64.s32 	%rd69, %r516;
	cvt.s64.s32 	%rd70, %r572;
	cvt.s64.s32 	%rd71, %r524;
	cvt.s64.s32 	%rd72, %r476;
	cvt.s64.s32 	%rd73, %r492;
	cvt.s64.s32 	%rd74, %r724;
	mul.f64 	%fd5, %fd963, %fd967;
	mul.f64 	%fd6, %fd5, %fd5;
	div.rn.f64 	%fd7, %fd5, 0d4008000000000000;
	cvt.s64.s32 	%rd75, %r732;
	cvt.s64.s32 	%rd76, %r660;
	cvt.s64.s32 	%rd77, %r684;
	cvt.s64.s32 	%rd78, %r644;
	cvt.s64.s32 	%rd79, %r692;
	cvt.s64.s32 	%rd80, %r716;
	cvt.s64.s32 	%rd81, %r700;
	cvt.s64.s32 	%rd82, %r652;
	cvt.s64.s32 	%rd83, %r676;
	cvt.s64.s32 	%rd84, %r668;
	cvt.s64.s32 	%rd85, %r708;

$L__BB1_2:
	setp.lt.s32 	%p7, %r152, 4;
	mov.u64 	%rd631, %rd630;
	@%p7 bra 	$L__BB1_6;

	or.b64  	%rd199, %rd630, %rd49;
	and.b64  	%rd200, %rd199, -4294967296;
	setp.eq.s64 	%p8, %rd200, 0;
	@%p8 bra 	$L__BB1_5;

	div.u64 	%rd631, %rd630, %rd49;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r740, %rd49;
	cvt.u32.u64 	%r741, %rd630;
	div.u32 	%r742, %r741, %r740;
	cvt.u64.u32 	%rd631, %r742;

$L__BB1_6:
	setp.lt.s32 	%p9, %r152, 3;
	@%p9 bra 	$L__BB1_10;

	or.b64  	%rd201, %rd631, %rd50;
	and.b64  	%rd202, %rd201, -4294967296;
	setp.eq.s64 	%p10, %rd202, 0;
	@%p10 bra 	$L__BB1_9;

	div.u64 	%rd631, %rd631, %rd50;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r743, %rd50;
	cvt.u32.u64 	%r744, %rd631;
	div.u32 	%r745, %r744, %r743;
	cvt.u64.u32 	%rd631, %r745;

$L__BB1_10:
	setp.lt.s32 	%p11, %r152, 2;
	@%p11 bra 	$L__BB1_14;

	or.b64  	%rd203, %rd631, %rd51;
	and.b64  	%rd204, %rd203, -4294967296;
	setp.eq.s64 	%p12, %rd204, 0;
	@%p12 bra 	$L__BB1_13;

	div.u64 	%rd631, %rd631, %rd51;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r746, %rd51;
	cvt.u32.u64 	%r747, %rd631;
	div.u32 	%r748, %r747, %r746;
	cvt.u64.u32 	%rd631, %r748;

$L__BB1_14:
	cvt.s64.s32 	%rd205, %rd631;
	setp.gt.s32 	%p13, %r152, 0;
	selp.b64 	%rd96, %rd205, 0, %p13;
	mul.lo.s64 	%rd206, %rd96, %rd52;
	add.s64 	%rd207, %rd48, %rd206;
	ld.global.u32 	%r8, [%rd207];
	setp.lt.u32 	%p14, %r8, 2;
	mul.lo.s64 	%rd208, %rd96, %rd54;
	add.s64 	%rd97, %rd35, %rd208;
	@%p14 bra 	$L__BB1_244;
	bra.uni 	$L__BB1_15;

$L__BB1_244:
	mul.lo.s64 	%rd406, %rd96, %rd53;
	add.s64 	%rd407, %rd34, %rd406;
	ld.global.u32 	%r847, [%rd97];
	ld.global.u32 	%r848, [%rd407];
	setp.eq.s32 	%p167, %r8, 1;
	selp.b32 	%r88, %r847, %r848, %p167;
	selp.b32 	%r89, %r848, %r847, %p167;
	cvt.s64.s32 	%rd408, %r89;
	mul.lo.s64 	%rd409, %rd408, %rd55;
	add.s64 	%rd410, %rd39, %rd409;
	cvt.s64.s32 	%rd123, %r88;
	mul.lo.s64 	%rd411, %rd123, %rd56;
	add.s64 	%rd412, %rd47, %rd411;
	ld.global.u32 	%r90, [%rd412];
	cvt.s64.s32 	%rd413, %r90;
	mul.lo.s64 	%rd414, %rd413, %rd57;
	add.s64 	%rd415, %rd31, %rd414;
	ld.global.u32 	%r91, [%rd410];
	cvt.s64.s32 	%rd416, %r91;
	mul.lo.s64 	%rd417, %rd416, %rd57;
	add.s64 	%rd418, %rd31, %rd417;
	ld.global.u32 	%r92, [%rd410+4];
	cvt.s64.s32 	%rd419, %r92;
	mul.lo.s64 	%rd420, %rd419, %rd57;
	add.s64 	%rd421, %rd31, %rd420;
	ld.global.u32 	%r93, [%rd410+8];
	cvt.s64.s32 	%rd422, %r93;
	mul.lo.s64 	%rd423, %rd422, %rd57;
	add.s64 	%rd424, %rd31, %rd423;
	mul.lo.s64 	%rd425, %rd413, %rd58;
	add.s64 	%rd426, %rd30, %rd425;
	ld.global.f64 	%fd513, [%rd426];
	ld.global.f64 	%fd514, [%rd426+8];
	ld.global.f64 	%fd515, [%rd426+16];
	mul.lo.s64 	%rd427, %rd416, %rd58;
	add.s64 	%rd428, %rd30, %rd427;
	ld.global.f64 	%fd516, [%rd428];
	ld.global.f64 	%fd517, [%rd428+8];
	ld.global.f64 	%fd518, [%rd428+16];
	mul.lo.s64 	%rd429, %rd419, %rd58;
	add.s64 	%rd430, %rd30, %rd429;
	ld.global.f64 	%fd519, [%rd430];
	ld.global.f64 	%fd520, [%rd430+8];
	ld.global.f64 	%fd521, [%rd430+16];
	mul.lo.s64 	%rd431, %rd422, %rd58;
	add.s64 	%rd432, %rd30, %rd431;
	ld.global.f64 	%fd522, [%rd432];
	ld.global.f64 	%fd523, [%rd432+8];
	ld.global.f64 	%fd524, [%rd432+16];
	mul.lo.s64 	%rd433, %rd96, %rd59;
	add.s64 	%rd434, %rd37, %rd433;
	ld.global.f64 	%fd525, [%rd434];
	ld.global.f64 	%fd526, [%rd434+8];
	mul.lo.s64 	%rd435, %rd123, %rd60;
	add.s64 	%rd436, %rd45, %rd435;
	mul.lo.s64 	%rd437, %rd408, %rd61;
	add.s64 	%rd438, %rd43, %rd437;
	ld.global.f64 	%fd2546, [%rd438];
	ld.global.f64 	%fd2547, [%rd436];
	add.f64 	%fd527, %fd2547, %fd2546;
	ld.global.f64 	%fd528, [%rd421];
	ld.global.f64 	%fd529, [%rd418];
	sub.f64 	%fd2548, %fd528, %fd529;
	ld.global.f64 	%fd530, [%rd421+8];
	ld.global.f64 	%fd531, [%rd418+8];
	sub.f64 	%fd2549, %fd530, %fd531;
	ld.global.f64 	%fd532, [%rd421+16];
	ld.global.f64 	%fd533, [%rd418+16];
	sub.f64 	%fd2550, %fd532, %fd533;
	ld.global.f64 	%fd534, [%rd424];
	sub.f64 	%fd2551, %fd534, %fd529;
	ld.global.f64 	%fd535, [%rd424+8];
	sub.f64 	%fd2552, %fd535, %fd531;
	ld.global.f64 	%fd536, [%rd424+16];
	sub.f64 	%fd2553, %fd536, %fd533;
	mul.f64 	%fd2554, %fd2549, %fd2553;
	mul.f64 	%fd2555, %fd2550, %fd2552;
	sub.f64 	%fd537, %fd2554, %fd2555;
	mul.f64 	%fd2556, %fd2550, %fd2551;
	mul.f64 	%fd2557, %fd2548, %fd2553;
	sub.f64 	%fd538, %fd2556, %fd2557;
	mul.f64 	%fd2558, %fd2548, %fd2552;
	mul.f64 	%fd2559, %fd2549, %fd2551;
	sub.f64 	%fd539, %fd2558, %fd2559;
	mul.f64 	%fd2560, %fd2549, %fd539;
	mul.f64 	%fd2561, %fd2550, %fd538;
	sub.f64 	%fd2562, %fd2560, %fd2561;
	mul.f64 	%fd2563, %fd2550, %fd537;
	mul.f64 	%fd2564, %fd2548, %fd539;
	sub.f64 	%fd2565, %fd2563, %fd2564;
	mul.f64 	%fd2566, %fd2548, %fd538;
	mul.f64 	%fd2567, %fd2549, %fd537;
	sub.f64 	%fd2568, %fd2566, %fd2567;
	mul.f64 	%fd2569, %fd2549, %fd2549;
	fma.rn.f64 	%fd2570, %fd2548, %fd2548, %fd2569;
	fma.rn.f64 	%fd540, %fd2550, %fd2550, %fd2570;
	mul.f64 	%fd2571, %fd2549, %fd2565;
	fma.rn.f64 	%fd2572, %fd2548, %fd2562, %fd2571;
	fma.rn.f64 	%fd2573, %fd2550, %fd2568, %fd2572;
	mul.f64 	%fd2574, %fd2565, %fd2565;
	fma.rn.f64 	%fd2575, %fd2562, %fd2562, %fd2574;
	fma.rn.f64 	%fd2576, %fd2568, %fd2568, %fd2575;
	ld.global.f64 	%fd541, [%rd415];
	sub.f64 	%fd542, %fd541, %fd529;
	ld.global.f64 	%fd543, [%rd415+8];
	sub.f64 	%fd544, %fd543, %fd531;
	ld.global.f64 	%fd545, [%rd415+16];
	sub.f64 	%fd546, %fd545, %fd533;
	mul.f64 	%fd2577, %fd544, %fd2549;
	fma.rn.f64 	%fd2578, %fd542, %fd2548, %fd2577;
	fma.rn.f64 	%fd2579, %fd546, %fd2550, %fd2578;
	mul.f64 	%fd2580, %fd544, %fd2565;
	fma.rn.f64 	%fd2581, %fd542, %fd2562, %fd2580;
	fma.rn.f64 	%fd2582, %fd546, %fd2568, %fd2581;
	div.rn.f64 	%fd2583, %fd2573, %fd540;
	mul.f64 	%fd2584, %fd2583, %fd2583;
	mul.f64 	%fd2585, %fd540, %fd2584;
	sub.f64 	%fd2586, %fd2576, %fd2585;
	mul.f64 	%fd2587, %fd2579, %fd2583;
	sub.f64 	%fd2588, %fd2582, %fd2587;
	div.rn.f64 	%fd2589, %fd2588, %fd2586;
	mul.f64 	%fd2590, %fd540, %fd2583;
	mul.f64 	%fd2591, %fd2590, %fd2589;
	sub.f64 	%fd2592, %fd2579, %fd2591;
	div.rn.f64 	%fd547, %fd2592, %fd540;
	setp.gt.f64 	%p168, %fd547, 0d0000000000000000;
	setp.lt.f64 	%p169, %fd547, 0d3FF0000000000000;
	setp.ge.f64 	%p170, %fd2589, 0d0000000000000000;
	and.pred  	%p171, %p168, %p169;
	and.pred  	%p172, %p170, %p171;
	mov.u32 	%r955, 3;
	@%p172 bra 	$L__BB1_250;

	sub.f64 	%fd2593, %fd534, %fd528;
	sub.f64 	%fd2594, %fd535, %fd530;
	mul.f64 	%fd2595, %fd2594, %fd539;
	sub.f64 	%fd2596, %fd536, %fd532;
	mul.f64 	%fd2597, %fd2596, %fd538;
	sub.f64 	%fd2598, %fd2595, %fd2597;
	mul.f64 	%fd2599, %fd2596, %fd537;
	mul.f64 	%fd2600, %fd2593, %fd539;
	sub.f64 	%fd2601, %fd2599, %fd2600;
	mul.f64 	%fd2602, %fd2593, %fd538;
	mul.f64 	%fd2603, %fd2594, %fd537;
	sub.f64 	%fd2604, %fd2602, %fd2603;
	mul.f64 	%fd2605, %fd2594, %fd2594;
	fma.rn.f64 	%fd2606, %fd2593, %fd2593, %fd2605;
	fma.rn.f64 	%fd2607, %fd2596, %fd2596, %fd2606;
	mul.f64 	%fd2608, %fd2594, %fd2601;
	fma.rn.f64 	%fd2609, %fd2593, %fd2598, %fd2608;
	fma.rn.f64 	%fd2610, %fd2596, %fd2604, %fd2609;
	mul.f64 	%fd2611, %fd2601, %fd2601;
	fma.rn.f64 	%fd2612, %fd2598, %fd2598, %fd2611;
	fma.rn.f64 	%fd2613, %fd2604, %fd2604, %fd2612;
	sub.f64 	%fd2614, %fd541, %fd528;
	sub.f64 	%fd2615, %fd543, %fd530;
	mul.f64 	%fd2616, %fd2615, %fd2594;
	fma.rn.f64 	%fd2617, %fd2614, %fd2593, %fd2616;
	sub.f64 	%fd2618, %fd545, %fd532;
	fma.rn.f64 	%fd2619, %fd2618, %fd2596, %fd2617;
	mul.f64 	%fd2620, %fd2615, %fd2601;
	fma.rn.f64 	%fd2621, %fd2614, %fd2598, %fd2620;
	fma.rn.f64 	%fd2622, %fd2618, %fd2604, %fd2621;
	div.rn.f64 	%fd2623, %fd2610, %fd2607;
	mul.f64 	%fd2624, %fd2623, %fd2623;
	mul.f64 	%fd2625, %fd2607, %fd2624;
	sub.f64 	%fd2626, %fd2613, %fd2625;
	mul.f64 	%fd2627, %fd2619, %fd2623;
	sub.f64 	%fd2628, %fd2622, %fd2627;
	div.rn.f64 	%fd2629, %fd2628, %fd2626;
	mul.f64 	%fd2630, %fd2607, %fd2623;
	mul.f64 	%fd2631, %fd2630, %fd2629;
	sub.f64 	%fd2632, %fd2619, %fd2631;
	div.rn.f64 	%fd548, %fd2632, %fd2607;
	setp.gt.f64 	%p173, %fd548, 0d0000000000000000;
	setp.lt.f64 	%p174, %fd548, 0d3FF0000000000000;
	setp.ge.f64 	%p175, %fd2629, 0d0000000000000000;
	and.pred  	%p176, %p173, %p174;
	and.pred  	%p177, %p175, %p176;
	mov.u32 	%r955, 4;
	@%p177 bra 	$L__BB1_250;

	sub.f64 	%fd2633, %fd529, %fd534;
	sub.f64 	%fd2634, %fd531, %fd535;
	mul.f64 	%fd2635, %fd2634, %fd539;
	sub.f64 	%fd2636, %fd533, %fd536;
	mul.f64 	%fd2637, %fd2636, %fd538;
	sub.f64 	%fd2638, %fd2635, %fd2637;
	mul.f64 	%fd2639, %fd2636, %fd537;
	mul.f64 	%fd2640, %fd2633, %fd539;
	sub.f64 	%fd2641, %fd2639, %fd2640;
	mul.f64 	%fd2642, %fd2633, %fd538;
	mul.f64 	%fd2643, %fd2634, %fd537;
	sub.f64 	%fd2644, %fd2642, %fd2643;
	mul.f64 	%fd2645, %fd2634, %fd2634;
	fma.rn.f64 	%fd2646, %fd2633, %fd2633, %fd2645;
	fma.rn.f64 	%fd2647, %fd2636, %fd2636, %fd2646;
	mul.f64 	%fd2648, %fd2634, %fd2641;
	fma.rn.f64 	%fd2649, %fd2633, %fd2638, %fd2648;
	fma.rn.f64 	%fd2650, %fd2636, %fd2644, %fd2649;
	mul.f64 	%fd2651, %fd2641, %fd2641;
	fma.rn.f64 	%fd2652, %fd2638, %fd2638, %fd2651;
	fma.rn.f64 	%fd2653, %fd2644, %fd2644, %fd2652;
	sub.f64 	%fd2654, %fd541, %fd534;
	sub.f64 	%fd2655, %fd543, %fd535;
	mul.f64 	%fd2656, %fd2634, %fd2655;
	fma.rn.f64 	%fd2657, %fd2633, %fd2654, %fd2656;
	sub.f64 	%fd2658, %fd545, %fd536;
	fma.rn.f64 	%fd2659, %fd2636, %fd2658, %fd2657;
	mul.f64 	%fd2660, %fd2655, %fd2641;
	fma.rn.f64 	%fd2661, %fd2654, %fd2638, %fd2660;
	fma.rn.f64 	%fd2662, %fd2658, %fd2644, %fd2661;
	div.rn.f64 	%fd2663, %fd2650, %fd2647;
	mul.f64 	%fd2664, %fd2663, %fd2663;
	mul.f64 	%fd2665, %fd2647, %fd2664;
	sub.f64 	%fd2666, %fd2653, %fd2665;
	mul.f64 	%fd2667, %fd2659, %fd2663;
	sub.f64 	%fd2668, %fd2662, %fd2667;
	div.rn.f64 	%fd2669, %fd2668, %fd2666;
	mul.f64 	%fd2670, %fd2647, %fd2663;
	mul.f64 	%fd2671, %fd2670, %fd2669;
	sub.f64 	%fd2672, %fd2659, %fd2671;
	div.rn.f64 	%fd549, %fd2672, %fd2647;
	setp.gt.f64 	%p178, %fd549, 0d0000000000000000;
	setp.lt.f64 	%p179, %fd549, 0d3FF0000000000000;
	setp.ge.f64 	%p180, %fd2669, 0d0000000000000000;
	and.pred  	%p181, %p178, %p179;
	and.pred  	%p182, %p180, %p181;
	mov.u32 	%r955, 5;
	@%p182 bra 	$L__BB1_250;

	setp.le.f64 	%p183, %fd547, 0d0000000000000000;
	setp.ge.f64 	%p184, %fd549, 0d3FF0000000000000;
	and.pred  	%p185, %p183, %p184;
	mov.u32 	%r955, 0;
	@%p185 bra 	$L__BB1_250;

	setp.le.f64 	%p186, %fd548, 0d0000000000000000;
	setp.ge.f64 	%p187, %fd547, 0d3FF0000000000000;
	and.pred  	%p188, %p186, %p187;
	mov.u32 	%r955, 1;
	@%p188 bra 	$L__BB1_250;

	setp.le.f64 	%p189, %fd549, 0d0000000000000000;
	setp.ge.f64 	%p190, %fd548, 0d3FF0000000000000;
	and.pred  	%p191, %p189, %p190;
	selp.b32 	%r955, 2, 6, %p191;

$L__BB1_250:
	setp.eq.s32 	%p192, %r955, 0;
	@%p192 bra 	$L__BB1_262;

	setp.eq.s32 	%p193, %r955, 1;
	@%p193 bra 	$L__BB1_261;
	bra.uni 	$L__BB1_252;

$L__BB1_261:
	sub.f64 	%fd2751, %fd541, %fd528;
	sub.f64 	%fd2752, %fd543, %fd530;
	mul.f64 	%fd2753, %fd2752, %fd2752;
	fma.rn.f64 	%fd2754, %fd2751, %fd2751, %fd2753;
	sub.f64 	%fd2755, %fd545, %fd532;
	fma.rn.f64 	%fd4464, %fd2755, %fd2755, %fd2754;
	bra.uni 	$L__BB1_263;

$L__BB1_15:
	cvt.s64.s32 	%rd613, %r588;
	mul.lo.s64 	%rd209, %rd96, %rd53;
	add.s64 	%rd210, %rd34, %rd209;
	mul.lo.s64 	%rd98, %rd96, %rd59;
	add.s64 	%rd211, %rd37, %rd98;
	ld.global.s32 	%rd99, [%rd97];
	mul.lo.s64 	%rd212, %rd99, %rd62;
	add.s64 	%rd213, %rd40, %rd212;
	ld.global.s32 	%rd100, [%rd210];
	mul.lo.s64 	%rd214, %rd100, %rd62;
	add.s64 	%rd215, %rd40, %rd214;
	mul.lo.s64 	%rd101, %rd99, %rd613;
	add.s64 	%rd216, %rd44, %rd101;
	mul.lo.s64 	%rd102, %rd100, %rd613;
	add.s64 	%rd217, %rd44, %rd102;
	ld.global.f64 	%fd971, [%rd217];
	ld.global.f64 	%fd972, [%rd216];
	add.f64 	%fd23, %fd972, %fd971;
	ld.global.u32 	%r9, [%rd213];
	cvt.s64.s32 	%rd103, %r9;
	mul.lo.s64 	%rd104, %rd103, %rd57;
	add.s64 	%rd218, %rd31, %rd104;
	ld.global.s32 	%rd105, [%rd213+4];
	mul.lo.s64 	%rd106, %rd105, %rd57;
	add.s64 	%rd219, %rd31, %rd106;
	ld.global.s32 	%rd107, [%rd215];
	mul.lo.s64 	%rd108, %rd107, %rd57;
	add.s64 	%rd220, %rd31, %rd108;
	ld.global.s32 	%rd109, [%rd215+4];
	mul.lo.s64 	%rd110, %rd109, %rd57;
	add.s64 	%rd221, %rd31, %rd110;
	mul.lo.s64 	%rd111, %rd103, %rd58;
	add.s64 	%rd222, %rd30, %rd111;
	mul.lo.s64 	%rd112, %rd105, %rd58;
	add.s64 	%rd223, %rd30, %rd112;
	mul.lo.s64 	%rd113, %rd107, %rd58;
	add.s64 	%rd224, %rd30, %rd113;
	mul.lo.s64 	%rd114, %rd109, %rd58;
	add.s64 	%rd225, %rd30, %rd114;
	ld.global.f64 	%fd24, [%rd211];
	ld.global.f64 	%fd26, [%rd218];
	ld.global.f64 	%fd27, [%rd219];
	ld.global.f64 	%fd29, [%rd211+8];
	ld.global.f64 	%fd31, [%rd220];
	ld.global.f64 	%fd32, [%rd221];
	ld.global.f64 	%fd33, [%rd218+8];
	ld.global.f64 	%fd34, [%rd219+8];
	ld.global.f64 	%fd36, [%rd220+8];
	ld.global.f64 	%fd37, [%rd221+8];
	ld.global.f64 	%fd38, [%rd218+16];
	ld.global.f64 	%fd39, [%rd219+16];
	ld.global.f64 	%fd41, [%rd220+16];
	ld.global.f64 	%fd42, [%rd221+16];
	ld.global.f64 	%fd43, [%rd222];
	ld.global.f64 	%fd44, [%rd223];
	ld.global.f64 	%fd46, [%rd224];
	ld.global.f64 	%fd47, [%rd225];
	ld.global.f64 	%fd48, [%rd222+8];
	ld.global.f64 	%fd49, [%rd223+8];
	ld.global.f64 	%fd51, [%rd224+8];
	ld.global.f64 	%fd52, [%rd225+8];
	ld.global.f64 	%fd53, [%rd222+16];
	ld.global.f64 	%fd54, [%rd223+16];
	ld.global.f64 	%fd56, [%rd224+16];
	ld.global.f64 	%fd57, [%rd225+16];
	sub.f64 	%fd58, %fd27, %fd26;
	sub.f64 	%fd59, %fd34, %fd33;
	sub.f64 	%fd60, %fd39, %fd38;
	sub.f64 	%fd61, %fd32, %fd31;
	sub.f64 	%fd62, %fd37, %fd36;
	sub.f64 	%fd63, %fd42, %fd41;
	sub.f64 	%fd64, %fd26, %fd31;
	sub.f64 	%fd65, %fd33, %fd36;
	sub.f64 	%fd66, %fd38, %fd41;
	mul.f64 	%fd980, %fd59, %fd59;
	fma.rn.f64 	%fd981, %fd58, %fd58, %fd980;
	fma.rn.f64 	%fd67, %fd60, %fd60, %fd981;
	mul.f64 	%fd982, %fd59, %fd62;
	fma.rn.f64 	%fd983, %fd58, %fd61, %fd982;
	fma.rn.f64 	%fd68, %fd60, %fd63, %fd983;
	mul.f64 	%fd984, %fd62, %fd62;
	fma.rn.f64 	%fd985, %fd61, %fd61, %fd984;
	fma.rn.f64 	%fd69, %fd63, %fd63, %fd985;
	mul.f64 	%fd986, %fd59, %fd65;
	fma.rn.f64 	%fd987, %fd58, %fd64, %fd986;
	fma.rn.f64 	%fd70, %fd60, %fd66, %fd987;
	mul.f64 	%fd988, %fd65, %fd62;
	fma.rn.f64 	%fd989, %fd64, %fd61, %fd988;
	fma.rn.f64 	%fd71, %fd66, %fd63, %fd989;
	mul.f64 	%fd990, %fd67, %fd69;
	mul.f64 	%fd991, %fd68, %fd68;
	sub.f64 	%fd72, %fd990, %fd991;
	mul.f64 	%fd992, %fd68, %fd71;
	mul.f64 	%fd993, %fd70, %fd69;
	sub.f64 	%fd73, %fd992, %fd993;
	setp.le.f64 	%p15, %fd73, 0d0000000000000000;
	@%p15 bra 	$L__BB1_19;

	setp.ge.f64 	%p1, %fd73, %fd72;
	add.f64 	%fd74, %fd71, %fd68;
	@%p1 bra 	$L__BB1_18;

	selp.f64 	%fd995, %fd69, %fd72, %p1;
	mul.f64 	%fd996, %fd67, %fd71;
	mul.f64 	%fd997, %fd70, %fd68;
	sub.f64 	%fd998, %fd996, %fd997;
	mul.f64 	%fd999, %fd60, %fd62;
	mul.f64 	%fd1000, %fd59, %fd63;
	sub.f64 	%fd1001, %fd1000, %fd999;
	mul.f64 	%fd1002, %fd58, %fd63;
	mul.f64 	%fd1003, %fd60, %fd61;
	sub.f64 	%fd1004, %fd1003, %fd1002;
	mul.f64 	%fd1005, %fd59, %fd61;
	mul.f64 	%fd1006, %fd58, %fd62;
	sub.f64 	%fd1007, %fd1006, %fd1005;
	setp.gt.f64 	%p16, %fd998, 0d0000000000000000;
	setp.lt.f64 	%p17, %fd998, %fd995;
	mul.f64 	%fd1008, %fd65, %fd1004;
	fma.rn.f64 	%fd1009, %fd64, %fd1001, %fd1008;
	fma.rn.f64 	%fd1010, %fd66, %fd1007, %fd1009;
	setp.eq.f64 	%p18, %fd1010, 0d0000000000000000;
	mul.f64 	%fd1011, %fd1004, %fd1004;
	fma.rn.f64 	%fd1012, %fd1001, %fd1001, %fd1011;
	fma.rn.f64 	%fd1013, %fd1007, %fd1007, %fd1012;
	mul.f64 	%fd1014, %fd67, 0d3BC79CA100000000;
	mul.f64 	%fd1015, %fd1014, %fd69;
	setp.lt.f64 	%p19, %fd1013, %fd1015;
	or.pred  	%p20, %p18, %p19;
	and.pred  	%p21, %p16, %p17;
	and.pred  	%p22, %p20, %p21;
	mul.f64 	%fd1016, %fd72, 0d3FE0000000000000;
	setp.lt.f64 	%p23, %fd73, %fd1016;
	selp.b32 	%r751, 2, 5, %p23;
	selp.f64 	%fd1017, %fd71, %fd74, %p23;
	selp.f64 	%fd4353, %fd69, %fd995, %p22;
	selp.b32 	%r921, %r751, 8, %p22;
	selp.f64 	%fd4354, %fd1017, %fd998, %p22;

$L__BB1_18:
	selp.f64 	%fd4355, %fd69, %fd4353, %p1;
	selp.b32 	%r922, 5, %r921, %p1;
	selp.f64 	%fd4356, %fd74, %fd4354, %p1;

$L__BB1_19:
	selp.f64 	%fd83, %fd69, %fd4355, %p15;
	selp.b32 	%r923, 2, %r922, %p15;
	selp.f64 	%fd84, %fd71, %fd4356, %p15;
	setp.gtu.f64 	%p26, %fd84, 0d0000000000000000;
	@%p26 bra 	$L__BB1_23;
	bra.uni 	$L__BB1_20;

$L__BB1_23:
	setp.ltu.f64 	%p29, %fd84, %fd83;
	@%p29 bra 	$L__BB1_27;

	mov.f64 	%fd1019, 0d0000000000000000;
	sub.f64 	%fd1020, %fd1019, %fd70;
	add.f64 	%fd86, %fd1020, %fd68;
	setp.le.f64 	%p30, %fd86, 0d0000000000000000;
	mov.u32 	%r923, 1;
	@%p30 bra 	$L__BB1_27;

	setp.ge.f64 	%p31, %fd86, %fd67;
	mov.u32 	%r923, 4;
	@%p31 bra 	$L__BB1_27;

	mov.u32 	%r923, 7;
	bra.uni 	$L__BB1_27;

$L__BB1_20:
	mov.f64 	%fd1018, 0d0000000000000000;
	sub.f64 	%fd85, %fd1018, %fd70;
	setp.le.f64 	%p27, %fd85, 0d0000000000000000;
	mov.u32 	%r923, 0;
	@%p27 bra 	$L__BB1_27;

	setp.ge.f64 	%p28, %fd85, %fd67;
	mov.u32 	%r923, 3;
	@%p28 bra 	$L__BB1_27;

	mov.u32 	%r923, 6;

$L__BB1_27:
	setp.eq.s32 	%p32, %r923, 0;
	@%p32 bra 	$L__BB1_43;

	setp.eq.s32 	%p33, %r923, 1;
	@%p33 bra 	$L__BB1_42;
	bra.uni 	$L__BB1_29;

$L__BB1_42:
	sub.f64 	%fd1119, %fd26, %fd32;
	sub.f64 	%fd1120, %fd33, %fd37;
	mul.f64 	%fd1121, %fd1120, %fd1120;
	fma.rn.f64 	%fd1122, %fd1119, %fd1119, %fd1121;
	sub.f64 	%fd1123, %fd38, %fd42;
	fma.rn.f64 	%fd4357, %fd1123, %fd1123, %fd1122;
	bra.uni 	$L__BB1_44;

$L__BB1_262:
	mul.f64 	%fd2756, %fd544, %fd544;
	fma.rn.f64 	%fd2757, %fd542, %fd542, %fd2756;
	fma.rn.f64 	%fd4464, %fd546, %fd546, %fd2757;
	bra.uni 	$L__BB1_263;

$L__BB1_43:
	mul.f64 	%fd1124, %fd65, %fd65;
	fma.rn.f64 	%fd1125, %fd64, %fd64, %fd1124;
	fma.rn.f64 	%fd4357, %fd66, %fd66, %fd1125;
	bra.uni 	$L__BB1_44;

$L__BB1_252:
	setp.eq.s32 	%p194, %r955, 2;
	@%p194 bra 	$L__BB1_260;
	bra.uni 	$L__BB1_253;

$L__BB1_260:
	sub.f64 	%fd2746, %fd541, %fd534;
	sub.f64 	%fd2747, %fd543, %fd535;
	mul.f64 	%fd2748, %fd2747, %fd2747;
	fma.rn.f64 	%fd2749, %fd2746, %fd2746, %fd2748;
	sub.f64 	%fd2750, %fd545, %fd536;
	fma.rn.f64 	%fd4464, %fd2750, %fd2750, %fd2749;
	bra.uni 	$L__BB1_263;

$L__BB1_29:
	setp.eq.s32 	%p34, %r923, 2;
	@%p34 bra 	$L__BB1_41;
	bra.uni 	$L__BB1_30;

$L__BB1_41:
	sub.f64 	%fd1101, %fd31, %fd26;
	sub.f64 	%fd1102, %fd42, %fd38;
	sub.f64 	%fd1103, %fd36, %fd33;
	mul.f64 	%fd1104, %fd1103, %fd1102;
	sub.f64 	%fd1105, %fd37, %fd33;
	sub.f64 	%fd1106, %fd41, %fd38;
	mul.f64 	%fd1107, %fd1106, %fd1105;
	sub.f64 	%fd1108, %fd1104, %fd1107;
	sub.f64 	%fd1109, %fd32, %fd26;
	mul.f64 	%fd1110, %fd1106, %fd1109;
	mul.f64 	%fd1111, %fd1101, %fd1102;
	sub.f64 	%fd1112, %fd1110, %fd1111;
	mul.f64 	%fd1113, %fd1101, %fd1105;
	mul.f64 	%fd1114, %fd1103, %fd1109;
	sub.f64 	%fd1115, %fd1113, %fd1114;
	mul.f64 	%fd1116, %fd1112, %fd1112;
	fma.rn.f64 	%fd1117, %fd1108, %fd1108, %fd1116;
	fma.rn.f64 	%fd1118, %fd1115, %fd1115, %fd1117;
	div.rn.f64 	%fd4357, %fd1118, %fd69;
	bra.uni 	$L__BB1_44;

$L__BB1_253:
	setp.eq.s32 	%p195, %r955, 3;
	@%p195 bra 	$L__BB1_259;
	bra.uni 	$L__BB1_254;

$L__BB1_259:
	sub.f64 	%fd2728, %fd529, %fd541;
	sub.f64 	%fd2729, %fd532, %fd545;
	sub.f64 	%fd2730, %fd531, %fd543;
	mul.f64 	%fd2731, %fd2730, %fd2729;
	sub.f64 	%fd2732, %fd530, %fd543;
	sub.f64 	%fd2733, %fd533, %fd545;
	mul.f64 	%fd2734, %fd2733, %fd2732;
	sub.f64 	%fd2735, %fd2731, %fd2734;
	sub.f64 	%fd2736, %fd528, %fd541;
	mul.f64 	%fd2737, %fd2733, %fd2736;
	mul.f64 	%fd2738, %fd2728, %fd2729;
	sub.f64 	%fd2739, %fd2737, %fd2738;
	mul.f64 	%fd2740, %fd2728, %fd2732;
	mul.f64 	%fd2741, %fd2730, %fd2736;
	sub.f64 	%fd2742, %fd2740, %fd2741;
	mul.f64 	%fd2743, %fd2739, %fd2739;
	fma.rn.f64 	%fd2744, %fd2735, %fd2735, %fd2743;
	fma.rn.f64 	%fd2745, %fd2742, %fd2742, %fd2744;
	div.rn.f64 	%fd4464, %fd2745, %fd540;
	bra.uni 	$L__BB1_263;

$L__BB1_30:
	setp.eq.s32 	%p35, %r923, 3;
	@%p35 bra 	$L__BB1_40;
	bra.uni 	$L__BB1_31;

$L__BB1_40:
	sub.f64 	%fd1096, %fd27, %fd31;
	sub.f64 	%fd1097, %fd34, %fd36;
	mul.f64 	%fd1098, %fd1097, %fd1097;
	fma.rn.f64 	%fd1099, %fd1096, %fd1096, %fd1098;
	sub.f64 	%fd1100, %fd39, %fd41;
	fma.rn.f64 	%fd4357, %fd1100, %fd1100, %fd1099;
	bra.uni 	$L__BB1_44;

$L__BB1_254:
	setp.eq.s32 	%p196, %r955, 4;
	@%p196 bra 	$L__BB1_258;
	bra.uni 	$L__BB1_255;

$L__BB1_258:
	sub.f64 	%fd2704, %fd528, %fd541;
	sub.f64 	%fd2705, %fd536, %fd545;
	sub.f64 	%fd2706, %fd530, %fd543;
	mul.f64 	%fd2707, %fd2706, %fd2705;
	sub.f64 	%fd2708, %fd535, %fd543;
	sub.f64 	%fd2709, %fd532, %fd545;
	mul.f64 	%fd2710, %fd2709, %fd2708;
	sub.f64 	%fd2711, %fd2707, %fd2710;
	sub.f64 	%fd2712, %fd534, %fd541;
	mul.f64 	%fd2713, %fd2709, %fd2712;
	mul.f64 	%fd2714, %fd2704, %fd2705;
	sub.f64 	%fd2715, %fd2713, %fd2714;
	mul.f64 	%fd2716, %fd2704, %fd2708;
	mul.f64 	%fd2717, %fd2706, %fd2712;
	sub.f64 	%fd2718, %fd2716, %fd2717;
	mul.f64 	%fd2719, %fd2715, %fd2715;
	fma.rn.f64 	%fd2720, %fd2711, %fd2711, %fd2719;
	fma.rn.f64 	%fd2721, %fd2718, %fd2718, %fd2720;
	sub.f64 	%fd2722, %fd534, %fd528;
	sub.f64 	%fd2723, %fd535, %fd530;
	mul.f64 	%fd2724, %fd2723, %fd2723;
	fma.rn.f64 	%fd2725, %fd2722, %fd2722, %fd2724;
	sub.f64 	%fd2726, %fd536, %fd532;
	fma.rn.f64 	%fd2727, %fd2726, %fd2726, %fd2725;
	div.rn.f64 	%fd4464, %fd2721, %fd2727;
	bra.uni 	$L__BB1_263;

$L__BB1_31:
	setp.eq.s32 	%p36, %r923, 4;
	@%p36 bra 	$L__BB1_39;
	bra.uni 	$L__BB1_32;

$L__BB1_39:
	sub.f64 	%fd1091, %fd27, %fd32;
	sub.f64 	%fd1092, %fd34, %fd37;
	mul.f64 	%fd1093, %fd1092, %fd1092;
	fma.rn.f64 	%fd1094, %fd1091, %fd1091, %fd1093;
	sub.f64 	%fd1095, %fd39, %fd42;
	fma.rn.f64 	%fd4357, %fd1095, %fd1095, %fd1094;
	bra.uni 	$L__BB1_44;

$L__BB1_255:
	setp.eq.s32 	%p197, %r955, 5;
	@%p197 bra 	$L__BB1_257;
	bra.uni 	$L__BB1_256;

$L__BB1_257:
	sub.f64 	%fd2680, %fd534, %fd541;
	sub.f64 	%fd2681, %fd533, %fd545;
	sub.f64 	%fd2682, %fd535, %fd543;
	mul.f64 	%fd2683, %fd2681, %fd2682;
	sub.f64 	%fd2684, %fd531, %fd543;
	sub.f64 	%fd2685, %fd536, %fd545;
	mul.f64 	%fd2686, %fd2684, %fd2685;
	sub.f64 	%fd2687, %fd2683, %fd2686;
	sub.f64 	%fd2688, %fd529, %fd541;
	mul.f64 	%fd2689, %fd2688, %fd2685;
	mul.f64 	%fd2690, %fd2681, %fd2680;
	sub.f64 	%fd2691, %fd2689, %fd2690;
	mul.f64 	%fd2692, %fd2684, %fd2680;
	mul.f64 	%fd2693, %fd2688, %fd2682;
	sub.f64 	%fd2694, %fd2692, %fd2693;
	mul.f64 	%fd2695, %fd2691, %fd2691;
	fma.rn.f64 	%fd2696, %fd2687, %fd2687, %fd2695;
	fma.rn.f64 	%fd2697, %fd2694, %fd2694, %fd2696;
	sub.f64 	%fd2698, %fd529, %fd534;
	sub.f64 	%fd2699, %fd531, %fd535;
	mul.f64 	%fd2700, %fd2699, %fd2699;
	fma.rn.f64 	%fd2701, %fd2698, %fd2698, %fd2700;
	sub.f64 	%fd2702, %fd533, %fd536;
	fma.rn.f64 	%fd2703, %fd2702, %fd2702, %fd2701;
	div.rn.f64 	%fd4464, %fd2697, %fd2703;
	bra.uni 	$L__BB1_263;

$L__BB1_32:
	setp.eq.s32 	%p37, %r923, 5;
	@%p37 bra 	$L__BB1_38;
	bra.uni 	$L__BB1_33;

$L__BB1_38:
	sub.f64 	%fd1073, %fd31, %fd27;
	sub.f64 	%fd1074, %fd42, %fd39;
	sub.f64 	%fd1075, %fd36, %fd34;
	mul.f64 	%fd1076, %fd1075, %fd1074;
	sub.f64 	%fd1077, %fd37, %fd34;
	sub.f64 	%fd1078, %fd41, %fd39;
	mul.f64 	%fd1079, %fd1078, %fd1077;
	sub.f64 	%fd1080, %fd1076, %fd1079;
	sub.f64 	%fd1081, %fd32, %fd27;
	mul.f64 	%fd1082, %fd1078, %fd1081;
	mul.f64 	%fd1083, %fd1073, %fd1074;
	sub.f64 	%fd1084, %fd1082, %fd1083;
	mul.f64 	%fd1085, %fd1073, %fd1077;
	mul.f64 	%fd1086, %fd1075, %fd1081;
	sub.f64 	%fd1087, %fd1085, %fd1086;
	mul.f64 	%fd1088, %fd1084, %fd1084;
	fma.rn.f64 	%fd1089, %fd1080, %fd1080, %fd1088;
	fma.rn.f64 	%fd1090, %fd1087, %fd1087, %fd1089;
	div.rn.f64 	%fd4357, %fd1090, %fd69;
	bra.uni 	$L__BB1_44;

$L__BB1_256:
	mul.f64 	%fd2673, %fd544, %fd538;
	fma.rn.f64 	%fd2674, %fd542, %fd537, %fd2673;
	fma.rn.f64 	%fd2675, %fd546, %fd539, %fd2674;
	mul.f64 	%fd2676, %fd2675, %fd2675;
	mul.f64 	%fd2677, %fd538, %fd538;
	fma.rn.f64 	%fd2678, %fd537, %fd537, %fd2677;
	fma.rn.f64 	%fd2679, %fd539, %fd539, %fd2678;
	div.rn.f64 	%fd4464, %fd2676, %fd2679;

$L__BB1_263:
	mul.f64 	%fd2759, %fd527, %fd527;
	sub.f64 	%fd558, %fd4464, %fd2759;
	mov.f64 	%fd2760, 0d3FF0000000000000;
	sub.f64 	%fd2761, %fd2760, %fd525;
	sub.f64 	%fd559, %fd2761, %fd526;
	fma.rn.f64 	%fd560, %fd2, %fd527, %fd1;
	mul.lo.s64 	%rd439, %rd123, %rd64;
	add.s64 	%rd124, %rd42, %rd439;
	ld.global.f64 	%fd2762, [%rd124];
	mul.f64 	%fd2763, %fd2762, %fd965;
	mul.f64 	%fd561, %fd2763, %fd962;
	setp.geu.f64 	%p198, %fd558, %fd560;
	@%p198 bra 	$L__BB1_272;

	div.rn.f64 	%fd4465, %fd558, %fd560;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r956}, %fd4465;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r957, %temp}, %fd4465;
	}
	setp.gt.s32 	%p199, %r956, 1048575;
	mov.u32 	%r958, -1023;
	@%p199 bra 	$L__BB1_266;

	mul.f64 	%fd4465, %fd4465, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r956}, %fd4465;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r957, %temp}, %fd4465;
	}
	mov.u32 	%r958, -1077;

$L__BB1_266:
	add.s32 	%r855, %r956, -1;
	setp.lt.u32 	%p200, %r855, 2146435071;
	@%p200 bra 	$L__BB1_268;
	bra.uni 	$L__BB1_267;

$L__BB1_268:
	shr.u32 	%r857, %r956, 20;
	add.s32 	%r959, %r958, %r857;
	and.b32  	%r858, %r956, -2146435073;
	or.b32  	%r859, %r858, 1072693248;
	mov.b64 	%fd4466, {%r957, %r859};
	setp.lt.s32 	%p202, %r859, 1073127583;
	@%p202 bra 	$L__BB1_270;

	{
	.reg .b32 %temp;
	mov.b64 	{%r860, %temp}, %fd4466;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r861}, %fd4466;
	}
	add.s32 	%r862, %r861, -1048576;
	mov.b64 	%fd4466, {%r860, %r862};
	add.s32 	%r959, %r959, 1;

$L__BB1_270:
	add.f64 	%fd2766, %fd4466, 0d3FF0000000000000;
	mov.f64 	%fd2767, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2768, %fd2766;
	neg.f64 	%fd2769, %fd2766;
	fma.rn.f64 	%fd2770, %fd2769, %fd2768, %fd2767;
	fma.rn.f64 	%fd2771, %fd2770, %fd2770, %fd2770;
	fma.rn.f64 	%fd2772, %fd2771, %fd2768, %fd2768;
	add.f64 	%fd2773, %fd4466, 0dBFF0000000000000;
	mul.f64 	%fd2774, %fd2773, %fd2772;
	fma.rn.f64 	%fd2775, %fd2773, %fd2772, %fd2774;
	mul.f64 	%fd2776, %fd2775, %fd2775;
	mov.f64 	%fd2777, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2778, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2779, %fd2778, %fd2776, %fd2777;
	mov.f64 	%fd2780, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2781, %fd2779, %fd2776, %fd2780;
	mov.f64 	%fd2782, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2783, %fd2781, %fd2776, %fd2782;
	mov.f64 	%fd2784, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2785, %fd2783, %fd2776, %fd2784;
	mov.f64 	%fd2786, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2787, %fd2785, %fd2776, %fd2786;
	mov.f64 	%fd2788, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2789, %fd2787, %fd2776, %fd2788;
	mov.f64 	%fd2790, 0d3FB5555555555554;
	fma.rn.f64 	%fd2791, %fd2789, %fd2776, %fd2790;
	sub.f64 	%fd2792, %fd2773, %fd2775;
	add.f64 	%fd2793, %fd2792, %fd2792;
	neg.f64 	%fd2794, %fd2775;
	fma.rn.f64 	%fd2795, %fd2794, %fd2773, %fd2793;
	mul.f64 	%fd2796, %fd2772, %fd2795;
	mul.f64 	%fd2797, %fd2776, %fd2791;
	fma.rn.f64 	%fd2798, %fd2797, %fd2775, %fd2796;
	xor.b32  	%r863, %r959, -2147483648;
	mov.u32 	%r864, -2147483648;
	mov.u32 	%r865, 1127219200;
	mov.b64 	%fd2799, {%r863, %r865};
	mov.b64 	%fd2800, {%r864, %r865};
	sub.f64 	%fd2801, %fd2799, %fd2800;
	mov.f64 	%fd2802, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2803, %fd2801, %fd2802, %fd2775;
	neg.f64 	%fd2804, %fd2801;
	fma.rn.f64 	%fd2805, %fd2804, %fd2802, %fd2803;
	sub.f64 	%fd2806, %fd2805, %fd2775;
	sub.f64 	%fd2807, %fd2798, %fd2806;
	mov.f64 	%fd2808, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2809, %fd2801, %fd2808, %fd2807;
	add.f64 	%fd4467, %fd2803, %fd2809;
	bra.uni 	$L__BB1_271;

$L__BB1_267:
	mov.f64 	%fd2764, 0d7FF0000000000000;
	fma.rn.f64 	%fd2765, %fd4465, %fd2764, %fd2764;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r856}, %fd4465;
	}
	mov.b32 	%f7, %r856;
	setp.eq.f32 	%p201, %f7, 0f00000000;
	selp.f64 	%fd4467, 0dFFF0000000000000, %fd2765, %p201;

$L__BB1_271:
	sub.f64 	%fd2810, %fd558, %fd560;
	div.rn.f64 	%fd2811, %fd2810, %fd560;
	mul.f64 	%fd2812, %fd4, %fd2811;
	mul.f64 	%fd2813, %fd2811, %fd2812;
	mul.f64 	%fd4468, %fd2813, %fd4467;

$L__BB1_272:
	setp.lt.f64 	%p203, %fd558, %fd560;
	selp.f64 	%fd573, %fd4468, 0d0000000000000000, %p203;
	mul.f64 	%fd2814, %fd561, %fd573;
	mul.f64 	%fd2815, %fd2814, %fd966;
	setp.nan.f64 	%p4, %fd2815, %fd2815;
	setp.num.f64 	%p204, %fd2815, %fd2815;
	@%p204 bra 	$L__BB1_274;

	add.u64 	%rd440, %SP, 0;
	add.u64 	%rd441, %SPL, 0;
	mov.u64 	%rd442, $str$3;
	cvta.global.u64 	%rd443, %rd442;
	st.local.u64 	[%rd441], %rd443;
	mov.u64 	%rd444, $str$5;
	cvta.global.u64 	%rd445, %rd444;
	{ // callseq 255, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd445;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd440;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r866, [retval0+0];
	} // callseq 255

$L__BB1_274:
	setp.eq.s32 	%p205, %r342, 0;
	@%p205 bra 	$L__BB1_276;

	mul.lo.s64 	%rd446, %rd96, %rd69;
	add.s64 	%rd447, %rd33, %rd446;
	ld.global.u32 	%r979, [%rd447];
	cvt.s64.s32 	%rd448, %r979;
	mul.lo.s64 	%rd449, %rd448, %rd70;
	add.s64 	%rd450, %rd29, %rd449;
	mul.lo.s64 	%rd451, %rd96, %rd71;
	add.s64 	%rd452, %rd32, %rd451;
	ld.global.u32 	%r980, [%rd452];
	cvt.s64.s32 	%rd453, %r980;
	mul.lo.s64 	%rd454, %rd453, %rd70;
	add.s64 	%rd455, %rd29, %rd454;
	ld.global.f64 	%fd4578, [%rd450];
	add.f64 	%fd2816, %fd4578, %fd4578;
	ld.global.f64 	%fd4579, [%rd455];
	mul.f64 	%fd2817, %fd2816, %fd4579;
	add.f64 	%fd2818, %fd4578, %fd4579;
	setp.neu.f64 	%p206, %fd2818, 0d0000000000000000;
	div.rn.f64 	%fd2819, %fd2817, %fd2818;
	selp.f64 	%fd4577, %fd2819, 0d0000000000000000, %p206;
	mul.lo.s64 	%rd456, %rd96, %rd72;
	add.s64 	%rd457, %rd38, %rd456;
	ld.global.f64 	%fd4580, [%rd457];
	mul.f64 	%fd2820, %fd4580, %fd4577;
	mul.lo.s64 	%rd458, %rd96, %rd73;
	add.s64 	%rd459, %rd36, %rd458;
	ld.global.f64 	%fd4476, [%rd459];
	ld.global.f64 	%fd4475, [%rd459+8];
	ld.global.f64 	%fd4474, [%rd459+16];
	mul.f64 	%fd4574, %fd2820, %fd966;
	mov.u16 	%rs276, 0;
	bra.uni 	$L__BB1_286;

$L__BB1_276:
	ld.global.f64 	%fd582, [%rd124];
	@%p198 bra 	$L__BB1_285;

	div.rn.f64 	%fd4469, %fd558, %fd560;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r960}, %fd4469;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r961, %temp}, %fd4469;
	}
	setp.gt.s32 	%p208, %r960, 1048575;
	mov.u32 	%r962, -1023;
	@%p208 bra 	$L__BB1_279;

	mul.f64 	%fd4469, %fd4469, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r960}, %fd4469;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r961, %temp}, %fd4469;
	}
	mov.u32 	%r962, -1077;

$L__BB1_279:
	add.s32 	%r869, %r960, -1;
	setp.lt.u32 	%p209, %r869, 2146435071;
	@%p209 bra 	$L__BB1_281;
	bra.uni 	$L__BB1_280;

$L__BB1_281:
	shr.u32 	%r871, %r960, 20;
	add.s32 	%r963, %r962, %r871;
	and.b32  	%r872, %r960, -2146435073;
	or.b32  	%r873, %r872, 1072693248;
	mov.b64 	%fd4470, {%r961, %r873};
	setp.lt.s32 	%p211, %r873, 1073127583;
	@%p211 bra 	$L__BB1_283;

	{
	.reg .b32 %temp;
	mov.b64 	{%r874, %temp}, %fd4470;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r875}, %fd4470;
	}
	add.s32 	%r876, %r875, -1048576;
	mov.b64 	%fd4470, {%r874, %r876};
	add.s32 	%r963, %r963, 1;

$L__BB1_283:
	add.f64 	%fd2824, %fd4470, 0d3FF0000000000000;
	mov.f64 	%fd2825, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2826, %fd2824;
	neg.f64 	%fd2827, %fd2824;
	fma.rn.f64 	%fd2828, %fd2827, %fd2826, %fd2825;
	fma.rn.f64 	%fd2829, %fd2828, %fd2828, %fd2828;
	fma.rn.f64 	%fd2830, %fd2829, %fd2826, %fd2826;
	add.f64 	%fd2831, %fd4470, 0dBFF0000000000000;
	mul.f64 	%fd2832, %fd2831, %fd2830;
	fma.rn.f64 	%fd2833, %fd2831, %fd2830, %fd2832;
	mul.f64 	%fd2834, %fd2833, %fd2833;
	mov.f64 	%fd2835, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2836, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2837, %fd2836, %fd2834, %fd2835;
	mov.f64 	%fd2838, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2839, %fd2837, %fd2834, %fd2838;
	mov.f64 	%fd2840, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2841, %fd2839, %fd2834, %fd2840;
	mov.f64 	%fd2842, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2843, %fd2841, %fd2834, %fd2842;
	mov.f64 	%fd2844, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2845, %fd2843, %fd2834, %fd2844;
	mov.f64 	%fd2846, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2847, %fd2845, %fd2834, %fd2846;
	mov.f64 	%fd2848, 0d3FB5555555555554;
	fma.rn.f64 	%fd2849, %fd2847, %fd2834, %fd2848;
	sub.f64 	%fd2850, %fd2831, %fd2833;
	add.f64 	%fd2851, %fd2850, %fd2850;
	neg.f64 	%fd2852, %fd2833;
	fma.rn.f64 	%fd2853, %fd2852, %fd2831, %fd2851;
	mul.f64 	%fd2854, %fd2830, %fd2853;
	mul.f64 	%fd2855, %fd2834, %fd2849;
	fma.rn.f64 	%fd2856, %fd2855, %fd2833, %fd2854;
	xor.b32  	%r877, %r963, -2147483648;
	mov.u32 	%r878, -2147483648;
	mov.u32 	%r879, 1127219200;
	mov.b64 	%fd2857, {%r877, %r879};
	mov.b64 	%fd2858, {%r878, %r879};
	sub.f64 	%fd2859, %fd2857, %fd2858;
	mov.f64 	%fd2860, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2861, %fd2859, %fd2860, %fd2833;
	neg.f64 	%fd2862, %fd2859;
	fma.rn.f64 	%fd2863, %fd2862, %fd2860, %fd2861;
	sub.f64 	%fd2864, %fd2863, %fd2833;
	sub.f64 	%fd2865, %fd2856, %fd2864;
	mov.f64 	%fd2866, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2867, %fd2859, %fd2866, %fd2865;
	add.f64 	%fd4471, %fd2861, %fd2867;
	bra.uni 	$L__BB1_284;

$L__BB1_280:
	mov.f64 	%fd2822, 0d7FF0000000000000;
	fma.rn.f64 	%fd2823, %fd4469, %fd2822, %fd2822;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r870}, %fd4469;
	}
	mov.b32 	%f8, %r870;
	setp.eq.f32 	%p210, %f8, 0f00000000;
	selp.f64 	%fd4471, 0dFFF0000000000000, %fd2823, %p210;

$L__BB1_284:
	sub.f64 	%fd2868, %fd558, %fd560;
	div.rn.f64 	%fd2869, %fd2868, %fd560;
	mul.f64 	%fd2870, %fd4, %fd2869;
	mul.f64 	%fd2871, %fd2869, %fd2870;
	mul.f64 	%fd4472, %fd2871, %fd4471;

$L__BB1_285:
	selp.f64 	%fd4576, %fd4472, 0d0000000000000000, %p203;
	mov.f64 	%fd4474, 0d0000000000000000;
	mul.f64 	%fd2875, %fd582, %fd965;
	mul.f64 	%fd4575, %fd2875, %fd962;
	mov.u16 	%rs276, 1;
	mov.f64 	%fd4475, %fd4474;
	mov.f64 	%fd4476, %fd4474;

$L__BB1_286:
	mul.lo.s64 	%rd461, %rd413, %rd65;
	cvta.to.global.u64 	%rd462, %rd171;
	add.s64 	%rd125, %rd462, %rd461;
	mul.lo.s64 	%rd463, %rd413, %rd66;
	cvta.to.global.u64 	%rd464, %rd130;
	add.s64 	%rd126, %rd464, %rd463;
	setp.eq.s16 	%p213, %rs276, 0;
	@%p213 bra 	$L__BB1_305;

	setp.eq.s64 	%p214, %rd171, 0;
	@%p214 bra 	$L__BB1_289;

	ld.global.f64 	%fd2876, [%rd125];
	add.f64 	%fd4483, %fd2876, 0d0000000000000000;
	bra.uni 	$L__BB1_291;

$L__BB1_305:
	setp.eq.s64 	%p225, %rd171, 0;
	@%p225 bra 	$L__BB1_307;

	ld.global.f64 	%fd2976, [%rd125];
	add.f64 	%fd4493, %fd2976, 0d0000000000000000;
	bra.uni 	$L__BB1_309;

$L__BB1_289:
	setp.eq.s64 	%p215, %rd130, 0;
	mov.f64 	%fd4483, 0d0000000000000000;
	@%p215 bra 	$L__BB1_291;

	ld.global.f64 	%fd2878, [%rd126];
	add.f64 	%fd4483, %fd2878, 0d0000000000000000;

$L__BB1_291:
	fma.rn.f64 	%fd2880, %fd4483, %fd966, 0d0000000000000000;
	fma.rn.f64 	%fd609, %fd4576, %fd2880, 0d0000000000000000;
	fma.rn.f64 	%fd610, %fd4575, %fd2880, 0d0000000000000000;
	@%p198 bra 	$L__BB1_299;

	sub.f64 	%fd2881, %fd558, %fd560;
	div.rn.f64 	%fd4486, %fd2881, %fd560;
	mul.f64 	%fd4487, %fd4, %fd4486;
	div.rn.f64 	%fd4489, %fd558, %fd560;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r966}, %fd4489;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r967, %temp}, %fd4489;
	}
	setp.gt.s32 	%p217, %r966, 1048575;
	mov.u32 	%r968, -1023;
	mov.f64 	%fd4484, %fd4489;
	@%p217 bra 	$L__BB1_294;

	mul.f64 	%fd4484, %fd4489, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r966}, %fd4484;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r967, %temp}, %fd4484;
	}
	mov.u32 	%r968, -1077;

$L__BB1_294:
	mul.f64 	%fd4488, %fd4486, %fd4487;
	add.s32 	%r882, %r966, -1;
	setp.lt.u32 	%p218, %r882, 2146435071;
	@%p218 bra 	$L__BB1_296;
	bra.uni 	$L__BB1_295;

$L__BB1_296:
	shr.u32 	%r884, %r966, 20;
	add.s32 	%r969, %r968, %r884;
	and.b32  	%r885, %r966, -2146435073;
	or.b32  	%r886, %r885, 1072693248;
	mov.b64 	%fd4485, {%r967, %r886};
	setp.lt.s32 	%p220, %r886, 1073127583;
	@%p220 bra 	$L__BB1_298;

	{
	.reg .b32 %temp;
	mov.b64 	{%r887, %temp}, %fd4485;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r888}, %fd4485;
	}
	add.s32 	%r889, %r888, -1048576;
	mov.b64 	%fd4485, {%r887, %r889};
	add.s32 	%r969, %r969, 1;

$L__BB1_298:
	add.f64 	%fd2884, %fd4485, 0d3FF0000000000000;
	mov.f64 	%fd2885, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2886, %fd2884;
	neg.f64 	%fd2887, %fd2884;
	fma.rn.f64 	%fd2888, %fd2887, %fd2886, %fd2885;
	fma.rn.f64 	%fd2889, %fd2888, %fd2888, %fd2888;
	fma.rn.f64 	%fd2890, %fd2889, %fd2886, %fd2886;
	add.f64 	%fd2891, %fd4485, 0dBFF0000000000000;
	mul.f64 	%fd2892, %fd2891, %fd2890;
	fma.rn.f64 	%fd2893, %fd2891, %fd2890, %fd2892;
	mul.f64 	%fd2894, %fd2893, %fd2893;
	mov.f64 	%fd2895, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2896, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2897, %fd2896, %fd2894, %fd2895;
	mov.f64 	%fd2898, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2899, %fd2897, %fd2894, %fd2898;
	mov.f64 	%fd2900, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2901, %fd2899, %fd2894, %fd2900;
	mov.f64 	%fd2902, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2903, %fd2901, %fd2894, %fd2902;
	mov.f64 	%fd2904, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2905, %fd2903, %fd2894, %fd2904;
	mov.f64 	%fd2906, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2907, %fd2905, %fd2894, %fd2906;
	mov.f64 	%fd2908, 0d3FB5555555555554;
	fma.rn.f64 	%fd2909, %fd2907, %fd2894, %fd2908;
	sub.f64 	%fd2910, %fd2891, %fd2893;
	add.f64 	%fd2911, %fd2910, %fd2910;
	neg.f64 	%fd2912, %fd2893;
	fma.rn.f64 	%fd2913, %fd2912, %fd2891, %fd2911;
	mul.f64 	%fd2914, %fd2890, %fd2913;
	mul.f64 	%fd2915, %fd2894, %fd2909;
	fma.rn.f64 	%fd2916, %fd2915, %fd2893, %fd2914;
	xor.b32  	%r890, %r969, -2147483648;
	mov.u32 	%r891, -2147483648;
	mov.u32 	%r892, 1127219200;
	mov.b64 	%fd2917, {%r890, %r892};
	mov.b64 	%fd2918, {%r891, %r892};
	sub.f64 	%fd2919, %fd2917, %fd2918;
	mov.f64 	%fd2920, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2921, %fd2919, %fd2920, %fd2893;
	neg.f64 	%fd2922, %fd2919;
	fma.rn.f64 	%fd2923, %fd2922, %fd2920, %fd2921;
	sub.f64 	%fd2924, %fd2923, %fd2893;
	sub.f64 	%fd2925, %fd2916, %fd2924;
	mov.f64 	%fd2926, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2927, %fd2919, %fd2926, %fd2925;
	add.f64 	%fd4490, %fd2921, %fd2927;
	bra.uni 	$L__BB1_299;

$L__BB1_295:
	mov.f64 	%fd2882, 0d7FF0000000000000;
	fma.rn.f64 	%fd2883, %fd4484, %fd2882, %fd2882;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r883}, %fd4484;
	}
	mov.b32 	%f9, %r883;
	setp.eq.f32 	%p219, %f9, 0f00000000;
	selp.f64 	%fd4490, 0dFFF0000000000000, %fd2883, %p219;

$L__BB1_299:
	selp.f64 	%fd627, %fd610, 0d0000000000000000, %p203;
	mov.f64 	%fd4499, 0d0000000000000000;
	mov.f64 	%fd4505, %fd4499;
	mov.f64 	%fd4506, %fd4499;
	@%p198 bra 	$L__BB1_301;

	fma.rn.f64 	%fd2930, %fd627, %fd4490, 0d0000000000000000;
	mov.f64 	%fd2931, 0d0000000000000000;
	fma.rn.f64 	%fd2932, %fd627, %fd4488, 0d0000000000000000;
	rcp.rn.f64 	%fd2933, %fd4489;
	fma.rn.f64 	%fd2934, %fd2933, %fd2932, 0d0000000000000000;
	div.rn.f64 	%fd2935, %fd2934, %fd560;
	add.f64 	%fd2936, %fd2935, 0d0000000000000000;
	mul.f64 	%fd2937, %fd4489, %fd2934;
	div.rn.f64 	%fd2938, %fd2937, %fd560;
	sub.f64 	%fd2939, %fd2931, %fd2938;
	fma.rn.f64 	%fd2940, %fd4486, %fd2930, 0d0000000000000000;
	fma.rn.f64 	%fd2941, %fd4487, %fd2930, 0d0000000000000000;
	div.rn.f64 	%fd2942, %fd2941, %fd560;
	add.f64 	%fd2943, %fd2942, 0d0000000000000000;
	mul.f64 	%fd2944, %fd4486, %fd2941;
	div.rn.f64 	%fd2945, %fd2944, %fd560;
	sub.f64 	%fd2946, %fd2939, %fd2945;
	fma.rn.f64 	%fd2947, %fd4, %fd2940, 0d0000000000000000;
	div.rn.f64 	%fd2948, %fd2947, %fd560;
	add.f64 	%fd2949, %fd2943, %fd2948;
	mul.f64 	%fd2950, %fd4486, %fd2947;
	div.rn.f64 	%fd2951, %fd2950, %fd560;
	sub.f64 	%fd2952, %fd2946, %fd2951;
	add.f64 	%fd4505, %fd2936, %fd2949;
	sub.f64 	%fd4506, %fd2952, %fd2949;

$L__BB1_301:
	fma.rn.f64 	%fd2953, %fd609, %fd962, 0d0000000000000000;
	fma.rn.f64 	%fd632, %fd2953, %fd965, 0d0000000000000000;
	setp.eq.s64 	%p223, %rd193, 0;
	@%p223 bra 	$L__BB1_303;

	mul.lo.s64 	%rd467, %rd123, %rd74;
	add.s64 	%rd465, %rd193, %rd467;
	// begin inline asm
	{ atom.add.f64 %fd2954,[%rd465],%fd632; }

	// end inline asm
	mov.f64 	%fd4500, %fd4499;
	mov.f64 	%fd4501, %fd4499;
	mov.f64 	%fd4502, %fd4499;
	mov.f64 	%fd4503, %fd4499;
	mov.f64 	%fd4504, %fd4499;
	bra.uni 	$L__BB1_327;

$L__BB1_303:
	setp.eq.s64 	%p224, %rd164, 0;
	mov.f64 	%fd4500, %fd4499;
	mov.f64 	%fd4501, %fd4499;
	mov.f64 	%fd4502, %fd4499;
	mov.f64 	%fd4503, %fd4499;
	mov.f64 	%fd4504, %fd4499;
	@%p224 bra 	$L__BB1_327;

	add.s64 	%rd468, %rd164, %rd439;
	// begin inline asm
	{ atom.add.f64 %fd2968,[%rd468],%fd632; }

	// end inline asm
	mov.f64 	%fd4500, %fd4499;
	mov.f64 	%fd4501, %fd4499;
	mov.f64 	%fd4502, %fd4499;
	mov.f64 	%fd4503, %fd4499;
	mov.f64 	%fd4504, %fd4499;
	bra.uni 	$L__BB1_327;

$L__BB1_307:
	setp.eq.s64 	%p226, %rd130, 0;
	mov.f64 	%fd4493, 0d0000000000000000;
	@%p226 bra 	$L__BB1_309;

	ld.global.f64 	%fd2978, [%rd126];
	add.f64 	%fd4493, %fd2978, 0d0000000000000000;

$L__BB1_309:
	mul.f64 	%fd2979, %fd4476, %fd4476;
	mov.f64 	%fd2980, 0d3FF0000000000000;
	sub.f64 	%fd636, %fd2980, %fd2979;
	mul.f64 	%fd2981, %fd4476, %fd4475;
	mov.f64 	%fd2982, 0d0000000000000000;
	sub.f64 	%fd637, %fd2982, %fd2981;
	mul.f64 	%fd2983, %fd4476, %fd4474;
	sub.f64 	%fd638, %fd2982, %fd2983;
	mul.f64 	%fd2984, %fd4475, %fd4475;
	sub.f64 	%fd639, %fd2980, %fd2984;
	mul.f64 	%fd2985, %fd4475, %fd4474;
	sub.f64 	%fd640, %fd2982, %fd2985;
	mul.f64 	%fd2986, %fd4474, %fd4474;
	sub.f64 	%fd641, %fd2980, %fd2986;
	mul.f64 	%fd2987, %fd522, %fd526;
	mul.f64 	%fd2988, %fd519, %fd525;
	mul.f64 	%fd2989, %fd516, %fd559;
	sub.f64 	%fd2990, %fd513, %fd2989;
	sub.f64 	%fd2991, %fd2990, %fd2988;
	sub.f64 	%fd2992, %fd2991, %fd2987;
	mul.f64 	%fd2993, %fd534, %fd526;
	mul.f64 	%fd2994, %fd528, %fd525;
	mul.f64 	%fd2995, %fd529, %fd559;
	sub.f64 	%fd2996, %fd541, %fd2995;
	sub.f64 	%fd2997, %fd2996, %fd2994;
	sub.f64 	%fd2998, %fd2997, %fd2993;
	sub.f64 	%fd642, %fd2998, %fd2992;
	mul.f64 	%fd2999, %fd523, %fd526;
	mul.f64 	%fd3000, %fd520, %fd525;
	mul.f64 	%fd3001, %fd517, %fd559;
	sub.f64 	%fd3002, %fd514, %fd3001;
	sub.f64 	%fd3003, %fd3002, %fd3000;
	sub.f64 	%fd3004, %fd3003, %fd2999;
	mul.f64 	%fd3005, %fd535, %fd526;
	mul.f64 	%fd3006, %fd530, %fd525;
	mul.f64 	%fd3007, %fd531, %fd559;
	sub.f64 	%fd3008, %fd543, %fd3007;
	sub.f64 	%fd3009, %fd3008, %fd3006;
	sub.f64 	%fd3010, %fd3009, %fd3005;
	sub.f64 	%fd643, %fd3010, %fd3004;
	mul.f64 	%fd3011, %fd637, %fd643;
	mul.f64 	%fd3012, %fd639, %fd643;
	mul.f64 	%fd3013, %fd640, %fd643;
	fma.rn.f64 	%fd3014, %fd636, %fd642, %fd3011;
	fma.rn.f64 	%fd3015, %fd637, %fd642, %fd3012;
	fma.rn.f64 	%fd3016, %fd638, %fd642, %fd3013;
	mul.f64 	%fd3017, %fd524, %fd526;
	mul.f64 	%fd3018, %fd521, %fd525;
	mul.f64 	%fd3019, %fd518, %fd559;
	sub.f64 	%fd3020, %fd515, %fd3019;
	sub.f64 	%fd3021, %fd3020, %fd3018;
	sub.f64 	%fd3022, %fd3021, %fd3017;
	mul.f64 	%fd3023, %fd536, %fd526;
	mul.f64 	%fd3024, %fd532, %fd525;
	mul.f64 	%fd3025, %fd533, %fd559;
	sub.f64 	%fd3026, %fd545, %fd3025;
	sub.f64 	%fd3027, %fd3026, %fd3024;
	sub.f64 	%fd3028, %fd3027, %fd3023;
	sub.f64 	%fd644, %fd3028, %fd3022;
	fma.rn.f64 	%fd3029, %fd638, %fd644, %fd3014;
	fma.rn.f64 	%fd3030, %fd640, %fd644, %fd3015;
	fma.rn.f64 	%fd3031, %fd641, %fd644, %fd3016;
	div.rn.f64 	%fd645, %fd3029, %fd963;
	div.rn.f64 	%fd646, %fd3030, %fd963;
	div.rn.f64 	%fd647, %fd3031, %fd963;
	mul.f64 	%fd3032, %fd646, %fd646;
	fma.rn.f64 	%fd3033, %fd645, %fd645, %fd3032;
	fma.rn.f64 	%fd3034, %fd647, %fd647, %fd3033;
	sqrt.rn.f64 	%fd648, %fd3034;
	setp.ge.f64 	%p227, %fd648, %fd967;
	mul.f64 	%fd4494, %fd648, %fd963;
	@%p227 bra 	$L__BB1_311;

	mul.f64 	%fd4332, %fd648, %fd963;
	mul.f64 	%fd3035, %fd4332, %fd4332;
	sub.f64 	%fd3037, %fd2982, %fd4332;
	div.rn.f64 	%fd3038, %fd3037, 0d4008000000000000;
	add.f64 	%fd3039, %fd5, %fd3038;
	mul.f64 	%fd3040, %fd3035, %fd3039;
	div.rn.f64 	%fd3041, %fd3040, %fd6;
	add.f64 	%fd4494, %fd7, %fd3041;

$L__BB1_311:
	add.f64 	%fd3042, %fd4493, 0d0000000000000000;
	fma.rn.f64 	%fd652, %fd3042, %fd4494, 0d0000000000000000;
	fma.rn.f64 	%fd4495, %fd4574, %fd3042, 0d0000000000000000;
	@%p227 bra 	$L__BB1_313;

	mul.f64 	%fd4331, %fd648, %fd963;
	mul.f64 	%fd3043, %fd4331, %fd4331;
	mov.f64 	%fd3044, 0d0000000000000000;
	sub.f64 	%fd3045, %fd3044, %fd4331;
	div.rn.f64 	%fd3046, %fd3045, 0d4008000000000000;
	add.f64 	%fd3047, %fd5, %fd3046;
	div.rn.f64 	%fd3048, %fd4495, %fd6;
	add.f64 	%fd3049, %fd3048, 0d0000000000000000;
	fma.rn.f64 	%fd3050, %fd3049, %fd3047, 0d0000000000000000;
	fma.rn.f64 	%fd3051, %fd3049, %fd3043, 0d0000000000000000;
	div.rn.f64 	%fd3052, %fd3051, 0d4008000000000000;
	add.f64 	%fd3053, %fd3052, 0d0000000000000000;
	sub.f64 	%fd3054, %fd3044, %fd3053;
	fma.rn.f64 	%fd3055, %fd4331, %fd3050, %fd3054;
	fma.rn.f64 	%fd4495, %fd4331, %fd3050, %fd3055;

$L__BB1_313:
	fma.rn.f64 	%fd656, %fd4495, %fd963, 0d0000000000000000;
	mov.f64 	%fd3058, 0d0000000000000000;
	setp.leu.f64 	%p229, %fd648, 0d0000000000000000;
	mov.f64 	%fd4496, %fd3058;
	mov.f64 	%fd4497, %fd3058;
	mov.f64 	%fd4498, %fd3058;
	@%p229 bra 	$L__BB1_315;

	div.rn.f64 	%fd3059, %fd645, %fd648;
	div.rn.f64 	%fd3060, %fd646, %fd648;
	div.rn.f64 	%fd3061, %fd647, %fd648;
	fma.rn.f64 	%fd4496, %fd3059, %fd656, 0d0000000000000000;
	fma.rn.f64 	%fd4497, %fd3060, %fd656, 0d0000000000000000;
	fma.rn.f64 	%fd4498, %fd3061, %fd656, 0d0000000000000000;

$L__BB1_315:
	mov.f64 	%fd4327, 0d3FF0000000000000;
	mul.f64 	%fd4326, %fd4474, %fd4474;
	sub.f64 	%fd4325, %fd4327, %fd4326;
	mul.f64 	%fd4324, %fd4476, %fd4474;
	sub.f64 	%fd4323, %fd2982, %fd4324;
	mul.f64 	%fd4322, %fd4476, %fd4476;
	sub.f64 	%fd4321, %fd4327, %fd4322;
	mul.f64 	%fd4320, %fd4475, %fd4474;
	sub.f64 	%fd4319, %fd2982, %fd4320;
	mul.f64 	%fd4318, %fd4475, %fd4475;
	sub.f64 	%fd4317, %fd4327, %fd4318;
	mul.f64 	%fd4316, %fd4476, %fd4475;
	sub.f64 	%fd4315, %fd2982, %fd4316;
	div.rn.f64 	%fd3062, %fd4496, %fd963;
	add.f64 	%fd3063, %fd3062, 0d0000000000000000;
	div.rn.f64 	%fd3065, %fd4497, %fd963;
	add.f64 	%fd3066, %fd3065, 0d0000000000000000;
	div.rn.f64 	%fd3067, %fd4498, %fd963;
	add.f64 	%fd3068, %fd3067, 0d0000000000000000;
	fma.rn.f64 	%fd3069, %fd3063, %fd642, 0d0000000000000000;
	fma.rn.f64 	%fd3070, %fd3063, %fd643, 0d0000000000000000;
	fma.rn.f64 	%fd3071, %fd3063, %fd644, 0d0000000000000000;
	fma.rn.f64 	%fd3072, %fd3066, %fd642, 0d0000000000000000;
	fma.rn.f64 	%fd3073, %fd3066, %fd643, 0d0000000000000000;
	fma.rn.f64 	%fd3074, %fd3066, %fd644, 0d0000000000000000;
	fma.rn.f64 	%fd3075, %fd3068, %fd642, 0d0000000000000000;
	fma.rn.f64 	%fd3076, %fd3068, %fd643, 0d0000000000000000;
	fma.rn.f64 	%fd3077, %fd3068, %fd644, 0d0000000000000000;
	mul.f64 	%fd3078, %fd4315, %fd3066;
	mul.f64 	%fd3079, %fd4317, %fd3066;
	mul.f64 	%fd3080, %fd4319, %fd3066;
	fma.rn.f64 	%fd3081, %fd4321, %fd3063, %fd3078;
	fma.rn.f64 	%fd3082, %fd4315, %fd3063, %fd3079;
	fma.rn.f64 	%fd3083, %fd4323, %fd3063, %fd3080;
	fma.rn.f64 	%fd3084, %fd4319, %fd3068, %fd3082;
	fma.rn.f64 	%fd3085, %fd4325, %fd3068, %fd3083;
	add.f64 	%fd4503, %fd3084, 0d0000000000000000;
	fma.rn.f64 	%fd3086, %fd4323, %fd3068, %fd3081;
	add.f64 	%fd4502, %fd3086, 0d0000000000000000;
	add.f64 	%fd4504, %fd3085, 0d0000000000000000;
	sub.f64 	%fd3087, %fd3058, %fd3069;
	sub.f64 	%fd3088, %fd3058, %fd3072;
	sub.f64 	%fd3089, %fd3058, %fd3075;
	sub.f64 	%fd3090, %fd3058, %fd3070;
	sub.f64 	%fd3091, %fd3058, %fd3073;
	sub.f64 	%fd3092, %fd3058, %fd3076;
	sub.f64 	%fd3093, %fd3058, %fd3071;
	sub.f64 	%fd3094, %fd3058, %fd3074;
	sub.f64 	%fd3095, %fd3058, %fd3077;
	mul.f64 	%fd3096, %fd3088, %fd4475;
	mul.f64 	%fd3097, %fd3091, %fd4475;
	mul.f64 	%fd3098, %fd3094, %fd4475;
	fma.rn.f64 	%fd3099, %fd3087, %fd4476, %fd3096;
	fma.rn.f64 	%fd3100, %fd3090, %fd4476, %fd3097;
	fma.rn.f64 	%fd3101, %fd3093, %fd4476, %fd3098;
	fma.rn.f64 	%fd3102, %fd3092, %fd4474, %fd3100;
	fma.rn.f64 	%fd3103, %fd3095, %fd4474, %fd3101;
	add.f64 	%fd3104, %fd3102, 0d0000000000000000;
	fma.rn.f64 	%fd3105, %fd3089, %fd4474, %fd3099;
	add.f64 	%fd3106, %fd3105, 0d0000000000000000;
	add.f64 	%fd3107, %fd3103, 0d0000000000000000;
	mul.f64 	%fd3108, %fd3090, %fd4475;
	mul.f64 	%fd3109, %fd3092, %fd4475;
	fma.rn.f64 	%fd3110, %fd3087, %fd4476, %fd3108;
	fma.rn.f64 	%fd3111, %fd3088, %fd4476, %fd3097;
	fma.rn.f64 	%fd3112, %fd3089, %fd4476, %fd3109;
	fma.rn.f64 	%fd3113, %fd3093, %fd4474, %fd3110;
	fma.rn.f64 	%fd3114, %fd3094, %fd4474, %fd3111;
	fma.rn.f64 	%fd3115, %fd3095, %fd4474, %fd3112;
	add.f64 	%fd666, %fd3106, %fd3113;
	add.f64 	%fd667, %fd3104, %fd3114;
	add.f64 	%fd668, %fd3107, %fd3115;
	setp.eq.s64 	%p230, %rd177, 0;
	@%p230 bra 	$L__BB1_317;

	mul.lo.s64 	%rd474, %rd96, %rd76;
	add.s64 	%rd471, %rd177, %rd474;
	// begin inline asm
	{ atom.add.f64 %fd3116,[%rd471],%fd666; }

	// end inline asm
	add.s64 	%rd472, %rd471, 8;
	// begin inline asm
	{ atom.add.f64 %fd3118,[%rd472],%fd667; }

	// end inline asm
	add.s64 	%rd473, %rd471, 16;
	// begin inline asm
	{ atom.add.f64 %fd3120,[%rd473],%fd668; }

	// end inline asm
	bra.uni 	$L__BB1_319;

$L__BB1_317:
	setp.eq.s64 	%p231, %rd136, 0;
	@%p231 bra 	$L__BB1_319;

	mul.lo.s64 	%rd478, %rd96, %rd73;
	add.s64 	%rd475, %rd136, %rd478;
	// begin inline asm
	{ atom.add.f64 %fd3122,[%rd475],%fd666; }

	// end inline asm
	add.s64 	%rd476, %rd475, 8;
	// begin inline asm
	{ atom.add.f64 %fd3124,[%rd476],%fd667; }

	// end inline asm
	add.s64 	%rd477, %rd475, 16;
	// begin inline asm
	{ atom.add.f64 %fd3126,[%rd477],%fd668; }

	// end inline asm

$L__BB1_319:
	setp.eq.s64 	%p232, %rd173, 0;
	fma.rn.f64 	%fd3128, %fd652, %fd966, 0d0000000000000000;
	fma.rn.f64 	%fd669, %fd4580, %fd3128, 0d0000000000000000;
	fma.rn.f64 	%fd670, %fd4577, %fd3128, 0d0000000000000000;
	@%p232 bra 	$L__BB1_321;

	mul.lo.s64 	%rd480, %rd96, %rd78;
	add.s64 	%rd479, %rd173, %rd480;
	// begin inline asm
	{ atom.add.f64 %fd3129,[%rd479],%fd670; }

	// end inline asm
	bra.uni 	$L__BB1_323;

$L__BB1_321:
	setp.eq.s64 	%p233, %rd132, 0;
	@%p233 bra 	$L__BB1_323;

	mul.lo.s64 	%rd482, %rd96, %rd72;
	add.s64 	%rd481, %rd132, %rd482;
	// begin inline asm
	{ atom.add.f64 %fd3131,[%rd481],%fd670; }

	// end inline asm

$L__BB1_323:
	mov.f64 	%fd4505, 0d0000000000000000;
	sub.f64 	%fd4500, %fd4505, %fd4503;
	sub.f64 	%fd4499, %fd4505, %fd4502;
	sub.f64 	%fd4501, %fd4505, %fd4504;
	setp.eq.s64 	%p234, %rd185, 0;
	add.f64 	%fd3134, %fd4578, %fd4578;
	mul.f64 	%fd3135, %fd3134, %fd4579;
	add.f64 	%fd3136, %fd4578, %fd4579;
	setp.neu.f64 	%p235, %fd3136, 0d0000000000000000;
	div.rn.f64 	%fd3137, %fd3135, %fd3136;
	selp.f64 	%fd3138, %fd669, 0d0000000000000000, %p235;
	div.rn.f64 	%fd3139, %fd3138, %fd3136;
	add.f64 	%fd3140, %fd3139, 0d0000000000000000;
	mul.f64 	%fd3141, %fd3137, %fd3138;
	div.rn.f64 	%fd3142, %fd3141, %fd3136;
	sub.f64 	%fd3143, %fd4505, %fd3142;
	add.f64 	%fd3144, %fd3143, 0d0000000000000000;
	fma.rn.f64 	%fd3145, %fd4579, %fd3140, 0d0000000000000000;
	fma.rn.f64 	%fd674, %fd3134, %fd3140, %fd3144;
	fma.rn.f64 	%fd675, %fd3145, 0d4000000000000000, %fd3144;
	@%p234 bra 	$L__BB1_325;

	cvt.s64.s32 	%rd485, %r980;
	mul.lo.s64 	%rd486, %rd485, %rd79;
	add.s64 	%rd483, %rd185, %rd486;
	// begin inline asm
	{ atom.add.f64 %fd3146,[%rd483],%fd674; }

	// end inline asm
	cvt.s64.s32 	%rd487, %r979;
	mul.lo.s64 	%rd488, %rd487, %rd79;
	add.s64 	%rd484, %rd185, %rd488;
	// begin inline asm
	{ atom.add.f64 %fd3148,[%rd484],%fd675; }

	// end inline asm
	mov.f64 	%fd4506, %fd4505;
	bra.uni 	$L__BB1_327;

$L__BB1_325:
	setp.eq.s64 	%p236, %rd156, 0;
	mov.f64 	%fd4506, %fd4505;
	@%p236 bra 	$L__BB1_327;

	cvt.s64.s32 	%rd491, %r980;
	mul.lo.s64 	%rd492, %rd491, %rd70;
	add.s64 	%rd489, %rd156, %rd492;
	// begin inline asm
	{ atom.add.f64 %fd3154,[%rd489],%fd674; }

	// end inline asm
	cvt.s64.s32 	%rd493, %r979;
	mul.lo.s64 	%rd494, %rd493, %rd70;
	add.s64 	%rd490, %rd156, %rd494;
	// begin inline asm
	{ atom.add.f64 %fd3156,[%rd490],%fd675; }

	// end inline asm
	mov.f64 	%fd4506, %fd4505;

$L__BB1_327:
	mov.pred 	%p237, 0;
	xor.pred  	%p238, %p4, %p237;
	not.pred 	%p239, %p238;
	@%p239 bra 	$L__BB1_329;

	add.u64 	%rd495, %SP, 0;
	add.u64 	%rd496, %SPL, 0;
	mov.u64 	%rd497, $str$3;
	cvta.global.u64 	%rd498, %rd497;
	st.local.u64 	[%rd496], %rd498;
	mov.u64 	%rd499, $str$6;
	cvta.global.u64 	%rd500, %rd499;
	{ // callseq 256, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd500;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd495;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r893, [retval0+0];
	} // callseq 256

$L__BB1_329:
	fma.rn.f64 	%fd684, %fd3, %fd573, 0d0000000000000000;
	fma.rn.f64 	%fd685, %fd3, %fd561, 0d0000000000000000;
	@%p198 bra 	$L__BB1_337;

	sub.f64 	%fd3161, %fd558, %fd560;
	div.rn.f64 	%fd4509, %fd3161, %fd560;
	mul.f64 	%fd4510, %fd4, %fd4509;
	div.rn.f64 	%fd4512, %fd558, %fd560;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r970}, %fd4512;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r971, %temp}, %fd4512;
	}
	setp.gt.s32 	%p241, %r970, 1048575;
	mov.u32 	%r972, -1023;
	mov.f64 	%fd4507, %fd4512;
	@%p241 bra 	$L__BB1_332;

	mul.f64 	%fd4507, %fd4512, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r970}, %fd4507;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r971, %temp}, %fd4507;
	}
	mov.u32 	%r972, -1077;

$L__BB1_332:
	mul.f64 	%fd4511, %fd4509, %fd4510;
	add.s32 	%r896, %r970, -1;
	setp.lt.u32 	%p242, %r896, 2146435071;
	@%p242 bra 	$L__BB1_334;
	bra.uni 	$L__BB1_333;

$L__BB1_334:
	shr.u32 	%r898, %r970, 20;
	add.s32 	%r973, %r972, %r898;
	and.b32  	%r899, %r970, -2146435073;
	or.b32  	%r900, %r899, 1072693248;
	mov.b64 	%fd4508, {%r971, %r900};
	setp.lt.s32 	%p244, %r900, 1073127583;
	@%p244 bra 	$L__BB1_336;

	{
	.reg .b32 %temp;
	mov.b64 	{%r901, %temp}, %fd4508;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r902}, %fd4508;
	}
	add.s32 	%r903, %r902, -1048576;
	mov.b64 	%fd4508, {%r901, %r903};
	add.s32 	%r973, %r973, 1;

$L__BB1_336:
	add.f64 	%fd3164, %fd4508, 0d3FF0000000000000;
	mov.f64 	%fd3165, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd3166, %fd3164;
	neg.f64 	%fd3167, %fd3164;
	fma.rn.f64 	%fd3168, %fd3167, %fd3166, %fd3165;
	fma.rn.f64 	%fd3169, %fd3168, %fd3168, %fd3168;
	fma.rn.f64 	%fd3170, %fd3169, %fd3166, %fd3166;
	add.f64 	%fd3171, %fd4508, 0dBFF0000000000000;
	mul.f64 	%fd3172, %fd3171, %fd3170;
	fma.rn.f64 	%fd3173, %fd3171, %fd3170, %fd3172;
	mul.f64 	%fd3174, %fd3173, %fd3173;
	mov.f64 	%fd3175, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd3176, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd3177, %fd3176, %fd3174, %fd3175;
	mov.f64 	%fd3178, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd3179, %fd3177, %fd3174, %fd3178;
	mov.f64 	%fd3180, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd3181, %fd3179, %fd3174, %fd3180;
	mov.f64 	%fd3182, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd3183, %fd3181, %fd3174, %fd3182;
	mov.f64 	%fd3184, 0d3F624924923BE72D;
	fma.rn.f64 	%fd3185, %fd3183, %fd3174, %fd3184;
	mov.f64 	%fd3186, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd3187, %fd3185, %fd3174, %fd3186;
	mov.f64 	%fd3188, 0d3FB5555555555554;
	fma.rn.f64 	%fd3189, %fd3187, %fd3174, %fd3188;
	sub.f64 	%fd3190, %fd3171, %fd3173;
	add.f64 	%fd3191, %fd3190, %fd3190;
	neg.f64 	%fd3192, %fd3173;
	fma.rn.f64 	%fd3193, %fd3192, %fd3171, %fd3191;
	mul.f64 	%fd3194, %fd3170, %fd3193;
	mul.f64 	%fd3195, %fd3174, %fd3189;
	fma.rn.f64 	%fd3196, %fd3195, %fd3173, %fd3194;
	xor.b32  	%r904, %r973, -2147483648;
	mov.u32 	%r905, -2147483648;
	mov.u32 	%r906, 1127219200;
	mov.b64 	%fd3197, {%r904, %r906};
	mov.b64 	%fd3198, {%r905, %r906};
	sub.f64 	%fd3199, %fd3197, %fd3198;
	mov.f64 	%fd3200, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd3201, %fd3199, %fd3200, %fd3173;
	neg.f64 	%fd3202, %fd3199;
	fma.rn.f64 	%fd3203, %fd3202, %fd3200, %fd3201;
	sub.f64 	%fd3204, %fd3203, %fd3173;
	sub.f64 	%fd3205, %fd3196, %fd3204;
	mov.f64 	%fd3206, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd3207, %fd3199, %fd3206, %fd3205;
	add.f64 	%fd4513, %fd3201, %fd3207;
	bra.uni 	$L__BB1_337;

$L__BB1_333:
	mov.f64 	%fd3162, 0d7FF0000000000000;
	fma.rn.f64 	%fd3163, %fd4507, %fd3162, %fd3162;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r897}, %fd4507;
	}
	mov.b32 	%f10, %r897;
	setp.eq.f32 	%p243, %f10, 0f00000000;
	selp.f64 	%fd4513, 0dFFF0000000000000, %fd3163, %p243;

$L__BB1_337:
	selp.f64 	%fd702, %fd685, 0d0000000000000000, %p203;
	@%p198 bra 	$L__BB1_339;

	fma.rn.f64 	%fd3208, %fd702, %fd4513, 0d0000000000000000;
	fma.rn.f64 	%fd3209, %fd702, %fd4511, 0d0000000000000000;
	rcp.rn.f64 	%fd3210, %fd4512;
	fma.rn.f64 	%fd3211, %fd3210, %fd3209, 0d0000000000000000;
	div.rn.f64 	%fd3212, %fd3211, %fd560;
	add.f64 	%fd3213, %fd4505, %fd3212;
	mul.f64 	%fd3214, %fd4512, %fd3211;
	div.rn.f64 	%fd3215, %fd3214, %fd560;
	sub.f64 	%fd3216, %fd4506, %fd3215;
	fma.rn.f64 	%fd3217, %fd4509, %fd3208, 0d0000000000000000;
	fma.rn.f64 	%fd3218, %fd4510, %fd3208, 0d0000000000000000;
	div.rn.f64 	%fd3219, %fd3218, %fd560;
	add.f64 	%fd3220, %fd3219, 0d0000000000000000;
	mul.f64 	%fd3221, %fd4509, %fd3218;
	div.rn.f64 	%fd3222, %fd3221, %fd560;
	sub.f64 	%fd3223, %fd3216, %fd3222;
	fma.rn.f64 	%fd3224, %fd4, %fd3217, 0d0000000000000000;
	div.rn.f64 	%fd3225, %fd3224, %fd560;
	add.f64 	%fd3226, %fd3220, %fd3225;
	mul.f64 	%fd3227, %fd4509, %fd3224;
	div.rn.f64 	%fd3228, %fd3227, %fd560;
	sub.f64 	%fd3229, %fd3223, %fd3228;
	add.f64 	%fd4505, %fd3213, %fd3226;
	sub.f64 	%fd4506, %fd3229, %fd3226;

$L__BB1_339:
	fma.rn.f64 	%fd3230, %fd684, %fd962, 0d0000000000000000;
	fma.rn.f64 	%fd707, %fd3230, %fd965, 0d0000000000000000;
	setp.eq.s64 	%p247, %rd193, 0;
	@%p247 bra 	$L__BB1_341;

	mul.lo.s64 	%rd503, %rd123, %rd74;
	add.s64 	%rd501, %rd193, %rd503;
	// begin inline asm
	{ atom.add.f64 %fd3231,[%rd501],%fd707; }

	// end inline asm
	bra.uni 	$L__BB1_343;

$L__BB1_341:
	setp.eq.s64 	%p248, %rd164, 0;
	@%p248 bra 	$L__BB1_343;

	add.s64 	%rd504, %rd164, %rd439;
	// begin inline asm
	{ atom.add.f64 %fd3233,[%rd504],%fd707; }

	// end inline asm

$L__BB1_343:
	mov.f64 	%fd4330, 0d3FF0000000000000;
	sub.f64 	%fd4329, %fd4330, %fd525;
	sub.f64 	%fd4328, %fd4329, %fd526;
	sub.f64 	%fd4299, %fd532, %fd533;
	sub.f64 	%fd4298, %fd530, %fd531;
	sub.f64 	%fd4297, %fd528, %fd529;
	add.f64 	%fd3235, %fd4506, 0d0000000000000000;
	mov.f64 	%fd3236, 0d0000000000000000;
	sub.f64 	%fd3237, %fd3236, %fd4499;
	fma.rn.f64 	%fd708, %fd526, %fd3237, 0d0000000000000000;
	sub.f64 	%fd3238, %fd3236, %fd4500;
	fma.rn.f64 	%fd709, %fd526, %fd3238, 0d0000000000000000;
	sub.f64 	%fd3239, %fd3236, %fd4501;
	fma.rn.f64 	%fd710, %fd526, %fd3239, 0d0000000000000000;
	mul.f64 	%fd3240, %fd523, %fd3238;
	fma.rn.f64 	%fd3241, %fd522, %fd3237, %fd3240;
	fma.rn.f64 	%fd3242, %fd524, %fd3239, %fd3241;
	add.f64 	%fd711, %fd4499, 0d0000000000000000;
	sub.f64 	%fd3243, %fd3236, %fd711;
	add.f64 	%fd712, %fd4500, 0d0000000000000000;
	sub.f64 	%fd3244, %fd3236, %fd712;
	add.f64 	%fd713, %fd4501, 0d0000000000000000;
	sub.f64 	%fd3245, %fd3236, %fd713;
	fma.rn.f64 	%fd714, %fd525, %fd3243, 0d0000000000000000;
	fma.rn.f64 	%fd715, %fd525, %fd3244, 0d0000000000000000;
	fma.rn.f64 	%fd716, %fd525, %fd3245, 0d0000000000000000;
	fma.rn.f64 	%fd3246, %fd2, %fd3235, 0d0000000000000000;
	mul.f64 	%fd3247, %fd520, %fd3244;
	fma.rn.f64 	%fd3248, %fd519, %fd3243, %fd3247;
	fma.rn.f64 	%fd3249, %fd521, %fd3245, %fd3248;
	fma.rn.f64 	%fd717, %fd4328, %fd3243, 0d0000000000000000;
	fma.rn.f64 	%fd718, %fd4328, %fd3244, 0d0000000000000000;
	fma.rn.f64 	%fd719, %fd4328, %fd3245, 0d0000000000000000;
	add.f64 	%fd3250, %fd3249, 0d0000000000000000;
	mul.f64 	%fd3251, %fd517, %fd3244;
	fma.rn.f64 	%fd3252, %fd516, %fd3243, %fd3251;
	fma.rn.f64 	%fd3253, %fd518, %fd3245, %fd3252;
	add.f64 	%fd3254, %fd3253, 0d0000000000000000;
	sub.f64 	%fd3255, %fd3250, %fd3254;
	add.f64 	%fd3256, %fd3242, 0d0000000000000000;
	sub.f64 	%fd3257, %fd3236, %fd4502;
	fma.rn.f64 	%fd4524, %fd526, %fd3257, 0d0000000000000000;
	sub.f64 	%fd3258, %fd3236, %fd4503;
	fma.rn.f64 	%fd4523, %fd526, %fd3258, 0d0000000000000000;
	sub.f64 	%fd3259, %fd3236, %fd4504;
	fma.rn.f64 	%fd4522, %fd526, %fd3259, 0d0000000000000000;
	sub.f64 	%fd3260, %fd3256, %fd3254;
	mul.f64 	%fd3261, %fd535, %fd3258;
	fma.rn.f64 	%fd3262, %fd534, %fd3257, %fd3261;
	fma.rn.f64 	%fd3263, %fd536, %fd3259, %fd3262;
	add.f64 	%fd723, %fd4502, 0d0000000000000000;
	sub.f64 	%fd3264, %fd3236, %fd723;
	add.f64 	%fd724, %fd4503, 0d0000000000000000;
	sub.f64 	%fd3265, %fd3236, %fd724;
	add.f64 	%fd725, %fd4504, 0d0000000000000000;
	sub.f64 	%fd3266, %fd3236, %fd725;
	fma.rn.f64 	%fd4521, %fd525, %fd3264, 0d0000000000000000;
	fma.rn.f64 	%fd4520, %fd525, %fd3265, 0d0000000000000000;
	fma.rn.f64 	%fd4519, %fd525, %fd3266, 0d0000000000000000;
	mul.f64 	%fd3267, %fd530, %fd3265;
	fma.rn.f64 	%fd3268, %fd528, %fd3264, %fd3267;
	fma.rn.f64 	%fd3269, %fd532, %fd3266, %fd3268;
	fma.rn.f64 	%fd4518, %fd4328, %fd3264, 0d0000000000000000;
	fma.rn.f64 	%fd4517, %fd4328, %fd3265, 0d0000000000000000;
	fma.rn.f64 	%fd4516, %fd4328, %fd3266, 0d0000000000000000;
	add.f64 	%fd3270, %fd3260, %fd3263;
	add.f64 	%fd3271, %fd3255, %fd3269;
	mul.f64 	%fd3272, %fd531, %fd3265;
	fma.rn.f64 	%fd3273, %fd529, %fd3264, %fd3272;
	fma.rn.f64 	%fd3274, %fd533, %fd3266, %fd3273;
	add.f64 	%fd3275, %fd3274, 0d0000000000000000;
	sub.f64 	%fd732, %fd3270, %fd3275;
	sub.f64 	%fd733, %fd3271, %fd3275;
	add.f64 	%fd734, %fd4505, 0d0000000000000000;
	sub.f64 	%fd3276, %fd3236, %fd4505;
	fma.rn.f64 	%fd3277, %fd527, %fd3276, %fd3246;
	fma.rn.f64 	%fd735, %fd527, %fd3276, %fd3277;
	mul.f64 	%fd3290, %fd4297, %fd4297;
	fma.rn.f64 	%fd3291, %fd4298, %fd4298, %fd3290;
	fma.rn.f64 	%fd748, %fd4299, %fd4299, %fd3291;
	mul.f64 	%fd3298, %fd542, %fd4297;
	fma.rn.f64 	%fd3299, %fd544, %fd4298, %fd3298;
	fma.rn.f64 	%fd752, %fd546, %fd4299, %fd3299;
	div.rn.f64 	%fd753, %fd2573, %fd748;
	mul.f64 	%fd754, %fd753, %fd753;
	mul.f64 	%fd3303, %fd748, %fd754;
	sub.f64 	%fd755, %fd2576, %fd3303;
	mul.f64 	%fd3304, %fd752, %fd753;
	sub.f64 	%fd3305, %fd2582, %fd3304;
	div.rn.f64 	%fd756, %fd3305, %fd755;
	mul.f64 	%fd757, %fd748, %fd753;
	mul.f64 	%fd3306, %fd757, %fd756;
	sub.f64 	%fd3307, %fd752, %fd3306;
	div.rn.f64 	%fd758, %fd3307, %fd748;
	setp.gt.f64 	%p249, %fd758, 0d0000000000000000;
	setp.lt.f64 	%p250, %fd758, 0d3FF0000000000000;
	setp.ge.f64 	%p251, %fd756, 0d0000000000000000;
	and.pred  	%p252, %p249, %p250;
	and.pred  	%p5, %p251, %p252;
	mov.u32 	%r974, 3;
	@%p5 bra 	$L__BB1_349;

	sub.f64 	%fd3308, %fd534, %fd528;
	sub.f64 	%fd3309, %fd535, %fd530;
	mul.f64 	%fd3310, %fd3309, %fd539;
	sub.f64 	%fd3311, %fd536, %fd532;
	mul.f64 	%fd3312, %fd3311, %fd538;
	sub.f64 	%fd3313, %fd3310, %fd3312;
	mul.f64 	%fd3314, %fd3311, %fd537;
	mul.f64 	%fd3315, %fd3308, %fd539;
	sub.f64 	%fd3316, %fd3314, %fd3315;
	mul.f64 	%fd3317, %fd3308, %fd538;
	mul.f64 	%fd3318, %fd3309, %fd537;
	sub.f64 	%fd3319, %fd3317, %fd3318;
	mul.f64 	%fd3320, %fd3308, %fd3308;
	fma.rn.f64 	%fd3321, %fd3309, %fd3309, %fd3320;
	fma.rn.f64 	%fd3322, %fd3311, %fd3311, %fd3321;
	mul.f64 	%fd3323, %fd3309, %fd3316;
	fma.rn.f64 	%fd3324, %fd3308, %fd3313, %fd3323;
	fma.rn.f64 	%fd3325, %fd3311, %fd3319, %fd3324;
	mul.f64 	%fd3326, %fd3316, %fd3316;
	fma.rn.f64 	%fd3327, %fd3313, %fd3313, %fd3326;
	fma.rn.f64 	%fd3328, %fd3319, %fd3319, %fd3327;
	sub.f64 	%fd3329, %fd541, %fd528;
	mul.f64 	%fd3330, %fd3329, %fd3308;
	sub.f64 	%fd3331, %fd543, %fd530;
	fma.rn.f64 	%fd3332, %fd3331, %fd3309, %fd3330;
	sub.f64 	%fd3333, %fd545, %fd532;
	fma.rn.f64 	%fd3334, %fd3333, %fd3311, %fd3332;
	mul.f64 	%fd3335, %fd3331, %fd3316;
	fma.rn.f64 	%fd3336, %fd3329, %fd3313, %fd3335;
	fma.rn.f64 	%fd3337, %fd3333, %fd3319, %fd3336;
	div.rn.f64 	%fd3338, %fd3325, %fd3322;
	mul.f64 	%fd3339, %fd3338, %fd3338;
	mul.f64 	%fd3340, %fd3322, %fd3339;
	sub.f64 	%fd3341, %fd3328, %fd3340;
	mul.f64 	%fd3342, %fd3334, %fd3338;
	sub.f64 	%fd3343, %fd3337, %fd3342;
	div.rn.f64 	%fd3344, %fd3343, %fd3341;
	mul.f64 	%fd3345, %fd3322, %fd3338;
	mul.f64 	%fd3346, %fd3345, %fd3344;
	sub.f64 	%fd3347, %fd3334, %fd3346;
	div.rn.f64 	%fd759, %fd3347, %fd3322;
	setp.gt.f64 	%p253, %fd759, 0d0000000000000000;
	setp.lt.f64 	%p254, %fd759, 0d3FF0000000000000;
	setp.ge.f64 	%p255, %fd3344, 0d0000000000000000;
	and.pred  	%p256, %p253, %p254;
	and.pred  	%p257, %p255, %p256;
	mov.u32 	%r974, 4;
	@%p257 bra 	$L__BB1_349;

	sub.f64 	%fd3348, %fd529, %fd534;
	sub.f64 	%fd3349, %fd531, %fd535;
	mul.f64 	%fd3350, %fd3349, %fd539;
	sub.f64 	%fd3351, %fd533, %fd536;
	mul.f64 	%fd3352, %fd3351, %fd538;
	sub.f64 	%fd3353, %fd3350, %fd3352;
	mul.f64 	%fd3354, %fd3351, %fd537;
	mul.f64 	%fd3355, %fd3348, %fd539;
	sub.f64 	%fd3356, %fd3354, %fd3355;
	mul.f64 	%fd3357, %fd3348, %fd538;
	mul.f64 	%fd3358, %fd3349, %fd537;
	sub.f64 	%fd3359, %fd3357, %fd3358;
	mul.f64 	%fd3360, %fd3348, %fd3348;
	fma.rn.f64 	%fd3361, %fd3349, %fd3349, %fd3360;
	fma.rn.f64 	%fd3362, %fd3351, %fd3351, %fd3361;
	mul.f64 	%fd3363, %fd3349, %fd3356;
	fma.rn.f64 	%fd3364, %fd3348, %fd3353, %fd3363;
	fma.rn.f64 	%fd3365, %fd3351, %fd3359, %fd3364;
	mul.f64 	%fd3366, %fd3356, %fd3356;
	fma.rn.f64 	%fd3367, %fd3353, %fd3353, %fd3366;
	fma.rn.f64 	%fd3368, %fd3359, %fd3359, %fd3367;
	sub.f64 	%fd3369, %fd541, %fd534;
	mul.f64 	%fd3370, %fd3348, %fd3369;
	sub.f64 	%fd3371, %fd543, %fd535;
	fma.rn.f64 	%fd3372, %fd3349, %fd3371, %fd3370;
	sub.f64 	%fd3373, %fd545, %fd536;
	fma.rn.f64 	%fd3374, %fd3351, %fd3373, %fd3372;
	mul.f64 	%fd3375, %fd3371, %fd3356;
	fma.rn.f64 	%fd3376, %fd3369, %fd3353, %fd3375;
	fma.rn.f64 	%fd3377, %fd3373, %fd3359, %fd3376;
	div.rn.f64 	%fd3378, %fd3365, %fd3362;
	mul.f64 	%fd3379, %fd3378, %fd3378;
	mul.f64 	%fd3380, %fd3362, %fd3379;
	sub.f64 	%fd3381, %fd3368, %fd3380;
	mul.f64 	%fd3382, %fd3374, %fd3378;
	sub.f64 	%fd3383, %fd3377, %fd3382;
	div.rn.f64 	%fd3384, %fd3383, %fd3381;
	mul.f64 	%fd3385, %fd3362, %fd3378;
	mul.f64 	%fd3386, %fd3385, %fd3384;
	sub.f64 	%fd3387, %fd3374, %fd3386;
	div.rn.f64 	%fd760, %fd3387, %fd3362;
	setp.gt.f64 	%p258, %fd760, 0d0000000000000000;
	setp.lt.f64 	%p259, %fd760, 0d3FF0000000000000;
	setp.ge.f64 	%p260, %fd3384, 0d0000000000000000;
	and.pred  	%p261, %p258, %p259;
	and.pred  	%p262, %p260, %p261;
	mov.u32 	%r974, 5;
	@%p262 bra 	$L__BB1_349;

	setp.le.f64 	%p263, %fd758, 0d0000000000000000;
	setp.ge.f64 	%p264, %fd760, 0d3FF0000000000000;
	and.pred  	%p265, %p263, %p264;
	mov.u32 	%r974, 0;
	@%p265 bra 	$L__BB1_349;

	setp.le.f64 	%p266, %fd759, 0d0000000000000000;
	setp.ge.f64 	%p267, %fd758, 0d3FF0000000000000;
	and.pred  	%p268, %p266, %p267;
	mov.u32 	%r974, 1;
	@%p268 bra 	$L__BB1_349;

	setp.le.f64 	%p269, %fd760, 0d0000000000000000;
	setp.ge.f64 	%p270, %fd759, 0d3FF0000000000000;
	and.pred  	%p271, %p269, %p270;
	selp.b32 	%r974, 2, 6, %p271;

$L__BB1_349:
	setp.eq.s32 	%p272, %r974, 0;
	@%p272 bra 	$L__BB1_361;

	setp.eq.s32 	%p273, %r974, 1;
	@%p273 bra 	$L__BB1_360;
	bra.uni 	$L__BB1_351;

$L__BB1_360:
	sub.f64 	%fd3677, %fd541, %fd528;
	add.f64 	%fd3678, %fd3677, %fd3677;
	sub.f64 	%fd3679, %fd543, %fd530;
	add.f64 	%fd3680, %fd3679, %fd3679;
	sub.f64 	%fd3681, %fd545, %fd532;
	add.f64 	%fd3682, %fd3681, %fd3681;
	fma.rn.f64 	%fd3683, %fd3678, %fd734, 0d0000000000000000;
	fma.rn.f64 	%fd3684, %fd3680, %fd734, 0d0000000000000000;
	fma.rn.f64 	%fd3685, %fd3682, %fd734, 0d0000000000000000;
	add.f64 	%fd4527, %fd723, %fd3683;
	add.f64 	%fd4526, %fd724, %fd3684;
	add.f64 	%fd4525, %fd725, %fd3685;
	sub.f64 	%fd4521, %fd4521, %fd3683;
	sub.f64 	%fd4520, %fd4520, %fd3684;
	sub.f64 	%fd4519, %fd4519, %fd3685;
	bra.uni 	$L__BB1_362;

$L__BB1_361:
	add.f64 	%fd3686, %fd542, %fd542;
	add.f64 	%fd3687, %fd544, %fd544;
	add.f64 	%fd3688, %fd546, %fd546;
	fma.rn.f64 	%fd3689, %fd3686, %fd734, 0d0000000000000000;
	fma.rn.f64 	%fd3690, %fd3687, %fd734, 0d0000000000000000;
	fma.rn.f64 	%fd3691, %fd3688, %fd734, 0d0000000000000000;
	add.f64 	%fd4527, %fd723, %fd3689;
	add.f64 	%fd4526, %fd724, %fd3690;
	add.f64 	%fd4525, %fd725, %fd3691;
	sub.f64 	%fd4518, %fd4518, %fd3689;
	sub.f64 	%fd4517, %fd4517, %fd3690;
	sub.f64 	%fd4516, %fd4516, %fd3691;
	bra.uni 	$L__BB1_362;

$L__BB1_351:
	setp.eq.s32 	%p274, %r974, 2;
	@%p274 bra 	$L__BB1_359;
	bra.uni 	$L__BB1_352;

$L__BB1_359:
	sub.f64 	%fd3668, %fd541, %fd534;
	add.f64 	%fd3669, %fd3668, %fd3668;
	sub.f64 	%fd3670, %fd543, %fd535;
	add.f64 	%fd3671, %fd3670, %fd3670;
	sub.f64 	%fd3672, %fd545, %fd536;
	add.f64 	%fd3673, %fd3672, %fd3672;
	fma.rn.f64 	%fd3674, %fd3669, %fd734, 0d0000000000000000;
	fma.rn.f64 	%fd3675, %fd3671, %fd734, 0d0000000000000000;
	fma.rn.f64 	%fd3676, %fd3673, %fd734, 0d0000000000000000;
	add.f64 	%fd4527, %fd723, %fd3674;
	add.f64 	%fd4526, %fd724, %fd3675;
	add.f64 	%fd4525, %fd725, %fd3676;
	sub.f64 	%fd4524, %fd4524, %fd3674;
	sub.f64 	%fd4523, %fd4523, %fd3675;
	sub.f64 	%fd4522, %fd4522, %fd3676;
	bra.uni 	$L__BB1_362;

$L__BB1_352:
	setp.eq.s32 	%p275, %r974, 3;
	@%p275 bra 	$L__BB1_358;
	bra.uni 	$L__BB1_353;

$L__BB1_358:
	sub.f64 	%fd4314, %fd532, %fd533;
	sub.f64 	%fd4313, %fd530, %fd531;
	sub.f64 	%fd4312, %fd528, %fd529;
	sub.f64 	%fd3598, %fd529, %fd541;
	sub.f64 	%fd3599, %fd532, %fd545;
	sub.f64 	%fd3600, %fd531, %fd543;
	mul.f64 	%fd3601, %fd3600, %fd3599;
	sub.f64 	%fd3602, %fd530, %fd543;
	sub.f64 	%fd3603, %fd533, %fd545;
	mul.f64 	%fd3604, %fd3603, %fd3602;
	sub.f64 	%fd3605, %fd3601, %fd3604;
	sub.f64 	%fd3606, %fd528, %fd541;
	mul.f64 	%fd3607, %fd3603, %fd3606;
	mul.f64 	%fd3608, %fd3598, %fd3599;
	sub.f64 	%fd3609, %fd3607, %fd3608;
	mul.f64 	%fd3610, %fd3598, %fd3602;
	mul.f64 	%fd3611, %fd3600, %fd3606;
	sub.f64 	%fd3612, %fd3610, %fd3611;
	mul.f64 	%fd3613, %fd3609, %fd3609;
	fma.rn.f64 	%fd3614, %fd3605, %fd3605, %fd3613;
	fma.rn.f64 	%fd3615, %fd3612, %fd3612, %fd3614;
	div.rn.f64 	%fd3616, %fd3615, %fd748;
	div.rn.f64 	%fd3617, %fd734, %fd748;
	add.f64 	%fd3618, %fd3617, 0d0000000000000000;
	mov.f64 	%fd3619, 0d0000000000000000;
	mul.f64 	%fd3620, %fd3616, %fd734;
	div.rn.f64 	%fd3621, %fd3620, %fd748;
	sub.f64 	%fd3622, %fd3619, %fd3621;
	add.f64 	%fd3623, %fd4312, %fd4312;
	add.f64 	%fd3624, %fd4313, %fd4313;
	add.f64 	%fd3625, %fd4314, %fd4314;
	fma.rn.f64 	%fd3626, %fd3623, %fd3622, 0d0000000000000000;
	fma.rn.f64 	%fd3627, %fd3624, %fd3622, 0d0000000000000000;
	fma.rn.f64 	%fd3628, %fd3625, %fd3622, 0d0000000000000000;
	add.f64 	%fd3629, %fd3605, %fd3605;
	add.f64 	%fd3630, %fd3609, %fd3609;
	add.f64 	%fd3631, %fd3612, %fd3612;
	fma.rn.f64 	%fd3632, %fd3629, %fd3618, 0d0000000000000000;
	fma.rn.f64 	%fd3633, %fd3630, %fd3618, 0d0000000000000000;
	fma.rn.f64 	%fd3634, %fd3631, %fd3618, 0d0000000000000000;
	mul.f64 	%fd3635, %fd3602, %fd3634;
	mul.f64 	%fd3636, %fd3599, %fd3633;
	sub.f64 	%fd3637, %fd3635, %fd3636;
	mul.f64 	%fd3638, %fd3599, %fd3632;
	mul.f64 	%fd3639, %fd3606, %fd3634;
	sub.f64 	%fd3640, %fd3638, %fd3639;
	mul.f64 	%fd3641, %fd3606, %fd3633;
	mul.f64 	%fd3642, %fd3602, %fd3632;
	sub.f64 	%fd3643, %fd3641, %fd3642;
	add.f64 	%fd3644, %fd3637, 0d0000000000000000;
	add.f64 	%fd3645, %fd3640, 0d0000000000000000;
	add.f64 	%fd3646, %fd3643, 0d0000000000000000;
	mul.f64 	%fd3647, %fd3600, %fd3634;
	mul.f64 	%fd3648, %fd3603, %fd3633;
	mul.f64 	%fd3649, %fd3603, %fd3632;
	mul.f64 	%fd3650, %fd3598, %fd3634;
	mul.f64 	%fd3651, %fd3598, %fd3633;
	mul.f64 	%fd3652, %fd3600, %fd3632;
	sub.f64 	%fd3653, %fd3648, %fd3647;
	add.f64 	%fd3654, %fd3653, 0d0000000000000000;
	sub.f64 	%fd3655, %fd3650, %fd3649;
	add.f64 	%fd3656, %fd3655, 0d0000000000000000;
	sub.f64 	%fd3657, %fd3652, %fd3651;
	add.f64 	%fd3658, %fd3657, 0d0000000000000000;
	add.f64 	%fd3659, %fd3626, %fd4521;
	add.f64 	%fd3660, %fd3627, %fd4520;
	add.f64 	%fd3661, %fd3628, %fd4519;
	sub.f64 	%fd3662, %fd4518, %fd3626;
	sub.f64 	%fd3663, %fd4517, %fd3627;
	sub.f64 	%fd3664, %fd4516, %fd3628;
	add.f64 	%fd4521, %fd3654, %fd3659;
	add.f64 	%fd4520, %fd3656, %fd3660;
	add.f64 	%fd4519, %fd3658, %fd3661;
	sub.f64 	%fd3665, %fd723, %fd3654;
	sub.f64 	%fd3666, %fd724, %fd3656;
	sub.f64 	%fd3667, %fd725, %fd3658;
	add.f64 	%fd4518, %fd3644, %fd3662;
	add.f64 	%fd4517, %fd3645, %fd3663;
	add.f64 	%fd4516, %fd3646, %fd3664;
	sub.f64 	%fd4527, %fd3665, %fd3644;
	sub.f64 	%fd4526, %fd3666, %fd3645;
	sub.f64 	%fd4525, %fd3667, %fd3646;
	bra.uni 	$L__BB1_362;

$L__BB1_353:
	setp.eq.s32 	%p276, %r974, 4;
	@%p276 bra 	$L__BB1_357;
	bra.uni 	$L__BB1_354;

$L__BB1_357:
	sub.f64 	%fd3522, %fd528, %fd541;
	sub.f64 	%fd3523, %fd536, %fd545;
	sub.f64 	%fd3524, %fd530, %fd543;
	mul.f64 	%fd3525, %fd3524, %fd3523;
	sub.f64 	%fd3526, %fd535, %fd543;
	sub.f64 	%fd3527, %fd532, %fd545;
	mul.f64 	%fd3528, %fd3527, %fd3526;
	sub.f64 	%fd3529, %fd3525, %fd3528;
	sub.f64 	%fd3530, %fd534, %fd541;
	mul.f64 	%fd3531, %fd3527, %fd3530;
	mul.f64 	%fd3532, %fd3522, %fd3523;
	sub.f64 	%fd3533, %fd3531, %fd3532;
	mul.f64 	%fd3534, %fd3522, %fd3526;
	mul.f64 	%fd3535, %fd3524, %fd3530;
	sub.f64 	%fd3536, %fd3534, %fd3535;
	mul.f64 	%fd3537, %fd3533, %fd3533;
	fma.rn.f64 	%fd3538, %fd3529, %fd3529, %fd3537;
	fma.rn.f64 	%fd3539, %fd3536, %fd3536, %fd3538;
	sub.f64 	%fd3540, %fd534, %fd528;
	mul.f64 	%fd3541, %fd3540, %fd3540;
	sub.f64 	%fd3542, %fd535, %fd530;
	fma.rn.f64 	%fd3543, %fd3542, %fd3542, %fd3541;
	sub.f64 	%fd3544, %fd536, %fd532;
	fma.rn.f64 	%fd3545, %fd3544, %fd3544, %fd3543;
	div.rn.f64 	%fd3546, %fd3539, %fd3545;
	div.rn.f64 	%fd3547, %fd734, %fd3545;
	add.f64 	%fd3548, %fd3547, 0d0000000000000000;
	mov.f64 	%fd3549, 0d0000000000000000;
	mul.f64 	%fd3550, %fd3546, %fd734;
	div.rn.f64 	%fd3551, %fd3550, %fd3545;
	sub.f64 	%fd3552, %fd3549, %fd3551;
	add.f64 	%fd3553, %fd3540, %fd3540;
	add.f64 	%fd3554, %fd3542, %fd3542;
	add.f64 	%fd3555, %fd3544, %fd3544;
	fma.rn.f64 	%fd3556, %fd3553, %fd3552, 0d0000000000000000;
	fma.rn.f64 	%fd3557, %fd3554, %fd3552, 0d0000000000000000;
	fma.rn.f64 	%fd3558, %fd3555, %fd3552, 0d0000000000000000;
	add.f64 	%fd3559, %fd3529, %fd3529;
	add.f64 	%fd3560, %fd3533, %fd3533;
	add.f64 	%fd3561, %fd3536, %fd3536;
	fma.rn.f64 	%fd3562, %fd3559, %fd3548, 0d0000000000000000;
	fma.rn.f64 	%fd3563, %fd3560, %fd3548, 0d0000000000000000;
	fma.rn.f64 	%fd3564, %fd3561, %fd3548, 0d0000000000000000;
	mul.f64 	%fd3565, %fd3526, %fd3564;
	mul.f64 	%fd3566, %fd3523, %fd3563;
	sub.f64 	%fd3567, %fd3565, %fd3566;
	mul.f64 	%fd3568, %fd3523, %fd3562;
	mul.f64 	%fd3569, %fd3530, %fd3564;
	sub.f64 	%fd3570, %fd3568, %fd3569;
	mul.f64 	%fd3571, %fd3530, %fd3563;
	mul.f64 	%fd3572, %fd3526, %fd3562;
	sub.f64 	%fd3573, %fd3571, %fd3572;
	add.f64 	%fd3574, %fd3567, 0d0000000000000000;
	add.f64 	%fd3575, %fd3570, 0d0000000000000000;
	add.f64 	%fd3576, %fd3573, 0d0000000000000000;
	mul.f64 	%fd3577, %fd3524, %fd3564;
	mul.f64 	%fd3578, %fd3527, %fd3563;
	mul.f64 	%fd3579, %fd3527, %fd3562;
	mul.f64 	%fd3580, %fd3522, %fd3564;
	mul.f64 	%fd3581, %fd3522, %fd3563;
	mul.f64 	%fd3582, %fd3524, %fd3562;
	sub.f64 	%fd3583, %fd3578, %fd3577;
	add.f64 	%fd3584, %fd3583, 0d0000000000000000;
	sub.f64 	%fd3585, %fd3580, %fd3579;
	add.f64 	%fd3586, %fd3585, 0d0000000000000000;
	sub.f64 	%fd3587, %fd3582, %fd3581;
	add.f64 	%fd3588, %fd3587, 0d0000000000000000;
	add.f64 	%fd3589, %fd3556, %fd4524;
	add.f64 	%fd3590, %fd3557, %fd4523;
	add.f64 	%fd3591, %fd3558, %fd4522;
	sub.f64 	%fd3592, %fd4521, %fd3556;
	sub.f64 	%fd3593, %fd4520, %fd3557;
	sub.f64 	%fd3594, %fd4519, %fd3558;
	add.f64 	%fd4524, %fd3584, %fd3589;
	add.f64 	%fd4523, %fd3586, %fd3590;
	add.f64 	%fd4522, %fd3588, %fd3591;
	sub.f64 	%fd3595, %fd723, %fd3584;
	sub.f64 	%fd3596, %fd724, %fd3586;
	sub.f64 	%fd3597, %fd725, %fd3588;
	add.f64 	%fd4521, %fd3574, %fd3592;
	add.f64 	%fd4520, %fd3575, %fd3593;
	add.f64 	%fd4519, %fd3576, %fd3594;
	sub.f64 	%fd4527, %fd3595, %fd3574;
	sub.f64 	%fd4526, %fd3596, %fd3575;
	sub.f64 	%fd4525, %fd3597, %fd3576;
	bra.uni 	$L__BB1_362;

$L__BB1_354:
	setp.eq.s32 	%p277, %r974, 5;
	@%p277 bra 	$L__BB1_356;
	bra.uni 	$L__BB1_355;

$L__BB1_356:
	sub.f64 	%fd3446, %fd534, %fd541;
	sub.f64 	%fd3447, %fd533, %fd545;
	sub.f64 	%fd3448, %fd535, %fd543;
	mul.f64 	%fd3449, %fd3447, %fd3448;
	sub.f64 	%fd3450, %fd531, %fd543;
	sub.f64 	%fd3451, %fd536, %fd545;
	mul.f64 	%fd3452, %fd3450, %fd3451;
	sub.f64 	%fd3453, %fd3449, %fd3452;
	sub.f64 	%fd3454, %fd529, %fd541;
	mul.f64 	%fd3455, %fd3454, %fd3451;
	mul.f64 	%fd3456, %fd3447, %fd3446;
	sub.f64 	%fd3457, %fd3455, %fd3456;
	mul.f64 	%fd3458, %fd3450, %fd3446;
	mul.f64 	%fd3459, %fd3454, %fd3448;
	sub.f64 	%fd3460, %fd3458, %fd3459;
	mul.f64 	%fd3461, %fd3457, %fd3457;
	fma.rn.f64 	%fd3462, %fd3453, %fd3453, %fd3461;
	fma.rn.f64 	%fd3463, %fd3460, %fd3460, %fd3462;
	sub.f64 	%fd3464, %fd529, %fd534;
	mul.f64 	%fd3465, %fd3464, %fd3464;
	sub.f64 	%fd3466, %fd531, %fd535;
	fma.rn.f64 	%fd3467, %fd3466, %fd3466, %fd3465;
	sub.f64 	%fd3468, %fd533, %fd536;
	fma.rn.f64 	%fd3469, %fd3468, %fd3468, %fd3467;
	div.rn.f64 	%fd3470, %fd3463, %fd3469;
	div.rn.f64 	%fd3471, %fd734, %fd3469;
	add.f64 	%fd3472, %fd3471, 0d0000000000000000;
	mov.f64 	%fd3473, 0d0000000000000000;
	mul.f64 	%fd3474, %fd3470, %fd734;
	div.rn.f64 	%fd3475, %fd3474, %fd3469;
	sub.f64 	%fd3476, %fd3473, %fd3475;
	add.f64 	%fd3477, %fd3464, %fd3464;
	add.f64 	%fd3478, %fd3466, %fd3466;
	add.f64 	%fd3479, %fd3468, %fd3468;
	fma.rn.f64 	%fd3480, %fd3477, %fd3476, 0d0000000000000000;
	fma.rn.f64 	%fd3481, %fd3478, %fd3476, 0d0000000000000000;
	fma.rn.f64 	%fd3482, %fd3479, %fd3476, 0d0000000000000000;
	add.f64 	%fd3483, %fd3453, %fd3453;
	add.f64 	%fd3484, %fd3457, %fd3457;
	add.f64 	%fd3485, %fd3460, %fd3460;
	fma.rn.f64 	%fd3486, %fd3483, %fd3472, 0d0000000000000000;
	fma.rn.f64 	%fd3487, %fd3484, %fd3472, 0d0000000000000000;
	fma.rn.f64 	%fd3488, %fd3485, %fd3472, 0d0000000000000000;
	mul.f64 	%fd3489, %fd3450, %fd3488;
	mul.f64 	%fd3490, %fd3447, %fd3487;
	sub.f64 	%fd3491, %fd3489, %fd3490;
	mul.f64 	%fd3492, %fd3447, %fd3486;
	mul.f64 	%fd3493, %fd3454, %fd3488;
	sub.f64 	%fd3494, %fd3492, %fd3493;
	mul.f64 	%fd3495, %fd3454, %fd3487;
	mul.f64 	%fd3496, %fd3450, %fd3486;
	sub.f64 	%fd3497, %fd3495, %fd3496;
	add.f64 	%fd3498, %fd3491, 0d0000000000000000;
	add.f64 	%fd3499, %fd3494, 0d0000000000000000;
	add.f64 	%fd3500, %fd3497, 0d0000000000000000;
	mul.f64 	%fd3501, %fd3448, %fd3488;
	mul.f64 	%fd3502, %fd3451, %fd3487;
	mul.f64 	%fd3503, %fd3451, %fd3486;
	mul.f64 	%fd3504, %fd3446, %fd3488;
	mul.f64 	%fd3505, %fd3446, %fd3487;
	mul.f64 	%fd3506, %fd3448, %fd3486;
	sub.f64 	%fd3507, %fd3502, %fd3501;
	add.f64 	%fd3508, %fd3507, 0d0000000000000000;
	sub.f64 	%fd3509, %fd3504, %fd3503;
	add.f64 	%fd3510, %fd3509, 0d0000000000000000;
	sub.f64 	%fd3511, %fd3506, %fd3505;
	add.f64 	%fd3512, %fd3511, 0d0000000000000000;
	add.f64 	%fd3513, %fd3480, %fd4518;
	add.f64 	%fd3514, %fd3481, %fd4517;
	add.f64 	%fd3515, %fd3482, %fd4516;
	sub.f64 	%fd3516, %fd4524, %fd3480;
	sub.f64 	%fd3517, %fd4523, %fd3481;
	sub.f64 	%fd3518, %fd4522, %fd3482;
	add.f64 	%fd4518, %fd3508, %fd3513;
	add.f64 	%fd4517, %fd3510, %fd3514;
	add.f64 	%fd4516, %fd3512, %fd3515;
	sub.f64 	%fd3519, %fd723, %fd3508;
	sub.f64 	%fd3520, %fd724, %fd3510;
	sub.f64 	%fd3521, %fd725, %fd3512;
	add.f64 	%fd4524, %fd3498, %fd3516;
	add.f64 	%fd4523, %fd3499, %fd3517;
	add.f64 	%fd4522, %fd3500, %fd3518;
	sub.f64 	%fd4527, %fd3519, %fd3498;
	sub.f64 	%fd4526, %fd3520, %fd3499;
	sub.f64 	%fd4525, %fd3521, %fd3500;
	bra.uni 	$L__BB1_362;

$L__BB1_355:
	sub.f64 	%fd4305, %fd534, %fd529;
	sub.f64 	%fd4304, %fd536, %fd533;
	sub.f64 	%fd4303, %fd535, %fd531;
	sub.f64 	%fd4302, %fd532, %fd533;
	sub.f64 	%fd4301, %fd530, %fd531;
	sub.f64 	%fd4300, %fd528, %fd529;
	mul.f64 	%fd3388, %fd544, %fd538;
	fma.rn.f64 	%fd3389, %fd542, %fd537, %fd3388;
	fma.rn.f64 	%fd3390, %fd546, %fd539, %fd3389;
	mul.f64 	%fd3391, %fd3390, %fd3390;
	mul.f64 	%fd3392, %fd538, %fd538;
	fma.rn.f64 	%fd3393, %fd537, %fd537, %fd3392;
	fma.rn.f64 	%fd3394, %fd539, %fd539, %fd3393;
	div.rn.f64 	%fd3395, %fd3391, %fd3394;
	div.rn.f64 	%fd3396, %fd734, %fd3394;
	add.f64 	%fd3397, %fd3396, 0d0000000000000000;
	mov.f64 	%fd3398, 0d0000000000000000;
	mul.f64 	%fd3399, %fd3395, %fd734;
	div.rn.f64 	%fd3400, %fd3399, %fd3394;
	sub.f64 	%fd3401, %fd3398, %fd3400;
	add.f64 	%fd3402, %fd537, %fd537;
	add.f64 	%fd3403, %fd538, %fd538;
	add.f64 	%fd3404, %fd539, %fd539;
	fma.rn.f64 	%fd3405, %fd3402, %fd3401, 0d0000000000000000;
	fma.rn.f64 	%fd3406, %fd3403, %fd3401, 0d0000000000000000;
	fma.rn.f64 	%fd3407, %fd3404, %fd3401, 0d0000000000000000;
	fma.rn.f64 	%fd3408, %fd3390, %fd3397, 0d0000000000000000;
	fma.rn.f64 	%fd3409, %fd3390, %fd3397, %fd3408;
	fma.rn.f64 	%fd3410, %fd537, %fd3409, 0d0000000000000000;
	fma.rn.f64 	%fd3411, %fd538, %fd3409, 0d0000000000000000;
	fma.rn.f64 	%fd3412, %fd539, %fd3409, 0d0000000000000000;
	fma.rn.f64 	%fd3413, %fd542, %fd3409, %fd3405;
	fma.rn.f64 	%fd3414, %fd544, %fd3409, %fd3406;
	fma.rn.f64 	%fd3415, %fd546, %fd3409, %fd3407;
	mul.f64 	%fd3416, %fd4303, %fd3415;
	mul.f64 	%fd3417, %fd4304, %fd3414;
	sub.f64 	%fd3418, %fd3416, %fd3417;
	mul.f64 	%fd3419, %fd4304, %fd3413;
	mul.f64 	%fd3420, %fd4305, %fd3415;
	sub.f64 	%fd3421, %fd3419, %fd3420;
	mul.f64 	%fd3422, %fd4305, %fd3414;
	mul.f64 	%fd3423, %fd4303, %fd3413;
	sub.f64 	%fd3424, %fd3422, %fd3423;
	add.f64 	%fd3425, %fd3418, 0d0000000000000000;
	add.f64 	%fd3426, %fd3421, 0d0000000000000000;
	add.f64 	%fd3427, %fd3424, 0d0000000000000000;
	mul.f64 	%fd3428, %fd4301, %fd3415;
	mul.f64 	%fd3429, %fd4302, %fd3414;
	mul.f64 	%fd3430, %fd4302, %fd3413;
	mul.f64 	%fd3431, %fd4300, %fd3415;
	mul.f64 	%fd3432, %fd4300, %fd3414;
	mul.f64 	%fd3433, %fd4301, %fd3413;
	sub.f64 	%fd3434, %fd3429, %fd3428;
	add.f64 	%fd3435, %fd3434, 0d0000000000000000;
	sub.f64 	%fd3436, %fd3431, %fd3430;
	add.f64 	%fd3437, %fd3436, 0d0000000000000000;
	sub.f64 	%fd3438, %fd3433, %fd3432;
	add.f64 	%fd3439, %fd3438, 0d0000000000000000;
	add.f64 	%fd4527, %fd723, %fd3410;
	add.f64 	%fd4526, %fd724, %fd3411;
	add.f64 	%fd4525, %fd725, %fd3412;
	sub.f64 	%fd3440, %fd4518, %fd3410;
	sub.f64 	%fd3441, %fd4517, %fd3411;
	sub.f64 	%fd3442, %fd4516, %fd3412;
	add.f64 	%fd4524, %fd3435, %fd4524;
	add.f64 	%fd4523, %fd3437, %fd4523;
	add.f64 	%fd4522, %fd3439, %fd4522;
	sub.f64 	%fd3443, %fd3440, %fd3435;
	sub.f64 	%fd3444, %fd3441, %fd3437;
	sub.f64 	%fd3445, %fd3442, %fd3439;
	add.f64 	%fd4521, %fd3425, %fd4521;
	add.f64 	%fd4520, %fd3426, %fd4520;
	add.f64 	%fd4519, %fd3427, %fd4519;
	sub.f64 	%fd4518, %fd3443, %fd3425;
	sub.f64 	%fd4517, %fd3444, %fd3426;
	sub.f64 	%fd4516, %fd3445, %fd3427;

$L__BB1_362:
	mov.f64 	%fd3700, 0d0000000000000000;
	mov.f64 	%fd4557, %fd3700;
	mov.f64 	%fd4558, %fd3700;
	mov.f64 	%fd4559, %fd3700;
	mov.f64 	%fd4560, %fd3700;
	mov.f64 	%fd4561, %fd3700;
	mov.f64 	%fd4562, %fd3700;
	mov.f64 	%fd4563, %fd3700;
	mov.f64 	%fd4564, %fd3700;
	mov.f64 	%fd4565, %fd3700;
	@%p5 bra 	$L__BB1_366;

	sub.f64 	%fd830, %fd534, %fd528;
	sub.f64 	%fd831, %fd535, %fd530;
	mul.f64 	%fd3709, %fd831, %fd539;
	sub.f64 	%fd832, %fd536, %fd532;
	mul.f64 	%fd3710, %fd832, %fd538;
	sub.f64 	%fd833, %fd3709, %fd3710;
	mul.f64 	%fd3711, %fd832, %fd537;
	mul.f64 	%fd3712, %fd830, %fd539;
	sub.f64 	%fd834, %fd3711, %fd3712;
	mul.f64 	%fd3713, %fd830, %fd538;
	mul.f64 	%fd3714, %fd831, %fd537;
	sub.f64 	%fd835, %fd3713, %fd3714;
	mul.f64 	%fd3715, %fd830, %fd830;
	fma.rn.f64 	%fd3716, %fd831, %fd831, %fd3715;
	fma.rn.f64 	%fd836, %fd832, %fd832, %fd3716;
	mul.f64 	%fd3717, %fd831, %fd834;
	fma.rn.f64 	%fd3718, %fd830, %fd833, %fd3717;
	fma.rn.f64 	%fd3719, %fd832, %fd835, %fd3718;
	mul.f64 	%fd3720, %fd834, %fd834;
	fma.rn.f64 	%fd3721, %fd833, %fd833, %fd3720;
	fma.rn.f64 	%fd3722, %fd835, %fd835, %fd3721;
	sub.f64 	%fd837, %fd541, %fd528;
	mul.f64 	%fd3723, %fd837, %fd830;
	sub.f64 	%fd838, %fd543, %fd530;
	fma.rn.f64 	%fd3724, %fd838, %fd831, %fd3723;
	sub.f64 	%fd839, %fd545, %fd532;
	fma.rn.f64 	%fd840, %fd839, %fd832, %fd3724;
	mul.f64 	%fd3725, %fd838, %fd834;
	fma.rn.f64 	%fd3726, %fd837, %fd833, %fd3725;
	fma.rn.f64 	%fd3727, %fd839, %fd835, %fd3726;
	div.rn.f64 	%fd841, %fd3719, %fd836;
	mul.f64 	%fd842, %fd841, %fd841;
	mul.f64 	%fd3728, %fd836, %fd842;
	sub.f64 	%fd843, %fd3722, %fd3728;
	mul.f64 	%fd3729, %fd840, %fd841;
	sub.f64 	%fd3730, %fd3727, %fd3729;
	div.rn.f64 	%fd844, %fd3730, %fd843;
	mul.f64 	%fd845, %fd836, %fd841;
	mul.f64 	%fd3731, %fd845, %fd844;
	sub.f64 	%fd3732, %fd840, %fd3731;
	div.rn.f64 	%fd846, %fd3732, %fd836;
	setp.gt.f64 	%p278, %fd846, 0d0000000000000000;
	mov.f64 	%fd3708, 0d0000000000000000;
	setp.lt.f64 	%p279, %fd846, 0d3FF0000000000000;
	setp.ge.f64 	%p280, %fd844, 0d0000000000000000;
	and.pred  	%p281, %p278, %p279;
	and.pred  	%p282, %p280, %p281;
	mov.f64 	%fd4537, %fd3708;
	mov.f64 	%fd4538, %fd3708;
	mov.f64 	%fd4539, %fd3708;
	mov.f64 	%fd4540, %fd3708;
	mov.f64 	%fd4541, %fd3708;
	mov.f64 	%fd4542, %fd3708;
	mov.f64 	%fd4543, %fd3708;
	mov.f64 	%fd4544, %fd3708;
	@%p282 bra 	$L__BB1_365;

	sub.f64 	%fd3733, %fd529, %fd534;
	sub.f64 	%fd3734, %fd531, %fd535;
	mul.f64 	%fd3735, %fd3734, %fd539;
	sub.f64 	%fd3736, %fd533, %fd536;
	mul.f64 	%fd3737, %fd3736, %fd538;
	sub.f64 	%fd3738, %fd3735, %fd3737;
	mul.f64 	%fd3739, %fd3736, %fd537;
	mul.f64 	%fd3740, %fd3733, %fd539;
	sub.f64 	%fd3741, %fd3739, %fd3740;
	mul.f64 	%fd3742, %fd3733, %fd538;
	mul.f64 	%fd3743, %fd3734, %fd537;
	sub.f64 	%fd3744, %fd3742, %fd3743;
	mul.f64 	%fd3745, %fd3733, %fd3733;
	fma.rn.f64 	%fd3746, %fd3734, %fd3734, %fd3745;
	fma.rn.f64 	%fd3747, %fd3736, %fd3736, %fd3746;
	mul.f64 	%fd3748, %fd3734, %fd3741;
	fma.rn.f64 	%fd3749, %fd3733, %fd3738, %fd3748;
	fma.rn.f64 	%fd3750, %fd3736, %fd3744, %fd3749;
	mul.f64 	%fd3751, %fd3741, %fd3741;
	fma.rn.f64 	%fd3752, %fd3738, %fd3738, %fd3751;
	fma.rn.f64 	%fd3753, %fd3744, %fd3744, %fd3752;
	sub.f64 	%fd3754, %fd541, %fd534;
	mul.f64 	%fd3755, %fd3733, %fd3754;
	sub.f64 	%fd3756, %fd543, %fd535;
	fma.rn.f64 	%fd3757, %fd3734, %fd3756, %fd3755;
	sub.f64 	%fd3758, %fd545, %fd536;
	fma.rn.f64 	%fd3759, %fd3736, %fd3758, %fd3757;
	mul.f64 	%fd3760, %fd3756, %fd3741;
	fma.rn.f64 	%fd3761, %fd3754, %fd3738, %fd3760;
	fma.rn.f64 	%fd3762, %fd3758, %fd3744, %fd3761;
	div.rn.f64 	%fd3763, %fd3750, %fd3747;
	mul.f64 	%fd3764, %fd3763, %fd3763;
	mul.f64 	%fd3765, %fd3747, %fd3764;
	sub.f64 	%fd3766, %fd3753, %fd3765;
	mul.f64 	%fd3767, %fd3759, %fd3763;
	sub.f64 	%fd3768, %fd3762, %fd3767;
	div.rn.f64 	%fd3769, %fd3768, %fd3766;
	mul.f64 	%fd3770, %fd3747, %fd3763;
	mul.f64 	%fd3771, %fd3770, %fd3769;
	sub.f64 	%fd3772, %fd3759, %fd3771;
	div.rn.f64 	%fd3773, %fd3772, %fd3747;
	mov.f64 	%fd3774, 0d0000000000000000;
	div.rn.f64 	%fd3775, %fd3774, %fd3747;
	add.f64 	%fd3776, %fd3775, 0d0000000000000000;
	mul.f64 	%fd3777, %fd3773, 0d0000000000000000;
	div.rn.f64 	%fd3778, %fd3777, %fd3747;
	sub.f64 	%fd3779, %fd3774, %fd3778;
	sub.f64 	%fd3780, %fd3774, %fd3776;
	fma.rn.f64 	%fd3781, %fd3780, %fd3769, 0d0000000000000000;
	fma.rn.f64 	%fd3782, %fd3780, %fd3770, 0d0000000000000000;
	fma.rn.f64 	%fd3783, %fd3763, %fd3781, %fd3779;
	fma.rn.f64 	%fd3784, %fd3747, %fd3781, 0d0000000000000000;
	div.rn.f64 	%fd3785, %fd3782, %fd3766;
	add.f64 	%fd4538, %fd3785, 0d0000000000000000;
	mul.f64 	%fd3786, %fd3782, %fd3769;
	div.rn.f64 	%fd3787, %fd3786, %fd3766;
	sub.f64 	%fd3788, %fd3774, %fd3787;
	sub.f64 	%fd3789, %fd3774, %fd4538;
	fma.rn.f64 	%fd3790, %fd3759, %fd3789, %fd3784;
	fma.rn.f64 	%fd3791, %fd3763, %fd3789, %fd3776;
	add.f64 	%fd4537, %fd3791, 0d0000000000000000;
	add.f64 	%fd4541, %fd3788, 0d0000000000000000;
	sub.f64 	%fd3792, %fd3774, %fd3788;
	fma.rn.f64 	%fd3793, %fd3747, %fd3792, 0d0000000000000000;
	fma.rn.f64 	%fd3794, %fd3764, %fd3792, %fd3783;
	fma.rn.f64 	%fd3795, %fd3763, %fd3793, %fd3790;
	fma.rn.f64 	%fd3796, %fd3763, %fd3793, %fd3795;
	div.rn.f64 	%fd3797, %fd3796, %fd3747;
	add.f64 	%fd4540, %fd3797, 0d0000000000000000;
	mul.f64 	%fd3798, %fd3763, %fd3796;
	div.rn.f64 	%fd3799, %fd3798, %fd3747;
	sub.f64 	%fd3800, %fd3794, %fd3799;
	add.f64 	%fd4539, %fd3800, 0d0000000000000000;
	fma.rn.f64 	%fd3801, %fd3754, %fd4538, 0d0000000000000000;
	fma.rn.f64 	%fd3802, %fd3756, %fd4538, 0d0000000000000000;
	fma.rn.f64 	%fd3803, %fd3758, %fd4538, 0d0000000000000000;
	fma.rn.f64 	%fd3804, %fd3738, %fd4538, 0d0000000000000000;
	fma.rn.f64 	%fd3805, %fd3741, %fd4538, 0d0000000000000000;
	fma.rn.f64 	%fd3806, %fd3744, %fd4538, 0d0000000000000000;
	fma.rn.f64 	%fd3807, %fd3754, %fd4537, 0d0000000000000000;
	fma.rn.f64 	%fd3808, %fd3756, %fd4537, 0d0000000000000000;
	fma.rn.f64 	%fd3809, %fd3758, %fd4537, 0d0000000000000000;
	fma.rn.f64 	%fd3810, %fd3733, %fd4537, %fd3804;
	fma.rn.f64 	%fd3811, %fd3734, %fd4537, %fd3805;
	fma.rn.f64 	%fd3812, %fd3736, %fd4537, %fd3806;
	add.f64 	%fd4527, %fd3810, %fd4527;
	add.f64 	%fd4526, %fd3811, %fd4526;
	add.f64 	%fd4525, %fd3812, %fd4525;
	sub.f64 	%fd3813, %fd4524, %fd3810;
	sub.f64 	%fd3814, %fd4523, %fd3811;
	sub.f64 	%fd3815, %fd4522, %fd3812;
	fma.rn.f64 	%fd3816, %fd3744, %fd4541, 0d0000000000000000;
	add.f64 	%fd3817, %fd3803, %fd3816;
	add.f64 	%fd3818, %fd3816, %fd3817;
	fma.rn.f64 	%fd3819, %fd3741, %fd4541, 0d0000000000000000;
	add.f64 	%fd3820, %fd3802, %fd3819;
	add.f64 	%fd3821, %fd3819, %fd3820;
	fma.rn.f64 	%fd3822, %fd3738, %fd4541, 0d0000000000000000;
	add.f64 	%fd3823, %fd3801, %fd3822;
	add.f64 	%fd3824, %fd3822, %fd3823;
	fma.rn.f64 	%fd3825, %fd3744, %fd4540, 0d0000000000000000;
	fma.rn.f64 	%fd3826, %fd3736, %fd4540, 0d0000000000000000;
	add.f64 	%fd3827, %fd3818, %fd3826;
	add.f64 	%fd3828, %fd3809, %fd3825;
	fma.rn.f64 	%fd3829, %fd3741, %fd4540, 0d0000000000000000;
	fma.rn.f64 	%fd3830, %fd3734, %fd4540, 0d0000000000000000;
	add.f64 	%fd3831, %fd3821, %fd3830;
	add.f64 	%fd3832, %fd3808, %fd3829;
	fma.rn.f64 	%fd3833, %fd3738, %fd4540, 0d0000000000000000;
	fma.rn.f64 	%fd3834, %fd3733, %fd4540, 0d0000000000000000;
	add.f64 	%fd3835, %fd3824, %fd3834;
	add.f64 	%fd3836, %fd3807, %fd3833;
	fma.rn.f64 	%fd3837, %fd3736, %fd4539, 0d0000000000000000;
	add.f64 	%fd3838, %fd3828, %fd3837;
	add.f64 	%fd3839, %fd3837, %fd3838;
	fma.rn.f64 	%fd3840, %fd3734, %fd4539, 0d0000000000000000;
	add.f64 	%fd3841, %fd3832, %fd3840;
	add.f64 	%fd3842, %fd3840, %fd3841;
	fma.rn.f64 	%fd3843, %fd3733, %fd4539, 0d0000000000000000;
	add.f64 	%fd3844, %fd3836, %fd3843;
	add.f64 	%fd3845, %fd3843, %fd3844;
	mul.f64 	%fd3846, %fd538, %fd3827;
	mul.f64 	%fd3847, %fd539, %fd3831;
	sub.f64 	%fd3848, %fd3846, %fd3847;
	mul.f64 	%fd3849, %fd539, %fd3835;
	mul.f64 	%fd3850, %fd537, %fd3827;
	sub.f64 	%fd3851, %fd3849, %fd3850;
	mul.f64 	%fd3852, %fd537, %fd3831;
	mul.f64 	%fd3853, %fd538, %fd3835;
	sub.f64 	%fd3854, %fd3852, %fd3853;
	add.f64 	%fd3855, %fd3848, %fd3845;
	add.f64 	%fd3856, %fd3851, %fd3842;
	add.f64 	%fd3857, %fd3854, %fd3839;
	mul.f64 	%fd3858, %fd3734, %fd3827;
	mul.f64 	%fd3859, %fd3736, %fd3831;
	mul.f64 	%fd3860, %fd3736, %fd3835;
	mul.f64 	%fd3861, %fd3733, %fd3827;
	mul.f64 	%fd3862, %fd3733, %fd3831;
	mul.f64 	%fd3863, %fd3734, %fd3835;
	sub.f64 	%fd3864, %fd3859, %fd3858;
	add.f64 	%fd4542, %fd3864, 0d0000000000000000;
	sub.f64 	%fd3865, %fd3861, %fd3860;
	add.f64 	%fd4543, %fd3865, 0d0000000000000000;
	sub.f64 	%fd3866, %fd3863, %fd3862;
	add.f64 	%fd4544, %fd3866, 0d0000000000000000;
	add.f64 	%fd4518, %fd3855, %fd4518;
	add.f64 	%fd4517, %fd3856, %fd4517;
	add.f64 	%fd4516, %fd3857, %fd4516;
	sub.f64 	%fd4524, %fd3813, %fd3855;
	sub.f64 	%fd4523, %fd3814, %fd3856;
	sub.f64 	%fd4522, %fd3815, %fd3857;

$L__BB1_365:
	div.rn.f64 	%fd3868, %fd3708, %fd836;
	add.f64 	%fd3869, %fd3868, 0d0000000000000000;
	mul.f64 	%fd3870, %fd846, 0d0000000000000000;
	div.rn.f64 	%fd3871, %fd3870, %fd836;
	sub.f64 	%fd3872, %fd3708, %fd3871;
	sub.f64 	%fd3873, %fd3708, %fd3869;
	fma.rn.f64 	%fd3874, %fd3873, %fd844, 0d0000000000000000;
	fma.rn.f64 	%fd3875, %fd3873, %fd845, 0d0000000000000000;
	fma.rn.f64 	%fd3876, %fd841, %fd3874, %fd3872;
	fma.rn.f64 	%fd3877, %fd836, %fd3874, 0d0000000000000000;
	div.rn.f64 	%fd3878, %fd3875, %fd843;
	add.f64 	%fd3879, %fd3878, 0d0000000000000000;
	mul.f64 	%fd3880, %fd3875, %fd844;
	div.rn.f64 	%fd3881, %fd3880, %fd843;
	sub.f64 	%fd3882, %fd3708, %fd3881;
	sub.f64 	%fd3883, %fd3708, %fd3879;
	fma.rn.f64 	%fd3884, %fd840, %fd3883, %fd3877;
	fma.rn.f64 	%fd3885, %fd841, %fd3883, %fd3869;
	add.f64 	%fd3886, %fd3885, 0d0000000000000000;
	add.f64 	%fd3887, %fd3882, 0d0000000000000000;
	sub.f64 	%fd3888, %fd3708, %fd3882;
	fma.rn.f64 	%fd3889, %fd836, %fd3888, 0d0000000000000000;
	fma.rn.f64 	%fd3890, %fd842, %fd3888, %fd3876;
	fma.rn.f64 	%fd3891, %fd841, %fd3889, %fd3884;
	fma.rn.f64 	%fd3892, %fd841, %fd3889, %fd3891;
	div.rn.f64 	%fd3893, %fd3892, %fd836;
	add.f64 	%fd3894, %fd3893, 0d0000000000000000;
	mul.f64 	%fd3895, %fd841, %fd3892;
	div.rn.f64 	%fd3896, %fd3895, %fd836;
	sub.f64 	%fd3897, %fd3890, %fd3896;
	add.f64 	%fd3898, %fd3897, 0d0000000000000000;
	add.f64 	%fd4558, %fd3879, %fd4538;
	add.f64 	%fd4557, %fd3886, %fd4537;
	add.f64 	%fd4562, %fd3887, %fd4541;
	add.f64 	%fd4560, %fd3894, %fd4540;
	add.f64 	%fd4559, %fd3898, %fd4539;
	add.f64 	%fd3899, %fd4558, 0d0000000000000000;
	fma.rn.f64 	%fd3900, %fd837, %fd3899, 0d0000000000000000;
	fma.rn.f64 	%fd3901, %fd838, %fd3899, 0d0000000000000000;
	fma.rn.f64 	%fd3902, %fd839, %fd3899, 0d0000000000000000;
	fma.rn.f64 	%fd3903, %fd833, %fd3899, 0d0000000000000000;
	fma.rn.f64 	%fd3904, %fd834, %fd3899, 0d0000000000000000;
	fma.rn.f64 	%fd3905, %fd835, %fd3899, 0d0000000000000000;
	add.f64 	%fd3906, %fd4557, 0d0000000000000000;
	fma.rn.f64 	%fd3907, %fd837, %fd3906, 0d0000000000000000;
	fma.rn.f64 	%fd3908, %fd838, %fd3906, 0d0000000000000000;
	fma.rn.f64 	%fd3909, %fd839, %fd3906, 0d0000000000000000;
	fma.rn.f64 	%fd3910, %fd830, %fd3906, %fd3903;
	fma.rn.f64 	%fd3911, %fd831, %fd3906, %fd3904;
	fma.rn.f64 	%fd3912, %fd832, %fd3906, %fd3905;
	add.f64 	%fd4527, %fd4527, %fd3910;
	add.f64 	%fd4526, %fd4526, %fd3911;
	add.f64 	%fd4525, %fd4525, %fd3912;
	sub.f64 	%fd3913, %fd4521, %fd3910;
	sub.f64 	%fd3914, %fd4520, %fd3911;
	sub.f64 	%fd3915, %fd4519, %fd3912;
	add.f64 	%fd3916, %fd4562, 0d0000000000000000;
	fma.rn.f64 	%fd3917, %fd835, %fd3916, 0d0000000000000000;
	add.f64 	%fd3918, %fd3902, %fd3917;
	add.f64 	%fd3919, %fd3917, %fd3918;
	fma.rn.f64 	%fd3920, %fd834, %fd3916, 0d0000000000000000;
	add.f64 	%fd3921, %fd3901, %fd3920;
	add.f64 	%fd3922, %fd3920, %fd3921;
	fma.rn.f64 	%fd3923, %fd833, %fd3916, 0d0000000000000000;
	add.f64 	%fd3924, %fd3900, %fd3923;
	add.f64 	%fd3925, %fd3923, %fd3924;
	add.f64 	%fd3926, %fd4560, 0d0000000000000000;
	add.f64 	%fd4561, %fd4540, %fd3926;
	add.f64 	%fd3927, %fd4561, 0d0000000000000000;
	fma.rn.f64 	%fd3928, %fd835, %fd3927, 0d0000000000000000;
	fma.rn.f64 	%fd3929, %fd832, %fd3927, 0d0000000000000000;
	add.f64 	%fd3930, %fd3929, %fd3919;
	add.f64 	%fd3931, %fd3909, %fd3928;
	fma.rn.f64 	%fd3932, %fd834, %fd3927, 0d0000000000000000;
	fma.rn.f64 	%fd3933, %fd831, %fd3927, 0d0000000000000000;
	add.f64 	%fd3934, %fd3933, %fd3922;
	add.f64 	%fd3935, %fd3908, %fd3932;
	fma.rn.f64 	%fd3936, %fd833, %fd3927, 0d0000000000000000;
	fma.rn.f64 	%fd3937, %fd830, %fd3927, 0d0000000000000000;
	add.f64 	%fd3938, %fd3937, %fd3925;
	add.f64 	%fd3939, %fd3907, %fd3936;
	add.f64 	%fd3940, %fd4559, 0d0000000000000000;
	fma.rn.f64 	%fd3941, %fd832, %fd3940, 0d0000000000000000;
	add.f64 	%fd3942, %fd3941, %fd3931;
	add.f64 	%fd3943, %fd3941, %fd3942;
	fma.rn.f64 	%fd3944, %fd831, %fd3940, 0d0000000000000000;
	add.f64 	%fd3945, %fd3944, %fd3935;
	add.f64 	%fd3946, %fd3944, %fd3945;
	fma.rn.f64 	%fd3947, %fd830, %fd3940, 0d0000000000000000;
	add.f64 	%fd3948, %fd3947, %fd3939;
	add.f64 	%fd3949, %fd3947, %fd3948;
	mul.f64 	%fd3950, %fd538, %fd3930;
	mul.f64 	%fd3951, %fd539, %fd3934;
	sub.f64 	%fd3952, %fd3950, %fd3951;
	mul.f64 	%fd3953, %fd539, %fd3938;
	mul.f64 	%fd3954, %fd537, %fd3930;
	sub.f64 	%fd3955, %fd3953, %fd3954;
	mul.f64 	%fd3956, %fd537, %fd3934;
	mul.f64 	%fd3957, %fd538, %fd3938;
	sub.f64 	%fd3958, %fd3956, %fd3957;
	add.f64 	%fd3959, %fd3949, %fd3952;
	add.f64 	%fd3960, %fd3946, %fd3955;
	add.f64 	%fd3961, %fd3943, %fd3958;
	mul.f64 	%fd3962, %fd831, %fd3930;
	mul.f64 	%fd3963, %fd832, %fd3934;
	sub.f64 	%fd3964, %fd3962, %fd3963;
	mul.f64 	%fd3965, %fd832, %fd3938;
	mul.f64 	%fd3966, %fd830, %fd3930;
	sub.f64 	%fd3967, %fd3965, %fd3966;
	mul.f64 	%fd3968, %fd830, %fd3934;
	mul.f64 	%fd3969, %fd831, %fd3938;
	sub.f64 	%fd3970, %fd3968, %fd3969;
	sub.f64 	%fd4563, %fd4542, %fd3964;
	sub.f64 	%fd4564, %fd4543, %fd3967;
	sub.f64 	%fd4565, %fd4544, %fd3970;
	add.f64 	%fd4524, %fd3959, %fd4524;
	add.f64 	%fd4523, %fd3960, %fd4523;
	add.f64 	%fd4522, %fd3961, %fd4522;
	sub.f64 	%fd4521, %fd3913, %fd3959;
	sub.f64 	%fd4520, %fd3914, %fd3960;
	sub.f64 	%fd4519, %fd3915, %fd3961;

$L__BB1_366:
	mul.f64 	%fd4333, %fd748, %fd753;
	sub.f64 	%fd4311, %fd534, %fd529;
	sub.f64 	%fd4310, %fd536, %fd533;
	sub.f64 	%fd4309, %fd535, %fd531;
	sub.f64 	%fd4308, %fd532, %fd533;
	sub.f64 	%fd4307, %fd530, %fd531;
	sub.f64 	%fd4306, %fd528, %fd529;
	mul.f64 	%fd4295, %fd4307, %fd537;
	mul.f64 	%fd4294, %fd4306, %fd538;
	sub.f64 	%fd4293, %fd4294, %fd4295;
	mul.f64 	%fd4292, %fd4306, %fd539;
	mul.f64 	%fd4291, %fd4308, %fd537;
	sub.f64 	%fd4290, %fd4291, %fd4292;
	mul.f64 	%fd4289, %fd4308, %fd538;
	mul.f64 	%fd4288, %fd4307, %fd539;
	sub.f64 	%fd4287, %fd4288, %fd4289;
	mul.f64 	%fd4286, %fd753, %fd753;
	div.rn.f64 	%fd3972, %fd3700, %fd748;
	add.f64 	%fd3973, %fd3972, 0d0000000000000000;
	mul.f64 	%fd3974, %fd758, 0d0000000000000000;
	div.rn.f64 	%fd3975, %fd3974, %fd748;
	sub.f64 	%fd3976, %fd3700, %fd3975;
	sub.f64 	%fd3977, %fd3700, %fd3973;
	fma.rn.f64 	%fd3978, %fd3977, %fd756, 0d0000000000000000;
	fma.rn.f64 	%fd3979, %fd3977, %fd4333, 0d0000000000000000;
	fma.rn.f64 	%fd3980, %fd753, %fd3978, %fd3976;
	fma.rn.f64 	%fd3981, %fd748, %fd3978, 0d0000000000000000;
	div.rn.f64 	%fd3982, %fd3979, %fd755;
	add.f64 	%fd3983, %fd3982, 0d0000000000000000;
	mul.f64 	%fd3984, %fd3979, %fd756;
	div.rn.f64 	%fd3985, %fd3984, %fd755;
	sub.f64 	%fd3986, %fd3700, %fd3985;
	sub.f64 	%fd3987, %fd3700, %fd3983;
	fma.rn.f64 	%fd3988, %fd752, %fd3987, %fd3981;
	fma.rn.f64 	%fd3989, %fd753, %fd3987, %fd3973;
	add.f64 	%fd3990, %fd3989, 0d0000000000000000;
	add.f64 	%fd3991, %fd3986, 0d0000000000000000;
	sub.f64 	%fd3992, %fd3700, %fd3986;
	fma.rn.f64 	%fd3993, %fd748, %fd3992, 0d0000000000000000;
	fma.rn.f64 	%fd3994, %fd4286, %fd3992, %fd3980;
	fma.rn.f64 	%fd3995, %fd753, %fd3993, %fd3988;
	fma.rn.f64 	%fd3996, %fd753, %fd3993, %fd3995;
	div.rn.f64 	%fd3997, %fd3996, %fd748;
	add.f64 	%fd3998, %fd3997, 0d0000000000000000;
	mul.f64 	%fd3999, %fd753, %fd3996;
	div.rn.f64 	%fd4000, %fd3999, %fd748;
	sub.f64 	%fd4001, %fd3994, %fd4000;
	add.f64 	%fd4002, %fd4001, 0d0000000000000000;
	add.f64 	%fd4003, %fd3983, %fd4558;
	add.f64 	%fd4004, %fd3990, %fd4557;
	add.f64 	%fd4005, %fd3991, %fd4562;
	add.f64 	%fd4006, %fd3998, %fd4560;
	add.f64 	%fd4007, %fd4002, %fd4559;
	add.f64 	%fd4008, %fd4003, 0d0000000000000000;
	fma.rn.f64 	%fd4009, %fd542, %fd4008, 0d0000000000000000;
	fma.rn.f64 	%fd4010, %fd544, %fd4008, 0d0000000000000000;
	fma.rn.f64 	%fd4011, %fd546, %fd4008, 0d0000000000000000;
	fma.rn.f64 	%fd4012, %fd4287, %fd4008, 0d0000000000000000;
	fma.rn.f64 	%fd4013, %fd4290, %fd4008, 0d0000000000000000;
	fma.rn.f64 	%fd4014, %fd4293, %fd4008, 0d0000000000000000;
	add.f64 	%fd4015, %fd4004, 0d0000000000000000;
	fma.rn.f64 	%fd4016, %fd542, %fd4015, 0d0000000000000000;
	fma.rn.f64 	%fd4017, %fd544, %fd4015, 0d0000000000000000;
	fma.rn.f64 	%fd4018, %fd546, %fd4015, 0d0000000000000000;
	fma.rn.f64 	%fd4019, %fd4306, %fd4015, %fd4012;
	fma.rn.f64 	%fd4020, %fd4307, %fd4015, %fd4013;
	fma.rn.f64 	%fd4021, %fd4308, %fd4015, %fd4014;
	add.f64 	%fd920, %fd4527, %fd4019;
	add.f64 	%fd921, %fd4526, %fd4020;
	add.f64 	%fd922, %fd4525, %fd4021;
	sub.f64 	%fd4022, %fd4518, %fd4019;
	sub.f64 	%fd4023, %fd4517, %fd4020;
	sub.f64 	%fd4024, %fd4516, %fd4021;
	add.f64 	%fd4025, %fd4005, 0d0000000000000000;
	fma.rn.f64 	%fd4026, %fd4293, %fd4025, 0d0000000000000000;
	add.f64 	%fd4027, %fd4011, %fd4026;
	add.f64 	%fd4028, %fd4026, %fd4027;
	fma.rn.f64 	%fd4029, %fd4290, %fd4025, 0d0000000000000000;
	add.f64 	%fd4030, %fd4010, %fd4029;
	add.f64 	%fd4031, %fd4029, %fd4030;
	fma.rn.f64 	%fd4032, %fd4287, %fd4025, 0d0000000000000000;
	add.f64 	%fd4033, %fd4009, %fd4032;
	add.f64 	%fd4034, %fd4032, %fd4033;
	add.f64 	%fd4035, %fd4006, 0d0000000000000000;
	add.f64 	%fd4036, %fd4561, %fd4035;
	add.f64 	%fd4037, %fd4036, 0d0000000000000000;
	fma.rn.f64 	%fd4038, %fd4293, %fd4037, 0d0000000000000000;
	fma.rn.f64 	%fd4039, %fd4308, %fd4037, 0d0000000000000000;
	add.f64 	%fd4040, %fd4039, %fd4028;
	add.f64 	%fd4041, %fd4018, %fd4038;
	fma.rn.f64 	%fd4042, %fd4290, %fd4037, 0d0000000000000000;
	fma.rn.f64 	%fd4043, %fd4307, %fd4037, 0d0000000000000000;
	add.f64 	%fd4044, %fd4043, %fd4031;
	add.f64 	%fd4045, %fd4017, %fd4042;
	fma.rn.f64 	%fd4046, %fd4287, %fd4037, 0d0000000000000000;
	fma.rn.f64 	%fd4047, %fd4306, %fd4037, 0d0000000000000000;
	add.f64 	%fd4048, %fd4047, %fd4034;
	add.f64 	%fd4049, %fd4016, %fd4046;
	add.f64 	%fd4050, %fd4007, 0d0000000000000000;
	fma.rn.f64 	%fd4051, %fd4308, %fd4050, 0d0000000000000000;
	add.f64 	%fd4052, %fd4051, %fd4041;
	add.f64 	%fd4053, %fd4051, %fd4052;
	fma.rn.f64 	%fd4054, %fd4307, %fd4050, 0d0000000000000000;
	add.f64 	%fd4055, %fd4054, %fd4045;
	add.f64 	%fd4056, %fd4054, %fd4055;
	fma.rn.f64 	%fd4057, %fd4306, %fd4050, 0d0000000000000000;
	add.f64 	%fd4058, %fd4057, %fd4049;
	add.f64 	%fd4059, %fd4057, %fd4058;
	mul.f64 	%fd4060, %fd538, %fd4040;
	mul.f64 	%fd4061, %fd539, %fd4044;
	sub.f64 	%fd4062, %fd4060, %fd4061;
	mul.f64 	%fd4063, %fd539, %fd4048;
	mul.f64 	%fd4064, %fd537, %fd4040;
	sub.f64 	%fd4065, %fd4063, %fd4064;
	mul.f64 	%fd4066, %fd537, %fd4044;
	mul.f64 	%fd4067, %fd538, %fd4048;
	sub.f64 	%fd4068, %fd4066, %fd4067;
	add.f64 	%fd4069, %fd4059, %fd4062;
	add.f64 	%fd4070, %fd4056, %fd4065;
	add.f64 	%fd4071, %fd4053, %fd4068;
	mul.f64 	%fd4072, %fd4307, %fd4040;
	mul.f64 	%fd4073, %fd4308, %fd4044;
	sub.f64 	%fd4074, %fd4072, %fd4073;
	mul.f64 	%fd4075, %fd4308, %fd4048;
	mul.f64 	%fd4076, %fd4306, %fd4040;
	sub.f64 	%fd4077, %fd4075, %fd4076;
	mul.f64 	%fd4078, %fd4306, %fd4044;
	mul.f64 	%fd4079, %fd4307, %fd4048;
	sub.f64 	%fd4080, %fd4078, %fd4079;
	sub.f64 	%fd4081, %fd4563, %fd4074;
	sub.f64 	%fd4082, %fd4564, %fd4077;
	sub.f64 	%fd4083, %fd4565, %fd4080;
	mul.f64 	%fd4084, %fd4309, %fd4083;
	mul.f64 	%fd4085, %fd4310, %fd4082;
	sub.f64 	%fd4086, %fd4084, %fd4085;
	mul.f64 	%fd4087, %fd4310, %fd4081;
	mul.f64 	%fd4088, %fd4311, %fd4083;
	sub.f64 	%fd4089, %fd4087, %fd4088;
	mul.f64 	%fd4090, %fd4311, %fd4082;
	mul.f64 	%fd4091, %fd4309, %fd4081;
	sub.f64 	%fd4092, %fd4090, %fd4091;
	add.f64 	%fd4093, %fd4069, %fd4086;
	add.f64 	%fd4094, %fd4070, %fd4089;
	add.f64 	%fd4095, %fd4071, %fd4092;
	mul.f64 	%fd4096, %fd4307, %fd4083;
	mul.f64 	%fd4097, %fd4308, %fd4082;
	mul.f64 	%fd4098, %fd4308, %fd4081;
	mul.f64 	%fd4099, %fd4306, %fd4083;
	mul.f64 	%fd4100, %fd4306, %fd4082;
	mul.f64 	%fd4101, %fd4307, %fd4081;
	sub.f64 	%fd4102, %fd4097, %fd4096;
	add.f64 	%fd4103, %fd4102, 0d0000000000000000;
	sub.f64 	%fd4104, %fd4099, %fd4098;
	add.f64 	%fd4105, %fd4104, 0d0000000000000000;
	sub.f64 	%fd4106, %fd4101, %fd4100;
	add.f64 	%fd4107, %fd4106, 0d0000000000000000;
	add.f64 	%fd923, %fd4524, %fd4103;
	add.f64 	%fd924, %fd4523, %fd4105;
	add.f64 	%fd925, %fd4107, %fd4522;
	sub.f64 	%fd4108, %fd4022, %fd4103;
	sub.f64 	%fd4109, %fd4023, %fd4105;
	sub.f64 	%fd4110, %fd4024, %fd4107;
	add.f64 	%fd926, %fd4093, %fd4521;
	add.f64 	%fd927, %fd4094, %fd4520;
	add.f64 	%fd928, %fd4095, %fd4519;
	sub.f64 	%fd929, %fd4108, %fd4093;
	sub.f64 	%fd930, %fd4109, %fd4094;
	sub.f64 	%fd931, %fd4110, %fd4095;
	add.f64 	%fd932, %fd735, 0d0000000000000000;
	setp.eq.s64 	%p283, %rd191, 0;
	@%p283 bra 	$L__BB1_368;

	mul.lo.s64 	%rd509, %rd408, %rd80;
	add.s64 	%rd507, %rd191, %rd509;
	// begin inline asm
	{ atom.add.f64 %fd4111,[%rd507],%fd932; }

	// end inline asm
	bra.uni 	$L__BB1_370;

$L__BB1_368:
	setp.eq.s64 	%p284, %rd162, 0;
	@%p284 bra 	$L__BB1_370;

	add.s64 	%rd510, %rd162, %rd437;
	// begin inline asm
	{ atom.add.f64 %fd4113,[%rd510],%fd932; }

	// end inline asm

$L__BB1_370:
	setp.eq.s64 	%p285, %rd187, 0;
	@%p285 bra 	$L__BB1_372;

	mul.lo.s64 	%rd515, %rd123, %rd81;
	add.s64 	%rd513, %rd187, %rd515;
	// begin inline asm
	{ atom.add.f64 %fd4115,[%rd513],%fd932; }

	// end inline asm
	bra.uni 	$L__BB1_374;

$L__BB1_372:
	setp.eq.s64 	%p286, %rd158, 0;
	@%p286 bra 	$L__BB1_374;

	add.s64 	%rd516, %rd158, %rd435;
	// begin inline asm
	{ atom.add.f64 %fd4117,[%rd516],%fd932; }

	// end inline asm

$L__BB1_374:
	setp.eq.s64 	%p287, %rd175, 0;
	add.f64 	%fd933, %fd732, 0d0000000000000000;
	@%p287 bra 	$L__BB1_376;

	mul.lo.s64 	%rd521, %rd96, %rd82;
	add.s64 	%rd519, %rd175, %rd521;
	mov.f64 	%fd4120, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4119,[%rd519],%fd4120; }

	// end inline asm
	add.s64 	%rd520, %rd519, 8;
	// begin inline asm
	{ atom.add.f64 %fd4121,[%rd520],%fd933; }

	// end inline asm
	bra.uni 	$L__BB1_378;

$L__BB1_376:
	setp.eq.s64 	%p288, %rd134, 0;
	@%p288 bra 	$L__BB1_378;

	add.s64 	%rd522, %rd134, %rd433;
	mov.f64 	%fd4124, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4123,[%rd522],%fd4124; }

	// end inline asm
	add.s64 	%rd523, %rd522, 8;
	// begin inline asm
	{ atom.add.f64 %fd4125,[%rd523],%fd933; }

	// end inline asm

$L__BB1_378:
	add.f64 	%fd934, %fd733, 0d0000000000000000;
	@%p287 bra 	$L__BB1_380;

	mul.lo.s64 	%rd527, %rd96, %rd82;
	add.s64 	%rd525, %rd175, %rd527;
	// begin inline asm
	{ atom.add.f64 %fd4127,[%rd525],%fd934; }

	// end inline asm
	add.s64 	%rd526, %rd525, 8;
	mov.f64 	%fd4130, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4129,[%rd526],%fd4130; }

	// end inline asm
	bra.uni 	$L__BB1_382;

$L__BB1_380:
	setp.eq.s64 	%p290, %rd134, 0;
	@%p290 bra 	$L__BB1_382;

	add.s64 	%rd528, %rd134, %rd433;
	// begin inline asm
	{ atom.add.f64 %fd4131,[%rd528],%fd934; }

	// end inline asm
	add.s64 	%rd529, %rd528, 8;
	mov.f64 	%fd4134, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd4133,[%rd529],%fd4134; }

	// end inline asm

$L__BB1_382:
	setp.eq.s64 	%p291, %rd181, 0;
	@%p291 bra 	$L__BB1_384;

	mul.lo.s64 	%rd535, %rd422, %rd83;
	add.s64 	%rd531, %rd181, %rd535;
	// begin inline asm
	{ atom.add.f64 %fd4135,[%rd531],%fd708; }

	// end inline asm
	add.s64 	%rd532, %rd531, 8;
	// begin inline asm
	{ atom.add.f64 %fd4137,[%rd532],%fd709; }

	// end inline asm
	add.s64 	%rd533, %rd531, 16;
	// begin inline asm
	{ atom.add.f64 %fd4139,[%rd533],%fd710; }

	// end inline asm
	bra.uni 	$L__BB1_386;

$L__BB1_384:
	setp.eq.s64 	%p292, %rd152, 0;
	@%p292 bra 	$L__BB1_386;

	add.s64 	%rd536, %rd152, %rd431;
	// begin inline asm
	{ atom.add.f64 %fd4141,[%rd536],%fd708; }

	// end inline asm
	add.s64 	%rd537, %rd536, 8;
	// begin inline asm
	{ atom.add.f64 %fd4143,[%rd537],%fd709; }

	// end inline asm
	add.s64 	%rd538, %rd536, 16;
	// begin inline asm
	{ atom.add.f64 %fd4145,[%rd538],%fd710; }

	// end inline asm

$L__BB1_386:
	@%p291 bra 	$L__BB1_388;

	mul.lo.s64 	%rd545, %rd419, %rd83;
	add.s64 	%rd541, %rd181, %rd545;
	// begin inline asm
	{ atom.add.f64 %fd4147,[%rd541],%fd714; }

	// end inline asm
	add.s64 	%rd542, %rd541, 8;
	// begin inline asm
	{ atom.add.f64 %fd4149,[%rd542],%fd715; }

	// end inline asm
	add.s64 	%rd543, %rd541, 16;
	// begin inline asm
	{ atom.add.f64 %fd4151,[%rd543],%fd716; }

	// end inline asm
	bra.uni 	$L__BB1_390;

$L__BB1_388:
	setp.eq.s64 	%p294, %rd152, 0;
	@%p294 bra 	$L__BB1_390;

	add.s64 	%rd546, %rd152, %rd429;
	// begin inline asm
	{ atom.add.f64 %fd4153,[%rd546],%fd714; }

	// end inline asm
	add.s64 	%rd547, %rd546, 8;
	// begin inline asm
	{ atom.add.f64 %fd4155,[%rd547],%fd715; }

	// end inline asm
	add.s64 	%rd548, %rd546, 16;
	// begin inline asm
	{ atom.add.f64 %fd4157,[%rd548],%fd716; }

	// end inline asm

$L__BB1_390:
	@%p291 bra 	$L__BB1_392;

	mul.lo.s64 	%rd555, %rd416, %rd83;
	add.s64 	%rd551, %rd181, %rd555;
	// begin inline asm
	{ atom.add.f64 %fd4159,[%rd551],%fd717; }

	// end inline asm
	add.s64 	%rd552, %rd551, 8;
	// begin inline asm
	{ atom.add.f64 %fd4161,[%rd552],%fd718; }

	// end inline asm
	add.s64 	%rd553, %rd551, 16;
	// begin inline asm
	{ atom.add.f64 %fd4163,[%rd553],%fd719; }

	// end inline asm
	bra.uni 	$L__BB1_394;

$L__BB1_392:
	setp.eq.s64 	%p296, %rd152, 0;
	@%p296 bra 	$L__BB1_398;

	add.s64 	%rd556, %rd152, %rd427;
	// begin inline asm
	{ atom.add.f64 %fd4165,[%rd556],%fd717; }

	// end inline asm
	add.s64 	%rd557, %rd556, 8;
	// begin inline asm
	{ atom.add.f64 %fd4167,[%rd557],%fd718; }

	// end inline asm
	add.s64 	%rd558, %rd556, 16;
	// begin inline asm
	{ atom.add.f64 %fd4169,[%rd558],%fd719; }

	// end inline asm

$L__BB1_394:
	@%p291 bra 	$L__BB1_396;

	mul.lo.s64 	%rd565, %rd413, %rd83;
	add.s64 	%rd561, %rd181, %rd565;
	// begin inline asm
	{ atom.add.f64 %fd4171,[%rd561],%fd711; }

	// end inline asm
	add.s64 	%rd562, %rd561, 8;
	// begin inline asm
	{ atom.add.f64 %fd4173,[%rd562],%fd712; }

	// end inline asm
	add.s64 	%rd563, %rd561, 16;
	// begin inline asm
	{ atom.add.f64 %fd4175,[%rd563],%fd713; }

	// end inline asm
	bra.uni 	$L__BB1_398;

$L__BB1_396:
	setp.eq.s64 	%p298, %rd152, 0;
	@%p298 bra 	$L__BB1_398;

	add.s64 	%rd566, %rd152, %rd425;
	// begin inline asm
	{ atom.add.f64 %fd4177,[%rd566],%fd711; }

	// end inline asm
	add.s64 	%rd567, %rd566, 8;
	// begin inline asm
	{ atom.add.f64 %fd4179,[%rd567],%fd712; }

	// end inline asm
	add.s64 	%rd568, %rd566, 16;
	// begin inline asm
	{ atom.add.f64 %fd4181,[%rd568],%fd713; }

	// end inline asm

$L__BB1_398:
	setp.eq.s64 	%p299, %rd179, 0;
	add.f64 	%fd935, %fd923, 0d0000000000000000;
	add.f64 	%fd936, %fd924, 0d0000000000000000;
	add.f64 	%fd937, %fd925, 0d0000000000000000;
	@%p299 bra 	$L__BB1_400;

	mul.lo.s64 	%rd575, %rd422, %rd84;
	add.s64 	%rd571, %rd179, %rd575;
	// begin inline asm
	{ atom.add.f64 %fd4183,[%rd571],%fd935; }

	// end inline asm
	add.s64 	%rd572, %rd571, 8;
	// begin inline asm
	{ atom.add.f64 %fd4185,[%rd572],%fd936; }

	// end inline asm
	add.s64 	%rd573, %rd571, 16;
	// begin inline asm
	{ atom.add.f64 %fd4187,[%rd573],%fd937; }

	// end inline asm
	bra.uni 	$L__BB1_402;

$L__BB1_400:
	setp.eq.s64 	%p300, %rd148, 0;
	@%p300 bra 	$L__BB1_402;

	add.s64 	%rd576, %rd148, %rd423;
	// begin inline asm
	{ atom.add.f64 %fd4189,[%rd576],%fd935; }

	// end inline asm
	add.s64 	%rd577, %rd576, 8;
	// begin inline asm
	{ atom.add.f64 %fd4191,[%rd577],%fd936; }

	// end inline asm
	add.s64 	%rd578, %rd576, 16;
	// begin inline asm
	{ atom.add.f64 %fd4193,[%rd578],%fd937; }

	// end inline asm

$L__BB1_402:
	add.f64 	%fd938, %fd926, 0d0000000000000000;
	add.f64 	%fd939, %fd927, 0d0000000000000000;
	add.f64 	%fd940, %fd928, 0d0000000000000000;
	@%p299 bra 	$L__BB1_404;

	mul.lo.s64 	%rd585, %rd419, %rd84;
	add.s64 	%rd581, %rd179, %rd585;
	// begin inline asm
	{ atom.add.f64 %fd4195,[%rd581],%fd938; }

	// end inline asm
	add.s64 	%rd582, %rd581, 8;
	// begin inline asm
	{ atom.add.f64 %fd4197,[%rd582],%fd939; }

	// end inline asm
	add.s64 	%rd583, %rd581, 16;
	// begin inline asm
	{ atom.add.f64 %fd4199,[%rd583],%fd940; }

	// end inline asm
	bra.uni 	$L__BB1_406;

$L__BB1_404:
	setp.eq.s64 	%p302, %rd148, 0;
	@%p302 bra 	$L__BB1_406;

	add.s64 	%rd586, %rd148, %rd420;
	// begin inline asm
	{ atom.add.f64 %fd4201,[%rd586],%fd938; }

	// end inline asm
	add.s64 	%rd587, %rd586, 8;
	// begin inline asm
	{ atom.add.f64 %fd4203,[%rd587],%fd939; }

	// end inline asm
	add.s64 	%rd588, %rd586, 16;
	// begin inline asm
	{ atom.add.f64 %fd4205,[%rd588],%fd940; }

	// end inline asm

$L__BB1_406:
	add.f64 	%fd941, %fd929, 0d0000000000000000;
	add.f64 	%fd942, %fd930, 0d0000000000000000;
	add.f64 	%fd943, %fd931, 0d0000000000000000;
	@%p299 bra 	$L__BB1_408;

	mul.lo.s64 	%rd595, %rd416, %rd84;
	add.s64 	%rd591, %rd179, %rd595;
	// begin inline asm
	{ atom.add.f64 %fd4207,[%rd591],%fd941; }

	// end inline asm
	add.s64 	%rd592, %rd591, 8;
	// begin inline asm
	{ atom.add.f64 %fd4209,[%rd592],%fd942; }

	// end inline asm
	add.s64 	%rd593, %rd591, 16;
	// begin inline asm
	{ atom.add.f64 %fd4211,[%rd593],%fd943; }

	// end inline asm
	bra.uni 	$L__BB1_410;

$L__BB1_408:
	setp.eq.s64 	%p304, %rd148, 0;
	@%p304 bra 	$L__BB1_410;

	add.s64 	%rd596, %rd148, %rd417;
	// begin inline asm
	{ atom.add.f64 %fd4213,[%rd596],%fd941; }

	// end inline asm
	add.s64 	%rd597, %rd596, 8;
	// begin inline asm
	{ atom.add.f64 %fd4215,[%rd597],%fd942; }

	// end inline asm
	add.s64 	%rd598, %rd596, 16;
	// begin inline asm
	{ atom.add.f64 %fd4217,[%rd598],%fd943; }

	// end inline asm

$L__BB1_410:
	add.f64 	%fd944, %fd920, 0d0000000000000000;
	add.f64 	%fd945, %fd921, 0d0000000000000000;
	add.f64 	%fd946, %fd922, 0d0000000000000000;
	@%p299 bra 	$L__BB1_412;

	mul.lo.s64 	%rd605, %rd413, %rd84;
	add.s64 	%rd601, %rd179, %rd605;
	// begin inline asm
	{ atom.add.f64 %fd4219,[%rd601],%fd944; }

	// end inline asm
	add.s64 	%rd602, %rd601, 8;
	// begin inline asm
	{ atom.add.f64 %fd4221,[%rd602],%fd945; }

	// end inline asm
	add.s64 	%rd603, %rd601, 16;
	// begin inline asm
	{ atom.add.f64 %fd4223,[%rd603],%fd946; }

	// end inline asm
	bra.uni 	$L__BB1_414;

$L__BB1_412:
	setp.eq.s64 	%p306, %rd148, 0;
	@%p306 bra 	$L__BB1_414;

	add.s64 	%rd606, %rd148, %rd414;
	// begin inline asm
	{ atom.add.f64 %fd4225,[%rd606],%fd944; }

	// end inline asm
	add.s64 	%rd607, %rd606, 8;
	// begin inline asm
	{ atom.add.f64 %fd4227,[%rd607],%fd945; }

	// end inline asm
	add.s64 	%rd608, %rd606, 16;
	// begin inline asm
	{ atom.add.f64 %fd4229,[%rd608],%fd946; }

	// end inline asm
	bra.uni 	$L__BB1_414;

$L__BB1_33:
	setp.eq.s32 	%p38, %r923, 6;
	@%p38 bra 	$L__BB1_37;
	bra.uni 	$L__BB1_34;

$L__BB1_37:
	sub.f64 	%fd1058, %fd27, %fd31;
	sub.f64 	%fd1059, %fd39, %fd41;
	mul.f64 	%fd1060, %fd65, %fd1059;
	sub.f64 	%fd1061, %fd34, %fd36;
	mul.f64 	%fd1062, %fd1061, %fd66;
	sub.f64 	%fd1063, %fd1060, %fd1062;
	mul.f64 	%fd1064, %fd1058, %fd66;
	mul.f64 	%fd1065, %fd64, %fd1059;
	sub.f64 	%fd1066, %fd1064, %fd1065;
	mul.f64 	%fd1067, %fd64, %fd1061;
	mul.f64 	%fd1068, %fd1058, %fd65;
	sub.f64 	%fd1069, %fd1067, %fd1068;
	mul.f64 	%fd1070, %fd1066, %fd1066;
	fma.rn.f64 	%fd1071, %fd1063, %fd1063, %fd1070;
	fma.rn.f64 	%fd1072, %fd1069, %fd1069, %fd1071;
	div.rn.f64 	%fd4357, %fd1072, %fd67;
	bra.uni 	$L__BB1_44;

$L__BB1_34:
	setp.eq.s32 	%p39, %r923, 7;
	@%p39 bra 	$L__BB1_36;
	bra.uni 	$L__BB1_35;

$L__BB1_36:
	sub.f64 	%fd1040, %fd26, %fd32;
	sub.f64 	%fd1041, %fd39, %fd42;
	sub.f64 	%fd1042, %fd33, %fd37;
	mul.f64 	%fd1043, %fd1042, %fd1041;
	sub.f64 	%fd1044, %fd34, %fd37;
	sub.f64 	%fd1045, %fd38, %fd42;
	mul.f64 	%fd1046, %fd1044, %fd1045;
	sub.f64 	%fd1047, %fd1043, %fd1046;
	sub.f64 	%fd1048, %fd27, %fd32;
	mul.f64 	%fd1049, %fd1048, %fd1045;
	mul.f64 	%fd1050, %fd1040, %fd1041;
	sub.f64 	%fd1051, %fd1049, %fd1050;
	mul.f64 	%fd1052, %fd1040, %fd1044;
	mul.f64 	%fd1053, %fd1048, %fd1042;
	sub.f64 	%fd1054, %fd1052, %fd1053;
	mul.f64 	%fd1055, %fd1051, %fd1051;
	fma.rn.f64 	%fd1056, %fd1047, %fd1047, %fd1055;
	fma.rn.f64 	%fd1057, %fd1054, %fd1054, %fd1056;
	div.rn.f64 	%fd4357, %fd1057, %fd67;
	bra.uni 	$L__BB1_44;

$L__BB1_35:
	sub.f64 	%fd1021, %fd31, %fd26;
	mul.f64 	%fd1022, %fd60, %fd62;
	mul.f64 	%fd1023, %fd59, %fd63;
	sub.f64 	%fd1024, %fd1023, %fd1022;
	mul.f64 	%fd1025, %fd58, %fd63;
	mul.f64 	%fd1026, %fd60, %fd61;
	sub.f64 	%fd1027, %fd1026, %fd1025;
	mul.f64 	%fd1028, %fd59, %fd61;
	mul.f64 	%fd1029, %fd58, %fd62;
	sub.f64 	%fd1030, %fd1029, %fd1028;
	sub.f64 	%fd1031, %fd36, %fd33;
	mul.f64 	%fd1032, %fd1031, %fd1027;
	fma.rn.f64 	%fd1033, %fd1021, %fd1024, %fd1032;
	sub.f64 	%fd1034, %fd41, %fd38;
	fma.rn.f64 	%fd1035, %fd1034, %fd1030, %fd1033;
	mul.f64 	%fd1036, %fd1035, %fd1035;
	mul.f64 	%fd1037, %fd1027, %fd1027;
	fma.rn.f64 	%fd1038, %fd1024, %fd1024, %fd1037;
	fma.rn.f64 	%fd1039, %fd1030, %fd1030, %fd1038;
	div.rn.f64 	%fd4357, %fd1036, %fd1039;

$L__BB1_44:
	cvt.s64.s32 	%rd614, %r564;
	mul.f64 	%fd1127, %fd23, %fd23;
	sub.f64 	%fd97, %fd4357, %fd1127;
	mul.lo.s64 	%rd115, %rd103, %rd614;
	add.s64 	%rd226, %rd46, %rd115;
	mul.lo.s64 	%rd116, %rd105, %rd614;
	add.s64 	%rd227, %rd46, %rd116;
	mul.lo.s64 	%rd117, %rd107, %rd614;
	add.s64 	%rd228, %rd46, %rd117;
	mul.lo.s64 	%rd118, %rd109, %rd614;
	add.s64 	%rd229, %rd46, %rd118;
	ld.global.f64 	%fd1128, [%rd227];
	ld.global.f64 	%fd1129, [%rd226];
	sub.f64 	%fd98, %fd1128, %fd1129;
	ld.global.f64 	%fd1130, [%rd227+8];
	ld.global.f64 	%fd1131, [%rd226+8];
	sub.f64 	%fd99, %fd1130, %fd1131;
	ld.global.f64 	%fd1132, [%rd227+16];
	ld.global.f64 	%fd1133, [%rd226+16];
	sub.f64 	%fd100, %fd1132, %fd1133;
	ld.global.f64 	%fd1134, [%rd229];
	ld.global.f64 	%fd1135, [%rd228];
	sub.f64 	%fd101, %fd1134, %fd1135;
	ld.global.f64 	%fd1136, [%rd229+8];
	ld.global.f64 	%fd1137, [%rd228+8];
	sub.f64 	%fd102, %fd1136, %fd1137;
	ld.global.f64 	%fd1138, [%rd229+16];
	ld.global.f64 	%fd1139, [%rd228+16];
	sub.f64 	%fd103, %fd1138, %fd1139;
	mul.f64 	%fd1140, %fd99, %fd99;
	fma.rn.f64 	%fd1141, %fd98, %fd98, %fd1140;
	fma.rn.f64 	%fd1142, %fd100, %fd100, %fd1141;
	mul.f64 	%fd104, %fd1142, 0d3F50624DE0000000;
	mul.f64 	%fd1143, %fd102, %fd102;
	fma.rn.f64 	%fd1144, %fd101, %fd101, %fd1143;
	fma.rn.f64 	%fd105, %fd103, %fd103, %fd1144;
	mul.f64 	%fd106, %fd104, %fd105;
	mul.f64 	%fd1145, %fd60, %fd62;
	mul.f64 	%fd1146, %fd59, %fd63;
	sub.f64 	%fd107, %fd1146, %fd1145;
	mul.f64 	%fd1147, %fd58, %fd63;
	mul.f64 	%fd1148, %fd60, %fd61;
	sub.f64 	%fd108, %fd1148, %fd1147;
	mul.f64 	%fd1149, %fd59, %fd61;
	mul.f64 	%fd1150, %fd58, %fd62;
	sub.f64 	%fd109, %fd1150, %fd1149;
	mul.f64 	%fd1151, %fd108, %fd108;
	fma.rn.f64 	%fd1152, %fd107, %fd107, %fd1151;
	fma.rn.f64 	%fd110, %fd109, %fd109, %fd1152;
	setp.geu.f64 	%p40, %fd110, %fd106;
	mov.f64 	%fd4358, 0d3FF0000000000000;
	@%p40 bra 	$L__BB1_46;

	div.rn.f64 	%fd1153, %fd110, %fd106;
	mov.f64 	%fd1154, 0d0000000000000000;
	sub.f64 	%fd1155, %fd1154, %fd1153;
	add.f64 	%fd1156, %fd1155, 0d4000000000000000;
	mul.f64 	%fd4358, %fd1153, %fd1156;

$L__BB1_46:
	fma.rn.f64 	%fd113, %fd2, %fd23, %fd1;
	mul.lo.s64 	%rd119, %rd99, %rd68;
	add.s64 	%rd120, %rd41, %rd119;
	mul.lo.s64 	%rd121, %rd100, %rd68;
	add.s64 	%rd122, %rd41, %rd121;
	ld.global.f64 	%fd114, [%rd122];
	ld.global.f64 	%fd115, [%rd120];
	add.f64 	%fd1158, %fd115, %fd114;
	mul.f64 	%fd1159, %fd1158, %fd965;
	mul.f64 	%fd116, %fd1159, %fd962;
	mul.f64 	%fd117, %fd4358, %fd116;
	setp.geu.f64 	%p41, %fd97, %fd113;
	@%p41 bra 	$L__BB1_55;

	div.rn.f64 	%fd4359, %fd97, %fd113;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r924}, %fd4359;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r925, %temp}, %fd4359;
	}
	setp.gt.s32 	%p42, %r924, 1048575;
	mov.u32 	%r926, -1023;
	@%p42 bra 	$L__BB1_49;

	mul.f64 	%fd4359, %fd4359, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r924}, %fd4359;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r925, %temp}, %fd4359;
	}
	mov.u32 	%r926, -1077;

$L__BB1_49:
	add.s32 	%r760, %r924, -1;
	setp.lt.u32 	%p43, %r760, 2146435071;
	@%p43 bra 	$L__BB1_51;
	bra.uni 	$L__BB1_50;

$L__BB1_51:
	shr.u32 	%r762, %r924, 20;
	add.s32 	%r927, %r926, %r762;
	and.b32  	%r763, %r924, -2146435073;
	or.b32  	%r764, %r763, 1072693248;
	mov.b64 	%fd4360, {%r925, %r764};
	setp.lt.s32 	%p45, %r764, 1073127583;
	@%p45 bra 	$L__BB1_53;

	{
	.reg .b32 %temp;
	mov.b64 	{%r765, %temp}, %fd4360;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r766}, %fd4360;
	}
	add.s32 	%r767, %r766, -1048576;
	mov.b64 	%fd4360, {%r765, %r767};
	add.s32 	%r927, %r927, 1;

$L__BB1_53:
	add.f64 	%fd1162, %fd4360, 0d3FF0000000000000;
	mov.f64 	%fd1163, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1164, %fd1162;
	neg.f64 	%fd1165, %fd1162;
	fma.rn.f64 	%fd1166, %fd1165, %fd1164, %fd1163;
	fma.rn.f64 	%fd1167, %fd1166, %fd1166, %fd1166;
	fma.rn.f64 	%fd1168, %fd1167, %fd1164, %fd1164;
	add.f64 	%fd1169, %fd4360, 0dBFF0000000000000;
	mul.f64 	%fd1170, %fd1169, %fd1168;
	fma.rn.f64 	%fd1171, %fd1169, %fd1168, %fd1170;
	mul.f64 	%fd1172, %fd1171, %fd1171;
	mov.f64 	%fd1173, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1174, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1175, %fd1174, %fd1172, %fd1173;
	mov.f64 	%fd1176, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1177, %fd1175, %fd1172, %fd1176;
	mov.f64 	%fd1178, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1179, %fd1177, %fd1172, %fd1178;
	mov.f64 	%fd1180, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1181, %fd1179, %fd1172, %fd1180;
	mov.f64 	%fd1182, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1183, %fd1181, %fd1172, %fd1182;
	mov.f64 	%fd1184, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1185, %fd1183, %fd1172, %fd1184;
	mov.f64 	%fd1186, 0d3FB5555555555554;
	fma.rn.f64 	%fd1187, %fd1185, %fd1172, %fd1186;
	sub.f64 	%fd1188, %fd1169, %fd1171;
	add.f64 	%fd1189, %fd1188, %fd1188;
	neg.f64 	%fd1190, %fd1171;
	fma.rn.f64 	%fd1191, %fd1190, %fd1169, %fd1189;
	mul.f64 	%fd1192, %fd1168, %fd1191;
	mul.f64 	%fd1193, %fd1172, %fd1187;
	fma.rn.f64 	%fd1194, %fd1193, %fd1171, %fd1192;
	xor.b32  	%r768, %r927, -2147483648;
	mov.u32 	%r769, -2147483648;
	mov.u32 	%r770, 1127219200;
	mov.b64 	%fd1195, {%r768, %r770};
	mov.b64 	%fd1196, {%r769, %r770};
	sub.f64 	%fd1197, %fd1195, %fd1196;
	mov.f64 	%fd1198, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1199, %fd1197, %fd1198, %fd1171;
	neg.f64 	%fd1200, %fd1197;
	fma.rn.f64 	%fd1201, %fd1200, %fd1198, %fd1199;
	sub.f64 	%fd1202, %fd1201, %fd1171;
	sub.f64 	%fd1203, %fd1194, %fd1202;
	mov.f64 	%fd1204, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1205, %fd1197, %fd1204, %fd1203;
	add.f64 	%fd4361, %fd1199, %fd1205;
	bra.uni 	$L__BB1_54;

$L__BB1_50:
	mov.f64 	%fd1160, 0d7FF0000000000000;
	fma.rn.f64 	%fd1161, %fd4359, %fd1160, %fd1160;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r761}, %fd4359;
	}
	mov.b32 	%f1, %r761;
	setp.eq.f32 	%p44, %f1, 0f00000000;
	selp.f64 	%fd4361, 0dFFF0000000000000, %fd1161, %p44;

$L__BB1_54:
	sub.f64 	%fd1206, %fd97, %fd113;
	div.rn.f64 	%fd1207, %fd1206, %fd113;
	mul.f64 	%fd1208, %fd4, %fd1207;
	mul.f64 	%fd1209, %fd1207, %fd1208;
	mul.f64 	%fd4362, %fd1209, %fd4361;

$L__BB1_55:
	setp.lt.f64 	%p46, %fd97, %fd113;
	selp.f64 	%fd129, %fd4362, 0d0000000000000000, %p46;
	mul.f64 	%fd1210, %fd117, %fd129;
	mul.f64 	%fd130, %fd1210, %fd966;
	setp.num.f64 	%p47, %fd130, %fd130;
	@%p47 bra 	$L__BB1_66;

	@%p41 bra 	$L__BB1_65;

	div.rn.f64 	%fd4363, %fd97, %fd113;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r928}, %fd4363;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r929, %temp}, %fd4363;
	}
	setp.gt.s32 	%p49, %r928, 1048575;
	mov.u32 	%r930, -1023;
	@%p49 bra 	$L__BB1_59;

	mul.f64 	%fd4363, %fd4363, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r928}, %fd4363;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r929, %temp}, %fd4363;
	}
	mov.u32 	%r930, -1077;

$L__BB1_59:
	add.s32 	%r773, %r928, -1;
	setp.lt.u32 	%p50, %r773, 2146435071;
	@%p50 bra 	$L__BB1_61;
	bra.uni 	$L__BB1_60;

$L__BB1_61:
	shr.u32 	%r775, %r928, 20;
	add.s32 	%r931, %r930, %r775;
	and.b32  	%r776, %r928, -2146435073;
	or.b32  	%r777, %r776, 1072693248;
	mov.b64 	%fd4364, {%r929, %r777};
	setp.lt.s32 	%p52, %r777, 1073127583;
	@%p52 bra 	$L__BB1_63;

	{
	.reg .b32 %temp;
	mov.b64 	{%r778, %temp}, %fd4364;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r779}, %fd4364;
	}
	add.s32 	%r780, %r779, -1048576;
	mov.b64 	%fd4364, {%r778, %r780};
	add.s32 	%r931, %r931, 1;

$L__BB1_63:
	add.f64 	%fd1214, %fd4364, 0d3FF0000000000000;
	mov.f64 	%fd1215, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1216, %fd1214;
	neg.f64 	%fd1217, %fd1214;
	fma.rn.f64 	%fd1218, %fd1217, %fd1216, %fd1215;
	fma.rn.f64 	%fd1219, %fd1218, %fd1218, %fd1218;
	fma.rn.f64 	%fd1220, %fd1219, %fd1216, %fd1216;
	add.f64 	%fd1221, %fd4364, 0dBFF0000000000000;
	mul.f64 	%fd1222, %fd1221, %fd1220;
	fma.rn.f64 	%fd1223, %fd1221, %fd1220, %fd1222;
	mul.f64 	%fd1224, %fd1223, %fd1223;
	mov.f64 	%fd1225, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1226, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1227, %fd1226, %fd1224, %fd1225;
	mov.f64 	%fd1228, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1229, %fd1227, %fd1224, %fd1228;
	mov.f64 	%fd1230, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1231, %fd1229, %fd1224, %fd1230;
	mov.f64 	%fd1232, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1233, %fd1231, %fd1224, %fd1232;
	mov.f64 	%fd1234, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1235, %fd1233, %fd1224, %fd1234;
	mov.f64 	%fd1236, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1237, %fd1235, %fd1224, %fd1236;
	mov.f64 	%fd1238, 0d3FB5555555555554;
	fma.rn.f64 	%fd1239, %fd1237, %fd1224, %fd1238;
	sub.f64 	%fd1240, %fd1221, %fd1223;
	add.f64 	%fd1241, %fd1240, %fd1240;
	neg.f64 	%fd1242, %fd1223;
	fma.rn.f64 	%fd1243, %fd1242, %fd1221, %fd1241;
	mul.f64 	%fd1244, %fd1220, %fd1243;
	mul.f64 	%fd1245, %fd1224, %fd1239;
	fma.rn.f64 	%fd1246, %fd1245, %fd1223, %fd1244;
	xor.b32  	%r781, %r931, -2147483648;
	mov.u32 	%r782, -2147483648;
	mov.u32 	%r783, 1127219200;
	mov.b64 	%fd1247, {%r781, %r783};
	mov.b64 	%fd1248, {%r782, %r783};
	sub.f64 	%fd1249, %fd1247, %fd1248;
	mov.f64 	%fd1250, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1251, %fd1249, %fd1250, %fd1223;
	neg.f64 	%fd1252, %fd1249;
	fma.rn.f64 	%fd1253, %fd1252, %fd1250, %fd1251;
	sub.f64 	%fd1254, %fd1253, %fd1223;
	sub.f64 	%fd1255, %fd1246, %fd1254;
	mov.f64 	%fd1256, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1257, %fd1249, %fd1256, %fd1255;
	add.f64 	%fd4365, %fd1251, %fd1257;
	bra.uni 	$L__BB1_64;

$L__BB1_60:
	mov.f64 	%fd1212, 0d7FF0000000000000;
	fma.rn.f64 	%fd1213, %fd4363, %fd1212, %fd1212;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r774}, %fd4363;
	}
	mov.b32 	%f2, %r774;
	setp.eq.f32 	%p51, %f2, 0f00000000;
	selp.f64 	%fd4365, 0dFFF0000000000000, %fd1213, %p51;

$L__BB1_64:
	sub.f64 	%fd1258, %fd97, %fd113;
	div.rn.f64 	%fd1259, %fd1258, %fd113;
	mul.f64 	%fd1260, %fd4, %fd1259;
	mul.f64 	%fd1261, %fd1259, %fd1260;
	mul.f64 	%fd4366, %fd1261, %fd4365;

$L__BB1_65:
	add.u64 	%rd230, %SP, 16;
	add.u64 	%rd231, %SPL, 16;
	st.local.v2.f64 	[%rd231], {%fd115, %fd114};
	selp.f64 	%fd1262, %fd4366, 0d0000000000000000, %p46;
	st.local.v2.f64 	[%rd231+16], {%fd1262, %fd97};
	st.local.v2.f64 	[%rd231+32], {%fd113, %fd23};
	mov.u64 	%rd232, $str$4;
	cvta.global.u64 	%rd233, %rd232;
	{ // callseq 254, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd233;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd230;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r784, [retval0+0];
	} // callseq 254

$L__BB1_66:
	setp.eq.s32 	%p54, %r342, 0;
	@%p54 bra 	$L__BB1_68;

	mul.lo.s64 	%rd234, %rd96, %rd69;
	add.s64 	%rd235, %rd33, %rd234;
	ld.global.u32 	%r977, [%rd235];
	cvt.s64.s32 	%rd236, %r977;
	mul.lo.s64 	%rd237, %rd236, %rd70;
	add.s64 	%rd238, %rd29, %rd237;
	mul.lo.s64 	%rd239, %rd96, %rd71;
	add.s64 	%rd240, %rd32, %rd239;
	ld.global.u32 	%r976, [%rd240];
	cvt.s64.s32 	%rd241, %r976;
	mul.lo.s64 	%rd242, %rd241, %rd70;
	add.s64 	%rd243, %rd29, %rd242;
	ld.global.f64 	%fd4569, [%rd238];
	add.f64 	%fd1263, %fd4569, %fd4569;
	ld.global.f64 	%fd4568, [%rd243];
	mul.f64 	%fd1264, %fd1263, %fd4568;
	add.f64 	%fd1265, %fd4569, %fd4568;
	setp.neu.f64 	%p55, %fd1265, 0d0000000000000000;
	div.rn.f64 	%fd1266, %fd1264, %fd1265;
	selp.f64 	%fd4570, %fd1266, 0d0000000000000000, %p55;
	mul.lo.s64 	%rd244, %rd96, %rd72;
	add.s64 	%rd245, %rd38, %rd244;
	ld.global.f64 	%fd4567, [%rd245];
	mul.f64 	%fd1267, %fd4567, %fd4570;
	mul.lo.s64 	%rd246, %rd96, %rd73;
	add.s64 	%rd247, %rd36, %rd246;
	ld.global.f64 	%fd4378, [%rd247];
	ld.global.f64 	%fd4379, [%rd247+8];
	ld.global.f64 	%fd4380, [%rd247+16];
	mul.f64 	%fd4566, %fd1267, %fd966;
	mov.u32 	%r975, %r9;
	bra.uni 	$L__BB1_78;

$L__BB1_68:
	ld.global.f64 	%fd1269, [%rd122];
	ld.global.f64 	%fd1270, [%rd120];
	add.f64 	%fd1271, %fd1270, %fd1269;
	mul.f64 	%fd1272, %fd1271, %fd965;
	mul.f64 	%fd4573, %fd1272, %fd962;
	@%p41 bra 	$L__BB1_77;

	div.rn.f64 	%fd4367, %fd97, %fd113;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r932}, %fd4367;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r933, %temp}, %fd4367;
	}
	setp.gt.s32 	%p57, %r932, 1048575;
	mov.u32 	%r934, -1023;
	@%p57 bra 	$L__BB1_71;

	mul.f64 	%fd4367, %fd4367, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r932}, %fd4367;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r933, %temp}, %fd4367;
	}
	mov.u32 	%r934, -1077;

$L__BB1_71:
	add.s32 	%r787, %r932, -1;
	setp.lt.u32 	%p58, %r787, 2146435071;
	@%p58 bra 	$L__BB1_73;
	bra.uni 	$L__BB1_72;

$L__BB1_73:
	shr.u32 	%r789, %r932, 20;
	add.s32 	%r935, %r934, %r789;
	and.b32  	%r790, %r932, -2146435073;
	or.b32  	%r791, %r790, 1072693248;
	mov.b64 	%fd4368, {%r933, %r791};
	setp.lt.s32 	%p60, %r791, 1073127583;
	@%p60 bra 	$L__BB1_75;

	{
	.reg .b32 %temp;
	mov.b64 	{%r792, %temp}, %fd4368;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r793}, %fd4368;
	}
	add.s32 	%r794, %r793, -1048576;
	mov.b64 	%fd4368, {%r792, %r794};
	add.s32 	%r935, %r935, 1;

$L__BB1_75:
	add.f64 	%fd1275, %fd4368, 0d3FF0000000000000;
	mov.f64 	%fd1276, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1277, %fd1275;
	neg.f64 	%fd1278, %fd1275;
	fma.rn.f64 	%fd1279, %fd1278, %fd1277, %fd1276;
	fma.rn.f64 	%fd1280, %fd1279, %fd1279, %fd1279;
	fma.rn.f64 	%fd1281, %fd1280, %fd1277, %fd1277;
	add.f64 	%fd1282, %fd4368, 0dBFF0000000000000;
	mul.f64 	%fd1283, %fd1282, %fd1281;
	fma.rn.f64 	%fd1284, %fd1282, %fd1281, %fd1283;
	mul.f64 	%fd1285, %fd1284, %fd1284;
	mov.f64 	%fd1286, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1287, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1288, %fd1287, %fd1285, %fd1286;
	mov.f64 	%fd1289, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1290, %fd1288, %fd1285, %fd1289;
	mov.f64 	%fd1291, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1292, %fd1290, %fd1285, %fd1291;
	mov.f64 	%fd1293, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1294, %fd1292, %fd1285, %fd1293;
	mov.f64 	%fd1295, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1296, %fd1294, %fd1285, %fd1295;
	mov.f64 	%fd1297, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1298, %fd1296, %fd1285, %fd1297;
	mov.f64 	%fd1299, 0d3FB5555555555554;
	fma.rn.f64 	%fd1300, %fd1298, %fd1285, %fd1299;
	sub.f64 	%fd1301, %fd1282, %fd1284;
	add.f64 	%fd1302, %fd1301, %fd1301;
	neg.f64 	%fd1303, %fd1284;
	fma.rn.f64 	%fd1304, %fd1303, %fd1282, %fd1302;
	mul.f64 	%fd1305, %fd1281, %fd1304;
	mul.f64 	%fd1306, %fd1285, %fd1300;
	fma.rn.f64 	%fd1307, %fd1306, %fd1284, %fd1305;
	xor.b32  	%r795, %r935, -2147483648;
	mov.u32 	%r796, -2147483648;
	mov.u32 	%r797, 1127219200;
	mov.b64 	%fd1308, {%r795, %r797};
	mov.b64 	%fd1309, {%r796, %r797};
	sub.f64 	%fd1310, %fd1308, %fd1309;
	mov.f64 	%fd1311, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1312, %fd1310, %fd1311, %fd1284;
	neg.f64 	%fd1313, %fd1310;
	fma.rn.f64 	%fd1314, %fd1313, %fd1311, %fd1312;
	sub.f64 	%fd1315, %fd1314, %fd1284;
	sub.f64 	%fd1316, %fd1307, %fd1315;
	mov.f64 	%fd1317, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1318, %fd1310, %fd1317, %fd1316;
	add.f64 	%fd4369, %fd1312, %fd1318;
	bra.uni 	$L__BB1_76;

$L__BB1_72:
	mov.f64 	%fd1273, 0d7FF0000000000000;
	fma.rn.f64 	%fd1274, %fd4367, %fd1273, %fd1273;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r788}, %fd4367;
	}
	mov.b32 	%f3, %r788;
	setp.eq.f32 	%p59, %f3, 0f00000000;
	selp.f64 	%fd4369, 0dFFF0000000000000, %fd1274, %p59;

$L__BB1_76:
	sub.f64 	%fd1319, %fd97, %fd113;
	div.rn.f64 	%fd1320, %fd1319, %fd113;
	mul.f64 	%fd1321, %fd4, %fd1320;
	mul.f64 	%fd1322, %fd1320, %fd1321;
	mul.f64 	%fd4370, %fd1322, %fd4369;

$L__BB1_77:
	selp.f64 	%fd4571, %fd4370, 0d0000000000000000, %p46;
	mov.f64 	%fd4378, 0d0000000000000000;
	mul.f64 	%fd4572, %fd4358, %fd4573;
	mov.u32 	%r978, %r9;
	mov.f64 	%fd4379, %fd4378;
	mov.f64 	%fd4380, %fd4378;

$L__BB1_78:
	@%p54 bra 	$L__BB1_101;

	setp.eq.s64 	%p63, %rd171, 0;
	@%p63 bra 	$L__BB1_81;

	cvta.to.global.u64 	%rd248, %rd171;
	cvt.s64.s32 	%rd249, %r975;
	mul.lo.s64 	%rd250, %rd249, %rd65;
	add.s64 	%rd251, %rd248, %rd250;
	ld.global.f64 	%fd1326, [%rd251];
	add.f64 	%fd4382, %fd1326, 0d0000000000000000;
	bra.uni 	$L__BB1_83;

$L__BB1_101:
	setp.eq.s64 	%p75, %rd171, 0;
	@%p75 bra 	$L__BB1_103;

	cvta.to.global.u64 	%rd280, %rd171;
	cvt.s64.s32 	%rd281, %r978;
	mul.lo.s64 	%rd282, %rd281, %rd65;
	add.s64 	%rd283, %rd280, %rd282;
	ld.global.f64 	%fd1501, [%rd283];
	add.f64 	%fd4388, %fd1501, 0d0000000000000000;
	bra.uni 	$L__BB1_105;

$L__BB1_81:
	setp.eq.s64 	%p64, %rd130, 0;
	mov.f64 	%fd4382, 0d0000000000000000;
	@%p64 bra 	$L__BB1_83;

	cvta.to.global.u64 	%rd252, %rd130;
	cvt.s64.s32 	%rd253, %r975;
	mul.lo.s64 	%rd254, %rd253, %rd66;
	add.s64 	%rd255, %rd252, %rd254;
	ld.global.f64 	%fd1328, [%rd255];
	add.f64 	%fd4382, %fd1328, 0d0000000000000000;

$L__BB1_83:
	mov.f64 	%fd4285, 0d3FF0000000000000;
	sub.f64 	%fd4284, %fd4285, %fd24;
	mul.f64 	%fd4283, %fd38, %fd4284;
	fma.rn.f64 	%fd4282, %fd39, %fd24, %fd4283;
	mul.f64 	%fd4281, %fd53, %fd4284;
	fma.rn.f64 	%fd4280, %fd54, %fd24, %fd4281;
	mul.f64 	%fd4279, %fd33, %fd4284;
	fma.rn.f64 	%fd4278, %fd34, %fd24, %fd4279;
	mul.f64 	%fd4277, %fd48, %fd4284;
	fma.rn.f64 	%fd4276, %fd49, %fd24, %fd4277;
	mul.f64 	%fd4275, %fd26, %fd4284;
	fma.rn.f64 	%fd4274, %fd27, %fd24, %fd4275;
	mul.f64 	%fd4273, %fd43, %fd4284;
	fma.rn.f64 	%fd4272, %fd44, %fd24, %fd4273;
	mov.f64 	%fd4269, 0d3FF0000000000000;
	sub.f64 	%fd4268, %fd4269, %fd29;
	mul.f64 	%fd1329, %fd4378, %fd4378;
	mov.f64 	%fd1330, 0d3FF0000000000000;
	sub.f64 	%fd178, %fd1330, %fd1329;
	mul.f64 	%fd1331, %fd4378, %fd4379;
	mov.f64 	%fd1332, 0d0000000000000000;
	sub.f64 	%fd179, %fd1332, %fd1331;
	mul.f64 	%fd1333, %fd4378, %fd4380;
	sub.f64 	%fd180, %fd1332, %fd1333;
	mul.f64 	%fd1334, %fd4379, %fd4379;
	sub.f64 	%fd181, %fd1330, %fd1334;
	mul.f64 	%fd1335, %fd4379, %fd4380;
	sub.f64 	%fd182, %fd1332, %fd1335;
	mul.f64 	%fd1336, %fd4380, %fd4380;
	sub.f64 	%fd183, %fd1330, %fd1336;
	mul.f64 	%fd1337, %fd47, %fd29;
	mul.f64 	%fd1338, %fd46, %fd4268;
	sub.f64 	%fd1339, %fd4272, %fd1338;
	sub.f64 	%fd1340, %fd1339, %fd1337;
	mul.f64 	%fd1341, %fd32, %fd29;
	mul.f64 	%fd1342, %fd31, %fd4268;
	sub.f64 	%fd1343, %fd4274, %fd1342;
	sub.f64 	%fd1344, %fd1343, %fd1341;
	sub.f64 	%fd184, %fd1344, %fd1340;
	mul.f64 	%fd1345, %fd52, %fd29;
	mul.f64 	%fd1346, %fd51, %fd4268;
	sub.f64 	%fd1347, %fd4276, %fd1346;
	sub.f64 	%fd1348, %fd1347, %fd1345;
	mul.f64 	%fd1349, %fd37, %fd29;
	mul.f64 	%fd1350, %fd36, %fd4268;
	sub.f64 	%fd1351, %fd4278, %fd1350;
	sub.f64 	%fd1352, %fd1351, %fd1349;
	sub.f64 	%fd185, %fd1352, %fd1348;
	mul.f64 	%fd1353, %fd185, %fd179;
	mul.f64 	%fd1354, %fd185, %fd181;
	mul.f64 	%fd1355, %fd185, %fd182;
	fma.rn.f64 	%fd1356, %fd184, %fd178, %fd1353;
	fma.rn.f64 	%fd1357, %fd184, %fd179, %fd1354;
	fma.rn.f64 	%fd1358, %fd184, %fd180, %fd1355;
	mul.f64 	%fd1359, %fd57, %fd29;
	mul.f64 	%fd1360, %fd56, %fd4268;
	sub.f64 	%fd1361, %fd4280, %fd1360;
	sub.f64 	%fd1362, %fd1361, %fd1359;
	mul.f64 	%fd1363, %fd42, %fd29;
	mul.f64 	%fd1364, %fd41, %fd4268;
	sub.f64 	%fd1365, %fd4282, %fd1364;
	sub.f64 	%fd1366, %fd1365, %fd1363;
	sub.f64 	%fd186, %fd1366, %fd1362;
	fma.rn.f64 	%fd1367, %fd186, %fd180, %fd1356;
	fma.rn.f64 	%fd1368, %fd186, %fd182, %fd1357;
	fma.rn.f64 	%fd1369, %fd186, %fd183, %fd1358;
	div.rn.f64 	%fd187, %fd1367, %fd963;
	div.rn.f64 	%fd188, %fd1368, %fd963;
	div.rn.f64 	%fd189, %fd1369, %fd963;
	mul.f64 	%fd1370, %fd188, %fd188;
	fma.rn.f64 	%fd1371, %fd187, %fd187, %fd1370;
	fma.rn.f64 	%fd1372, %fd189, %fd189, %fd1371;
	sqrt.rn.f64 	%fd190, %fd1372;
	setp.ge.f64 	%p65, %fd190, %fd967;
	mul.f64 	%fd4383, %fd190, %fd963;
	@%p65 bra 	$L__BB1_85;

	mov.f64 	%fd4296, 0d0000000000000000;
	mul.f64 	%fd4271, %fd190, %fd963;
	mul.f64 	%fd1373, %fd4271, %fd4271;
	sub.f64 	%fd1375, %fd4296, %fd4271;
	div.rn.f64 	%fd1376, %fd1375, 0d4008000000000000;
	add.f64 	%fd1377, %fd5, %fd1376;
	mul.f64 	%fd1378, %fd1373, %fd1377;
	div.rn.f64 	%fd1379, %fd1378, %fd6;
	add.f64 	%fd4383, %fd7, %fd1379;

$L__BB1_85:
	add.f64 	%fd1380, %fd4382, 0d0000000000000000;
	fma.rn.f64 	%fd194, %fd1380, %fd4383, 0d0000000000000000;
	fma.rn.f64 	%fd4384, %fd4566, %fd1380, 0d0000000000000000;
	@%p65 bra 	$L__BB1_87;

	mul.f64 	%fd4270, %fd190, %fd963;
	mul.f64 	%fd1381, %fd4270, %fd4270;
	mov.f64 	%fd1382, 0d0000000000000000;
	sub.f64 	%fd1383, %fd1382, %fd4270;
	div.rn.f64 	%fd1384, %fd1383, 0d4008000000000000;
	add.f64 	%fd1385, %fd5, %fd1384;
	div.rn.f64 	%fd1386, %fd4384, %fd6;
	add.f64 	%fd1387, %fd1386, 0d0000000000000000;
	fma.rn.f64 	%fd1388, %fd1387, %fd1385, 0d0000000000000000;
	fma.rn.f64 	%fd1389, %fd1381, %fd1387, 0d0000000000000000;
	div.rn.f64 	%fd1390, %fd1389, 0d4008000000000000;
	add.f64 	%fd1391, %fd1390, 0d0000000000000000;
	sub.f64 	%fd1392, %fd1382, %fd1391;
	fma.rn.f64 	%fd1393, %fd4270, %fd1388, %fd1392;
	fma.rn.f64 	%fd4384, %fd4270, %fd1388, %fd1393;

$L__BB1_87:
	fma.rn.f64 	%fd198, %fd4384, %fd963, 0d0000000000000000;
	mov.f64 	%fd1396, 0d0000000000000000;
	setp.leu.f64 	%p67, %fd190, 0d0000000000000000;
	mov.f64 	%fd4385, %fd1396;
	mov.f64 	%fd4386, %fd1396;
	mov.f64 	%fd4387, %fd1396;
	@%p67 bra 	$L__BB1_89;

	div.rn.f64 	%fd1397, %fd187, %fd190;
	div.rn.f64 	%fd1398, %fd188, %fd190;
	div.rn.f64 	%fd1399, %fd189, %fd190;
	fma.rn.f64 	%fd4387, %fd1397, %fd198, 0d0000000000000000;
	fma.rn.f64 	%fd4386, %fd1398, %fd198, 0d0000000000000000;
	fma.rn.f64 	%fd4385, %fd1399, %fd198, 0d0000000000000000;

$L__BB1_89:
	mov.f64 	%fd4263, 0d3FF0000000000000;
	mul.f64 	%fd4262, %fd4380, %fd4380;
	sub.f64 	%fd4261, %fd4263, %fd4262;
	mul.f64 	%fd4260, %fd4379, %fd4380;
	sub.f64 	%fd4259, %fd1332, %fd4260;
	mul.f64 	%fd4258, %fd4379, %fd4379;
	sub.f64 	%fd4257, %fd4263, %fd4258;
	mul.f64 	%fd4256, %fd4378, %fd4380;
	sub.f64 	%fd4255, %fd1332, %fd4256;
	mul.f64 	%fd4254, %fd4378, %fd4379;
	sub.f64 	%fd4253, %fd1332, %fd4254;
	mul.f64 	%fd4252, %fd4378, %fd4378;
	sub.f64 	%fd4251, %fd4263, %fd4252;
	div.rn.f64 	%fd1400, %fd4387, %fd963;
	add.f64 	%fd1401, %fd1400, 0d0000000000000000;
	div.rn.f64 	%fd1403, %fd4386, %fd963;
	add.f64 	%fd1404, %fd1403, 0d0000000000000000;
	div.rn.f64 	%fd1405, %fd4385, %fd963;
	add.f64 	%fd1406, %fd1405, 0d0000000000000000;
	fma.rn.f64 	%fd1407, %fd184, %fd1401, 0d0000000000000000;
	fma.rn.f64 	%fd1408, %fd185, %fd1401, 0d0000000000000000;
	fma.rn.f64 	%fd1409, %fd186, %fd1401, 0d0000000000000000;
	fma.rn.f64 	%fd1410, %fd184, %fd1404, 0d0000000000000000;
	fma.rn.f64 	%fd1411, %fd185, %fd1404, 0d0000000000000000;
	fma.rn.f64 	%fd1412, %fd186, %fd1404, 0d0000000000000000;
	fma.rn.f64 	%fd1413, %fd184, %fd1406, 0d0000000000000000;
	fma.rn.f64 	%fd1414, %fd185, %fd1406, 0d0000000000000000;
	fma.rn.f64 	%fd1415, %fd186, %fd1406, 0d0000000000000000;
	mul.f64 	%fd1416, %fd4251, %fd1401;
	mul.f64 	%fd1417, %fd4253, %fd1401;
	mul.f64 	%fd1418, %fd4255, %fd1401;
	fma.rn.f64 	%fd1419, %fd4253, %fd1404, %fd1416;
	fma.rn.f64 	%fd1420, %fd4257, %fd1404, %fd1417;
	fma.rn.f64 	%fd1421, %fd4259, %fd1404, %fd1418;
	fma.rn.f64 	%fd1422, %fd4255, %fd1406, %fd1419;
	fma.rn.f64 	%fd1423, %fd4259, %fd1406, %fd1420;
	fma.rn.f64 	%fd1424, %fd4261, %fd1406, %fd1421;
	add.f64 	%fd4400, %fd1422, 0d0000000000000000;
	add.f64 	%fd4399, %fd1423, 0d0000000000000000;
	add.f64 	%fd4398, %fd1424, 0d0000000000000000;
	sub.f64 	%fd1425, %fd1396, %fd1407;
	sub.f64 	%fd1426, %fd1396, %fd1410;
	sub.f64 	%fd1427, %fd1396, %fd1413;
	sub.f64 	%fd1428, %fd1396, %fd1408;
	sub.f64 	%fd1429, %fd1396, %fd1411;
	sub.f64 	%fd1430, %fd1396, %fd1414;
	sub.f64 	%fd1431, %fd1396, %fd1409;
	sub.f64 	%fd1432, %fd1396, %fd1412;
	sub.f64 	%fd1433, %fd1396, %fd1415;
	mul.f64 	%fd1434, %fd4378, %fd1425;
	mul.f64 	%fd1435, %fd4378, %fd1431;
	mul.f64 	%fd1436, %fd4379, %fd1429;
	fma.rn.f64 	%fd1437, %fd4379, %fd1426, %fd1434;
	fma.rn.f64 	%fd1438, %fd4378, %fd1428, %fd1436;
	fma.rn.f64 	%fd1439, %fd4379, %fd1432, %fd1435;
	fma.rn.f64 	%fd1440, %fd4380, %fd1427, %fd1437;
	fma.rn.f64 	%fd1441, %fd4380, %fd1430, %fd1438;
	fma.rn.f64 	%fd1442, %fd4380, %fd1433, %fd1439;
	add.f64 	%fd1443, %fd1440, 0d0000000000000000;
	add.f64 	%fd1444, %fd1441, 0d0000000000000000;
	add.f64 	%fd1445, %fd1442, 0d0000000000000000;
	mul.f64 	%fd1446, %fd4379, %fd1428;
	mul.f64 	%fd1447, %fd4379, %fd1430;
	fma.rn.f64 	%fd1448, %fd4378, %fd1425, %fd1446;
	fma.rn.f64 	%fd1449, %fd4378, %fd1426, %fd1436;
	fma.rn.f64 	%fd1450, %fd4378, %fd1427, %fd1447;
	fma.rn.f64 	%fd1451, %fd4380, %fd1431, %fd1448;
	fma.rn.f64 	%fd1452, %fd4380, %fd1432, %fd1449;
	fma.rn.f64 	%fd1453, %fd4380, %fd1433, %fd1450;
	add.f64 	%fd208, %fd1451, %fd1443;
	add.f64 	%fd209, %fd1452, %fd1444;
	add.f64 	%fd210, %fd1453, %fd1445;
	setp.eq.s64 	%p68, %rd177, 0;
	@%p68 bra 	$L__BB1_91;

	mul.lo.s64 	%rd259, %rd96, %rd76;
	add.s64 	%rd256, %rd177, %rd259;
	// begin inline asm
	{ atom.add.f64 %fd1454,[%rd256],%fd208; }

	// end inline asm
	add.s64 	%rd257, %rd256, 8;
	// begin inline asm
	{ atom.add.f64 %fd1456,[%rd257],%fd209; }

	// end inline asm
	add.s64 	%rd258, %rd256, 16;
	// begin inline asm
	{ atom.add.f64 %fd1458,[%rd258],%fd210; }

	// end inline asm
	bra.uni 	$L__BB1_93;

$L__BB1_91:
	setp.eq.s64 	%p69, %rd136, 0;
	@%p69 bra 	$L__BB1_93;

	mul.lo.s64 	%rd263, %rd96, %rd73;
	add.s64 	%rd260, %rd136, %rd263;
	// begin inline asm
	{ atom.add.f64 %fd1460,[%rd260],%fd208; }

	// end inline asm
	add.s64 	%rd261, %rd260, 8;
	// begin inline asm
	{ atom.add.f64 %fd1462,[%rd261],%fd209; }

	// end inline asm
	add.s64 	%rd262, %rd260, 16;
	// begin inline asm
	{ atom.add.f64 %fd1464,[%rd262],%fd210; }

	// end inline asm

$L__BB1_93:
	setp.eq.s64 	%p70, %rd173, 0;
	fma.rn.f64 	%fd1466, %fd194, %fd966, 0d0000000000000000;
	fma.rn.f64 	%fd211, %fd4567, %fd1466, 0d0000000000000000;
	fma.rn.f64 	%fd212, %fd4570, %fd1466, 0d0000000000000000;
	@%p70 bra 	$L__BB1_95;

	mul.lo.s64 	%rd265, %rd96, %rd78;
	add.s64 	%rd264, %rd173, %rd265;
	// begin inline asm
	{ atom.add.f64 %fd1467,[%rd264],%fd212; }

	// end inline asm
	bra.uni 	$L__BB1_97;

$L__BB1_95:
	setp.eq.s64 	%p71, %rd132, 0;
	@%p71 bra 	$L__BB1_97;

	mul.lo.s64 	%rd267, %rd96, %rd72;
	add.s64 	%rd266, %rd132, %rd267;
	// begin inline asm
	{ atom.add.f64 %fd1469,[%rd266],%fd212; }

	// end inline asm

$L__BB1_97:
	mov.f64 	%fd4404, 0d0000000000000000;
	sub.f64 	%fd4403, %fd4404, %fd4400;
	sub.f64 	%fd4402, %fd4404, %fd4399;
	sub.f64 	%fd4401, %fd4404, %fd4398;
	setp.eq.s64 	%p72, %rd185, 0;
	add.f64 	%fd1472, %fd4569, %fd4569;
	mul.f64 	%fd1473, %fd4568, %fd1472;
	add.f64 	%fd1474, %fd4568, %fd4569;
	setp.neu.f64 	%p73, %fd1474, 0d0000000000000000;
	div.rn.f64 	%fd1475, %fd1473, %fd1474;
	selp.f64 	%fd1476, %fd211, 0d0000000000000000, %p73;
	div.rn.f64 	%fd1477, %fd1476, %fd1474;
	add.f64 	%fd1478, %fd1477, 0d0000000000000000;
	mul.f64 	%fd1479, %fd1475, %fd1476;
	div.rn.f64 	%fd1480, %fd1479, %fd1474;
	sub.f64 	%fd1481, %fd4404, %fd1480;
	add.f64 	%fd1482, %fd1481, 0d0000000000000000;
	fma.rn.f64 	%fd1483, %fd4568, %fd1478, 0d0000000000000000;
	fma.rn.f64 	%fd216, %fd1472, %fd1478, %fd1482;
	fma.rn.f64 	%fd217, %fd1483, 0d4000000000000000, %fd1482;
	@%p72 bra 	$L__BB1_99;

	cvt.s64.s32 	%rd270, %r976;
	mul.lo.s64 	%rd271, %rd270, %rd79;
	add.s64 	%rd268, %rd185, %rd271;
	// begin inline asm
	{ atom.add.f64 %fd1484,[%rd268],%fd216; }

	// end inline asm
	cvt.s64.s32 	%rd272, %r977;
	mul.lo.s64 	%rd273, %rd272, %rd79;
	add.s64 	%rd269, %rd185, %rd273;
	// begin inline asm
	{ atom.add.f64 %fd1486,[%rd269],%fd217; }

	// end inline asm
	mov.f64 	%fd4405, %fd4404;
	mov.f64 	%fd4406, %fd4404;
	bra.uni 	$L__BB1_119;

$L__BB1_99:
	setp.eq.s64 	%p74, %rd156, 0;
	mov.f64 	%fd4405, %fd4404;
	mov.f64 	%fd4406, %fd4404;
	@%p74 bra 	$L__BB1_119;

	cvt.s64.s32 	%rd276, %r976;
	mul.lo.s64 	%rd277, %rd276, %rd70;
	add.s64 	%rd274, %rd156, %rd277;
	// begin inline asm
	{ atom.add.f64 %fd1494,[%rd274],%fd216; }

	// end inline asm
	cvt.s64.s32 	%rd278, %r977;
	mul.lo.s64 	%rd279, %rd278, %rd70;
	add.s64 	%rd275, %rd156, %rd279;
	// begin inline asm
	{ atom.add.f64 %fd1496,[%rd275],%fd217; }

	// end inline asm
	mov.f64 	%fd4405, %fd4404;
	mov.f64 	%fd4406, %fd4404;
	bra.uni 	$L__BB1_119;

$L__BB1_103:
	setp.eq.s64 	%p76, %rd130, 0;
	mov.f64 	%fd4388, 0d0000000000000000;
	@%p76 bra 	$L__BB1_105;

	cvta.to.global.u64 	%rd284, %rd130;
	cvt.s64.s32 	%rd285, %r978;
	mul.lo.s64 	%rd286, %rd285, %rd66;
	add.s64 	%rd287, %rd284, %rd286;
	ld.global.f64 	%fd1503, [%rd287];
	add.f64 	%fd4388, %fd1503, 0d0000000000000000;

$L__BB1_105:
	@%p41 bra 	$L__BB1_113;

	sub.f64 	%fd1506, %fd97, %fd113;
	div.rn.f64 	%fd4391, %fd1506, %fd113;
	mul.f64 	%fd4392, %fd4, %fd4391;
	div.rn.f64 	%fd4394, %fd97, %fd113;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r940}, %fd4394;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r941, %temp}, %fd4394;
	}
	setp.gt.s32 	%p78, %r940, 1048575;
	mov.u32 	%r942, -1023;
	mov.f64 	%fd4389, %fd4394;
	@%p78 bra 	$L__BB1_108;

	mul.f64 	%fd4389, %fd4394, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r940}, %fd4389;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r941, %temp}, %fd4389;
	}
	mov.u32 	%r942, -1077;

$L__BB1_108:
	mul.f64 	%fd4393, %fd4391, %fd4392;
	add.s32 	%r800, %r940, -1;
	setp.lt.u32 	%p79, %r800, 2146435071;
	@%p79 bra 	$L__BB1_110;
	bra.uni 	$L__BB1_109;

$L__BB1_110:
	shr.u32 	%r802, %r940, 20;
	add.s32 	%r943, %r942, %r802;
	and.b32  	%r803, %r940, -2146435073;
	or.b32  	%r804, %r803, 1072693248;
	mov.b64 	%fd4390, {%r941, %r804};
	setp.lt.s32 	%p81, %r804, 1073127583;
	@%p81 bra 	$L__BB1_112;

	{
	.reg .b32 %temp;
	mov.b64 	{%r805, %temp}, %fd4390;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r806}, %fd4390;
	}
	add.s32 	%r807, %r806, -1048576;
	mov.b64 	%fd4390, {%r805, %r807};
	add.s32 	%r943, %r943, 1;

$L__BB1_112:
	add.f64 	%fd1509, %fd4390, 0d3FF0000000000000;
	mov.f64 	%fd1510, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1511, %fd1509;
	neg.f64 	%fd1512, %fd1509;
	fma.rn.f64 	%fd1513, %fd1512, %fd1511, %fd1510;
	fma.rn.f64 	%fd1514, %fd1513, %fd1513, %fd1513;
	fma.rn.f64 	%fd1515, %fd1514, %fd1511, %fd1511;
	add.f64 	%fd1516, %fd4390, 0dBFF0000000000000;
	mul.f64 	%fd1517, %fd1516, %fd1515;
	fma.rn.f64 	%fd1518, %fd1516, %fd1515, %fd1517;
	mul.f64 	%fd1519, %fd1518, %fd1518;
	mov.f64 	%fd1520, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1521, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1522, %fd1521, %fd1519, %fd1520;
	mov.f64 	%fd1523, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1524, %fd1522, %fd1519, %fd1523;
	mov.f64 	%fd1525, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1526, %fd1524, %fd1519, %fd1525;
	mov.f64 	%fd1527, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1528, %fd1526, %fd1519, %fd1527;
	mov.f64 	%fd1529, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1530, %fd1528, %fd1519, %fd1529;
	mov.f64 	%fd1531, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1532, %fd1530, %fd1519, %fd1531;
	mov.f64 	%fd1533, 0d3FB5555555555554;
	fma.rn.f64 	%fd1534, %fd1532, %fd1519, %fd1533;
	sub.f64 	%fd1535, %fd1516, %fd1518;
	add.f64 	%fd1536, %fd1535, %fd1535;
	neg.f64 	%fd1537, %fd1518;
	fma.rn.f64 	%fd1538, %fd1537, %fd1516, %fd1536;
	mul.f64 	%fd1539, %fd1515, %fd1538;
	mul.f64 	%fd1540, %fd1519, %fd1534;
	fma.rn.f64 	%fd1541, %fd1540, %fd1518, %fd1539;
	xor.b32  	%r808, %r943, -2147483648;
	mov.u32 	%r809, -2147483648;
	mov.u32 	%r810, 1127219200;
	mov.b64 	%fd1542, {%r808, %r810};
	mov.b64 	%fd1543, {%r809, %r810};
	sub.f64 	%fd1544, %fd1542, %fd1543;
	mov.f64 	%fd1545, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1546, %fd1544, %fd1545, %fd1518;
	neg.f64 	%fd1547, %fd1544;
	fma.rn.f64 	%fd1548, %fd1547, %fd1545, %fd1546;
	sub.f64 	%fd1549, %fd1548, %fd1518;
	sub.f64 	%fd1550, %fd1541, %fd1549;
	mov.f64 	%fd1551, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1552, %fd1544, %fd1551, %fd1550;
	add.f64 	%fd4395, %fd1546, %fd1552;
	bra.uni 	$L__BB1_113;

$L__BB1_109:
	mov.f64 	%fd1507, 0d7FF0000000000000;
	fma.rn.f64 	%fd1508, %fd4389, %fd1507, %fd1507;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r801}, %fd4389;
	}
	mov.b32 	%f4, %r801;
	setp.eq.f32 	%p80, %f4, 0f00000000;
	selp.f64 	%fd4395, 0dFFF0000000000000, %fd1508, %p80;

$L__BB1_113:
	fma.rn.f64 	%fd4335, %fd4388, %fd966, 0d0000000000000000;
	fma.rn.f64 	%fd4334, %fd4572, %fd4335, 0d0000000000000000;
	selp.f64 	%fd239, %fd4334, 0d0000000000000000, %p46;
	mov.f64 	%fd4398, 0d0000000000000000;
	mov.f64 	%fd4405, %fd4398;
	mov.f64 	%fd4406, %fd4398;
	@%p41 bra 	$L__BB1_115;

	fma.rn.f64 	%fd1555, %fd239, %fd4395, 0d0000000000000000;
	mov.f64 	%fd1556, 0d0000000000000000;
	fma.rn.f64 	%fd1557, %fd239, %fd4393, 0d0000000000000000;
	rcp.rn.f64 	%fd1558, %fd4394;
	fma.rn.f64 	%fd1559, %fd1558, %fd1557, 0d0000000000000000;
	div.rn.f64 	%fd1560, %fd1559, %fd113;
	add.f64 	%fd1561, %fd1560, 0d0000000000000000;
	mul.f64 	%fd1562, %fd4394, %fd1559;
	div.rn.f64 	%fd1563, %fd1562, %fd113;
	sub.f64 	%fd1564, %fd1556, %fd1563;
	fma.rn.f64 	%fd1565, %fd4391, %fd1555, 0d0000000000000000;
	fma.rn.f64 	%fd1566, %fd4392, %fd1555, 0d0000000000000000;
	div.rn.f64 	%fd1567, %fd1566, %fd113;
	add.f64 	%fd1568, %fd1567, 0d0000000000000000;
	mul.f64 	%fd1569, %fd4391, %fd1566;
	div.rn.f64 	%fd1570, %fd1569, %fd113;
	sub.f64 	%fd1571, %fd1564, %fd1570;
	fma.rn.f64 	%fd1572, %fd4, %fd1565, 0d0000000000000000;
	div.rn.f64 	%fd1573, %fd1572, %fd113;
	add.f64 	%fd1574, %fd1568, %fd1573;
	mul.f64 	%fd1575, %fd4391, %fd1572;
	div.rn.f64 	%fd1576, %fd1575, %fd113;
	sub.f64 	%fd1577, %fd1571, %fd1576;
	add.f64 	%fd4405, %fd1561, %fd1574;
	sub.f64 	%fd4406, %fd1577, %fd1574;

$L__BB1_115:
	fma.rn.f64 	%fd4337, %fd4388, %fd966, 0d0000000000000000;
	fma.rn.f64 	%fd4336, %fd4571, %fd4337, 0d0000000000000000;
	fma.rn.f64 	%fd1578, %fd4358, %fd4336, 0d0000000000000000;
	fma.rn.f64 	%fd4404, %fd4573, %fd4336, 0d0000000000000000;
	fma.rn.f64 	%fd1579, %fd1578, %fd962, 0d0000000000000000;
	fma.rn.f64 	%fd245, %fd1579, %fd965, 0d0000000000000000;
	setp.eq.s64 	%p84, %rd195, 0;
	@%p84 bra 	$L__BB1_117;

	mul.lo.s64 	%rd290, %rd100, %rd75;
	add.s64 	%rd288, %rd195, %rd290;
	// begin inline asm
	{ atom.add.f64 %fd1580,[%rd288],%fd245; }

	// end inline asm
	mul.lo.s64 	%rd291, %rd99, %rd75;
	add.s64 	%rd289, %rd195, %rd291;
	// begin inline asm
	{ atom.add.f64 %fd1582,[%rd289],%fd245; }

	// end inline asm
	mov.f64 	%fd4399, %fd4398;
	mov.f64 	%fd4400, %fd4398;
	mov.f64 	%fd4401, %fd4398;
	mov.f64 	%fd4402, %fd4398;
	mov.f64 	%fd4403, %fd4398;
	bra.uni 	$L__BB1_119;

$L__BB1_117:
	setp.eq.s64 	%p85, %rd166, 0;
	mov.f64 	%fd4399, %fd4398;
	mov.f64 	%fd4400, %fd4398;
	mov.f64 	%fd4401, %fd4398;
	mov.f64 	%fd4402, %fd4398;
	mov.f64 	%fd4403, %fd4398;
	@%p85 bra 	$L__BB1_119;

	add.s64 	%rd292, %rd166, %rd121;
	// begin inline asm
	{ atom.add.f64 %fd1596,[%rd292],%fd245; }

	// end inline asm
	add.s64 	%rd293, %rd166, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd1598,[%rd293],%fd245; }

	// end inline asm
	mov.f64 	%fd4399, %fd4398;
	mov.f64 	%fd4400, %fd4398;
	mov.f64 	%fd4401, %fd4398;
	mov.f64 	%fd4402, %fd4398;
	mov.f64 	%fd4403, %fd4398;

$L__BB1_119:
	@%p47 bra 	$L__BB1_133;

	@%p41 bra 	$L__BB1_129;

	div.rn.f64 	%fd255, %fd97, %fd113;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r944}, %fd255;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r945, %temp}, %fd255;
	}
	setp.gt.s32 	%p88, %r944, 1048575;
	mov.u32 	%r946, -1023;
	mov.f64 	%fd4407, %fd255;
	@%p88 bra 	$L__BB1_123;

	mul.f64 	%fd4407, %fd255, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r944}, %fd4407;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r945, %temp}, %fd4407;
	}
	mov.u32 	%r946, -1077;

$L__BB1_123:
	add.s32 	%r813, %r944, -1;
	setp.lt.u32 	%p89, %r813, 2146435071;
	@%p89 bra 	$L__BB1_125;
	bra.uni 	$L__BB1_124;

$L__BB1_125:
	shr.u32 	%r815, %r944, 20;
	add.s32 	%r947, %r946, %r815;
	and.b32  	%r816, %r944, -2146435073;
	or.b32  	%r817, %r816, 1072693248;
	mov.b64 	%fd4408, {%r945, %r817};
	setp.lt.s32 	%p91, %r817, 1073127583;
	@%p91 bra 	$L__BB1_127;

	{
	.reg .b32 %temp;
	mov.b64 	{%r818, %temp}, %fd4408;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r819}, %fd4408;
	}
	add.s32 	%r820, %r819, -1048576;
	mov.b64 	%fd4408, {%r818, %r820};
	add.s32 	%r947, %r947, 1;

$L__BB1_127:
	add.f64 	%fd1608, %fd4408, 0d3FF0000000000000;
	mov.f64 	%fd1609, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1610, %fd1608;
	neg.f64 	%fd1611, %fd1608;
	fma.rn.f64 	%fd1612, %fd1611, %fd1610, %fd1609;
	fma.rn.f64 	%fd1613, %fd1612, %fd1612, %fd1612;
	fma.rn.f64 	%fd1614, %fd1613, %fd1610, %fd1610;
	add.f64 	%fd1615, %fd4408, 0dBFF0000000000000;
	mul.f64 	%fd1616, %fd1615, %fd1614;
	fma.rn.f64 	%fd1617, %fd1615, %fd1614, %fd1616;
	mul.f64 	%fd1618, %fd1617, %fd1617;
	mov.f64 	%fd1619, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1620, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1621, %fd1620, %fd1618, %fd1619;
	mov.f64 	%fd1622, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1623, %fd1621, %fd1618, %fd1622;
	mov.f64 	%fd1624, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1625, %fd1623, %fd1618, %fd1624;
	mov.f64 	%fd1626, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1627, %fd1625, %fd1618, %fd1626;
	mov.f64 	%fd1628, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1629, %fd1627, %fd1618, %fd1628;
	mov.f64 	%fd1630, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1631, %fd1629, %fd1618, %fd1630;
	mov.f64 	%fd1632, 0d3FB5555555555554;
	fma.rn.f64 	%fd1633, %fd1631, %fd1618, %fd1632;
	sub.f64 	%fd1634, %fd1615, %fd1617;
	add.f64 	%fd1635, %fd1634, %fd1634;
	neg.f64 	%fd1636, %fd1617;
	fma.rn.f64 	%fd1637, %fd1636, %fd1615, %fd1635;
	mul.f64 	%fd1638, %fd1614, %fd1637;
	mul.f64 	%fd1639, %fd1618, %fd1633;
	fma.rn.f64 	%fd1640, %fd1639, %fd1617, %fd1638;
	xor.b32  	%r821, %r947, -2147483648;
	mov.u32 	%r822, -2147483648;
	mov.u32 	%r823, 1127219200;
	mov.b64 	%fd1641, {%r821, %r823};
	mov.b64 	%fd1642, {%r822, %r823};
	sub.f64 	%fd1643, %fd1641, %fd1642;
	mov.f64 	%fd1644, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1645, %fd1643, %fd1644, %fd1617;
	neg.f64 	%fd1646, %fd1643;
	fma.rn.f64 	%fd1647, %fd1646, %fd1644, %fd1645;
	sub.f64 	%fd1648, %fd1647, %fd1617;
	sub.f64 	%fd1649, %fd1640, %fd1648;
	mov.f64 	%fd1650, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1651, %fd1643, %fd1650, %fd1649;
	add.f64 	%fd4409, %fd1645, %fd1651;
	bra.uni 	$L__BB1_128;

$L__BB1_124:
	mov.f64 	%fd1606, 0d7FF0000000000000;
	fma.rn.f64 	%fd1607, %fd4407, %fd1606, %fd1606;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r814}, %fd4407;
	}
	mov.b32 	%f5, %r814;
	setp.eq.f32 	%p90, %f5, 0f00000000;
	selp.f64 	%fd4409, 0dFFF0000000000000, %fd1607, %p90;

$L__BB1_128:
	sub.f64 	%fd1652, %fd97, %fd113;
	div.rn.f64 	%fd1653, %fd1652, %fd113;
	mul.f64 	%fd1654, %fd4, %fd1653;
	fma.rn.f64 	%fd1655, %fd4409, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd1656, %fd1653, %fd1654;
	fma.rn.f64 	%fd1657, %fd1656, 0d0000000000000000, 0d0000000000000000;
	rcp.rn.f64 	%fd1658, %fd255;
	fma.rn.f64 	%fd1659, %fd1658, %fd1657, 0d0000000000000000;
	div.rn.f64 	%fd1660, %fd1659, %fd113;
	add.f64 	%fd1661, %fd1660, %fd4405;
	mul.f64 	%fd1662, %fd255, %fd1659;
	div.rn.f64 	%fd1663, %fd1662, %fd113;
	sub.f64 	%fd1664, %fd4406, %fd1663;
	fma.rn.f64 	%fd1665, %fd1653, %fd1655, 0d0000000000000000;
	fma.rn.f64 	%fd1666, %fd1654, %fd1655, 0d0000000000000000;
	div.rn.f64 	%fd1667, %fd1666, %fd113;
	add.f64 	%fd1668, %fd1667, 0d0000000000000000;
	mul.f64 	%fd1669, %fd1653, %fd1666;
	div.rn.f64 	%fd1670, %fd1669, %fd113;
	sub.f64 	%fd1671, %fd1664, %fd1670;
	fma.rn.f64 	%fd1672, %fd4, %fd1665, 0d0000000000000000;
	div.rn.f64 	%fd1673, %fd1672, %fd113;
	add.f64 	%fd1674, %fd1668, %fd1673;
	mul.f64 	%fd1675, %fd1653, %fd1672;
	div.rn.f64 	%fd1676, %fd1675, %fd113;
	sub.f64 	%fd1677, %fd1671, %fd1676;
	add.f64 	%fd4405, %fd1661, %fd1674;
	sub.f64 	%fd4406, %fd1677, %fd1674;

$L__BB1_129:
	setp.eq.s64 	%p92, %rd195, 0;
	@%p92 bra 	$L__BB1_131;

	mul.lo.s64 	%rd296, %rd100, %rd75;
	add.s64 	%rd294, %rd195, %rd296;
	mov.f64 	%fd1681, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1678,[%rd294],%fd1681; }

	// end inline asm
	mul.lo.s64 	%rd297, %rd99, %rd75;
	add.s64 	%rd295, %rd195, %rd297;
	// begin inline asm
	{ atom.add.f64 %fd1680,[%rd295],%fd1681; }

	// end inline asm
	bra.uni 	$L__BB1_133;

$L__BB1_131:
	setp.eq.s64 	%p93, %rd166, 0;
	@%p93 bra 	$L__BB1_133;

	add.s64 	%rd298, %rd166, %rd121;
	mov.f64 	%fd1685, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1682,[%rd298],%fd1685; }

	// end inline asm
	add.s64 	%rd299, %rd166, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd1684,[%rd299],%fd1685; }

	// end inline asm

$L__BB1_133:
	fma.rn.f64 	%fd270, %fd3, %fd129, 0d0000000000000000;
	fma.rn.f64 	%fd271, %fd3, %fd117, 0d0000000000000000;
	@%p41 bra 	$L__BB1_141;

	sub.f64 	%fd1687, %fd97, %fd113;
	div.rn.f64 	%fd4416, %fd1687, %fd113;
	mul.f64 	%fd4417, %fd4, %fd4416;
	div.rn.f64 	%fd4419, %fd97, %fd113;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r948}, %fd4419;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r949, %temp}, %fd4419;
	}
	setp.gt.s32 	%p95, %r948, 1048575;
	mov.u32 	%r950, -1023;
	mov.f64 	%fd4414, %fd4419;
	@%p95 bra 	$L__BB1_136;

	mul.f64 	%fd4414, %fd4419, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r948}, %fd4414;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r949, %temp}, %fd4414;
	}
	mov.u32 	%r950, -1077;

$L__BB1_136:
	mul.f64 	%fd4418, %fd4416, %fd4417;
	add.s32 	%r826, %r948, -1;
	setp.lt.u32 	%p96, %r826, 2146435071;
	@%p96 bra 	$L__BB1_138;
	bra.uni 	$L__BB1_137;

$L__BB1_138:
	shr.u32 	%r828, %r948, 20;
	add.s32 	%r951, %r950, %r828;
	and.b32  	%r829, %r948, -2146435073;
	or.b32  	%r830, %r829, 1072693248;
	mov.b64 	%fd4415, {%r949, %r830};
	setp.lt.s32 	%p98, %r830, 1073127583;
	@%p98 bra 	$L__BB1_140;

	{
	.reg .b32 %temp;
	mov.b64 	{%r831, %temp}, %fd4415;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r832}, %fd4415;
	}
	add.s32 	%r833, %r832, -1048576;
	mov.b64 	%fd4415, {%r831, %r833};
	add.s32 	%r951, %r951, 1;

$L__BB1_140:
	add.f64 	%fd1690, %fd4415, 0d3FF0000000000000;
	mov.f64 	%fd1691, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1692, %fd1690;
	neg.f64 	%fd1693, %fd1690;
	fma.rn.f64 	%fd1694, %fd1693, %fd1692, %fd1691;
	fma.rn.f64 	%fd1695, %fd1694, %fd1694, %fd1694;
	fma.rn.f64 	%fd1696, %fd1695, %fd1692, %fd1692;
	add.f64 	%fd1697, %fd4415, 0dBFF0000000000000;
	mul.f64 	%fd1698, %fd1697, %fd1696;
	fma.rn.f64 	%fd1699, %fd1697, %fd1696, %fd1698;
	mul.f64 	%fd1700, %fd1699, %fd1699;
	mov.f64 	%fd1701, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1702, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1703, %fd1702, %fd1700, %fd1701;
	mov.f64 	%fd1704, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1705, %fd1703, %fd1700, %fd1704;
	mov.f64 	%fd1706, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1707, %fd1705, %fd1700, %fd1706;
	mov.f64 	%fd1708, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1709, %fd1707, %fd1700, %fd1708;
	mov.f64 	%fd1710, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1711, %fd1709, %fd1700, %fd1710;
	mov.f64 	%fd1712, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1713, %fd1711, %fd1700, %fd1712;
	mov.f64 	%fd1714, 0d3FB5555555555554;
	fma.rn.f64 	%fd1715, %fd1713, %fd1700, %fd1714;
	sub.f64 	%fd1716, %fd1697, %fd1699;
	add.f64 	%fd1717, %fd1716, %fd1716;
	neg.f64 	%fd1718, %fd1699;
	fma.rn.f64 	%fd1719, %fd1718, %fd1697, %fd1717;
	mul.f64 	%fd1720, %fd1696, %fd1719;
	mul.f64 	%fd1721, %fd1700, %fd1715;
	fma.rn.f64 	%fd1722, %fd1721, %fd1699, %fd1720;
	xor.b32  	%r834, %r951, -2147483648;
	mov.u32 	%r835, -2147483648;
	mov.u32 	%r836, 1127219200;
	mov.b64 	%fd1723, {%r834, %r836};
	mov.b64 	%fd1724, {%r835, %r836};
	sub.f64 	%fd1725, %fd1723, %fd1724;
	mov.f64 	%fd1726, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1727, %fd1725, %fd1726, %fd1699;
	neg.f64 	%fd1728, %fd1725;
	fma.rn.f64 	%fd1729, %fd1728, %fd1726, %fd1727;
	sub.f64 	%fd1730, %fd1729, %fd1699;
	sub.f64 	%fd1731, %fd1722, %fd1730;
	mov.f64 	%fd1732, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1733, %fd1725, %fd1732, %fd1731;
	add.f64 	%fd4420, %fd1727, %fd1733;
	bra.uni 	$L__BB1_141;

$L__BB1_137:
	mov.f64 	%fd1688, 0d7FF0000000000000;
	fma.rn.f64 	%fd1689, %fd4414, %fd1688, %fd1688;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r827}, %fd4414;
	}
	mov.b32 	%f6, %r827;
	setp.eq.f32 	%p97, %f6, 0f00000000;
	selp.f64 	%fd4420, 0dFFF0000000000000, %fd1689, %p97;

$L__BB1_141:
	selp.f64 	%fd288, %fd271, 0d0000000000000000, %p46;
	@%p41 bra 	$L__BB1_143;

	fma.rn.f64 	%fd1734, %fd288, %fd4420, 0d0000000000000000;
	fma.rn.f64 	%fd1735, %fd288, %fd4418, 0d0000000000000000;
	rcp.rn.f64 	%fd1736, %fd4419;
	fma.rn.f64 	%fd1737, %fd1736, %fd1735, 0d0000000000000000;
	div.rn.f64 	%fd1738, %fd1737, %fd113;
	add.f64 	%fd1739, %fd4405, %fd1738;
	mul.f64 	%fd1740, %fd4419, %fd1737;
	div.rn.f64 	%fd1741, %fd1740, %fd113;
	sub.f64 	%fd1742, %fd4406, %fd1741;
	fma.rn.f64 	%fd1743, %fd4416, %fd1734, 0d0000000000000000;
	fma.rn.f64 	%fd1744, %fd4417, %fd1734, 0d0000000000000000;
	div.rn.f64 	%fd1745, %fd1744, %fd113;
	add.f64 	%fd1746, %fd1745, 0d0000000000000000;
	mul.f64 	%fd1747, %fd4416, %fd1744;
	div.rn.f64 	%fd1748, %fd1747, %fd113;
	sub.f64 	%fd1749, %fd1742, %fd1748;
	fma.rn.f64 	%fd1750, %fd4, %fd1743, 0d0000000000000000;
	div.rn.f64 	%fd1751, %fd1750, %fd113;
	add.f64 	%fd1752, %fd1746, %fd1751;
	mul.f64 	%fd1753, %fd4416, %fd1750;
	div.rn.f64 	%fd1754, %fd1753, %fd113;
	sub.f64 	%fd1755, %fd1749, %fd1754;
	add.f64 	%fd4405, %fd1739, %fd1752;
	sub.f64 	%fd4406, %fd1755, %fd1752;

$L__BB1_143:
	mul.f64 	%fd4264, %fd1159, %fd962;
	fma.rn.f64 	%fd1756, %fd4358, %fd270, 0d0000000000000000;
	fma.rn.f64 	%fd293, %fd4264, %fd270, %fd4404;
	fma.rn.f64 	%fd1757, %fd1756, %fd962, 0d0000000000000000;
	fma.rn.f64 	%fd294, %fd1757, %fd965, 0d0000000000000000;
	setp.eq.s64 	%p101, %rd195, 0;
	@%p101 bra 	$L__BB1_145;

	mul.lo.s64 	%rd302, %rd100, %rd75;
	add.s64 	%rd300, %rd195, %rd302;
	// begin inline asm
	{ atom.add.f64 %fd1758,[%rd300],%fd294; }

	// end inline asm
	mul.lo.s64 	%rd303, %rd99, %rd75;
	add.s64 	%rd301, %rd195, %rd303;
	// begin inline asm
	{ atom.add.f64 %fd1760,[%rd301],%fd294; }

	// end inline asm
	bra.uni 	$L__BB1_147;

$L__BB1_145:
	setp.eq.s64 	%p102, %rd166, 0;
	@%p102 bra 	$L__BB1_147;

	add.s64 	%rd304, %rd166, %rd121;
	// begin inline asm
	{ atom.add.f64 %fd1762,[%rd304],%fd294; }

	// end inline asm
	add.s64 	%rd305, %rd166, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd1764,[%rd305],%fd294; }

	// end inline asm

$L__BB1_147:
	setp.geu.f64 	%p308, %fd110, %fd106;
	add.f64 	%fd1768, %fd4406, 0d0000000000000000;
	mov.f64 	%fd1767, 0d0000000000000000;
	fma.rn.f64 	%fd295, %fd2, %fd1768, 0d0000000000000000;
	mov.f64 	%fd4423, %fd1767;
	mov.f64 	%fd4424, %fd1767;
	@%p308 bra 	$L__BB1_149;

	add.f64 	%fd1769, %fd293, 0d0000000000000000;
	mov.f64 	%fd1770, 0d0000000000000000;
	div.rn.f64 	%fd1771, %fd110, %fd106;
	sub.f64 	%fd1772, %fd1770, %fd1771;
	add.f64 	%fd1773, %fd1772, 0d4000000000000000;
	fma.rn.f64 	%fd1774, %fd1771, %fd1769, 0d0000000000000000;
	fma.rn.f64 	%fd1775, %fd1773, %fd1769, 0d0000000000000000;
	sub.f64 	%fd1776, %fd1775, %fd1774;
	div.rn.f64 	%fd1777, %fd1776, %fd106;
	add.f64 	%fd4424, %fd1777, 0d0000000000000000;
	mul.f64 	%fd1778, %fd1771, %fd1776;
	div.rn.f64 	%fd1779, %fd1778, %fd106;
	sub.f64 	%fd4423, %fd1770, %fd1779;

$L__BB1_149:
	add.f64 	%fd300, %fd107, %fd107;
	add.f64 	%fd301, %fd108, %fd108;
	add.f64 	%fd302, %fd109, %fd109;
	fma.rn.f64 	%fd1780, %fd300, %fd4424, 0d0000000000000000;
	fma.rn.f64 	%fd1782, %fd301, %fd4424, 0d0000000000000000;
	fma.rn.f64 	%fd1783, %fd302, %fd4424, 0d0000000000000000;
	mul.f64 	%fd1784, %fd62, %fd1783;
	mul.f64 	%fd1785, %fd63, %fd1782;
	sub.f64 	%fd1786, %fd1784, %fd1785;
	mul.f64 	%fd1787, %fd63, %fd1780;
	mul.f64 	%fd1788, %fd61, %fd1783;
	sub.f64 	%fd1789, %fd1787, %fd1788;
	mul.f64 	%fd1790, %fd61, %fd1782;
	mul.f64 	%fd1791, %fd62, %fd1780;
	sub.f64 	%fd1792, %fd1790, %fd1791;
	add.f64 	%fd4434, %fd1786, 0d0000000000000000;
	add.f64 	%fd4433, %fd1789, 0d0000000000000000;
	add.f64 	%fd4432, %fd1792, 0d0000000000000000;
	mul.f64 	%fd1793, %fd59, %fd1783;
	mul.f64 	%fd1794, %fd60, %fd1782;
	mul.f64 	%fd1795, %fd60, %fd1780;
	mul.f64 	%fd1796, %fd58, %fd1783;
	mul.f64 	%fd1797, %fd58, %fd1782;
	mul.f64 	%fd1798, %fd59, %fd1780;
	sub.f64 	%fd1799, %fd1794, %fd1793;
	add.f64 	%fd4440, %fd1799, 0d0000000000000000;
	sub.f64 	%fd1800, %fd1796, %fd1795;
	add.f64 	%fd4439, %fd1800, 0d0000000000000000;
	sub.f64 	%fd1801, %fd1798, %fd1797;
	add.f64 	%fd4438, %fd1801, 0d0000000000000000;
	sub.f64 	%fd4437, %fd1767, %fd4440;
	sub.f64 	%fd4436, %fd1767, %fd4439;
	sub.f64 	%fd4435, %fd1767, %fd4438;
	sub.f64 	%fd4431, %fd1767, %fd4434;
	sub.f64 	%fd4430, %fd1767, %fd4433;
	sub.f64 	%fd4429, %fd1767, %fd4432;
	add.f64 	%fd1802, %fd4423, 0d0000000000000000;
	fma.rn.f64 	%fd1803, %fd105, %fd1802, 0d0000000000000000;
	fma.rn.f64 	%fd1804, %fd104, %fd1802, 0d0000000000000000;
	add.f64 	%fd1805, %fd101, %fd101;
	add.f64 	%fd1806, %fd102, %fd102;
	add.f64 	%fd1807, %fd103, %fd103;
	fma.rn.f64 	%fd315, %fd1805, %fd1804, 0d0000000000000000;
	fma.rn.f64 	%fd316, %fd1806, %fd1804, 0d0000000000000000;
	fma.rn.f64 	%fd317, %fd1807, %fd1804, 0d0000000000000000;
	fma.rn.f64 	%fd1808, %fd1803, 0d3F50624DE0000000, 0d0000000000000000;
	add.f64 	%fd1809, %fd98, %fd98;
	add.f64 	%fd1810, %fd99, %fd99;
	add.f64 	%fd1811, %fd100, %fd100;
	fma.rn.f64 	%fd318, %fd1809, %fd1808, 0d0000000000000000;
	fma.rn.f64 	%fd319, %fd1810, %fd1808, 0d0000000000000000;
	fma.rn.f64 	%fd320, %fd1811, %fd1808, 0d0000000000000000;
	setp.eq.s64 	%p104, %rd183, 0;
	@%p104 bra 	$L__BB1_151;

	mul.lo.s64 	%rd309, %rd109, %rd77;
	add.s64 	%rd306, %rd183, %rd309;
	// begin inline asm
	{ atom.add.f64 %fd1812,[%rd306],%fd315; }

	// end inline asm
	add.s64 	%rd307, %rd306, 8;
	// begin inline asm
	{ atom.add.f64 %fd1814,[%rd307],%fd316; }

	// end inline asm
	add.s64 	%rd308, %rd306, 16;
	// begin inline asm
	{ atom.add.f64 %fd1816,[%rd308],%fd317; }

	// end inline asm
	bra.uni 	$L__BB1_153;

$L__BB1_151:
	setp.eq.s64 	%p105, %rd154, 0;
	@%p105 bra 	$L__BB1_153;

	mul.lo.s64 	%rd625, %rd109, %rd614;
	add.s64 	%rd310, %rd154, %rd625;
	// begin inline asm
	{ atom.add.f64 %fd1818,[%rd310],%fd315; }

	// end inline asm
	add.s64 	%rd311, %rd310, 8;
	// begin inline asm
	{ atom.add.f64 %fd1820,[%rd311],%fd316; }

	// end inline asm
	add.s64 	%rd312, %rd310, 16;
	// begin inline asm
	{ atom.add.f64 %fd1822,[%rd312],%fd317; }

	// end inline asm

$L__BB1_153:
	mov.f64 	%fd1824, 0d0000000000000000;
	sub.f64 	%fd1825, %fd1824, %fd315;
	add.f64 	%fd321, %fd1825, 0d0000000000000000;
	sub.f64 	%fd1826, %fd1824, %fd316;
	add.f64 	%fd322, %fd1826, 0d0000000000000000;
	sub.f64 	%fd1827, %fd1824, %fd317;
	add.f64 	%fd323, %fd1827, 0d0000000000000000;
	@%p104 bra 	$L__BB1_155;

	mul.lo.s64 	%rd316, %rd107, %rd77;
	add.s64 	%rd313, %rd183, %rd316;
	// begin inline asm
	{ atom.add.f64 %fd1828,[%rd313],%fd321; }

	// end inline asm
	add.s64 	%rd314, %rd313, 8;
	// begin inline asm
	{ atom.add.f64 %fd1830,[%rd314],%fd322; }

	// end inline asm
	add.s64 	%rd315, %rd313, 16;
	// begin inline asm
	{ atom.add.f64 %fd1832,[%rd315],%fd323; }

	// end inline asm
	bra.uni 	$L__BB1_157;

$L__BB1_155:
	setp.eq.s64 	%p107, %rd154, 0;
	@%p107 bra 	$L__BB1_161;

	mul.lo.s64 	%rd624, %rd107, %rd614;
	add.s64 	%rd317, %rd154, %rd624;
	// begin inline asm
	{ atom.add.f64 %fd1834,[%rd317],%fd321; }

	// end inline asm
	add.s64 	%rd318, %rd317, 8;
	// begin inline asm
	{ atom.add.f64 %fd1836,[%rd318],%fd322; }

	// end inline asm
	add.s64 	%rd319, %rd317, 16;
	// begin inline asm
	{ atom.add.f64 %fd1838,[%rd319],%fd323; }

	// end inline asm

$L__BB1_157:
	@%p104 bra 	$L__BB1_159;

	mul.lo.s64 	%rd323, %rd105, %rd77;
	add.s64 	%rd320, %rd183, %rd323;
	// begin inline asm
	{ atom.add.f64 %fd1840,[%rd320],%fd318; }

	// end inline asm
	add.s64 	%rd321, %rd320, 8;
	// begin inline asm
	{ atom.add.f64 %fd1842,[%rd321],%fd319; }

	// end inline asm
	add.s64 	%rd322, %rd320, 16;
	// begin inline asm
	{ atom.add.f64 %fd1844,[%rd322],%fd320; }

	// end inline asm
	bra.uni 	$L__BB1_161;

$L__BB1_159:
	setp.eq.s64 	%p109, %rd154, 0;
	@%p109 bra 	$L__BB1_161;

	mul.lo.s64 	%rd623, %rd105, %rd614;
	add.s64 	%rd324, %rd154, %rd623;
	// begin inline asm
	{ atom.add.f64 %fd1846,[%rd324],%fd318; }

	// end inline asm
	add.s64 	%rd325, %rd324, 8;
	// begin inline asm
	{ atom.add.f64 %fd1848,[%rd325],%fd319; }

	// end inline asm
	add.s64 	%rd326, %rd324, 16;
	// begin inline asm
	{ atom.add.f64 %fd1850,[%rd326],%fd320; }

	// end inline asm

$L__BB1_161:
	mov.f64 	%fd1852, 0d0000000000000000;
	sub.f64 	%fd1853, %fd1852, %fd318;
	add.f64 	%fd324, %fd1853, 0d0000000000000000;
	sub.f64 	%fd1854, %fd1852, %fd319;
	add.f64 	%fd325, %fd1854, 0d0000000000000000;
	sub.f64 	%fd1855, %fd1852, %fd320;
	add.f64 	%fd326, %fd1855, 0d0000000000000000;
	@%p104 bra 	$L__BB1_163;

	mul.lo.s64 	%rd330, %rd103, %rd77;
	add.s64 	%rd327, %rd183, %rd330;
	// begin inline asm
	{ atom.add.f64 %fd1856,[%rd327],%fd324; }

	// end inline asm
	add.s64 	%rd328, %rd327, 8;
	// begin inline asm
	{ atom.add.f64 %fd1858,[%rd328],%fd325; }

	// end inline asm
	add.s64 	%rd329, %rd327, 16;
	// begin inline asm
	{ atom.add.f64 %fd1860,[%rd329],%fd326; }

	// end inline asm
	bra.uni 	$L__BB1_165;

$L__BB1_163:
	setp.eq.s64 	%p111, %rd154, 0;
	@%p111 bra 	$L__BB1_165;

	add.s64 	%rd331, %rd154, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd1862,[%rd331],%fd324; }

	// end inline asm
	add.s64 	%rd332, %rd331, 8;
	// begin inline asm
	{ atom.add.f64 %fd1864,[%rd332],%fd325; }

	// end inline asm
	add.s64 	%rd333, %rd331, 16;
	// begin inline asm
	{ atom.add.f64 %fd1866,[%rd333],%fd326; }

	// end inline asm

$L__BB1_165:
	mov.f64 	%fd1869, 0d0000000000000000;
	add.f64 	%fd327, %fd4405, 0d0000000000000000;
	sub.f64 	%fd1870, %fd1869, %fd4405;
	fma.rn.f64 	%fd1871, %fd23, %fd1870, %fd295;
	fma.rn.f64 	%fd328, %fd23, %fd1870, %fd1871;
	@%p15 bra 	$L__BB1_169;

	setp.ge.f64 	%p2, %fd73, %fd72;
	add.f64 	%fd329, %fd71, %fd68;
	@%p2 bra 	$L__BB1_168;

	selp.f64 	%fd1873, %fd69, %fd72, %p2;
	mul.f64 	%fd1874, %fd67, %fd71;
	mul.f64 	%fd1875, %fd70, %fd68;
	sub.f64 	%fd1876, %fd1874, %fd1875;
	setp.gt.f64 	%p113, %fd1876, 0d0000000000000000;
	setp.lt.f64 	%p114, %fd1876, %fd1873;
	mul.f64 	%fd1877, %fd65, %fd108;
	fma.rn.f64 	%fd1878, %fd64, %fd107, %fd1877;
	fma.rn.f64 	%fd1879, %fd66, %fd109, %fd1878;
	setp.eq.f64 	%p115, %fd1879, 0d0000000000000000;
	mul.f64 	%fd1880, %fd67, 0d3BC79CA100000000;
	mul.f64 	%fd1881, %fd1880, %fd69;
	setp.lt.f64 	%p116, %fd110, %fd1881;
	or.pred  	%p117, %p115, %p116;
	and.pred  	%p118, %p113, %p114;
	and.pred  	%p119, %p117, %p118;
	mul.f64 	%fd1882, %fd72, 0d3FE0000000000000;
	setp.lt.f64 	%p120, %fd73, %fd1882;
	selp.b32 	%r839, 2, 5, %p120;
	selp.f64 	%fd1883, %fd71, %fd329, %p120;
	selp.f64 	%fd4425, %fd69, %fd1873, %p119;
	selp.b32 	%r952, %r839, 8, %p119;
	selp.f64 	%fd4426, %fd1883, %fd1876, %p119;

$L__BB1_168:
	selp.f64 	%fd4427, %fd69, %fd4425, %p2;
	selp.b32 	%r953, 5, %r952, %p2;
	selp.f64 	%fd4428, %fd329, %fd4426, %p2;

$L__BB1_169:
	selp.f64 	%fd338, %fd69, %fd4427, %p15;
	selp.b32 	%r954, 2, %r953, %p15;
	selp.f64 	%fd339, %fd71, %fd4428, %p15;
	setp.gtu.f64 	%p123, %fd339, 0d0000000000000000;
	@%p123 bra 	$L__BB1_173;
	bra.uni 	$L__BB1_170;

$L__BB1_173:
	setp.ltu.f64 	%p126, %fd339, %fd338;
	@%p126 bra 	$L__BB1_177;

	mov.f64 	%fd1885, 0d0000000000000000;
	sub.f64 	%fd1886, %fd1885, %fd70;
	add.f64 	%fd341, %fd1886, %fd68;
	setp.le.f64 	%p127, %fd341, 0d0000000000000000;
	mov.u32 	%r954, 1;
	@%p127 bra 	$L__BB1_177;

	setp.ge.f64 	%p128, %fd341, %fd67;
	mov.u32 	%r954, 4;
	@%p128 bra 	$L__BB1_177;

	mov.u32 	%r954, 7;
	bra.uni 	$L__BB1_177;

$L__BB1_170:
	mov.f64 	%fd1884, 0d0000000000000000;
	sub.f64 	%fd340, %fd1884, %fd70;
	setp.le.f64 	%p124, %fd340, 0d0000000000000000;
	mov.u32 	%r954, 0;
	@%p124 bra 	$L__BB1_177;

	setp.ge.f64 	%p125, %fd340, %fd67;
	mov.u32 	%r954, 3;
	@%p125 bra 	$L__BB1_177;

	mov.u32 	%r954, 6;

$L__BB1_177:
	setp.eq.s32 	%p129, %r954, 0;
	@%p129 bra 	$L__BB1_193;

	setp.eq.s32 	%p130, %r954, 1;
	@%p130 bra 	$L__BB1_192;
	bra.uni 	$L__BB1_179;

$L__BB1_192:
	sub.f64 	%fd2237, %fd26, %fd32;
	add.f64 	%fd2238, %fd2237, %fd2237;
	sub.f64 	%fd2239, %fd33, %fd37;
	add.f64 	%fd2240, %fd2239, %fd2239;
	sub.f64 	%fd2241, %fd38, %fd42;
	add.f64 	%fd2242, %fd2241, %fd2241;
	fma.rn.f64 	%fd2243, %fd2238, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2244, %fd2240, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2245, %fd2242, %fd327, 0d0000000000000000;
	add.f64 	%fd4431, %fd2243, %fd4431;
	add.f64 	%fd4430, %fd2244, %fd4430;
	add.f64 	%fd4429, %fd2245, %fd4429;
	sub.f64 	%fd4440, %fd4440, %fd2243;
	sub.f64 	%fd4439, %fd4439, %fd2244;
	sub.f64 	%fd4438, %fd4438, %fd2245;
	bra.uni 	$L__BB1_194;

$L__BB1_193:
	add.f64 	%fd2246, %fd64, %fd64;
	add.f64 	%fd2247, %fd65, %fd65;
	add.f64 	%fd2248, %fd66, %fd66;
	fma.rn.f64 	%fd2249, %fd2246, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2250, %fd2247, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2251, %fd2248, %fd327, 0d0000000000000000;
	add.f64 	%fd4431, %fd2249, %fd4431;
	add.f64 	%fd4430, %fd2250, %fd4430;
	add.f64 	%fd4429, %fd2251, %fd4429;
	sub.f64 	%fd4437, %fd4437, %fd2249;
	sub.f64 	%fd4436, %fd4436, %fd2250;
	sub.f64 	%fd4435, %fd4435, %fd2251;
	bra.uni 	$L__BB1_194;

$L__BB1_179:
	setp.eq.s32 	%p131, %r954, 2;
	@%p131 bra 	$L__BB1_191;
	bra.uni 	$L__BB1_180;

$L__BB1_191:
	sub.f64 	%fd2167, %fd31, %fd26;
	sub.f64 	%fd2168, %fd42, %fd38;
	sub.f64 	%fd2169, %fd36, %fd33;
	mul.f64 	%fd2170, %fd2169, %fd2168;
	sub.f64 	%fd2171, %fd37, %fd33;
	sub.f64 	%fd2172, %fd41, %fd38;
	mul.f64 	%fd2173, %fd2172, %fd2171;
	sub.f64 	%fd2174, %fd2170, %fd2173;
	sub.f64 	%fd2175, %fd32, %fd26;
	mul.f64 	%fd2176, %fd2172, %fd2175;
	mul.f64 	%fd2177, %fd2167, %fd2168;
	sub.f64 	%fd2178, %fd2176, %fd2177;
	mul.f64 	%fd2179, %fd2167, %fd2171;
	mul.f64 	%fd2180, %fd2169, %fd2175;
	sub.f64 	%fd2181, %fd2179, %fd2180;
	mul.f64 	%fd2182, %fd2178, %fd2178;
	fma.rn.f64 	%fd2183, %fd2174, %fd2174, %fd2182;
	fma.rn.f64 	%fd2184, %fd2181, %fd2181, %fd2183;
	div.rn.f64 	%fd2185, %fd2184, %fd69;
	div.rn.f64 	%fd2186, %fd327, %fd69;
	add.f64 	%fd2187, %fd2186, 0d0000000000000000;
	mov.f64 	%fd2188, 0d0000000000000000;
	mul.f64 	%fd2189, %fd2185, %fd327;
	div.rn.f64 	%fd2190, %fd2189, %fd69;
	sub.f64 	%fd2191, %fd2188, %fd2190;
	add.f64 	%fd2192, %fd61, %fd61;
	add.f64 	%fd2193, %fd62, %fd62;
	add.f64 	%fd2194, %fd63, %fd63;
	fma.rn.f64 	%fd2195, %fd2192, %fd2191, 0d0000000000000000;
	fma.rn.f64 	%fd2196, %fd2193, %fd2191, 0d0000000000000000;
	fma.rn.f64 	%fd2197, %fd2194, %fd2191, 0d0000000000000000;
	add.f64 	%fd2198, %fd2174, %fd2174;
	add.f64 	%fd2199, %fd2178, %fd2178;
	add.f64 	%fd2200, %fd2181, %fd2181;
	fma.rn.f64 	%fd2201, %fd2198, %fd2187, 0d0000000000000000;
	fma.rn.f64 	%fd2202, %fd2199, %fd2187, 0d0000000000000000;
	fma.rn.f64 	%fd2203, %fd2200, %fd2187, 0d0000000000000000;
	mul.f64 	%fd2204, %fd2171, %fd2203;
	mul.f64 	%fd2205, %fd2168, %fd2202;
	sub.f64 	%fd2206, %fd2204, %fd2205;
	mul.f64 	%fd2207, %fd2168, %fd2201;
	mul.f64 	%fd2208, %fd2175, %fd2203;
	sub.f64 	%fd2209, %fd2207, %fd2208;
	mul.f64 	%fd2210, %fd2175, %fd2202;
	mul.f64 	%fd2211, %fd2171, %fd2201;
	sub.f64 	%fd2212, %fd2210, %fd2211;
	add.f64 	%fd2213, %fd2206, 0d0000000000000000;
	add.f64 	%fd2214, %fd2209, 0d0000000000000000;
	add.f64 	%fd2215, %fd2212, 0d0000000000000000;
	mul.f64 	%fd2216, %fd2169, %fd2203;
	mul.f64 	%fd2217, %fd2172, %fd2202;
	mul.f64 	%fd2218, %fd2172, %fd2201;
	mul.f64 	%fd2219, %fd2167, %fd2203;
	mul.f64 	%fd2220, %fd2167, %fd2202;
	mul.f64 	%fd2221, %fd2169, %fd2201;
	sub.f64 	%fd2222, %fd2217, %fd2216;
	add.f64 	%fd2223, %fd2222, 0d0000000000000000;
	sub.f64 	%fd2224, %fd2219, %fd2218;
	add.f64 	%fd2225, %fd2224, 0d0000000000000000;
	sub.f64 	%fd2226, %fd2221, %fd2220;
	add.f64 	%fd2227, %fd2226, 0d0000000000000000;
	add.f64 	%fd2228, %fd2195, %fd4440;
	add.f64 	%fd2229, %fd2196, %fd4439;
	add.f64 	%fd2230, %fd2197, %fd4438;
	sub.f64 	%fd2231, %fd4437, %fd2195;
	sub.f64 	%fd2232, %fd4436, %fd2196;
	sub.f64 	%fd2233, %fd4435, %fd2197;
	add.f64 	%fd4440, %fd2223, %fd2228;
	add.f64 	%fd4439, %fd2225, %fd2229;
	add.f64 	%fd4438, %fd2227, %fd2230;
	sub.f64 	%fd2234, %fd4431, %fd2223;
	sub.f64 	%fd2235, %fd4430, %fd2225;
	sub.f64 	%fd2236, %fd4429, %fd2227;
	add.f64 	%fd4437, %fd2213, %fd2231;
	add.f64 	%fd4436, %fd2214, %fd2232;
	add.f64 	%fd4435, %fd2215, %fd2233;
	sub.f64 	%fd4431, %fd2234, %fd2213;
	sub.f64 	%fd4430, %fd2235, %fd2214;
	sub.f64 	%fd4429, %fd2236, %fd2215;
	bra.uni 	$L__BB1_194;

$L__BB1_180:
	setp.eq.s32 	%p132, %r954, 3;
	@%p132 bra 	$L__BB1_190;
	bra.uni 	$L__BB1_181;

$L__BB1_190:
	sub.f64 	%fd2158, %fd27, %fd31;
	add.f64 	%fd2159, %fd2158, %fd2158;
	sub.f64 	%fd2160, %fd34, %fd36;
	add.f64 	%fd2161, %fd2160, %fd2160;
	sub.f64 	%fd2162, %fd39, %fd41;
	add.f64 	%fd2163, %fd2162, %fd2162;
	fma.rn.f64 	%fd2164, %fd2159, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2165, %fd2161, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2166, %fd2163, %fd327, 0d0000000000000000;
	add.f64 	%fd4434, %fd2164, %fd4434;
	add.f64 	%fd4433, %fd2165, %fd4433;
	add.f64 	%fd4432, %fd2166, %fd4432;
	sub.f64 	%fd4437, %fd4437, %fd2164;
	sub.f64 	%fd4436, %fd4436, %fd2165;
	sub.f64 	%fd4435, %fd4435, %fd2166;
	bra.uni 	$L__BB1_194;

$L__BB1_181:
	setp.eq.s32 	%p133, %r954, 4;
	@%p133 bra 	$L__BB1_189;
	bra.uni 	$L__BB1_182;

$L__BB1_189:
	sub.f64 	%fd2149, %fd27, %fd32;
	add.f64 	%fd2150, %fd2149, %fd2149;
	sub.f64 	%fd2151, %fd34, %fd37;
	add.f64 	%fd2152, %fd2151, %fd2151;
	sub.f64 	%fd2153, %fd39, %fd42;
	add.f64 	%fd2154, %fd2153, %fd2153;
	fma.rn.f64 	%fd2155, %fd2150, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2156, %fd2152, %fd327, 0d0000000000000000;
	fma.rn.f64 	%fd2157, %fd2154, %fd327, 0d0000000000000000;
	add.f64 	%fd4434, %fd2155, %fd4434;
	add.f64 	%fd4433, %fd2156, %fd4433;
	add.f64 	%fd4432, %fd2157, %fd4432;
	sub.f64 	%fd4440, %fd4440, %fd2155;
	sub.f64 	%fd4439, %fd4439, %fd2156;
	sub.f64 	%fd4438, %fd4438, %fd2157;
	bra.uni 	$L__BB1_194;

$L__BB1_182:
	setp.eq.s32 	%p134, %r954, 5;
	@%p134 bra 	$L__BB1_188;
	bra.uni 	$L__BB1_183;

$L__BB1_188:
	sub.f64 	%fd2079, %fd31, %fd27;
	sub.f64 	%fd2080, %fd42, %fd39;
	sub.f64 	%fd2081, %fd36, %fd34;
	mul.f64 	%fd2082, %fd2081, %fd2080;
	sub.f64 	%fd2083, %fd37, %fd34;
	sub.f64 	%fd2084, %fd41, %fd39;
	mul.f64 	%fd2085, %fd2084, %fd2083;
	sub.f64 	%fd2086, %fd2082, %fd2085;
	sub.f64 	%fd2087, %fd32, %fd27;
	mul.f64 	%fd2088, %fd2084, %fd2087;
	mul.f64 	%fd2089, %fd2079, %fd2080;
	sub.f64 	%fd2090, %fd2088, %fd2089;
	mul.f64 	%fd2091, %fd2079, %fd2083;
	mul.f64 	%fd2092, %fd2081, %fd2087;
	sub.f64 	%fd2093, %fd2091, %fd2092;
	mul.f64 	%fd2094, %fd2090, %fd2090;
	fma.rn.f64 	%fd2095, %fd2086, %fd2086, %fd2094;
	fma.rn.f64 	%fd2096, %fd2093, %fd2093, %fd2095;
	div.rn.f64 	%fd2097, %fd2096, %fd69;
	div.rn.f64 	%fd2098, %fd327, %fd69;
	add.f64 	%fd2099, %fd2098, 0d0000000000000000;
	mov.f64 	%fd2100, 0d0000000000000000;
	mul.f64 	%fd2101, %fd2097, %fd327;
	div.rn.f64 	%fd2102, %fd2101, %fd69;
	sub.f64 	%fd2103, %fd2100, %fd2102;
	add.f64 	%fd2104, %fd61, %fd61;
	add.f64 	%fd2105, %fd62, %fd62;
	add.f64 	%fd2106, %fd63, %fd63;
	fma.rn.f64 	%fd2107, %fd2104, %fd2103, 0d0000000000000000;
	fma.rn.f64 	%fd2108, %fd2105, %fd2103, 0d0000000000000000;
	fma.rn.f64 	%fd2109, %fd2106, %fd2103, 0d0000000000000000;
	add.f64 	%fd2110, %fd2086, %fd2086;
	add.f64 	%fd2111, %fd2090, %fd2090;
	add.f64 	%fd2112, %fd2093, %fd2093;
	fma.rn.f64 	%fd2113, %fd2110, %fd2099, 0d0000000000000000;
	fma.rn.f64 	%fd2114, %fd2111, %fd2099, 0d0000000000000000;
	fma.rn.f64 	%fd2115, %fd2112, %fd2099, 0d0000000000000000;
	mul.f64 	%fd2116, %fd2083, %fd2115;
	mul.f64 	%fd2117, %fd2080, %fd2114;
	sub.f64 	%fd2118, %fd2116, %fd2117;
	mul.f64 	%fd2119, %fd2080, %fd2113;
	mul.f64 	%fd2120, %fd2087, %fd2115;
	sub.f64 	%fd2121, %fd2119, %fd2120;
	mul.f64 	%fd2122, %fd2087, %fd2114;
	mul.f64 	%fd2123, %fd2083, %fd2113;
	sub.f64 	%fd2124, %fd2122, %fd2123;
	add.f64 	%fd2125, %fd2118, 0d0000000000000000;
	add.f64 	%fd2126, %fd2121, 0d0000000000000000;
	add.f64 	%fd2127, %fd2124, 0d0000000000000000;
	mul.f64 	%fd2128, %fd2081, %fd2115;
	mul.f64 	%fd2129, %fd2084, %fd2114;
	mul.f64 	%fd2130, %fd2084, %fd2113;
	mul.f64 	%fd2131, %fd2079, %fd2115;
	mul.f64 	%fd2132, %fd2079, %fd2114;
	mul.f64 	%fd2133, %fd2081, %fd2113;
	sub.f64 	%fd2134, %fd2129, %fd2128;
	add.f64 	%fd2135, %fd2134, 0d0000000000000000;
	sub.f64 	%fd2136, %fd2131, %fd2130;
	add.f64 	%fd2137, %fd2136, 0d0000000000000000;
	sub.f64 	%fd2138, %fd2133, %fd2132;
	add.f64 	%fd2139, %fd2138, 0d0000000000000000;
	add.f64 	%fd2140, %fd2107, %fd4440;
	add.f64 	%fd2141, %fd2108, %fd4439;
	add.f64 	%fd2142, %fd2109, %fd4438;
	sub.f64 	%fd2143, %fd4437, %fd2107;
	sub.f64 	%fd2144, %fd4436, %fd2108;
	sub.f64 	%fd2145, %fd4435, %fd2109;
	add.f64 	%fd4440, %fd2135, %fd2140;
	add.f64 	%fd4439, %fd2137, %fd2141;
	add.f64 	%fd4438, %fd2139, %fd2142;
	sub.f64 	%fd2146, %fd4434, %fd2135;
	sub.f64 	%fd2147, %fd4433, %fd2137;
	sub.f64 	%fd2148, %fd4432, %fd2139;
	add.f64 	%fd4437, %fd2125, %fd2143;
	add.f64 	%fd4436, %fd2126, %fd2144;
	add.f64 	%fd4435, %fd2127, %fd2145;
	sub.f64 	%fd4434, %fd2146, %fd2125;
	sub.f64 	%fd4433, %fd2147, %fd2126;
	sub.f64 	%fd4432, %fd2148, %fd2127;
	bra.uni 	$L__BB1_194;

$L__BB1_183:
	setp.eq.s32 	%p135, %r954, 6;
	@%p135 bra 	$L__BB1_187;
	bra.uni 	$L__BB1_184;

$L__BB1_187:
	sub.f64 	%fd2012, %fd27, %fd31;
	sub.f64 	%fd2013, %fd39, %fd41;
	mul.f64 	%fd2014, %fd65, %fd2013;
	sub.f64 	%fd2015, %fd34, %fd36;
	mul.f64 	%fd2016, %fd2015, %fd66;
	sub.f64 	%fd2017, %fd2014, %fd2016;
	mul.f64 	%fd2018, %fd2012, %fd66;
	mul.f64 	%fd2019, %fd64, %fd2013;
	sub.f64 	%fd2020, %fd2018, %fd2019;
	mul.f64 	%fd2021, %fd64, %fd2015;
	mul.f64 	%fd2022, %fd2012, %fd65;
	sub.f64 	%fd2023, %fd2021, %fd2022;
	mul.f64 	%fd2024, %fd2020, %fd2020;
	fma.rn.f64 	%fd2025, %fd2017, %fd2017, %fd2024;
	fma.rn.f64 	%fd2026, %fd2023, %fd2023, %fd2025;
	div.rn.f64 	%fd2027, %fd2026, %fd67;
	div.rn.f64 	%fd2028, %fd327, %fd67;
	add.f64 	%fd2029, %fd2028, 0d0000000000000000;
	mov.f64 	%fd2030, 0d0000000000000000;
	mul.f64 	%fd2031, %fd2027, %fd327;
	div.rn.f64 	%fd2032, %fd2031, %fd67;
	sub.f64 	%fd2033, %fd2030, %fd2032;
	add.f64 	%fd2034, %fd58, %fd58;
	add.f64 	%fd2035, %fd59, %fd59;
	add.f64 	%fd2036, %fd60, %fd60;
	fma.rn.f64 	%fd2037, %fd2034, %fd2033, 0d0000000000000000;
	fma.rn.f64 	%fd2038, %fd2035, %fd2033, 0d0000000000000000;
	fma.rn.f64 	%fd2039, %fd2036, %fd2033, 0d0000000000000000;
	add.f64 	%fd2040, %fd2017, %fd2017;
	add.f64 	%fd2041, %fd2020, %fd2020;
	add.f64 	%fd2042, %fd2023, %fd2023;
	fma.rn.f64 	%fd2043, %fd2040, %fd2029, 0d0000000000000000;
	fma.rn.f64 	%fd2044, %fd2041, %fd2029, 0d0000000000000000;
	fma.rn.f64 	%fd2045, %fd2042, %fd2029, 0d0000000000000000;
	mul.f64 	%fd2046, %fd2015, %fd2045;
	mul.f64 	%fd2047, %fd2013, %fd2044;
	sub.f64 	%fd2048, %fd2046, %fd2047;
	mul.f64 	%fd2049, %fd2013, %fd2043;
	mul.f64 	%fd2050, %fd2012, %fd2045;
	sub.f64 	%fd2051, %fd2049, %fd2050;
	mul.f64 	%fd2052, %fd2012, %fd2044;
	mul.f64 	%fd2053, %fd2015, %fd2043;
	sub.f64 	%fd2054, %fd2052, %fd2053;
	add.f64 	%fd2055, %fd2048, 0d0000000000000000;
	add.f64 	%fd2056, %fd2051, 0d0000000000000000;
	add.f64 	%fd2057, %fd2054, 0d0000000000000000;
	mul.f64 	%fd2058, %fd65, %fd2045;
	mul.f64 	%fd2059, %fd66, %fd2044;
	mul.f64 	%fd2060, %fd66, %fd2043;
	mul.f64 	%fd2061, %fd64, %fd2045;
	mul.f64 	%fd2062, %fd64, %fd2044;
	mul.f64 	%fd2063, %fd65, %fd2043;
	sub.f64 	%fd2064, %fd2059, %fd2058;
	add.f64 	%fd2065, %fd2064, 0d0000000000000000;
	sub.f64 	%fd2066, %fd2061, %fd2060;
	add.f64 	%fd2067, %fd2066, 0d0000000000000000;
	sub.f64 	%fd2068, %fd2063, %fd2062;
	add.f64 	%fd2069, %fd2068, 0d0000000000000000;
	add.f64 	%fd2070, %fd2037, %fd4434;
	add.f64 	%fd2071, %fd2038, %fd4433;
	add.f64 	%fd2072, %fd2039, %fd4432;
	sub.f64 	%fd2073, %fd4431, %fd2037;
	sub.f64 	%fd2074, %fd4430, %fd2038;
	sub.f64 	%fd2075, %fd4429, %fd2039;
	add.f64 	%fd4434, %fd2065, %fd2070;
	add.f64 	%fd4433, %fd2067, %fd2071;
	add.f64 	%fd4432, %fd2069, %fd2072;
	sub.f64 	%fd2076, %fd4437, %fd2065;
	sub.f64 	%fd2077, %fd4436, %fd2067;
	sub.f64 	%fd2078, %fd4435, %fd2069;
	add.f64 	%fd4431, %fd2055, %fd2073;
	add.f64 	%fd4430, %fd2056, %fd2074;
	add.f64 	%fd4429, %fd2057, %fd2075;
	sub.f64 	%fd4437, %fd2076, %fd2055;
	sub.f64 	%fd4436, %fd2077, %fd2056;
	sub.f64 	%fd4435, %fd2078, %fd2057;
	bra.uni 	$L__BB1_194;

$L__BB1_184:
	setp.eq.s32 	%p136, %r954, 7;
	@%p136 bra 	$L__BB1_186;
	bra.uni 	$L__BB1_185;

$L__BB1_186:
	sub.f64 	%fd1942, %fd26, %fd32;
	sub.f64 	%fd1943, %fd39, %fd42;
	sub.f64 	%fd1944, %fd33, %fd37;
	mul.f64 	%fd1945, %fd1944, %fd1943;
	sub.f64 	%fd1946, %fd34, %fd37;
	sub.f64 	%fd1947, %fd38, %fd42;
	mul.f64 	%fd1948, %fd1946, %fd1947;
	sub.f64 	%fd1949, %fd1945, %fd1948;
	sub.f64 	%fd1950, %fd27, %fd32;
	mul.f64 	%fd1951, %fd1950, %fd1947;
	mul.f64 	%fd1952, %fd1942, %fd1943;
	sub.f64 	%fd1953, %fd1951, %fd1952;
	mul.f64 	%fd1954, %fd1942, %fd1946;
	mul.f64 	%fd1955, %fd1950, %fd1944;
	sub.f64 	%fd1956, %fd1954, %fd1955;
	mul.f64 	%fd1957, %fd1953, %fd1953;
	fma.rn.f64 	%fd1958, %fd1949, %fd1949, %fd1957;
	fma.rn.f64 	%fd1959, %fd1956, %fd1956, %fd1958;
	div.rn.f64 	%fd1960, %fd1959, %fd67;
	div.rn.f64 	%fd1961, %fd327, %fd67;
	add.f64 	%fd1962, %fd1961, 0d0000000000000000;
	mov.f64 	%fd1963, 0d0000000000000000;
	mul.f64 	%fd1964, %fd1960, %fd327;
	div.rn.f64 	%fd1965, %fd1964, %fd67;
	sub.f64 	%fd1966, %fd1963, %fd1965;
	add.f64 	%fd1967, %fd58, %fd58;
	add.f64 	%fd1968, %fd59, %fd59;
	add.f64 	%fd1969, %fd60, %fd60;
	fma.rn.f64 	%fd1970, %fd1967, %fd1966, 0d0000000000000000;
	fma.rn.f64 	%fd1971, %fd1968, %fd1966, 0d0000000000000000;
	fma.rn.f64 	%fd1972, %fd1969, %fd1966, 0d0000000000000000;
	add.f64 	%fd1973, %fd1949, %fd1949;
	add.f64 	%fd1974, %fd1953, %fd1953;
	add.f64 	%fd1975, %fd1956, %fd1956;
	fma.rn.f64 	%fd1976, %fd1973, %fd1962, 0d0000000000000000;
	fma.rn.f64 	%fd1977, %fd1974, %fd1962, 0d0000000000000000;
	fma.rn.f64 	%fd1978, %fd1975, %fd1962, 0d0000000000000000;
	mul.f64 	%fd1979, %fd1946, %fd1978;
	mul.f64 	%fd1980, %fd1943, %fd1977;
	sub.f64 	%fd1981, %fd1979, %fd1980;
	mul.f64 	%fd1982, %fd1943, %fd1976;
	mul.f64 	%fd1983, %fd1950, %fd1978;
	sub.f64 	%fd1984, %fd1982, %fd1983;
	mul.f64 	%fd1985, %fd1950, %fd1977;
	mul.f64 	%fd1986, %fd1946, %fd1976;
	sub.f64 	%fd1987, %fd1985, %fd1986;
	add.f64 	%fd1988, %fd1981, 0d0000000000000000;
	add.f64 	%fd1989, %fd1984, 0d0000000000000000;
	add.f64 	%fd1990, %fd1987, 0d0000000000000000;
	mul.f64 	%fd1991, %fd1944, %fd1978;
	mul.f64 	%fd1992, %fd1947, %fd1977;
	mul.f64 	%fd1993, %fd1947, %fd1976;
	mul.f64 	%fd1994, %fd1942, %fd1978;
	mul.f64 	%fd1995, %fd1942, %fd1977;
	mul.f64 	%fd1996, %fd1944, %fd1976;
	sub.f64 	%fd1997, %fd1992, %fd1991;
	add.f64 	%fd1998, %fd1997, 0d0000000000000000;
	sub.f64 	%fd1999, %fd1994, %fd1993;
	add.f64 	%fd2000, %fd1999, 0d0000000000000000;
	sub.f64 	%fd2001, %fd1996, %fd1995;
	add.f64 	%fd2002, %fd2001, 0d0000000000000000;
	add.f64 	%fd2003, %fd1970, %fd4434;
	add.f64 	%fd2004, %fd1971, %fd4433;
	add.f64 	%fd2005, %fd1972, %fd4432;
	sub.f64 	%fd2006, %fd4431, %fd1970;
	sub.f64 	%fd2007, %fd4430, %fd1971;
	sub.f64 	%fd2008, %fd4429, %fd1972;
	add.f64 	%fd4434, %fd1998, %fd2003;
	add.f64 	%fd4433, %fd2000, %fd2004;
	add.f64 	%fd4432, %fd2002, %fd2005;
	sub.f64 	%fd2009, %fd4440, %fd1998;
	sub.f64 	%fd2010, %fd4439, %fd2000;
	sub.f64 	%fd2011, %fd4438, %fd2002;
	add.f64 	%fd4431, %fd1988, %fd2006;
	add.f64 	%fd4430, %fd1989, %fd2007;
	add.f64 	%fd4429, %fd1990, %fd2008;
	sub.f64 	%fd4440, %fd2009, %fd1988;
	sub.f64 	%fd4439, %fd2010, %fd1989;
	sub.f64 	%fd4438, %fd2011, %fd1990;
	bra.uni 	$L__BB1_194;

$L__BB1_185:
	sub.f64 	%fd1887, %fd31, %fd26;
	sub.f64 	%fd1888, %fd36, %fd33;
	mul.f64 	%fd1889, %fd1888, %fd108;
	fma.rn.f64 	%fd1890, %fd1887, %fd107, %fd1889;
	sub.f64 	%fd1891, %fd41, %fd38;
	fma.rn.f64 	%fd1892, %fd1891, %fd109, %fd1890;
	mul.f64 	%fd1893, %fd1892, %fd1892;
	div.rn.f64 	%fd1894, %fd1893, %fd110;
	div.rn.f64 	%fd1895, %fd327, %fd110;
	add.f64 	%fd1896, %fd1895, 0d0000000000000000;
	mov.f64 	%fd1897, 0d0000000000000000;
	mul.f64 	%fd1898, %fd1894, %fd327;
	div.rn.f64 	%fd1899, %fd1898, %fd110;
	sub.f64 	%fd1900, %fd1897, %fd1899;
	fma.rn.f64 	%fd1901, %fd300, %fd1900, 0d0000000000000000;
	fma.rn.f64 	%fd1902, %fd301, %fd1900, 0d0000000000000000;
	fma.rn.f64 	%fd1903, %fd302, %fd1900, 0d0000000000000000;
	fma.rn.f64 	%fd1904, %fd1892, %fd1896, 0d0000000000000000;
	fma.rn.f64 	%fd1905, %fd1892, %fd1896, %fd1904;
	fma.rn.f64 	%fd1906, %fd107, %fd1905, 0d0000000000000000;
	fma.rn.f64 	%fd1907, %fd108, %fd1905, 0d0000000000000000;
	fma.rn.f64 	%fd1908, %fd109, %fd1905, 0d0000000000000000;
	fma.rn.f64 	%fd1909, %fd1887, %fd1905, %fd1901;
	fma.rn.f64 	%fd1910, %fd1888, %fd1905, %fd1902;
	fma.rn.f64 	%fd1911, %fd1891, %fd1905, %fd1903;
	mul.f64 	%fd1912, %fd62, %fd1911;
	mul.f64 	%fd1913, %fd63, %fd1910;
	sub.f64 	%fd1914, %fd1912, %fd1913;
	mul.f64 	%fd1915, %fd63, %fd1909;
	mul.f64 	%fd1916, %fd61, %fd1911;
	sub.f64 	%fd1917, %fd1915, %fd1916;
	mul.f64 	%fd1918, %fd61, %fd1910;
	mul.f64 	%fd1919, %fd62, %fd1909;
	sub.f64 	%fd1920, %fd1918, %fd1919;
	add.f64 	%fd1921, %fd1914, 0d0000000000000000;
	add.f64 	%fd1922, %fd1917, 0d0000000000000000;
	add.f64 	%fd1923, %fd1920, 0d0000000000000000;
	mul.f64 	%fd1924, %fd59, %fd1911;
	mul.f64 	%fd1925, %fd60, %fd1910;
	mul.f64 	%fd1926, %fd60, %fd1909;
	mul.f64 	%fd1927, %fd58, %fd1911;
	mul.f64 	%fd1928, %fd58, %fd1910;
	mul.f64 	%fd1929, %fd59, %fd1909;
	sub.f64 	%fd1930, %fd1925, %fd1924;
	add.f64 	%fd1931, %fd1930, 0d0000000000000000;
	sub.f64 	%fd1932, %fd1927, %fd1926;
	add.f64 	%fd1933, %fd1932, 0d0000000000000000;
	sub.f64 	%fd1934, %fd1929, %fd1928;
	add.f64 	%fd1935, %fd1934, 0d0000000000000000;
	add.f64 	%fd1936, %fd1906, %fd4437;
	add.f64 	%fd1937, %fd1907, %fd4436;
	add.f64 	%fd1938, %fd1908, %fd4435;
	sub.f64 	%fd1939, %fd4431, %fd1906;
	sub.f64 	%fd1940, %fd4430, %fd1907;
	sub.f64 	%fd1941, %fd4429, %fd1908;
	add.f64 	%fd4440, %fd1931, %fd4440;
	add.f64 	%fd4439, %fd1933, %fd4439;
	add.f64 	%fd4438, %fd1935, %fd4438;
	sub.f64 	%fd4437, %fd1936, %fd1931;
	sub.f64 	%fd4436, %fd1937, %fd1933;
	sub.f64 	%fd4435, %fd1938, %fd1935;
	add.f64 	%fd4434, %fd1921, %fd4434;
	add.f64 	%fd4433, %fd1922, %fd4433;
	add.f64 	%fd4432, %fd1923, %fd4432;
	sub.f64 	%fd4431, %fd1939, %fd1921;
	sub.f64 	%fd4430, %fd1940, %fd1922;
	sub.f64 	%fd4429, %fd1941, %fd1923;

$L__BB1_194:
	mov.f64 	%fd2256, 0d7FF8000000000000;
	add.f64 	%fd4444, %fd2256, 0d0000000000000000;
	mov.f64 	%fd4441, 0d0000000000000000;
	mov.f64 	%fd4442, 0d0000000000000000;
	mov.f64 	%fd4443, 0d0000000000000000;
	@%p15 bra 	$L__BB1_197;

	setp.ge.f64 	%p140, %fd73, %fd72;
	mov.pred 	%p309, 0;
	@%p140 bra 	$L__BB1_197;

	mul.f64 	%fd2262, %fd67, 0d3BC79CA100000000;
	fma.rn.f64 	%fd4444, %fd2262, 0d0000000000000000, 0d0000000000000000;
	mov.pred 	%p309, -1;
	mov.f64 	%fd4441, %fd107;
	mov.f64 	%fd4442, %fd108;
	mov.f64 	%fd4443, %fd109;

$L__BB1_197:
	setp.gtu.f64 	%p142, %fd73, 0d0000000000000000;
	and.pred  	%p143, %p309, %p142;
	@%p143 bra 	$L__BB1_199;
	bra.uni 	$L__BB1_198;

$L__BB1_199:
	mul.f64 	%fd4448, %fd69, 0d0000000000000000;
	add.f64 	%fd2277, %fd4448, 0d0000000000000000;
	fma.rn.f64 	%fd2278, %fd2277, 0d3BC79CA100000000, 0d0000000000000000;
	add.f64 	%fd2279, %fd4441, %fd4441;
	add.f64 	%fd2280, %fd4442, %fd4442;
	add.f64 	%fd2281, %fd4443, %fd4443;
	fma.rn.f64 	%fd2282, %fd2279, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2283, %fd2280, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2284, %fd2281, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2285, %fd64, 0d0000000000000000, %fd2282;
	fma.rn.f64 	%fd2286, %fd65, 0d0000000000000000, %fd2283;
	fma.rn.f64 	%fd2287, %fd66, 0d0000000000000000, %fd2284;
	fma.rn.f64 	%fd4455, %fd4441, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4456, %fd4442, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd4457, %fd4443, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd2288, %fd62, %fd2287;
	mul.f64 	%fd2289, %fd63, %fd2286;
	sub.f64 	%fd2290, %fd2288, %fd2289;
	mul.f64 	%fd2291, %fd63, %fd2285;
	mul.f64 	%fd2292, %fd61, %fd2287;
	sub.f64 	%fd2293, %fd2291, %fd2292;
	mul.f64 	%fd2294, %fd61, %fd2286;
	mul.f64 	%fd2295, %fd62, %fd2285;
	sub.f64 	%fd2296, %fd2294, %fd2295;
	add.f64 	%fd4461, %fd2290, 0d0000000000000000;
	add.f64 	%fd4462, %fd2293, 0d0000000000000000;
	add.f64 	%fd4463, %fd2296, 0d0000000000000000;
	mul.f64 	%fd2297, %fd59, %fd2287;
	mul.f64 	%fd2298, %fd60, %fd2286;
	mul.f64 	%fd2299, %fd60, %fd2285;
	mul.f64 	%fd2300, %fd58, %fd2287;
	mul.f64 	%fd2301, %fd58, %fd2286;
	mul.f64 	%fd2302, %fd59, %fd2285;
	sub.f64 	%fd2303, %fd2298, %fd2297;
	add.f64 	%fd4458, %fd2303, 0d0000000000000000;
	sub.f64 	%fd2304, %fd2300, %fd2299;
	add.f64 	%fd4459, %fd2304, 0d0000000000000000;
	sub.f64 	%fd2305, %fd2302, %fd2301;
	add.f64 	%fd4460, %fd2305, 0d0000000000000000;
	mul.f64 	%fd4449, %fd70, 0d0000000000000000;
	add.f64 	%fd4453, %fd4449, 0d0000000000000000;
	mul.f64 	%fd4446, %fd68, 0d0000000000000000;
	add.f64 	%fd4451, %fd4446, 0d0000000000000000;
	mul.f64 	%fd4447, %fd71, 0d0000000000000000;
	add.f64 	%fd4454, %fd4447, %fd2278;
	mul.f64 	%fd4445, %fd67, 0d0000000000000000;
	add.f64 	%fd4450, %fd4445, 0d0000000000000000;
	bra.uni 	$L__BB1_200;

$L__BB1_198:
	mul.f64 	%fd4449, %fd70, 0d0000000000000000;
	mov.f64 	%fd4450, 0d0000000000000000;
	mul.f64 	%fd4448, %fd69, 0d0000000000000000;
	mul.f64 	%fd4447, %fd71, 0d0000000000000000;
	mul.f64 	%fd4446, %fd68, 0d0000000000000000;
	mul.f64 	%fd4445, %fd67, 0d0000000000000000;
	mov.f64 	%fd4451, %fd4450;
	mov.f64 	%fd4444, %fd4450;
	mov.f64 	%fd4453, %fd4450;
	mov.f64 	%fd4454, %fd4450;
	mov.f64 	%fd4455, %fd4450;
	mov.f64 	%fd4456, %fd4450;
	mov.f64 	%fd4457, %fd4450;
	mov.f64 	%fd4458, %fd4450;
	mov.f64 	%fd4459, %fd4450;
	mov.f64 	%fd4460, %fd4450;
	mov.f64 	%fd4461, %fd4450;
	mov.f64 	%fd4462, %fd4450;
	mov.f64 	%fd4463, %fd4450;

$L__BB1_200:
	mov.f64 	%fd4267, 0d3FF0000000000000;
	sub.f64 	%fd4266, %fd4267, %fd24;
	sub.f64 	%fd4265, %fd4267, %fd29;
	add.f64 	%fd2306, %fd4450, 0d0000000000000000;
	mov.f64 	%fd2307, 0d0000000000000000;
	selp.f64 	%fd2308, %fd2306, %fd4450, %p15;
	add.f64 	%fd2309, %fd4444, 0d0000000000000000;
	selp.f64 	%fd2310, %fd2309, %fd4444, %p15;
	add.f64 	%fd2311, %fd4449, %fd2310;
	add.f64 	%fd2312, %fd4446, %fd2308;
	add.f64 	%fd2313, %fd4447, %fd4453;
	add.f64 	%fd2314, %fd4446, %fd2313;
	add.f64 	%fd2315, %fd4446, %fd2314;
	add.f64 	%fd2316, %fd4445, %fd2311;
	fma.rn.f64 	%fd2317, %fd64, %fd2312, %fd4458;
	fma.rn.f64 	%fd2318, %fd65, %fd2312, %fd4459;
	fma.rn.f64 	%fd2319, %fd66, %fd2312, %fd4460;
	fma.rn.f64 	%fd2320, %fd61, %fd2312, %fd4455;
	fma.rn.f64 	%fd2321, %fd62, %fd2312, %fd4456;
	fma.rn.f64 	%fd2322, %fd63, %fd2312, %fd4457;
	add.f64 	%fd2323, %fd4448, %fd4451;
	fma.rn.f64 	%fd2324, %fd64, %fd2323, %fd4461;
	fma.rn.f64 	%fd2325, %fd65, %fd2323, %fd4462;
	fma.rn.f64 	%fd2326, %fd66, %fd2323, %fd4463;
	fma.rn.f64 	%fd2327, %fd58, %fd2323, %fd2320;
	fma.rn.f64 	%fd2328, %fd59, %fd2323, %fd2321;
	fma.rn.f64 	%fd2329, %fd60, %fd2323, %fd2322;
	add.f64 	%fd2330, %fd61, %fd61;
	add.f64 	%fd2331, %fd62, %fd62;
	add.f64 	%fd2332, %fd63, %fd63;
	fma.rn.f64 	%fd2333, %fd2330, %fd2316, %fd2317;
	fma.rn.f64 	%fd2334, %fd2331, %fd2316, %fd2318;
	fma.rn.f64 	%fd2335, %fd2332, %fd2316, %fd2319;
	fma.rn.f64 	%fd2336, %fd61, %fd2315, %fd2324;
	fma.rn.f64 	%fd2337, %fd62, %fd2315, %fd2325;
	fma.rn.f64 	%fd2338, %fd63, %fd2315, %fd2326;
	fma.rn.f64 	%fd2339, %fd58, %fd2315, %fd2333;
	fma.rn.f64 	%fd2340, %fd59, %fd2315, %fd2334;
	fma.rn.f64 	%fd2341, %fd60, %fd2315, %fd2335;
	add.f64 	%fd2342, %fd58, %fd58;
	add.f64 	%fd2343, %fd4448, %fd4454;
	add.f64 	%fd2344, %fd59, %fd59;
	add.f64 	%fd2345, %fd60, %fd60;
	fma.rn.f64 	%fd2346, %fd2342, %fd2343, %fd2336;
	fma.rn.f64 	%fd2347, %fd2344, %fd2343, %fd2337;
	fma.rn.f64 	%fd2348, %fd2345, %fd2343, %fd2338;
	add.f64 	%fd2349, %fd4431, %fd2327;
	add.f64 	%fd2350, %fd4430, %fd2328;
	add.f64 	%fd2351, %fd4429, %fd2329;
	sub.f64 	%fd2352, %fd4437, %fd2327;
	sub.f64 	%fd2353, %fd4436, %fd2328;
	sub.f64 	%fd2354, %fd4435, %fd2329;
	add.f64 	%fd2355, %fd4440, %fd2339;
	add.f64 	%fd2356, %fd4439, %fd2340;
	add.f64 	%fd2357, %fd4438, %fd2341;
	sub.f64 	%fd2358, %fd2352, %fd2339;
	sub.f64 	%fd2359, %fd2353, %fd2340;
	sub.f64 	%fd2360, %fd2354, %fd2341;
	add.f64 	%fd2361, %fd4434, %fd2346;
	add.f64 	%fd2362, %fd4433, %fd2347;
	add.f64 	%fd2363, %fd4432, %fd2348;
	sub.f64 	%fd2364, %fd2349, %fd2346;
	sub.f64 	%fd2365, %fd2350, %fd2347;
	sub.f64 	%fd2366, %fd2351, %fd2348;
	add.f64 	%fd2367, %fd4401, 0d0000000000000000;
	sub.f64 	%fd2368, %fd2307, %fd2367;
	fma.rn.f64 	%fd2369, %fd57, %fd2368, 0d0000000000000000;
	fma.rn.f64 	%fd473, %fd29, %fd2368, 0d0000000000000000;
	fma.rn.f64 	%fd2370, %fd56, %fd2368, 0d0000000000000000;
	fma.rn.f64 	%fd474, %fd4265, %fd2368, 0d0000000000000000;
	sub.f64 	%fd2371, %fd2369, %fd2370;
	fma.rn.f64 	%fd2372, %fd54, %fd2367, 0d0000000000000000;
	fma.rn.f64 	%fd475, %fd24, %fd2367, 0d0000000000000000;
	fma.rn.f64 	%fd2373, %fd53, %fd2367, 0d0000000000000000;
	fma.rn.f64 	%fd476, %fd4266, %fd2367, 0d0000000000000000;
	sub.f64 	%fd2374, %fd2372, %fd2373;
	add.f64 	%fd2375, %fd4402, 0d0000000000000000;
	sub.f64 	%fd2376, %fd2307, %fd2375;
	fma.rn.f64 	%fd2377, %fd52, %fd2376, %fd2371;
	fma.rn.f64 	%fd477, %fd29, %fd2376, 0d0000000000000000;
	fma.rn.f64 	%fd2378, %fd51, %fd2376, 0d0000000000000000;
	fma.rn.f64 	%fd478, %fd4265, %fd2376, 0d0000000000000000;
	sub.f64 	%fd2379, %fd2377, %fd2378;
	fma.rn.f64 	%fd2380, %fd49, %fd2375, %fd2374;
	fma.rn.f64 	%fd479, %fd24, %fd2375, 0d0000000000000000;
	fma.rn.f64 	%fd2381, %fd48, %fd2375, 0d0000000000000000;
	fma.rn.f64 	%fd480, %fd4266, %fd2375, 0d0000000000000000;
	sub.f64 	%fd2382, %fd2380, %fd2381;
	add.f64 	%fd2383, %fd4403, 0d0000000000000000;
	sub.f64 	%fd2384, %fd2307, %fd2383;
	fma.rn.f64 	%fd2385, %fd47, %fd2384, %fd2379;
	fma.rn.f64 	%fd481, %fd29, %fd2384, 0d0000000000000000;
	fma.rn.f64 	%fd2386, %fd46, %fd2384, 0d0000000000000000;
	fma.rn.f64 	%fd482, %fd4265, %fd2384, 0d0000000000000000;
	sub.f64 	%fd2387, %fd2385, %fd2386;
	fma.rn.f64 	%fd2388, %fd44, %fd2383, %fd2382;
	fma.rn.f64 	%fd483, %fd24, %fd2383, 0d0000000000000000;
	fma.rn.f64 	%fd2389, %fd43, %fd2383, 0d0000000000000000;
	fma.rn.f64 	%fd484, %fd4266, %fd2383, 0d0000000000000000;
	sub.f64 	%fd2390, %fd2388, %fd2389;
	add.f64 	%fd2391, %fd4398, 0d0000000000000000;
	sub.f64 	%fd2392, %fd2307, %fd2391;
	fma.rn.f64 	%fd2393, %fd42, %fd2392, %fd2387;
	fma.rn.f64 	%fd2394, %fd29, %fd2392, 0d0000000000000000;
	add.f64 	%fd485, %fd2394, %fd2357;
	fma.rn.f64 	%fd2395, %fd41, %fd2392, 0d0000000000000000;
	fma.rn.f64 	%fd2396, %fd4265, %fd2392, 0d0000000000000000;
	add.f64 	%fd486, %fd2396, %fd2360;
	sub.f64 	%fd2397, %fd2393, %fd2395;
	fma.rn.f64 	%fd2398, %fd39, %fd2391, %fd2390;
	fma.rn.f64 	%fd2399, %fd24, %fd2391, 0d0000000000000000;
	add.f64 	%fd487, %fd2399, %fd2363;
	fma.rn.f64 	%fd2400, %fd38, %fd2391, 0d0000000000000000;
	fma.rn.f64 	%fd2401, %fd4266, %fd2391, 0d0000000000000000;
	add.f64 	%fd488, %fd2401, %fd2366;
	sub.f64 	%fd2402, %fd2398, %fd2400;
	add.f64 	%fd2403, %fd4399, 0d0000000000000000;
	sub.f64 	%fd2404, %fd2307, %fd2403;
	fma.rn.f64 	%fd2405, %fd37, %fd2404, %fd2397;
	fma.rn.f64 	%fd2406, %fd29, %fd2404, 0d0000000000000000;
	add.f64 	%fd489, %fd2406, %fd2356;
	fma.rn.f64 	%fd2407, %fd36, %fd2404, 0d0000000000000000;
	fma.rn.f64 	%fd2408, %fd4265, %fd2404, 0d0000000000000000;
	add.f64 	%fd490, %fd2408, %fd2359;
	sub.f64 	%fd2409, %fd2405, %fd2407;
	fma.rn.f64 	%fd2410, %fd34, %fd2403, %fd2402;
	fma.rn.f64 	%fd2411, %fd24, %fd2403, 0d0000000000000000;
	add.f64 	%fd491, %fd2411, %fd2362;
	fma.rn.f64 	%fd2412, %fd33, %fd2403, 0d0000000000000000;
	fma.rn.f64 	%fd2413, %fd4266, %fd2403, 0d0000000000000000;
	add.f64 	%fd492, %fd2413, %fd2365;
	sub.f64 	%fd2414, %fd2410, %fd2412;
	add.f64 	%fd2415, %fd4400, 0d0000000000000000;
	sub.f64 	%fd2416, %fd2307, %fd2415;
	fma.rn.f64 	%fd2417, %fd32, %fd2416, %fd2409;
	fma.rn.f64 	%fd2418, %fd29, %fd2416, 0d0000000000000000;
	add.f64 	%fd493, %fd2418, %fd2355;
	fma.rn.f64 	%fd2419, %fd31, %fd2416, 0d0000000000000000;
	fma.rn.f64 	%fd2420, %fd4265, %fd2416, 0d0000000000000000;
	add.f64 	%fd494, %fd2420, %fd2358;
	sub.f64 	%fd2421, %fd2417, %fd2419;
	fma.rn.f64 	%fd2422, %fd27, %fd2415, %fd2414;
	fma.rn.f64 	%fd2423, %fd24, %fd2415, 0d0000000000000000;
	add.f64 	%fd495, %fd2423, %fd2361;
	fma.rn.f64 	%fd2424, %fd26, %fd2415, 0d0000000000000000;
	fma.rn.f64 	%fd2425, %fd4266, %fd2415, 0d0000000000000000;
	add.f64 	%fd496, %fd2425, %fd2364;
	sub.f64 	%fd497, %fd2422, %fd2424;
	add.f64 	%fd498, %fd2421, 0d0000000000000000;
	setp.eq.s64 	%p145, %rd175, 0;
	@%p145 bra 	$L__BB1_202;

	mul.lo.s64 	%rd336, %rd96, %rd82;
	add.s64 	%rd334, %rd175, %rd336;
	// begin inline asm
	{ atom.add.f64 %fd2426,[%rd334],%fd2307; }

	// end inline asm
	add.s64 	%rd335, %rd334, 8;
	// begin inline asm
	{ atom.add.f64 %fd2428,[%rd335],%fd498; }

	// end inline asm
	bra.uni 	$L__BB1_204;

$L__BB1_202:
	setp.eq.s64 	%p146, %rd134, 0;
	@%p146 bra 	$L__BB1_204;

	add.s64 	%rd337, %rd134, %rd98;
	mov.f64 	%fd2431, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2430,[%rd337],%fd2431; }

	// end inline asm
	add.s64 	%rd338, %rd337, 8;
	// begin inline asm
	{ atom.add.f64 %fd2432,[%rd338],%fd498; }

	// end inline asm

$L__BB1_204:
	add.f64 	%fd499, %fd497, 0d0000000000000000;
	@%p145 bra 	$L__BB1_206;

	mul.lo.s64 	%rd341, %rd96, %rd82;
	add.s64 	%rd339, %rd175, %rd341;
	// begin inline asm
	{ atom.add.f64 %fd2434,[%rd339],%fd499; }

	// end inline asm
	add.s64 	%rd340, %rd339, 8;
	mov.f64 	%fd2437, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2436,[%rd340],%fd2437; }

	// end inline asm
	bra.uni 	$L__BB1_208;

$L__BB1_206:
	setp.eq.s64 	%p148, %rd134, 0;
	@%p148 bra 	$L__BB1_208;

	add.s64 	%rd342, %rd134, %rd98;
	// begin inline asm
	{ atom.add.f64 %fd2438,[%rd342],%fd499; }

	// end inline asm
	add.s64 	%rd343, %rd342, 8;
	mov.f64 	%fd2441, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd2440,[%rd343],%fd2441; }

	// end inline asm

$L__BB1_208:
	setp.eq.s64 	%p149, %rd181, 0;
	@%p149 bra 	$L__BB1_210;

	mul.lo.s64 	%rd347, %rd109, %rd83;
	add.s64 	%rd344, %rd181, %rd347;
	// begin inline asm
	{ atom.add.f64 %fd2442,[%rd344],%fd481; }

	// end inline asm
	add.s64 	%rd345, %rd344, 8;
	// begin inline asm
	{ atom.add.f64 %fd2444,[%rd345],%fd477; }

	// end inline asm
	add.s64 	%rd346, %rd344, 16;
	// begin inline asm
	{ atom.add.f64 %fd2446,[%rd346],%fd473; }

	// end inline asm
	bra.uni 	$L__BB1_212;

$L__BB1_210:
	setp.eq.s64 	%p150, %rd152, 0;
	@%p150 bra 	$L__BB1_212;

	mul.lo.s64 	%rd622, %rd109, %rd58;
	add.s64 	%rd348, %rd152, %rd622;
	// begin inline asm
	{ atom.add.f64 %fd2448,[%rd348],%fd481; }

	// end inline asm
	add.s64 	%rd349, %rd348, 8;
	// begin inline asm
	{ atom.add.f64 %fd2450,[%rd349],%fd477; }

	// end inline asm
	add.s64 	%rd350, %rd348, 16;
	// begin inline asm
	{ atom.add.f64 %fd2452,[%rd350],%fd473; }

	// end inline asm

$L__BB1_212:
	@%p149 bra 	$L__BB1_214;

	mul.lo.s64 	%rd354, %rd107, %rd83;
	add.s64 	%rd351, %rd181, %rd354;
	// begin inline asm
	{ atom.add.f64 %fd2454,[%rd351],%fd482; }

	// end inline asm
	add.s64 	%rd352, %rd351, 8;
	// begin inline asm
	{ atom.add.f64 %fd2456,[%rd352],%fd478; }

	// end inline asm
	add.s64 	%rd353, %rd351, 16;
	// begin inline asm
	{ atom.add.f64 %fd2458,[%rd353],%fd474; }

	// end inline asm
	bra.uni 	$L__BB1_216;

$L__BB1_214:
	setp.eq.s64 	%p152, %rd152, 0;
	@%p152 bra 	$L__BB1_216;

	mul.lo.s64 	%rd621, %rd107, %rd58;
	add.s64 	%rd355, %rd152, %rd621;
	// begin inline asm
	{ atom.add.f64 %fd2460,[%rd355],%fd482; }

	// end inline asm
	add.s64 	%rd356, %rd355, 8;
	// begin inline asm
	{ atom.add.f64 %fd2462,[%rd356],%fd478; }

	// end inline asm
	add.s64 	%rd357, %rd355, 16;
	// begin inline asm
	{ atom.add.f64 %fd2464,[%rd357],%fd474; }

	// end inline asm

$L__BB1_216:
	@%p149 bra 	$L__BB1_218;

	mul.lo.s64 	%rd361, %rd105, %rd83;
	add.s64 	%rd358, %rd181, %rd361;
	// begin inline asm
	{ atom.add.f64 %fd2466,[%rd358],%fd483; }

	// end inline asm
	add.s64 	%rd359, %rd358, 8;
	// begin inline asm
	{ atom.add.f64 %fd2468,[%rd359],%fd479; }

	// end inline asm
	add.s64 	%rd360, %rd358, 16;
	// begin inline asm
	{ atom.add.f64 %fd2470,[%rd360],%fd475; }

	// end inline asm
	bra.uni 	$L__BB1_220;

$L__BB1_218:
	setp.eq.s64 	%p154, %rd152, 0;
	@%p154 bra 	$L__BB1_220;

	mul.lo.s64 	%rd620, %rd105, %rd58;
	add.s64 	%rd362, %rd152, %rd620;
	// begin inline asm
	{ atom.add.f64 %fd2472,[%rd362],%fd483; }

	// end inline asm
	add.s64 	%rd363, %rd362, 8;
	// begin inline asm
	{ atom.add.f64 %fd2474,[%rd363],%fd479; }

	// end inline asm
	add.s64 	%rd364, %rd362, 16;
	// begin inline asm
	{ atom.add.f64 %fd2476,[%rd364],%fd475; }

	// end inline asm

$L__BB1_220:
	@%p149 bra 	$L__BB1_222;

	mul.lo.s64 	%rd368, %rd103, %rd83;
	add.s64 	%rd365, %rd181, %rd368;
	// begin inline asm
	{ atom.add.f64 %fd2478,[%rd365],%fd484; }

	// end inline asm
	add.s64 	%rd366, %rd365, 8;
	// begin inline asm
	{ atom.add.f64 %fd2480,[%rd366],%fd480; }

	// end inline asm
	add.s64 	%rd367, %rd365, 16;
	// begin inline asm
	{ atom.add.f64 %fd2482,[%rd367],%fd476; }

	// end inline asm
	bra.uni 	$L__BB1_224;

$L__BB1_222:
	setp.eq.s64 	%p156, %rd152, 0;
	@%p156 bra 	$L__BB1_224;

	cvt.s64.s32 	%rd629, %r9;
	mul.lo.s64 	%rd628, %rd629, %rd58;
	add.s64 	%rd369, %rd152, %rd628;
	// begin inline asm
	{ atom.add.f64 %fd2484,[%rd369],%fd484; }

	// end inline asm
	add.s64 	%rd370, %rd369, 8;
	// begin inline asm
	{ atom.add.f64 %fd2486,[%rd370],%fd480; }

	// end inline asm
	add.s64 	%rd371, %rd369, 16;
	// begin inline asm
	{ atom.add.f64 %fd2488,[%rd371],%fd476; }

	// end inline asm

$L__BB1_224:
	setp.eq.s64 	%p157, %rd179, 0;
	add.f64 	%fd500, %fd493, 0d0000000000000000;
	add.f64 	%fd501, %fd489, 0d0000000000000000;
	add.f64 	%fd502, %fd485, 0d0000000000000000;
	@%p157 bra 	$L__BB1_226;

	mul.lo.s64 	%rd375, %rd109, %rd84;
	add.s64 	%rd372, %rd179, %rd375;
	// begin inline asm
	{ atom.add.f64 %fd2490,[%rd372],%fd500; }

	// end inline asm
	add.s64 	%rd373, %rd372, 8;
	// begin inline asm
	{ atom.add.f64 %fd2492,[%rd373],%fd501; }

	// end inline asm
	add.s64 	%rd374, %rd372, 16;
	// begin inline asm
	{ atom.add.f64 %fd2494,[%rd374],%fd502; }

	// end inline asm
	bra.uni 	$L__BB1_228;

$L__BB1_226:
	setp.eq.s64 	%p158, %rd148, 0;
	@%p158 bra 	$L__BB1_228;

	mul.lo.s64 	%rd619, %rd109, %rd57;
	add.s64 	%rd376, %rd148, %rd619;
	// begin inline asm
	{ atom.add.f64 %fd2496,[%rd376],%fd500; }

	// end inline asm
	add.s64 	%rd377, %rd376, 8;
	// begin inline asm
	{ atom.add.f64 %fd2498,[%rd377],%fd501; }

	// end inline asm
	add.s64 	%rd378, %rd376, 16;
	// begin inline asm
	{ atom.add.f64 %fd2500,[%rd378],%fd502; }

	// end inline asm

$L__BB1_228:
	add.f64 	%fd503, %fd494, 0d0000000000000000;
	add.f64 	%fd504, %fd490, 0d0000000000000000;
	add.f64 	%fd505, %fd486, 0d0000000000000000;
	@%p157 bra 	$L__BB1_230;

	mul.lo.s64 	%rd382, %rd107, %rd84;
	add.s64 	%rd379, %rd179, %rd382;
	// begin inline asm
	{ atom.add.f64 %fd2502,[%rd379],%fd503; }

	// end inline asm
	add.s64 	%rd380, %rd379, 8;
	// begin inline asm
	{ atom.add.f64 %fd2504,[%rd380],%fd504; }

	// end inline asm
	add.s64 	%rd381, %rd379, 16;
	// begin inline asm
	{ atom.add.f64 %fd2506,[%rd381],%fd505; }

	// end inline asm
	bra.uni 	$L__BB1_232;

$L__BB1_230:
	setp.eq.s64 	%p160, %rd148, 0;
	@%p160 bra 	$L__BB1_232;

	mul.lo.s64 	%rd618, %rd107, %rd57;
	add.s64 	%rd383, %rd148, %rd618;
	// begin inline asm
	{ atom.add.f64 %fd2508,[%rd383],%fd503; }

	// end inline asm
	add.s64 	%rd384, %rd383, 8;
	// begin inline asm
	{ atom.add.f64 %fd2510,[%rd384],%fd504; }

	// end inline asm
	add.s64 	%rd385, %rd383, 16;
	// begin inline asm
	{ atom.add.f64 %fd2512,[%rd385],%fd505; }

	// end inline asm

$L__BB1_232:
	add.f64 	%fd506, %fd495, 0d0000000000000000;
	add.f64 	%fd507, %fd491, 0d0000000000000000;
	add.f64 	%fd508, %fd487, 0d0000000000000000;
	@%p157 bra 	$L__BB1_234;

	mul.lo.s64 	%rd389, %rd105, %rd84;
	add.s64 	%rd386, %rd179, %rd389;
	// begin inline asm
	{ atom.add.f64 %fd2514,[%rd386],%fd506; }

	// end inline asm
	add.s64 	%rd387, %rd386, 8;
	// begin inline asm
	{ atom.add.f64 %fd2516,[%rd387],%fd507; }

	// end inline asm
	add.s64 	%rd388, %rd386, 16;
	// begin inline asm
	{ atom.add.f64 %fd2518,[%rd388],%fd508; }

	// end inline asm
	bra.uni 	$L__BB1_236;

$L__BB1_234:
	setp.eq.s64 	%p162, %rd148, 0;
	@%p162 bra 	$L__BB1_236;

	mul.lo.s64 	%rd617, %rd105, %rd57;
	add.s64 	%rd390, %rd148, %rd617;
	// begin inline asm
	{ atom.add.f64 %fd2520,[%rd390],%fd506; }

	// end inline asm
	add.s64 	%rd391, %rd390, 8;
	// begin inline asm
	{ atom.add.f64 %fd2522,[%rd391],%fd507; }

	// end inline asm
	add.s64 	%rd392, %rd390, 16;
	// begin inline asm
	{ atom.add.f64 %fd2524,[%rd392],%fd508; }

	// end inline asm

$L__BB1_236:
	add.f64 	%fd509, %fd496, 0d0000000000000000;
	add.f64 	%fd510, %fd492, 0d0000000000000000;
	add.f64 	%fd511, %fd488, 0d0000000000000000;
	@%p157 bra 	$L__BB1_238;

	mul.lo.s64 	%rd396, %rd103, %rd84;
	add.s64 	%rd393, %rd179, %rd396;
	// begin inline asm
	{ atom.add.f64 %fd2526,[%rd393],%fd509; }

	// end inline asm
	add.s64 	%rd394, %rd393, 8;
	// begin inline asm
	{ atom.add.f64 %fd2528,[%rd394],%fd510; }

	// end inline asm
	add.s64 	%rd395, %rd393, 16;
	// begin inline asm
	{ atom.add.f64 %fd2530,[%rd395],%fd511; }

	// end inline asm
	bra.uni 	$L__BB1_240;

$L__BB1_238:
	setp.eq.s64 	%p164, %rd148, 0;
	@%p164 bra 	$L__BB1_240;

	cvt.s64.s32 	%rd627, %r9;
	mul.lo.s64 	%rd626, %rd627, %rd57;
	add.s64 	%rd397, %rd148, %rd626;
	// begin inline asm
	{ atom.add.f64 %fd2532,[%rd397],%fd509; }

	// end inline asm
	add.s64 	%rd398, %rd397, 8;
	// begin inline asm
	{ atom.add.f64 %fd2534,[%rd398],%fd510; }

	// end inline asm
	add.s64 	%rd399, %rd397, 16;
	// begin inline asm
	{ atom.add.f64 %fd2536,[%rd399],%fd511; }

	// end inline asm

$L__BB1_240:
	setp.eq.s64 	%p165, %rd189, 0;
	add.f64 	%fd512, %fd328, 0d0000000000000000;
	@%p165 bra 	$L__BB1_242;

	mul.lo.s64 	%rd402, %rd100, %rd85;
	add.s64 	%rd400, %rd189, %rd402;
	// begin inline asm
	{ atom.add.f64 %fd2538,[%rd400],%fd512; }

	// end inline asm
	mul.lo.s64 	%rd403, %rd99, %rd85;
	add.s64 	%rd401, %rd189, %rd403;
	// begin inline asm
	{ atom.add.f64 %fd2540,[%rd401],%fd512; }

	// end inline asm
	bra.uni 	$L__BB1_414;

$L__BB1_242:
	setp.eq.s64 	%p166, %rd160, 0;
	@%p166 bra 	$L__BB1_414;

	mul.lo.s64 	%rd616, %rd99, %rd613;
	mul.lo.s64 	%rd615, %rd100, %rd613;
	add.s64 	%rd404, %rd160, %rd615;
	// begin inline asm
	{ atom.add.f64 %fd2542,[%rd404],%fd512; }

	// end inline asm
	add.s64 	%rd405, %rd160, %rd616;
	// begin inline asm
	{ atom.add.f64 %fd2544,[%rd405],%fd512; }

	// end inline asm

$L__BB1_414:
	ld.param.u64 	%rd612, [val_IPC_collisions_cuda_kernel_backward_param_0+24];
	mov.u32 	%r914, %ntid.x;
	mov.u32 	%r913, %nctaid.x;
	mul.wide.u32 	%rd611, %r914, %r913;
	add.s64 	%rd630, %rd630, %rd611;
	setp.lt.u64 	%p307, %rd630, %rd612;
	@%p307 bra 	$L__BB1_2;

$L__BB1_415:
	ret;

}
	// .globl	add_projected_x_to_y_cuda_kernel_forward
.visible .entry add_projected_x_to_y_cuda_kernel_forward(
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_forward_param_4[56]
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<92>;
	.reg .f64 	%fd<35>;
	.reg .b64 	%rd<71>;


	ld.param.v2.u32 	{%r43, %r44}, [add_projected_x_to_y_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r45, %r46}, [add_projected_x_to_y_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r51, %r52}, [add_projected_x_to_y_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r59, %r60}, [add_projected_x_to_y_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r67, %r68}, [add_projected_x_to_y_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r75, %r76}, [add_projected_x_to_y_cuda_kernel_forward_param_4+32];
	ld.param.u64 	%rd34, [add_projected_x_to_y_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd32, [add_projected_x_to_y_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd30, [add_projected_x_to_y_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd28, [add_projected_x_to_y_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd27, [add_projected_x_to_y_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [add_projected_x_to_y_cuda_kernel_forward_param_0+16];
	mov.u32 	%r79, %ntid.x;
	cvt.u64.u32 	%rd1, %r79;
	mov.u32 	%r80, %ctaid.x;
	mul.wide.u32 	%rd36, %r79, %r80;
	mov.u32 	%r81, %tid.x;
	cvt.u64.u32 	%rd37, %r81;
	add.s64 	%rd67, %rd36, %rd37;
	setp.ge.u64 	%p1, %rd67, %rd27;
	@%p1 bra 	$L__BB2_15;

	cvta.to.global.u64 	%rd4, %rd34;
	cvta.to.global.u64 	%rd5, %rd32;
	cvta.to.global.u64 	%rd6, %rd30;
	cvt.s64.s32 	%rd8, %r46;
	cvt.s64.s32 	%rd9, %r45;
	cvt.s64.s32 	%rd10, %r44;
	cvt.s64.s32 	%rd11, %r67;
	cvt.s64.s32 	%rd12, %r59;
	cvt.s64.s32 	%rd13, %r75;
	cvt.s64.s32 	%rd14, %r51;
	mov.u32 	%r82, %nctaid.x;
	cvt.u64.u32 	%rd38, %r82;
	mul.lo.s64 	%rd15, %rd1, %rd38;

$L__BB2_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd68, %rd67;
	@%p2 bra 	$L__BB2_6;

	or.b64  	%rd39, %rd67, %rd8;
	and.b64  	%rd40, %rd39, -4294967296;
	setp.eq.s64 	%p3, %rd40, 0;
	@%p3 bra 	$L__BB2_5;

	div.u64 	%rd68, %rd67, %rd8;
	bra.uni 	$L__BB2_6;

$L__BB2_5:
	cvt.u32.u64 	%r83, %rd8;
	cvt.u32.u64 	%r84, %rd67;
	div.u32 	%r85, %r84, %r83;
	cvt.u64.u32 	%rd68, %r85;

$L__BB2_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB2_10;

	or.b64  	%rd41, %rd68, %rd9;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd68, %rd68, %rd9;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r86, %rd9;
	cvt.u32.u64 	%r87, %rd68;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd68, %r88;

$L__BB2_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB2_14;

	or.b64  	%rd43, %rd68, %rd10;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p7, %rd44, 0;
	@%p7 bra 	$L__BB2_13;

	div.u64 	%rd68, %rd68, %rd10;
	bra.uni 	$L__BB2_14;

$L__BB2_13:
	cvt.u32.u64 	%r89, %rd10;
	cvt.u32.u64 	%r90, %rd68;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd68, %r91;

$L__BB2_14:
	cvt.s64.s32 	%rd57, %rd68;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd58, %rd57, 0, %p8;
	mul.lo.s64 	%rd59, %rd58, %rd11;
	add.s64 	%rd60, %rd5, %rd59;
	mul.lo.s64 	%rd61, %rd58, %rd12;
	add.s64 	%rd62, %rd6, %rd61;
	mul.lo.s64 	%rd63, %rd58, %rd13;
	add.s64 	%rd64, %rd4, %rd63;
	ld.global.f64 	%fd25, [%rd64];
	mov.f64 	%fd26, 0d3FF0000000000000;
	sub.f64 	%fd27, %fd26, %fd25;
	ld.global.f64 	%fd28, [%rd64+8];
	sub.f64 	%fd29, %fd27, %fd28;
	ld.global.f64 	%fd30, [%rd64+16];
	sub.f64 	%fd31, %fd29, %fd30;
	ld.global.f64 	%fd32, [%rd62];
	mul.f64 	%fd2, %fd32, %fd31;
	ld.global.f64 	%fd33, [%rd62+8];
	mul.f64 	%fd4, %fd33, %fd31;
	ld.global.f64 	%fd34, [%rd62+16];
	mul.f64 	%fd6, %fd34, %fd31;
	mul.f64 	%fd8, %fd32, %fd25;
	mul.f64 	%fd10, %fd33, %fd25;
	mul.f64 	%fd12, %fd34, %fd25;
	mul.f64 	%fd14, %fd32, %fd28;
	mul.f64 	%fd16, %fd33, %fd28;
	mul.f64 	%fd18, %fd34, %fd28;
	mul.f64 	%fd20, %fd32, %fd30;
	mul.f64 	%fd22, %fd33, %fd30;
	mul.f64 	%fd24, %fd34, %fd30;
	ld.global.s32 	%rd65, [%rd60];
	mul.lo.s64 	%rd66, %rd65, %rd14;
	add.s64 	%rd45, %rd28, %rd66;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd45],%fd2; }

	// end inline asm
	add.s64 	%rd46, %rd45, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd46],%fd4; }

	// end inline asm
	add.s64 	%rd47, %rd45, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd47],%fd6; }

	// end inline asm
	add.s64 	%rd48, %rd45, 24;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd48],%fd8; }

	// end inline asm
	add.s64 	%rd49, %rd45, 32;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd49],%fd10; }

	// end inline asm
	add.s64 	%rd50, %rd45, 40;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd50],%fd12; }

	// end inline asm
	add.s64 	%rd51, %rd45, 48;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd51],%fd14; }

	// end inline asm
	add.s64 	%rd52, %rd45, 56;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd52],%fd16; }

	// end inline asm
	add.s64 	%rd53, %rd45, 64;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd53],%fd18; }

	// end inline asm
	add.s64 	%rd54, %rd45, 72;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd54],%fd20; }

	// end inline asm
	add.s64 	%rd55, %rd45, 80;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd55],%fd22; }

	// end inline asm
	add.s64 	%rd56, %rd45, 88;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd56],%fd24; }

	// end inline asm
	add.s64 	%rd67, %rd67, %rd15;
	setp.lt.u64 	%p9, %rd67, %rd27;
	@%p9 bra 	$L__BB2_2;

$L__BB2_15:
	ret;

}
	// .globl	add_projected_x_to_y_cuda_kernel_backward
.visible .entry add_projected_x_to_y_cuda_kernel_backward(
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 add_projected_x_to_y_cuda_kernel_backward_param_8[56]
)
{
	.reg .pred 	%p<26>;
	.reg .b16 	%rs<57>;
	.reg .b32 	%r<143>;
	.reg .f64 	%fd<242>;
	.reg .b64 	%rd<130>;


	ld.param.v2.u32 	{%r70, %r71}, [add_projected_x_to_y_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [add_projected_x_to_y_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [add_projected_x_to_y_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [add_projected_x_to_y_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [add_projected_x_to_y_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r102, %r103}, [add_projected_x_to_y_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r110, %r111}, [add_projected_x_to_y_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r118, %r119}, [add_projected_x_to_y_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r126, %r127}, [add_projected_x_to_y_cuda_kernel_backward_param_8+32];
	ld.param.u64 	%rd54, [add_projected_x_to_y_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd52, [add_projected_x_to_y_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd50, [add_projected_x_to_y_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd49, [add_projected_x_to_y_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd48, [add_projected_x_to_y_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd46, [add_projected_x_to_y_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd45, [add_projected_x_to_y_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd44, [add_projected_x_to_y_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd43, [add_projected_x_to_y_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd41, [add_projected_x_to_y_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [add_projected_x_to_y_cuda_kernel_backward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd56, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd57, %r132;
	add.s64 	%rd126, %rd56, %rd57;
	setp.ge.u64 	%p1, %rd126, %rd41;
	@%p1 bra 	$L__BB3_47;

	cvta.to.global.u64 	%rd11, %rd48;
	cvta.to.global.u64 	%rd12, %rd46;
	cvta.to.global.u64 	%rd13, %rd44;
	cvt.s64.s32 	%rd15, %r73;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r71;
	cvt.s64.s32 	%rd18, %r94;
	cvt.s64.s32 	%rd19, %r86;
	cvt.s64.s32 	%rd20, %r102;
	cvt.s64.s32 	%rd21, %r110;
	cvt.s64.s32 	%rd22, %r78;
	cvt.s64.s32 	%rd23, %r126;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd58, %r133;
	mul.lo.s64 	%rd24, %rd1, %rd58;
	cvt.s64.s32 	%rd25, %r118;

$L__BB3_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd127, %rd126;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd59, %rd126, %rd15;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p3, %rd60, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd127, %rd126, %rd15;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r134, %rd15;
	cvt.u32.u64 	%r135, %rd126;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd127, %r136;

$L__BB3_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd61, %rd127, %rd16;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p5, %rd62, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd127, %rd127, %rd16;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r137, %rd16;
	cvt.u32.u64 	%r138, %rd127;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd127, %r139;

$L__BB3_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd63, %rd127, %rd17;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p7, %rd64, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd127, %rd127, %rd17;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r140, %rd17;
	cvt.u32.u64 	%r141, %rd127;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd127, %r142;

$L__BB3_14:
	cvt.s64.s32 	%rd65, %rd127;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd36, %rd65, 0, %p8;
	mul.lo.s64 	%rd66, %rd36, %rd18;
	add.s64 	%rd67, %rd12, %rd66;
	ld.global.s32 	%rd37, [%rd67];
	mul.lo.s64 	%rd38, %rd36, %rd19;
	add.s64 	%rd68, %rd13, %rd38;
	ld.global.f64 	%fd1, [%rd68];
	ld.global.f64 	%fd2, [%rd68+8];
	ld.global.f64 	%fd3, [%rd68+16];
	mul.lo.s64 	%rd39, %rd36, %rd20;
	add.s64 	%rd69, %rd11, %rd39;
	ld.global.f64 	%fd4, [%rd69];
	ld.global.f64 	%fd5, [%rd69+8];
	ld.global.f64 	%fd6, [%rd69+16];
	setp.eq.s64 	%p9, %rd50, 0;
	@%p9 bra 	$L__BB3_16;

	cvta.to.global.u64 	%rd124, %rd50;
	mul.lo.s64 	%rd70, %rd37, %rd21;
	add.s64 	%rd71, %rd124, %rd70;
	ld.global.f64 	%fd54, [%rd71];
	add.f64 	%fd241, %fd54, 0d0000000000000000;
	ld.global.f64 	%fd55, [%rd71+8];
	add.f64 	%fd240, %fd55, 0d0000000000000000;
	ld.global.f64 	%fd56, [%rd71+16];
	add.f64 	%fd239, %fd56, 0d0000000000000000;
	ld.global.f64 	%fd57, [%rd71+24];
	add.f64 	%fd238, %fd57, 0d0000000000000000;
	ld.global.f64 	%fd58, [%rd71+32];
	add.f64 	%fd237, %fd58, 0d0000000000000000;
	ld.global.f64 	%fd59, [%rd71+40];
	add.f64 	%fd236, %fd59, 0d0000000000000000;
	ld.global.f64 	%fd60, [%rd71+48];
	add.f64 	%fd235, %fd60, 0d0000000000000000;
	ld.global.f64 	%fd61, [%rd71+56];
	add.f64 	%fd234, %fd61, 0d0000000000000000;
	ld.global.f64 	%fd62, [%rd71+64];
	add.f64 	%fd233, %fd62, 0d0000000000000000;
	ld.global.f64 	%fd63, [%rd71+72];
	add.f64 	%fd232, %fd63, 0d0000000000000000;
	ld.global.f64 	%fd64, [%rd71+80];
	add.f64 	%fd231, %fd64, 0d0000000000000000;
	ld.global.f64 	%fd65, [%rd71+88];
	add.f64 	%fd230, %fd65, 0d0000000000000000;
	bra.uni 	$L__BB3_18;

$L__BB3_16:
	setp.eq.s64 	%p10, %rd43, 0;
	mov.f64 	%fd230, 0d0000000000000000;
	mov.f64 	%fd231, %fd230;
	mov.f64 	%fd232, %fd230;
	mov.f64 	%fd233, %fd230;
	mov.f64 	%fd234, %fd230;
	mov.f64 	%fd235, %fd230;
	mov.f64 	%fd236, %fd230;
	mov.f64 	%fd237, %fd230;
	mov.f64 	%fd238, %fd230;
	mov.f64 	%fd239, %fd230;
	mov.f64 	%fd240, %fd230;
	mov.f64 	%fd241, %fd230;
	@%p10 bra 	$L__BB3_18;

	cvta.to.global.u64 	%rd125, %rd43;
	mul.lo.s64 	%rd72, %rd37, %rd22;
	add.s64 	%rd73, %rd125, %rd72;
	ld.global.f64 	%fd78, [%rd73];
	add.f64 	%fd241, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd73+8];
	add.f64 	%fd240, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd73+16];
	add.f64 	%fd239, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd73+24];
	add.f64 	%fd238, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd73+32];
	add.f64 	%fd237, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd73+40];
	add.f64 	%fd236, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd73+48];
	add.f64 	%fd235, %fd84, 0d0000000000000000;
	ld.global.f64 	%fd85, [%rd73+56];
	add.f64 	%fd234, %fd85, 0d0000000000000000;
	ld.global.f64 	%fd86, [%rd73+64];
	add.f64 	%fd233, %fd86, 0d0000000000000000;
	ld.global.f64 	%fd87, [%rd73+72];
	add.f64 	%fd232, %fd87, 0d0000000000000000;
	ld.global.f64 	%fd88, [%rd73+80];
	add.f64 	%fd231, %fd88, 0d0000000000000000;
	ld.global.f64 	%fd89, [%rd73+88];
	add.f64 	%fd230, %fd89, 0d0000000000000000;

$L__BB3_18:
	mov.f64 	%fd90, 0d3FF0000000000000;
	sub.f64 	%fd91, %fd90, %fd4;
	sub.f64 	%fd92, %fd91, %fd5;
	sub.f64 	%fd93, %fd92, %fd6;
	add.f64 	%fd94, %fd230, 0d0000000000000000;
	fma.rn.f64 	%fd95, %fd3, %fd94, 0d0000000000000000;
	fma.rn.f64 	%fd96, %fd6, %fd94, 0d0000000000000000;
	add.f64 	%fd97, %fd231, 0d0000000000000000;
	fma.rn.f64 	%fd98, %fd2, %fd97, 0d0000000000000000;
	fma.rn.f64 	%fd99, %fd6, %fd97, 0d0000000000000000;
	add.f64 	%fd100, %fd95, %fd98;
	add.f64 	%fd101, %fd232, 0d0000000000000000;
	fma.rn.f64 	%fd102, %fd1, %fd101, 0d0000000000000000;
	fma.rn.f64 	%fd103, %fd6, %fd101, 0d0000000000000000;
	add.f64 	%fd104, %fd100, %fd102;
	add.f64 	%fd105, %fd233, 0d0000000000000000;
	fma.rn.f64 	%fd106, %fd3, %fd105, 0d0000000000000000;
	fma.rn.f64 	%fd107, %fd5, %fd105, 0d0000000000000000;
	add.f64 	%fd108, %fd96, %fd107;
	add.f64 	%fd109, %fd234, 0d0000000000000000;
	fma.rn.f64 	%fd110, %fd2, %fd109, 0d0000000000000000;
	fma.rn.f64 	%fd111, %fd5, %fd109, 0d0000000000000000;
	add.f64 	%fd112, %fd99, %fd111;
	add.f64 	%fd113, %fd106, %fd110;
	add.f64 	%fd114, %fd235, 0d0000000000000000;
	fma.rn.f64 	%fd115, %fd1, %fd114, 0d0000000000000000;
	fma.rn.f64 	%fd116, %fd5, %fd114, 0d0000000000000000;
	add.f64 	%fd117, %fd103, %fd116;
	add.f64 	%fd118, %fd113, %fd115;
	add.f64 	%fd119, %fd236, 0d0000000000000000;
	fma.rn.f64 	%fd120, %fd3, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd121, %fd4, %fd119, 0d0000000000000000;
	add.f64 	%fd122, %fd108, %fd121;
	add.f64 	%fd123, %fd237, 0d0000000000000000;
	fma.rn.f64 	%fd124, %fd2, %fd123, 0d0000000000000000;
	fma.rn.f64 	%fd125, %fd4, %fd123, 0d0000000000000000;
	add.f64 	%fd126, %fd112, %fd125;
	add.f64 	%fd127, %fd120, %fd124;
	add.f64 	%fd128, %fd238, 0d0000000000000000;
	fma.rn.f64 	%fd129, %fd1, %fd128, 0d0000000000000000;
	fma.rn.f64 	%fd130, %fd4, %fd128, 0d0000000000000000;
	add.f64 	%fd131, %fd117, %fd130;
	add.f64 	%fd132, %fd127, %fd129;
	add.f64 	%fd133, %fd239, 0d0000000000000000;
	fma.rn.f64 	%fd134, %fd3, %fd133, 0d0000000000000000;
	fma.rn.f64 	%fd135, %fd93, %fd133, 0d0000000000000000;
	add.f64 	%fd43, %fd122, %fd135;
	add.f64 	%fd136, %fd240, 0d0000000000000000;
	fma.rn.f64 	%fd137, %fd2, %fd136, 0d0000000000000000;
	fma.rn.f64 	%fd138, %fd93, %fd136, 0d0000000000000000;
	add.f64 	%fd44, %fd126, %fd138;
	add.f64 	%fd139, %fd134, %fd137;
	add.f64 	%fd140, %fd241, 0d0000000000000000;
	fma.rn.f64 	%fd141, %fd1, %fd140, 0d0000000000000000;
	fma.rn.f64 	%fd142, %fd93, %fd140, 0d0000000000000000;
	add.f64 	%fd45, %fd131, %fd142;
	add.f64 	%fd46, %fd139, %fd141;
	add.f64 	%fd47, %fd132, 0d0000000000000000;
	add.f64 	%fd48, %fd118, 0d0000000000000000;
	add.f64 	%fd49, %fd104, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd54, 0;
	@%p11 bra 	$L__BB3_20;

	mul.lo.s64 	%rd77, %rd36, %rd23;
	add.s64 	%rd74, %rd54, %rd77;
	mov.f64 	%fd146, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd74],%fd146; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd75],%fd146; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd76],%fd49; }

	// end inline asm
	bra.uni 	$L__BB3_22;

$L__BB3_20:
	setp.eq.s64 	%p12, %rd49, 0;
	@%p12 bra 	$L__BB3_22;

	add.s64 	%rd78, %rd49, %rd39;
	mov.f64 	%fd152, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd78],%fd152; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd79],%fd152; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd80],%fd49; }

	// end inline asm

$L__BB3_22:
	@%p11 bra 	$L__BB3_24;

	mul.lo.s64 	%rd84, %rd36, %rd23;
	add.s64 	%rd81, %rd54, %rd84;
	mov.f64 	%fd160, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd81],%fd160; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd82],%fd48; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd83],%fd160; }

	// end inline asm
	bra.uni 	$L__BB3_26;

$L__BB3_24:
	setp.eq.s64 	%p14, %rd49, 0;
	@%p14 bra 	$L__BB3_26;

	add.s64 	%rd85, %rd49, %rd39;
	mov.f64 	%fd166, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd85],%fd166; }

	// end inline asm
	add.s64 	%rd86, %rd85, 8;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd86],%fd48; }

	// end inline asm
	add.s64 	%rd87, %rd85, 16;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd87],%fd166; }

	// end inline asm

$L__BB3_26:
	@%p11 bra 	$L__BB3_28;

	mul.lo.s64 	%rd91, %rd36, %rd23;
	add.s64 	%rd88, %rd54, %rd91;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd88],%fd47; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	mov.f64 	%fd172, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd89],%fd172; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd90],%fd172; }

	// end inline asm
	bra.uni 	$L__BB3_30;

$L__BB3_28:
	setp.eq.s64 	%p16, %rd49, 0;
	@%p16 bra 	$L__BB3_30;

	add.s64 	%rd92, %rd49, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd92],%fd47; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	mov.f64 	%fd178, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd93],%fd178; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd94],%fd178; }

	// end inline asm

$L__BB3_30:
	add.f64 	%fd179, %fd46, 0d0000000000000000;
	mov.f64 	%fd180, 0d0000000000000000;
	sub.f64 	%fd181, %fd180, %fd179;
	add.f64 	%fd50, %fd181, 0d0000000000000000;
	@%p11 bra 	$L__BB3_32;

	mul.lo.s64 	%rd98, %rd36, %rd23;
	add.s64 	%rd95, %rd54, %rd98;
	// begin inline asm
	{ atom.add.f64 %fd182,[%rd95],%fd180; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd184,[%rd96],%fd180; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd186,[%rd97],%fd50; }

	// end inline asm
	bra.uni 	$L__BB3_34;

$L__BB3_32:
	setp.eq.s64 	%p18, %rd49, 0;
	@%p18 bra 	$L__BB3_34;

	add.s64 	%rd99, %rd49, %rd39;
	mov.f64 	%fd191, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd99],%fd191; }

	// end inline asm
	add.s64 	%rd100, %rd99, 8;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd100],%fd191; }

	// end inline asm
	add.s64 	%rd101, %rd99, 16;
	// begin inline asm
	{ atom.add.f64 %fd192,[%rd101],%fd50; }

	// end inline asm

$L__BB3_34:
	@%p11 bra 	$L__BB3_36;

	mul.lo.s64 	%rd105, %rd36, %rd23;
	add.s64 	%rd102, %rd54, %rd105;
	mov.f64 	%fd199, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd194,[%rd102],%fd199; }

	// end inline asm
	add.s64 	%rd103, %rd102, 8;
	// begin inline asm
	{ atom.add.f64 %fd196,[%rd103],%fd50; }

	// end inline asm
	add.s64 	%rd104, %rd102, 16;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd104],%fd199; }

	// end inline asm
	bra.uni 	$L__BB3_38;

$L__BB3_36:
	setp.eq.s64 	%p20, %rd49, 0;
	@%p20 bra 	$L__BB3_38;

	add.s64 	%rd106, %rd49, %rd39;
	mov.f64 	%fd205, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd106],%fd205; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	// begin inline asm
	{ atom.add.f64 %fd202,[%rd107],%fd50; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd204,[%rd108],%fd205; }

	// end inline asm

$L__BB3_38:
	@%p11 bra 	$L__BB3_40;

	mul.lo.s64 	%rd112, %rd36, %rd23;
	add.s64 	%rd109, %rd54, %rd112;
	// begin inline asm
	{ atom.add.f64 %fd206,[%rd109],%fd50; }

	// end inline asm
	add.s64 	%rd110, %rd109, 8;
	mov.f64 	%fd211, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd208,[%rd110],%fd211; }

	// end inline asm
	add.s64 	%rd111, %rd109, 16;
	// begin inline asm
	{ atom.add.f64 %fd210,[%rd111],%fd211; }

	// end inline asm
	bra.uni 	$L__BB3_42;

$L__BB3_40:
	setp.eq.s64 	%p22, %rd49, 0;
	@%p22 bra 	$L__BB3_42;

	add.s64 	%rd113, %rd49, %rd39;
	// begin inline asm
	{ atom.add.f64 %fd212,[%rd113],%fd50; }

	// end inline asm
	add.s64 	%rd114, %rd113, 8;
	mov.f64 	%fd217, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd214,[%rd114],%fd217; }

	// end inline asm
	add.s64 	%rd115, %rd113, 16;
	// begin inline asm
	{ atom.add.f64 %fd216,[%rd115],%fd217; }

	// end inline asm

$L__BB3_42:
	setp.eq.s64 	%p23, %rd52, 0;
	add.f64 	%fd51, %fd45, 0d0000000000000000;
	add.f64 	%fd52, %fd44, 0d0000000000000000;
	add.f64 	%fd53, %fd43, 0d0000000000000000;
	@%p23 bra 	$L__BB3_44;

	mul.lo.s64 	%rd119, %rd36, %rd25;
	add.s64 	%rd116, %rd52, %rd119;
	// begin inline asm
	{ atom.add.f64 %fd218,[%rd116],%fd51; }

	// end inline asm
	add.s64 	%rd117, %rd116, 8;
	// begin inline asm
	{ atom.add.f64 %fd220,[%rd117],%fd52; }

	// end inline asm
	add.s64 	%rd118, %rd116, 16;
	// begin inline asm
	{ atom.add.f64 %fd222,[%rd118],%fd53; }

	// end inline asm
	bra.uni 	$L__BB3_46;

$L__BB3_44:
	setp.eq.s64 	%p24, %rd45, 0;
	@%p24 bra 	$L__BB3_46;

	add.s64 	%rd120, %rd45, %rd38;
	// begin inline asm
	{ atom.add.f64 %fd224,[%rd120],%fd51; }

	// end inline asm
	add.s64 	%rd121, %rd120, 8;
	// begin inline asm
	{ atom.add.f64 %fd226,[%rd121],%fd52; }

	// end inline asm
	add.s64 	%rd122, %rd120, 16;
	// begin inline asm
	{ atom.add.f64 %fd228,[%rd122],%fd53; }

	// end inline asm

$L__BB3_46:
	ld.param.u64 	%rd123, [add_projected_x_to_y_cuda_kernel_backward_param_0+24];
	add.s64 	%rd126, %rd126, %rd24;
	setp.lt.u64 	%p25, %rd126, %rd123;
	@%p25 bra 	$L__BB3_2;

$L__BB3_47:
	ret;

}
	// .globl	initialize_friction_collisions_cuda_kernel_forward
.visible .entry initialize_friction_collisions_cuda_kernel_forward(
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_11[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_12[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_13[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_14[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_15[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_forward_param_16[56],
	.param .f64 initialize_friction_collisions_cuda_kernel_forward_param_17,
	.param .f64 initialize_friction_collisions_cuda_kernel_forward_param_18,
	.param .f64 initialize_friction_collisions_cuda_kernel_forward_param_19
)
{
	.reg .pred 	%p<154>;
	.reg .b16 	%rs<129>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<412>;
	.reg .f64 	%fd<1210>;
	.reg .b64 	%rd<171>;


	ld.param.v2.u32 	{%r189, %r190}, [initialize_friction_collisions_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r191, %r192}, [initialize_friction_collisions_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r197, %r198}, [initialize_friction_collisions_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r205, %r206}, [initialize_friction_collisions_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r213, %r214}, [initialize_friction_collisions_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r221, %r222}, [initialize_friction_collisions_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r229, %r230}, [initialize_friction_collisions_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r237, %r238}, [initialize_friction_collisions_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r245, %r246}, [initialize_friction_collisions_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r253, %r254}, [initialize_friction_collisions_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r261, %r262}, [initialize_friction_collisions_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r269, %r270}, [initialize_friction_collisions_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r277, %r278}, [initialize_friction_collisions_cuda_kernel_forward_param_11+32];
	ld.param.v2.u32 	{%r285, %r286}, [initialize_friction_collisions_cuda_kernel_forward_param_12+32];
	ld.param.v2.u32 	{%r293, %r294}, [initialize_friction_collisions_cuda_kernel_forward_param_13+32];
	ld.param.v2.u32 	{%r301, %r302}, [initialize_friction_collisions_cuda_kernel_forward_param_14+32];
	ld.param.v2.u32 	{%r309, %r310}, [initialize_friction_collisions_cuda_kernel_forward_param_15+32];
	ld.param.v2.u32 	{%r317, %r318}, [initialize_friction_collisions_cuda_kernel_forward_param_16+32];
	ld.param.f64 	%fd294, [initialize_friction_collisions_cuda_kernel_forward_param_17];
	ld.param.f64 	%fd295, [initialize_friction_collisions_cuda_kernel_forward_param_18];
	ld.param.f64 	%fd296, [initialize_friction_collisions_cuda_kernel_forward_param_19];
	ld.param.u64 	%rd94, [initialize_friction_collisions_cuda_kernel_forward_param_16];
	ld.param.u64 	%rd92, [initialize_friction_collisions_cuda_kernel_forward_param_15];
	ld.param.u64 	%rd90, [initialize_friction_collisions_cuda_kernel_forward_param_14];
	ld.param.u64 	%rd88, [initialize_friction_collisions_cuda_kernel_forward_param_13];
	ld.param.u64 	%rd86, [initialize_friction_collisions_cuda_kernel_forward_param_12];
	ld.param.u64 	%rd84, [initialize_friction_collisions_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd82, [initialize_friction_collisions_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd80, [initialize_friction_collisions_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd78, [initialize_friction_collisions_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd76, [initialize_friction_collisions_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd74, [initialize_friction_collisions_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd72, [initialize_friction_collisions_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd70, [initialize_friction_collisions_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd68, [initialize_friction_collisions_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd66, [initialize_friction_collisions_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd64, [initialize_friction_collisions_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd63, [initialize_friction_collisions_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r44, [initialize_friction_collisions_cuda_kernel_forward_param_0+16];
	mov.u32 	%r321, %ntid.x;
	cvt.u64.u32 	%rd1, %r321;
	mov.u32 	%r322, %ctaid.x;
	mul.wide.u32 	%rd96, %r321, %r322;
	mov.u32 	%r323, %tid.x;
	cvt.u64.u32 	%rd97, %r323;
	add.s64 	%rd167, %rd96, %rd97;
	setp.ge.u64 	%p4, %rd167, %rd63;
	@%p4 bra 	$L__BB4_157;

	cvta.to.global.u64 	%rd4, %rd76;
	cvta.to.global.u64 	%rd5, %rd72;
	cvta.to.global.u64 	%rd6, %rd70;
	cvta.to.global.u64 	%rd7, %rd68;
	cvta.to.global.u64 	%rd8, %rd66;
	cvta.to.global.u64 	%rd9, %rd94;
	cvta.to.global.u64 	%rd10, %rd92;
	cvta.to.global.u64 	%rd11, %rd90;
	cvta.to.global.u64 	%rd12, %rd88;
	cvta.to.global.u64 	%rd13, %rd86;
	cvta.to.global.u64 	%rd14, %rd84;
	cvta.to.global.u64 	%rd15, %rd82;
	cvta.to.global.u64 	%rd16, %rd80;
	cvta.to.global.u64 	%rd17, %rd78;
	cvta.to.global.u64 	%rd18, %rd74;
	cvta.to.global.u64 	%rd19, %rd64;
	cvt.s64.s32 	%rd20, %r192;
	cvt.s64.s32 	%rd21, %r191;
	cvt.s64.s32 	%rd22, %r190;
	cvt.s64.s32 	%rd23, %r197;
	cvt.s64.s32 	%rd24, %r237;
	cvt.s64.s32 	%rd25, %r229;
	cvt.s64.s32 	%rd26, %r221;
	cvt.s64.s32 	%rd27, %r317;
	cvt.s64.s32 	%rd28, %r269;
	cvt.s64.s32 	%rd29, %r285;
	cvt.s64.s32 	%rd30, %r253;
	cvt.s64.s32 	%rd31, %r245;
	mov.u32 	%r324, %nctaid.x;
	cvt.u64.u32 	%rd98, %r324;
	mul.lo.s64 	%rd32, %rd1, %rd98;
	cvt.s64.s32 	%rd33, %r309;
	cvt.s64.s32 	%rd34, %r277;
	mul.f64 	%fd1, %fd294, %fd294;
	add.f64 	%fd2, %fd294, %fd294;
	cvt.s64.s32 	%rd35, %r293;
	cvt.s64.s32 	%rd36, %r261;
	cvt.s64.s32 	%rd37, %r205;
	cvt.s64.s32 	%rd38, %r213;
	cvt.s64.s32 	%rd39, %r301;

$L__BB4_2:
	setp.lt.s32 	%p5, %r44, 4;
	mov.u64 	%rd168, %rd167;
	@%p5 bra 	$L__BB4_6;

	or.b64  	%rd99, %rd167, %rd20;
	and.b64  	%rd100, %rd99, -4294967296;
	setp.eq.s64 	%p6, %rd100, 0;
	@%p6 bra 	$L__BB4_5;

	div.u64 	%rd168, %rd167, %rd20;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r325, %rd20;
	cvt.u32.u64 	%r326, %rd167;
	div.u32 	%r327, %r326, %r325;
	cvt.u64.u32 	%rd168, %r327;

$L__BB4_6:
	setp.lt.s32 	%p7, %r44, 3;
	@%p7 bra 	$L__BB4_10;

	or.b64  	%rd101, %rd168, %rd21;
	and.b64  	%rd102, %rd101, -4294967296;
	setp.eq.s64 	%p8, %rd102, 0;
	@%p8 bra 	$L__BB4_9;

	div.u64 	%rd168, %rd168, %rd21;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r328, %rd21;
	cvt.u32.u64 	%r329, %rd168;
	div.u32 	%r330, %r329, %r328;
	cvt.u64.u32 	%rd168, %r330;

$L__BB4_10:
	setp.lt.s32 	%p9, %r44, 2;
	@%p9 bra 	$L__BB4_14;

	or.b64  	%rd103, %rd168, %rd22;
	and.b64  	%rd104, %rd103, -4294967296;
	setp.eq.s64 	%p10, %rd104, 0;
	@%p10 bra 	$L__BB4_13;

	div.u64 	%rd168, %rd168, %rd22;
	bra.uni 	$L__BB4_14;

$L__BB4_13:
	cvt.u32.u64 	%r331, %rd22;
	cvt.u32.u64 	%r332, %rd168;
	div.u32 	%r333, %r332, %r331;
	cvt.u64.u32 	%rd168, %r333;

$L__BB4_14:
	cvt.s64.s32 	%rd105, %rd168;
	setp.gt.s32 	%p11, %r44, 0;
	selp.b64 	%rd106, %rd105, 0, %p11;
	mov.u64 	%rd107, 0;
	mul.lo.s64 	%rd108, %rd106, %rd23;
	add.s64 	%rd50, %rd19, %rd108;
	st.global.u64 	[%rd50], %rd107;
	mul.lo.s64 	%rd109, %rd106, %rd24;
	add.s64 	%rd110, %rd18, %rd109;
	ld.global.u32 	%r2, [%rd110];
	setp.lt.u32 	%p12, %r2, 2;
	mul.lo.s64 	%rd111, %rd106, %rd26;
	add.s64 	%rd51, %rd6, %rd111;
	mul.lo.s64 	%rd112, %rd106, %rd25;
	add.s64 	%rd52, %rd5, %rd112;
	mul.lo.s64 	%rd113, %rd106, %rd37;
	add.s64 	%rd53, %rd8, %rd113;
	mul.lo.s64 	%rd114, %rd106, %rd38;
	add.s64 	%rd54, %rd7, %rd114;
	@%p12 bra 	$L__BB4_99;
	bra.uni 	$L__BB4_15;

$L__BB4_99:
	ld.global.u32 	%r368, [%rd51];
	ld.global.u32 	%r369, [%rd52];
	setp.eq.s32 	%p82, %r2, 1;
	selp.b32 	%r370, %r368, %r369, %p82;
	selp.b32 	%r371, %r369, %r368, %p82;
	cvt.s64.s32 	%rd143, %r371;
	mul.lo.s64 	%rd144, %rd143, %rd27;
	add.s64 	%rd145, %rd9, %rd144;
	cvt.s64.s32 	%rd61, %r370;
	mul.lo.s64 	%rd146, %rd61, %rd28;
	add.s64 	%rd147, %rd15, %rd146;
	mul.lo.s64 	%rd148, %rd143, %rd29;
	add.s64 	%rd149, %rd13, %rd148;
	ld.global.f64 	%fd662, [%rd149];
	ld.global.f64 	%fd663, [%rd147];
	add.f64 	%fd176, %fd663, %fd662;
	mul.lo.s64 	%rd150, %rd61, %rd30;
	add.s64 	%rd151, %rd17, %rd150;
	ld.global.s32 	%rd152, [%rd151];
	mul.lo.s64 	%rd153, %rd152, %rd31;
	add.s64 	%rd154, %rd4, %rd153;
	ld.global.s32 	%rd155, [%rd145];
	mul.lo.s64 	%rd156, %rd155, %rd31;
	add.s64 	%rd157, %rd4, %rd156;
	ld.global.s32 	%rd158, [%rd145+4];
	mul.lo.s64 	%rd159, %rd158, %rd31;
	add.s64 	%rd160, %rd4, %rd159;
	ld.global.s32 	%rd161, [%rd145+8];
	mul.lo.s64 	%rd162, %rd161, %rd31;
	add.s64 	%rd163, %rd4, %rd162;
	ld.global.f64 	%fd177, [%rd160];
	ld.global.f64 	%fd178, [%rd157];
	sub.f64 	%fd179, %fd177, %fd178;
	ld.global.f64 	%fd180, [%rd160+8];
	ld.global.f64 	%fd181, [%rd157+8];
	sub.f64 	%fd182, %fd180, %fd181;
	ld.global.f64 	%fd183, [%rd160+16];
	ld.global.f64 	%fd184, [%rd157+16];
	sub.f64 	%fd185, %fd183, %fd184;
	ld.global.f64 	%fd186, [%rd163];
	sub.f64 	%fd187, %fd186, %fd178;
	ld.global.f64 	%fd188, [%rd163+8];
	sub.f64 	%fd189, %fd188, %fd181;
	ld.global.f64 	%fd190, [%rd163+16];
	sub.f64 	%fd191, %fd190, %fd184;
	mul.f64 	%fd664, %fd182, %fd191;
	mul.f64 	%fd665, %fd185, %fd189;
	sub.f64 	%fd192, %fd664, %fd665;
	mul.f64 	%fd666, %fd185, %fd187;
	mul.f64 	%fd667, %fd179, %fd191;
	sub.f64 	%fd193, %fd666, %fd667;
	mul.f64 	%fd668, %fd179, %fd189;
	mul.f64 	%fd669, %fd182, %fd187;
	sub.f64 	%fd194, %fd668, %fd669;
	mul.f64 	%fd670, %fd182, %fd194;
	mul.f64 	%fd671, %fd185, %fd193;
	sub.f64 	%fd672, %fd670, %fd671;
	mul.f64 	%fd673, %fd185, %fd192;
	mul.f64 	%fd674, %fd179, %fd194;
	sub.f64 	%fd675, %fd673, %fd674;
	mul.f64 	%fd676, %fd179, %fd193;
	mul.f64 	%fd677, %fd182, %fd192;
	sub.f64 	%fd678, %fd676, %fd677;
	mul.f64 	%fd679, %fd182, %fd182;
	fma.rn.f64 	%fd680, %fd179, %fd179, %fd679;
	fma.rn.f64 	%fd195, %fd185, %fd185, %fd680;
	mul.f64 	%fd681, %fd182, %fd675;
	fma.rn.f64 	%fd682, %fd179, %fd672, %fd681;
	fma.rn.f64 	%fd683, %fd185, %fd678, %fd682;
	mul.f64 	%fd684, %fd675, %fd675;
	fma.rn.f64 	%fd685, %fd672, %fd672, %fd684;
	fma.rn.f64 	%fd686, %fd678, %fd678, %fd685;
	ld.global.f64 	%fd196, [%rd154];
	sub.f64 	%fd197, %fd196, %fd178;
	ld.global.f64 	%fd198, [%rd154+8];
	sub.f64 	%fd199, %fd198, %fd181;
	ld.global.f64 	%fd200, [%rd154+16];
	sub.f64 	%fd201, %fd200, %fd184;
	mul.f64 	%fd687, %fd199, %fd182;
	fma.rn.f64 	%fd688, %fd197, %fd179, %fd687;
	fma.rn.f64 	%fd202, %fd201, %fd185, %fd688;
	mul.f64 	%fd689, %fd199, %fd675;
	fma.rn.f64 	%fd690, %fd197, %fd672, %fd689;
	fma.rn.f64 	%fd691, %fd201, %fd678, %fd690;
	div.rn.f64 	%fd692, %fd683, %fd195;
	mul.f64 	%fd693, %fd692, %fd692;
	mul.f64 	%fd694, %fd195, %fd693;
	sub.f64 	%fd695, %fd686, %fd694;
	mul.f64 	%fd696, %fd202, %fd692;
	sub.f64 	%fd697, %fd691, %fd696;
	div.rn.f64 	%fd698, %fd697, %fd695;
	mul.f64 	%fd699, %fd195, %fd692;
	mul.f64 	%fd700, %fd699, %fd698;
	sub.f64 	%fd701, %fd202, %fd700;
	div.rn.f64 	%fd203, %fd701, %fd195;
	setp.gt.f64 	%p83, %fd203, 0d0000000000000000;
	setp.lt.f64 	%p84, %fd203, 0d3FF0000000000000;
	setp.ge.f64 	%p85, %fd698, 0d0000000000000000;
	and.pred  	%p86, %p83, %p84;
	and.pred  	%p3, %p85, %p86;
	mov.u32 	%r406, 3;
	@%p3 bra 	$L__BB4_105;

	sub.f64 	%fd702, %fd186, %fd177;
	sub.f64 	%fd703, %fd188, %fd180;
	mul.f64 	%fd704, %fd703, %fd194;
	sub.f64 	%fd705, %fd190, %fd183;
	mul.f64 	%fd706, %fd705, %fd193;
	sub.f64 	%fd707, %fd704, %fd706;
	mul.f64 	%fd708, %fd705, %fd192;
	mul.f64 	%fd709, %fd702, %fd194;
	sub.f64 	%fd710, %fd708, %fd709;
	mul.f64 	%fd711, %fd702, %fd193;
	mul.f64 	%fd712, %fd703, %fd192;
	sub.f64 	%fd713, %fd711, %fd712;
	mul.f64 	%fd714, %fd703, %fd703;
	fma.rn.f64 	%fd715, %fd702, %fd702, %fd714;
	fma.rn.f64 	%fd716, %fd705, %fd705, %fd715;
	mul.f64 	%fd717, %fd703, %fd710;
	fma.rn.f64 	%fd718, %fd702, %fd707, %fd717;
	fma.rn.f64 	%fd719, %fd705, %fd713, %fd718;
	mul.f64 	%fd720, %fd710, %fd710;
	fma.rn.f64 	%fd721, %fd707, %fd707, %fd720;
	fma.rn.f64 	%fd722, %fd713, %fd713, %fd721;
	sub.f64 	%fd723, %fd196, %fd177;
	sub.f64 	%fd724, %fd198, %fd180;
	mul.f64 	%fd725, %fd724, %fd703;
	fma.rn.f64 	%fd726, %fd723, %fd702, %fd725;
	sub.f64 	%fd727, %fd200, %fd183;
	fma.rn.f64 	%fd728, %fd727, %fd705, %fd726;
	mul.f64 	%fd729, %fd724, %fd710;
	fma.rn.f64 	%fd730, %fd723, %fd707, %fd729;
	fma.rn.f64 	%fd731, %fd727, %fd713, %fd730;
	div.rn.f64 	%fd732, %fd719, %fd716;
	mul.f64 	%fd733, %fd732, %fd732;
	mul.f64 	%fd734, %fd716, %fd733;
	sub.f64 	%fd735, %fd722, %fd734;
	mul.f64 	%fd736, %fd728, %fd732;
	sub.f64 	%fd737, %fd731, %fd736;
	div.rn.f64 	%fd738, %fd737, %fd735;
	mul.f64 	%fd739, %fd716, %fd732;
	mul.f64 	%fd740, %fd739, %fd738;
	sub.f64 	%fd741, %fd728, %fd740;
	div.rn.f64 	%fd204, %fd741, %fd716;
	setp.gt.f64 	%p87, %fd204, 0d0000000000000000;
	setp.lt.f64 	%p88, %fd204, 0d3FF0000000000000;
	setp.ge.f64 	%p89, %fd738, 0d0000000000000000;
	and.pred  	%p90, %p87, %p88;
	and.pred  	%p91, %p89, %p90;
	mov.u32 	%r406, 4;
	@%p91 bra 	$L__BB4_105;

	sub.f64 	%fd742, %fd178, %fd186;
	sub.f64 	%fd743, %fd181, %fd188;
	mul.f64 	%fd744, %fd743, %fd194;
	sub.f64 	%fd745, %fd184, %fd190;
	mul.f64 	%fd746, %fd745, %fd193;
	sub.f64 	%fd747, %fd744, %fd746;
	mul.f64 	%fd748, %fd745, %fd192;
	mul.f64 	%fd749, %fd742, %fd194;
	sub.f64 	%fd750, %fd748, %fd749;
	mul.f64 	%fd751, %fd742, %fd193;
	mul.f64 	%fd752, %fd743, %fd192;
	sub.f64 	%fd753, %fd751, %fd752;
	mul.f64 	%fd754, %fd743, %fd743;
	fma.rn.f64 	%fd755, %fd742, %fd742, %fd754;
	fma.rn.f64 	%fd756, %fd745, %fd745, %fd755;
	mul.f64 	%fd757, %fd743, %fd750;
	fma.rn.f64 	%fd758, %fd742, %fd747, %fd757;
	fma.rn.f64 	%fd759, %fd745, %fd753, %fd758;
	mul.f64 	%fd760, %fd750, %fd750;
	fma.rn.f64 	%fd761, %fd747, %fd747, %fd760;
	fma.rn.f64 	%fd762, %fd753, %fd753, %fd761;
	sub.f64 	%fd763, %fd196, %fd186;
	sub.f64 	%fd764, %fd198, %fd188;
	mul.f64 	%fd765, %fd743, %fd764;
	fma.rn.f64 	%fd766, %fd742, %fd763, %fd765;
	sub.f64 	%fd767, %fd200, %fd190;
	fma.rn.f64 	%fd768, %fd745, %fd767, %fd766;
	mul.f64 	%fd769, %fd764, %fd750;
	fma.rn.f64 	%fd770, %fd763, %fd747, %fd769;
	fma.rn.f64 	%fd771, %fd767, %fd753, %fd770;
	div.rn.f64 	%fd772, %fd759, %fd756;
	mul.f64 	%fd773, %fd772, %fd772;
	mul.f64 	%fd774, %fd756, %fd773;
	sub.f64 	%fd775, %fd762, %fd774;
	mul.f64 	%fd776, %fd768, %fd772;
	sub.f64 	%fd777, %fd771, %fd776;
	div.rn.f64 	%fd778, %fd777, %fd775;
	mul.f64 	%fd779, %fd756, %fd772;
	mul.f64 	%fd780, %fd779, %fd778;
	sub.f64 	%fd781, %fd768, %fd780;
	div.rn.f64 	%fd205, %fd781, %fd756;
	setp.gt.f64 	%p92, %fd205, 0d0000000000000000;
	setp.lt.f64 	%p93, %fd205, 0d3FF0000000000000;
	setp.ge.f64 	%p94, %fd778, 0d0000000000000000;
	and.pred  	%p95, %p92, %p93;
	and.pred  	%p96, %p94, %p95;
	mov.u32 	%r406, 5;
	@%p96 bra 	$L__BB4_105;

	setp.le.f64 	%p97, %fd203, 0d0000000000000000;
	setp.ge.f64 	%p98, %fd205, 0d3FF0000000000000;
	and.pred  	%p99, %p97, %p98;
	mov.u32 	%r406, 0;
	@%p99 bra 	$L__BB4_105;

	setp.le.f64 	%p100, %fd204, 0d0000000000000000;
	setp.ge.f64 	%p101, %fd203, 0d3FF0000000000000;
	and.pred  	%p102, %p100, %p101;
	mov.u32 	%r406, 1;
	@%p102 bra 	$L__BB4_105;

	setp.le.f64 	%p103, %fd205, 0d0000000000000000;
	setp.ge.f64 	%p104, %fd204, 0d3FF0000000000000;
	and.pred  	%p105, %p103, %p104;
	selp.b32 	%r406, 2, 6, %p105;

$L__BB4_105:
	setp.eq.s32 	%p106, %r406, 0;
	@%p106 bra 	$L__BB4_117;

	setp.eq.s32 	%p107, %r406, 1;
	@%p107 bra 	$L__BB4_116;
	bra.uni 	$L__BB4_107;

$L__BB4_116:
	sub.f64 	%fd860, %fd196, %fd177;
	sub.f64 	%fd861, %fd198, %fd180;
	mul.f64 	%fd862, %fd861, %fd861;
	fma.rn.f64 	%fd863, %fd860, %fd860, %fd862;
	sub.f64 	%fd864, %fd200, %fd183;
	fma.rn.f64 	%fd1182, %fd864, %fd864, %fd863;
	bra.uni 	$L__BB4_118;

$L__BB4_15:
	ld.global.s32 	%rd55, [%rd51];
	mul.lo.s64 	%rd115, %rd55, %rd33;
	add.s64 	%rd116, %rd10, %rd115;
	ld.global.s32 	%rd56, [%rd52];
	mul.lo.s64 	%rd117, %rd56, %rd33;
	add.s64 	%rd118, %rd10, %rd117;
	mul.lo.s64 	%rd119, %rd55, %rd34;
	add.s64 	%rd120, %rd14, %rd119;
	mul.lo.s64 	%rd121, %rd56, %rd34;
	add.s64 	%rd122, %rd14, %rd121;
	ld.global.f64 	%fd298, [%rd122];
	ld.global.f64 	%fd299, [%rd120];
	add.f64 	%fd3, %fd299, %fd298;
	ld.global.s32 	%rd57, [%rd116];
	mul.lo.s64 	%rd123, %rd57, %rd31;
	add.s64 	%rd124, %rd4, %rd123;
	ld.global.s32 	%rd58, [%rd116+4];
	mul.lo.s64 	%rd125, %rd58, %rd31;
	add.s64 	%rd126, %rd4, %rd125;
	ld.global.s32 	%rd59, [%rd118];
	mul.lo.s64 	%rd127, %rd59, %rd31;
	add.s64 	%rd128, %rd4, %rd127;
	ld.global.s32 	%rd60, [%rd118+4];
	mul.lo.s64 	%rd129, %rd60, %rd31;
	add.s64 	%rd130, %rd4, %rd129;
	ld.global.f64 	%fd4, [%rd126];
	ld.global.f64 	%fd5, [%rd124];
	sub.f64 	%fd6, %fd4, %fd5;
	ld.global.f64 	%fd7, [%rd126+8];
	ld.global.f64 	%fd8, [%rd124+8];
	sub.f64 	%fd9, %fd7, %fd8;
	ld.global.f64 	%fd10, [%rd126+16];
	ld.global.f64 	%fd11, [%rd124+16];
	sub.f64 	%fd12, %fd10, %fd11;
	ld.global.f64 	%fd13, [%rd130];
	ld.global.f64 	%fd14, [%rd128];
	sub.f64 	%fd15, %fd13, %fd14;
	ld.global.f64 	%fd16, [%rd130+8];
	ld.global.f64 	%fd17, [%rd128+8];
	sub.f64 	%fd18, %fd16, %fd17;
	ld.global.f64 	%fd19, [%rd130+16];
	ld.global.f64 	%fd20, [%rd128+16];
	sub.f64 	%fd21, %fd19, %fd20;
	sub.f64 	%fd22, %fd5, %fd14;
	sub.f64 	%fd23, %fd8, %fd17;
	sub.f64 	%fd24, %fd11, %fd20;
	mul.f64 	%fd300, %fd9, %fd9;
	fma.rn.f64 	%fd301, %fd6, %fd6, %fd300;
	fma.rn.f64 	%fd25, %fd12, %fd12, %fd301;
	mul.f64 	%fd302, %fd9, %fd18;
	fma.rn.f64 	%fd303, %fd6, %fd15, %fd302;
	fma.rn.f64 	%fd26, %fd12, %fd21, %fd303;
	mul.f64 	%fd304, %fd18, %fd18;
	fma.rn.f64 	%fd305, %fd15, %fd15, %fd304;
	fma.rn.f64 	%fd27, %fd21, %fd21, %fd305;
	mul.f64 	%fd306, %fd9, %fd23;
	fma.rn.f64 	%fd307, %fd6, %fd22, %fd306;
	fma.rn.f64 	%fd28, %fd12, %fd24, %fd307;
	mul.f64 	%fd308, %fd23, %fd18;
	fma.rn.f64 	%fd309, %fd22, %fd15, %fd308;
	fma.rn.f64 	%fd29, %fd24, %fd21, %fd309;
	mul.f64 	%fd310, %fd25, %fd27;
	mul.f64 	%fd311, %fd26, %fd26;
	sub.f64 	%fd30, %fd310, %fd311;
	mul.f64 	%fd312, %fd26, %fd29;
	mul.f64 	%fd313, %fd28, %fd27;
	sub.f64 	%fd31, %fd312, %fd313;
	setp.le.f64 	%p13, %fd31, 0d0000000000000000;
	@%p13 bra 	$L__BB4_19;

	setp.ge.f64 	%p1, %fd31, %fd30;
	add.f64 	%fd32, %fd29, %fd26;
	@%p1 bra 	$L__BB4_18;

	setp.le.f64 	%p152, %fd31, 0d0000000000000000;
	selp.b32 	%r394, 2, 8, %p152;
	sub.f64 	%fd1113, %fd11, %fd20;
	sub.f64 	%fd1112, %fd5, %fd14;
	sub.f64 	%fd1111, %fd8, %fd17;
	sub.f64 	%fd1110, %fd13, %fd14;
	sub.f64 	%fd1109, %fd19, %fd20;
	sub.f64 	%fd1108, %fd16, %fd17;
	selp.f64 	%fd315, %fd27, %fd30, %p1;
	mul.f64 	%fd316, %fd25, %fd29;
	mul.f64 	%fd317, %fd28, %fd26;
	sub.f64 	%fd318, %fd316, %fd317;
	mul.f64 	%fd319, %fd12, %fd1108;
	mul.f64 	%fd320, %fd9, %fd1109;
	sub.f64 	%fd321, %fd320, %fd319;
	mul.f64 	%fd322, %fd6, %fd1109;
	mul.f64 	%fd323, %fd12, %fd1110;
	sub.f64 	%fd324, %fd323, %fd322;
	mul.f64 	%fd325, %fd9, %fd1110;
	mul.f64 	%fd326, %fd6, %fd1108;
	sub.f64 	%fd327, %fd326, %fd325;
	setp.gt.f64 	%p14, %fd318, 0d0000000000000000;
	setp.lt.f64 	%p15, %fd318, %fd315;
	mul.f64 	%fd328, %fd1111, %fd324;
	fma.rn.f64 	%fd329, %fd1112, %fd321, %fd328;
	fma.rn.f64 	%fd330, %fd1113, %fd327, %fd329;
	setp.eq.f64 	%p16, %fd330, 0d0000000000000000;
	mul.f64 	%fd331, %fd324, %fd324;
	fma.rn.f64 	%fd332, %fd321, %fd321, %fd331;
	fma.rn.f64 	%fd333, %fd327, %fd327, %fd332;
	mul.f64 	%fd334, %fd25, 0d3BC79CA100000000;
	mul.f64 	%fd335, %fd334, %fd27;
	setp.lt.f64 	%p17, %fd333, %fd335;
	or.pred  	%p18, %p16, %p17;
	and.pred  	%p19, %p14, %p15;
	and.pred  	%p20, %p18, %p19;
	mul.f64 	%fd336, %fd30, 0d3FE0000000000000;
	setp.lt.f64 	%p21, %fd31, %fd336;
	selp.b32 	%r336, 2, 5, %p21;
	selp.f64 	%fd337, %fd29, %fd32, %p21;
	selp.f64 	%fd1136, %fd27, %fd315, %p20;
	selp.b32 	%r337, 5, %r394, %p1;
	selp.b32 	%r396, %r336, %r337, %p20;
	selp.f64 	%fd1135, %fd337, %fd318, %p20;

$L__BB4_18:
	selp.f64 	%fd1138, %fd27, %fd1136, %p1;
	selp.b32 	%r397, 5, %r396, %p1;
	selp.f64 	%fd1137, %fd32, %fd1135, %p1;

$L__BB4_19:
	selp.f64 	%fd41, %fd27, %fd1138, %p13;
	selp.b32 	%r398, 2, %r397, %p13;
	selp.f64 	%fd42, %fd29, %fd1137, %p13;
	setp.gtu.f64 	%p24, %fd42, 0d0000000000000000;
	@%p24 bra 	$L__BB4_23;
	bra.uni 	$L__BB4_20;

$L__BB4_23:
	setp.ltu.f64 	%p27, %fd42, %fd41;
	@%p27 bra 	$L__BB4_27;

	mov.f64 	%fd339, 0d0000000000000000;
	sub.f64 	%fd340, %fd339, %fd28;
	add.f64 	%fd44, %fd340, %fd26;
	setp.le.f64 	%p28, %fd44, 0d0000000000000000;
	mov.u32 	%r398, 1;
	@%p28 bra 	$L__BB4_27;

	setp.ge.f64 	%p29, %fd44, %fd25;
	mov.u32 	%r398, 4;
	@%p29 bra 	$L__BB4_27;

	mov.u32 	%r398, 7;
	bra.uni 	$L__BB4_27;

$L__BB4_20:
	mov.f64 	%fd338, 0d0000000000000000;
	sub.f64 	%fd43, %fd338, %fd28;
	setp.le.f64 	%p25, %fd43, 0d0000000000000000;
	mov.u32 	%r398, 0;
	@%p25 bra 	$L__BB4_27;

	setp.ge.f64 	%p26, %fd43, %fd25;
	mov.u32 	%r398, 3;
	@%p26 bra 	$L__BB4_27;

	mov.u32 	%r398, 6;

$L__BB4_27:
	setp.eq.s32 	%p30, %r398, 0;
	@%p30 bra 	$L__BB4_43;

	setp.eq.s32 	%p31, %r398, 1;
	@%p31 bra 	$L__BB4_42;
	bra.uni 	$L__BB4_29;

$L__BB4_42:
	sub.f64 	%fd439, %fd5, %fd13;
	sub.f64 	%fd440, %fd8, %fd16;
	mul.f64 	%fd441, %fd440, %fd440;
	fma.rn.f64 	%fd442, %fd439, %fd439, %fd441;
	sub.f64 	%fd443, %fd11, %fd19;
	fma.rn.f64 	%fd1139, %fd443, %fd443, %fd442;
	bra.uni 	$L__BB4_44;

$L__BB4_117:
	sub.f64 	%fd1107, %fd200, %fd184;
	sub.f64 	%fd1106, %fd196, %fd178;
	sub.f64 	%fd1105, %fd198, %fd181;
	mul.f64 	%fd865, %fd1105, %fd1105;
	fma.rn.f64 	%fd866, %fd1106, %fd1106, %fd865;
	fma.rn.f64 	%fd1182, %fd1107, %fd1107, %fd866;
	bra.uni 	$L__BB4_118;

$L__BB4_43:
	sub.f64 	%fd1134, %fd11, %fd20;
	sub.f64 	%fd1133, %fd5, %fd14;
	sub.f64 	%fd1132, %fd8, %fd17;
	mul.f64 	%fd444, %fd1132, %fd1132;
	fma.rn.f64 	%fd445, %fd1133, %fd1133, %fd444;
	fma.rn.f64 	%fd1139, %fd1134, %fd1134, %fd445;
	bra.uni 	$L__BB4_44;

$L__BB4_107:
	setp.eq.s32 	%p108, %r406, 2;
	@%p108 bra 	$L__BB4_115;
	bra.uni 	$L__BB4_108;

$L__BB4_115:
	sub.f64 	%fd855, %fd196, %fd186;
	sub.f64 	%fd856, %fd198, %fd188;
	mul.f64 	%fd857, %fd856, %fd856;
	fma.rn.f64 	%fd858, %fd855, %fd855, %fd857;
	sub.f64 	%fd859, %fd200, %fd190;
	fma.rn.f64 	%fd1182, %fd859, %fd859, %fd858;
	bra.uni 	$L__BB4_118;

$L__BB4_29:
	setp.eq.s32 	%p32, %r398, 2;
	@%p32 bra 	$L__BB4_41;
	bra.uni 	$L__BB4_30;

$L__BB4_41:
	sub.f64 	%fd421, %fd14, %fd5;
	sub.f64 	%fd422, %fd19, %fd11;
	sub.f64 	%fd423, %fd17, %fd8;
	mul.f64 	%fd424, %fd423, %fd422;
	sub.f64 	%fd425, %fd16, %fd8;
	sub.f64 	%fd426, %fd20, %fd11;
	mul.f64 	%fd427, %fd426, %fd425;
	sub.f64 	%fd428, %fd424, %fd427;
	sub.f64 	%fd429, %fd13, %fd5;
	mul.f64 	%fd430, %fd426, %fd429;
	mul.f64 	%fd431, %fd421, %fd422;
	sub.f64 	%fd432, %fd430, %fd431;
	mul.f64 	%fd433, %fd421, %fd425;
	mul.f64 	%fd434, %fd423, %fd429;
	sub.f64 	%fd435, %fd433, %fd434;
	mul.f64 	%fd436, %fd432, %fd432;
	fma.rn.f64 	%fd437, %fd428, %fd428, %fd436;
	fma.rn.f64 	%fd438, %fd435, %fd435, %fd437;
	div.rn.f64 	%fd1139, %fd438, %fd27;
	bra.uni 	$L__BB4_44;

$L__BB4_108:
	setp.eq.s32 	%p109, %r406, 3;
	@%p109 bra 	$L__BB4_114;
	bra.uni 	$L__BB4_109;

$L__BB4_114:
	sub.f64 	%fd837, %fd178, %fd196;
	sub.f64 	%fd838, %fd183, %fd200;
	sub.f64 	%fd839, %fd181, %fd198;
	mul.f64 	%fd840, %fd839, %fd838;
	sub.f64 	%fd841, %fd180, %fd198;
	sub.f64 	%fd842, %fd184, %fd200;
	mul.f64 	%fd843, %fd842, %fd841;
	sub.f64 	%fd844, %fd840, %fd843;
	sub.f64 	%fd845, %fd177, %fd196;
	mul.f64 	%fd846, %fd842, %fd845;
	mul.f64 	%fd847, %fd837, %fd838;
	sub.f64 	%fd848, %fd846, %fd847;
	mul.f64 	%fd849, %fd837, %fd841;
	mul.f64 	%fd850, %fd839, %fd845;
	sub.f64 	%fd851, %fd849, %fd850;
	mul.f64 	%fd852, %fd848, %fd848;
	fma.rn.f64 	%fd853, %fd844, %fd844, %fd852;
	fma.rn.f64 	%fd854, %fd851, %fd851, %fd853;
	div.rn.f64 	%fd1182, %fd854, %fd195;
	bra.uni 	$L__BB4_118;

$L__BB4_30:
	setp.eq.s32 	%p33, %r398, 3;
	@%p33 bra 	$L__BB4_40;
	bra.uni 	$L__BB4_31;

$L__BB4_40:
	sub.f64 	%fd416, %fd4, %fd14;
	sub.f64 	%fd417, %fd7, %fd17;
	mul.f64 	%fd418, %fd417, %fd417;
	fma.rn.f64 	%fd419, %fd416, %fd416, %fd418;
	sub.f64 	%fd420, %fd10, %fd20;
	fma.rn.f64 	%fd1139, %fd420, %fd420, %fd419;
	bra.uni 	$L__BB4_44;

$L__BB4_109:
	setp.eq.s32 	%p110, %r406, 4;
	@%p110 bra 	$L__BB4_113;
	bra.uni 	$L__BB4_110;

$L__BB4_113:
	sub.f64 	%fd813, %fd177, %fd196;
	sub.f64 	%fd814, %fd190, %fd200;
	sub.f64 	%fd815, %fd180, %fd198;
	mul.f64 	%fd816, %fd815, %fd814;
	sub.f64 	%fd817, %fd188, %fd198;
	sub.f64 	%fd818, %fd183, %fd200;
	mul.f64 	%fd819, %fd818, %fd817;
	sub.f64 	%fd820, %fd816, %fd819;
	sub.f64 	%fd821, %fd186, %fd196;
	mul.f64 	%fd822, %fd818, %fd821;
	mul.f64 	%fd823, %fd813, %fd814;
	sub.f64 	%fd824, %fd822, %fd823;
	mul.f64 	%fd825, %fd813, %fd817;
	mul.f64 	%fd826, %fd815, %fd821;
	sub.f64 	%fd827, %fd825, %fd826;
	mul.f64 	%fd828, %fd824, %fd824;
	fma.rn.f64 	%fd829, %fd820, %fd820, %fd828;
	fma.rn.f64 	%fd830, %fd827, %fd827, %fd829;
	sub.f64 	%fd831, %fd186, %fd177;
	sub.f64 	%fd832, %fd188, %fd180;
	mul.f64 	%fd833, %fd832, %fd832;
	fma.rn.f64 	%fd834, %fd831, %fd831, %fd833;
	sub.f64 	%fd835, %fd190, %fd183;
	fma.rn.f64 	%fd836, %fd835, %fd835, %fd834;
	div.rn.f64 	%fd1182, %fd830, %fd836;
	bra.uni 	$L__BB4_118;

$L__BB4_31:
	setp.eq.s32 	%p34, %r398, 4;
	@%p34 bra 	$L__BB4_39;
	bra.uni 	$L__BB4_32;

$L__BB4_39:
	sub.f64 	%fd411, %fd4, %fd13;
	sub.f64 	%fd412, %fd7, %fd16;
	mul.f64 	%fd413, %fd412, %fd412;
	fma.rn.f64 	%fd414, %fd411, %fd411, %fd413;
	sub.f64 	%fd415, %fd10, %fd19;
	fma.rn.f64 	%fd1139, %fd415, %fd415, %fd414;
	bra.uni 	$L__BB4_44;

$L__BB4_110:
	setp.eq.s32 	%p111, %r406, 5;
	@%p111 bra 	$L__BB4_112;
	bra.uni 	$L__BB4_111;

$L__BB4_112:
	sub.f64 	%fd789, %fd186, %fd196;
	sub.f64 	%fd790, %fd184, %fd200;
	sub.f64 	%fd791, %fd188, %fd198;
	mul.f64 	%fd792, %fd790, %fd791;
	sub.f64 	%fd793, %fd181, %fd198;
	sub.f64 	%fd794, %fd190, %fd200;
	mul.f64 	%fd795, %fd793, %fd794;
	sub.f64 	%fd796, %fd792, %fd795;
	sub.f64 	%fd797, %fd178, %fd196;
	mul.f64 	%fd798, %fd797, %fd794;
	mul.f64 	%fd799, %fd790, %fd789;
	sub.f64 	%fd800, %fd798, %fd799;
	mul.f64 	%fd801, %fd793, %fd789;
	mul.f64 	%fd802, %fd797, %fd791;
	sub.f64 	%fd803, %fd801, %fd802;
	mul.f64 	%fd804, %fd800, %fd800;
	fma.rn.f64 	%fd805, %fd796, %fd796, %fd804;
	fma.rn.f64 	%fd806, %fd803, %fd803, %fd805;
	sub.f64 	%fd807, %fd178, %fd186;
	sub.f64 	%fd808, %fd181, %fd188;
	mul.f64 	%fd809, %fd808, %fd808;
	fma.rn.f64 	%fd810, %fd807, %fd807, %fd809;
	sub.f64 	%fd811, %fd184, %fd190;
	fma.rn.f64 	%fd812, %fd811, %fd811, %fd810;
	div.rn.f64 	%fd1182, %fd806, %fd812;
	bra.uni 	$L__BB4_118;

$L__BB4_32:
	setp.eq.s32 	%p35, %r398, 5;
	@%p35 bra 	$L__BB4_38;
	bra.uni 	$L__BB4_33;

$L__BB4_38:
	sub.f64 	%fd393, %fd14, %fd4;
	sub.f64 	%fd394, %fd19, %fd10;
	sub.f64 	%fd395, %fd17, %fd7;
	mul.f64 	%fd396, %fd395, %fd394;
	sub.f64 	%fd397, %fd16, %fd7;
	sub.f64 	%fd398, %fd20, %fd10;
	mul.f64 	%fd399, %fd398, %fd397;
	sub.f64 	%fd400, %fd396, %fd399;
	sub.f64 	%fd401, %fd13, %fd4;
	mul.f64 	%fd402, %fd398, %fd401;
	mul.f64 	%fd403, %fd393, %fd394;
	sub.f64 	%fd404, %fd402, %fd403;
	mul.f64 	%fd405, %fd393, %fd397;
	mul.f64 	%fd406, %fd395, %fd401;
	sub.f64 	%fd407, %fd405, %fd406;
	mul.f64 	%fd408, %fd404, %fd404;
	fma.rn.f64 	%fd409, %fd400, %fd400, %fd408;
	fma.rn.f64 	%fd410, %fd407, %fd407, %fd409;
	div.rn.f64 	%fd1139, %fd410, %fd27;
	bra.uni 	$L__BB4_44;

$L__BB4_111:
	sub.f64 	%fd1089, %fd200, %fd184;
	sub.f64 	%fd1088, %fd196, %fd178;
	sub.f64 	%fd1087, %fd198, %fd181;
	mul.f64 	%fd782, %fd1087, %fd193;
	fma.rn.f64 	%fd783, %fd1088, %fd192, %fd782;
	fma.rn.f64 	%fd784, %fd1089, %fd194, %fd783;
	mul.f64 	%fd785, %fd784, %fd784;
	mul.f64 	%fd786, %fd193, %fd193;
	fma.rn.f64 	%fd787, %fd192, %fd192, %fd786;
	fma.rn.f64 	%fd788, %fd194, %fd194, %fd787;
	div.rn.f64 	%fd1182, %fd785, %fd788;

$L__BB4_118:
	mul.f64 	%fd867, %fd176, %fd176;
	sub.f64 	%fd214, %fd1182, %fd867;
	fma.rn.f64 	%fd215, %fd2, %fd176, %fd1;
	setp.geu.f64 	%p112, %fd214, %fd215;
	@%p112 bra 	$L__BB4_127;

	mul.lo.s64 	%rd164, %rd61, %rd35;
	add.s64 	%rd165, %rd12, %rd164;
	ld.global.f64 	%fd216, [%rd165];
	div.rn.f64 	%fd1183, %fd214, %fd215;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r407}, %fd1183;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r408, %temp}, %fd1183;
	}
	setp.gt.s32 	%p113, %r407, 1048575;
	mov.u32 	%r409, -1023;
	@%p113 bra 	$L__BB4_121;

	mul.f64 	%fd1183, %fd1183, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r407}, %fd1183;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r408, %temp}, %fd1183;
	}
	mov.u32 	%r409, -1077;

$L__BB4_121:
	add.s32 	%r378, %r407, -1;
	setp.lt.u32 	%p114, %r378, 2146435071;
	@%p114 bra 	$L__BB4_123;
	bra.uni 	$L__BB4_122;

$L__BB4_123:
	shr.u32 	%r380, %r407, 20;
	add.s32 	%r410, %r409, %r380;
	and.b32  	%r381, %r407, -2146435073;
	or.b32  	%r382, %r381, 1072693248;
	mov.b64 	%fd1184, {%r408, %r382};
	setp.lt.s32 	%p116, %r382, 1073127583;
	@%p116 bra 	$L__BB4_125;

	{
	.reg .b32 %temp;
	mov.b64 	{%r383, %temp}, %fd1184;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r384}, %fd1184;
	}
	add.s32 	%r385, %r384, -1048576;
	mov.b64 	%fd1184, {%r383, %r385};
	add.s32 	%r410, %r410, 1;

$L__BB4_125:
	add.f64 	%fd870, %fd1184, 0d3FF0000000000000;
	mov.f64 	%fd871, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd872, %fd870;
	neg.f64 	%fd873, %fd870;
	fma.rn.f64 	%fd874, %fd873, %fd872, %fd871;
	fma.rn.f64 	%fd875, %fd874, %fd874, %fd874;
	fma.rn.f64 	%fd876, %fd875, %fd872, %fd872;
	add.f64 	%fd877, %fd1184, 0dBFF0000000000000;
	mul.f64 	%fd878, %fd877, %fd876;
	fma.rn.f64 	%fd879, %fd877, %fd876, %fd878;
	mul.f64 	%fd880, %fd879, %fd879;
	mov.f64 	%fd881, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd882, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd883, %fd882, %fd880, %fd881;
	mov.f64 	%fd884, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd885, %fd883, %fd880, %fd884;
	mov.f64 	%fd886, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd887, %fd885, %fd880, %fd886;
	mov.f64 	%fd888, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd889, %fd887, %fd880, %fd888;
	mov.f64 	%fd890, 0d3F624924923BE72D;
	fma.rn.f64 	%fd891, %fd889, %fd880, %fd890;
	mov.f64 	%fd892, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd893, %fd891, %fd880, %fd892;
	mov.f64 	%fd894, 0d3FB5555555555554;
	fma.rn.f64 	%fd895, %fd893, %fd880, %fd894;
	sub.f64 	%fd896, %fd877, %fd879;
	add.f64 	%fd897, %fd896, %fd896;
	neg.f64 	%fd898, %fd879;
	fma.rn.f64 	%fd899, %fd898, %fd877, %fd897;
	mul.f64 	%fd900, %fd876, %fd899;
	mul.f64 	%fd901, %fd880, %fd895;
	fma.rn.f64 	%fd902, %fd901, %fd879, %fd900;
	xor.b32  	%r386, %r410, -2147483648;
	mov.u32 	%r387, -2147483648;
	mov.u32 	%r388, 1127219200;
	mov.b64 	%fd903, {%r386, %r388};
	mov.b64 	%fd904, {%r387, %r388};
	sub.f64 	%fd905, %fd903, %fd904;
	mov.f64 	%fd906, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd907, %fd905, %fd906, %fd879;
	neg.f64 	%fd908, %fd905;
	fma.rn.f64 	%fd909, %fd908, %fd906, %fd907;
	sub.f64 	%fd910, %fd909, %fd879;
	sub.f64 	%fd911, %fd902, %fd910;
	mov.f64 	%fd912, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd913, %fd905, %fd912, %fd911;
	add.f64 	%fd1185, %fd907, %fd913;
	bra.uni 	$L__BB4_126;

$L__BB4_122:
	mov.f64 	%fd868, 0d7FF0000000000000;
	fma.rn.f64 	%fd869, %fd1183, %fd868, %fd868;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r379}, %fd1183;
	}
	mov.b32 	%f2, %r379;
	setp.eq.f32 	%p115, %f2, 0f00000000;
	selp.f64 	%fd1185, 0dFFF0000000000000, %fd869, %p115;

$L__BB4_126:
	sub.f64 	%fd914, %fd214, %fd215;
	div.rn.f64 	%fd915, %fd914, %fd215;
	mul.f64 	%fd916, %fd915, %fd1185;
	mul.f64 	%fd917, %fd916, 0dC000000000000000;
	div.rn.f64 	%fd918, %fd917, %fd215;
	mul.f64 	%fd919, %fd915, %fd915;
	div.rn.f64 	%fd920, %fd919, %fd214;
	sub.f64 	%fd921, %fd918, %fd920;
	mul.f64 	%fd922, %fd921, %fd295;
	mul.f64 	%fd923, %fd216, %fd296;
	mul.f64 	%fd924, %fd923, %fd294;
	mul.f64 	%fd925, %fd924, %fd922;
	mul.f64 	%fd926, %fd925, 0dC000000000000000;
	sqrt.rn.f64 	%fd927, %fd214;
	fma.rn.f64 	%fd928, %fd927, %fd926, 0d0000000000000000;
	st.global.f64 	[%rd50], %fd928;

$L__BB4_127:
	mov.u32 	%r411, 3;
	@%p3 bra 	$L__BB4_133;

	sub.f64 	%fd929, %fd186, %fd177;
	sub.f64 	%fd930, %fd188, %fd180;
	mul.f64 	%fd931, %fd930, %fd194;
	sub.f64 	%fd932, %fd190, %fd183;
	mul.f64 	%fd933, %fd932, %fd193;
	sub.f64 	%fd934, %fd931, %fd933;
	mul.f64 	%fd935, %fd932, %fd192;
	mul.f64 	%fd936, %fd929, %fd194;
	sub.f64 	%fd937, %fd935, %fd936;
	mul.f64 	%fd938, %fd929, %fd193;
	mul.f64 	%fd939, %fd930, %fd192;
	sub.f64 	%fd940, %fd938, %fd939;
	mul.f64 	%fd941, %fd930, %fd930;
	fma.rn.f64 	%fd942, %fd929, %fd929, %fd941;
	fma.rn.f64 	%fd943, %fd932, %fd932, %fd942;
	mul.f64 	%fd944, %fd930, %fd937;
	fma.rn.f64 	%fd945, %fd929, %fd934, %fd944;
	fma.rn.f64 	%fd946, %fd932, %fd940, %fd945;
	mul.f64 	%fd947, %fd937, %fd937;
	fma.rn.f64 	%fd948, %fd934, %fd934, %fd947;
	fma.rn.f64 	%fd949, %fd940, %fd940, %fd948;
	sub.f64 	%fd950, %fd196, %fd177;
	sub.f64 	%fd951, %fd198, %fd180;
	mul.f64 	%fd952, %fd951, %fd930;
	fma.rn.f64 	%fd953, %fd950, %fd929, %fd952;
	sub.f64 	%fd954, %fd200, %fd183;
	fma.rn.f64 	%fd955, %fd954, %fd932, %fd953;
	mul.f64 	%fd956, %fd951, %fd937;
	fma.rn.f64 	%fd957, %fd950, %fd934, %fd956;
	fma.rn.f64 	%fd958, %fd954, %fd940, %fd957;
	div.rn.f64 	%fd959, %fd946, %fd943;
	mul.f64 	%fd960, %fd959, %fd959;
	mul.f64 	%fd961, %fd943, %fd960;
	sub.f64 	%fd962, %fd949, %fd961;
	mul.f64 	%fd963, %fd955, %fd959;
	sub.f64 	%fd964, %fd958, %fd963;
	div.rn.f64 	%fd965, %fd964, %fd962;
	mul.f64 	%fd966, %fd943, %fd959;
	mul.f64 	%fd967, %fd966, %fd965;
	sub.f64 	%fd968, %fd955, %fd967;
	div.rn.f64 	%fd226, %fd968, %fd943;
	setp.gt.f64 	%p117, %fd226, 0d0000000000000000;
	setp.lt.f64 	%p118, %fd226, 0d3FF0000000000000;
	setp.ge.f64 	%p119, %fd965, 0d0000000000000000;
	and.pred  	%p120, %p117, %p118;
	and.pred  	%p121, %p119, %p120;
	mov.u32 	%r411, 4;
	@%p121 bra 	$L__BB4_133;

	sub.f64 	%fd969, %fd178, %fd186;
	sub.f64 	%fd970, %fd181, %fd188;
	mul.f64 	%fd971, %fd970, %fd194;
	sub.f64 	%fd972, %fd184, %fd190;
	mul.f64 	%fd973, %fd972, %fd193;
	sub.f64 	%fd974, %fd971, %fd973;
	mul.f64 	%fd975, %fd972, %fd192;
	mul.f64 	%fd976, %fd969, %fd194;
	sub.f64 	%fd977, %fd975, %fd976;
	mul.f64 	%fd978, %fd969, %fd193;
	mul.f64 	%fd979, %fd970, %fd192;
	sub.f64 	%fd980, %fd978, %fd979;
	mul.f64 	%fd981, %fd970, %fd970;
	fma.rn.f64 	%fd982, %fd969, %fd969, %fd981;
	fma.rn.f64 	%fd983, %fd972, %fd972, %fd982;
	mul.f64 	%fd984, %fd970, %fd977;
	fma.rn.f64 	%fd985, %fd969, %fd974, %fd984;
	fma.rn.f64 	%fd986, %fd972, %fd980, %fd985;
	mul.f64 	%fd987, %fd977, %fd977;
	fma.rn.f64 	%fd988, %fd974, %fd974, %fd987;
	fma.rn.f64 	%fd989, %fd980, %fd980, %fd988;
	sub.f64 	%fd990, %fd196, %fd186;
	sub.f64 	%fd991, %fd198, %fd188;
	mul.f64 	%fd992, %fd970, %fd991;
	fma.rn.f64 	%fd993, %fd969, %fd990, %fd992;
	sub.f64 	%fd994, %fd200, %fd190;
	fma.rn.f64 	%fd995, %fd972, %fd994, %fd993;
	mul.f64 	%fd996, %fd991, %fd977;
	fma.rn.f64 	%fd997, %fd990, %fd974, %fd996;
	fma.rn.f64 	%fd998, %fd994, %fd980, %fd997;
	div.rn.f64 	%fd999, %fd986, %fd983;
	mul.f64 	%fd1000, %fd999, %fd999;
	mul.f64 	%fd1001, %fd983, %fd1000;
	sub.f64 	%fd1002, %fd989, %fd1001;
	mul.f64 	%fd1003, %fd995, %fd999;
	sub.f64 	%fd1004, %fd998, %fd1003;
	div.rn.f64 	%fd1005, %fd1004, %fd1002;
	mul.f64 	%fd1006, %fd983, %fd999;
	mul.f64 	%fd1007, %fd1006, %fd1005;
	sub.f64 	%fd1008, %fd995, %fd1007;
	div.rn.f64 	%fd227, %fd1008, %fd983;
	setp.gt.f64 	%p122, %fd227, 0d0000000000000000;
	setp.lt.f64 	%p123, %fd227, 0d3FF0000000000000;
	setp.ge.f64 	%p124, %fd1005, 0d0000000000000000;
	and.pred  	%p125, %p122, %p123;
	and.pred  	%p126, %p124, %p125;
	mov.u32 	%r411, 5;
	@%p126 bra 	$L__BB4_133;

	setp.le.f64 	%p127, %fd203, 0d0000000000000000;
	setp.ge.f64 	%p128, %fd227, 0d3FF0000000000000;
	and.pred  	%p129, %p127, %p128;
	mov.u32 	%r411, 0;
	@%p129 bra 	$L__BB4_133;

	setp.le.f64 	%p130, %fd226, 0d0000000000000000;
	setp.ge.f64 	%p131, %fd203, 0d3FF0000000000000;
	and.pred  	%p132, %p130, %p131;
	mov.u32 	%r411, 1;
	@%p132 bra 	$L__BB4_133;

	setp.le.f64 	%p133, %fd227, 0d0000000000000000;
	setp.ge.f64 	%p134, %fd226, 0d3FF0000000000000;
	and.pred  	%p135, %p133, %p134;
	selp.b32 	%r411, 2, 6, %p135;

$L__BB4_133:
	setp.eq.s32 	%p136, %r411, 0;
	mov.f64 	%fd1208, 0d0000000000000000;
	mov.f64 	%fd1209, %fd1208;
	@%p136 bra 	$L__BB4_155;

	setp.eq.s32 	%p137, %r411, 1;
	selp.f64 	%fd1204, 0d3FF0000000000000, 0d0000000000000000, %p137;
	mov.f64 	%fd1205, 0d0000000000000000;
	@%p137 bra 	$L__BB4_153;
	bra.uni 	$L__BB4_135;

$L__BB4_153:
	mov.f64 	%fd1206, 0d0000000000000000;
	mov.f64 	%fd1207, %fd1206;
	bra.uni 	$L__BB4_154;

$L__BB4_135:
	setp.eq.s32 	%p138, %r411, 2;
	selp.f64 	%fd1011, 0d3FF0000000000000, 0d0000000000000000, %p138;
	selp.f64 	%fd1201, %fd1011, %fd1205, %p138;
	selp.f64 	%fd1200, 0d0000000000000000, %fd1204, %p138;
	@%p138 bra 	$L__BB4_151;
	bra.uni 	$L__BB4_136;

$L__BB4_151:
	mov.f64 	%fd1202, 0d0000000000000000;
	mov.f64 	%fd1203, %fd1202;
	bra.uni 	$L__BB4_152;

$L__BB4_136:
	setp.ne.s32 	%p139, %r411, 3;
	mov.f64 	%fd1186, 0d0000000000000000;
	@%p139 bra 	$L__BB4_138;

	div.rn.f64 	%fd1186, %fd202, %fd195;

$L__BB4_138:
	setp.eq.s32 	%p140, %r411, 3;
	selp.f64 	%fd1196, %fd1186, %fd1200, %p140;
	selp.f64 	%fd1197, 0d0000000000000000, %fd1201, %p140;
	@%p140 bra 	$L__BB4_149;
	bra.uni 	$L__BB4_139;

$L__BB4_149:
	mov.f64 	%fd1198, 0d0000000000000000;
	mov.f64 	%fd1199, %fd1198;
	bra.uni 	$L__BB4_150;

$L__BB4_139:
	setp.ne.s32 	%p141, %r411, 4;
	mov.f64 	%fd1187, 0d0000000000000000;
	mov.f64 	%fd1188, %fd1187;
	@%p141 bra 	$L__BB4_141;

	sub.f64 	%fd1015, %fd196, %fd177;
	sub.f64 	%fd1016, %fd186, %fd177;
	sub.f64 	%fd1017, %fd188, %fd180;
	sub.f64 	%fd1018, %fd198, %fd180;
	mul.f64 	%fd1019, %fd1018, %fd1017;
	fma.rn.f64 	%fd1020, %fd1015, %fd1016, %fd1019;
	sub.f64 	%fd1021, %fd190, %fd183;
	sub.f64 	%fd1022, %fd200, %fd183;
	fma.rn.f64 	%fd1023, %fd1022, %fd1021, %fd1020;
	mul.f64 	%fd1024, %fd1017, %fd1017;
	fma.rn.f64 	%fd1025, %fd1016, %fd1016, %fd1024;
	fma.rn.f64 	%fd1026, %fd1021, %fd1021, %fd1025;
	div.rn.f64 	%fd1188, %fd1023, %fd1026;
	mov.f64 	%fd1027, 0d3FF0000000000000;
	sub.f64 	%fd1187, %fd1027, %fd1188;

$L__BB4_141:
	setp.eq.s32 	%p142, %r411, 4;
	selp.f64 	%fd1192, %fd1187, %fd1196, %p142;
	selp.f64 	%fd1193, %fd1188, %fd1197, %p142;
	@%p142 bra 	$L__BB4_147;
	bra.uni 	$L__BB4_142;

$L__BB4_147:
	mov.f64 	%fd1194, 0d0000000000000000;
	mov.f64 	%fd1195, %fd1194;
	bra.uni 	$L__BB4_148;

$L__BB4_142:
	setp.ne.s32 	%p143, %r411, 5;
	mov.f64 	%fd1190, 0d0000000000000000;
	mov.f64 	%fd1189, %fd1190;
	@%p143 bra 	$L__BB4_144;

	sub.f64 	%fd1104, %fd190, %fd184;
	sub.f64 	%fd1103, %fd186, %fd178;
	sub.f64 	%fd1102, %fd188, %fd181;
	sub.f64 	%fd1101, %fd200, %fd184;
	sub.f64 	%fd1100, %fd196, %fd178;
	sub.f64 	%fd1099, %fd198, %fd181;
	mul.f64 	%fd1029, %fd1099, %fd1102;
	fma.rn.f64 	%fd1030, %fd1100, %fd1103, %fd1029;
	fma.rn.f64 	%fd1031, %fd1101, %fd1104, %fd1030;
	mul.f64 	%fd1032, %fd1102, %fd1102;
	fma.rn.f64 	%fd1033, %fd1103, %fd1103, %fd1032;
	fma.rn.f64 	%fd1034, %fd1104, %fd1104, %fd1033;
	div.rn.f64 	%fd1189, %fd1031, %fd1034;

$L__BB4_144:
	setp.eq.s32 	%p144, %r411, 5;
	selp.f64 	%fd253, %fd1189, %fd1193, %p144;
	selp.f64 	%fd252, 0d0000000000000000, %fd1192, %p144;
	mov.f64 	%fd1191, %fd1190;
	@%p144 bra 	$L__BB4_146;

	sub.f64 	%fd1098, %fd183, %fd184;
	sub.f64 	%fd1097, %fd177, %fd178;
	sub.f64 	%fd1096, %fd180, %fd181;
	sub.f64 	%fd1095, %fd190, %fd184;
	sub.f64 	%fd1094, %fd186, %fd178;
	sub.f64 	%fd1093, %fd188, %fd181;
	sub.f64 	%fd1092, %fd200, %fd184;
	sub.f64 	%fd1091, %fd196, %fd178;
	sub.f64 	%fd1090, %fd198, %fd181;
	mul.f64 	%fd1037, %fd1096, %fd1093;
	fma.rn.f64 	%fd1038, %fd1097, %fd1094, %fd1037;
	fma.rn.f64 	%fd1039, %fd1098, %fd1095, %fd1038;
	mul.f64 	%fd1040, %fd1093, %fd1093;
	fma.rn.f64 	%fd1041, %fd1094, %fd1094, %fd1040;
	fma.rn.f64 	%fd1042, %fd1095, %fd1095, %fd1041;
	mul.f64 	%fd1043, %fd1090, %fd1093;
	fma.rn.f64 	%fd1044, %fd1091, %fd1094, %fd1043;
	fma.rn.f64 	%fd1045, %fd1092, %fd1095, %fd1044;
	div.rn.f64 	%fd1046, %fd1039, %fd195;
	mul.f64 	%fd1047, %fd1046, %fd1046;
	mul.f64 	%fd1048, %fd195, %fd1047;
	sub.f64 	%fd1049, %fd1042, %fd1048;
	mul.f64 	%fd1050, %fd202, %fd1046;
	sub.f64 	%fd1051, %fd1045, %fd1050;
	div.rn.f64 	%fd1191, %fd1051, %fd1049;
	mul.f64 	%fd1052, %fd195, %fd1046;
	mul.f64 	%fd1053, %fd1052, %fd1191;
	sub.f64 	%fd1054, %fd202, %fd1053;
	div.rn.f64 	%fd1190, %fd1054, %fd195;

$L__BB4_146:
	selp.f64 	%fd1194, %fd252, %fd1190, %p144;
	selp.f64 	%fd1195, %fd253, %fd1191, %p144;

$L__BB4_148:
	selp.f64 	%fd1198, %fd1192, %fd1194, %p142;
	selp.f64 	%fd1199, %fd1193, %fd1195, %p142;

$L__BB4_150:
	selp.f64 	%fd1202, %fd1196, %fd1198, %p140;
	selp.f64 	%fd1203, %fd1197, %fd1199, %p140;

$L__BB4_152:
	selp.f64 	%fd1206, %fd1200, %fd1202, %p138;
	selp.f64 	%fd1207, %fd1201, %fd1203, %p138;

$L__BB4_154:
	selp.f64 	%fd1208, %fd1204, %fd1206, %p137;
	selp.f64 	%fd1209, %fd1205, %fd1207, %p137;

$L__BB4_155:
	selp.f64 	%fd1063, 0d0000000000000000, %fd1209, %p136;
	selp.f64 	%fd1064, 0d0000000000000000, %fd1208, %p136;
	mov.f64 	%fd1065, 0d3FF0000000000000;
	sub.f64 	%fd1066, %fd1065, %fd1064;
	sub.f64 	%fd1067, %fd1066, %fd1063;
	mul.f64 	%fd1068, %fd178, %fd1067;
	mul.f64 	%fd1069, %fd181, %fd1067;
	mul.f64 	%fd1070, %fd184, %fd1067;
	fma.rn.f64 	%fd1071, %fd177, %fd1064, %fd1068;
	fma.rn.f64 	%fd1072, %fd180, %fd1064, %fd1069;
	fma.rn.f64 	%fd1073, %fd183, %fd1064, %fd1070;
	fma.rn.f64 	%fd1074, %fd186, %fd1063, %fd1071;
	fma.rn.f64 	%fd1075, %fd188, %fd1063, %fd1072;
	fma.rn.f64 	%fd1076, %fd190, %fd1063, %fd1073;
	sub.f64 	%fd1077, %fd196, %fd1074;
	sub.f64 	%fd1078, %fd198, %fd1075;
	sub.f64 	%fd1079, %fd200, %fd1076;
	mul.f64 	%fd1080, %fd1078, %fd1078;
	fma.rn.f64 	%fd1081, %fd1077, %fd1077, %fd1080;
	fma.rn.f64 	%fd1082, %fd1079, %fd1079, %fd1081;
	sqrt.rn.f64 	%fd1083, %fd1082;
	div.rn.f64 	%fd1084, %fd1077, %fd1083;
	div.rn.f64 	%fd1085, %fd1078, %fd1083;
	div.rn.f64 	%fd1086, %fd1079, %fd1083;
	st.global.f64 	[%rd53], %fd1064;
	st.global.f64 	[%rd53+8], %fd1063;
	st.global.f64 	[%rd54], %fd1084;
	st.global.f64 	[%rd54+8], %fd1085;
	st.global.f64 	[%rd54+16], %fd1086;
	bra.uni 	$L__BB4_156;

$L__BB4_33:
	setp.eq.s32 	%p36, %r398, 6;
	@%p36 bra 	$L__BB4_37;
	bra.uni 	$L__BB4_34;

$L__BB4_37:
	sub.f64 	%fd1131, %fd11, %fd20;
	sub.f64 	%fd1130, %fd5, %fd14;
	sub.f64 	%fd1129, %fd8, %fd17;
	sub.f64 	%fd378, %fd4, %fd14;
	sub.f64 	%fd379, %fd10, %fd20;
	mul.f64 	%fd380, %fd1129, %fd379;
	sub.f64 	%fd381, %fd7, %fd17;
	mul.f64 	%fd382, %fd381, %fd1131;
	sub.f64 	%fd383, %fd380, %fd382;
	mul.f64 	%fd384, %fd378, %fd1131;
	mul.f64 	%fd385, %fd1130, %fd379;
	sub.f64 	%fd386, %fd384, %fd385;
	mul.f64 	%fd387, %fd1130, %fd381;
	mul.f64 	%fd388, %fd378, %fd1129;
	sub.f64 	%fd389, %fd387, %fd388;
	mul.f64 	%fd390, %fd386, %fd386;
	fma.rn.f64 	%fd391, %fd383, %fd383, %fd390;
	fma.rn.f64 	%fd392, %fd389, %fd389, %fd391;
	div.rn.f64 	%fd1139, %fd392, %fd25;
	bra.uni 	$L__BB4_44;

$L__BB4_34:
	setp.eq.s32 	%p37, %r398, 7;
	@%p37 bra 	$L__BB4_36;
	bra.uni 	$L__BB4_35;

$L__BB4_36:
	sub.f64 	%fd360, %fd5, %fd13;
	sub.f64 	%fd361, %fd10, %fd19;
	sub.f64 	%fd362, %fd8, %fd16;
	mul.f64 	%fd363, %fd362, %fd361;
	sub.f64 	%fd364, %fd7, %fd16;
	sub.f64 	%fd365, %fd11, %fd19;
	mul.f64 	%fd366, %fd364, %fd365;
	sub.f64 	%fd367, %fd363, %fd366;
	sub.f64 	%fd368, %fd4, %fd13;
	mul.f64 	%fd369, %fd368, %fd365;
	mul.f64 	%fd370, %fd360, %fd361;
	sub.f64 	%fd371, %fd369, %fd370;
	mul.f64 	%fd372, %fd360, %fd364;
	mul.f64 	%fd373, %fd368, %fd362;
	sub.f64 	%fd374, %fd372, %fd373;
	mul.f64 	%fd375, %fd371, %fd371;
	fma.rn.f64 	%fd376, %fd367, %fd367, %fd375;
	fma.rn.f64 	%fd377, %fd374, %fd374, %fd376;
	div.rn.f64 	%fd1139, %fd377, %fd25;
	bra.uni 	$L__BB4_44;

$L__BB4_35:
	sub.f64 	%fd1116, %fd13, %fd14;
	sub.f64 	%fd1115, %fd19, %fd20;
	sub.f64 	%fd1114, %fd16, %fd17;
	sub.f64 	%fd341, %fd14, %fd5;
	mul.f64 	%fd342, %fd12, %fd1114;
	mul.f64 	%fd343, %fd9, %fd1115;
	sub.f64 	%fd344, %fd343, %fd342;
	mul.f64 	%fd345, %fd6, %fd1115;
	mul.f64 	%fd346, %fd12, %fd1116;
	sub.f64 	%fd347, %fd346, %fd345;
	mul.f64 	%fd348, %fd9, %fd1116;
	mul.f64 	%fd349, %fd6, %fd1114;
	sub.f64 	%fd350, %fd349, %fd348;
	sub.f64 	%fd351, %fd17, %fd8;
	mul.f64 	%fd352, %fd351, %fd347;
	fma.rn.f64 	%fd353, %fd341, %fd344, %fd352;
	sub.f64 	%fd354, %fd20, %fd11;
	fma.rn.f64 	%fd355, %fd354, %fd350, %fd353;
	mul.f64 	%fd356, %fd355, %fd355;
	mul.f64 	%fd357, %fd347, %fd347;
	fma.rn.f64 	%fd358, %fd344, %fd344, %fd357;
	fma.rn.f64 	%fd359, %fd350, %fd350, %fd358;
	div.rn.f64 	%fd1139, %fd356, %fd359;

$L__BB4_44:
	mul.f64 	%fd446, %fd3, %fd3;
	sub.f64 	%fd55, %fd1139, %fd446;
	fma.rn.f64 	%fd56, %fd2, %fd3, %fd1;
	setp.geu.f64 	%p38, %fd55, %fd56;
	@%p38 bra 	$L__BB4_56;

	sub.f64 	%fd1128, %fd13, %fd14;
	sub.f64 	%fd1127, %fd19, %fd20;
	sub.f64 	%fd1126, %fd16, %fd17;
	mul.lo.s64 	%rd131, %rd57, %rd36;
	add.s64 	%rd132, %rd16, %rd131;
	mul.lo.s64 	%rd133, %rd58, %rd36;
	add.s64 	%rd134, %rd16, %rd133;
	mul.lo.s64 	%rd135, %rd59, %rd36;
	add.s64 	%rd136, %rd16, %rd135;
	mul.lo.s64 	%rd137, %rd60, %rd36;
	add.s64 	%rd138, %rd16, %rd137;
	ld.global.f64 	%fd448, [%rd134];
	ld.global.f64 	%fd449, [%rd132];
	sub.f64 	%fd450, %fd448, %fd449;
	ld.global.f64 	%fd451, [%rd134+8];
	ld.global.f64 	%fd452, [%rd132+8];
	sub.f64 	%fd453, %fd451, %fd452;
	ld.global.f64 	%fd454, [%rd134+16];
	ld.global.f64 	%fd455, [%rd132+16];
	sub.f64 	%fd456, %fd454, %fd455;
	ld.global.f64 	%fd457, [%rd138];
	ld.global.f64 	%fd458, [%rd136];
	sub.f64 	%fd459, %fd457, %fd458;
	ld.global.f64 	%fd460, [%rd138+8];
	ld.global.f64 	%fd461, [%rd136+8];
	sub.f64 	%fd462, %fd460, %fd461;
	ld.global.f64 	%fd463, [%rd138+16];
	ld.global.f64 	%fd464, [%rd136+16];
	sub.f64 	%fd465, %fd463, %fd464;
	mul.f64 	%fd466, %fd453, %fd453;
	fma.rn.f64 	%fd467, %fd450, %fd450, %fd466;
	fma.rn.f64 	%fd468, %fd456, %fd456, %fd467;
	mul.f64 	%fd469, %fd468, 0d3F50624DE0000000;
	mul.f64 	%fd470, %fd462, %fd462;
	fma.rn.f64 	%fd471, %fd459, %fd459, %fd470;
	fma.rn.f64 	%fd472, %fd465, %fd465, %fd471;
	mul.f64 	%fd57, %fd469, %fd472;
	mul.f64 	%fd473, %fd12, %fd1126;
	mul.f64 	%fd474, %fd9, %fd1127;
	sub.f64 	%fd475, %fd474, %fd473;
	mul.f64 	%fd476, %fd6, %fd1127;
	mul.f64 	%fd477, %fd12, %fd1128;
	sub.f64 	%fd478, %fd477, %fd476;
	mul.f64 	%fd479, %fd9, %fd1128;
	mul.f64 	%fd480, %fd6, %fd1126;
	sub.f64 	%fd481, %fd480, %fd479;
	mul.f64 	%fd482, %fd478, %fd478;
	fma.rn.f64 	%fd483, %fd475, %fd475, %fd482;
	fma.rn.f64 	%fd58, %fd481, %fd481, %fd483;
	setp.geu.f64 	%p39, %fd58, %fd57;
	mov.f64 	%fd1140, 0d3FF0000000000000;
	@%p39 bra 	$L__BB4_47;

	div.rn.f64 	%fd484, %fd58, %fd57;
	mov.f64 	%fd485, 0d0000000000000000;
	sub.f64 	%fd486, %fd485, %fd484;
	add.f64 	%fd487, %fd486, 0d4000000000000000;
	mul.f64 	%fd1140, %fd484, %fd487;

$L__BB4_47:
	setp.neu.f64 	%p40, %fd1140, 0d3FF0000000000000;
	@%p40 bra 	$L__BB4_56;

	mul.lo.s64 	%rd139, %rd55, %rd39;
	add.s64 	%rd140, %rd11, %rd139;
	mul.lo.s64 	%rd141, %rd56, %rd39;
	add.s64 	%rd142, %rd11, %rd141;
	ld.global.f64 	%fd488, [%rd142];
	ld.global.f64 	%fd489, [%rd140];
	add.f64 	%fd61, %fd489, %fd488;
	div.rn.f64 	%fd1141, %fd55, %fd56;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r399}, %fd1141;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r400, %temp}, %fd1141;
	}
	setp.gt.s32 	%p41, %r399, 1048575;
	mov.u32 	%r401, -1023;
	@%p41 bra 	$L__BB4_50;

	mul.f64 	%fd1141, %fd1141, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r399}, %fd1141;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r400, %temp}, %fd1141;
	}
	mov.u32 	%r401, -1077;

$L__BB4_50:
	add.s32 	%r346, %r399, -1;
	setp.lt.u32 	%p42, %r346, 2146435071;
	@%p42 bra 	$L__BB4_52;
	bra.uni 	$L__BB4_51;

$L__BB4_52:
	shr.u32 	%r348, %r399, 20;
	add.s32 	%r402, %r401, %r348;
	and.b32  	%r349, %r399, -2146435073;
	or.b32  	%r350, %r349, 1072693248;
	mov.b64 	%fd1142, {%r400, %r350};
	setp.lt.s32 	%p44, %r350, 1073127583;
	@%p44 bra 	$L__BB4_54;

	{
	.reg .b32 %temp;
	mov.b64 	{%r351, %temp}, %fd1142;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r352}, %fd1142;
	}
	add.s32 	%r353, %r352, -1048576;
	mov.b64 	%fd1142, {%r351, %r353};
	add.s32 	%r402, %r402, 1;

$L__BB4_54:
	add.f64 	%fd492, %fd1142, 0d3FF0000000000000;
	mov.f64 	%fd493, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd494, %fd492;
	neg.f64 	%fd495, %fd492;
	fma.rn.f64 	%fd496, %fd495, %fd494, %fd493;
	fma.rn.f64 	%fd497, %fd496, %fd496, %fd496;
	fma.rn.f64 	%fd498, %fd497, %fd494, %fd494;
	add.f64 	%fd499, %fd1142, 0dBFF0000000000000;
	mul.f64 	%fd500, %fd499, %fd498;
	fma.rn.f64 	%fd501, %fd499, %fd498, %fd500;
	mul.f64 	%fd502, %fd501, %fd501;
	mov.f64 	%fd503, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd504, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd505, %fd504, %fd502, %fd503;
	mov.f64 	%fd506, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd507, %fd505, %fd502, %fd506;
	mov.f64 	%fd508, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd509, %fd507, %fd502, %fd508;
	mov.f64 	%fd510, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd511, %fd509, %fd502, %fd510;
	mov.f64 	%fd512, 0d3F624924923BE72D;
	fma.rn.f64 	%fd513, %fd511, %fd502, %fd512;
	mov.f64 	%fd514, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd515, %fd513, %fd502, %fd514;
	mov.f64 	%fd516, 0d3FB5555555555554;
	fma.rn.f64 	%fd517, %fd515, %fd502, %fd516;
	sub.f64 	%fd518, %fd499, %fd501;
	add.f64 	%fd519, %fd518, %fd518;
	neg.f64 	%fd520, %fd501;
	fma.rn.f64 	%fd521, %fd520, %fd499, %fd519;
	mul.f64 	%fd522, %fd498, %fd521;
	mul.f64 	%fd523, %fd502, %fd517;
	fma.rn.f64 	%fd524, %fd523, %fd501, %fd522;
	xor.b32  	%r354, %r402, -2147483648;
	mov.u32 	%r355, -2147483648;
	mov.u32 	%r356, 1127219200;
	mov.b64 	%fd525, {%r354, %r356};
	mov.b64 	%fd526, {%r355, %r356};
	sub.f64 	%fd527, %fd525, %fd526;
	mov.f64 	%fd528, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd529, %fd527, %fd528, %fd501;
	neg.f64 	%fd530, %fd527;
	fma.rn.f64 	%fd531, %fd530, %fd528, %fd529;
	sub.f64 	%fd532, %fd531, %fd501;
	sub.f64 	%fd533, %fd524, %fd532;
	mov.f64 	%fd534, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd535, %fd527, %fd534, %fd533;
	add.f64 	%fd1143, %fd529, %fd535;
	bra.uni 	$L__BB4_55;

$L__BB4_51:
	mov.f64 	%fd490, 0d7FF0000000000000;
	fma.rn.f64 	%fd491, %fd1141, %fd490, %fd490;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r347}, %fd1141;
	}
	mov.b32 	%f1, %r347;
	setp.eq.f32 	%p43, %f1, 0f00000000;
	selp.f64 	%fd1143, 0dFFF0000000000000, %fd491, %p43;

$L__BB4_55:
	sub.f64 	%fd536, %fd55, %fd56;
	div.rn.f64 	%fd537, %fd536, %fd56;
	mul.f64 	%fd538, %fd537, %fd1143;
	mul.f64 	%fd539, %fd538, 0dC000000000000000;
	div.rn.f64 	%fd540, %fd539, %fd56;
	mul.f64 	%fd541, %fd537, %fd537;
	div.rn.f64 	%fd542, %fd541, %fd55;
	sub.f64 	%fd543, %fd540, %fd542;
	mul.f64 	%fd544, %fd543, %fd295;
	mul.f64 	%fd545, %fd61, %fd296;
	mul.f64 	%fd546, %fd545, %fd294;
	mul.f64 	%fd547, %fd546, %fd544;
	mul.f64 	%fd548, %fd547, 0dC000000000000000;
	sqrt.rn.f64 	%fd549, %fd55;
	fma.rn.f64 	%fd550, %fd549, %fd548, 0d0000000000000000;
	st.global.f64 	[%rd50], %fd550;

$L__BB4_56:
	@%p13 bra 	$L__BB4_60;

	setp.ge.f64 	%p2, %fd31, %fd30;
	add.f64 	%fd71, %fd29, %fd26;
	@%p2 bra 	$L__BB4_59;

	setp.le.f64 	%p153, %fd31, 0d0000000000000000;
	selp.b32 	%r395, 2, 8, %p153;
	sub.f64 	%fd1122, %fd11, %fd20;
	sub.f64 	%fd1121, %fd5, %fd14;
	sub.f64 	%fd1120, %fd8, %fd17;
	sub.f64 	%fd1119, %fd13, %fd14;
	sub.f64 	%fd1118, %fd19, %fd20;
	sub.f64 	%fd1117, %fd16, %fd17;
	selp.f64 	%fd553, %fd27, %fd30, %p2;
	mul.f64 	%fd554, %fd25, %fd29;
	mul.f64 	%fd555, %fd28, %fd26;
	sub.f64 	%fd556, %fd554, %fd555;
	mul.f64 	%fd557, %fd12, %fd1117;
	mul.f64 	%fd558, %fd9, %fd1118;
	sub.f64 	%fd559, %fd558, %fd557;
	mul.f64 	%fd560, %fd6, %fd1118;
	mul.f64 	%fd561, %fd12, %fd1119;
	sub.f64 	%fd562, %fd561, %fd560;
	mul.f64 	%fd563, %fd9, %fd1119;
	mul.f64 	%fd564, %fd6, %fd1117;
	sub.f64 	%fd565, %fd564, %fd563;
	setp.gt.f64 	%p46, %fd556, 0d0000000000000000;
	setp.lt.f64 	%p47, %fd556, %fd553;
	mul.f64 	%fd566, %fd1120, %fd562;
	fma.rn.f64 	%fd567, %fd1121, %fd559, %fd566;
	fma.rn.f64 	%fd568, %fd1122, %fd565, %fd567;
	setp.eq.f64 	%p48, %fd568, 0d0000000000000000;
	mul.f64 	%fd569, %fd562, %fd562;
	fma.rn.f64 	%fd570, %fd559, %fd559, %fd569;
	fma.rn.f64 	%fd571, %fd565, %fd565, %fd570;
	mul.f64 	%fd572, %fd25, 0d3BC79CA100000000;
	mul.f64 	%fd573, %fd572, %fd27;
	setp.lt.f64 	%p49, %fd571, %fd573;
	or.pred  	%p50, %p48, %p49;
	and.pred  	%p51, %p46, %p47;
	and.pred  	%p52, %p50, %p51;
	mul.f64 	%fd574, %fd30, 0d3FE0000000000000;
	setp.lt.f64 	%p53, %fd31, %fd574;
	selp.b32 	%r359, 2, 5, %p53;
	selp.f64 	%fd575, %fd29, %fd71, %p53;
	selp.f64 	%fd1145, %fd27, %fd553, %p52;
	selp.b32 	%r360, 5, %r395, %p2;
	selp.b32 	%r403, %r359, %r360, %p52;
	selp.f64 	%fd1144, %fd575, %fd556, %p52;

$L__BB4_59:
	selp.f64 	%fd1147, %fd27, %fd1145, %p2;
	selp.b32 	%r404, 5, %r403, %p2;
	selp.f64 	%fd1146, %fd71, %fd1144, %p2;

$L__BB4_60:
	selp.f64 	%fd80, %fd27, %fd1147, %p13;
	selp.b32 	%r405, 2, %r404, %p13;
	selp.f64 	%fd81, %fd29, %fd1146, %p13;
	setp.gtu.f64 	%p56, %fd81, 0d0000000000000000;
	@%p56 bra 	$L__BB4_64;
	bra.uni 	$L__BB4_61;

$L__BB4_64:
	setp.ltu.f64 	%p59, %fd81, %fd80;
	@%p59 bra 	$L__BB4_68;

	mov.f64 	%fd577, 0d0000000000000000;
	sub.f64 	%fd578, %fd577, %fd28;
	add.f64 	%fd83, %fd578, %fd26;
	setp.le.f64 	%p60, %fd83, 0d0000000000000000;
	mov.u32 	%r405, 1;
	@%p60 bra 	$L__BB4_68;

	setp.ge.f64 	%p61, %fd83, %fd25;
	mov.u32 	%r405, 4;
	@%p61 bra 	$L__BB4_68;

	mov.u32 	%r405, 7;
	bra.uni 	$L__BB4_68;

$L__BB4_61:
	mov.f64 	%fd576, 0d0000000000000000;
	sub.f64 	%fd82, %fd576, %fd28;
	setp.le.f64 	%p57, %fd82, 0d0000000000000000;
	mov.u32 	%r405, 0;
	@%p57 bra 	$L__BB4_68;

	setp.ge.f64 	%p58, %fd82, %fd25;
	mov.u32 	%r405, 3;
	@%p58 bra 	$L__BB4_68;

	mov.u32 	%r405, 6;

$L__BB4_68:
	setp.eq.s32 	%p62, %r405, 0;
	mov.f64 	%fd1180, 0d0000000000000000;
	mov.f64 	%fd1181, %fd1180;
	@%p62 bra 	$L__BB4_98;

	setp.eq.s32 	%p63, %r405, 1;
	selp.f64 	%fd1177, 0d3FF0000000000000, 0d0000000000000000, %p63;
	mov.f64 	%fd1176, 0d0000000000000000;
	@%p63 bra 	$L__BB4_96;
	bra.uni 	$L__BB4_70;

$L__BB4_96:
	mov.f64 	%fd1178, 0d0000000000000000;
	mov.f64 	%fd1179, %fd1178;
	bra.uni 	$L__BB4_97;

$L__BB4_70:
	setp.ne.s32 	%p64, %r405, 2;
	mov.f64 	%fd1148, 0d0000000000000000;
	@%p64 bra 	$L__BB4_72;

	div.rn.f64 	%fd1148, %fd29, %fd27;

$L__BB4_72:
	setp.eq.s32 	%p65, %r405, 2;
	selp.f64 	%fd1173, %fd1148, %fd1177, %p65;
	selp.f64 	%fd1172, 0d0000000000000000, %fd1176, %p65;
	@%p65 bra 	$L__BB4_94;
	bra.uni 	$L__BB4_73;

$L__BB4_94:
	mov.f64 	%fd1174, 0d0000000000000000;
	mov.f64 	%fd1175, %fd1174;
	bra.uni 	$L__BB4_95;

$L__BB4_73:
	setp.eq.s32 	%p66, %r405, 3;
	selp.f64 	%fd582, 0d3FF0000000000000, 0d0000000000000000, %p66;
	selp.f64 	%fd1168, %fd582, %fd1172, %p66;
	selp.f64 	%fd1169, 0d0000000000000000, %fd1173, %p66;
	@%p66 bra 	$L__BB4_92;
	bra.uni 	$L__BB4_74;

$L__BB4_92:
	mov.f64 	%fd1170, 0d0000000000000000;
	mov.f64 	%fd1171, %fd1170;
	bra.uni 	$L__BB4_93;

$L__BB4_74:
	setp.eq.s32 	%p67, %r405, 4;
	selp.f64 	%fd583, 0d3FF0000000000000, 0d0000000000000000, %p67;
	selp.f64 	%fd1164, %fd583, %fd1168, %p67;
	selp.f64 	%fd1165, %fd583, %fd1169, %p67;
	@%p67 bra 	$L__BB4_90;
	bra.uni 	$L__BB4_75;

$L__BB4_90:
	mov.f64 	%fd1166, 0d0000000000000000;
	mov.f64 	%fd1167, %fd1166;
	bra.uni 	$L__BB4_91;

$L__BB4_75:
	setp.ne.s32 	%p68, %r405, 5;
	mov.f64 	%fd1149, 0d0000000000000000;
	mov.f64 	%fd1150, %fd1149;
	@%p68 bra 	$L__BB4_77;

	sub.f64 	%fd1125, %fd13, %fd14;
	sub.f64 	%fd1124, %fd19, %fd20;
	sub.f64 	%fd1123, %fd16, %fd17;
	sub.f64 	%fd587, %fd4, %fd14;
	sub.f64 	%fd588, %fd7, %fd17;
	mul.f64 	%fd589, %fd588, %fd1123;
	fma.rn.f64 	%fd590, %fd587, %fd1125, %fd589;
	sub.f64 	%fd591, %fd10, %fd20;
	fma.rn.f64 	%fd592, %fd591, %fd1124, %fd590;
	div.rn.f64 	%fd1149, %fd592, %fd27;
	mov.f64 	%fd1150, 0d3FF0000000000000;

$L__BB4_77:
	setp.eq.s32 	%p69, %r405, 5;
	selp.f64 	%fd1160, %fd1150, %fd1164, %p69;
	selp.f64 	%fd1161, %fd1149, %fd1165, %p69;
	@%p69 bra 	$L__BB4_88;
	bra.uni 	$L__BB4_78;

$L__BB4_88:
	mov.f64 	%fd1162, 0d0000000000000000;
	mov.f64 	%fd1163, %fd1162;
	bra.uni 	$L__BB4_89;

$L__BB4_78:
	setp.ne.s32 	%p70, %r405, 6;
	mov.f64 	%fd1151, 0d0000000000000000;
	@%p70 bra 	$L__BB4_80;

	sub.f64 	%fd594, %fd14, %fd5;
	sub.f64 	%fd595, %fd17, %fd8;
	mul.f64 	%fd596, %fd9, %fd595;
	fma.rn.f64 	%fd597, %fd6, %fd594, %fd596;
	sub.f64 	%fd598, %fd20, %fd11;
	fma.rn.f64 	%fd599, %fd12, %fd598, %fd597;
	div.rn.f64 	%fd1151, %fd599, %fd25;

$L__BB4_80:
	setp.eq.s32 	%p71, %r405, 6;
	selp.f64 	%fd1156, %fd1151, %fd1160, %p71;
	selp.f64 	%fd1157, 0d0000000000000000, %fd1161, %p71;
	@%p71 bra 	$L__BB4_86;
	bra.uni 	$L__BB4_81;

$L__BB4_86:
	mov.f64 	%fd1158, 0d0000000000000000;
	mov.f64 	%fd1159, %fd1158;
	bra.uni 	$L__BB4_87;

$L__BB4_81:
	setp.ne.s32 	%p72, %r405, 7;
	mov.f64 	%fd1154, 0d0000000000000000;
	mov.f64 	%fd1152, %fd1154;
	mov.f64 	%fd1153, %fd1154;
	@%p72 bra 	$L__BB4_83;

	sub.f64 	%fd603, %fd13, %fd5;
	sub.f64 	%fd604, %fd16, %fd8;
	mul.f64 	%fd605, %fd9, %fd604;
	fma.rn.f64 	%fd606, %fd6, %fd603, %fd605;
	sub.f64 	%fd607, %fd19, %fd11;
	fma.rn.f64 	%fd608, %fd12, %fd607, %fd606;
	div.rn.f64 	%fd1153, %fd608, %fd25;
	mov.f64 	%fd1152, 0d3FF0000000000000;

$L__BB4_83:
	setp.eq.s32 	%p73, %r405, 7;
	selp.f64 	%fd119, %fd1152, %fd1157, %p73;
	selp.f64 	%fd118, %fd1153, %fd1156, %p73;
	mov.f64 	%fd1155, %fd1154;
	@%p73 bra 	$L__BB4_85;

	mov.f64 	%fd611, 0d0000000000000000;
	sub.f64 	%fd612, %fd611, %fd26;
	div.rn.f64 	%fd613, %fd612, %fd25;
	mul.f64 	%fd614, %fd613, %fd613;
	mul.f64 	%fd615, %fd25, %fd614;
	sub.f64 	%fd616, %fd27, %fd615;
	sub.f64 	%fd617, %fd611, %fd28;
	mul.f64 	%fd618, %fd617, %fd613;
	sub.f64 	%fd619, %fd29, %fd618;
	div.rn.f64 	%fd1154, %fd619, %fd616;
	mul.f64 	%fd620, %fd25, %fd613;
	mul.f64 	%fd621, %fd620, %fd1154;
	sub.f64 	%fd622, %fd617, %fd621;
	div.rn.f64 	%fd1155, %fd622, %fd25;

$L__BB4_85:
	selp.f64 	%fd1159, %fd118, %fd1155, %p73;
	selp.f64 	%fd1158, %fd119, %fd1154, %p73;

$L__BB4_87:
	selp.f64 	%fd1163, %fd1156, %fd1159, %p71;
	selp.f64 	%fd1162, %fd1157, %fd1158, %p71;

$L__BB4_89:
	selp.f64 	%fd1167, %fd1160, %fd1163, %p69;
	selp.f64 	%fd1166, %fd1161, %fd1162, %p69;

$L__BB4_91:
	selp.f64 	%fd1171, %fd1164, %fd1167, %p67;
	selp.f64 	%fd1170, %fd1165, %fd1166, %p67;

$L__BB4_93:
	selp.f64 	%fd1175, %fd1168, %fd1171, %p66;
	selp.f64 	%fd1174, %fd1169, %fd1170, %p66;

$L__BB4_95:
	selp.f64 	%fd1179, %fd1172, %fd1175, %p65;
	selp.f64 	%fd1178, %fd1173, %fd1174, %p65;

$L__BB4_97:
	selp.f64 	%fd1181, %fd1176, %fd1179, %p63;
	selp.f64 	%fd1180, %fd1177, %fd1178, %p63;

$L__BB4_98:
	selp.f64 	%fd635, 0d0000000000000000, %fd1180, %p62;
	selp.f64 	%fd636, 0d0000000000000000, %fd1181, %p62;
	mov.f64 	%fd637, 0d3FF0000000000000;
	sub.f64 	%fd638, %fd637, %fd636;
	mul.f64 	%fd639, %fd5, %fd638;
	mul.f64 	%fd640, %fd8, %fd638;
	mul.f64 	%fd641, %fd11, %fd638;
	fma.rn.f64 	%fd642, %fd4, %fd636, %fd639;
	fma.rn.f64 	%fd643, %fd7, %fd636, %fd640;
	fma.rn.f64 	%fd644, %fd10, %fd636, %fd641;
	sub.f64 	%fd645, %fd637, %fd635;
	mul.f64 	%fd646, %fd14, %fd645;
	mul.f64 	%fd647, %fd17, %fd645;
	mul.f64 	%fd648, %fd20, %fd645;
	fma.rn.f64 	%fd649, %fd13, %fd635, %fd646;
	fma.rn.f64 	%fd650, %fd16, %fd635, %fd647;
	fma.rn.f64 	%fd651, %fd19, %fd635, %fd648;
	sub.f64 	%fd652, %fd642, %fd649;
	sub.f64 	%fd653, %fd643, %fd650;
	sub.f64 	%fd654, %fd644, %fd651;
	mul.f64 	%fd655, %fd653, %fd653;
	fma.rn.f64 	%fd656, %fd652, %fd652, %fd655;
	fma.rn.f64 	%fd657, %fd654, %fd654, %fd656;
	sqrt.rn.f64 	%fd658, %fd657;
	div.rn.f64 	%fd659, %fd652, %fd658;
	div.rn.f64 	%fd660, %fd653, %fd658;
	div.rn.f64 	%fd661, %fd654, %fd658;
	st.global.f64 	[%rd53], %fd636;
	st.global.f64 	[%rd53+8], %fd635;
	st.global.f64 	[%rd54], %fd659;
	st.global.f64 	[%rd54+8], %fd660;
	st.global.f64 	[%rd54+16], %fd661;

$L__BB4_156:
	ld.param.u64 	%rd166, [initialize_friction_collisions_cuda_kernel_forward_param_0+24];
	add.s64 	%rd167, %rd167, %rd32;
	setp.lt.u64 	%p151, %rd167, %rd166;
	@%p151 bra 	$L__BB4_2;

$L__BB4_157:
	ret;

}
	// .globl	initialize_friction_collisions_cuda_kernel_backward
.visible .entry initialize_friction_collisions_cuda_kernel_backward(
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_16[56],
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_17,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_18,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_19,
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_21[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_22[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_23[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_24[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_25[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_26[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_27[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_28[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_29[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_30[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_31[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_32[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_33[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_34[56],
	.param .align 8 .b8 initialize_friction_collisions_cuda_kernel_backward_param_35[56],
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_36,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_37,
	.param .f64 initialize_friction_collisions_cuda_kernel_backward_param_38
)
{
	.reg .pred 	%p<324>;
	.reg .b16 	%rs<367>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<680>;
	.reg .f64 	%fd<6419>;
	.reg .b64 	%rd<374>;


	ld.param.v2.u32 	{%r320, %r321}, [initialize_friction_collisions_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r322, %r323}, [initialize_friction_collisions_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r328, %r329}, [initialize_friction_collisions_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r336, %r337}, [initialize_friction_collisions_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r344, %r345}, [initialize_friction_collisions_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r352, %r353}, [initialize_friction_collisions_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r360, %r361}, [initialize_friction_collisions_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r368, %r369}, [initialize_friction_collisions_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r376, %r377}, [initialize_friction_collisions_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r384, %r385}, [initialize_friction_collisions_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r392, %r393}, [initialize_friction_collisions_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r400, %r401}, [initialize_friction_collisions_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r408, %r409}, [initialize_friction_collisions_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r416, %r417}, [initialize_friction_collisions_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r424, %r425}, [initialize_friction_collisions_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r432, %r433}, [initialize_friction_collisions_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r440, %r441}, [initialize_friction_collisions_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r448, %r449}, [initialize_friction_collisions_cuda_kernel_backward_param_16+32];
	ld.param.f64 	%fd1736, [initialize_friction_collisions_cuda_kernel_backward_param_17];
	ld.param.f64 	%fd1737, [initialize_friction_collisions_cuda_kernel_backward_param_18];
	ld.param.f64 	%fd1738, [initialize_friction_collisions_cuda_kernel_backward_param_19];
	ld.param.v2.u32 	{%r456, %r457}, [initialize_friction_collisions_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r464, %r465}, [initialize_friction_collisions_cuda_kernel_backward_param_21+32];
	ld.param.v2.u32 	{%r472, %r473}, [initialize_friction_collisions_cuda_kernel_backward_param_22+32];
	ld.param.v2.u32 	{%r480, %r481}, [initialize_friction_collisions_cuda_kernel_backward_param_26+32];
	ld.param.v2.u32 	{%r488, %r489}, [initialize_friction_collisions_cuda_kernel_backward_param_28+32];
	ld.param.v2.u32 	{%r496, %r497}, [initialize_friction_collisions_cuda_kernel_backward_param_29+32];
	ld.param.v2.u32 	{%r504, %r505}, [initialize_friction_collisions_cuda_kernel_backward_param_30+32];
	ld.param.v2.u32 	{%r512, %r513}, [initialize_friction_collisions_cuda_kernel_backward_param_31+32];
	ld.param.v2.u32 	{%r520, %r521}, [initialize_friction_collisions_cuda_kernel_backward_param_32+32];
	ld.param.v2.u32 	{%r528, %r529}, [initialize_friction_collisions_cuda_kernel_backward_param_33+32];
	ld.param.u64 	%rd153, [initialize_friction_collisions_cuda_kernel_backward_param_33];
	ld.param.u64 	%rd151, [initialize_friction_collisions_cuda_kernel_backward_param_32];
	ld.param.u64 	%rd149, [initialize_friction_collisions_cuda_kernel_backward_param_31];
	ld.param.u64 	%rd147, [initialize_friction_collisions_cuda_kernel_backward_param_30];
	ld.param.u64 	%rd145, [initialize_friction_collisions_cuda_kernel_backward_param_29];
	ld.param.u64 	%rd143, [initialize_friction_collisions_cuda_kernel_backward_param_28];
	ld.param.u64 	%rd141, [initialize_friction_collisions_cuda_kernel_backward_param_26];
	ld.param.u64 	%rd139, [initialize_friction_collisions_cuda_kernel_backward_param_22];
	ld.param.u64 	%rd137, [initialize_friction_collisions_cuda_kernel_backward_param_21];
	ld.param.u64 	%rd135, [initialize_friction_collisions_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd133, [initialize_friction_collisions_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd131, [initialize_friction_collisions_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd130, [initialize_friction_collisions_cuda_kernel_backward_param_14+8];
	ld.param.u64 	%rd129, [initialize_friction_collisions_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd128, [initialize_friction_collisions_cuda_kernel_backward_param_13+8];
	ld.param.u64 	%rd127, [initialize_friction_collisions_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd126, [initialize_friction_collisions_cuda_kernel_backward_param_12+8];
	ld.param.u64 	%rd125, [initialize_friction_collisions_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd124, [initialize_friction_collisions_cuda_kernel_backward_param_11+8];
	ld.param.u64 	%rd123, [initialize_friction_collisions_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd122, [initialize_friction_collisions_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd121, [initialize_friction_collisions_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd120, [initialize_friction_collisions_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd119, [initialize_friction_collisions_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd117, [initialize_friction_collisions_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd116, [initialize_friction_collisions_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd115, [initialize_friction_collisions_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd113, [initialize_friction_collisions_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd111, [initialize_friction_collisions_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd109, [initialize_friction_collisions_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd108, [initialize_friction_collisions_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd106, [initialize_friction_collisions_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd104, [initialize_friction_collisions_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd102, [initialize_friction_collisions_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r85, [initialize_friction_collisions_cuda_kernel_backward_param_0+16];
	mov.u32 	%r532, %ntid.x;
	mov.u32 	%r533, %ctaid.x;
	mul.wide.u32 	%rd155, %r532, %r533;
	mov.u32 	%r534, %tid.x;
	cvt.u64.u32 	%rd156, %r534;
	add.s64 	%rd370, %rd155, %rd156;
	setp.ge.u64 	%p22, %rd370, %rd102;
	@%p22 bra 	$L__BB5_369;

	cvta.to.global.u64 	%rd23, %rd115;
	cvta.to.global.u64 	%rd24, %rd111;
	cvta.to.global.u64 	%rd25, %rd109;
	cvta.to.global.u64 	%rd26, %rd133;
	cvta.to.global.u64 	%rd27, %rd131;
	cvta.to.global.u64 	%rd28, %rd129;
	cvta.to.global.u64 	%rd29, %rd127;
	cvta.to.global.u64 	%rd30, %rd125;
	cvta.to.global.u64 	%rd31, %rd123;
	cvta.to.global.u64 	%rd32, %rd121;
	cvta.to.global.u64 	%rd33, %rd119;
	cvta.to.global.u64 	%rd34, %rd117;
	cvta.to.global.u64 	%rd35, %rd113;
	cvt.s64.s32 	%rd36, %r323;
	cvt.s64.s32 	%rd37, %r322;
	cvt.s64.s32 	%rd38, %r321;
	cvt.s64.s32 	%rd39, %r368;
	cvt.s64.s32 	%rd40, %r360;
	cvt.s64.s32 	%rd41, %r352;
	cvt.s64.s32 	%rd42, %r448;
	cvt.s64.s32 	%rd43, %r400;
	cvt.s64.s32 	%rd44, %r416;
	cvt.s64.s32 	%rd45, %r384;
	cvt.s64.s32 	%rd46, %r376;
	cvt.s64.s32 	%rd47, %r440;
	cvt.s64.s32 	%rd48, %r408;
	cvt.s64.s32 	%rd49, %r472;
	mul.f64 	%fd1, %fd1736, %fd1736;
	add.f64 	%fd2, %fd1736, %fd1736;
	cvt.s64.s32 	%rd50, %r344;
	cvt.s64.s32 	%rd51, %r464;
	cvt.s64.s32 	%rd52, %r424;
	cvt.s64.s32 	%rd53, %r336;
	cvt.s64.s32 	%rd54, %r392;
	cvt.s64.s32 	%rd55, %r432;
	cvt.s64.s32 	%rd56, %r456;
	cvt.s64.s32 	%rd57, %r328;
	cvt.s64.s32 	%rd58, %r480;
	cvt.s64.s32 	%rd59, %r520;
	cvt.s64.s32 	%rd60, %r512;
	cvt.s64.s32 	%rd61, %r496;
	cvt.s64.s32 	%rd62, %r488;
	cvt.s64.s32 	%rd63, %r528;
	cvt.s64.s32 	%rd64, %r504;

$L__BB5_2:
	setp.lt.s32 	%p23, %r85, 4;
	mov.u64 	%rd371, %rd370;
	@%p23 bra 	$L__BB5_6;

	or.b64  	%rd157, %rd370, %rd36;
	and.b64  	%rd158, %rd157, -4294967296;
	setp.eq.s64 	%p24, %rd158, 0;
	@%p24 bra 	$L__BB5_5;

	div.u64 	%rd371, %rd370, %rd36;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r536, %rd36;
	cvt.u32.u64 	%r537, %rd370;
	div.u32 	%r538, %r537, %r536;
	cvt.u64.u32 	%rd371, %r538;

$L__BB5_6:
	setp.lt.s32 	%p25, %r85, 3;
	@%p25 bra 	$L__BB5_10;

	or.b64  	%rd159, %rd371, %rd37;
	and.b64  	%rd160, %rd159, -4294967296;
	setp.eq.s64 	%p26, %rd160, 0;
	@%p26 bra 	$L__BB5_9;

	div.u64 	%rd371, %rd371, %rd37;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r539, %rd37;
	cvt.u32.u64 	%r540, %rd371;
	div.u32 	%r541, %r540, %r539;
	cvt.u64.u32 	%rd371, %r541;

$L__BB5_10:
	setp.lt.s32 	%p27, %r85, 2;
	@%p27 bra 	$L__BB5_14;

	or.b64  	%rd161, %rd371, %rd38;
	and.b64  	%rd162, %rd161, -4294967296;
	setp.eq.s64 	%p28, %rd162, 0;
	@%p28 bra 	$L__BB5_13;

	div.u64 	%rd371, %rd371, %rd38;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r542, %rd38;
	cvt.u32.u64 	%r543, %rd371;
	div.u32 	%r544, %r543, %r542;
	cvt.u64.u32 	%rd371, %r544;

$L__BB5_14:
	cvt.s64.s32 	%rd163, %rd371;
	setp.gt.s32 	%p29, %r85, 0;
	selp.b64 	%rd75, %rd163, 0, %p29;
	mul.lo.s64 	%rd164, %rd75, %rd39;
	add.s64 	%rd165, %rd35, %rd164;
	ld.global.u32 	%r6, [%rd165];
	setp.lt.u32 	%p30, %r6, 2;
	mul.lo.s64 	%rd166, %rd75, %rd41;
	add.s64 	%rd76, %rd25, %rd166;
	@%p30 bra 	$L__BB5_218;
	bra.uni 	$L__BB5_15;

$L__BB5_218:
	mul.lo.s64 	%rd291, %rd75, %rd40;
	add.s64 	%rd292, %rd24, %rd291;
	ld.global.u32 	%r598, [%rd76];
	ld.global.u32 	%r599, [%rd292];
	setp.eq.s32 	%p180, %r6, 1;
	selp.b32 	%r600, %r598, %r599, %p180;
	selp.b32 	%r601, %r599, %r598, %p180;
	cvt.s64.s32 	%rd89, %r601;
	mul.lo.s64 	%rd293, %rd89, %rd42;
	add.s64 	%rd294, %rd26, %rd293;
	cvt.s64.s32 	%rd90, %r600;
	mul.lo.s64 	%rd91, %rd90, %rd43;
	add.s64 	%rd295, %rd32, %rd91;
	mul.lo.s64 	%rd92, %rd89, %rd44;
	add.s64 	%rd296, %rd30, %rd92;
	ld.global.f64 	%fd3445, [%rd296];
	ld.global.f64 	%fd3446, [%rd295];
	add.f64 	%fd943, %fd3446, %fd3445;
	mul.lo.s64 	%rd297, %rd90, %rd45;
	add.s64 	%rd298, %rd34, %rd297;
	ld.global.s32 	%rd93, [%rd298];
	mul.lo.s64 	%rd94, %rd93, %rd46;
	add.s64 	%rd299, %rd23, %rd94;
	ld.global.s32 	%rd95, [%rd294];
	mul.lo.s64 	%rd96, %rd95, %rd46;
	add.s64 	%rd300, %rd23, %rd96;
	ld.global.s32 	%rd97, [%rd294+4];
	mul.lo.s64 	%rd98, %rd97, %rd46;
	add.s64 	%rd301, %rd23, %rd98;
	ld.global.s32 	%rd99, [%rd294+8];
	mul.lo.s64 	%rd100, %rd99, %rd46;
	add.s64 	%rd302, %rd23, %rd100;
	ld.global.f64 	%fd944, [%rd301];
	ld.global.f64 	%fd945, [%rd300];
	sub.f64 	%fd946, %fd944, %fd945;
	ld.global.f64 	%fd947, [%rd301+8];
	ld.global.f64 	%fd948, [%rd300+8];
	sub.f64 	%fd949, %fd947, %fd948;
	ld.global.f64 	%fd950, [%rd301+16];
	ld.global.f64 	%fd951, [%rd300+16];
	sub.f64 	%fd952, %fd950, %fd951;
	ld.global.f64 	%fd953, [%rd302];
	sub.f64 	%fd954, %fd953, %fd945;
	ld.global.f64 	%fd955, [%rd302+8];
	sub.f64 	%fd956, %fd955, %fd948;
	ld.global.f64 	%fd957, [%rd302+16];
	sub.f64 	%fd958, %fd957, %fd951;
	mul.f64 	%fd3447, %fd949, %fd958;
	mul.f64 	%fd3448, %fd952, %fd956;
	sub.f64 	%fd959, %fd3447, %fd3448;
	mul.f64 	%fd3449, %fd952, %fd954;
	mul.f64 	%fd3450, %fd946, %fd958;
	sub.f64 	%fd960, %fd3449, %fd3450;
	mul.f64 	%fd3451, %fd946, %fd956;
	mul.f64 	%fd3452, %fd949, %fd954;
	sub.f64 	%fd961, %fd3451, %fd3452;
	mul.f64 	%fd3453, %fd949, %fd961;
	mul.f64 	%fd3454, %fd952, %fd960;
	sub.f64 	%fd962, %fd3453, %fd3454;
	mul.f64 	%fd3455, %fd952, %fd959;
	mul.f64 	%fd3456, %fd946, %fd961;
	sub.f64 	%fd963, %fd3455, %fd3456;
	mul.f64 	%fd3457, %fd946, %fd960;
	mul.f64 	%fd3458, %fd949, %fd959;
	sub.f64 	%fd964, %fd3457, %fd3458;
	mul.f64 	%fd3459, %fd949, %fd949;
	fma.rn.f64 	%fd3460, %fd946, %fd946, %fd3459;
	fma.rn.f64 	%fd965, %fd952, %fd952, %fd3460;
	mul.f64 	%fd3461, %fd949, %fd963;
	fma.rn.f64 	%fd3462, %fd946, %fd962, %fd3461;
	fma.rn.f64 	%fd3463, %fd952, %fd964, %fd3462;
	mul.f64 	%fd3464, %fd963, %fd963;
	fma.rn.f64 	%fd3465, %fd962, %fd962, %fd3464;
	fma.rn.f64 	%fd3466, %fd964, %fd964, %fd3465;
	ld.global.f64 	%fd966, [%rd299];
	sub.f64 	%fd967, %fd966, %fd945;
	ld.global.f64 	%fd968, [%rd299+8];
	sub.f64 	%fd969, %fd968, %fd948;
	ld.global.f64 	%fd970, [%rd299+16];
	sub.f64 	%fd971, %fd970, %fd951;
	mul.f64 	%fd3467, %fd969, %fd949;
	fma.rn.f64 	%fd3468, %fd967, %fd946, %fd3467;
	fma.rn.f64 	%fd972, %fd971, %fd952, %fd3468;
	mul.f64 	%fd3469, %fd969, %fd963;
	fma.rn.f64 	%fd3470, %fd967, %fd962, %fd3469;
	fma.rn.f64 	%fd3471, %fd971, %fd964, %fd3470;
	div.rn.f64 	%fd973, %fd3463, %fd965;
	mul.f64 	%fd974, %fd973, %fd973;
	mul.f64 	%fd3472, %fd965, %fd974;
	sub.f64 	%fd975, %fd3466, %fd3472;
	mul.f64 	%fd3473, %fd972, %fd973;
	sub.f64 	%fd3474, %fd3471, %fd3473;
	div.rn.f64 	%fd976, %fd3474, %fd975;
	mul.f64 	%fd977, %fd965, %fd973;
	mul.f64 	%fd3475, %fd977, %fd976;
	sub.f64 	%fd3476, %fd972, %fd3475;
	div.rn.f64 	%fd978, %fd3476, %fd965;
	setp.gt.f64 	%p181, %fd978, 0d0000000000000000;
	setp.lt.f64 	%p182, %fd978, 0d3FF0000000000000;
	setp.ge.f64 	%p183, %fd976, 0d0000000000000000;
	and.pred  	%p184, %p181, %p182;
	and.pred  	%p15, %p183, %p184;
	mov.u16 	%rs348, 3;
	@%p15 bra 	$L__BB5_224;

	sub.f64 	%fd3477, %fd953, %fd944;
	sub.f64 	%fd3478, %fd955, %fd947;
	mul.f64 	%fd3479, %fd3478, %fd961;
	sub.f64 	%fd3480, %fd957, %fd950;
	mul.f64 	%fd3481, %fd3480, %fd960;
	sub.f64 	%fd3482, %fd3479, %fd3481;
	mul.f64 	%fd3483, %fd3480, %fd959;
	mul.f64 	%fd3484, %fd3477, %fd961;
	sub.f64 	%fd3485, %fd3483, %fd3484;
	mul.f64 	%fd3486, %fd3477, %fd960;
	mul.f64 	%fd3487, %fd3478, %fd959;
	sub.f64 	%fd3488, %fd3486, %fd3487;
	mul.f64 	%fd3489, %fd3478, %fd3478;
	fma.rn.f64 	%fd3490, %fd3477, %fd3477, %fd3489;
	fma.rn.f64 	%fd3491, %fd3480, %fd3480, %fd3490;
	mul.f64 	%fd3492, %fd3478, %fd3485;
	fma.rn.f64 	%fd3493, %fd3477, %fd3482, %fd3492;
	fma.rn.f64 	%fd3494, %fd3480, %fd3488, %fd3493;
	mul.f64 	%fd3495, %fd3485, %fd3485;
	fma.rn.f64 	%fd3496, %fd3482, %fd3482, %fd3495;
	fma.rn.f64 	%fd3497, %fd3488, %fd3488, %fd3496;
	sub.f64 	%fd3498, %fd966, %fd944;
	sub.f64 	%fd3499, %fd968, %fd947;
	mul.f64 	%fd3500, %fd3499, %fd3478;
	fma.rn.f64 	%fd3501, %fd3498, %fd3477, %fd3500;
	sub.f64 	%fd3502, %fd970, %fd950;
	fma.rn.f64 	%fd3503, %fd3502, %fd3480, %fd3501;
	mul.f64 	%fd3504, %fd3499, %fd3485;
	fma.rn.f64 	%fd3505, %fd3498, %fd3482, %fd3504;
	fma.rn.f64 	%fd3506, %fd3502, %fd3488, %fd3505;
	div.rn.f64 	%fd3507, %fd3494, %fd3491;
	mul.f64 	%fd3508, %fd3507, %fd3507;
	mul.f64 	%fd3509, %fd3491, %fd3508;
	sub.f64 	%fd3510, %fd3497, %fd3509;
	mul.f64 	%fd3511, %fd3503, %fd3507;
	sub.f64 	%fd3512, %fd3506, %fd3511;
	div.rn.f64 	%fd3513, %fd3512, %fd3510;
	mul.f64 	%fd3514, %fd3491, %fd3507;
	mul.f64 	%fd3515, %fd3514, %fd3513;
	sub.f64 	%fd3516, %fd3503, %fd3515;
	div.rn.f64 	%fd979, %fd3516, %fd3491;
	setp.gt.f64 	%p185, %fd979, 0d0000000000000000;
	setp.lt.f64 	%p186, %fd979, 0d3FF0000000000000;
	setp.ge.f64 	%p187, %fd3513, 0d0000000000000000;
	and.pred  	%p188, %p185, %p186;
	and.pred  	%p189, %p187, %p188;
	mov.u16 	%rs348, 4;
	@%p189 bra 	$L__BB5_224;

	sub.f64 	%fd3517, %fd945, %fd953;
	sub.f64 	%fd3518, %fd948, %fd955;
	mul.f64 	%fd3519, %fd3518, %fd961;
	sub.f64 	%fd3520, %fd951, %fd957;
	mul.f64 	%fd3521, %fd3520, %fd960;
	sub.f64 	%fd3522, %fd3519, %fd3521;
	mul.f64 	%fd3523, %fd3520, %fd959;
	mul.f64 	%fd3524, %fd3517, %fd961;
	sub.f64 	%fd3525, %fd3523, %fd3524;
	mul.f64 	%fd3526, %fd3517, %fd960;
	mul.f64 	%fd3527, %fd3518, %fd959;
	sub.f64 	%fd3528, %fd3526, %fd3527;
	mul.f64 	%fd3529, %fd3518, %fd3518;
	fma.rn.f64 	%fd3530, %fd3517, %fd3517, %fd3529;
	fma.rn.f64 	%fd3531, %fd3520, %fd3520, %fd3530;
	mul.f64 	%fd3532, %fd3518, %fd3525;
	fma.rn.f64 	%fd3533, %fd3517, %fd3522, %fd3532;
	fma.rn.f64 	%fd3534, %fd3520, %fd3528, %fd3533;
	mul.f64 	%fd3535, %fd3525, %fd3525;
	fma.rn.f64 	%fd3536, %fd3522, %fd3522, %fd3535;
	fma.rn.f64 	%fd3537, %fd3528, %fd3528, %fd3536;
	sub.f64 	%fd3538, %fd966, %fd953;
	sub.f64 	%fd3539, %fd968, %fd955;
	mul.f64 	%fd3540, %fd3518, %fd3539;
	fma.rn.f64 	%fd3541, %fd3517, %fd3538, %fd3540;
	sub.f64 	%fd3542, %fd970, %fd957;
	fma.rn.f64 	%fd3543, %fd3520, %fd3542, %fd3541;
	mul.f64 	%fd3544, %fd3539, %fd3525;
	fma.rn.f64 	%fd3545, %fd3538, %fd3522, %fd3544;
	fma.rn.f64 	%fd3546, %fd3542, %fd3528, %fd3545;
	div.rn.f64 	%fd3547, %fd3534, %fd3531;
	mul.f64 	%fd3548, %fd3547, %fd3547;
	mul.f64 	%fd3549, %fd3531, %fd3548;
	sub.f64 	%fd3550, %fd3537, %fd3549;
	mul.f64 	%fd3551, %fd3543, %fd3547;
	sub.f64 	%fd3552, %fd3546, %fd3551;
	div.rn.f64 	%fd3553, %fd3552, %fd3550;
	mul.f64 	%fd3554, %fd3531, %fd3547;
	mul.f64 	%fd3555, %fd3554, %fd3553;
	sub.f64 	%fd3556, %fd3543, %fd3555;
	div.rn.f64 	%fd980, %fd3556, %fd3531;
	setp.gt.f64 	%p190, %fd980, 0d0000000000000000;
	setp.lt.f64 	%p191, %fd980, 0d3FF0000000000000;
	setp.ge.f64 	%p192, %fd3553, 0d0000000000000000;
	and.pred  	%p193, %p190, %p191;
	and.pred  	%p194, %p192, %p193;
	mov.u16 	%rs348, 5;
	@%p194 bra 	$L__BB5_224;

	setp.le.f64 	%p195, %fd978, 0d0000000000000000;
	setp.ge.f64 	%p196, %fd980, 0d3FF0000000000000;
	and.pred  	%p197, %p195, %p196;
	mov.u16 	%rs348, 0;
	@%p197 bra 	$L__BB5_224;

	setp.le.f64 	%p198, %fd979, 0d0000000000000000;
	setp.ge.f64 	%p199, %fd978, 0d3FF0000000000000;
	and.pred  	%p200, %p198, %p199;
	mov.u16 	%rs348, 1;
	@%p200 bra 	$L__BB5_224;

	setp.le.f64 	%p201, %fd980, 0d0000000000000000;
	setp.ge.f64 	%p202, %fd979, 0d3FF0000000000000;
	and.pred  	%p203, %p201, %p202;
	selp.b16 	%rs348, 2, 6, %p203;

$L__BB5_224:
	mov.f64 	%fd6041, 0d4415AF1D80000000;
	setp.gt.s16 	%p204, %rs348, 2;
	@%p204 bra 	$L__BB5_229;
	bra.uni 	$L__BB5_225;

$L__BB5_229:
	setp.gt.s16 	%p205, %rs348, 4;
	@%p205 bra 	$L__BB5_233;

	setp.eq.s16 	%p208, %rs348, 3;
	@%p208 bra 	$L__BB5_237;

	setp.eq.s16 	%p209, %rs348, 4;
	@%p209 bra 	$L__BB5_232;
	bra.uni 	$L__BB5_240;

$L__BB5_232:
	sub.f64 	%fd3589, %fd944, %fd966;
	sub.f64 	%fd3590, %fd957, %fd970;
	sub.f64 	%fd3591, %fd947, %fd968;
	mul.f64 	%fd3592, %fd3591, %fd3590;
	sub.f64 	%fd3593, %fd955, %fd968;
	sub.f64 	%fd3594, %fd950, %fd970;
	mul.f64 	%fd3595, %fd3594, %fd3593;
	sub.f64 	%fd3596, %fd3592, %fd3595;
	sub.f64 	%fd3597, %fd953, %fd966;
	mul.f64 	%fd3598, %fd3594, %fd3597;
	mul.f64 	%fd3599, %fd3589, %fd3590;
	sub.f64 	%fd3600, %fd3598, %fd3599;
	mul.f64 	%fd3601, %fd3589, %fd3593;
	mul.f64 	%fd3602, %fd3591, %fd3597;
	sub.f64 	%fd3603, %fd3601, %fd3602;
	mul.f64 	%fd3604, %fd3600, %fd3600;
	fma.rn.f64 	%fd3605, %fd3596, %fd3596, %fd3604;
	fma.rn.f64 	%fd3606, %fd3603, %fd3603, %fd3605;
	sub.f64 	%fd3607, %fd953, %fd944;
	sub.f64 	%fd3608, %fd955, %fd947;
	mul.f64 	%fd3609, %fd3608, %fd3608;
	fma.rn.f64 	%fd3610, %fd3607, %fd3607, %fd3609;
	sub.f64 	%fd3611, %fd957, %fd950;
	fma.rn.f64 	%fd3612, %fd3611, %fd3611, %fd3610;
	div.rn.f64 	%fd6041, %fd3606, %fd3612;
	bra.uni 	$L__BB5_240;

$L__BB5_15:
	cvt.s64.s32 	%rd363, %r408;
	mul.lo.s64 	%rd167, %rd75, %rd40;
	add.s64 	%rd168, %rd24, %rd167;
	ld.global.s32 	%rd77, [%rd76];
	mul.lo.s64 	%rd169, %rd77, %rd47;
	add.s64 	%rd170, %rd27, %rd169;
	ld.global.s32 	%rd78, [%rd168];
	mul.lo.s64 	%rd171, %rd78, %rd47;
	add.s64 	%rd172, %rd27, %rd171;
	mul.lo.s64 	%rd79, %rd77, %rd363;
	add.s64 	%rd173, %rd31, %rd79;
	mul.lo.s64 	%rd80, %rd78, %rd363;
	add.s64 	%rd174, %rd31, %rd80;
	ld.global.f64 	%fd1741, [%rd174];
	ld.global.f64 	%fd1742, [%rd173];
	add.f64 	%fd12, %fd1742, %fd1741;
	ld.global.u32 	%r7, [%rd170];
	cvt.s64.s32 	%rd81, %r7;
	mul.lo.s64 	%rd82, %rd81, %rd46;
	add.s64 	%rd175, %rd23, %rd82;
	ld.global.u32 	%r8, [%rd170+4];
	cvt.s64.s32 	%rd83, %r8;
	mul.lo.s64 	%rd84, %rd83, %rd46;
	add.s64 	%rd176, %rd23, %rd84;
	ld.global.u32 	%r9, [%rd172];
	cvt.s64.s32 	%rd85, %r9;
	mul.lo.s64 	%rd86, %rd85, %rd46;
	add.s64 	%rd177, %rd23, %rd86;
	ld.global.u32 	%r10, [%rd172+4];
	cvt.s64.s32 	%rd87, %r10;
	mul.lo.s64 	%rd88, %rd87, %rd46;
	add.s64 	%rd178, %rd23, %rd88;
	ld.global.f64 	%fd13, [%rd176];
	ld.global.f64 	%fd14, [%rd175];
	sub.f64 	%fd15, %fd13, %fd14;
	ld.global.f64 	%fd16, [%rd176+8];
	ld.global.f64 	%fd17, [%rd175+8];
	sub.f64 	%fd18, %fd16, %fd17;
	ld.global.f64 	%fd19, [%rd176+16];
	ld.global.f64 	%fd20, [%rd175+16];
	sub.f64 	%fd21, %fd19, %fd20;
	ld.global.f64 	%fd22, [%rd178];
	ld.global.f64 	%fd23, [%rd177];
	sub.f64 	%fd24, %fd22, %fd23;
	ld.global.f64 	%fd25, [%rd178+8];
	ld.global.f64 	%fd26, [%rd177+8];
	sub.f64 	%fd27, %fd25, %fd26;
	ld.global.f64 	%fd28, [%rd178+16];
	ld.global.f64 	%fd29, [%rd177+16];
	sub.f64 	%fd30, %fd28, %fd29;
	sub.f64 	%fd31, %fd14, %fd23;
	sub.f64 	%fd32, %fd17, %fd26;
	sub.f64 	%fd33, %fd20, %fd29;
	mul.f64 	%fd1743, %fd18, %fd18;
	fma.rn.f64 	%fd1744, %fd15, %fd15, %fd1743;
	fma.rn.f64 	%fd34, %fd21, %fd21, %fd1744;
	mul.f64 	%fd1745, %fd18, %fd27;
	fma.rn.f64 	%fd1746, %fd15, %fd24, %fd1745;
	fma.rn.f64 	%fd35, %fd21, %fd30, %fd1746;
	mul.f64 	%fd1747, %fd27, %fd27;
	fma.rn.f64 	%fd1748, %fd24, %fd24, %fd1747;
	fma.rn.f64 	%fd36, %fd30, %fd30, %fd1748;
	mul.f64 	%fd1749, %fd18, %fd32;
	fma.rn.f64 	%fd1750, %fd15, %fd31, %fd1749;
	fma.rn.f64 	%fd37, %fd21, %fd33, %fd1750;
	mul.f64 	%fd1751, %fd32, %fd27;
	fma.rn.f64 	%fd1752, %fd31, %fd24, %fd1751;
	fma.rn.f64 	%fd38, %fd33, %fd30, %fd1752;
	mul.f64 	%fd1753, %fd34, %fd36;
	mul.f64 	%fd1754, %fd35, %fd35;
	sub.f64 	%fd39, %fd1753, %fd1754;
	mul.f64 	%fd1755, %fd35, %fd38;
	mul.f64 	%fd1756, %fd37, %fd36;
	sub.f64 	%fd40, %fd1755, %fd1756;
	setp.le.f64 	%p31, %fd40, 0d0000000000000000;
	@%p31 bra 	$L__BB5_19;

	setp.ge.f64 	%p1, %fd40, %fd39;
	add.f64 	%fd41, %fd38, %fd35;
	@%p1 bra 	$L__BB5_18;

	selp.f64 	%fd1758, %fd36, %fd39, %p1;
	mul.f64 	%fd1759, %fd34, %fd38;
	mul.f64 	%fd1760, %fd37, %fd35;
	sub.f64 	%fd1761, %fd1759, %fd1760;
	mul.f64 	%fd1762, %fd21, %fd27;
	mul.f64 	%fd1763, %fd18, %fd30;
	sub.f64 	%fd1764, %fd1763, %fd1762;
	mul.f64 	%fd1765, %fd15, %fd30;
	mul.f64 	%fd1766, %fd21, %fd24;
	sub.f64 	%fd1767, %fd1766, %fd1765;
	mul.f64 	%fd1768, %fd18, %fd24;
	mul.f64 	%fd1769, %fd15, %fd27;
	sub.f64 	%fd1770, %fd1769, %fd1768;
	setp.gt.f64 	%p32, %fd1761, 0d0000000000000000;
	setp.lt.f64 	%p33, %fd1761, %fd1758;
	mul.f64 	%fd1771, %fd32, %fd1767;
	fma.rn.f64 	%fd1772, %fd31, %fd1764, %fd1771;
	fma.rn.f64 	%fd1773, %fd33, %fd1770, %fd1772;
	setp.eq.f64 	%p34, %fd1773, 0d0000000000000000;
	mul.f64 	%fd1774, %fd1767, %fd1767;
	fma.rn.f64 	%fd1775, %fd1764, %fd1764, %fd1774;
	fma.rn.f64 	%fd1776, %fd1770, %fd1770, %fd1775;
	mul.f64 	%fd1777, %fd34, 0d3BC79CA100000000;
	mul.f64 	%fd1778, %fd1777, %fd36;
	setp.lt.f64 	%p35, %fd1776, %fd1778;
	or.pred  	%p36, %p34, %p35;
	and.pred  	%p37, %p32, %p33;
	and.pred  	%p38, %p36, %p37;
	mul.f64 	%fd1779, %fd39, 0d3FE0000000000000;
	setp.lt.f64 	%p39, %fd40, %fd1779;
	selp.b32 	%r547, 2, 5, %p39;
	selp.f64 	%fd1780, %fd38, %fd41, %p39;
	selp.f64 	%fd5576, %fd36, %fd1758, %p38;
	selp.b32 	%r645, %r547, 8, %p38;
	selp.f64 	%fd5577, %fd1780, %fd1761, %p38;

$L__BB5_18:
	selp.f64 	%fd5578, %fd36, %fd5576, %p1;
	selp.b32 	%r646, 5, %r645, %p1;
	selp.f64 	%fd5579, %fd41, %fd5577, %p1;

$L__BB5_19:
	selp.f64 	%fd50, %fd36, %fd5578, %p31;
	selp.b32 	%r647, 2, %r646, %p31;
	selp.f64 	%fd51, %fd38, %fd5579, %p31;
	setp.gtu.f64 	%p42, %fd51, 0d0000000000000000;
	@%p42 bra 	$L__BB5_23;
	bra.uni 	$L__BB5_20;

$L__BB5_23:
	setp.ltu.f64 	%p45, %fd51, %fd50;
	@%p45 bra 	$L__BB5_27;

	mov.f64 	%fd1782, 0d0000000000000000;
	sub.f64 	%fd1783, %fd1782, %fd37;
	add.f64 	%fd53, %fd1783, %fd35;
	setp.le.f64 	%p46, %fd53, 0d0000000000000000;
	mov.u32 	%r647, 1;
	@%p46 bra 	$L__BB5_27;

	setp.ge.f64 	%p47, %fd53, %fd34;
	mov.u32 	%r647, 4;
	@%p47 bra 	$L__BB5_27;

	mov.u32 	%r647, 7;
	bra.uni 	$L__BB5_27;

$L__BB5_20:
	mov.f64 	%fd1781, 0d0000000000000000;
	sub.f64 	%fd52, %fd1781, %fd37;
	setp.le.f64 	%p43, %fd52, 0d0000000000000000;
	mov.u32 	%r647, 0;
	@%p43 bra 	$L__BB5_27;

	setp.ge.f64 	%p44, %fd52, %fd34;
	mov.u32 	%r647, 3;
	@%p44 bra 	$L__BB5_27;

	mov.u32 	%r647, 6;

$L__BB5_27:
	setp.eq.s32 	%p48, %r647, 0;
	@%p48 bra 	$L__BB5_43;

	setp.eq.s32 	%p49, %r647, 1;
	@%p49 bra 	$L__BB5_42;
	bra.uni 	$L__BB5_29;

$L__BB5_42:
	sub.f64 	%fd1882, %fd14, %fd22;
	sub.f64 	%fd1883, %fd17, %fd25;
	mul.f64 	%fd1884, %fd1883, %fd1883;
	fma.rn.f64 	%fd1885, %fd1882, %fd1882, %fd1884;
	sub.f64 	%fd1886, %fd20, %fd28;
	fma.rn.f64 	%fd5580, %fd1886, %fd1886, %fd1885;
	bra.uni 	$L__BB5_44;

$L__BB5_225:
	setp.eq.s16 	%p210, %rs348, 0;
	@%p210 bra 	$L__BB5_239;

	setp.eq.s16 	%p211, %rs348, 1;
	@%p211 bra 	$L__BB5_238;

	setp.eq.s16 	%p212, %rs348, 2;
	@%p212 bra 	$L__BB5_228;
	bra.uni 	$L__BB5_240;

$L__BB5_228:
	sub.f64 	%fd3631, %fd966, %fd953;
	sub.f64 	%fd3632, %fd968, %fd955;
	mul.f64 	%fd3633, %fd3632, %fd3632;
	fma.rn.f64 	%fd3634, %fd3631, %fd3631, %fd3633;
	sub.f64 	%fd3635, %fd970, %fd957;
	fma.rn.f64 	%fd6041, %fd3635, %fd3635, %fd3634;
	bra.uni 	$L__BB5_240;

$L__BB5_43:
	mul.f64 	%fd1887, %fd32, %fd32;
	fma.rn.f64 	%fd1888, %fd31, %fd31, %fd1887;
	fma.rn.f64 	%fd5580, %fd33, %fd33, %fd1888;
	bra.uni 	$L__BB5_44;

$L__BB5_29:
	setp.eq.s32 	%p50, %r647, 2;
	@%p50 bra 	$L__BB5_41;
	bra.uni 	$L__BB5_30;

$L__BB5_41:
	sub.f64 	%fd1864, %fd23, %fd14;
	sub.f64 	%fd1865, %fd28, %fd20;
	sub.f64 	%fd1866, %fd26, %fd17;
	mul.f64 	%fd1867, %fd1866, %fd1865;
	sub.f64 	%fd1868, %fd25, %fd17;
	sub.f64 	%fd1869, %fd29, %fd20;
	mul.f64 	%fd1870, %fd1869, %fd1868;
	sub.f64 	%fd1871, %fd1867, %fd1870;
	sub.f64 	%fd1872, %fd22, %fd14;
	mul.f64 	%fd1873, %fd1869, %fd1872;
	mul.f64 	%fd1874, %fd1864, %fd1865;
	sub.f64 	%fd1875, %fd1873, %fd1874;
	mul.f64 	%fd1876, %fd1864, %fd1868;
	mul.f64 	%fd1877, %fd1866, %fd1872;
	sub.f64 	%fd1878, %fd1876, %fd1877;
	mul.f64 	%fd1879, %fd1875, %fd1875;
	fma.rn.f64 	%fd1880, %fd1871, %fd1871, %fd1879;
	fma.rn.f64 	%fd1881, %fd1878, %fd1878, %fd1880;
	div.rn.f64 	%fd5580, %fd1881, %fd36;
	bra.uni 	$L__BB5_44;

$L__BB5_233:
	setp.eq.s16 	%p206, %rs348, 5;
	@%p206 bra 	$L__BB5_236;

	setp.ne.s16 	%p207, %rs348, 6;
	@%p207 bra 	$L__BB5_240;

	mul.f64 	%fd3558, %fd969, %fd960;
	fma.rn.f64 	%fd3559, %fd967, %fd959, %fd3558;
	fma.rn.f64 	%fd3560, %fd971, %fd961, %fd3559;
	mul.f64 	%fd3561, %fd3560, %fd3560;
	mul.f64 	%fd3562, %fd960, %fd960;
	fma.rn.f64 	%fd3563, %fd959, %fd959, %fd3562;
	fma.rn.f64 	%fd3564, %fd961, %fd961, %fd3563;
	div.rn.f64 	%fd6041, %fd3561, %fd3564;
	bra.uni 	$L__BB5_240;

$L__BB5_30:
	setp.eq.s32 	%p51, %r647, 3;
	@%p51 bra 	$L__BB5_40;
	bra.uni 	$L__BB5_31;

$L__BB5_40:
	sub.f64 	%fd1859, %fd13, %fd23;
	sub.f64 	%fd1860, %fd16, %fd26;
	mul.f64 	%fd1861, %fd1860, %fd1860;
	fma.rn.f64 	%fd1862, %fd1859, %fd1859, %fd1861;
	sub.f64 	%fd1863, %fd19, %fd29;
	fma.rn.f64 	%fd5580, %fd1863, %fd1863, %fd1862;
	bra.uni 	$L__BB5_44;

$L__BB5_237:
	sub.f64 	%fd3613, %fd945, %fd966;
	sub.f64 	%fd3614, %fd950, %fd970;
	sub.f64 	%fd3615, %fd948, %fd968;
	mul.f64 	%fd3616, %fd3615, %fd3614;
	sub.f64 	%fd3617, %fd947, %fd968;
	sub.f64 	%fd3618, %fd951, %fd970;
	mul.f64 	%fd3619, %fd3618, %fd3617;
	sub.f64 	%fd3620, %fd3616, %fd3619;
	sub.f64 	%fd3621, %fd944, %fd966;
	mul.f64 	%fd3622, %fd3618, %fd3621;
	mul.f64 	%fd3623, %fd3613, %fd3614;
	sub.f64 	%fd3624, %fd3622, %fd3623;
	mul.f64 	%fd3625, %fd3613, %fd3617;
	mul.f64 	%fd3626, %fd3615, %fd3621;
	sub.f64 	%fd3627, %fd3625, %fd3626;
	mul.f64 	%fd3628, %fd3624, %fd3624;
	fma.rn.f64 	%fd3629, %fd3620, %fd3620, %fd3628;
	fma.rn.f64 	%fd3630, %fd3627, %fd3627, %fd3629;
	div.rn.f64 	%fd6041, %fd3630, %fd965;
	bra.uni 	$L__BB5_240;

$L__BB5_239:
	mul.f64 	%fd3641, %fd969, %fd969;
	fma.rn.f64 	%fd3642, %fd967, %fd967, %fd3641;
	fma.rn.f64 	%fd6041, %fd971, %fd971, %fd3642;
	bra.uni 	$L__BB5_240;

$L__BB5_238:
	sub.f64 	%fd3636, %fd966, %fd944;
	sub.f64 	%fd3637, %fd968, %fd947;
	mul.f64 	%fd3638, %fd3637, %fd3637;
	fma.rn.f64 	%fd3639, %fd3636, %fd3636, %fd3638;
	sub.f64 	%fd3640, %fd970, %fd950;
	fma.rn.f64 	%fd6041, %fd3640, %fd3640, %fd3639;
	bra.uni 	$L__BB5_240;

$L__BB5_236:
	sub.f64 	%fd3565, %fd953, %fd966;
	sub.f64 	%fd3566, %fd951, %fd970;
	sub.f64 	%fd3567, %fd955, %fd968;
	mul.f64 	%fd3568, %fd3566, %fd3567;
	sub.f64 	%fd3569, %fd948, %fd968;
	sub.f64 	%fd3570, %fd957, %fd970;
	mul.f64 	%fd3571, %fd3569, %fd3570;
	sub.f64 	%fd3572, %fd3568, %fd3571;
	sub.f64 	%fd3573, %fd945, %fd966;
	mul.f64 	%fd3574, %fd3573, %fd3570;
	mul.f64 	%fd3575, %fd3566, %fd3565;
	sub.f64 	%fd3576, %fd3574, %fd3575;
	mul.f64 	%fd3577, %fd3569, %fd3565;
	mul.f64 	%fd3578, %fd3573, %fd3567;
	sub.f64 	%fd3579, %fd3577, %fd3578;
	mul.f64 	%fd3580, %fd3576, %fd3576;
	fma.rn.f64 	%fd3581, %fd3572, %fd3572, %fd3580;
	fma.rn.f64 	%fd3582, %fd3579, %fd3579, %fd3581;
	sub.f64 	%fd3583, %fd945, %fd953;
	sub.f64 	%fd3584, %fd948, %fd955;
	mul.f64 	%fd3585, %fd3584, %fd3584;
	fma.rn.f64 	%fd3586, %fd3583, %fd3583, %fd3585;
	sub.f64 	%fd3587, %fd951, %fd957;
	fma.rn.f64 	%fd3588, %fd3587, %fd3587, %fd3586;
	div.rn.f64 	%fd6041, %fd3582, %fd3588;

$L__BB5_240:
	mul.f64 	%fd3643, %fd943, %fd943;
	sub.f64 	%fd989, %fd6041, %fd3643;
	fma.rn.f64 	%fd990, %fd2, %fd943, %fd1;
	setp.geu.f64 	%p213, %fd989, %fd990;
	mov.u16 	%rs349, 0;
	@%p213 bra 	$L__BB5_249;

	mul.lo.s64 	%rd303, %rd90, %rd52;
	add.s64 	%rd304, %rd29, %rd303;
	ld.global.f64 	%fd3644, [%rd304];
	mul.f64 	%fd3645, %fd3644, %fd1738;
	mul.f64 	%fd6415, %fd3645, %fd1736;
	div.rn.f64 	%fd6042, %fd989, %fd990;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r666}, %fd6042;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r667, %temp}, %fd6042;
	}
	setp.gt.s32 	%p214, %r666, 1048575;
	mov.u32 	%r668, -1023;
	@%p214 bra 	$L__BB5_243;

	mul.f64 	%fd6042, %fd6042, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r666}, %fd6042;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r667, %temp}, %fd6042;
	}
	mov.u32 	%r668, -1077;

$L__BB5_243:
	add.s32 	%r604, %r666, -1;
	setp.lt.u32 	%p215, %r604, 2146435071;
	@%p215 bra 	$L__BB5_245;
	bra.uni 	$L__BB5_244;

$L__BB5_245:
	shr.u32 	%r606, %r666, 20;
	add.s32 	%r669, %r668, %r606;
	and.b32  	%r607, %r666, -2146435073;
	or.b32  	%r608, %r607, 1072693248;
	mov.b64 	%fd6043, {%r667, %r608};
	setp.lt.s32 	%p217, %r608, 1073127583;
	@%p217 bra 	$L__BB5_247;

	{
	.reg .b32 %temp;
	mov.b64 	{%r609, %temp}, %fd6043;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r610}, %fd6043;
	}
	add.s32 	%r611, %r610, -1048576;
	mov.b64 	%fd6043, {%r609, %r611};
	add.s32 	%r669, %r669, 1;

$L__BB5_247:
	add.f64 	%fd3648, %fd6043, 0d3FF0000000000000;
	mov.f64 	%fd3649, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd3650, %fd3648;
	neg.f64 	%fd3651, %fd3648;
	fma.rn.f64 	%fd3652, %fd3651, %fd3650, %fd3649;
	fma.rn.f64 	%fd3653, %fd3652, %fd3652, %fd3652;
	fma.rn.f64 	%fd3654, %fd3653, %fd3650, %fd3650;
	add.f64 	%fd3655, %fd6043, 0dBFF0000000000000;
	mul.f64 	%fd3656, %fd3655, %fd3654;
	fma.rn.f64 	%fd3657, %fd3655, %fd3654, %fd3656;
	mul.f64 	%fd3658, %fd3657, %fd3657;
	mov.f64 	%fd3659, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd3660, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd3661, %fd3660, %fd3658, %fd3659;
	mov.f64 	%fd3662, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd3663, %fd3661, %fd3658, %fd3662;
	mov.f64 	%fd3664, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd3665, %fd3663, %fd3658, %fd3664;
	mov.f64 	%fd3666, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd3667, %fd3665, %fd3658, %fd3666;
	mov.f64 	%fd3668, 0d3F624924923BE72D;
	fma.rn.f64 	%fd3669, %fd3667, %fd3658, %fd3668;
	mov.f64 	%fd3670, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd3671, %fd3669, %fd3658, %fd3670;
	mov.f64 	%fd3672, 0d3FB5555555555554;
	fma.rn.f64 	%fd3673, %fd3671, %fd3658, %fd3672;
	sub.f64 	%fd3674, %fd3655, %fd3657;
	add.f64 	%fd3675, %fd3674, %fd3674;
	neg.f64 	%fd3676, %fd3657;
	fma.rn.f64 	%fd3677, %fd3676, %fd3655, %fd3675;
	mul.f64 	%fd3678, %fd3654, %fd3677;
	mul.f64 	%fd3679, %fd3658, %fd3673;
	fma.rn.f64 	%fd3680, %fd3679, %fd3657, %fd3678;
	xor.b32  	%r612, %r669, -2147483648;
	mov.u32 	%r613, -2147483648;
	mov.u32 	%r614, 1127219200;
	mov.b64 	%fd3681, {%r612, %r614};
	mov.b64 	%fd3682, {%r613, %r614};
	sub.f64 	%fd3683, %fd3681, %fd3682;
	mov.f64 	%fd3684, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd3685, %fd3683, %fd3684, %fd3657;
	neg.f64 	%fd3686, %fd3683;
	fma.rn.f64 	%fd3687, %fd3686, %fd3684, %fd3685;
	sub.f64 	%fd3688, %fd3687, %fd3657;
	sub.f64 	%fd3689, %fd3680, %fd3688;
	mov.f64 	%fd3690, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd3691, %fd3683, %fd3690, %fd3689;
	add.f64 	%fd6044, %fd3685, %fd3691;
	bra.uni 	$L__BB5_248;

$L__BB5_244:
	mov.f64 	%fd3646, 0d7FF0000000000000;
	fma.rn.f64 	%fd3647, %fd6042, %fd3646, %fd3646;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r605}, %fd6042;
	}
	mov.b32 	%f3, %r605;
	setp.eq.f32 	%p216, %f3, 0f00000000;
	selp.f64 	%fd6044, 0dFFF0000000000000, %fd3647, %p216;

$L__BB5_248:
	sub.f64 	%fd3692, %fd989, %fd990;
	div.rn.f64 	%fd3693, %fd3692, %fd990;
	mul.f64 	%fd3694, %fd3693, %fd6044;
	mul.f64 	%fd3695, %fd3694, 0dC000000000000000;
	div.rn.f64 	%fd3696, %fd3695, %fd990;
	mul.f64 	%fd3697, %fd3693, %fd3693;
	div.rn.f64 	%fd3698, %fd3697, %fd989;
	sub.f64 	%fd3699, %fd3696, %fd3698;
	mul.f64 	%fd6416, %fd3699, %fd1737;
	mul.f64 	%fd3700, %fd6415, %fd6416;
	fma.rn.f64 	%fd6417, %fd6415, %fd6416, %fd3700;
	sqrt.rn.f64 	%fd6418, %fd989;
	mov.u16 	%rs349, 1;

$L__BB5_249:
	setp.eq.s64 	%p218, %rd139, 0;
	@%p218 bra 	$L__BB5_251;

	cvta.to.global.u64 	%rd305, %rd139;
	mul.lo.s64 	%rd306, %rd75, %rd49;
	add.s64 	%rd307, %rd305, %rd306;
	ld.global.f64 	%fd3701, [%rd307];
	add.f64 	%fd6051, %fd3701, 0d0000000000000000;
	ld.global.f64 	%fd3702, [%rd307+8];
	add.f64 	%fd6050, %fd3702, 0d0000000000000000;
	ld.global.f64 	%fd3703, [%rd307+16];
	add.f64 	%fd6049, %fd3703, 0d0000000000000000;
	bra.uni 	$L__BB5_253;

$L__BB5_251:
	setp.eq.s64 	%p219, %rd108, 0;
	mov.f64 	%fd6049, 0d0000000000000000;
	mov.f64 	%fd6050, %fd6049;
	mov.f64 	%fd6051, %fd6049;
	@%p219 bra 	$L__BB5_253;

	cvta.to.global.u64 	%rd308, %rd108;
	mul.lo.s64 	%rd309, %rd75, %rd50;
	add.s64 	%rd310, %rd308, %rd309;
	ld.global.f64 	%fd3707, [%rd310];
	add.f64 	%fd6051, %fd3707, 0d0000000000000000;
	ld.global.f64 	%fd3708, [%rd310+8];
	add.f64 	%fd6050, %fd3708, 0d0000000000000000;
	ld.global.f64 	%fd3709, [%rd310+16];
	add.f64 	%fd6049, %fd3709, 0d0000000000000000;

$L__BB5_253:
	setp.eq.s64 	%p220, %rd137, 0;
	@%p220 bra 	$L__BB5_255;

	cvta.to.global.u64 	%rd311, %rd137;
	mul.lo.s64 	%rd312, %rd75, %rd51;
	add.s64 	%rd313, %rd311, %rd312;
	ld.global.f64 	%fd3710, [%rd313];
	add.f64 	%fd6053, %fd3710, 0d0000000000000000;
	ld.global.f64 	%fd3711, [%rd313+8];
	add.f64 	%fd6052, %fd3711, 0d0000000000000000;
	bra.uni 	$L__BB5_257;

$L__BB5_255:
	setp.eq.s64 	%p221, %rd106, 0;
	mov.f64 	%fd6052, 0d0000000000000000;
	mov.f64 	%fd6053, %fd6052;
	@%p221 bra 	$L__BB5_257;

	cvta.to.global.u64 	%rd314, %rd106;
	mul.lo.s64 	%rd315, %rd75, %rd53;
	add.s64 	%rd316, %rd314, %rd315;
	ld.global.f64 	%fd3714, [%rd316];
	add.f64 	%fd6053, %fd3714, 0d0000000000000000;
	ld.global.f64 	%fd3715, [%rd316+8];
	add.f64 	%fd6052, %fd3715, 0d0000000000000000;

$L__BB5_257:
	mov.u32 	%r670, 3;
	@%p15 bra 	$L__BB5_263;

	sub.f64 	%fd3716, %fd953, %fd944;
	sub.f64 	%fd3717, %fd955, %fd947;
	mul.f64 	%fd3718, %fd3717, %fd961;
	sub.f64 	%fd3719, %fd957, %fd950;
	mul.f64 	%fd3720, %fd3719, %fd960;
	sub.f64 	%fd3721, %fd3718, %fd3720;
	mul.f64 	%fd3722, %fd3719, %fd959;
	mul.f64 	%fd3723, %fd3716, %fd961;
	sub.f64 	%fd3724, %fd3722, %fd3723;
	mul.f64 	%fd3725, %fd3716, %fd960;
	mul.f64 	%fd3726, %fd3717, %fd959;
	sub.f64 	%fd3727, %fd3725, %fd3726;
	mul.f64 	%fd3728, %fd3716, %fd3716;
	fma.rn.f64 	%fd3729, %fd3717, %fd3717, %fd3728;
	fma.rn.f64 	%fd3730, %fd3719, %fd3719, %fd3729;
	mul.f64 	%fd3731, %fd3717, %fd3724;
	fma.rn.f64 	%fd3732, %fd3716, %fd3721, %fd3731;
	fma.rn.f64 	%fd3733, %fd3719, %fd3727, %fd3732;
	mul.f64 	%fd3734, %fd3724, %fd3724;
	fma.rn.f64 	%fd3735, %fd3721, %fd3721, %fd3734;
	fma.rn.f64 	%fd3736, %fd3727, %fd3727, %fd3735;
	sub.f64 	%fd3737, %fd966, %fd944;
	mul.f64 	%fd3738, %fd3737, %fd3716;
	sub.f64 	%fd3739, %fd968, %fd947;
	fma.rn.f64 	%fd3740, %fd3739, %fd3717, %fd3738;
	sub.f64 	%fd3741, %fd970, %fd950;
	fma.rn.f64 	%fd3742, %fd3741, %fd3719, %fd3740;
	mul.f64 	%fd3743, %fd3739, %fd3724;
	fma.rn.f64 	%fd3744, %fd3737, %fd3721, %fd3743;
	fma.rn.f64 	%fd3745, %fd3741, %fd3727, %fd3744;
	div.rn.f64 	%fd3746, %fd3733, %fd3730;
	mul.f64 	%fd3747, %fd3746, %fd3746;
	mul.f64 	%fd3748, %fd3730, %fd3747;
	sub.f64 	%fd3749, %fd3736, %fd3748;
	mul.f64 	%fd3750, %fd3742, %fd3746;
	sub.f64 	%fd3751, %fd3745, %fd3750;
	div.rn.f64 	%fd3752, %fd3751, %fd3749;
	mul.f64 	%fd3753, %fd3730, %fd3746;
	mul.f64 	%fd3754, %fd3753, %fd3752;
	sub.f64 	%fd3755, %fd3742, %fd3754;
	div.rn.f64 	%fd1023, %fd3755, %fd3730;
	setp.gt.f64 	%p222, %fd1023, 0d0000000000000000;
	setp.lt.f64 	%p223, %fd1023, 0d3FF0000000000000;
	setp.ge.f64 	%p224, %fd3752, 0d0000000000000000;
	and.pred  	%p225, %p222, %p223;
	and.pred  	%p226, %p224, %p225;
	mov.u32 	%r670, 4;
	@%p226 bra 	$L__BB5_263;

	sub.f64 	%fd3756, %fd945, %fd953;
	sub.f64 	%fd3757, %fd948, %fd955;
	mul.f64 	%fd3758, %fd3757, %fd961;
	sub.f64 	%fd3759, %fd951, %fd957;
	mul.f64 	%fd3760, %fd3759, %fd960;
	sub.f64 	%fd3761, %fd3758, %fd3760;
	mul.f64 	%fd3762, %fd3759, %fd959;
	mul.f64 	%fd3763, %fd3756, %fd961;
	sub.f64 	%fd3764, %fd3762, %fd3763;
	mul.f64 	%fd3765, %fd3756, %fd960;
	mul.f64 	%fd3766, %fd3757, %fd959;
	sub.f64 	%fd3767, %fd3765, %fd3766;
	mul.f64 	%fd3768, %fd3756, %fd3756;
	fma.rn.f64 	%fd3769, %fd3757, %fd3757, %fd3768;
	fma.rn.f64 	%fd3770, %fd3759, %fd3759, %fd3769;
	mul.f64 	%fd3771, %fd3757, %fd3764;
	fma.rn.f64 	%fd3772, %fd3756, %fd3761, %fd3771;
	fma.rn.f64 	%fd3773, %fd3759, %fd3767, %fd3772;
	mul.f64 	%fd3774, %fd3764, %fd3764;
	fma.rn.f64 	%fd3775, %fd3761, %fd3761, %fd3774;
	fma.rn.f64 	%fd3776, %fd3767, %fd3767, %fd3775;
	sub.f64 	%fd3777, %fd966, %fd953;
	mul.f64 	%fd3778, %fd3756, %fd3777;
	sub.f64 	%fd3779, %fd968, %fd955;
	fma.rn.f64 	%fd3780, %fd3757, %fd3779, %fd3778;
	sub.f64 	%fd3781, %fd970, %fd957;
	fma.rn.f64 	%fd3782, %fd3759, %fd3781, %fd3780;
	mul.f64 	%fd3783, %fd3779, %fd3764;
	fma.rn.f64 	%fd3784, %fd3777, %fd3761, %fd3783;
	fma.rn.f64 	%fd3785, %fd3781, %fd3767, %fd3784;
	div.rn.f64 	%fd3786, %fd3773, %fd3770;
	mul.f64 	%fd3787, %fd3786, %fd3786;
	mul.f64 	%fd3788, %fd3770, %fd3787;
	sub.f64 	%fd3789, %fd3776, %fd3788;
	mul.f64 	%fd3790, %fd3782, %fd3786;
	sub.f64 	%fd3791, %fd3785, %fd3790;
	div.rn.f64 	%fd3792, %fd3791, %fd3789;
	mul.f64 	%fd3793, %fd3770, %fd3786;
	mul.f64 	%fd3794, %fd3793, %fd3792;
	sub.f64 	%fd3795, %fd3782, %fd3794;
	div.rn.f64 	%fd1024, %fd3795, %fd3770;
	setp.gt.f64 	%p227, %fd1024, 0d0000000000000000;
	setp.lt.f64 	%p228, %fd1024, 0d3FF0000000000000;
	setp.ge.f64 	%p229, %fd3792, 0d0000000000000000;
	and.pred  	%p230, %p227, %p228;
	and.pred  	%p231, %p229, %p230;
	mov.u32 	%r670, 5;
	@%p231 bra 	$L__BB5_263;

	setp.le.f64 	%p232, %fd978, 0d0000000000000000;
	setp.ge.f64 	%p233, %fd1024, 0d3FF0000000000000;
	and.pred  	%p234, %p232, %p233;
	mov.u32 	%r670, 0;
	@%p234 bra 	$L__BB5_263;

	setp.le.f64 	%p235, %fd1023, 0d0000000000000000;
	setp.ge.f64 	%p236, %fd978, 0d3FF0000000000000;
	and.pred  	%p237, %p235, %p236;
	mov.u32 	%r670, 1;
	@%p237 bra 	$L__BB5_263;

	setp.le.f64 	%p238, %fd1024, 0d0000000000000000;
	setp.ge.f64 	%p239, %fd1023, 0d3FF0000000000000;
	and.pred  	%p240, %p238, %p239;
	selp.b32 	%r670, 2, 6, %p240;

$L__BB5_263:
	setp.eq.s32 	%p241, %r670, 0;
	mov.f64 	%fd6153, 0d0000000000000000;
	mov.f64 	%fd6154, %fd6153;
	mov.f64 	%fd6155, %fd6153;
	mov.f64 	%fd6156, %fd6153;
	mov.f64 	%fd6157, %fd6153;
	mov.f64 	%fd6158, %fd6153;
	mov.f64 	%fd6121, %fd6153;
	mov.f64 	%fd6122, %fd6153;
	mov.f64 	%fd6123, %fd6153;
	mov.f64 	%fd6124, %fd6153;
	mov.f64 	%fd6125, %fd6153;
	mov.f64 	%fd6126, %fd6153;
	mov.f64 	%fd6097, %fd6153;
	mov.f64 	%fd6098, %fd6153;
	mov.f64 	%fd6099, %fd6153;
	mov.f64 	%fd6100, %fd6153;
	mov.f64 	%fd6101, %fd6153;
	mov.f64 	%fd6102, %fd6153;
	mov.f64 	%fd6103, %fd6153;
	mov.f64 	%fd6104, %fd6153;
	mov.f64 	%fd6105, %fd6153;
	mov.f64 	%fd6106, %fd6153;
	mov.f64 	%fd6107, %fd6153;
	mov.f64 	%fd6108, %fd6153;
	mov.f64 	%fd6109, %fd6153;
	mov.f64 	%fd6110, %fd6153;
	mov.f64 	%fd6111, %fd6153;
	mov.f64 	%fd6256, %fd6153;
	mov.f64 	%fd6257, %fd6153;
	@%p241 bra 	$L__BB5_285;

	setp.eq.s32 	%p16, %r670, 1;
	selp.f64 	%fd6190, 0d3FF0000000000000, 0d0000000000000000, %p16;
	mov.f64 	%fd6191, 0d0000000000000000;
	@%p16 bra 	$L__BB5_283;
	bra.uni 	$L__BB5_265;

$L__BB5_283:
	mov.f64 	%fd6153, 0d0000000000000000;
	mov.f64 	%fd6154, %fd6153;
	mov.f64 	%fd6155, %fd6153;
	mov.f64 	%fd6156, %fd6153;
	mov.f64 	%fd6157, %fd6153;
	mov.f64 	%fd6158, %fd6153;
	mov.f64 	%fd6121, %fd6153;
	mov.f64 	%fd6122, %fd6153;
	mov.f64 	%fd6123, %fd6153;
	mov.f64 	%fd6124, %fd6153;
	mov.f64 	%fd6125, %fd6153;
	mov.f64 	%fd6126, %fd6153;
	mov.f64 	%fd6097, %fd6153;
	mov.f64 	%fd6098, %fd6153;
	mov.f64 	%fd6099, %fd6153;
	mov.f64 	%fd6100, %fd6153;
	mov.f64 	%fd6101, %fd6153;
	mov.f64 	%fd6102, %fd6153;
	mov.f64 	%fd6103, %fd6153;
	mov.f64 	%fd6104, %fd6153;
	mov.f64 	%fd6105, %fd6153;
	mov.f64 	%fd6106, %fd6153;
	mov.f64 	%fd6107, %fd6153;
	mov.f64 	%fd6108, %fd6153;
	mov.f64 	%fd6109, %fd6153;
	mov.f64 	%fd6110, %fd6153;
	mov.f64 	%fd6111, %fd6153;
	mov.f64 	%fd6219, %fd6153;
	mov.f64 	%fd6220, %fd6153;
	bra.uni 	$L__BB5_284;

$L__BB5_265:
	setp.eq.s32 	%p17, %r670, 2;
	selp.f64 	%fd3826, 0d3FF0000000000000, 0d0000000000000000, %p17;
	selp.f64 	%fd6152, %fd3826, %fd6191, %p17;
	selp.f64 	%fd6151, 0d0000000000000000, %fd6190, %p17;
	@%p17 bra 	$L__BB5_281;
	bra.uni 	$L__BB5_266;

$L__BB5_281:
	mov.f64 	%fd6153, 0d0000000000000000;
	mov.f64 	%fd6154, %fd6153;
	mov.f64 	%fd6155, %fd6153;
	mov.f64 	%fd6156, %fd6153;
	mov.f64 	%fd6157, %fd6153;
	mov.f64 	%fd6158, %fd6153;
	mov.f64 	%fd6121, %fd6153;
	mov.f64 	%fd6122, %fd6153;
	mov.f64 	%fd6123, %fd6153;
	mov.f64 	%fd6124, %fd6153;
	mov.f64 	%fd6125, %fd6153;
	mov.f64 	%fd6126, %fd6153;
	mov.f64 	%fd6097, %fd6153;
	mov.f64 	%fd6098, %fd6153;
	mov.f64 	%fd6099, %fd6153;
	mov.f64 	%fd6100, %fd6153;
	mov.f64 	%fd6101, %fd6153;
	mov.f64 	%fd6102, %fd6153;
	mov.f64 	%fd6103, %fd6153;
	mov.f64 	%fd6104, %fd6153;
	mov.f64 	%fd6105, %fd6153;
	mov.f64 	%fd6106, %fd6153;
	mov.f64 	%fd6107, %fd6153;
	mov.f64 	%fd6108, %fd6153;
	mov.f64 	%fd6109, %fd6153;
	mov.f64 	%fd6110, %fd6153;
	mov.f64 	%fd6111, %fd6153;
	mov.f64 	%fd6180, %fd6153;
	mov.f64 	%fd6181, %fd6153;
	bra.uni 	$L__BB5_282;

$L__BB5_266:
	setp.eq.s32 	%p18, %r670, 3;
	setp.ne.s32 	%p242, %r670, 3;
	mov.f64 	%fd6153, 0d0000000000000000;
	mov.f64 	%fd6154, %fd6153;
	mov.f64 	%fd6155, %fd6153;
	mov.f64 	%fd6156, %fd6153;
	mov.f64 	%fd6157, %fd6153;
	mov.f64 	%fd6158, %fd6153;
	mov.f64 	%fd6182, %fd6153;
	@%p242 bra 	$L__BB5_268;

	div.rn.f64 	%fd6182, %fd972, %fd965;
	mov.f64 	%fd6153, %fd971;
	mov.f64 	%fd6154, %fd969;
	mov.f64 	%fd6155, %fd967;
	mov.f64 	%fd6156, %fd952;
	mov.f64 	%fd6157, %fd949;
	mov.f64 	%fd6158, %fd946;

$L__BB5_268:
	selp.f64 	%fd6119, %fd6182, %fd6151, %p18;
	selp.f64 	%fd6120, 0d0000000000000000, %fd6152, %p18;
	@%p18 bra 	$L__BB5_279;
	bra.uni 	$L__BB5_269;

$L__BB5_279:
	mov.f64 	%fd6121, 0d0000000000000000;
	mov.f64 	%fd6122, %fd6121;
	mov.f64 	%fd6123, %fd6121;
	mov.f64 	%fd6124, %fd6121;
	mov.f64 	%fd6125, %fd6121;
	mov.f64 	%fd6126, %fd6121;
	mov.f64 	%fd6097, %fd6121;
	mov.f64 	%fd6098, %fd6121;
	mov.f64 	%fd6099, %fd6121;
	mov.f64 	%fd6100, %fd6121;
	mov.f64 	%fd6101, %fd6121;
	mov.f64 	%fd6102, %fd6121;
	mov.f64 	%fd6103, %fd6121;
	mov.f64 	%fd6104, %fd6121;
	mov.f64 	%fd6105, %fd6121;
	mov.f64 	%fd6106, %fd6121;
	mov.f64 	%fd6107, %fd6121;
	mov.f64 	%fd6108, %fd6121;
	mov.f64 	%fd6109, %fd6121;
	mov.f64 	%fd6110, %fd6121;
	mov.f64 	%fd6111, %fd6121;
	mov.f64 	%fd6142, %fd6121;
	mov.f64 	%fd6143, %fd6121;
	bra.uni 	$L__BB5_280;

$L__BB5_31:
	setp.eq.s32 	%p52, %r647, 4;
	@%p52 bra 	$L__BB5_39;
	bra.uni 	$L__BB5_32;

$L__BB5_39:
	sub.f64 	%fd1854, %fd13, %fd22;
	sub.f64 	%fd1855, %fd16, %fd25;
	mul.f64 	%fd1856, %fd1855, %fd1855;
	fma.rn.f64 	%fd1857, %fd1854, %fd1854, %fd1856;
	sub.f64 	%fd1858, %fd19, %fd28;
	fma.rn.f64 	%fd5580, %fd1858, %fd1858, %fd1857;
	bra.uni 	$L__BB5_44;

$L__BB5_269:
	setp.eq.s32 	%p19, %r670, 4;
	setp.ne.s32 	%p244, %r670, 4;
	mov.f64 	%fd6121, 0d0000000000000000;
	mov.f64 	%fd6122, %fd6121;
	mov.f64 	%fd6123, %fd6121;
	mov.f64 	%fd6124, %fd6121;
	mov.f64 	%fd6125, %fd6121;
	mov.f64 	%fd6126, %fd6121;
	mov.f64 	%fd6145, %fd6121;
	mov.f64 	%fd6069, %fd6121;
	@%p244 bra 	$L__BB5_271;

	sub.f64 	%fd6123, %fd966, %fd944;
	sub.f64 	%fd6126, %fd953, %fd944;
	mul.f64 	%fd3844, %fd6123, %fd6126;
	sub.f64 	%fd6125, %fd955, %fd947;
	sub.f64 	%fd6122, %fd968, %fd947;
	fma.rn.f64 	%fd3845, %fd6122, %fd6125, %fd3844;
	sub.f64 	%fd6124, %fd957, %fd950;
	sub.f64 	%fd6121, %fd970, %fd950;
	fma.rn.f64 	%fd3846, %fd6121, %fd6124, %fd3845;
	mul.f64 	%fd3847, %fd6126, %fd6126;
	fma.rn.f64 	%fd3848, %fd6125, %fd6125, %fd3847;
	fma.rn.f64 	%fd6144, %fd6124, %fd6124, %fd3848;
	div.rn.f64 	%fd6145, %fd3846, %fd6144;
	mov.f64 	%fd3849, 0d3FF0000000000000;
	sub.f64 	%fd6069, %fd3849, %fd6145;

$L__BB5_271:
	selp.f64 	%fd6095, %fd6069, %fd6119, %p19;
	selp.f64 	%fd6096, %fd6145, %fd6120, %p19;
	@%p19 bra 	$L__BB5_277;
	bra.uni 	$L__BB5_272;

$L__BB5_277:
	mov.f64 	%fd6097, 0d0000000000000000;
	mov.f64 	%fd6098, %fd6097;
	mov.f64 	%fd6099, %fd6097;
	mov.f64 	%fd6100, %fd6097;
	mov.f64 	%fd6101, %fd6097;
	mov.f64 	%fd6102, %fd6097;
	mov.f64 	%fd6103, %fd6097;
	mov.f64 	%fd6104, %fd6097;
	mov.f64 	%fd6105, %fd6097;
	mov.f64 	%fd6106, %fd6097;
	mov.f64 	%fd6107, %fd6097;
	mov.f64 	%fd6108, %fd6097;
	mov.f64 	%fd6109, %fd6097;
	mov.f64 	%fd6110, %fd6097;
	mov.f64 	%fd6111, %fd6097;
	mov.f64 	%fd6112, %fd6097;
	mov.f64 	%fd6113, %fd6097;
	bra.uni 	$L__BB5_278;

$L__BB5_32:
	setp.eq.s32 	%p53, %r647, 5;
	@%p53 bra 	$L__BB5_38;
	bra.uni 	$L__BB5_33;

$L__BB5_38:
	sub.f64 	%fd1836, %fd23, %fd13;
	sub.f64 	%fd1837, %fd28, %fd19;
	sub.f64 	%fd1838, %fd26, %fd16;
	mul.f64 	%fd1839, %fd1838, %fd1837;
	sub.f64 	%fd1840, %fd25, %fd16;
	sub.f64 	%fd1841, %fd29, %fd19;
	mul.f64 	%fd1842, %fd1841, %fd1840;
	sub.f64 	%fd1843, %fd1839, %fd1842;
	sub.f64 	%fd1844, %fd22, %fd13;
	mul.f64 	%fd1845, %fd1841, %fd1844;
	mul.f64 	%fd1846, %fd1836, %fd1837;
	sub.f64 	%fd1847, %fd1845, %fd1846;
	mul.f64 	%fd1848, %fd1836, %fd1840;
	mul.f64 	%fd1849, %fd1838, %fd1844;
	sub.f64 	%fd1850, %fd1848, %fd1849;
	mul.f64 	%fd1851, %fd1847, %fd1847;
	fma.rn.f64 	%fd1852, %fd1843, %fd1843, %fd1851;
	fma.rn.f64 	%fd1853, %fd1850, %fd1850, %fd1852;
	div.rn.f64 	%fd5580, %fd1853, %fd36;
	bra.uni 	$L__BB5_44;

$L__BB5_272:
	setp.eq.s32 	%p20, %r670, 5;
	setp.ne.s32 	%p246, %r670, 5;
	mov.f64 	%fd6103, 0d0000000000000000;
	mov.f64 	%fd6097, %fd6103;
	mov.f64 	%fd6098, %fd6103;
	mov.f64 	%fd6099, %fd6103;
	mov.f64 	%fd6100, %fd6103;
	mov.f64 	%fd6101, %fd6103;
	mov.f64 	%fd6102, %fd6103;
	mov.f64 	%fd6115, %fd6103;
	@%p246 bra 	$L__BB5_274;

	mul.f64 	%fd3858, %fd967, %fd954;
	fma.rn.f64 	%fd3859, %fd969, %fd956, %fd3858;
	fma.rn.f64 	%fd3860, %fd971, %fd958, %fd3859;
	mul.f64 	%fd3861, %fd954, %fd954;
	fma.rn.f64 	%fd3862, %fd956, %fd956, %fd3861;
	fma.rn.f64 	%fd6114, %fd958, %fd958, %fd3862;
	div.rn.f64 	%fd6115, %fd3860, %fd6114;
	mov.f64 	%fd6097, %fd971;
	mov.f64 	%fd6098, %fd969;
	mov.f64 	%fd6099, %fd967;
	mov.f64 	%fd6100, %fd958;
	mov.f64 	%fd6101, %fd956;
	mov.f64 	%fd6102, %fd954;

$L__BB5_274:
	selp.f64 	%fd1081, %fd6115, %fd6096, %p20;
	selp.f64 	%fd1080, 0d0000000000000000, %fd6095, %p20;
	mov.f64 	%fd6104, %fd6103;
	mov.f64 	%fd6105, %fd6103;
	mov.f64 	%fd6106, %fd6103;
	mov.f64 	%fd6107, %fd6103;
	mov.f64 	%fd6108, %fd6103;
	mov.f64 	%fd6109, %fd6103;
	mov.f64 	%fd6110, %fd6103;
	mov.f64 	%fd6111, %fd6103;
	mov.f64 	%fd6090, %fd6103;
	mov.f64 	%fd6091, %fd6103;
	@%p20 bra 	$L__BB5_276;

	mul.f64 	%fd3875, %fd946, %fd954;
	fma.rn.f64 	%fd3876, %fd949, %fd956, %fd3875;
	fma.rn.f64 	%fd6116, %fd952, %fd958, %fd3876;
	mul.f64 	%fd3877, %fd954, %fd954;
	fma.rn.f64 	%fd3878, %fd956, %fd956, %fd3877;
	fma.rn.f64 	%fd6117, %fd958, %fd958, %fd3878;
	mul.f64 	%fd3879, %fd967, %fd954;
	fma.rn.f64 	%fd3880, %fd969, %fd956, %fd3879;
	fma.rn.f64 	%fd6118, %fd971, %fd958, %fd3880;
	div.rn.f64 	%fd3881, %fd6116, %fd965;
	mul.f64 	%fd3882, %fd3881, %fd3881;
	mul.f64 	%fd3883, %fd965, %fd3882;
	sub.f64 	%fd3884, %fd6117, %fd3883;
	mul.f64 	%fd3885, %fd972, %fd3881;
	sub.f64 	%fd3886, %fd6118, %fd3885;
	div.rn.f64 	%fd6090, %fd3886, %fd3884;
	mul.f64 	%fd3887, %fd965, %fd3881;
	mul.f64 	%fd3888, %fd3887, %fd6090;
	sub.f64 	%fd3889, %fd972, %fd3888;
	div.rn.f64 	%fd6091, %fd3889, %fd965;
	mov.f64 	%fd6103, %fd952;
	mov.f64 	%fd6104, %fd949;
	mov.f64 	%fd6105, %fd946;
	mov.f64 	%fd6106, %fd958;
	mov.f64 	%fd6107, %fd956;
	mov.f64 	%fd6108, %fd954;
	mov.f64 	%fd6109, %fd971;
	mov.f64 	%fd6110, %fd969;
	mov.f64 	%fd6111, %fd967;

$L__BB5_276:
	selp.f64 	%fd6113, %fd1080, %fd6091, %p20;
	selp.f64 	%fd6112, %fd1081, %fd6090, %p20;
	selp.u16 	%rs352, 1, 0, %p20;

$L__BB5_278:
	selp.f64 	%fd6143, %fd6095, %fd6113, %p19;
	selp.f64 	%fd6142, %fd6096, %fd6112, %p19;
	selp.u16 	%rs354, 1, 0, %p19;

$L__BB5_280:
	selp.f64 	%fd6181, %fd6119, %fd6143, %p18;
	selp.f64 	%fd6180, %fd6120, %fd6142, %p18;
	selp.u16 	%rs357, 1, 0, %p18;

$L__BB5_282:
	selp.f64 	%fd6220, %fd6151, %fd6181, %p17;
	selp.f64 	%fd6219, %fd6152, %fd6180, %p17;
	selp.u16 	%rs361, 1, 0, %p17;

$L__BB5_284:
	selp.f64 	%fd6257, %fd6190, %fd6220, %p16;
	selp.f64 	%fd6256, %fd6191, %fd6219, %p16;
	selp.u16 	%rs360, 1, 0, %p16;

$L__BB5_285:
	selp.f64 	%fd1291, 0d0000000000000000, %fd6256, %p241;
	selp.f64 	%fd1290, 0d0000000000000000, %fd6257, %p241;
	mov.f64 	%fd3992, 0d3FF0000000000000;
	sub.f64 	%fd3993, %fd3992, %fd1290;
	sub.f64 	%fd1292, %fd3993, %fd1291;
	mul.f64 	%fd3994, %fd945, %fd1292;
	mul.f64 	%fd3995, %fd948, %fd1292;
	mul.f64 	%fd3996, %fd951, %fd1292;
	fma.rn.f64 	%fd3997, %fd944, %fd1290, %fd3994;
	fma.rn.f64 	%fd3998, %fd947, %fd1290, %fd3995;
	fma.rn.f64 	%fd3999, %fd950, %fd1290, %fd3996;
	fma.rn.f64 	%fd4000, %fd953, %fd1291, %fd3997;
	fma.rn.f64 	%fd4001, %fd955, %fd1291, %fd3998;
	fma.rn.f64 	%fd4002, %fd957, %fd1291, %fd3999;
	sub.f64 	%fd4003, %fd966, %fd4000;
	sub.f64 	%fd4004, %fd968, %fd4001;
	sub.f64 	%fd4005, %fd970, %fd4002;
	mul.f64 	%fd4006, %fd4004, %fd4004;
	fma.rn.f64 	%fd4007, %fd4003, %fd4003, %fd4006;
	fma.rn.f64 	%fd4008, %fd4005, %fd4005, %fd4007;
	sqrt.rn.f64 	%fd4009, %fd4008;
	div.rn.f64 	%fd1293, %fd4003, %fd4009;
	div.rn.f64 	%fd1294, %fd4004, %fd4009;
	div.rn.f64 	%fd1295, %fd4005, %fd4009;
	add.f64 	%fd1296, %fd6053, 0d0000000000000000;
	add.f64 	%fd4010, %fd6051, 0d0000000000000000;
	add.f64 	%fd4011, %fd6050, 0d0000000000000000;
	mul.f64 	%fd4012, %fd4011, %fd4004;
	fma.rn.f64 	%fd4013, %fd4010, %fd4003, %fd4012;
	add.f64 	%fd4014, %fd6049, 0d0000000000000000;
	fma.rn.f64 	%fd4015, %fd4014, %fd4005, %fd4013;
	mul.f64 	%fd4016, %fd4009, %fd4009;
	div.rn.f64 	%fd1297, %fd4015, %fd4016;
	div.rn.f64 	%fd4017, %fd4010, %fd4009;
	add.f64 	%fd6268, %fd4017, 0d0000000000000000;
	div.rn.f64 	%fd4018, %fd4011, %fd4009;
	add.f64 	%fd6267, %fd4018, 0d0000000000000000;
	div.rn.f64 	%fd4019, %fd4014, %fd4009;
	add.f64 	%fd6266, %fd4019, 0d0000000000000000;
	setp.leu.f64 	%p254, %fd4009, 0d0000000000000000;
	@%p254 bra 	$L__BB5_287;

	mov.f64 	%fd4020, 0d0000000000000000;
	sub.f64 	%fd4021, %fd4020, %fd1297;
	fma.rn.f64 	%fd6268, %fd1293, %fd4021, %fd6268;
	fma.rn.f64 	%fd6267, %fd1294, %fd4021, %fd6267;
	fma.rn.f64 	%fd6266, %fd1295, %fd4021, %fd6266;

$L__BB5_287:
	add.f64 	%fd6299, %fd6268, 0d0000000000000000;
	mov.f64 	%fd6338, 0d0000000000000000;
	add.f64 	%fd6298, %fd6267, 0d0000000000000000;
	add.f64 	%fd6297, %fd6266, 0d0000000000000000;
	sub.f64 	%fd4023, %fd6338, %fd6267;
	add.f64 	%fd4024, %fd4023, 0d0000000000000000;
	sub.f64 	%fd4025, %fd6338, %fd6268;
	add.f64 	%fd4026, %fd4025, 0d0000000000000000;
	sub.f64 	%fd4027, %fd6338, %fd6266;
	add.f64 	%fd4028, %fd4027, 0d0000000000000000;
	fma.rn.f64 	%fd6308, %fd1291, %fd4026, 0d0000000000000000;
	fma.rn.f64 	%fd6307, %fd1291, %fd4024, 0d0000000000000000;
	fma.rn.f64 	%fd6306, %fd1291, %fd4028, 0d0000000000000000;
	mul.f64 	%fd4029, %fd953, %fd4026;
	fma.rn.f64 	%fd4030, %fd955, %fd4024, %fd4029;
	fma.rn.f64 	%fd4031, %fd957, %fd4028, %fd4030;
	add.f64 	%fd4032, %fd4031, 0d0000000000000000;
	fma.rn.f64 	%fd6305, %fd1290, %fd4026, 0d0000000000000000;
	fma.rn.f64 	%fd6304, %fd1290, %fd4024, 0d0000000000000000;
	fma.rn.f64 	%fd6303, %fd1290, %fd4028, 0d0000000000000000;
	add.f64 	%fd4033, %fd6052, 0d0000000000000000;
	add.f64 	%fd4034, %fd4033, %fd4032;
	mul.f64 	%fd4035, %fd944, %fd4026;
	fma.rn.f64 	%fd4036, %fd947, %fd4024, %fd4035;
	fma.rn.f64 	%fd4037, %fd950, %fd4028, %fd4036;
	add.f64 	%fd4038, %fd4037, 0d0000000000000000;
	fma.rn.f64 	%fd6302, %fd1292, %fd4026, 0d0000000000000000;
	fma.rn.f64 	%fd6301, %fd1292, %fd4024, 0d0000000000000000;
	fma.rn.f64 	%fd6300, %fd1292, %fd4028, 0d0000000000000000;
	add.f64 	%fd4039, %fd1296, %fd4038;
	mul.f64 	%fd4040, %fd945, %fd4026;
	fma.rn.f64 	%fd4041, %fd948, %fd4024, %fd4040;
	fma.rn.f64 	%fd4042, %fd951, %fd4028, %fd4041;
	add.f64 	%fd4043, %fd4042, 0d0000000000000000;
	sub.f64 	%fd4044, %fd6338, %fd4043;
	add.f64 	%fd1319, %fd4034, %fd4044;
	add.f64 	%fd1320, %fd4039, %fd4044;
	or.b16  	%rs309, %rs361, %rs360;
	and.b16  	%rs310, %rs309, 255;
	setp.ne.s16 	%p256, %rs310, 0;
	or.pred  	%p257, %p256, %p241;
	@%p257 bra 	$L__BB5_297;

	add.f64 	%fd6296, %fd1320, 0d0000000000000000;
	and.b16  	%rs311, %rs357, 255;
	setp.eq.s16 	%p258, %rs311, 0;
	@%p258 bra 	$L__BB5_290;

	div.rn.f64 	%fd4045, %fd6296, %fd965;
	add.f64 	%fd4046, %fd4045, 0d0000000000000000;
	mov.f64 	%fd4047, 0d0000000000000000;
	mul.f64 	%fd4048, %fd6182, %fd6296;
	div.rn.f64 	%fd4049, %fd4048, %fd965;
	sub.f64 	%fd4050, %fd4047, %fd4049;
	add.f64 	%fd4051, %fd6158, %fd6158;
	add.f64 	%fd4052, %fd6157, %fd6157;
	add.f64 	%fd4053, %fd6156, %fd6156;
	fma.rn.f64 	%fd4054, %fd4051, %fd4050, 0d0000000000000000;
	fma.rn.f64 	%fd4055, %fd4052, %fd4050, 0d0000000000000000;
	fma.rn.f64 	%fd4056, %fd4053, %fd4050, 0d0000000000000000;
	fma.rn.f64 	%fd4057, %fd6158, %fd4046, 0d0000000000000000;
	fma.rn.f64 	%fd4058, %fd6157, %fd4046, 0d0000000000000000;
	fma.rn.f64 	%fd4059, %fd6156, %fd4046, 0d0000000000000000;
	fma.rn.f64 	%fd4060, %fd6155, %fd4046, %fd4054;
	fma.rn.f64 	%fd4061, %fd6154, %fd4046, %fd4055;
	fma.rn.f64 	%fd4062, %fd6153, %fd4046, %fd4056;
	add.f64 	%fd6305, %fd6305, %fd4060;
	add.f64 	%fd6304, %fd6304, %fd4061;
	add.f64 	%fd6303, %fd6303, %fd4062;
	sub.f64 	%fd4063, %fd6302, %fd4060;
	sub.f64 	%fd4064, %fd6301, %fd4061;
	sub.f64 	%fd4065, %fd6300, %fd4062;
	add.f64 	%fd6299, %fd6299, %fd4057;
	add.f64 	%fd6298, %fd6298, %fd4058;
	add.f64 	%fd6297, %fd6297, %fd4059;
	sub.f64 	%fd6302, %fd4063, %fd4057;
	sub.f64 	%fd6301, %fd4064, %fd4058;
	sub.f64 	%fd6300, %fd4065, %fd4059;
	bra.uni 	$L__BB5_297;

$L__BB5_290:
	add.f64 	%fd6295, %fd1319, 0d0000000000000000;
	and.b16  	%rs312, %rs354, 255;
	setp.eq.s16 	%p21, %rs312, 0;
	setp.ne.s16 	%p259, %rs312, 0;
	@%p259 bra 	$L__BB5_295;

	and.b16  	%rs313, %rs352, 255;
	setp.ne.s16 	%p260, %rs313, 0;
	mov.f64 	%fd6281, %fd6295;
	mov.f64 	%fd6282, %fd6296;
	@%p260 bra 	$L__BB5_293;

	div.rn.f64 	%fd4068, %fd6116, %fd965;
	mul.f64 	%fd4069, %fd4068, %fd4068;
	mul.f64 	%fd4070, %fd965, %fd4069;
	sub.f64 	%fd4071, %fd6117, %fd4070;
	mul.f64 	%fd4072, %fd972, %fd4068;
	sub.f64 	%fd4073, %fd6118, %fd4072;
	div.rn.f64 	%fd4074, %fd4073, %fd4071;
	mul.f64 	%fd4075, %fd965, %fd4068;
	mul.f64 	%fd4076, %fd4075, %fd4074;
	sub.f64 	%fd4077, %fd972, %fd4076;
	div.rn.f64 	%fd4078, %fd4077, %fd965;
	div.rn.f64 	%fd4079, %fd6296, %fd965;
	add.f64 	%fd4080, %fd4079, 0d0000000000000000;
	mov.f64 	%fd6281, 0d0000000000000000;
	mul.f64 	%fd4081, %fd4078, %fd6296;
	div.rn.f64 	%fd4082, %fd4081, %fd965;
	sub.f64 	%fd4083, %fd6281, %fd4082;
	sub.f64 	%fd4084, %fd6281, %fd4080;
	fma.rn.f64 	%fd4085, %fd4074, %fd4084, 0d0000000000000000;
	fma.rn.f64 	%fd4086, %fd4075, %fd4084, %fd6295;
	fma.rn.f64 	%fd4087, %fd4068, %fd4085, %fd4083;
	fma.rn.f64 	%fd4088, %fd965, %fd4085, 0d0000000000000000;
	div.rn.f64 	%fd4089, %fd4086, %fd4071;
	add.f64 	%fd4090, %fd4089, 0d0000000000000000;
	mul.f64 	%fd4091, %fd4074, %fd4086;
	div.rn.f64 	%fd4092, %fd4091, %fd4071;
	sub.f64 	%fd4093, %fd6281, %fd4092;
	sub.f64 	%fd4094, %fd6281, %fd4090;
	fma.rn.f64 	%fd4095, %fd972, %fd4094, %fd4088;
	fma.rn.f64 	%fd4096, %fd4068, %fd4094, %fd4080;
	add.f64 	%fd4097, %fd4096, 0d0000000000000000;
	add.f64 	%fd4098, %fd4093, 0d0000000000000000;
	sub.f64 	%fd4099, %fd6281, %fd4093;
	fma.rn.f64 	%fd4100, %fd965, %fd4099, 0d0000000000000000;
	fma.rn.f64 	%fd4101, %fd4069, %fd4099, %fd4087;
	fma.rn.f64 	%fd4102, %fd4068, %fd4100, %fd4095;
	fma.rn.f64 	%fd4103, %fd4068, %fd4100, %fd4102;
	div.rn.f64 	%fd4104, %fd4103, %fd965;
	add.f64 	%fd4105, %fd4104, 0d0000000000000000;
	mul.f64 	%fd4106, %fd4068, %fd4103;
	div.rn.f64 	%fd4107, %fd4106, %fd965;
	sub.f64 	%fd4108, %fd4101, %fd4107;
	add.f64 	%fd4109, %fd4108, 0d0000000000000000;
	fma.rn.f64 	%fd4110, %fd6111, %fd4090, 0d0000000000000000;
	fma.rn.f64 	%fd4111, %fd6110, %fd4090, 0d0000000000000000;
	fma.rn.f64 	%fd4112, %fd6109, %fd4090, 0d0000000000000000;
	fma.rn.f64 	%fd4113, %fd6108, %fd4090, 0d0000000000000000;
	fma.rn.f64 	%fd4114, %fd6107, %fd4090, 0d0000000000000000;
	fma.rn.f64 	%fd4115, %fd6106, %fd4090, 0d0000000000000000;
	fma.rn.f64 	%fd4116, %fd6111, %fd4097, 0d0000000000000000;
	fma.rn.f64 	%fd4117, %fd6110, %fd4097, 0d0000000000000000;
	fma.rn.f64 	%fd4118, %fd6109, %fd4097, 0d0000000000000000;
	fma.rn.f64 	%fd4119, %fd6105, %fd4097, %fd4113;
	fma.rn.f64 	%fd4120, %fd6104, %fd4097, %fd4114;
	fma.rn.f64 	%fd4121, %fd6103, %fd4097, %fd4115;
	add.f64 	%fd4122, %fd6108, %fd6108;
	add.f64 	%fd4123, %fd6107, %fd6107;
	add.f64 	%fd4124, %fd6106, %fd6106;
	fma.rn.f64 	%fd4125, %fd4122, %fd4098, %fd4110;
	fma.rn.f64 	%fd4126, %fd4123, %fd4098, %fd4111;
	fma.rn.f64 	%fd4127, %fd4124, %fd4098, %fd4112;
	fma.rn.f64 	%fd4128, %fd6108, %fd4105, %fd4116;
	fma.rn.f64 	%fd4129, %fd6107, %fd4105, %fd4117;
	fma.rn.f64 	%fd4130, %fd6106, %fd4105, %fd4118;
	fma.rn.f64 	%fd4131, %fd6105, %fd4105, %fd4125;
	fma.rn.f64 	%fd4132, %fd6104, %fd4105, %fd4126;
	fma.rn.f64 	%fd4133, %fd6103, %fd4105, %fd4127;
	add.f64 	%fd4134, %fd6105, %fd6105;
	add.f64 	%fd4135, %fd6104, %fd6104;
	add.f64 	%fd4136, %fd6103, %fd6103;
	fma.rn.f64 	%fd4137, %fd4134, %fd4109, %fd4128;
	fma.rn.f64 	%fd4138, %fd4135, %fd4109, %fd4129;
	fma.rn.f64 	%fd4139, %fd4136, %fd4109, %fd4130;
	add.f64 	%fd6299, %fd6299, %fd4119;
	add.f64 	%fd6298, %fd6298, %fd4120;
	add.f64 	%fd6297, %fd6297, %fd4121;
	sub.f64 	%fd4140, %fd6302, %fd4119;
	sub.f64 	%fd4141, %fd6301, %fd4120;
	sub.f64 	%fd4142, %fd6300, %fd4121;
	add.f64 	%fd6308, %fd6308, %fd4131;
	add.f64 	%fd6307, %fd6307, %fd4132;
	add.f64 	%fd6306, %fd6306, %fd4133;
	sub.f64 	%fd4143, %fd4140, %fd4131;
	sub.f64 	%fd4144, %fd4141, %fd4132;
	sub.f64 	%fd4145, %fd4142, %fd4133;
	add.f64 	%fd6305, %fd6305, %fd4137;
	add.f64 	%fd6304, %fd6304, %fd4138;
	add.f64 	%fd6303, %fd6303, %fd4139;
	sub.f64 	%fd6302, %fd4143, %fd4137;
	sub.f64 	%fd6301, %fd4144, %fd4138;
	sub.f64 	%fd6300, %fd4145, %fd4139;
	mov.f64 	%fd6282, %fd6281;

$L__BB5_293:
	selp.f64 	%fd4146, 0d0000000000000000, %fd6296, %p21;
	add.f64 	%fd6296, %fd4146, %fd6282;
	selp.f64 	%fd4147, 0d0000000000000000, %fd6295, %p21;
	add.f64 	%fd6295, %fd4147, %fd6281;
	setp.eq.s16 	%p261, %rs313, 0;
	@%p261 bra 	$L__BB5_295;

	add.f64 	%fd4150, %fd6281, 0d0000000000000000;
	mov.f64 	%fd6295, 0d0000000000000000;
	div.rn.f64 	%fd4151, %fd4150, %fd6114;
	add.f64 	%fd4152, %fd4151, 0d0000000000000000;
	mul.f64 	%fd4153, %fd6115, %fd4150;
	div.rn.f64 	%fd4154, %fd4153, %fd6114;
	sub.f64 	%fd4155, %fd6295, %fd4154;
	add.f64 	%fd4156, %fd6102, %fd6102;
	add.f64 	%fd4157, %fd6101, %fd6101;
	add.f64 	%fd4158, %fd6100, %fd6100;
	fma.rn.f64 	%fd4159, %fd4156, %fd4155, 0d0000000000000000;
	fma.rn.f64 	%fd4160, %fd4157, %fd4155, 0d0000000000000000;
	fma.rn.f64 	%fd4161, %fd4158, %fd4155, 0d0000000000000000;
	fma.rn.f64 	%fd4162, %fd6102, %fd4152, 0d0000000000000000;
	fma.rn.f64 	%fd4163, %fd6101, %fd4152, 0d0000000000000000;
	fma.rn.f64 	%fd4164, %fd6100, %fd4152, 0d0000000000000000;
	fma.rn.f64 	%fd4165, %fd6099, %fd4152, %fd4159;
	fma.rn.f64 	%fd4166, %fd6098, %fd4152, %fd4160;
	fma.rn.f64 	%fd4167, %fd6097, %fd4152, %fd4161;
	add.f64 	%fd6308, %fd6308, %fd4165;
	add.f64 	%fd6307, %fd6307, %fd4166;
	add.f64 	%fd6306, %fd6306, %fd4167;
	sub.f64 	%fd4168, %fd6302, %fd4165;
	sub.f64 	%fd4169, %fd6301, %fd4166;
	sub.f64 	%fd4170, %fd6300, %fd4167;
	add.f64 	%fd6299, %fd6299, %fd4162;
	add.f64 	%fd6298, %fd6298, %fd4163;
	add.f64 	%fd6297, %fd6297, %fd4164;
	sub.f64 	%fd6302, %fd4168, %fd4162;
	sub.f64 	%fd6301, %fd4169, %fd4163;
	sub.f64 	%fd6300, %fd4170, %fd4164;
	mov.f64 	%fd6296, %fd6295;

$L__BB5_295:
	@%p21 bra 	$L__BB5_297;

	add.f64 	%fd4171, %fd6295, 0d0000000000000000;
	mov.f64 	%fd4172, 0d0000000000000000;
	add.f64 	%fd4173, %fd6296, 0d0000000000000000;
	sub.f64 	%fd4174, %fd4171, %fd4173;
	div.rn.f64 	%fd4175, %fd4174, %fd6144;
	add.f64 	%fd4176, %fd4175, 0d0000000000000000;
	mul.f64 	%fd4177, %fd6145, %fd4174;
	div.rn.f64 	%fd4178, %fd4177, %fd6144;
	sub.f64 	%fd4179, %fd4172, %fd4178;
	add.f64 	%fd4180, %fd6126, %fd6126;
	add.f64 	%fd4181, %fd6125, %fd6125;
	add.f64 	%fd4182, %fd6124, %fd6124;
	fma.rn.f64 	%fd4183, %fd4180, %fd4179, 0d0000000000000000;
	fma.rn.f64 	%fd4184, %fd4181, %fd4179, 0d0000000000000000;
	fma.rn.f64 	%fd4185, %fd4182, %fd4179, 0d0000000000000000;
	fma.rn.f64 	%fd4186, %fd6126, %fd4176, 0d0000000000000000;
	fma.rn.f64 	%fd4187, %fd6125, %fd4176, 0d0000000000000000;
	fma.rn.f64 	%fd4188, %fd6124, %fd4176, 0d0000000000000000;
	fma.rn.f64 	%fd4189, %fd6123, %fd4176, %fd4183;
	fma.rn.f64 	%fd4190, %fd6122, %fd4176, %fd4184;
	fma.rn.f64 	%fd4191, %fd6121, %fd4176, %fd4185;
	add.f64 	%fd6308, %fd6308, %fd4189;
	add.f64 	%fd6307, %fd6307, %fd4190;
	add.f64 	%fd6306, %fd6306, %fd4191;
	sub.f64 	%fd4192, %fd6305, %fd4189;
	sub.f64 	%fd4193, %fd6304, %fd4190;
	sub.f64 	%fd4194, %fd6303, %fd4191;
	add.f64 	%fd6299, %fd6299, %fd4186;
	add.f64 	%fd6298, %fd6298, %fd4187;
	add.f64 	%fd6297, %fd6297, %fd4188;
	sub.f64 	%fd6305, %fd4192, %fd4186;
	sub.f64 	%fd6304, %fd4193, %fd4187;
	sub.f64 	%fd6303, %fd4194, %fd4188;

$L__BB5_297:
	mov.f64 	%fd6339, %fd6338;
	mov.f64 	%fd6340, %fd6338;
	mov.f64 	%fd6341, %fd6338;
	mov.f64 	%fd6342, %fd6338;
	mov.f64 	%fd6343, %fd6338;
	mov.f64 	%fd6344, %fd6338;
	mov.f64 	%fd6345, %fd6338;
	mov.f64 	%fd6346, %fd6338;
	@%p15 bra 	$L__BB5_301;

	sub.f64 	%fd1404, %fd953, %fd944;
	sub.f64 	%fd1405, %fd955, %fd947;
	mul.f64 	%fd4212, %fd1405, %fd961;
	sub.f64 	%fd1406, %fd957, %fd950;
	mul.f64 	%fd4213, %fd1406, %fd960;
	sub.f64 	%fd1407, %fd4212, %fd4213;
	mul.f64 	%fd4214, %fd1406, %fd959;
	mul.f64 	%fd4215, %fd1404, %fd961;
	sub.f64 	%fd1408, %fd4214, %fd4215;
	mul.f64 	%fd4216, %fd1404, %fd960;
	mul.f64 	%fd4217, %fd1405, %fd959;
	sub.f64 	%fd1409, %fd4216, %fd4217;
	mul.f64 	%fd4218, %fd1404, %fd1404;
	fma.rn.f64 	%fd4219, %fd1405, %fd1405, %fd4218;
	fma.rn.f64 	%fd1410, %fd1406, %fd1406, %fd4219;
	mul.f64 	%fd4220, %fd1405, %fd1408;
	fma.rn.f64 	%fd4221, %fd1404, %fd1407, %fd4220;
	fma.rn.f64 	%fd4222, %fd1406, %fd1409, %fd4221;
	mul.f64 	%fd4223, %fd1408, %fd1408;
	fma.rn.f64 	%fd4224, %fd1407, %fd1407, %fd4223;
	fma.rn.f64 	%fd4225, %fd1409, %fd1409, %fd4224;
	sub.f64 	%fd1411, %fd966, %fd944;
	mul.f64 	%fd4226, %fd1411, %fd1404;
	sub.f64 	%fd1412, %fd968, %fd947;
	fma.rn.f64 	%fd4227, %fd1412, %fd1405, %fd4226;
	sub.f64 	%fd1413, %fd970, %fd950;
	fma.rn.f64 	%fd1414, %fd1413, %fd1406, %fd4227;
	mul.f64 	%fd4228, %fd1412, %fd1408;
	fma.rn.f64 	%fd4229, %fd1411, %fd1407, %fd4228;
	fma.rn.f64 	%fd4230, %fd1413, %fd1409, %fd4229;
	div.rn.f64 	%fd1415, %fd4222, %fd1410;
	mul.f64 	%fd1416, %fd1415, %fd1415;
	mul.f64 	%fd4231, %fd1410, %fd1416;
	sub.f64 	%fd1417, %fd4225, %fd4231;
	mul.f64 	%fd4232, %fd1414, %fd1415;
	sub.f64 	%fd4233, %fd4230, %fd4232;
	div.rn.f64 	%fd1418, %fd4233, %fd1417;
	mul.f64 	%fd1419, %fd1410, %fd1415;
	mul.f64 	%fd4234, %fd1419, %fd1418;
	sub.f64 	%fd4235, %fd1414, %fd4234;
	div.rn.f64 	%fd1420, %fd4235, %fd1410;
	setp.gt.f64 	%p263, %fd1420, 0d0000000000000000;
	mov.f64 	%fd4211, 0d0000000000000000;
	setp.lt.f64 	%p264, %fd1420, 0d3FF0000000000000;
	setp.ge.f64 	%p265, %fd1418, 0d0000000000000000;
	and.pred  	%p266, %p263, %p264;
	and.pred  	%p267, %p265, %p266;
	mov.f64 	%fd6318, %fd4211;
	mov.f64 	%fd6319, %fd4211;
	mov.f64 	%fd6320, %fd4211;
	mov.f64 	%fd6321, %fd4211;
	mov.f64 	%fd6322, %fd4211;
	mov.f64 	%fd6323, %fd4211;
	mov.f64 	%fd6324, %fd4211;
	mov.f64 	%fd6325, %fd4211;
	@%p267 bra 	$L__BB5_300;

	sub.f64 	%fd4236, %fd945, %fd953;
	sub.f64 	%fd4237, %fd948, %fd955;
	mul.f64 	%fd4238, %fd4237, %fd961;
	sub.f64 	%fd4239, %fd951, %fd957;
	mul.f64 	%fd4240, %fd4239, %fd960;
	sub.f64 	%fd4241, %fd4238, %fd4240;
	mul.f64 	%fd4242, %fd4239, %fd959;
	mul.f64 	%fd4243, %fd4236, %fd961;
	sub.f64 	%fd4244, %fd4242, %fd4243;
	mul.f64 	%fd4245, %fd4236, %fd960;
	mul.f64 	%fd4246, %fd4237, %fd959;
	sub.f64 	%fd4247, %fd4245, %fd4246;
	mul.f64 	%fd4248, %fd4236, %fd4236;
	fma.rn.f64 	%fd4249, %fd4237, %fd4237, %fd4248;
	fma.rn.f64 	%fd4250, %fd4239, %fd4239, %fd4249;
	mul.f64 	%fd4251, %fd4237, %fd4244;
	fma.rn.f64 	%fd4252, %fd4236, %fd4241, %fd4251;
	fma.rn.f64 	%fd4253, %fd4239, %fd4247, %fd4252;
	mul.f64 	%fd4254, %fd4244, %fd4244;
	fma.rn.f64 	%fd4255, %fd4241, %fd4241, %fd4254;
	fma.rn.f64 	%fd4256, %fd4247, %fd4247, %fd4255;
	sub.f64 	%fd4257, %fd966, %fd953;
	mul.f64 	%fd4258, %fd4236, %fd4257;
	sub.f64 	%fd4259, %fd968, %fd955;
	fma.rn.f64 	%fd4260, %fd4237, %fd4259, %fd4258;
	sub.f64 	%fd4261, %fd970, %fd957;
	fma.rn.f64 	%fd4262, %fd4239, %fd4261, %fd4260;
	mul.f64 	%fd4263, %fd4259, %fd4244;
	fma.rn.f64 	%fd4264, %fd4257, %fd4241, %fd4263;
	fma.rn.f64 	%fd4265, %fd4261, %fd4247, %fd4264;
	div.rn.f64 	%fd4266, %fd4253, %fd4250;
	mul.f64 	%fd4267, %fd4266, %fd4266;
	mul.f64 	%fd4268, %fd4250, %fd4267;
	sub.f64 	%fd4269, %fd4256, %fd4268;
	mul.f64 	%fd4270, %fd4262, %fd4266;
	sub.f64 	%fd4271, %fd4265, %fd4270;
	div.rn.f64 	%fd4272, %fd4271, %fd4269;
	mul.f64 	%fd4273, %fd4250, %fd4266;
	mul.f64 	%fd4274, %fd4273, %fd4272;
	sub.f64 	%fd4275, %fd4262, %fd4274;
	div.rn.f64 	%fd4276, %fd4275, %fd4250;
	mov.f64 	%fd4277, 0d0000000000000000;
	div.rn.f64 	%fd4278, %fd4277, %fd4250;
	add.f64 	%fd4279, %fd4278, 0d0000000000000000;
	mul.f64 	%fd4280, %fd4276, 0d0000000000000000;
	div.rn.f64 	%fd4281, %fd4280, %fd4250;
	sub.f64 	%fd4282, %fd4277, %fd4281;
	sub.f64 	%fd4283, %fd4277, %fd4279;
	fma.rn.f64 	%fd4284, %fd4283, %fd4272, 0d0000000000000000;
	fma.rn.f64 	%fd4285, %fd4283, %fd4273, 0d0000000000000000;
	fma.rn.f64 	%fd4286, %fd4266, %fd4284, %fd4282;
	fma.rn.f64 	%fd4287, %fd4250, %fd4284, 0d0000000000000000;
	div.rn.f64 	%fd4288, %fd4285, %fd4269;
	add.f64 	%fd6321, %fd4288, 0d0000000000000000;
	mul.f64 	%fd4289, %fd4285, %fd4272;
	div.rn.f64 	%fd4290, %fd4289, %fd4269;
	sub.f64 	%fd4291, %fd4277, %fd4290;
	sub.f64 	%fd4292, %fd4277, %fd6321;
	fma.rn.f64 	%fd4293, %fd4262, %fd4292, %fd4287;
	fma.rn.f64 	%fd4294, %fd4266, %fd4292, %fd4279;
	add.f64 	%fd6322, %fd4294, 0d0000000000000000;
	add.f64 	%fd6318, %fd4291, 0d0000000000000000;
	sub.f64 	%fd4295, %fd4277, %fd4291;
	fma.rn.f64 	%fd4296, %fd4250, %fd4295, 0d0000000000000000;
	fma.rn.f64 	%fd4297, %fd4267, %fd4295, %fd4286;
	fma.rn.f64 	%fd4298, %fd4266, %fd4296, %fd4293;
	fma.rn.f64 	%fd4299, %fd4266, %fd4296, %fd4298;
	div.rn.f64 	%fd4300, %fd4299, %fd4250;
	add.f64 	%fd6319, %fd4300, 0d0000000000000000;
	mul.f64 	%fd4301, %fd4266, %fd4299;
	div.rn.f64 	%fd4302, %fd4301, %fd4250;
	sub.f64 	%fd4303, %fd4297, %fd4302;
	add.f64 	%fd6320, %fd4303, 0d0000000000000000;
	fma.rn.f64 	%fd4304, %fd4257, %fd6321, 0d0000000000000000;
	fma.rn.f64 	%fd4305, %fd4259, %fd6321, 0d0000000000000000;
	fma.rn.f64 	%fd4306, %fd4261, %fd6321, 0d0000000000000000;
	fma.rn.f64 	%fd4307, %fd4241, %fd6321, 0d0000000000000000;
	fma.rn.f64 	%fd4308, %fd4244, %fd6321, 0d0000000000000000;
	fma.rn.f64 	%fd4309, %fd4247, %fd6321, 0d0000000000000000;
	fma.rn.f64 	%fd4310, %fd4257, %fd6322, 0d0000000000000000;
	fma.rn.f64 	%fd4311, %fd4259, %fd6322, 0d0000000000000000;
	fma.rn.f64 	%fd4312, %fd4261, %fd6322, 0d0000000000000000;
	fma.rn.f64 	%fd4313, %fd4236, %fd6322, %fd4307;
	fma.rn.f64 	%fd4314, %fd4237, %fd6322, %fd4308;
	fma.rn.f64 	%fd4315, %fd4239, %fd6322, %fd4309;
	add.f64 	%fd6299, %fd4313, %fd6299;
	add.f64 	%fd6298, %fd4314, %fd6298;
	add.f64 	%fd6297, %fd4315, %fd6297;
	sub.f64 	%fd4316, %fd6308, %fd4313;
	sub.f64 	%fd4317, %fd6307, %fd4314;
	sub.f64 	%fd4318, %fd6306, %fd4315;
	fma.rn.f64 	%fd4319, %fd4247, %fd6318, 0d0000000000000000;
	add.f64 	%fd4320, %fd4306, %fd4319;
	add.f64 	%fd4321, %fd4319, %fd4320;
	fma.rn.f64 	%fd4322, %fd4244, %fd6318, 0d0000000000000000;
	add.f64 	%fd4323, %fd4305, %fd4322;
	add.f64 	%fd4324, %fd4322, %fd4323;
	fma.rn.f64 	%fd4325, %fd4241, %fd6318, 0d0000000000000000;
	add.f64 	%fd4326, %fd4304, %fd4325;
	add.f64 	%fd4327, %fd4325, %fd4326;
	fma.rn.f64 	%fd4328, %fd4247, %fd6319, 0d0000000000000000;
	fma.rn.f64 	%fd4329, %fd4239, %fd6319, 0d0000000000000000;
	add.f64 	%fd4330, %fd4321, %fd4329;
	add.f64 	%fd4331, %fd4312, %fd4328;
	fma.rn.f64 	%fd4332, %fd4244, %fd6319, 0d0000000000000000;
	fma.rn.f64 	%fd4333, %fd4237, %fd6319, 0d0000000000000000;
	add.f64 	%fd4334, %fd4324, %fd4333;
	add.f64 	%fd4335, %fd4311, %fd4332;
	fma.rn.f64 	%fd4336, %fd4241, %fd6319, 0d0000000000000000;
	fma.rn.f64 	%fd4337, %fd4236, %fd6319, 0d0000000000000000;
	add.f64 	%fd4338, %fd4327, %fd4337;
	add.f64 	%fd4339, %fd4310, %fd4336;
	fma.rn.f64 	%fd4340, %fd4239, %fd6320, 0d0000000000000000;
	add.f64 	%fd4341, %fd4331, %fd4340;
	add.f64 	%fd4342, %fd4340, %fd4341;
	fma.rn.f64 	%fd4343, %fd4237, %fd6320, 0d0000000000000000;
	add.f64 	%fd4344, %fd4335, %fd4343;
	add.f64 	%fd4345, %fd4343, %fd4344;
	fma.rn.f64 	%fd4346, %fd4236, %fd6320, 0d0000000000000000;
	add.f64 	%fd4347, %fd4339, %fd4346;
	add.f64 	%fd4348, %fd4346, %fd4347;
	mul.f64 	%fd4349, %fd960, %fd4330;
	mul.f64 	%fd4350, %fd961, %fd4334;
	sub.f64 	%fd4351, %fd4349, %fd4350;
	mul.f64 	%fd4352, %fd961, %fd4338;
	mul.f64 	%fd4353, %fd959, %fd4330;
	sub.f64 	%fd4354, %fd4352, %fd4353;
	mul.f64 	%fd4355, %fd959, %fd4334;
	mul.f64 	%fd4356, %fd960, %fd4338;
	sub.f64 	%fd4357, %fd4355, %fd4356;
	add.f64 	%fd4358, %fd4351, %fd4348;
	add.f64 	%fd4359, %fd4354, %fd4345;
	add.f64 	%fd4360, %fd4357, %fd4342;
	mul.f64 	%fd4361, %fd4237, %fd4330;
	mul.f64 	%fd4362, %fd4239, %fd4334;
	mul.f64 	%fd4363, %fd4239, %fd4338;
	mul.f64 	%fd4364, %fd4236, %fd4330;
	mul.f64 	%fd4365, %fd4236, %fd4334;
	mul.f64 	%fd4366, %fd4237, %fd4338;
	sub.f64 	%fd4367, %fd4362, %fd4361;
	add.f64 	%fd6323, %fd4367, 0d0000000000000000;
	sub.f64 	%fd4368, %fd4364, %fd4363;
	add.f64 	%fd6324, %fd4368, 0d0000000000000000;
	sub.f64 	%fd4369, %fd4366, %fd4365;
	add.f64 	%fd6325, %fd4369, 0d0000000000000000;
	add.f64 	%fd6302, %fd4358, %fd6302;
	add.f64 	%fd6301, %fd4359, %fd6301;
	add.f64 	%fd6300, %fd4360, %fd6300;
	sub.f64 	%fd6308, %fd4316, %fd4358;
	sub.f64 	%fd6307, %fd4317, %fd4359;
	sub.f64 	%fd6306, %fd4318, %fd4360;

$L__BB5_300:
	div.rn.f64 	%fd4371, %fd4211, %fd1410;
	add.f64 	%fd4372, %fd4371, 0d0000000000000000;
	mul.f64 	%fd4373, %fd1420, 0d0000000000000000;
	div.rn.f64 	%fd4374, %fd4373, %fd1410;
	sub.f64 	%fd4375, %fd4211, %fd4374;
	sub.f64 	%fd4376, %fd4211, %fd4372;
	fma.rn.f64 	%fd4377, %fd4376, %fd1418, 0d0000000000000000;
	fma.rn.f64 	%fd4378, %fd4376, %fd1419, 0d0000000000000000;
	fma.rn.f64 	%fd4379, %fd1415, %fd4377, %fd4375;
	fma.rn.f64 	%fd4380, %fd1410, %fd4377, 0d0000000000000000;
	div.rn.f64 	%fd4381, %fd4378, %fd1417;
	add.f64 	%fd4382, %fd4381, 0d0000000000000000;
	mul.f64 	%fd4383, %fd4378, %fd1418;
	div.rn.f64 	%fd4384, %fd4383, %fd1417;
	sub.f64 	%fd4385, %fd4211, %fd4384;
	sub.f64 	%fd4386, %fd4211, %fd4382;
	fma.rn.f64 	%fd4387, %fd1414, %fd4386, %fd4380;
	fma.rn.f64 	%fd4388, %fd1415, %fd4386, %fd4372;
	add.f64 	%fd4389, %fd4388, 0d0000000000000000;
	add.f64 	%fd4390, %fd4385, 0d0000000000000000;
	sub.f64 	%fd4391, %fd4211, %fd4385;
	fma.rn.f64 	%fd4392, %fd1410, %fd4391, 0d0000000000000000;
	fma.rn.f64 	%fd4393, %fd1416, %fd4391, %fd4379;
	fma.rn.f64 	%fd4394, %fd1415, %fd4392, %fd4387;
	fma.rn.f64 	%fd4395, %fd1415, %fd4392, %fd4394;
	div.rn.f64 	%fd4396, %fd4395, %fd1410;
	add.f64 	%fd4397, %fd4396, 0d0000000000000000;
	mul.f64 	%fd4398, %fd1415, %fd4395;
	div.rn.f64 	%fd4399, %fd4398, %fd1410;
	sub.f64 	%fd4400, %fd4393, %fd4399;
	add.f64 	%fd4401, %fd4400, 0d0000000000000000;
	add.f64 	%fd6342, %fd4382, %fd6321;
	add.f64 	%fd6343, %fd4389, %fd6322;
	add.f64 	%fd6338, %fd4390, %fd6318;
	add.f64 	%fd6340, %fd4397, %fd6319;
	add.f64 	%fd6341, %fd4401, %fd6320;
	add.f64 	%fd4402, %fd6342, 0d0000000000000000;
	fma.rn.f64 	%fd4403, %fd1411, %fd4402, 0d0000000000000000;
	fma.rn.f64 	%fd4404, %fd1412, %fd4402, 0d0000000000000000;
	fma.rn.f64 	%fd4405, %fd1413, %fd4402, 0d0000000000000000;
	fma.rn.f64 	%fd4406, %fd1407, %fd4402, 0d0000000000000000;
	fma.rn.f64 	%fd4407, %fd1408, %fd4402, 0d0000000000000000;
	fma.rn.f64 	%fd4408, %fd1409, %fd4402, 0d0000000000000000;
	add.f64 	%fd4409, %fd6343, 0d0000000000000000;
	fma.rn.f64 	%fd4410, %fd1411, %fd4409, 0d0000000000000000;
	fma.rn.f64 	%fd4411, %fd1412, %fd4409, 0d0000000000000000;
	fma.rn.f64 	%fd4412, %fd1413, %fd4409, 0d0000000000000000;
	fma.rn.f64 	%fd4413, %fd1404, %fd4409, %fd4406;
	fma.rn.f64 	%fd4414, %fd1405, %fd4409, %fd4407;
	fma.rn.f64 	%fd4415, %fd1406, %fd4409, %fd4408;
	add.f64 	%fd6299, %fd6299, %fd4413;
	add.f64 	%fd6298, %fd6298, %fd4414;
	add.f64 	%fd6297, %fd6297, %fd4415;
	sub.f64 	%fd4416, %fd6305, %fd4413;
	sub.f64 	%fd4417, %fd6304, %fd4414;
	sub.f64 	%fd4418, %fd6303, %fd4415;
	add.f64 	%fd4419, %fd6338, 0d0000000000000000;
	fma.rn.f64 	%fd4420, %fd1409, %fd4419, 0d0000000000000000;
	add.f64 	%fd4421, %fd4420, %fd4405;
	add.f64 	%fd4422, %fd4420, %fd4421;
	fma.rn.f64 	%fd4423, %fd1408, %fd4419, 0d0000000000000000;
	add.f64 	%fd4424, %fd4423, %fd4404;
	add.f64 	%fd4425, %fd4423, %fd4424;
	fma.rn.f64 	%fd4426, %fd1407, %fd4419, 0d0000000000000000;
	add.f64 	%fd4427, %fd4426, %fd4403;
	add.f64 	%fd4428, %fd4426, %fd4427;
	add.f64 	%fd4429, %fd6340, 0d0000000000000000;
	add.f64 	%fd6339, %fd6319, %fd4429;
	add.f64 	%fd4430, %fd6339, 0d0000000000000000;
	fma.rn.f64 	%fd4431, %fd1409, %fd4430, 0d0000000000000000;
	fma.rn.f64 	%fd4432, %fd1406, %fd4430, 0d0000000000000000;
	add.f64 	%fd4433, %fd4432, %fd4422;
	add.f64 	%fd4434, %fd4431, %fd4412;
	fma.rn.f64 	%fd4435, %fd1408, %fd4430, 0d0000000000000000;
	fma.rn.f64 	%fd4436, %fd1405, %fd4430, 0d0000000000000000;
	add.f64 	%fd4437, %fd4436, %fd4425;
	add.f64 	%fd4438, %fd4435, %fd4411;
	fma.rn.f64 	%fd4439, %fd1407, %fd4430, 0d0000000000000000;
	fma.rn.f64 	%fd4440, %fd1404, %fd4430, 0d0000000000000000;
	add.f64 	%fd4441, %fd4440, %fd4428;
	add.f64 	%fd4442, %fd4439, %fd4410;
	add.f64 	%fd4443, %fd6341, 0d0000000000000000;
	fma.rn.f64 	%fd4444, %fd1406, %fd4443, 0d0000000000000000;
	add.f64 	%fd4445, %fd4444, %fd4434;
	add.f64 	%fd4446, %fd4444, %fd4445;
	fma.rn.f64 	%fd4447, %fd1405, %fd4443, 0d0000000000000000;
	add.f64 	%fd4448, %fd4447, %fd4438;
	add.f64 	%fd4449, %fd4447, %fd4448;
	fma.rn.f64 	%fd4450, %fd1404, %fd4443, 0d0000000000000000;
	add.f64 	%fd4451, %fd4450, %fd4442;
	add.f64 	%fd4452, %fd4450, %fd4451;
	mul.f64 	%fd4453, %fd960, %fd4433;
	mul.f64 	%fd4454, %fd961, %fd4437;
	sub.f64 	%fd4455, %fd4453, %fd4454;
	mul.f64 	%fd4456, %fd961, %fd4441;
	mul.f64 	%fd4457, %fd959, %fd4433;
	sub.f64 	%fd4458, %fd4456, %fd4457;
	mul.f64 	%fd4459, %fd959, %fd4437;
	mul.f64 	%fd4460, %fd960, %fd4441;
	sub.f64 	%fd4461, %fd4459, %fd4460;
	add.f64 	%fd4462, %fd4452, %fd4455;
	add.f64 	%fd4463, %fd4449, %fd4458;
	add.f64 	%fd4464, %fd4446, %fd4461;
	mul.f64 	%fd4465, %fd1405, %fd4433;
	mul.f64 	%fd4466, %fd1406, %fd4437;
	sub.f64 	%fd4467, %fd4465, %fd4466;
	mul.f64 	%fd4468, %fd1406, %fd4441;
	mul.f64 	%fd4469, %fd1404, %fd4433;
	sub.f64 	%fd4470, %fd4468, %fd4469;
	mul.f64 	%fd4471, %fd1404, %fd4437;
	mul.f64 	%fd4472, %fd1405, %fd4441;
	sub.f64 	%fd4473, %fd4471, %fd4472;
	sub.f64 	%fd6344, %fd6323, %fd4467;
	sub.f64 	%fd6345, %fd6324, %fd4470;
	sub.f64 	%fd6346, %fd6325, %fd4473;
	add.f64 	%fd6308, %fd6308, %fd4462;
	add.f64 	%fd6307, %fd6307, %fd4463;
	add.f64 	%fd6306, %fd6306, %fd4464;
	sub.f64 	%fd6305, %fd4416, %fd4462;
	sub.f64 	%fd6304, %fd4417, %fd4463;
	sub.f64 	%fd6303, %fd4418, %fd4464;

$L__BB5_301:
	mul.f64 	%fd5560, %fd973, %fd973;
	sub.f64 	%fd5559, %fd957, %fd951;
	sub.f64 	%fd5558, %fd955, %fd948;
	sub.f64 	%fd5557, %fd953, %fd945;
	mov.f64 	%fd4475, 0d0000000000000000;
	div.rn.f64 	%fd4476, %fd4475, %fd965;
	add.f64 	%fd4477, %fd4476, 0d0000000000000000;
	mul.f64 	%fd4478, %fd978, 0d0000000000000000;
	div.rn.f64 	%fd4479, %fd4478, %fd965;
	sub.f64 	%fd4480, %fd4475, %fd4479;
	sub.f64 	%fd4481, %fd4475, %fd4477;
	fma.rn.f64 	%fd4482, %fd4481, %fd976, 0d0000000000000000;
	fma.rn.f64 	%fd4483, %fd4481, %fd977, 0d0000000000000000;
	fma.rn.f64 	%fd4484, %fd973, %fd4482, %fd4480;
	fma.rn.f64 	%fd4485, %fd965, %fd4482, 0d0000000000000000;
	div.rn.f64 	%fd4486, %fd4483, %fd975;
	add.f64 	%fd1494, %fd4486, 0d0000000000000000;
	mul.f64 	%fd4487, %fd4483, %fd976;
	div.rn.f64 	%fd4488, %fd4487, %fd975;
	sub.f64 	%fd4489, %fd4475, %fd4488;
	sub.f64 	%fd4490, %fd4475, %fd1494;
	fma.rn.f64 	%fd4491, %fd972, %fd4490, %fd4485;
	fma.rn.f64 	%fd4492, %fd973, %fd4490, %fd4477;
	add.f64 	%fd1495, %fd4492, 0d0000000000000000;
	add.f64 	%fd1496, %fd4489, 0d0000000000000000;
	sub.f64 	%fd4493, %fd4475, %fd4489;
	fma.rn.f64 	%fd4494, %fd965, %fd4493, 0d0000000000000000;
	fma.rn.f64 	%fd4495, %fd5560, %fd4493, %fd4484;
	fma.rn.f64 	%fd4496, %fd973, %fd4494, %fd4491;
	fma.rn.f64 	%fd4497, %fd973, %fd4494, %fd4496;
	div.rn.f64 	%fd4498, %fd4497, %fd965;
	add.f64 	%fd1497, %fd4498, 0d0000000000000000;
	mul.f64 	%fd4499, %fd973, %fd4497;
	div.rn.f64 	%fd4500, %fd4499, %fd965;
	sub.f64 	%fd4501, %fd4495, %fd4500;
	add.f64 	%fd1498, %fd4501, 0d0000000000000000;
	add.f64 	%fd4502, %fd1494, %fd6342;
	add.f64 	%fd4503, %fd1495, %fd6343;
	add.f64 	%fd4504, %fd1496, %fd6338;
	add.f64 	%fd4505, %fd1497, %fd6340;
	add.f64 	%fd4506, %fd1498, %fd6341;
	add.f64 	%fd4507, %fd4502, 0d0000000000000000;
	fma.rn.f64 	%fd4508, %fd967, %fd4507, 0d0000000000000000;
	fma.rn.f64 	%fd4509, %fd969, %fd4507, 0d0000000000000000;
	fma.rn.f64 	%fd4510, %fd971, %fd4507, 0d0000000000000000;
	fma.rn.f64 	%fd4511, %fd962, %fd4507, 0d0000000000000000;
	fma.rn.f64 	%fd4512, %fd963, %fd4507, 0d0000000000000000;
	fma.rn.f64 	%fd4513, %fd964, %fd4507, 0d0000000000000000;
	add.f64 	%fd4514, %fd4503, 0d0000000000000000;
	fma.rn.f64 	%fd4515, %fd967, %fd4514, 0d0000000000000000;
	fma.rn.f64 	%fd4516, %fd969, %fd4514, 0d0000000000000000;
	fma.rn.f64 	%fd4517, %fd971, %fd4514, 0d0000000000000000;
	fma.rn.f64 	%fd4518, %fd946, %fd4514, %fd4511;
	fma.rn.f64 	%fd4519, %fd949, %fd4514, %fd4512;
	fma.rn.f64 	%fd4520, %fd952, %fd4514, %fd4513;
	add.f64 	%fd1499, %fd6299, %fd4518;
	add.f64 	%fd1500, %fd6298, %fd4519;
	add.f64 	%fd1501, %fd6297, %fd4520;
	sub.f64 	%fd4521, %fd6302, %fd4518;
	sub.f64 	%fd4522, %fd6301, %fd4519;
	sub.f64 	%fd4523, %fd6300, %fd4520;
	add.f64 	%fd4524, %fd4504, 0d0000000000000000;
	fma.rn.f64 	%fd4525, %fd964, %fd4524, 0d0000000000000000;
	add.f64 	%fd4526, %fd4525, %fd4510;
	add.f64 	%fd4527, %fd4525, %fd4526;
	fma.rn.f64 	%fd4528, %fd963, %fd4524, 0d0000000000000000;
	add.f64 	%fd4529, %fd4528, %fd4509;
	add.f64 	%fd4530, %fd4528, %fd4529;
	fma.rn.f64 	%fd4531, %fd962, %fd4524, 0d0000000000000000;
	add.f64 	%fd4532, %fd4531, %fd4508;
	add.f64 	%fd4533, %fd4531, %fd4532;
	add.f64 	%fd4534, %fd4505, 0d0000000000000000;
	add.f64 	%fd4535, %fd6339, %fd4534;
	add.f64 	%fd4536, %fd4535, 0d0000000000000000;
	fma.rn.f64 	%fd4537, %fd964, %fd4536, 0d0000000000000000;
	fma.rn.f64 	%fd4538, %fd952, %fd4536, 0d0000000000000000;
	add.f64 	%fd4539, %fd4538, %fd4527;
	add.f64 	%fd4540, %fd4537, %fd4517;
	fma.rn.f64 	%fd4541, %fd963, %fd4536, 0d0000000000000000;
	fma.rn.f64 	%fd4542, %fd949, %fd4536, 0d0000000000000000;
	add.f64 	%fd4543, %fd4542, %fd4530;
	add.f64 	%fd4544, %fd4541, %fd4516;
	fma.rn.f64 	%fd4545, %fd962, %fd4536, 0d0000000000000000;
	fma.rn.f64 	%fd4546, %fd946, %fd4536, 0d0000000000000000;
	add.f64 	%fd4547, %fd4546, %fd4533;
	add.f64 	%fd4548, %fd4545, %fd4515;
	add.f64 	%fd4549, %fd4506, 0d0000000000000000;
	fma.rn.f64 	%fd4550, %fd952, %fd4549, 0d0000000000000000;
	add.f64 	%fd4551, %fd4550, %fd4540;
	add.f64 	%fd4552, %fd4550, %fd4551;
	fma.rn.f64 	%fd4553, %fd949, %fd4549, 0d0000000000000000;
	add.f64 	%fd4554, %fd4553, %fd4544;
	add.f64 	%fd4555, %fd4553, %fd4554;
	fma.rn.f64 	%fd4556, %fd946, %fd4549, 0d0000000000000000;
	add.f64 	%fd4557, %fd4556, %fd4548;
	add.f64 	%fd4558, %fd4556, %fd4557;
	mul.f64 	%fd4559, %fd960, %fd4539;
	mul.f64 	%fd4560, %fd961, %fd4543;
	sub.f64 	%fd4561, %fd4559, %fd4560;
	mul.f64 	%fd4562, %fd961, %fd4547;
	mul.f64 	%fd4563, %fd959, %fd4539;
	sub.f64 	%fd4564, %fd4562, %fd4563;
	mul.f64 	%fd4565, %fd959, %fd4543;
	mul.f64 	%fd4566, %fd960, %fd4547;
	sub.f64 	%fd4567, %fd4565, %fd4566;
	add.f64 	%fd4568, %fd4558, %fd4561;
	add.f64 	%fd4569, %fd4555, %fd4564;
	add.f64 	%fd4570, %fd4552, %fd4567;
	mul.f64 	%fd4571, %fd949, %fd4539;
	mul.f64 	%fd4572, %fd952, %fd4543;
	sub.f64 	%fd4573, %fd4571, %fd4572;
	mul.f64 	%fd4574, %fd952, %fd4547;
	mul.f64 	%fd4575, %fd946, %fd4539;
	sub.f64 	%fd4576, %fd4574, %fd4575;
	mul.f64 	%fd4577, %fd946, %fd4543;
	mul.f64 	%fd4578, %fd949, %fd4547;
	sub.f64 	%fd4579, %fd4577, %fd4578;
	sub.f64 	%fd4580, %fd6344, %fd4573;
	sub.f64 	%fd4581, %fd6345, %fd4576;
	sub.f64 	%fd4582, %fd6346, %fd4579;
	mul.f64 	%fd4583, %fd5558, %fd4582;
	mul.f64 	%fd4584, %fd5559, %fd4581;
	sub.f64 	%fd4585, %fd4583, %fd4584;
	mul.f64 	%fd4586, %fd5559, %fd4580;
	mul.f64 	%fd4587, %fd5557, %fd4582;
	sub.f64 	%fd4588, %fd4586, %fd4587;
	mul.f64 	%fd4589, %fd5557, %fd4581;
	mul.f64 	%fd4590, %fd5558, %fd4580;
	sub.f64 	%fd4591, %fd4589, %fd4590;
	add.f64 	%fd4592, %fd4568, %fd4585;
	add.f64 	%fd4593, %fd4569, %fd4588;
	add.f64 	%fd4594, %fd4570, %fd4591;
	mul.f64 	%fd4595, %fd949, %fd4582;
	mul.f64 	%fd4596, %fd952, %fd4581;
	mul.f64 	%fd4597, %fd952, %fd4580;
	mul.f64 	%fd4598, %fd946, %fd4582;
	mul.f64 	%fd4599, %fd946, %fd4581;
	mul.f64 	%fd4600, %fd949, %fd4580;
	sub.f64 	%fd4601, %fd4596, %fd4595;
	add.f64 	%fd4602, %fd4601, 0d0000000000000000;
	sub.f64 	%fd4603, %fd4598, %fd4597;
	add.f64 	%fd4604, %fd4603, 0d0000000000000000;
	sub.f64 	%fd4605, %fd4600, %fd4599;
	add.f64 	%fd4606, %fd4605, 0d0000000000000000;
	add.f64 	%fd6371, %fd6308, %fd4602;
	add.f64 	%fd6370, %fd6307, %fd4604;
	add.f64 	%fd6369, %fd6306, %fd4606;
	sub.f64 	%fd4607, %fd4521, %fd4602;
	sub.f64 	%fd4608, %fd4522, %fd4604;
	sub.f64 	%fd4609, %fd4523, %fd4606;
	add.f64 	%fd6368, %fd6305, %fd4592;
	add.f64 	%fd6367, %fd6304, %fd4593;
	add.f64 	%fd6366, %fd6303, %fd4594;
	sub.f64 	%fd6365, %fd4607, %fd4592;
	sub.f64 	%fd6364, %fd4608, %fd4593;
	sub.f64 	%fd6363, %fd4609, %fd4594;
	setp.eq.s16 	%p268, %rs349, 0;
	mov.f64 	%fd6358, %fd4475;
	mov.f64 	%fd6359, %fd4475;
	@%p268 bra 	$L__BB5_321;

	setp.eq.s64 	%p269, %rd135, 0;
	@%p269 bra 	$L__BB5_304;

	cvta.to.global.u64 	%rd317, %rd135;
	mul.lo.s64 	%rd318, %rd75, %rd56;
	add.s64 	%rd319, %rd317, %rd318;
	ld.global.f64 	%fd4610, [%rd319];
	add.f64 	%fd6347, %fd4610, 0d0000000000000000;
	bra.uni 	$L__BB5_306;

$L__BB5_304:
	setp.eq.s64 	%p270, %rd104, 0;
	mov.f64 	%fd6347, 0d0000000000000000;
	@%p270 bra 	$L__BB5_306;

	cvta.to.global.u64 	%rd320, %rd104;
	mul.lo.s64 	%rd321, %rd75, %rd57;
	add.s64 	%rd322, %rd320, %rd321;
	ld.global.f64 	%fd4612, [%rd322];
	add.f64 	%fd6347, %fd4612, 0d0000000000000000;

$L__BB5_306:
	mov.f64 	%fd6359, 0d0000000000000000;
	sub.f64 	%fd4615, %fd6359, %fd6347;
	fma.rn.f64 	%fd4616, %fd6418, %fd4615, 0d0000000000000000;
	fma.rn.f64 	%fd4617, %fd6417, %fd4615, 0d0000000000000000;
	rcp.rn.f64 	%fd4618, %fd6418;
	mul.f64 	%fd4619, %fd4618, 0d3FE0000000000000;
	fma.rn.f64 	%fd6358, %fd4619, %fd4617, 0d0000000000000000;
	fma.rn.f64 	%fd4620, %fd4616, 0d4000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd1515, %fd6416, %fd4620, 0d0000000000000000;
	fma.rn.f64 	%fd1516, %fd6415, %fd4620, 0d0000000000000000;
	@%p213 bra 	$L__BB5_315;

	sub.f64 	%fd4621, %fd989, %fd990;
	div.rn.f64 	%fd6351, %fd4621, %fd990;
	div.rn.f64 	%fd6352, %fd989, %fd990;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r671}, %fd6352;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r672, %temp}, %fd6352;
	}
	setp.gt.s32 	%p272, %r671, 1048575;
	mov.u32 	%r673, -1023;
	mov.f64 	%fd6348, %fd6352;
	@%p272 bra 	$L__BB5_309;

	mul.f64 	%fd6348, %fd6352, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r671}, %fd6348;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r672, %temp}, %fd6348;
	}
	mov.u32 	%r673, -1077;

$L__BB5_309:
	add.s32 	%r622, %r671, -1;
	setp.lt.u32 	%p273, %r622, 2146435071;
	@%p273 bra 	$L__BB5_311;
	bra.uni 	$L__BB5_310;

$L__BB5_311:
	shr.u32 	%r624, %r671, 20;
	add.s32 	%r674, %r673, %r624;
	and.b32  	%r625, %r671, -2146435073;
	or.b32  	%r626, %r625, 1072693248;
	mov.b64 	%fd6349, {%r672, %r626};
	setp.lt.s32 	%p275, %r626, 1073127583;
	@%p275 bra 	$L__BB5_313;

	{
	.reg .b32 %temp;
	mov.b64 	{%r627, %temp}, %fd6349;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r628}, %fd6349;
	}
	add.s32 	%r629, %r628, -1048576;
	mov.b64 	%fd6349, {%r627, %r629};
	add.s32 	%r674, %r674, 1;

$L__BB5_313:
	add.f64 	%fd4624, %fd6349, 0d3FF0000000000000;
	mov.f64 	%fd4625, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd4626, %fd4624;
	neg.f64 	%fd4627, %fd4624;
	fma.rn.f64 	%fd4628, %fd4627, %fd4626, %fd4625;
	fma.rn.f64 	%fd4629, %fd4628, %fd4628, %fd4628;
	fma.rn.f64 	%fd4630, %fd4629, %fd4626, %fd4626;
	add.f64 	%fd4631, %fd6349, 0dBFF0000000000000;
	mul.f64 	%fd4632, %fd4631, %fd4630;
	fma.rn.f64 	%fd4633, %fd4631, %fd4630, %fd4632;
	mul.f64 	%fd4634, %fd4633, %fd4633;
	mov.f64 	%fd4635, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd4636, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd4637, %fd4636, %fd4634, %fd4635;
	mov.f64 	%fd4638, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd4639, %fd4637, %fd4634, %fd4638;
	mov.f64 	%fd4640, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd4641, %fd4639, %fd4634, %fd4640;
	mov.f64 	%fd4642, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd4643, %fd4641, %fd4634, %fd4642;
	mov.f64 	%fd4644, 0d3F624924923BE72D;
	fma.rn.f64 	%fd4645, %fd4643, %fd4634, %fd4644;
	mov.f64 	%fd4646, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd4647, %fd4645, %fd4634, %fd4646;
	mov.f64 	%fd4648, 0d3FB5555555555554;
	fma.rn.f64 	%fd4649, %fd4647, %fd4634, %fd4648;
	sub.f64 	%fd4650, %fd4631, %fd4633;
	add.f64 	%fd4651, %fd4650, %fd4650;
	neg.f64 	%fd4652, %fd4633;
	fma.rn.f64 	%fd4653, %fd4652, %fd4631, %fd4651;
	mul.f64 	%fd4654, %fd4630, %fd4653;
	mul.f64 	%fd4655, %fd4634, %fd4649;
	fma.rn.f64 	%fd4656, %fd4655, %fd4633, %fd4654;
	xor.b32  	%r630, %r674, -2147483648;
	mov.u32 	%r631, -2147483648;
	mov.u32 	%r632, 1127219200;
	mov.b64 	%fd4657, {%r630, %r632};
	mov.b64 	%fd4658, {%r631, %r632};
	sub.f64 	%fd4659, %fd4657, %fd4658;
	mov.f64 	%fd4660, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd4661, %fd4659, %fd4660, %fd4633;
	neg.f64 	%fd4662, %fd4659;
	fma.rn.f64 	%fd4663, %fd4662, %fd4660, %fd4661;
	sub.f64 	%fd4664, %fd4663, %fd4633;
	sub.f64 	%fd4665, %fd4656, %fd4664;
	mov.f64 	%fd4666, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd4667, %fd4659, %fd4666, %fd4665;
	add.f64 	%fd6353, %fd4661, %fd4667;
	bra.uni 	$L__BB5_314;

$L__BB5_310:
	mov.f64 	%fd4622, 0d7FF0000000000000;
	fma.rn.f64 	%fd4623, %fd6348, %fd4622, %fd4622;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r623}, %fd6348;
	}
	mov.b32 	%f4, %r623;
	setp.eq.f32 	%p274, %f4, 0f00000000;
	selp.f64 	%fd6353, 0dFFF0000000000000, %fd4623, %p274;

$L__BB5_314:
	mul.f64 	%fd4668, %fd6351, %fd6353;
	mul.f64 	%fd4669, %fd4668, 0dC000000000000000;
	div.rn.f64 	%fd6354, %fd4669, %fd990;
	mul.f64 	%fd4670, %fd6351, %fd6351;
	div.rn.f64 	%fd6355, %fd4670, %fd989;

$L__BB5_315:
	setp.lt.f64 	%p277, %fd989, %fd990;
	selp.f64 	%fd1534, %fd1516, 0d0000000000000000, %p277;
	@%p213 bra 	$L__BB5_317;

	fma.rn.f64 	%fd4672, %fd1534, %fd1737, 0d0000000000000000;
	mov.f64 	%fd4673, 0d0000000000000000;
	sub.f64 	%fd4674, %fd4673, %fd4672;
	div.rn.f64 	%fd4675, %fd4674, %fd989;
	add.f64 	%fd4676, %fd4675, 0d0000000000000000;
	mul.f64 	%fd4677, %fd4674, %fd6355;
	div.rn.f64 	%fd4678, %fd4677, %fd989;
	sub.f64 	%fd4679, %fd6358, %fd4678;
	fma.rn.f64 	%fd4680, %fd4676, %fd6351, 0d0000000000000000;
	div.rn.f64 	%fd4681, %fd4680, %fd990;
	add.f64 	%fd4682, %fd4681, 0d0000000000000000;
	mul.f64 	%fd4683, %fd6351, %fd4680;
	div.rn.f64 	%fd4684, %fd4683, %fd990;
	sub.f64 	%fd4685, %fd4673, %fd4684;
	add.f64 	%fd4686, %fd4681, %fd4682;
	sub.f64 	%fd4687, %fd4685, %fd4684;
	div.rn.f64 	%fd4688, %fd4672, %fd990;
	add.f64 	%fd4689, %fd4688, 0d0000000000000000;
	mul.f64 	%fd4690, %fd4672, %fd6354;
	div.rn.f64 	%fd4691, %fd4690, %fd990;
	sub.f64 	%fd4692, %fd4687, %fd4691;
	add.f64 	%fd4693, %fd4689, %fd4689;
	sub.f64 	%fd4694, %fd4673, %fd4693;
	fma.rn.f64 	%fd4695, %fd4694, %fd6353, 0d0000000000000000;
	fma.rn.f64 	%fd4696, %fd4694, %fd6351, 0d0000000000000000;
	rcp.rn.f64 	%fd4697, %fd6352;
	fma.rn.f64 	%fd4698, %fd4697, %fd4696, 0d0000000000000000;
	div.rn.f64 	%fd4699, %fd4698, %fd990;
	add.f64 	%fd4700, %fd4699, %fd4679;
	mul.f64 	%fd4701, %fd6352, %fd4698;
	div.rn.f64 	%fd4702, %fd4701, %fd990;
	sub.f64 	%fd4703, %fd4692, %fd4702;
	div.rn.f64 	%fd4704, %fd4695, %fd990;
	add.f64 	%fd4705, %fd4686, %fd4704;
	mul.f64 	%fd4706, %fd6351, %fd4695;
	div.rn.f64 	%fd4707, %fd4706, %fd990;
	sub.f64 	%fd4708, %fd4703, %fd4707;
	add.f64 	%fd6358, %fd4705, %fd4700;
	sub.f64 	%fd6359, %fd4708, %fd4705;

$L__BB5_317:
	fma.rn.f64 	%fd4709, %fd1515, %fd1736, 0d0000000000000000;
	fma.rn.f64 	%fd1539, %fd4709, %fd1738, 0d0000000000000000;
	setp.eq.s64 	%p278, %rd151, 0;
	@%p278 bra 	$L__BB5_319;

	mul.lo.s64 	%rd324, %rd90, %rd59;
	add.s64 	%rd323, %rd151, %rd324;
	// begin inline asm
	{ atom.add.f64 %fd4710,[%rd323],%fd1539; }

	// end inline asm
	bra.uni 	$L__BB5_321;

$L__BB5_319:
	setp.eq.s64 	%p279, %rd128, 0;
	@%p279 bra 	$L__BB5_321;

	mul.lo.s64 	%rd326, %rd90, %rd52;
	add.s64 	%rd325, %rd128, %rd326;
	// begin inline asm
	{ atom.add.f64 %fd4712,[%rd325],%fd1539; }

	// end inline asm

$L__BB5_321:
	add.f64 	%fd4714, %fd6359, 0d0000000000000000;
	fma.rn.f64 	%fd4716, %fd2, %fd4714, 0d0000000000000000;
	add.f64 	%fd1542, %fd6358, 0d0000000000000000;
	sub.f64 	%fd4717, %fd4475, %fd6358;
	fma.rn.f64 	%fd4718, %fd943, %fd4717, %fd4716;
	fma.rn.f64 	%fd1543, %fd943, %fd4717, %fd4718;
	mov.u32 	%r675, 3;
	@%p15 bra 	$L__BB5_327;

	sub.f64 	%fd4719, %fd953, %fd944;
	sub.f64 	%fd4720, %fd955, %fd947;
	mul.f64 	%fd4721, %fd4720, %fd961;
	sub.f64 	%fd4722, %fd957, %fd950;
	mul.f64 	%fd4723, %fd4722, %fd960;
	sub.f64 	%fd4724, %fd4721, %fd4723;
	mul.f64 	%fd4725, %fd4722, %fd959;
	mul.f64 	%fd4726, %fd4719, %fd961;
	sub.f64 	%fd4727, %fd4725, %fd4726;
	mul.f64 	%fd4728, %fd4719, %fd960;
	mul.f64 	%fd4729, %fd4720, %fd959;
	sub.f64 	%fd4730, %fd4728, %fd4729;
	mul.f64 	%fd4731, %fd4719, %fd4719;
	fma.rn.f64 	%fd4732, %fd4720, %fd4720, %fd4731;
	fma.rn.f64 	%fd4733, %fd4722, %fd4722, %fd4732;
	mul.f64 	%fd4734, %fd4720, %fd4727;
	fma.rn.f64 	%fd4735, %fd4719, %fd4724, %fd4734;
	fma.rn.f64 	%fd4736, %fd4722, %fd4730, %fd4735;
	mul.f64 	%fd4737, %fd4727, %fd4727;
	fma.rn.f64 	%fd4738, %fd4724, %fd4724, %fd4737;
	fma.rn.f64 	%fd4739, %fd4730, %fd4730, %fd4738;
	sub.f64 	%fd4740, %fd966, %fd944;
	mul.f64 	%fd4741, %fd4740, %fd4719;
	sub.f64 	%fd4742, %fd968, %fd947;
	fma.rn.f64 	%fd4743, %fd4742, %fd4720, %fd4741;
	sub.f64 	%fd4744, %fd970, %fd950;
	fma.rn.f64 	%fd4745, %fd4744, %fd4722, %fd4743;
	mul.f64 	%fd4746, %fd4742, %fd4727;
	fma.rn.f64 	%fd4747, %fd4740, %fd4724, %fd4746;
	fma.rn.f64 	%fd4748, %fd4744, %fd4730, %fd4747;
	div.rn.f64 	%fd4749, %fd4736, %fd4733;
	mul.f64 	%fd4750, %fd4749, %fd4749;
	mul.f64 	%fd4751, %fd4733, %fd4750;
	sub.f64 	%fd4752, %fd4739, %fd4751;
	mul.f64 	%fd4753, %fd4745, %fd4749;
	sub.f64 	%fd4754, %fd4748, %fd4753;
	div.rn.f64 	%fd4755, %fd4754, %fd4752;
	mul.f64 	%fd4756, %fd4733, %fd4749;
	mul.f64 	%fd4757, %fd4756, %fd4755;
	sub.f64 	%fd4758, %fd4745, %fd4757;
	div.rn.f64 	%fd1544, %fd4758, %fd4733;
	setp.gt.f64 	%p280, %fd1544, 0d0000000000000000;
	setp.lt.f64 	%p281, %fd1544, 0d3FF0000000000000;
	setp.ge.f64 	%p282, %fd4755, 0d0000000000000000;
	and.pred  	%p283, %p280, %p281;
	and.pred  	%p284, %p282, %p283;
	mov.u32 	%r675, 4;
	@%p284 bra 	$L__BB5_327;

	sub.f64 	%fd4759, %fd945, %fd953;
	sub.f64 	%fd4760, %fd948, %fd955;
	mul.f64 	%fd4761, %fd4760, %fd961;
	sub.f64 	%fd4762, %fd951, %fd957;
	mul.f64 	%fd4763, %fd4762, %fd960;
	sub.f64 	%fd4764, %fd4761, %fd4763;
	mul.f64 	%fd4765, %fd4762, %fd959;
	mul.f64 	%fd4766, %fd4759, %fd961;
	sub.f64 	%fd4767, %fd4765, %fd4766;
	mul.f64 	%fd4768, %fd4759, %fd960;
	mul.f64 	%fd4769, %fd4760, %fd959;
	sub.f64 	%fd4770, %fd4768, %fd4769;
	mul.f64 	%fd4771, %fd4759, %fd4759;
	fma.rn.f64 	%fd4772, %fd4760, %fd4760, %fd4771;
	fma.rn.f64 	%fd4773, %fd4762, %fd4762, %fd4772;
	mul.f64 	%fd4774, %fd4760, %fd4767;
	fma.rn.f64 	%fd4775, %fd4759, %fd4764, %fd4774;
	fma.rn.f64 	%fd4776, %fd4762, %fd4770, %fd4775;
	mul.f64 	%fd4777, %fd4767, %fd4767;
	fma.rn.f64 	%fd4778, %fd4764, %fd4764, %fd4777;
	fma.rn.f64 	%fd4779, %fd4770, %fd4770, %fd4778;
	sub.f64 	%fd4780, %fd966, %fd953;
	mul.f64 	%fd4781, %fd4759, %fd4780;
	sub.f64 	%fd4782, %fd968, %fd955;
	fma.rn.f64 	%fd4783, %fd4760, %fd4782, %fd4781;
	sub.f64 	%fd4784, %fd970, %fd957;
	fma.rn.f64 	%fd4785, %fd4762, %fd4784, %fd4783;
	mul.f64 	%fd4786, %fd4782, %fd4767;
	fma.rn.f64 	%fd4787, %fd4780, %fd4764, %fd4786;
	fma.rn.f64 	%fd4788, %fd4784, %fd4770, %fd4787;
	div.rn.f64 	%fd4789, %fd4776, %fd4773;
	mul.f64 	%fd4790, %fd4789, %fd4789;
	mul.f64 	%fd4791, %fd4773, %fd4790;
	sub.f64 	%fd4792, %fd4779, %fd4791;
	mul.f64 	%fd4793, %fd4785, %fd4789;
	sub.f64 	%fd4794, %fd4788, %fd4793;
	div.rn.f64 	%fd4795, %fd4794, %fd4792;
	mul.f64 	%fd4796, %fd4773, %fd4789;
	mul.f64 	%fd4797, %fd4796, %fd4795;
	sub.f64 	%fd4798, %fd4785, %fd4797;
	div.rn.f64 	%fd1545, %fd4798, %fd4773;
	setp.gt.f64 	%p285, %fd1545, 0d0000000000000000;
	setp.lt.f64 	%p286, %fd1545, 0d3FF0000000000000;
	setp.ge.f64 	%p287, %fd4795, 0d0000000000000000;
	and.pred  	%p288, %p285, %p286;
	and.pred  	%p289, %p287, %p288;
	mov.u32 	%r675, 5;
	@%p289 bra 	$L__BB5_327;

	setp.le.f64 	%p290, %fd978, 0d0000000000000000;
	setp.ge.f64 	%p291, %fd1545, 0d3FF0000000000000;
	and.pred  	%p292, %p290, %p291;
	mov.u32 	%r675, 0;
	@%p292 bra 	$L__BB5_327;

	setp.le.f64 	%p293, %fd1544, 0d0000000000000000;
	setp.ge.f64 	%p294, %fd978, 0d3FF0000000000000;
	and.pred  	%p295, %p293, %p294;
	mov.u32 	%r675, 1;
	@%p295 bra 	$L__BB5_327;

	setp.le.f64 	%p296, %fd1545, 0d0000000000000000;
	setp.ge.f64 	%p297, %fd1544, 0d3FF0000000000000;
	and.pred  	%p298, %p296, %p297;
	selp.b32 	%r675, 2, 6, %p298;

$L__BB5_327:
	setp.eq.s32 	%p299, %r675, 0;
	@%p299 bra 	$L__BB5_339;

	setp.eq.s32 	%p300, %r675, 1;
	@%p300 bra 	$L__BB5_338;
	bra.uni 	$L__BB5_329;

$L__BB5_338:
	sub.f64 	%fd5088, %fd966, %fd944;
	add.f64 	%fd5089, %fd5088, %fd5088;
	sub.f64 	%fd5090, %fd968, %fd947;
	add.f64 	%fd5091, %fd5090, %fd5090;
	sub.f64 	%fd5092, %fd970, %fd950;
	add.f64 	%fd5093, %fd5092, %fd5092;
	fma.rn.f64 	%fd5094, %fd5089, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd5095, %fd5091, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd5096, %fd5093, %fd1542, 0d0000000000000000;
	add.f64 	%fd6362, %fd1499, %fd5094;
	add.f64 	%fd6361, %fd1500, %fd5095;
	add.f64 	%fd6360, %fd1501, %fd5096;
	sub.f64 	%fd6368, %fd6368, %fd5094;
	sub.f64 	%fd6367, %fd6367, %fd5095;
	sub.f64 	%fd6366, %fd6366, %fd5096;
	bra.uni 	$L__BB5_340;

$L__BB5_339:
	add.f64 	%fd5097, %fd967, %fd967;
	add.f64 	%fd5098, %fd969, %fd969;
	add.f64 	%fd5099, %fd971, %fd971;
	fma.rn.f64 	%fd5100, %fd5097, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd5101, %fd5098, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd5102, %fd5099, %fd1542, 0d0000000000000000;
	add.f64 	%fd6362, %fd1499, %fd5100;
	add.f64 	%fd6361, %fd1500, %fd5101;
	add.f64 	%fd6360, %fd1501, %fd5102;
	sub.f64 	%fd6365, %fd6365, %fd5100;
	sub.f64 	%fd6364, %fd6364, %fd5101;
	sub.f64 	%fd6363, %fd6363, %fd5102;
	bra.uni 	$L__BB5_340;

$L__BB5_329:
	setp.eq.s32 	%p301, %r675, 2;
	@%p301 bra 	$L__BB5_337;
	bra.uni 	$L__BB5_330;

$L__BB5_337:
	sub.f64 	%fd5079, %fd966, %fd953;
	add.f64 	%fd5080, %fd5079, %fd5079;
	sub.f64 	%fd5081, %fd968, %fd955;
	add.f64 	%fd5082, %fd5081, %fd5081;
	sub.f64 	%fd5083, %fd970, %fd957;
	add.f64 	%fd5084, %fd5083, %fd5083;
	fma.rn.f64 	%fd5085, %fd5080, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd5086, %fd5082, %fd1542, 0d0000000000000000;
	fma.rn.f64 	%fd5087, %fd5084, %fd1542, 0d0000000000000000;
	add.f64 	%fd6362, %fd1499, %fd5085;
	add.f64 	%fd6361, %fd1500, %fd5086;
	add.f64 	%fd6360, %fd1501, %fd5087;
	sub.f64 	%fd6371, %fd6371, %fd5085;
	sub.f64 	%fd6370, %fd6370, %fd5086;
	sub.f64 	%fd6369, %fd6369, %fd5087;
	bra.uni 	$L__BB5_340;

$L__BB5_330:
	setp.eq.s32 	%p302, %r675, 3;
	@%p302 bra 	$L__BB5_336;
	bra.uni 	$L__BB5_331;

$L__BB5_336:
	sub.f64 	%fd5009, %fd945, %fd966;
	sub.f64 	%fd5010, %fd950, %fd970;
	sub.f64 	%fd5011, %fd948, %fd968;
	mul.f64 	%fd5012, %fd5011, %fd5010;
	sub.f64 	%fd5013, %fd947, %fd968;
	sub.f64 	%fd5014, %fd951, %fd970;
	mul.f64 	%fd5015, %fd5014, %fd5013;
	sub.f64 	%fd5016, %fd5012, %fd5015;
	sub.f64 	%fd5017, %fd944, %fd966;
	mul.f64 	%fd5018, %fd5014, %fd5017;
	mul.f64 	%fd5019, %fd5009, %fd5010;
	sub.f64 	%fd5020, %fd5018, %fd5019;
	mul.f64 	%fd5021, %fd5009, %fd5013;
	mul.f64 	%fd5022, %fd5011, %fd5017;
	sub.f64 	%fd5023, %fd5021, %fd5022;
	mul.f64 	%fd5024, %fd5020, %fd5020;
	fma.rn.f64 	%fd5025, %fd5016, %fd5016, %fd5024;
	fma.rn.f64 	%fd5026, %fd5023, %fd5023, %fd5025;
	div.rn.f64 	%fd5027, %fd5026, %fd965;
	div.rn.f64 	%fd5028, %fd1542, %fd965;
	add.f64 	%fd5029, %fd5028, 0d0000000000000000;
	mov.f64 	%fd5030, 0d0000000000000000;
	mul.f64 	%fd5031, %fd5027, %fd1542;
	div.rn.f64 	%fd5032, %fd5031, %fd965;
	sub.f64 	%fd5033, %fd5030, %fd5032;
	add.f64 	%fd5034, %fd946, %fd946;
	add.f64 	%fd5035, %fd949, %fd949;
	add.f64 	%fd5036, %fd952, %fd952;
	fma.rn.f64 	%fd5037, %fd5034, %fd5033, 0d0000000000000000;
	fma.rn.f64 	%fd5038, %fd5035, %fd5033, 0d0000000000000000;
	fma.rn.f64 	%fd5039, %fd5036, %fd5033, 0d0000000000000000;
	add.f64 	%fd5040, %fd5016, %fd5016;
	add.f64 	%fd5041, %fd5020, %fd5020;
	add.f64 	%fd5042, %fd5023, %fd5023;
	fma.rn.f64 	%fd5043, %fd5040, %fd5029, 0d0000000000000000;
	fma.rn.f64 	%fd5044, %fd5041, %fd5029, 0d0000000000000000;
	fma.rn.f64 	%fd5045, %fd5042, %fd5029, 0d0000000000000000;
	mul.f64 	%fd5046, %fd5013, %fd5045;
	mul.f64 	%fd5047, %fd5010, %fd5044;
	sub.f64 	%fd5048, %fd5046, %fd5047;
	mul.f64 	%fd5049, %fd5010, %fd5043;
	mul.f64 	%fd5050, %fd5017, %fd5045;
	sub.f64 	%fd5051, %fd5049, %fd5050;
	mul.f64 	%fd5052, %fd5017, %fd5044;
	mul.f64 	%fd5053, %fd5013, %fd5043;
	sub.f64 	%fd5054, %fd5052, %fd5053;
	add.f64 	%fd5055, %fd5048, 0d0000000000000000;
	add.f64 	%fd5056, %fd5051, 0d0000000000000000;
	add.f64 	%fd5057, %fd5054, 0d0000000000000000;
	mul.f64 	%fd5058, %fd5011, %fd5045;
	mul.f64 	%fd5059, %fd5014, %fd5044;
	mul.f64 	%fd5060, %fd5014, %fd5043;
	mul.f64 	%fd5061, %fd5009, %fd5045;
	mul.f64 	%fd5062, %fd5009, %fd5044;
	mul.f64 	%fd5063, %fd5011, %fd5043;
	sub.f64 	%fd5064, %fd5059, %fd5058;
	add.f64 	%fd5065, %fd5064, 0d0000000000000000;
	sub.f64 	%fd5066, %fd5061, %fd5060;
	add.f64 	%fd5067, %fd5066, 0d0000000000000000;
	sub.f64 	%fd5068, %fd5063, %fd5062;
	add.f64 	%fd5069, %fd5068, 0d0000000000000000;
	add.f64 	%fd5070, %fd6368, %fd5037;
	add.f64 	%fd5071, %fd6367, %fd5038;
	add.f64 	%fd5072, %fd6366, %fd5039;
	sub.f64 	%fd5073, %fd6365, %fd5037;
	sub.f64 	%fd5074, %fd6364, %fd5038;
	sub.f64 	%fd5075, %fd6363, %fd5039;
	add.f64 	%fd6368, %fd5070, %fd5065;
	add.f64 	%fd6367, %fd5071, %fd5067;
	add.f64 	%fd6366, %fd5072, %fd5069;
	sub.f64 	%fd5076, %fd1499, %fd5065;
	sub.f64 	%fd5077, %fd1500, %fd5067;
	sub.f64 	%fd5078, %fd1501, %fd5069;
	add.f64 	%fd6365, %fd5073, %fd5055;
	add.f64 	%fd6364, %fd5074, %fd5056;
	add.f64 	%fd6363, %fd5075, %fd5057;
	sub.f64 	%fd6362, %fd5076, %fd5055;
	sub.f64 	%fd6361, %fd5077, %fd5056;
	sub.f64 	%fd6360, %fd5078, %fd5057;
	bra.uni 	$L__BB5_340;

$L__BB5_331:
	setp.eq.s32 	%p303, %r675, 4;
	@%p303 bra 	$L__BB5_335;
	bra.uni 	$L__BB5_332;

$L__BB5_335:
	sub.f64 	%fd4933, %fd944, %fd966;
	sub.f64 	%fd4934, %fd957, %fd970;
	sub.f64 	%fd4935, %fd947, %fd968;
	mul.f64 	%fd4936, %fd4935, %fd4934;
	sub.f64 	%fd4937, %fd955, %fd968;
	sub.f64 	%fd4938, %fd950, %fd970;
	mul.f64 	%fd4939, %fd4938, %fd4937;
	sub.f64 	%fd4940, %fd4936, %fd4939;
	sub.f64 	%fd4941, %fd953, %fd966;
	mul.f64 	%fd4942, %fd4938, %fd4941;
	mul.f64 	%fd4943, %fd4933, %fd4934;
	sub.f64 	%fd4944, %fd4942, %fd4943;
	mul.f64 	%fd4945, %fd4933, %fd4937;
	mul.f64 	%fd4946, %fd4935, %fd4941;
	sub.f64 	%fd4947, %fd4945, %fd4946;
	mul.f64 	%fd4948, %fd4944, %fd4944;
	fma.rn.f64 	%fd4949, %fd4940, %fd4940, %fd4948;
	fma.rn.f64 	%fd4950, %fd4947, %fd4947, %fd4949;
	sub.f64 	%fd4951, %fd953, %fd944;
	mul.f64 	%fd4952, %fd4951, %fd4951;
	sub.f64 	%fd4953, %fd955, %fd947;
	fma.rn.f64 	%fd4954, %fd4953, %fd4953, %fd4952;
	sub.f64 	%fd4955, %fd957, %fd950;
	fma.rn.f64 	%fd4956, %fd4955, %fd4955, %fd4954;
	div.rn.f64 	%fd4957, %fd4950, %fd4956;
	div.rn.f64 	%fd4958, %fd1542, %fd4956;
	add.f64 	%fd4959, %fd4958, 0d0000000000000000;
	mov.f64 	%fd4960, 0d0000000000000000;
	mul.f64 	%fd4961, %fd4957, %fd1542;
	div.rn.f64 	%fd4962, %fd4961, %fd4956;
	sub.f64 	%fd4963, %fd4960, %fd4962;
	add.f64 	%fd4964, %fd4951, %fd4951;
	add.f64 	%fd4965, %fd4953, %fd4953;
	add.f64 	%fd4966, %fd4955, %fd4955;
	fma.rn.f64 	%fd4967, %fd4964, %fd4963, 0d0000000000000000;
	fma.rn.f64 	%fd4968, %fd4965, %fd4963, 0d0000000000000000;
	fma.rn.f64 	%fd4969, %fd4966, %fd4963, 0d0000000000000000;
	add.f64 	%fd4970, %fd4940, %fd4940;
	add.f64 	%fd4971, %fd4944, %fd4944;
	add.f64 	%fd4972, %fd4947, %fd4947;
	fma.rn.f64 	%fd4973, %fd4970, %fd4959, 0d0000000000000000;
	fma.rn.f64 	%fd4974, %fd4971, %fd4959, 0d0000000000000000;
	fma.rn.f64 	%fd4975, %fd4972, %fd4959, 0d0000000000000000;
	mul.f64 	%fd4976, %fd4937, %fd4975;
	mul.f64 	%fd4977, %fd4934, %fd4974;
	sub.f64 	%fd4978, %fd4976, %fd4977;
	mul.f64 	%fd4979, %fd4934, %fd4973;
	mul.f64 	%fd4980, %fd4941, %fd4975;
	sub.f64 	%fd4981, %fd4979, %fd4980;
	mul.f64 	%fd4982, %fd4941, %fd4974;
	mul.f64 	%fd4983, %fd4937, %fd4973;
	sub.f64 	%fd4984, %fd4982, %fd4983;
	add.f64 	%fd4985, %fd4978, 0d0000000000000000;
	add.f64 	%fd4986, %fd4981, 0d0000000000000000;
	add.f64 	%fd4987, %fd4984, 0d0000000000000000;
	mul.f64 	%fd4988, %fd4935, %fd4975;
	mul.f64 	%fd4989, %fd4938, %fd4974;
	mul.f64 	%fd4990, %fd4938, %fd4973;
	mul.f64 	%fd4991, %fd4933, %fd4975;
	mul.f64 	%fd4992, %fd4933, %fd4974;
	mul.f64 	%fd4993, %fd4935, %fd4973;
	sub.f64 	%fd4994, %fd4989, %fd4988;
	add.f64 	%fd4995, %fd4994, 0d0000000000000000;
	sub.f64 	%fd4996, %fd4991, %fd4990;
	add.f64 	%fd4997, %fd4996, 0d0000000000000000;
	sub.f64 	%fd4998, %fd4993, %fd4992;
	add.f64 	%fd4999, %fd4998, 0d0000000000000000;
	add.f64 	%fd5000, %fd6371, %fd4967;
	add.f64 	%fd5001, %fd6370, %fd4968;
	add.f64 	%fd5002, %fd6369, %fd4969;
	sub.f64 	%fd5003, %fd6368, %fd4967;
	sub.f64 	%fd5004, %fd6367, %fd4968;
	sub.f64 	%fd5005, %fd6366, %fd4969;
	add.f64 	%fd6371, %fd5000, %fd4995;
	add.f64 	%fd6370, %fd5001, %fd4997;
	add.f64 	%fd6369, %fd5002, %fd4999;
	sub.f64 	%fd5006, %fd1499, %fd4995;
	sub.f64 	%fd5007, %fd1500, %fd4997;
	sub.f64 	%fd5008, %fd1501, %fd4999;
	add.f64 	%fd6368, %fd5003, %fd4985;
	add.f64 	%fd6367, %fd5004, %fd4986;
	add.f64 	%fd6366, %fd5005, %fd4987;
	sub.f64 	%fd6362, %fd5006, %fd4985;
	sub.f64 	%fd6361, %fd5007, %fd4986;
	sub.f64 	%fd6360, %fd5008, %fd4987;
	bra.uni 	$L__BB5_340;

$L__BB5_332:
	setp.eq.s32 	%p304, %r675, 5;
	@%p304 bra 	$L__BB5_334;
	bra.uni 	$L__BB5_333;

$L__BB5_334:
	sub.f64 	%fd4857, %fd953, %fd966;
	sub.f64 	%fd4858, %fd951, %fd970;
	sub.f64 	%fd4859, %fd955, %fd968;
	mul.f64 	%fd4860, %fd4858, %fd4859;
	sub.f64 	%fd4861, %fd948, %fd968;
	sub.f64 	%fd4862, %fd957, %fd970;
	mul.f64 	%fd4863, %fd4861, %fd4862;
	sub.f64 	%fd4864, %fd4860, %fd4863;
	sub.f64 	%fd4865, %fd945, %fd966;
	mul.f64 	%fd4866, %fd4865, %fd4862;
	mul.f64 	%fd4867, %fd4858, %fd4857;
	sub.f64 	%fd4868, %fd4866, %fd4867;
	mul.f64 	%fd4869, %fd4861, %fd4857;
	mul.f64 	%fd4870, %fd4865, %fd4859;
	sub.f64 	%fd4871, %fd4869, %fd4870;
	mul.f64 	%fd4872, %fd4868, %fd4868;
	fma.rn.f64 	%fd4873, %fd4864, %fd4864, %fd4872;
	fma.rn.f64 	%fd4874, %fd4871, %fd4871, %fd4873;
	sub.f64 	%fd4875, %fd945, %fd953;
	mul.f64 	%fd4876, %fd4875, %fd4875;
	sub.f64 	%fd4877, %fd948, %fd955;
	fma.rn.f64 	%fd4878, %fd4877, %fd4877, %fd4876;
	sub.f64 	%fd4879, %fd951, %fd957;
	fma.rn.f64 	%fd4880, %fd4879, %fd4879, %fd4878;
	div.rn.f64 	%fd4881, %fd4874, %fd4880;
	div.rn.f64 	%fd4882, %fd1542, %fd4880;
	add.f64 	%fd4883, %fd4882, 0d0000000000000000;
	mov.f64 	%fd4884, 0d0000000000000000;
	mul.f64 	%fd4885, %fd4881, %fd1542;
	div.rn.f64 	%fd4886, %fd4885, %fd4880;
	sub.f64 	%fd4887, %fd4884, %fd4886;
	add.f64 	%fd4888, %fd4875, %fd4875;
	add.f64 	%fd4889, %fd4877, %fd4877;
	add.f64 	%fd4890, %fd4879, %fd4879;
	fma.rn.f64 	%fd4891, %fd4888, %fd4887, 0d0000000000000000;
	fma.rn.f64 	%fd4892, %fd4889, %fd4887, 0d0000000000000000;
	fma.rn.f64 	%fd4893, %fd4890, %fd4887, 0d0000000000000000;
	add.f64 	%fd4894, %fd4864, %fd4864;
	add.f64 	%fd4895, %fd4868, %fd4868;
	add.f64 	%fd4896, %fd4871, %fd4871;
	fma.rn.f64 	%fd4897, %fd4894, %fd4883, 0d0000000000000000;
	fma.rn.f64 	%fd4898, %fd4895, %fd4883, 0d0000000000000000;
	fma.rn.f64 	%fd4899, %fd4896, %fd4883, 0d0000000000000000;
	mul.f64 	%fd4900, %fd4861, %fd4899;
	mul.f64 	%fd4901, %fd4858, %fd4898;
	sub.f64 	%fd4902, %fd4900, %fd4901;
	mul.f64 	%fd4903, %fd4858, %fd4897;
	mul.f64 	%fd4904, %fd4865, %fd4899;
	sub.f64 	%fd4905, %fd4903, %fd4904;
	mul.f64 	%fd4906, %fd4865, %fd4898;
	mul.f64 	%fd4907, %fd4861, %fd4897;
	sub.f64 	%fd4908, %fd4906, %fd4907;
	add.f64 	%fd4909, %fd4902, 0d0000000000000000;
	add.f64 	%fd4910, %fd4905, 0d0000000000000000;
	add.f64 	%fd4911, %fd4908, 0d0000000000000000;
	mul.f64 	%fd4912, %fd4859, %fd4899;
	mul.f64 	%fd4913, %fd4862, %fd4898;
	mul.f64 	%fd4914, %fd4862, %fd4897;
	mul.f64 	%fd4915, %fd4857, %fd4899;
	mul.f64 	%fd4916, %fd4857, %fd4898;
	mul.f64 	%fd4917, %fd4859, %fd4897;
	sub.f64 	%fd4918, %fd4913, %fd4912;
	add.f64 	%fd4919, %fd4918, 0d0000000000000000;
	sub.f64 	%fd4920, %fd4915, %fd4914;
	add.f64 	%fd4921, %fd4920, 0d0000000000000000;
	sub.f64 	%fd4922, %fd4917, %fd4916;
	add.f64 	%fd4923, %fd4922, 0d0000000000000000;
	add.f64 	%fd4924, %fd6365, %fd4891;
	add.f64 	%fd4925, %fd6364, %fd4892;
	add.f64 	%fd4926, %fd6363, %fd4893;
	sub.f64 	%fd4927, %fd6371, %fd4891;
	sub.f64 	%fd4928, %fd6370, %fd4892;
	sub.f64 	%fd4929, %fd6369, %fd4893;
	add.f64 	%fd6365, %fd4924, %fd4919;
	add.f64 	%fd6364, %fd4925, %fd4921;
	add.f64 	%fd6363, %fd4926, %fd4923;
	sub.f64 	%fd4930, %fd1499, %fd4919;
	sub.f64 	%fd4931, %fd1500, %fd4921;
	sub.f64 	%fd4932, %fd1501, %fd4923;
	add.f64 	%fd6371, %fd4927, %fd4909;
	add.f64 	%fd6370, %fd4928, %fd4910;
	add.f64 	%fd6369, %fd4929, %fd4911;
	sub.f64 	%fd6362, %fd4930, %fd4909;
	sub.f64 	%fd6361, %fd4931, %fd4910;
	sub.f64 	%fd6360, %fd4932, %fd4911;
	bra.uni 	$L__BB5_340;

$L__BB5_33:
	setp.eq.s32 	%p54, %r647, 6;
	@%p54 bra 	$L__BB5_37;
	bra.uni 	$L__BB5_34;

$L__BB5_37:
	sub.f64 	%fd1821, %fd13, %fd23;
	sub.f64 	%fd1822, %fd19, %fd29;
	mul.f64 	%fd1823, %fd32, %fd1822;
	sub.f64 	%fd1824, %fd16, %fd26;
	mul.f64 	%fd1825, %fd1824, %fd33;
	sub.f64 	%fd1826, %fd1823, %fd1825;
	mul.f64 	%fd1827, %fd1821, %fd33;
	mul.f64 	%fd1828, %fd31, %fd1822;
	sub.f64 	%fd1829, %fd1827, %fd1828;
	mul.f64 	%fd1830, %fd31, %fd1824;
	mul.f64 	%fd1831, %fd1821, %fd32;
	sub.f64 	%fd1832, %fd1830, %fd1831;
	mul.f64 	%fd1833, %fd1829, %fd1829;
	fma.rn.f64 	%fd1834, %fd1826, %fd1826, %fd1833;
	fma.rn.f64 	%fd1835, %fd1832, %fd1832, %fd1834;
	div.rn.f64 	%fd5580, %fd1835, %fd34;
	bra.uni 	$L__BB5_44;

$L__BB5_333:
	sub.f64 	%fd5563, %fd957, %fd951;
	sub.f64 	%fd5562, %fd955, %fd948;
	sub.f64 	%fd5561, %fd953, %fd945;
	mul.f64 	%fd4799, %fd969, %fd960;
	fma.rn.f64 	%fd4800, %fd967, %fd959, %fd4799;
	fma.rn.f64 	%fd4801, %fd971, %fd961, %fd4800;
	mul.f64 	%fd4802, %fd4801, %fd4801;
	mul.f64 	%fd4803, %fd960, %fd960;
	fma.rn.f64 	%fd4804, %fd959, %fd959, %fd4803;
	fma.rn.f64 	%fd4805, %fd961, %fd961, %fd4804;
	div.rn.f64 	%fd4806, %fd4802, %fd4805;
	div.rn.f64 	%fd4807, %fd1542, %fd4805;
	add.f64 	%fd4808, %fd4807, 0d0000000000000000;
	mov.f64 	%fd4809, 0d0000000000000000;
	mul.f64 	%fd4810, %fd4806, %fd1542;
	div.rn.f64 	%fd4811, %fd4810, %fd4805;
	sub.f64 	%fd4812, %fd4809, %fd4811;
	add.f64 	%fd4813, %fd959, %fd959;
	add.f64 	%fd4814, %fd960, %fd960;
	add.f64 	%fd4815, %fd961, %fd961;
	fma.rn.f64 	%fd4816, %fd4813, %fd4812, 0d0000000000000000;
	fma.rn.f64 	%fd4817, %fd4814, %fd4812, 0d0000000000000000;
	fma.rn.f64 	%fd4818, %fd4815, %fd4812, 0d0000000000000000;
	fma.rn.f64 	%fd4819, %fd4801, %fd4808, 0d0000000000000000;
	fma.rn.f64 	%fd4820, %fd4801, %fd4808, %fd4819;
	fma.rn.f64 	%fd4821, %fd959, %fd4820, 0d0000000000000000;
	fma.rn.f64 	%fd4822, %fd960, %fd4820, 0d0000000000000000;
	fma.rn.f64 	%fd4823, %fd961, %fd4820, 0d0000000000000000;
	fma.rn.f64 	%fd4824, %fd967, %fd4820, %fd4816;
	fma.rn.f64 	%fd4825, %fd969, %fd4820, %fd4817;
	fma.rn.f64 	%fd4826, %fd971, %fd4820, %fd4818;
	mul.f64 	%fd4827, %fd5562, %fd4826;
	mul.f64 	%fd4828, %fd5563, %fd4825;
	sub.f64 	%fd4829, %fd4827, %fd4828;
	mul.f64 	%fd4830, %fd5563, %fd4824;
	mul.f64 	%fd4831, %fd5561, %fd4826;
	sub.f64 	%fd4832, %fd4830, %fd4831;
	mul.f64 	%fd4833, %fd5561, %fd4825;
	mul.f64 	%fd4834, %fd5562, %fd4824;
	sub.f64 	%fd4835, %fd4833, %fd4834;
	add.f64 	%fd4836, %fd4829, 0d0000000000000000;
	add.f64 	%fd4837, %fd4832, 0d0000000000000000;
	add.f64 	%fd4838, %fd4835, 0d0000000000000000;
	mul.f64 	%fd4839, %fd949, %fd4826;
	mul.f64 	%fd4840, %fd952, %fd4825;
	mul.f64 	%fd4841, %fd952, %fd4824;
	mul.f64 	%fd4842, %fd946, %fd4826;
	mul.f64 	%fd4843, %fd946, %fd4825;
	mul.f64 	%fd4844, %fd949, %fd4824;
	sub.f64 	%fd4845, %fd4840, %fd4839;
	add.f64 	%fd4846, %fd4845, 0d0000000000000000;
	sub.f64 	%fd4847, %fd4842, %fd4841;
	add.f64 	%fd4848, %fd4847, 0d0000000000000000;
	sub.f64 	%fd4849, %fd4844, %fd4843;
	add.f64 	%fd4850, %fd4849, 0d0000000000000000;
	add.f64 	%fd6362, %fd1499, %fd4821;
	add.f64 	%fd6361, %fd1500, %fd4822;
	add.f64 	%fd6360, %fd1501, %fd4823;
	sub.f64 	%fd4851, %fd6365, %fd4821;
	sub.f64 	%fd4852, %fd6364, %fd4822;
	sub.f64 	%fd4853, %fd6363, %fd4823;
	add.f64 	%fd6371, %fd6371, %fd4846;
	add.f64 	%fd6370, %fd6370, %fd4848;
	add.f64 	%fd6369, %fd6369, %fd4850;
	sub.f64 	%fd4854, %fd4851, %fd4846;
	sub.f64 	%fd4855, %fd4852, %fd4848;
	sub.f64 	%fd4856, %fd4853, %fd4850;
	add.f64 	%fd6368, %fd6368, %fd4836;
	add.f64 	%fd6367, %fd6367, %fd4837;
	add.f64 	%fd6366, %fd6366, %fd4838;
	sub.f64 	%fd6365, %fd4854, %fd4836;
	sub.f64 	%fd6364, %fd4855, %fd4837;
	sub.f64 	%fd6363, %fd4856, %fd4838;

$L__BB5_340:
	mov.f64 	%fd6401, 0d0000000000000000;
	mov.f64 	%fd6402, %fd6401;
	mov.f64 	%fd6403, %fd6401;
	mov.f64 	%fd6404, %fd6401;
	mov.f64 	%fd6405, %fd6401;
	mov.f64 	%fd6406, %fd6401;
	mov.f64 	%fd6407, %fd6401;
	mov.f64 	%fd6408, %fd6401;
	mov.f64 	%fd6409, %fd6401;
	@%p15 bra 	$L__BB5_344;

	sub.f64 	%fd1615, %fd953, %fd944;
	sub.f64 	%fd1616, %fd955, %fd947;
	mul.f64 	%fd5120, %fd1616, %fd961;
	sub.f64 	%fd1617, %fd957, %fd950;
	mul.f64 	%fd5121, %fd1617, %fd960;
	sub.f64 	%fd1618, %fd5120, %fd5121;
	mul.f64 	%fd5122, %fd1617, %fd959;
	mul.f64 	%fd5123, %fd1615, %fd961;
	sub.f64 	%fd1619, %fd5122, %fd5123;
	mul.f64 	%fd5124, %fd1615, %fd960;
	mul.f64 	%fd5125, %fd1616, %fd959;
	sub.f64 	%fd1620, %fd5124, %fd5125;
	mul.f64 	%fd5126, %fd1615, %fd1615;
	fma.rn.f64 	%fd5127, %fd1616, %fd1616, %fd5126;
	fma.rn.f64 	%fd1621, %fd1617, %fd1617, %fd5127;
	mul.f64 	%fd5128, %fd1616, %fd1619;
	fma.rn.f64 	%fd5129, %fd1615, %fd1618, %fd5128;
	fma.rn.f64 	%fd5130, %fd1617, %fd1620, %fd5129;
	mul.f64 	%fd5131, %fd1619, %fd1619;
	fma.rn.f64 	%fd5132, %fd1618, %fd1618, %fd5131;
	fma.rn.f64 	%fd5133, %fd1620, %fd1620, %fd5132;
	sub.f64 	%fd1622, %fd966, %fd944;
	mul.f64 	%fd5134, %fd1622, %fd1615;
	sub.f64 	%fd1623, %fd968, %fd947;
	fma.rn.f64 	%fd5135, %fd1623, %fd1616, %fd5134;
	sub.f64 	%fd1624, %fd970, %fd950;
	fma.rn.f64 	%fd1625, %fd1624, %fd1617, %fd5135;
	mul.f64 	%fd5136, %fd1623, %fd1619;
	fma.rn.f64 	%fd5137, %fd1622, %fd1618, %fd5136;
	fma.rn.f64 	%fd5138, %fd1624, %fd1620, %fd5137;
	div.rn.f64 	%fd1626, %fd5130, %fd1621;
	mul.f64 	%fd1627, %fd1626, %fd1626;
	mul.f64 	%fd5139, %fd1621, %fd1627;
	sub.f64 	%fd1628, %fd5133, %fd5139;
	mul.f64 	%fd5140, %fd1625, %fd1626;
	sub.f64 	%fd5141, %fd5138, %fd5140;
	div.rn.f64 	%fd1629, %fd5141, %fd1628;
	mul.f64 	%fd1630, %fd1621, %fd1626;
	mul.f64 	%fd5142, %fd1630, %fd1629;
	sub.f64 	%fd5143, %fd1625, %fd5142;
	div.rn.f64 	%fd1631, %fd5143, %fd1621;
	setp.gt.f64 	%p305, %fd1631, 0d0000000000000000;
	mov.f64 	%fd5119, 0d0000000000000000;
	setp.lt.f64 	%p306, %fd1631, 0d3FF0000000000000;
	setp.ge.f64 	%p307, %fd1629, 0d0000000000000000;
	and.pred  	%p308, %p305, %p306;
	and.pred  	%p309, %p307, %p308;
	mov.f64 	%fd6381, %fd5119;
	mov.f64 	%fd6382, %fd5119;
	mov.f64 	%fd6383, %fd5119;
	mov.f64 	%fd6384, %fd5119;
	mov.f64 	%fd6385, %fd5119;
	mov.f64 	%fd6386, %fd5119;
	mov.f64 	%fd6387, %fd5119;
	mov.f64 	%fd6388, %fd5119;
	@%p309 bra 	$L__BB5_343;

	sub.f64 	%fd5144, %fd945, %fd953;
	sub.f64 	%fd5145, %fd948, %fd955;
	mul.f64 	%fd5146, %fd5145, %fd961;
	sub.f64 	%fd5147, %fd951, %fd957;
	mul.f64 	%fd5148, %fd5147, %fd960;
	sub.f64 	%fd5149, %fd5146, %fd5148;
	mul.f64 	%fd5150, %fd5147, %fd959;
	mul.f64 	%fd5151, %fd5144, %fd961;
	sub.f64 	%fd5152, %fd5150, %fd5151;
	mul.f64 	%fd5153, %fd5144, %fd960;
	mul.f64 	%fd5154, %fd5145, %fd959;
	sub.f64 	%fd5155, %fd5153, %fd5154;
	mul.f64 	%fd5156, %fd5144, %fd5144;
	fma.rn.f64 	%fd5157, %fd5145, %fd5145, %fd5156;
	fma.rn.f64 	%fd5158, %fd5147, %fd5147, %fd5157;
	mul.f64 	%fd5159, %fd5145, %fd5152;
	fma.rn.f64 	%fd5160, %fd5144, %fd5149, %fd5159;
	fma.rn.f64 	%fd5161, %fd5147, %fd5155, %fd5160;
	mul.f64 	%fd5162, %fd5152, %fd5152;
	fma.rn.f64 	%fd5163, %fd5149, %fd5149, %fd5162;
	fma.rn.f64 	%fd5164, %fd5155, %fd5155, %fd5163;
	sub.f64 	%fd5165, %fd966, %fd953;
	mul.f64 	%fd5166, %fd5144, %fd5165;
	sub.f64 	%fd5167, %fd968, %fd955;
	fma.rn.f64 	%fd5168, %fd5145, %fd5167, %fd5166;
	sub.f64 	%fd5169, %fd970, %fd957;
	fma.rn.f64 	%fd5170, %fd5147, %fd5169, %fd5168;
	mul.f64 	%fd5171, %fd5167, %fd5152;
	fma.rn.f64 	%fd5172, %fd5165, %fd5149, %fd5171;
	fma.rn.f64 	%fd5173, %fd5169, %fd5155, %fd5172;
	div.rn.f64 	%fd5174, %fd5161, %fd5158;
	mul.f64 	%fd5175, %fd5174, %fd5174;
	mul.f64 	%fd5176, %fd5158, %fd5175;
	sub.f64 	%fd5177, %fd5164, %fd5176;
	mul.f64 	%fd5178, %fd5170, %fd5174;
	sub.f64 	%fd5179, %fd5173, %fd5178;
	div.rn.f64 	%fd5180, %fd5179, %fd5177;
	mul.f64 	%fd5181, %fd5158, %fd5174;
	mul.f64 	%fd5182, %fd5181, %fd5180;
	sub.f64 	%fd5183, %fd5170, %fd5182;
	div.rn.f64 	%fd5184, %fd5183, %fd5158;
	mov.f64 	%fd5185, 0d0000000000000000;
	div.rn.f64 	%fd5186, %fd5185, %fd5158;
	add.f64 	%fd5187, %fd5186, 0d0000000000000000;
	mul.f64 	%fd5188, %fd5184, 0d0000000000000000;
	div.rn.f64 	%fd5189, %fd5188, %fd5158;
	sub.f64 	%fd5190, %fd5185, %fd5189;
	sub.f64 	%fd5191, %fd5185, %fd5187;
	fma.rn.f64 	%fd5192, %fd5191, %fd5180, 0d0000000000000000;
	fma.rn.f64 	%fd5193, %fd5191, %fd5181, 0d0000000000000000;
	fma.rn.f64 	%fd5194, %fd5174, %fd5192, %fd5190;
	fma.rn.f64 	%fd5195, %fd5158, %fd5192, 0d0000000000000000;
	div.rn.f64 	%fd5196, %fd5193, %fd5177;
	add.f64 	%fd6382, %fd5196, 0d0000000000000000;
	mul.f64 	%fd5197, %fd5193, %fd5180;
	div.rn.f64 	%fd5198, %fd5197, %fd5177;
	sub.f64 	%fd5199, %fd5185, %fd5198;
	sub.f64 	%fd5200, %fd5185, %fd6382;
	fma.rn.f64 	%fd5201, %fd5170, %fd5200, %fd5195;
	fma.rn.f64 	%fd5202, %fd5174, %fd5200, %fd5187;
	add.f64 	%fd6381, %fd5202, 0d0000000000000000;
	add.f64 	%fd6385, %fd5199, 0d0000000000000000;
	sub.f64 	%fd5203, %fd5185, %fd5199;
	fma.rn.f64 	%fd5204, %fd5158, %fd5203, 0d0000000000000000;
	fma.rn.f64 	%fd5205, %fd5175, %fd5203, %fd5194;
	fma.rn.f64 	%fd5206, %fd5174, %fd5204, %fd5201;
	fma.rn.f64 	%fd5207, %fd5174, %fd5204, %fd5206;
	div.rn.f64 	%fd5208, %fd5207, %fd5158;
	add.f64 	%fd6384, %fd5208, 0d0000000000000000;
	mul.f64 	%fd5209, %fd5174, %fd5207;
	div.rn.f64 	%fd5210, %fd5209, %fd5158;
	sub.f64 	%fd5211, %fd5205, %fd5210;
	add.f64 	%fd6383, %fd5211, 0d0000000000000000;
	fma.rn.f64 	%fd5212, %fd5165, %fd6382, 0d0000000000000000;
	fma.rn.f64 	%fd5213, %fd5167, %fd6382, 0d0000000000000000;
	fma.rn.f64 	%fd5214, %fd5169, %fd6382, 0d0000000000000000;
	fma.rn.f64 	%fd5215, %fd5149, %fd6382, 0d0000000000000000;
	fma.rn.f64 	%fd5216, %fd5152, %fd6382, 0d0000000000000000;
	fma.rn.f64 	%fd5217, %fd5155, %fd6382, 0d0000000000000000;
	fma.rn.f64 	%fd5218, %fd5165, %fd6381, 0d0000000000000000;
	fma.rn.f64 	%fd5219, %fd5167, %fd6381, 0d0000000000000000;
	fma.rn.f64 	%fd5220, %fd5169, %fd6381, 0d0000000000000000;
	fma.rn.f64 	%fd5221, %fd5144, %fd6381, %fd5215;
	fma.rn.f64 	%fd5222, %fd5145, %fd6381, %fd5216;
	fma.rn.f64 	%fd5223, %fd5147, %fd6381, %fd5217;
	add.f64 	%fd6362, %fd5221, %fd6362;
	add.f64 	%fd6361, %fd5222, %fd6361;
	add.f64 	%fd6360, %fd5223, %fd6360;
	sub.f64 	%fd5224, %fd6371, %fd5221;
	sub.f64 	%fd5225, %fd6370, %fd5222;
	sub.f64 	%fd5226, %fd6369, %fd5223;
	fma.rn.f64 	%fd5227, %fd5155, %fd6385, 0d0000000000000000;
	add.f64 	%fd5228, %fd5214, %fd5227;
	add.f64 	%fd5229, %fd5227, %fd5228;
	fma.rn.f64 	%fd5230, %fd5152, %fd6385, 0d0000000000000000;
	add.f64 	%fd5231, %fd5213, %fd5230;
	add.f64 	%fd5232, %fd5230, %fd5231;
	fma.rn.f64 	%fd5233, %fd5149, %fd6385, 0d0000000000000000;
	add.f64 	%fd5234, %fd5212, %fd5233;
	add.f64 	%fd5235, %fd5233, %fd5234;
	fma.rn.f64 	%fd5236, %fd5155, %fd6384, 0d0000000000000000;
	fma.rn.f64 	%fd5237, %fd5147, %fd6384, 0d0000000000000000;
	add.f64 	%fd5238, %fd5229, %fd5237;
	add.f64 	%fd5239, %fd5220, %fd5236;
	fma.rn.f64 	%fd5240, %fd5152, %fd6384, 0d0000000000000000;
	fma.rn.f64 	%fd5241, %fd5145, %fd6384, 0d0000000000000000;
	add.f64 	%fd5242, %fd5232, %fd5241;
	add.f64 	%fd5243, %fd5219, %fd5240;
	fma.rn.f64 	%fd5244, %fd5149, %fd6384, 0d0000000000000000;
	fma.rn.f64 	%fd5245, %fd5144, %fd6384, 0d0000000000000000;
	add.f64 	%fd5246, %fd5235, %fd5245;
	add.f64 	%fd5247, %fd5218, %fd5244;
	fma.rn.f64 	%fd5248, %fd5147, %fd6383, 0d0000000000000000;
	add.f64 	%fd5249, %fd5239, %fd5248;
	add.f64 	%fd5250, %fd5248, %fd5249;
	fma.rn.f64 	%fd5251, %fd5145, %fd6383, 0d0000000000000000;
	add.f64 	%fd5252, %fd5243, %fd5251;
	add.f64 	%fd5253, %fd5251, %fd5252;
	fma.rn.f64 	%fd5254, %fd5144, %fd6383, 0d0000000000000000;
	add.f64 	%fd5255, %fd5247, %fd5254;
	add.f64 	%fd5256, %fd5254, %fd5255;
	mul.f64 	%fd5257, %fd960, %fd5238;
	mul.f64 	%fd5258, %fd961, %fd5242;
	sub.f64 	%fd5259, %fd5257, %fd5258;
	mul.f64 	%fd5260, %fd961, %fd5246;
	mul.f64 	%fd5261, %fd959, %fd5238;
	sub.f64 	%fd5262, %fd5260, %fd5261;
	mul.f64 	%fd5263, %fd959, %fd5242;
	mul.f64 	%fd5264, %fd960, %fd5246;
	sub.f64 	%fd5265, %fd5263, %fd5264;
	add.f64 	%fd5266, %fd5259, %fd5256;
	add.f64 	%fd5267, %fd5262, %fd5253;
	add.f64 	%fd5268, %fd5265, %fd5250;
	mul.f64 	%fd5269, %fd5145, %fd5238;
	mul.f64 	%fd5270, %fd5147, %fd5242;
	mul.f64 	%fd5271, %fd5147, %fd5246;
	mul.f64 	%fd5272, %fd5144, %fd5238;
	mul.f64 	%fd5273, %fd5144, %fd5242;
	mul.f64 	%fd5274, %fd5145, %fd5246;
	sub.f64 	%fd5275, %fd5270, %fd5269;
	add.f64 	%fd6386, %fd5275, 0d0000000000000000;
	sub.f64 	%fd5276, %fd5272, %fd5271;
	add.f64 	%fd6387, %fd5276, 0d0000000000000000;
	sub.f64 	%fd5277, %fd5274, %fd5273;
	add.f64 	%fd6388, %fd5277, 0d0000000000000000;
	add.f64 	%fd6365, %fd5266, %fd6365;
	add.f64 	%fd6364, %fd5267, %fd6364;
	add.f64 	%fd6363, %fd5268, %fd6363;
	sub.f64 	%fd6371, %fd5224, %fd5266;
	sub.f64 	%fd6370, %fd5225, %fd5267;
	sub.f64 	%fd6369, %fd5226, %fd5268;

$L__BB5_343:
	div.rn.f64 	%fd5279, %fd5119, %fd1621;
	add.f64 	%fd5280, %fd5279, 0d0000000000000000;
	mul.f64 	%fd5281, %fd1631, 0d0000000000000000;
	div.rn.f64 	%fd5282, %fd5281, %fd1621;
	sub.f64 	%fd5283, %fd5119, %fd5282;
	sub.f64 	%fd5284, %fd5119, %fd5280;
	fma.rn.f64 	%fd5285, %fd5284, %fd1629, 0d0000000000000000;
	fma.rn.f64 	%fd5286, %fd5284, %fd1630, 0d0000000000000000;
	fma.rn.f64 	%fd5287, %fd1626, %fd5285, %fd5283;
	fma.rn.f64 	%fd5288, %fd1621, %fd5285, 0d0000000000000000;
	div.rn.f64 	%fd5289, %fd5286, %fd1628;
	add.f64 	%fd5290, %fd5289, 0d0000000000000000;
	mul.f64 	%fd5291, %fd5286, %fd1629;
	div.rn.f64 	%fd5292, %fd5291, %fd1628;
	sub.f64 	%fd5293, %fd5119, %fd5292;
	sub.f64 	%fd5294, %fd5119, %fd5290;
	fma.rn.f64 	%fd5295, %fd1625, %fd5294, %fd5288;
	fma.rn.f64 	%fd5296, %fd1626, %fd5294, %fd5280;
	add.f64 	%fd5297, %fd5296, 0d0000000000000000;
	add.f64 	%fd5298, %fd5293, 0d0000000000000000;
	sub.f64 	%fd5299, %fd5119, %fd5293;
	fma.rn.f64 	%fd5300, %fd1621, %fd5299, 0d0000000000000000;
	fma.rn.f64 	%fd5301, %fd1627, %fd5299, %fd5287;
	fma.rn.f64 	%fd5302, %fd1626, %fd5300, %fd5295;
	fma.rn.f64 	%fd5303, %fd1626, %fd5300, %fd5302;
	div.rn.f64 	%fd5304, %fd5303, %fd1621;
	add.f64 	%fd5305, %fd5304, 0d0000000000000000;
	mul.f64 	%fd5306, %fd1626, %fd5303;
	div.rn.f64 	%fd5307, %fd5306, %fd1621;
	sub.f64 	%fd5308, %fd5301, %fd5307;
	add.f64 	%fd5309, %fd5308, 0d0000000000000000;
	add.f64 	%fd6402, %fd5290, %fd6382;
	add.f64 	%fd6401, %fd5297, %fd6381;
	add.f64 	%fd6406, %fd5298, %fd6385;
	add.f64 	%fd6404, %fd5305, %fd6384;
	add.f64 	%fd6403, %fd5309, %fd6383;
	add.f64 	%fd5310, %fd6402, 0d0000000000000000;
	fma.rn.f64 	%fd5311, %fd1622, %fd5310, 0d0000000000000000;
	fma.rn.f64 	%fd5312, %fd1623, %fd5310, 0d0000000000000000;
	fma.rn.f64 	%fd5313, %fd1624, %fd5310, 0d0000000000000000;
	fma.rn.f64 	%fd5314, %fd1618, %fd5310, 0d0000000000000000;
	fma.rn.f64 	%fd5315, %fd1619, %fd5310, 0d0000000000000000;
	fma.rn.f64 	%fd5316, %fd1620, %fd5310, 0d0000000000000000;
	add.f64 	%fd5317, %fd6401, 0d0000000000000000;
	fma.rn.f64 	%fd5318, %fd1622, %fd5317, 0d0000000000000000;
	fma.rn.f64 	%fd5319, %fd1623, %fd5317, 0d0000000000000000;
	fma.rn.f64 	%fd5320, %fd1624, %fd5317, 0d0000000000000000;
	fma.rn.f64 	%fd5321, %fd1615, %fd5317, %fd5314;
	fma.rn.f64 	%fd5322, %fd1616, %fd5317, %fd5315;
	fma.rn.f64 	%fd5323, %fd1617, %fd5317, %fd5316;
	add.f64 	%fd6362, %fd6362, %fd5321;
	add.f64 	%fd6361, %fd6361, %fd5322;
	add.f64 	%fd6360, %fd6360, %fd5323;
	sub.f64 	%fd5324, %fd6368, %fd5321;
	sub.f64 	%fd5325, %fd6367, %fd5322;
	sub.f64 	%fd5326, %fd6366, %fd5323;
	add.f64 	%fd5327, %fd6406, 0d0000000000000000;
	fma.rn.f64 	%fd5328, %fd1620, %fd5327, 0d0000000000000000;
	add.f64 	%fd5329, %fd5313, %fd5328;
	add.f64 	%fd5330, %fd5328, %fd5329;
	fma.rn.f64 	%fd5331, %fd1619, %fd5327, 0d0000000000000000;
	add.f64 	%fd5332, %fd5312, %fd5331;
	add.f64 	%fd5333, %fd5331, %fd5332;
	fma.rn.f64 	%fd5334, %fd1618, %fd5327, 0d0000000000000000;
	add.f64 	%fd5335, %fd5311, %fd5334;
	add.f64 	%fd5336, %fd5334, %fd5335;
	add.f64 	%fd5337, %fd6404, 0d0000000000000000;
	add.f64 	%fd6405, %fd6384, %fd5337;
	add.f64 	%fd5338, %fd6405, 0d0000000000000000;
	fma.rn.f64 	%fd5339, %fd1620, %fd5338, 0d0000000000000000;
	fma.rn.f64 	%fd5340, %fd1617, %fd5338, 0d0000000000000000;
	add.f64 	%fd5341, %fd5340, %fd5330;
	add.f64 	%fd5342, %fd5320, %fd5339;
	fma.rn.f64 	%fd5343, %fd1619, %fd5338, 0d0000000000000000;
	fma.rn.f64 	%fd5344, %fd1616, %fd5338, 0d0000000000000000;
	add.f64 	%fd5345, %fd5344, %fd5333;
	add.f64 	%fd5346, %fd5319, %fd5343;
	fma.rn.f64 	%fd5347, %fd1618, %fd5338, 0d0000000000000000;
	fma.rn.f64 	%fd5348, %fd1615, %fd5338, 0d0000000000000000;
	add.f64 	%fd5349, %fd5348, %fd5336;
	add.f64 	%fd5350, %fd5318, %fd5347;
	add.f64 	%fd5351, %fd6403, 0d0000000000000000;
	fma.rn.f64 	%fd5352, %fd1617, %fd5351, 0d0000000000000000;
	add.f64 	%fd5353, %fd5352, %fd5342;
	add.f64 	%fd5354, %fd5352, %fd5353;
	fma.rn.f64 	%fd5355, %fd1616, %fd5351, 0d0000000000000000;
	add.f64 	%fd5356, %fd5355, %fd5346;
	add.f64 	%fd5357, %fd5355, %fd5356;
	fma.rn.f64 	%fd5358, %fd1615, %fd5351, 0d0000000000000000;
	add.f64 	%fd5359, %fd5358, %fd5350;
	add.f64 	%fd5360, %fd5358, %fd5359;
	mul.f64 	%fd5361, %fd960, %fd5341;
	mul.f64 	%fd5362, %fd961, %fd5345;
	sub.f64 	%fd5363, %fd5361, %fd5362;
	mul.f64 	%fd5364, %fd961, %fd5349;
	mul.f64 	%fd5365, %fd959, %fd5341;
	sub.f64 	%fd5366, %fd5364, %fd5365;
	mul.f64 	%fd5367, %fd959, %fd5345;
	mul.f64 	%fd5368, %fd960, %fd5349;
	sub.f64 	%fd5369, %fd5367, %fd5368;
	add.f64 	%fd5370, %fd5360, %fd5363;
	add.f64 	%fd5371, %fd5357, %fd5366;
	add.f64 	%fd5372, %fd5354, %fd5369;
	mul.f64 	%fd5373, %fd1616, %fd5341;
	mul.f64 	%fd5374, %fd1617, %fd5345;
	sub.f64 	%fd5375, %fd5373, %fd5374;
	mul.f64 	%fd5376, %fd1617, %fd5349;
	mul.f64 	%fd5377, %fd1615, %fd5341;
	sub.f64 	%fd5378, %fd5376, %fd5377;
	mul.f64 	%fd5379, %fd1615, %fd5345;
	mul.f64 	%fd5380, %fd1616, %fd5349;
	sub.f64 	%fd5381, %fd5379, %fd5380;
	sub.f64 	%fd6407, %fd6386, %fd5375;
	sub.f64 	%fd6408, %fd6387, %fd5378;
	sub.f64 	%fd6409, %fd6388, %fd5381;
	add.f64 	%fd6371, %fd6371, %fd5370;
	add.f64 	%fd6370, %fd6370, %fd5371;
	add.f64 	%fd6369, %fd6369, %fd5372;
	sub.f64 	%fd6368, %fd5324, %fd5370;
	sub.f64 	%fd6367, %fd5325, %fd5371;
	sub.f64 	%fd6366, %fd5326, %fd5372;

$L__BB5_344:
	sub.f64 	%fd5566, %fd957, %fd951;
	sub.f64 	%fd5565, %fd955, %fd948;
	sub.f64 	%fd5564, %fd953, %fd945;
	add.f64 	%fd5382, %fd1494, %fd6402;
	add.f64 	%fd5383, %fd5382, 0d0000000000000000;
	fma.rn.f64 	%fd5384, %fd967, %fd5383, 0d0000000000000000;
	fma.rn.f64 	%fd5385, %fd969, %fd5383, 0d0000000000000000;
	fma.rn.f64 	%fd5386, %fd971, %fd5383, 0d0000000000000000;
	fma.rn.f64 	%fd5387, %fd962, %fd5383, 0d0000000000000000;
	fma.rn.f64 	%fd5388, %fd963, %fd5383, 0d0000000000000000;
	fma.rn.f64 	%fd5389, %fd964, %fd5383, 0d0000000000000000;
	add.f64 	%fd5390, %fd1495, %fd6401;
	add.f64 	%fd5391, %fd5390, 0d0000000000000000;
	fma.rn.f64 	%fd5392, %fd967, %fd5391, 0d0000000000000000;
	fma.rn.f64 	%fd5393, %fd969, %fd5391, 0d0000000000000000;
	fma.rn.f64 	%fd5394, %fd971, %fd5391, 0d0000000000000000;
	fma.rn.f64 	%fd5395, %fd946, %fd5391, %fd5387;
	fma.rn.f64 	%fd5396, %fd949, %fd5391, %fd5388;
	fma.rn.f64 	%fd5397, %fd952, %fd5391, %fd5389;
	add.f64 	%fd1705, %fd6362, %fd5395;
	add.f64 	%fd1706, %fd6361, %fd5396;
	add.f64 	%fd1707, %fd6360, %fd5397;
	sub.f64 	%fd5398, %fd6365, %fd5395;
	sub.f64 	%fd5399, %fd6364, %fd5396;
	sub.f64 	%fd5400, %fd6363, %fd5397;
	add.f64 	%fd5401, %fd1496, %fd6406;
	add.f64 	%fd5402, %fd5401, 0d0000000000000000;
	fma.rn.f64 	%fd5403, %fd964, %fd5402, 0d0000000000000000;
	add.f64 	%fd5404, %fd5386, %fd5403;
	add.f64 	%fd5405, %fd5403, %fd5404;
	fma.rn.f64 	%fd5406, %fd963, %fd5402, 0d0000000000000000;
	add.f64 	%fd5407, %fd5385, %fd5406;
	add.f64 	%fd5408, %fd5406, %fd5407;
	fma.rn.f64 	%fd5409, %fd962, %fd5402, 0d0000000000000000;
	add.f64 	%fd5410, %fd5384, %fd5409;
	add.f64 	%fd5411, %fd5409, %fd5410;
	add.f64 	%fd5412, %fd1497, %fd6404;
	add.f64 	%fd5413, %fd5412, 0d0000000000000000;
	add.f64 	%fd5414, %fd6405, %fd5413;
	add.f64 	%fd5415, %fd5414, 0d0000000000000000;
	fma.rn.f64 	%fd5416, %fd964, %fd5415, 0d0000000000000000;
	fma.rn.f64 	%fd5417, %fd952, %fd5415, 0d0000000000000000;
	add.f64 	%fd5418, %fd5417, %fd5405;
	add.f64 	%fd5419, %fd5394, %fd5416;
	fma.rn.f64 	%fd5420, %fd963, %fd5415, 0d0000000000000000;
	fma.rn.f64 	%fd5421, %fd949, %fd5415, 0d0000000000000000;
	add.f64 	%fd5422, %fd5421, %fd5408;
	add.f64 	%fd5423, %fd5393, %fd5420;
	fma.rn.f64 	%fd5424, %fd962, %fd5415, 0d0000000000000000;
	fma.rn.f64 	%fd5425, %fd946, %fd5415, 0d0000000000000000;
	add.f64 	%fd5426, %fd5425, %fd5411;
	add.f64 	%fd5427, %fd5392, %fd5424;
	add.f64 	%fd5428, %fd1498, %fd6403;
	add.f64 	%fd5429, %fd5428, 0d0000000000000000;
	fma.rn.f64 	%fd5430, %fd952, %fd5429, 0d0000000000000000;
	add.f64 	%fd5431, %fd5430, %fd5419;
	add.f64 	%fd5432, %fd5430, %fd5431;
	fma.rn.f64 	%fd5433, %fd949, %fd5429, 0d0000000000000000;
	add.f64 	%fd5434, %fd5433, %fd5423;
	add.f64 	%fd5435, %fd5433, %fd5434;
	fma.rn.f64 	%fd5436, %fd946, %fd5429, 0d0000000000000000;
	add.f64 	%fd5437, %fd5436, %fd5427;
	add.f64 	%fd5438, %fd5436, %fd5437;
	mul.f64 	%fd5439, %fd960, %fd5418;
	mul.f64 	%fd5440, %fd961, %fd5422;
	sub.f64 	%fd5441, %fd5439, %fd5440;
	mul.f64 	%fd5442, %fd961, %fd5426;
	mul.f64 	%fd5443, %fd959, %fd5418;
	sub.f64 	%fd5444, %fd5442, %fd5443;
	mul.f64 	%fd5445, %fd959, %fd5422;
	mul.f64 	%fd5446, %fd960, %fd5426;
	sub.f64 	%fd5447, %fd5445, %fd5446;
	add.f64 	%fd5448, %fd5438, %fd5441;
	add.f64 	%fd5449, %fd5435, %fd5444;
	add.f64 	%fd5450, %fd5432, %fd5447;
	mul.f64 	%fd5451, %fd949, %fd5418;
	mul.f64 	%fd5452, %fd952, %fd5422;
	sub.f64 	%fd5453, %fd5451, %fd5452;
	mul.f64 	%fd5454, %fd952, %fd5426;
	mul.f64 	%fd5455, %fd946, %fd5418;
	sub.f64 	%fd5456, %fd5454, %fd5455;
	mul.f64 	%fd5457, %fd946, %fd5422;
	mul.f64 	%fd5458, %fd949, %fd5426;
	sub.f64 	%fd5459, %fd5457, %fd5458;
	sub.f64 	%fd5460, %fd6407, %fd5453;
	sub.f64 	%fd5461, %fd6408, %fd5456;
	sub.f64 	%fd5462, %fd6409, %fd5459;
	mul.f64 	%fd5463, %fd5565, %fd5462;
	mul.f64 	%fd5464, %fd5566, %fd5461;
	sub.f64 	%fd5465, %fd5463, %fd5464;
	mul.f64 	%fd5466, %fd5566, %fd5460;
	mul.f64 	%fd5467, %fd5564, %fd5462;
	sub.f64 	%fd5468, %fd5466, %fd5467;
	mul.f64 	%fd5469, %fd5564, %fd5461;
	mul.f64 	%fd5470, %fd5565, %fd5460;
	sub.f64 	%fd5471, %fd5469, %fd5470;
	add.f64 	%fd5472, %fd5448, %fd5465;
	add.f64 	%fd5473, %fd5449, %fd5468;
	add.f64 	%fd5474, %fd5450, %fd5471;
	mul.f64 	%fd5475, %fd949, %fd5462;
	mul.f64 	%fd5476, %fd952, %fd5461;
	mul.f64 	%fd5477, %fd952, %fd5460;
	mul.f64 	%fd5478, %fd946, %fd5462;
	mul.f64 	%fd5479, %fd946, %fd5461;
	mul.f64 	%fd5480, %fd949, %fd5460;
	sub.f64 	%fd5481, %fd5476, %fd5475;
	add.f64 	%fd5482, %fd5481, 0d0000000000000000;
	sub.f64 	%fd5483, %fd5478, %fd5477;
	add.f64 	%fd5484, %fd5483, 0d0000000000000000;
	sub.f64 	%fd5485, %fd5480, %fd5479;
	add.f64 	%fd5486, %fd5485, 0d0000000000000000;
	add.f64 	%fd5487, %fd6371, %fd5482;
	add.f64 	%fd5488, %fd6370, %fd5484;
	add.f64 	%fd5489, %fd6369, %fd5486;
	sub.f64 	%fd5490, %fd5398, %fd5482;
	sub.f64 	%fd5491, %fd5399, %fd5484;
	sub.f64 	%fd5492, %fd5400, %fd5486;
	add.f64 	%fd1708, %fd6368, %fd5472;
	add.f64 	%fd1709, %fd6367, %fd5473;
	add.f64 	%fd1710, %fd6366, %fd5474;
	sub.f64 	%fd1711, %fd5490, %fd5472;
	sub.f64 	%fd1712, %fd5491, %fd5473;
	sub.f64 	%fd1713, %fd5492, %fd5474;
	add.f64 	%fd1714, %fd5487, 0d0000000000000000;
	add.f64 	%fd1715, %fd5488, 0d0000000000000000;
	add.f64 	%fd1716, %fd5489, 0d0000000000000000;
	setp.eq.s64 	%p310, %rd141, 0;
	@%p310 bra 	$L__BB5_346;

	mul.lo.s64 	%rd330, %rd99, %rd58;
	add.s64 	%rd327, %rd141, %rd330;
	// begin inline asm
	{ atom.add.f64 %fd5493,[%rd327],%fd1714; }

	// end inline asm
	add.s64 	%rd328, %rd327, 8;
	// begin inline asm
	{ atom.add.f64 %fd5495,[%rd328],%fd1715; }

	// end inline asm
	add.s64 	%rd329, %rd327, 16;
	// begin inline asm
	{ atom.add.f64 %fd5497,[%rd329],%fd1716; }

	// end inline asm
	bra.uni 	$L__BB5_348;

$L__BB5_346:
	setp.eq.s64 	%p311, %rd116, 0;
	@%p311 bra 	$L__BB5_348;

	mul.lo.s64 	%rd369, %rd99, %rd46;
	add.s64 	%rd331, %rd116, %rd369;
	// begin inline asm
	{ atom.add.f64 %fd5499,[%rd331],%fd1714; }

	// end inline asm
	add.s64 	%rd332, %rd331, 8;
	// begin inline asm
	{ atom.add.f64 %fd5501,[%rd332],%fd1715; }

	// end inline asm
	add.s64 	%rd333, %rd331, 16;
	// begin inline asm
	{ atom.add.f64 %fd5503,[%rd333],%fd1716; }

	// end inline asm

$L__BB5_348:
	add.f64 	%fd1717, %fd1708, 0d0000000000000000;
	add.f64 	%fd1718, %fd1709, 0d0000000000000000;
	add.f64 	%fd1719, %fd1710, 0d0000000000000000;
	@%p310 bra 	$L__BB5_350;

	mul.lo.s64 	%rd337, %rd97, %rd58;
	add.s64 	%rd334, %rd141, %rd337;
	// begin inline asm
	{ atom.add.f64 %fd5505,[%rd334],%fd1717; }

	// end inline asm
	add.s64 	%rd335, %rd334, 8;
	// begin inline asm
	{ atom.add.f64 %fd5507,[%rd335],%fd1718; }

	// end inline asm
	add.s64 	%rd336, %rd334, 16;
	// begin inline asm
	{ atom.add.f64 %fd5509,[%rd336],%fd1719; }

	// end inline asm
	bra.uni 	$L__BB5_352;

$L__BB5_350:
	setp.eq.s64 	%p313, %rd116, 0;
	@%p313 bra 	$L__BB5_352;

	mul.lo.s64 	%rd368, %rd97, %rd46;
	add.s64 	%rd338, %rd116, %rd368;
	// begin inline asm
	{ atom.add.f64 %fd5511,[%rd338],%fd1717; }

	// end inline asm
	add.s64 	%rd339, %rd338, 8;
	// begin inline asm
	{ atom.add.f64 %fd5513,[%rd339],%fd1718; }

	// end inline asm
	add.s64 	%rd340, %rd338, 16;
	// begin inline asm
	{ atom.add.f64 %fd5515,[%rd340],%fd1719; }

	// end inline asm

$L__BB5_352:
	add.f64 	%fd1720, %fd1711, 0d0000000000000000;
	add.f64 	%fd1721, %fd1712, 0d0000000000000000;
	add.f64 	%fd1722, %fd1713, 0d0000000000000000;
	@%p310 bra 	$L__BB5_354;

	mul.lo.s64 	%rd344, %rd95, %rd58;
	add.s64 	%rd341, %rd141, %rd344;
	// begin inline asm
	{ atom.add.f64 %fd5517,[%rd341],%fd1720; }

	// end inline asm
	add.s64 	%rd342, %rd341, 8;
	// begin inline asm
	{ atom.add.f64 %fd5519,[%rd342],%fd1721; }

	// end inline asm
	add.s64 	%rd343, %rd341, 16;
	// begin inline asm
	{ atom.add.f64 %fd5521,[%rd343],%fd1722; }

	// end inline asm
	bra.uni 	$L__BB5_356;

$L__BB5_354:
	setp.eq.s64 	%p315, %rd116, 0;
	@%p315 bra 	$L__BB5_356;

	mul.lo.s64 	%rd367, %rd95, %rd46;
	add.s64 	%rd345, %rd116, %rd367;
	// begin inline asm
	{ atom.add.f64 %fd5523,[%rd345],%fd1720; }

	// end inline asm
	add.s64 	%rd346, %rd345, 8;
	// begin inline asm
	{ atom.add.f64 %fd5525,[%rd346],%fd1721; }

	// end inline asm
	add.s64 	%rd347, %rd345, 16;
	// begin inline asm
	{ atom.add.f64 %fd5527,[%rd347],%fd1722; }

	// end inline asm

$L__BB5_356:
	add.f64 	%fd1723, %fd1705, 0d0000000000000000;
	add.f64 	%fd1724, %fd1706, 0d0000000000000000;
	add.f64 	%fd1725, %fd1707, 0d0000000000000000;
	@%p310 bra 	$L__BB5_358;

	mul.lo.s64 	%rd351, %rd93, %rd58;
	add.s64 	%rd348, %rd141, %rd351;
	// begin inline asm
	{ atom.add.f64 %fd5529,[%rd348],%fd1723; }

	// end inline asm
	add.s64 	%rd349, %rd348, 8;
	// begin inline asm
	{ atom.add.f64 %fd5531,[%rd349],%fd1724; }

	// end inline asm
	add.s64 	%rd350, %rd348, 16;
	// begin inline asm
	{ atom.add.f64 %fd5533,[%rd350],%fd1725; }

	// end inline asm
	bra.uni 	$L__BB5_360;

$L__BB5_358:
	setp.eq.s64 	%p317, %rd116, 0;
	@%p317 bra 	$L__BB5_360;

	mul.lo.s64 	%rd366, %rd93, %rd46;
	add.s64 	%rd352, %rd116, %rd366;
	// begin inline asm
	{ atom.add.f64 %fd5535,[%rd352],%fd1723; }

	// end inline asm
	add.s64 	%rd353, %rd352, 8;
	// begin inline asm
	{ atom.add.f64 %fd5537,[%rd353],%fd1724; }

	// end inline asm
	add.s64 	%rd354, %rd352, 16;
	// begin inline asm
	{ atom.add.f64 %fd5539,[%rd354],%fd1725; }

	// end inline asm

$L__BB5_360:
	setp.eq.s64 	%p318, %rd149, 0;
	add.f64 	%fd1726, %fd1543, 0d0000000000000000;
	@%p318 bra 	$L__BB5_362;

	mul.lo.s64 	%rd356, %rd89, %rd60;
	add.s64 	%rd355, %rd149, %rd356;
	// begin inline asm
	{ atom.add.f64 %fd5541,[%rd355],%fd1726; }

	// end inline asm
	bra.uni 	$L__BB5_364;

$L__BB5_362:
	setp.eq.s64 	%p319, %rd126, 0;
	@%p319 bra 	$L__BB5_364;

	add.s64 	%rd357, %rd126, %rd92;
	// begin inline asm
	{ atom.add.f64 %fd5543,[%rd357],%fd1726; }

	// end inline asm

$L__BB5_364:
	setp.eq.s64 	%p320, %rd145, 0;
	@%p320 bra 	$L__BB5_366;

	mul.lo.s64 	%rd359, %rd90, %rd61;
	add.s64 	%rd358, %rd145, %rd359;
	// begin inline asm
	{ atom.add.f64 %fd5545,[%rd358],%fd1726; }

	// end inline asm
	bra.uni 	$L__BB5_368;

$L__BB5_366:
	setp.eq.s64 	%p321, %rd122, 0;
	@%p321 bra 	$L__BB5_368;

	add.s64 	%rd360, %rd122, %rd91;
	// begin inline asm
	{ atom.add.f64 %fd5547,[%rd360],%fd1726; }

	// end inline asm
	bra.uni 	$L__BB5_368;

$L__BB5_34:
	setp.eq.s32 	%p55, %r647, 7;
	@%p55 bra 	$L__BB5_36;
	bra.uni 	$L__BB5_35;

$L__BB5_36:
	sub.f64 	%fd1803, %fd14, %fd22;
	sub.f64 	%fd1804, %fd19, %fd28;
	sub.f64 	%fd1805, %fd17, %fd25;
	mul.f64 	%fd1806, %fd1805, %fd1804;
	sub.f64 	%fd1807, %fd16, %fd25;
	sub.f64 	%fd1808, %fd20, %fd28;
	mul.f64 	%fd1809, %fd1807, %fd1808;
	sub.f64 	%fd1810, %fd1806, %fd1809;
	sub.f64 	%fd1811, %fd13, %fd22;
	mul.f64 	%fd1812, %fd1811, %fd1808;
	mul.f64 	%fd1813, %fd1803, %fd1804;
	sub.f64 	%fd1814, %fd1812, %fd1813;
	mul.f64 	%fd1815, %fd1803, %fd1807;
	mul.f64 	%fd1816, %fd1811, %fd1805;
	sub.f64 	%fd1817, %fd1815, %fd1816;
	mul.f64 	%fd1818, %fd1814, %fd1814;
	fma.rn.f64 	%fd1819, %fd1810, %fd1810, %fd1818;
	fma.rn.f64 	%fd1820, %fd1817, %fd1817, %fd1819;
	div.rn.f64 	%fd5580, %fd1820, %fd34;
	bra.uni 	$L__BB5_44;

$L__BB5_35:
	sub.f64 	%fd1784, %fd23, %fd14;
	mul.f64 	%fd1785, %fd21, %fd27;
	mul.f64 	%fd1786, %fd18, %fd30;
	sub.f64 	%fd1787, %fd1786, %fd1785;
	mul.f64 	%fd1788, %fd15, %fd30;
	mul.f64 	%fd1789, %fd21, %fd24;
	sub.f64 	%fd1790, %fd1789, %fd1788;
	mul.f64 	%fd1791, %fd18, %fd24;
	mul.f64 	%fd1792, %fd15, %fd27;
	sub.f64 	%fd1793, %fd1792, %fd1791;
	sub.f64 	%fd1794, %fd26, %fd17;
	mul.f64 	%fd1795, %fd1794, %fd1790;
	fma.rn.f64 	%fd1796, %fd1784, %fd1787, %fd1795;
	sub.f64 	%fd1797, %fd29, %fd20;
	fma.rn.f64 	%fd1798, %fd1797, %fd1793, %fd1796;
	mul.f64 	%fd1799, %fd1798, %fd1798;
	mul.f64 	%fd1800, %fd1790, %fd1790;
	fma.rn.f64 	%fd1801, %fd1787, %fd1787, %fd1800;
	fma.rn.f64 	%fd1802, %fd1793, %fd1793, %fd1801;
	div.rn.f64 	%fd5580, %fd1799, %fd1802;

$L__BB5_44:
	mul.f64 	%fd1901, %fd12, %fd12;
	sub.f64 	%fd64, %fd5580, %fd1901;
	fma.rn.f64 	%fd65, %fd2, %fd12, %fd1;
	setp.geu.f64 	%p56, %fd64, %fd65;
	mov.f64 	%fd5585, 0d0000000000000000;
	mov.f64 	%fd5586, %fd5585;
	mov.f64 	%fd5587, %fd5585;
	mov.f64 	%fd5588, %fd5585;
	mov.f64 	%fd5589, %fd5585;
	mov.f64 	%fd5590, %fd5585;
	mov.f64 	%fd5591, %fd5585;
	mov.f64 	%fd5592, %fd5585;
	mov.f64 	%fd5593, %fd5585;
	mov.f64 	%fd5594, %fd5585;
	mov.f64 	%fd5595, %fd5585;
	mov.f64 	%fd5596, %fd5585;
	@%p56 bra 	$L__BB5_56;

	mul.lo.s64 	%rd179, %rd81, %rd54;
	add.s64 	%rd180, %rd33, %rd179;
	mul.lo.s64 	%rd181, %rd83, %rd54;
	add.s64 	%rd182, %rd33, %rd181;
	mul.lo.s64 	%rd183, %rd85, %rd54;
	add.s64 	%rd184, %rd33, %rd183;
	mul.lo.s64 	%rd185, %rd87, %rd54;
	add.s64 	%rd186, %rd33, %rd185;
	ld.global.f64 	%fd5590, [%rd182];
	ld.global.f64 	%fd5587, [%rd180];
	sub.f64 	%fd1903, %fd5590, %fd5587;
	ld.global.f64 	%fd5589, [%rd182+8];
	ld.global.f64 	%fd5586, [%rd180+8];
	sub.f64 	%fd1904, %fd5589, %fd5586;
	ld.global.f64 	%fd5588, [%rd182+16];
	ld.global.f64 	%fd5585, [%rd180+16];
	sub.f64 	%fd1905, %fd5588, %fd5585;
	ld.global.f64 	%fd5596, [%rd186];
	ld.global.f64 	%fd5593, [%rd184];
	sub.f64 	%fd1906, %fd5596, %fd5593;
	ld.global.f64 	%fd5595, [%rd186+8];
	ld.global.f64 	%fd5592, [%rd184+8];
	sub.f64 	%fd1907, %fd5595, %fd5592;
	ld.global.f64 	%fd5594, [%rd186+16];
	ld.global.f64 	%fd5591, [%rd184+16];
	sub.f64 	%fd1908, %fd5594, %fd5591;
	mul.f64 	%fd1909, %fd1904, %fd1904;
	fma.rn.f64 	%fd1910, %fd1903, %fd1903, %fd1909;
	fma.rn.f64 	%fd1911, %fd1905, %fd1905, %fd1910;
	mul.f64 	%fd1912, %fd1911, 0d3F50624DE0000000;
	mul.f64 	%fd1913, %fd1907, %fd1907;
	fma.rn.f64 	%fd1914, %fd1906, %fd1906, %fd1913;
	fma.rn.f64 	%fd1915, %fd1908, %fd1908, %fd1914;
	mul.f64 	%fd6410, %fd1912, %fd1915;
	mul.f64 	%fd1916, %fd21, %fd27;
	mul.f64 	%fd1917, %fd18, %fd30;
	sub.f64 	%fd1918, %fd1917, %fd1916;
	mul.f64 	%fd1919, %fd15, %fd30;
	mul.f64 	%fd1920, %fd21, %fd24;
	sub.f64 	%fd1921, %fd1920, %fd1919;
	mul.f64 	%fd1922, %fd18, %fd24;
	mul.f64 	%fd1923, %fd15, %fd27;
	sub.f64 	%fd1924, %fd1923, %fd1922;
	mul.f64 	%fd1925, %fd1921, %fd1921;
	fma.rn.f64 	%fd1926, %fd1918, %fd1918, %fd1925;
	fma.rn.f64 	%fd79, %fd1924, %fd1924, %fd1926;
	setp.geu.f64 	%p57, %fd79, %fd6410;
	mov.f64 	%fd5581, 0d3FF0000000000000;
	@%p57 bra 	$L__BB5_47;

	div.rn.f64 	%fd1927, %fd79, %fd6410;
	mov.f64 	%fd1928, 0d0000000000000000;
	sub.f64 	%fd1929, %fd1928, %fd1927;
	add.f64 	%fd1930, %fd1929, 0d4000000000000000;
	mul.f64 	%fd5581, %fd1927, %fd1930;

$L__BB5_47:
	setp.neu.f64 	%p58, %fd5581, 0d3FF0000000000000;
	setp.eq.f64 	%p59, %fd5581, 0d3FF0000000000000;
	selp.u16 	%rs366, 1, 0, %p59;
	mov.u32 	%r679, %r7;
	mov.u32 	%r678, %r8;
	mov.u32 	%r677, %r9;
	mov.u32 	%r676, %r10;
	@%p58 bra 	$L__BB5_56;

	mul.lo.s64 	%rd187, %rd77, %rd55;
	add.s64 	%rd188, %rd28, %rd187;
	mul.lo.s64 	%rd189, %rd78, %rd55;
	add.s64 	%rd190, %rd28, %rd189;
	ld.global.f64 	%fd1931, [%rd190];
	ld.global.f64 	%fd1932, [%rd188];
	add.f64 	%fd1933, %fd1932, %fd1931;
	mul.f64 	%fd1934, %fd1933, %fd1738;
	mul.f64 	%fd6414, %fd1934, %fd1736;
	div.rn.f64 	%fd5582, %fd64, %fd65;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r648}, %fd5582;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r649, %temp}, %fd5582;
	}
	setp.gt.s32 	%p60, %r648, 1048575;
	mov.u32 	%r650, -1023;
	@%p60 bra 	$L__BB5_50;

	mul.f64 	%fd5582, %fd5582, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r648}, %fd5582;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r649, %temp}, %fd5582;
	}
	mov.u32 	%r650, -1077;

$L__BB5_50:
	add.s32 	%r556, %r648, -1;
	setp.lt.u32 	%p61, %r556, 2146435071;
	@%p61 bra 	$L__BB5_52;
	bra.uni 	$L__BB5_51;

$L__BB5_52:
	shr.u32 	%r558, %r648, 20;
	add.s32 	%r651, %r650, %r558;
	and.b32  	%r559, %r648, -2146435073;
	or.b32  	%r560, %r559, 1072693248;
	mov.b64 	%fd5583, {%r649, %r560};
	setp.lt.s32 	%p63, %r560, 1073127583;
	@%p63 bra 	$L__BB5_54;

	{
	.reg .b32 %temp;
	mov.b64 	{%r561, %temp}, %fd5583;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r562}, %fd5583;
	}
	add.s32 	%r563, %r562, -1048576;
	mov.b64 	%fd5583, {%r561, %r563};
	add.s32 	%r651, %r651, 1;

$L__BB5_54:
	add.f64 	%fd1937, %fd5583, 0d3FF0000000000000;
	mov.f64 	%fd1938, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd1939, %fd1937;
	neg.f64 	%fd1940, %fd1937;
	fma.rn.f64 	%fd1941, %fd1940, %fd1939, %fd1938;
	fma.rn.f64 	%fd1942, %fd1941, %fd1941, %fd1941;
	fma.rn.f64 	%fd1943, %fd1942, %fd1939, %fd1939;
	add.f64 	%fd1944, %fd5583, 0dBFF0000000000000;
	mul.f64 	%fd1945, %fd1944, %fd1943;
	fma.rn.f64 	%fd1946, %fd1944, %fd1943, %fd1945;
	mul.f64 	%fd1947, %fd1946, %fd1946;
	mov.f64 	%fd1948, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1949, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1950, %fd1949, %fd1947, %fd1948;
	mov.f64 	%fd1951, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1952, %fd1950, %fd1947, %fd1951;
	mov.f64 	%fd1953, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1954, %fd1952, %fd1947, %fd1953;
	mov.f64 	%fd1955, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1956, %fd1954, %fd1947, %fd1955;
	mov.f64 	%fd1957, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1958, %fd1956, %fd1947, %fd1957;
	mov.f64 	%fd1959, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1960, %fd1958, %fd1947, %fd1959;
	mov.f64 	%fd1961, 0d3FB5555555555554;
	fma.rn.f64 	%fd1962, %fd1960, %fd1947, %fd1961;
	sub.f64 	%fd1963, %fd1944, %fd1946;
	add.f64 	%fd1964, %fd1963, %fd1963;
	neg.f64 	%fd1965, %fd1946;
	fma.rn.f64 	%fd1966, %fd1965, %fd1944, %fd1964;
	mul.f64 	%fd1967, %fd1943, %fd1966;
	mul.f64 	%fd1968, %fd1947, %fd1962;
	fma.rn.f64 	%fd1969, %fd1968, %fd1946, %fd1967;
	xor.b32  	%r564, %r651, -2147483648;
	mov.u32 	%r565, -2147483648;
	mov.u32 	%r566, 1127219200;
	mov.b64 	%fd1970, {%r564, %r566};
	mov.b64 	%fd1971, {%r565, %r566};
	sub.f64 	%fd1972, %fd1970, %fd1971;
	mov.f64 	%fd1973, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1974, %fd1972, %fd1973, %fd1946;
	neg.f64 	%fd1975, %fd1972;
	fma.rn.f64 	%fd1976, %fd1975, %fd1973, %fd1974;
	sub.f64 	%fd1977, %fd1976, %fd1946;
	sub.f64 	%fd1978, %fd1969, %fd1977;
	mov.f64 	%fd1979, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1980, %fd1972, %fd1979, %fd1978;
	add.f64 	%fd5584, %fd1974, %fd1980;
	bra.uni 	$L__BB5_55;

$L__BB5_51:
	mov.f64 	%fd1935, 0d7FF0000000000000;
	fma.rn.f64 	%fd1936, %fd5582, %fd1935, %fd1935;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r557}, %fd5582;
	}
	mov.b32 	%f1, %r557;
	setp.eq.f32 	%p62, %f1, 0f00000000;
	selp.f64 	%fd5584, 0dFFF0000000000000, %fd1936, %p62;

$L__BB5_55:
	sub.f64 	%fd1981, %fd64, %fd65;
	div.rn.f64 	%fd1982, %fd1981, %fd65;
	mul.f64 	%fd1983, %fd1982, %fd5584;
	mul.f64 	%fd1984, %fd1983, 0dC000000000000000;
	div.rn.f64 	%fd1985, %fd1984, %fd65;
	mul.f64 	%fd1986, %fd1982, %fd1982;
	div.rn.f64 	%fd1987, %fd1986, %fd64;
	sub.f64 	%fd1988, %fd1985, %fd1987;
	mul.f64 	%fd6413, %fd1988, %fd1737;
	mul.f64 	%fd1989, %fd6414, %fd6413;
	fma.rn.f64 	%fd6412, %fd6414, %fd6413, %fd1989;
	sqrt.rn.f64 	%fd6411, %fd64;
	mov.u32 	%r679, %r7;
	mov.u32 	%r678, %r8;
	mov.u32 	%r677, %r9;
	mov.u32 	%r676, %r10;

$L__BB5_56:
	setp.eq.s64 	%p64, %rd139, 0;
	@%p64 bra 	$L__BB5_58;

	cvta.to.global.u64 	%rd191, %rd139;
	mul.lo.s64 	%rd192, %rd75, %rd49;
	add.s64 	%rd193, %rd191, %rd192;
	ld.global.f64 	%fd1990, [%rd193];
	add.f64 	%fd5604, %fd1990, 0d0000000000000000;
	ld.global.f64 	%fd1991, [%rd193+8];
	add.f64 	%fd5603, %fd1991, 0d0000000000000000;
	ld.global.f64 	%fd1992, [%rd193+16];
	add.f64 	%fd5602, %fd1992, 0d0000000000000000;
	bra.uni 	$L__BB5_60;

$L__BB5_58:
	setp.eq.s64 	%p65, %rd108, 0;
	mov.f64 	%fd5602, 0d0000000000000000;
	mov.f64 	%fd5603, %fd5602;
	mov.f64 	%fd5604, %fd5602;
	@%p65 bra 	$L__BB5_60;

	cvta.to.global.u64 	%rd194, %rd108;
	mul.lo.s64 	%rd195, %rd75, %rd50;
	add.s64 	%rd196, %rd194, %rd195;
	ld.global.f64 	%fd1996, [%rd196];
	add.f64 	%fd5604, %fd1996, 0d0000000000000000;
	ld.global.f64 	%fd1997, [%rd196+8];
	add.f64 	%fd5603, %fd1997, 0d0000000000000000;
	ld.global.f64 	%fd1998, [%rd196+16];
	add.f64 	%fd5602, %fd1998, 0d0000000000000000;

$L__BB5_60:
	setp.eq.s64 	%p66, %rd137, 0;
	@%p66 bra 	$L__BB5_62;

	cvta.to.global.u64 	%rd197, %rd137;
	mul.lo.s64 	%rd198, %rd75, %rd51;
	add.s64 	%rd199, %rd197, %rd198;
	ld.global.f64 	%fd1999, [%rd199];
	add.f64 	%fd5606, %fd1999, 0d0000000000000000;
	ld.global.f64 	%fd2000, [%rd199+8];
	add.f64 	%fd5605, %fd2000, 0d0000000000000000;
	bra.uni 	$L__BB5_64;

$L__BB5_62:
	setp.eq.s64 	%p67, %rd106, 0;
	mov.f64 	%fd5605, 0d0000000000000000;
	mov.f64 	%fd5606, %fd5605;
	@%p67 bra 	$L__BB5_64;

	cvta.to.global.u64 	%rd200, %rd106;
	mul.lo.s64 	%rd201, %rd75, %rd53;
	add.s64 	%rd202, %rd200, %rd201;
	ld.global.f64 	%fd2003, [%rd202];
	add.f64 	%fd5606, %fd2003, 0d0000000000000000;
	ld.global.f64 	%fd2004, [%rd202+8];
	add.f64 	%fd5605, %fd2004, 0d0000000000000000;

$L__BB5_64:
	@%p31 bra 	$L__BB5_68;

	setp.ge.f64 	%p2, %fd40, %fd39;
	add.f64 	%fd127, %fd38, %fd35;
	@%p2 bra 	$L__BB5_67;

	selp.f64 	%fd2007, %fd36, %fd39, %p2;
	mul.f64 	%fd2008, %fd34, %fd38;
	mul.f64 	%fd2009, %fd37, %fd35;
	sub.f64 	%fd2010, %fd2008, %fd2009;
	mul.f64 	%fd2011, %fd21, %fd27;
	mul.f64 	%fd2012, %fd18, %fd30;
	sub.f64 	%fd2013, %fd2012, %fd2011;
	mul.f64 	%fd2014, %fd15, %fd30;
	mul.f64 	%fd2015, %fd21, %fd24;
	sub.f64 	%fd2016, %fd2015, %fd2014;
	mul.f64 	%fd2017, %fd18, %fd24;
	mul.f64 	%fd2018, %fd15, %fd27;
	sub.f64 	%fd2019, %fd2018, %fd2017;
	setp.gt.f64 	%p69, %fd2010, 0d0000000000000000;
	setp.lt.f64 	%p70, %fd2010, %fd2007;
	mul.f64 	%fd2020, %fd32, %fd2016;
	fma.rn.f64 	%fd2021, %fd31, %fd2013, %fd2020;
	fma.rn.f64 	%fd2022, %fd33, %fd2019, %fd2021;
	setp.eq.f64 	%p71, %fd2022, 0d0000000000000000;
	mul.f64 	%fd2023, %fd2016, %fd2016;
	fma.rn.f64 	%fd2024, %fd2013, %fd2013, %fd2023;
	fma.rn.f64 	%fd2025, %fd2019, %fd2019, %fd2024;
	mul.f64 	%fd2026, %fd34, 0d3BC79CA100000000;
	mul.f64 	%fd2027, %fd2026, %fd36;
	setp.lt.f64 	%p72, %fd2025, %fd2027;
	or.pred  	%p73, %p71, %p72;
	and.pred  	%p74, %p69, %p70;
	and.pred  	%p75, %p73, %p74;
	mul.f64 	%fd2028, %fd39, 0d3FE0000000000000;
	setp.lt.f64 	%p76, %fd40, %fd2028;
	selp.b32 	%r569, 2, 5, %p76;
	selp.f64 	%fd2029, %fd38, %fd127, %p76;
	selp.f64 	%fd5608, %fd36, %fd2007, %p75;
	selp.b32 	%r656, %r569, 8, %p75;
	selp.f64 	%fd5607, %fd2029, %fd2010, %p75;

$L__BB5_67:
	selp.f64 	%fd5610, %fd36, %fd5608, %p2;
	selp.b32 	%r657, 5, %r656, %p2;
	selp.f64 	%fd5609, %fd127, %fd5607, %p2;

$L__BB5_68:
	selp.f64 	%fd136, %fd36, %fd5610, %p31;
	selp.b32 	%r658, 2, %r657, %p31;
	selp.f64 	%fd137, %fd38, %fd5609, %p31;
	setp.gtu.f64 	%p79, %fd137, 0d0000000000000000;
	@%p79 bra 	$L__BB5_72;
	bra.uni 	$L__BB5_69;

$L__BB5_72:
	setp.ltu.f64 	%p82, %fd137, %fd136;
	@%p82 bra 	$L__BB5_76;

	mov.f64 	%fd2031, 0d0000000000000000;
	sub.f64 	%fd2032, %fd2031, %fd37;
	add.f64 	%fd139, %fd2032, %fd35;
	setp.le.f64 	%p83, %fd139, 0d0000000000000000;
	mov.u32 	%r658, 1;
	@%p83 bra 	$L__BB5_76;

	setp.ge.f64 	%p84, %fd139, %fd34;
	mov.u32 	%r658, 4;
	@%p84 bra 	$L__BB5_76;

	mov.u32 	%r658, 7;
	bra.uni 	$L__BB5_76;

$L__BB5_69:
	mov.f64 	%fd2030, 0d0000000000000000;
	sub.f64 	%fd138, %fd2030, %fd37;
	setp.le.f64 	%p80, %fd138, 0d0000000000000000;
	mov.u32 	%r658, 0;
	@%p80 bra 	$L__BB5_76;

	setp.ge.f64 	%p81, %fd138, %fd34;
	mov.u32 	%r658, 3;
	@%p81 bra 	$L__BB5_76;

	mov.u32 	%r658, 6;

$L__BB5_76:
	setp.eq.s32 	%p85, %r658, 0;
	mov.f64 	%fd5819, 0d0000000000000000;
	mov.f64 	%fd5820, %fd5819;
	mov.f64 	%fd5821, %fd5819;
	mov.f64 	%fd5822, %fd5819;
	mov.f64 	%fd5823, %fd5819;
	mov.f64 	%fd5824, %fd5819;
	mov.f64 	%fd5711, %fd5819;
	mov.f64 	%fd5712, %fd5819;
	mov.f64 	%fd5713, %fd5819;
	mov.f64 	%fd5714, %fd5819;
	mov.f64 	%fd5715, %fd5819;
	mov.f64 	%fd5716, %fd5819;
	mov.f64 	%fd5682, %fd5819;
	mov.f64 	%fd5683, %fd5819;
	mov.f64 	%fd5684, %fd5819;
	mov.f64 	%fd5685, %fd5819;
	mov.f64 	%fd5686, %fd5819;
	mov.f64 	%fd5687, %fd5819;
	mov.f64 	%fd5660, %fd5819;
	mov.f64 	%fd5661, %fd5819;
	mov.f64 	%fd5662, %fd5819;
	mov.f64 	%fd5663, %fd5819;
	mov.f64 	%fd5664, %fd5819;
	mov.f64 	%fd5665, %fd5819;
	mov.f64 	%fd5666, %fd5819;
	mov.f64 	%fd5667, %fd5819;
	mov.f64 	%fd5668, %fd5819;
	mov.f64 	%fd5669, %fd5819;
	mov.f64 	%fd5670, %fd5819;
	mov.f64 	%fd5671, %fd5819;
	mov.f64 	%fd5672, %fd5819;
	mov.f64 	%fd5673, %fd5819;
	mov.f64 	%fd5674, %fd5819;
	mov.f64 	%fd5893, %fd5819;
	mov.f64 	%fd5894, %fd5819;
	@%p85 bra 	$L__BB5_106;

	setp.eq.s32 	%p3, %r658, 1;
	selp.f64 	%fd5818, 0d3FF0000000000000, 0d0000000000000000, %p3;
	mov.f64 	%fd5817, 0d0000000000000000;
	@%p3 bra 	$L__BB5_104;
	bra.uni 	$L__BB5_78;

$L__BB5_104:
	mov.f64 	%fd5819, 0d0000000000000000;
	mov.f64 	%fd5820, %fd5819;
	mov.f64 	%fd5821, %fd5819;
	mov.f64 	%fd5822, %fd5819;
	mov.f64 	%fd5823, %fd5819;
	mov.f64 	%fd5824, %fd5819;
	mov.f64 	%fd5711, %fd5819;
	mov.f64 	%fd5712, %fd5819;
	mov.f64 	%fd5713, %fd5819;
	mov.f64 	%fd5714, %fd5819;
	mov.f64 	%fd5715, %fd5819;
	mov.f64 	%fd5716, %fd5819;
	mov.f64 	%fd5682, %fd5819;
	mov.f64 	%fd5683, %fd5819;
	mov.f64 	%fd5684, %fd5819;
	mov.f64 	%fd5685, %fd5819;
	mov.f64 	%fd5686, %fd5819;
	mov.f64 	%fd5687, %fd5819;
	mov.f64 	%fd5660, %fd5819;
	mov.f64 	%fd5661, %fd5819;
	mov.f64 	%fd5662, %fd5819;
	mov.f64 	%fd5663, %fd5819;
	mov.f64 	%fd5664, %fd5819;
	mov.f64 	%fd5665, %fd5819;
	mov.f64 	%fd5666, %fd5819;
	mov.f64 	%fd5667, %fd5819;
	mov.f64 	%fd5668, %fd5819;
	mov.f64 	%fd5669, %fd5819;
	mov.f64 	%fd5670, %fd5819;
	mov.f64 	%fd5671, %fd5819;
	mov.f64 	%fd5672, %fd5819;
	mov.f64 	%fd5673, %fd5819;
	mov.f64 	%fd5674, %fd5819;
	mov.f64 	%fd5852, %fd5819;
	mov.f64 	%fd5853, %fd5819;
	bra.uni 	$L__BB5_105;

$L__BB5_78:
	setp.eq.s32 	%p4, %r658, 2;
	setp.ne.s32 	%p86, %r658, 2;
	mov.f64 	%fd5819, 0d0000000000000000;
	mov.f64 	%fd5820, %fd5819;
	mov.f64 	%fd5821, %fd5819;
	mov.f64 	%fd5822, %fd5819;
	mov.f64 	%fd5823, %fd5819;
	mov.f64 	%fd5824, %fd5819;
	mov.f64 	%fd5854, %fd5819;
	@%p86 bra 	$L__BB5_80;

	div.rn.f64 	%fd5854, %fd38, %fd36;
	mov.f64 	%fd5819, %fd33;
	mov.f64 	%fd5820, %fd32;
	mov.f64 	%fd5821, %fd31;
	mov.f64 	%fd5822, %fd30;
	mov.f64 	%fd5823, %fd27;
	mov.f64 	%fd5824, %fd24;

$L__BB5_80:
	selp.f64 	%fd5782, %fd5854, %fd5818, %p4;
	selp.f64 	%fd5781, 0d0000000000000000, %fd5817, %p4;
	@%p4 bra 	$L__BB5_102;
	bra.uni 	$L__BB5_81;

$L__BB5_102:
	mov.f64 	%fd5711, 0d0000000000000000;
	mov.f64 	%fd5712, %fd5711;
	mov.f64 	%fd5713, %fd5711;
	mov.f64 	%fd5714, %fd5711;
	mov.f64 	%fd5715, %fd5711;
	mov.f64 	%fd5716, %fd5711;
	mov.f64 	%fd5682, %fd5711;
	mov.f64 	%fd5683, %fd5711;
	mov.f64 	%fd5684, %fd5711;
	mov.f64 	%fd5685, %fd5711;
	mov.f64 	%fd5686, %fd5711;
	mov.f64 	%fd5687, %fd5711;
	mov.f64 	%fd5660, %fd5711;
	mov.f64 	%fd5661, %fd5711;
	mov.f64 	%fd5662, %fd5711;
	mov.f64 	%fd5663, %fd5711;
	mov.f64 	%fd5664, %fd5711;
	mov.f64 	%fd5665, %fd5711;
	mov.f64 	%fd5666, %fd5711;
	mov.f64 	%fd5667, %fd5711;
	mov.f64 	%fd5668, %fd5711;
	mov.f64 	%fd5669, %fd5711;
	mov.f64 	%fd5670, %fd5711;
	mov.f64 	%fd5671, %fd5711;
	mov.f64 	%fd5672, %fd5711;
	mov.f64 	%fd5673, %fd5711;
	mov.f64 	%fd5674, %fd5711;
	mov.f64 	%fd5810, %fd5711;
	mov.f64 	%fd5811, %fd5711;
	bra.uni 	$L__BB5_103;

$L__BB5_81:
	setp.eq.s32 	%p5, %r658, 3;
	selp.f64 	%fd2077, 0d3FF0000000000000, 0d0000000000000000, %p5;
	selp.f64 	%fd5745, %fd2077, %fd5781, %p5;
	selp.f64 	%fd5746, 0d0000000000000000, %fd5782, %p5;
	@%p5 bra 	$L__BB5_100;
	bra.uni 	$L__BB5_82;

$L__BB5_100:
	mov.f64 	%fd5711, 0d0000000000000000;
	mov.f64 	%fd5712, %fd5711;
	mov.f64 	%fd5713, %fd5711;
	mov.f64 	%fd5714, %fd5711;
	mov.f64 	%fd5715, %fd5711;
	mov.f64 	%fd5716, %fd5711;
	mov.f64 	%fd5682, %fd5711;
	mov.f64 	%fd5683, %fd5711;
	mov.f64 	%fd5684, %fd5711;
	mov.f64 	%fd5685, %fd5711;
	mov.f64 	%fd5686, %fd5711;
	mov.f64 	%fd5687, %fd5711;
	mov.f64 	%fd5660, %fd5711;
	mov.f64 	%fd5661, %fd5711;
	mov.f64 	%fd5662, %fd5711;
	mov.f64 	%fd5663, %fd5711;
	mov.f64 	%fd5664, %fd5711;
	mov.f64 	%fd5665, %fd5711;
	mov.f64 	%fd5666, %fd5711;
	mov.f64 	%fd5667, %fd5711;
	mov.f64 	%fd5668, %fd5711;
	mov.f64 	%fd5669, %fd5711;
	mov.f64 	%fd5670, %fd5711;
	mov.f64 	%fd5671, %fd5711;
	mov.f64 	%fd5672, %fd5711;
	mov.f64 	%fd5673, %fd5711;
	mov.f64 	%fd5674, %fd5711;
	mov.f64 	%fd5774, %fd5711;
	mov.f64 	%fd5775, %fd5711;
	bra.uni 	$L__BB5_101;

$L__BB5_82:
	setp.eq.s32 	%p6, %r658, 4;
	selp.f64 	%fd2078, 0d3FF0000000000000, 0d0000000000000000, %p6;
	selp.f64 	%fd5709, %fd2078, %fd5745, %p6;
	selp.f64 	%fd5710, %fd2078, %fd5746, %p6;
	@%p6 bra 	$L__BB5_98;
	bra.uni 	$L__BB5_83;

$L__BB5_98:
	mov.f64 	%fd5711, 0d0000000000000000;
	mov.f64 	%fd5712, %fd5711;
	mov.f64 	%fd5713, %fd5711;
	mov.f64 	%fd5714, %fd5711;
	mov.f64 	%fd5715, %fd5711;
	mov.f64 	%fd5716, %fd5711;
	mov.f64 	%fd5682, %fd5711;
	mov.f64 	%fd5683, %fd5711;
	mov.f64 	%fd5684, %fd5711;
	mov.f64 	%fd5685, %fd5711;
	mov.f64 	%fd5686, %fd5711;
	mov.f64 	%fd5687, %fd5711;
	mov.f64 	%fd5660, %fd5711;
	mov.f64 	%fd5661, %fd5711;
	mov.f64 	%fd5662, %fd5711;
	mov.f64 	%fd5663, %fd5711;
	mov.f64 	%fd5664, %fd5711;
	mov.f64 	%fd5665, %fd5711;
	mov.f64 	%fd5666, %fd5711;
	mov.f64 	%fd5667, %fd5711;
	mov.f64 	%fd5668, %fd5711;
	mov.f64 	%fd5669, %fd5711;
	mov.f64 	%fd5670, %fd5711;
	mov.f64 	%fd5671, %fd5711;
	mov.f64 	%fd5672, %fd5711;
	mov.f64 	%fd5673, %fd5711;
	mov.f64 	%fd5674, %fd5711;
	mov.f64 	%fd5738, %fd5711;
	mov.f64 	%fd5739, %fd5711;
	bra.uni 	$L__BB5_99;

$L__BB5_83:
	setp.eq.s32 	%p7, %r658, 5;
	setp.ne.s32 	%p88, %r658, 5;
	mov.f64 	%fd5711, 0d0000000000000000;
	mov.f64 	%fd5712, %fd5711;
	mov.f64 	%fd5713, %fd5711;
	mov.f64 	%fd5714, %fd5711;
	mov.f64 	%fd5715, %fd5711;
	mov.f64 	%fd5716, %fd5711;
	mov.f64 	%fd5740, %fd5711;
	mov.f64 	%fd5626, %fd5711;
	@%p88 bra 	$L__BB5_85;

	sub.f64 	%fd5713, %fd13, %fd23;
	sub.f64 	%fd5712, %fd16, %fd26;
	mul.f64 	%fd2089, %fd5712, %fd27;
	fma.rn.f64 	%fd2090, %fd5713, %fd24, %fd2089;
	sub.f64 	%fd5711, %fd19, %fd29;
	fma.rn.f64 	%fd2091, %fd5711, %fd30, %fd2090;
	div.rn.f64 	%fd5740, %fd2091, %fd36;
	mov.f64 	%fd5626, 0d3FF0000000000000;
	mov.f64 	%fd5714, %fd30;
	mov.f64 	%fd5715, %fd27;
	mov.f64 	%fd5716, %fd24;

$L__BB5_85:
	selp.f64 	%fd5680, %fd5626, %fd5709, %p7;
	selp.f64 	%fd5681, %fd5740, %fd5710, %p7;
	@%p7 bra 	$L__BB5_96;
	bra.uni 	$L__BB5_86;

$L__BB5_96:
	mov.f64 	%fd5682, 0d0000000000000000;
	mov.f64 	%fd5683, %fd5682;
	mov.f64 	%fd5684, %fd5682;
	mov.f64 	%fd5685, %fd5682;
	mov.f64 	%fd5686, %fd5682;
	mov.f64 	%fd5687, %fd5682;
	mov.f64 	%fd5660, %fd5682;
	mov.f64 	%fd5661, %fd5682;
	mov.f64 	%fd5662, %fd5682;
	mov.f64 	%fd5663, %fd5682;
	mov.f64 	%fd5664, %fd5682;
	mov.f64 	%fd5665, %fd5682;
	mov.f64 	%fd5666, %fd5682;
	mov.f64 	%fd5667, %fd5682;
	mov.f64 	%fd5668, %fd5682;
	mov.f64 	%fd5669, %fd5682;
	mov.f64 	%fd5670, %fd5682;
	mov.f64 	%fd5671, %fd5682;
	mov.f64 	%fd5672, %fd5682;
	mov.f64 	%fd5673, %fd5682;
	mov.f64 	%fd5674, %fd5682;
	mov.f64 	%fd5703, %fd5682;
	mov.f64 	%fd5704, %fd5682;
	bra.uni 	$L__BB5_97;

$L__BB5_86:
	setp.eq.s32 	%p8, %r658, 6;
	setp.ne.s32 	%p90, %r658, 6;
	mov.f64 	%fd5682, 0d0000000000000000;
	mov.f64 	%fd5683, %fd5682;
	mov.f64 	%fd5684, %fd5682;
	mov.f64 	%fd5685, %fd5682;
	mov.f64 	%fd5686, %fd5682;
	mov.f64 	%fd5687, %fd5682;
	mov.f64 	%fd5705, %fd5682;
	@%p90 bra 	$L__BB5_88;

	sub.f64 	%fd5684, %fd23, %fd14;
	sub.f64 	%fd5683, %fd26, %fd17;
	mul.f64 	%fd2100, %fd18, %fd5683;
	fma.rn.f64 	%fd2101, %fd15, %fd5684, %fd2100;
	sub.f64 	%fd5682, %fd29, %fd20;
	fma.rn.f64 	%fd2102, %fd21, %fd5682, %fd2101;
	div.rn.f64 	%fd5705, %fd2102, %fd34;
	mov.f64 	%fd5685, %fd21;
	mov.f64 	%fd5686, %fd18;
	mov.f64 	%fd5687, %fd15;

$L__BB5_88:
	selp.f64 	%fd5658, %fd5705, %fd5680, %p8;
	selp.f64 	%fd5659, 0d0000000000000000, %fd5681, %p8;
	@%p8 bra 	$L__BB5_94;
	bra.uni 	$L__BB5_89;

$L__BB5_94:
	mov.f64 	%fd5660, 0d0000000000000000;
	mov.f64 	%fd5661, %fd5660;
	mov.f64 	%fd5662, %fd5660;
	mov.f64 	%fd5663, %fd5660;
	mov.f64 	%fd5664, %fd5660;
	mov.f64 	%fd5665, %fd5660;
	mov.f64 	%fd5666, %fd5660;
	mov.f64 	%fd5667, %fd5660;
	mov.f64 	%fd5668, %fd5660;
	mov.f64 	%fd5669, %fd5660;
	mov.f64 	%fd5670, %fd5660;
	mov.f64 	%fd5671, %fd5660;
	mov.f64 	%fd5672, %fd5660;
	mov.f64 	%fd5673, %fd5660;
	mov.f64 	%fd5674, %fd5660;
	mov.f64 	%fd5675, %fd5660;
	mov.f64 	%fd5676, %fd5660;
	bra.uni 	$L__BB5_95;

$L__BB5_89:
	setp.eq.s32 	%p9, %r658, 7;
	setp.ne.s32 	%p92, %r658, 7;
	mov.f64 	%fd5666, 0d0000000000000000;
	mov.f64 	%fd5660, %fd5666;
	mov.f64 	%fd5661, %fd5666;
	mov.f64 	%fd5662, %fd5666;
	mov.f64 	%fd5663, %fd5666;
	mov.f64 	%fd5664, %fd5666;
	mov.f64 	%fd5665, %fd5666;
	mov.f64 	%fd5642, %fd5666;
	mov.f64 	%fd5677, %fd5666;
	@%p92 bra 	$L__BB5_91;

	sub.f64 	%fd5662, %fd22, %fd14;
	sub.f64 	%fd5661, %fd25, %fd17;
	mul.f64 	%fd2113, %fd18, %fd5661;
	fma.rn.f64 	%fd2114, %fd15, %fd5662, %fd2113;
	sub.f64 	%fd5660, %fd28, %fd20;
	fma.rn.f64 	%fd2115, %fd21, %fd5660, %fd2114;
	div.rn.f64 	%fd5677, %fd2115, %fd34;
	mov.f64 	%fd5642, 0d3FF0000000000000;
	mov.f64 	%fd5663, %fd21;
	mov.f64 	%fd5664, %fd18;
	mov.f64 	%fd5665, %fd15;

$L__BB5_91:
	selp.f64 	%fd212, %fd5642, %fd5659, %p9;
	selp.f64 	%fd211, %fd5677, %fd5658, %p9;
	mov.f64 	%fd5667, %fd5666;
	mov.f64 	%fd5668, %fd5666;
	mov.f64 	%fd5669, %fd5666;
	mov.f64 	%fd5670, %fd5666;
	mov.f64 	%fd5671, %fd5666;
	mov.f64 	%fd5672, %fd5666;
	mov.f64 	%fd5673, %fd5666;
	mov.f64 	%fd5674, %fd5666;
	mov.f64 	%fd5654, %fd5666;
	mov.f64 	%fd5655, %fd5666;
	@%p9 bra 	$L__BB5_93;

	mov.f64 	%fd2128, 0d0000000000000000;
	sub.f64 	%fd5678, %fd2128, %fd35;
	div.rn.f64 	%fd2129, %fd5678, %fd34;
	mul.f64 	%fd2130, %fd2129, %fd2129;
	mul.f64 	%fd2131, %fd34, %fd2130;
	sub.f64 	%fd2132, %fd36, %fd2131;
	sub.f64 	%fd5679, %fd2128, %fd37;
	mul.f64 	%fd2133, %fd5679, %fd2129;
	sub.f64 	%fd2134, %fd38, %fd2133;
	div.rn.f64 	%fd5654, %fd2134, %fd2132;
	mul.f64 	%fd2135, %fd34, %fd2129;
	mul.f64 	%fd2136, %fd2135, %fd5654;
	sub.f64 	%fd2137, %fd5679, %fd2136;
	div.rn.f64 	%fd5655, %fd2137, %fd34;
	mov.f64 	%fd5666, %fd33;
	mov.f64 	%fd5667, %fd32;
	mov.f64 	%fd5668, %fd31;
	mov.f64 	%fd5669, %fd21;
	mov.f64 	%fd5670, %fd18;
	mov.f64 	%fd5671, %fd15;
	mov.f64 	%fd5672, %fd30;
	mov.f64 	%fd5673, %fd27;
	mov.f64 	%fd5674, %fd24;

$L__BB5_93:
	selp.f64 	%fd5676, %fd211, %fd5655, %p9;
	selp.f64 	%fd5675, %fd212, %fd5654, %p9;
	selp.u16 	%rs321, 1, 0, %p9;

$L__BB5_95:
	selp.f64 	%fd5704, %fd5658, %fd5676, %p8;
	selp.f64 	%fd5703, %fd5659, %fd5675, %p8;
	selp.u16 	%rs323, 1, 0, %p8;

$L__BB5_97:
	selp.f64 	%fd5739, %fd5680, %fd5704, %p7;
	selp.f64 	%fd5738, %fd5681, %fd5703, %p7;
	selp.u16 	%rs326, 1, 0, %p7;

$L__BB5_99:
	selp.f64 	%fd5775, %fd5709, %fd5739, %p6;
	selp.f64 	%fd5774, %fd5710, %fd5738, %p6;
	selp.u16 	%rs330, 1, 0, %p6;

$L__BB5_101:
	selp.f64 	%fd5811, %fd5745, %fd5775, %p5;
	selp.f64 	%fd5810, %fd5746, %fd5774, %p5;
	selp.u16 	%rs335, 1, 0, %p5;

$L__BB5_103:
	selp.f64 	%fd5853, %fd5781, %fd5811, %p4;
	selp.f64 	%fd5852, %fd5782, %fd5810, %p4;
	selp.u16 	%rs341, 1, 0, %p4;

$L__BB5_105:
	selp.f64 	%fd5894, %fd5817, %fd5853, %p3;
	selp.f64 	%fd5893, %fd5818, %fd5852, %p3;
	selp.u16 	%rs340, 1, 0, %p3;

$L__BB5_106:
	selp.f64 	%fd501, 0d0000000000000000, %fd5893, %p85;
	selp.f64 	%fd499, 0d0000000000000000, %fd5894, %p85;
	mov.f64 	%fd2306, 0d3FF0000000000000;
	sub.f64 	%fd500, %fd2306, %fd499;
	mul.f64 	%fd2307, %fd14, %fd500;
	mul.f64 	%fd2308, %fd17, %fd500;
	mul.f64 	%fd2309, %fd20, %fd500;
	fma.rn.f64 	%fd2310, %fd13, %fd499, %fd2307;
	fma.rn.f64 	%fd2311, %fd16, %fd499, %fd2308;
	fma.rn.f64 	%fd2312, %fd19, %fd499, %fd2309;
	sub.f64 	%fd502, %fd2306, %fd501;
	mul.f64 	%fd2313, %fd23, %fd502;
	mul.f64 	%fd2314, %fd26, %fd502;
	mul.f64 	%fd2315, %fd29, %fd502;
	fma.rn.f64 	%fd2316, %fd22, %fd501, %fd2313;
	fma.rn.f64 	%fd2317, %fd25, %fd501, %fd2314;
	fma.rn.f64 	%fd2318, %fd28, %fd501, %fd2315;
	sub.f64 	%fd2319, %fd2310, %fd2316;
	sub.f64 	%fd2320, %fd2311, %fd2317;
	sub.f64 	%fd2321, %fd2312, %fd2318;
	mul.f64 	%fd2322, %fd2320, %fd2320;
	fma.rn.f64 	%fd2323, %fd2319, %fd2319, %fd2322;
	fma.rn.f64 	%fd2324, %fd2321, %fd2321, %fd2323;
	sqrt.rn.f64 	%fd2325, %fd2324;
	div.rn.f64 	%fd503, %fd2319, %fd2325;
	div.rn.f64 	%fd504, %fd2320, %fd2325;
	div.rn.f64 	%fd505, %fd2321, %fd2325;
	add.f64 	%fd506, %fd5606, 0d0000000000000000;
	add.f64 	%fd2326, %fd5604, 0d0000000000000000;
	add.f64 	%fd2327, %fd5603, 0d0000000000000000;
	mul.f64 	%fd2328, %fd2327, %fd2320;
	fma.rn.f64 	%fd2329, %fd2326, %fd2319, %fd2328;
	add.f64 	%fd2330, %fd5602, 0d0000000000000000;
	fma.rn.f64 	%fd2331, %fd2330, %fd2321, %fd2329;
	mul.f64 	%fd2332, %fd2325, %fd2325;
	div.rn.f64 	%fd507, %fd2331, %fd2332;
	div.rn.f64 	%fd2333, %fd2326, %fd2325;
	add.f64 	%fd5903, %fd2333, 0d0000000000000000;
	div.rn.f64 	%fd2334, %fd2327, %fd2325;
	add.f64 	%fd5902, %fd2334, 0d0000000000000000;
	div.rn.f64 	%fd2335, %fd2330, %fd2325;
	add.f64 	%fd5901, %fd2335, 0d0000000000000000;
	setp.leu.f64 	%p102, %fd2325, 0d0000000000000000;
	@%p102 bra 	$L__BB5_108;

	mov.f64 	%fd2336, 0d0000000000000000;
	sub.f64 	%fd2337, %fd2336, %fd507;
	fma.rn.f64 	%fd5903, %fd503, %fd2337, %fd5903;
	fma.rn.f64 	%fd5902, %fd504, %fd2337, %fd5902;
	fma.rn.f64 	%fd5901, %fd505, %fd2337, %fd5901;

$L__BB5_108:
	mov.f64 	%fd2338, 0d0000000000000000;
	sub.f64 	%fd2339, %fd2338, %fd5903;
	add.f64 	%fd2340, %fd2339, 0d0000000000000000;
	sub.f64 	%fd2341, %fd2338, %fd5902;
	add.f64 	%fd2342, %fd2341, 0d0000000000000000;
	sub.f64 	%fd2343, %fd2338, %fd5901;
	add.f64 	%fd2344, %fd2343, 0d0000000000000000;
	fma.rn.f64 	%fd5956, %fd501, %fd2340, 0d0000000000000000;
	fma.rn.f64 	%fd5955, %fd501, %fd2342, 0d0000000000000000;
	fma.rn.f64 	%fd5954, %fd501, %fd2344, 0d0000000000000000;
	mul.f64 	%fd2345, %fd22, %fd2340;
	fma.rn.f64 	%fd2346, %fd25, %fd2342, %fd2345;
	fma.rn.f64 	%fd2347, %fd28, %fd2344, %fd2346;
	add.f64 	%fd2348, %fd2347, 0d0000000000000000;
	fma.rn.f64 	%fd5940, %fd502, %fd2340, 0d0000000000000000;
	fma.rn.f64 	%fd5939, %fd502, %fd2342, 0d0000000000000000;
	fma.rn.f64 	%fd5938, %fd502, %fd2344, 0d0000000000000000;
	add.f64 	%fd2349, %fd5605, 0d0000000000000000;
	add.f64 	%fd2350, %fd2349, %fd2348;
	mul.f64 	%fd2351, %fd23, %fd2340;
	fma.rn.f64 	%fd2352, %fd26, %fd2342, %fd2351;
	fma.rn.f64 	%fd2353, %fd29, %fd2344, %fd2352;
	add.f64 	%fd2354, %fd2353, 0d0000000000000000;
	sub.f64 	%fd2355, %fd2338, %fd2354;
	add.f64 	%fd2356, %fd5903, 0d0000000000000000;
	fma.rn.f64 	%fd5937, %fd499, %fd2356, 0d0000000000000000;
	add.f64 	%fd2357, %fd5902, 0d0000000000000000;
	fma.rn.f64 	%fd5936, %fd499, %fd2357, 0d0000000000000000;
	add.f64 	%fd2358, %fd5901, 0d0000000000000000;
	fma.rn.f64 	%fd5935, %fd499, %fd2358, 0d0000000000000000;
	add.f64 	%fd526, %fd2350, %fd2355;
	mul.f64 	%fd2359, %fd13, %fd2356;
	fma.rn.f64 	%fd2360, %fd16, %fd2357, %fd2359;
	fma.rn.f64 	%fd2361, %fd19, %fd2358, %fd2360;
	add.f64 	%fd2362, %fd2361, 0d0000000000000000;
	fma.rn.f64 	%fd5934, %fd500, %fd2356, 0d0000000000000000;
	fma.rn.f64 	%fd5933, %fd500, %fd2357, 0d0000000000000000;
	fma.rn.f64 	%fd5932, %fd500, %fd2358, 0d0000000000000000;
	add.f64 	%fd2363, %fd506, %fd2362;
	mul.f64 	%fd2364, %fd14, %fd2356;
	fma.rn.f64 	%fd2365, %fd17, %fd2357, %fd2364;
	fma.rn.f64 	%fd2366, %fd20, %fd2358, %fd2365;
	add.f64 	%fd2367, %fd2366, 0d0000000000000000;
	sub.f64 	%fd2368, %fd2338, %fd2367;
	add.f64 	%fd530, %fd2363, %fd2368;
	and.b16  	%rs282, %rs340, 255;
	setp.ne.s16 	%p104, %rs282, 0;
	or.pred  	%p105, %p104, %p85;
	@%p105 bra 	$L__BB5_122;

	add.f64 	%fd5944, %fd526, 0d0000000000000000;
	and.b16  	%rs283, %rs341, 255;
	setp.eq.s16 	%p106, %rs283, 0;
	@%p106 bra 	$L__BB5_111;

	div.rn.f64 	%fd2369, %fd5944, %fd36;
	add.f64 	%fd2370, %fd2369, 0d0000000000000000;
	mov.f64 	%fd2371, 0d0000000000000000;
	mul.f64 	%fd2372, %fd5854, %fd5944;
	div.rn.f64 	%fd2373, %fd2372, %fd36;
	sub.f64 	%fd2374, %fd2371, %fd2373;
	add.f64 	%fd2375, %fd5824, %fd5824;
	add.f64 	%fd2376, %fd5823, %fd5823;
	add.f64 	%fd2377, %fd5822, %fd5822;
	fma.rn.f64 	%fd2378, %fd2375, %fd2374, 0d0000000000000000;
	fma.rn.f64 	%fd2379, %fd2376, %fd2374, 0d0000000000000000;
	fma.rn.f64 	%fd2380, %fd2377, %fd2374, 0d0000000000000000;
	fma.rn.f64 	%fd2381, %fd5824, %fd2370, 0d0000000000000000;
	fma.rn.f64 	%fd2382, %fd5823, %fd2370, 0d0000000000000000;
	fma.rn.f64 	%fd2383, %fd5822, %fd2370, 0d0000000000000000;
	fma.rn.f64 	%fd2384, %fd5821, %fd2370, %fd2378;
	fma.rn.f64 	%fd2385, %fd5820, %fd2370, %fd2379;
	fma.rn.f64 	%fd2386, %fd5819, %fd2370, %fd2380;
	add.f64 	%fd5956, %fd5956, %fd2384;
	add.f64 	%fd5955, %fd5955, %fd2385;
	add.f64 	%fd5954, %fd5954, %fd2386;
	sub.f64 	%fd2387, %fd5940, %fd2384;
	sub.f64 	%fd2388, %fd5939, %fd2385;
	sub.f64 	%fd2389, %fd5938, %fd2386;
	add.f64 	%fd5934, %fd5934, %fd2381;
	add.f64 	%fd5933, %fd5933, %fd2382;
	add.f64 	%fd5932, %fd5932, %fd2383;
	sub.f64 	%fd5940, %fd2387, %fd2381;
	sub.f64 	%fd5939, %fd2388, %fd2382;
	sub.f64 	%fd5938, %fd2389, %fd2383;
	bra.uni 	$L__BB5_122;

$L__BB5_111:
	or.b16  	%rs284, %rs330, %rs335;
	and.b16  	%rs285, %rs284, 255;
	setp.ne.s16 	%p107, %rs285, 0;
	@%p107 bra 	$L__BB5_122;

	and.b16  	%rs286, %rs326, 255;
	setp.eq.s16 	%p10, %rs286, 0;
	setp.ne.s16 	%p108, %rs286, 0;
	@%p108 bra 	$L__BB5_120;

	add.f64 	%fd5931, %fd530, 0d0000000000000000;
	and.b16  	%rs287, %rs323, 255;
	setp.eq.s16 	%p11, %rs287, 0;
	setp.ne.s16 	%p109, %rs287, 0;
	mov.f64 	%fd5930, %fd5944;
	@%p109 bra 	$L__BB5_118;

	and.b16  	%rs288, %rs321, 255;
	setp.ne.s16 	%p110, %rs288, 0;
	mov.f64 	%fd5916, %fd5944;
	mov.f64 	%fd5917, %fd5931;
	@%p110 bra 	$L__BB5_116;

	div.rn.f64 	%fd2392, %fd5678, %fd34;
	mul.f64 	%fd2393, %fd2392, %fd2392;
	mul.f64 	%fd2394, %fd34, %fd2393;
	sub.f64 	%fd2395, %fd36, %fd2394;
	mul.f64 	%fd2396, %fd2392, %fd5679;
	sub.f64 	%fd2397, %fd38, %fd2396;
	div.rn.f64 	%fd2398, %fd2397, %fd2395;
	mul.f64 	%fd2399, %fd34, %fd2392;
	mul.f64 	%fd2400, %fd2399, %fd2398;
	sub.f64 	%fd2401, %fd5679, %fd2400;
	div.rn.f64 	%fd2402, %fd2401, %fd34;
	div.rn.f64 	%fd2403, %fd5931, %fd34;
	add.f64 	%fd2404, %fd2403, 0d0000000000000000;
	mov.f64 	%fd5916, 0d0000000000000000;
	mul.f64 	%fd2405, %fd2402, %fd5931;
	div.rn.f64 	%fd2406, %fd2405, %fd34;
	sub.f64 	%fd2407, %fd5916, %fd2406;
	sub.f64 	%fd2408, %fd5916, %fd2404;
	fma.rn.f64 	%fd2409, %fd2398, %fd2408, 0d0000000000000000;
	fma.rn.f64 	%fd2410, %fd2399, %fd2408, %fd5944;
	fma.rn.f64 	%fd2411, %fd2392, %fd2409, %fd2407;
	fma.rn.f64 	%fd2412, %fd34, %fd2409, 0d0000000000000000;
	div.rn.f64 	%fd2413, %fd2410, %fd2395;
	add.f64 	%fd2414, %fd2413, 0d0000000000000000;
	mul.f64 	%fd2415, %fd2398, %fd2410;
	div.rn.f64 	%fd2416, %fd2415, %fd2395;
	sub.f64 	%fd2417, %fd5916, %fd2416;
	sub.f64 	%fd2418, %fd5916, %fd2414;
	fma.rn.f64 	%fd2419, %fd5679, %fd2418, %fd2412;
	fma.rn.f64 	%fd2420, %fd2392, %fd2418, %fd2404;
	add.f64 	%fd2421, %fd2420, 0d0000000000000000;
	add.f64 	%fd2422, %fd2417, 0d0000000000000000;
	sub.f64 	%fd2423, %fd5916, %fd2417;
	fma.rn.f64 	%fd2424, %fd34, %fd2423, 0d0000000000000000;
	fma.rn.f64 	%fd2425, %fd2393, %fd2423, %fd2411;
	fma.rn.f64 	%fd2426, %fd2392, %fd2424, %fd2419;
	fma.rn.f64 	%fd2427, %fd2392, %fd2424, %fd2426;
	div.rn.f64 	%fd2428, %fd2427, %fd34;
	add.f64 	%fd2429, %fd2428, 0d0000000000000000;
	mul.f64 	%fd2430, %fd2392, %fd2427;
	div.rn.f64 	%fd2431, %fd2430, %fd34;
	sub.f64 	%fd2432, %fd2425, %fd2431;
	add.f64 	%fd2433, %fd2432, 0d0000000000000000;
	fma.rn.f64 	%fd2434, %fd5674, %fd2414, 0d0000000000000000;
	fma.rn.f64 	%fd2435, %fd5673, %fd2414, 0d0000000000000000;
	fma.rn.f64 	%fd2436, %fd5672, %fd2414, 0d0000000000000000;
	fma.rn.f64 	%fd2437, %fd5668, %fd2414, 0d0000000000000000;
	fma.rn.f64 	%fd2438, %fd5667, %fd2414, 0d0000000000000000;
	fma.rn.f64 	%fd2439, %fd5666, %fd2414, 0d0000000000000000;
	sub.f64 	%fd2440, %fd5916, %fd2421;
	fma.rn.f64 	%fd2441, %fd5671, %fd2440, %fd2434;
	fma.rn.f64 	%fd2442, %fd5670, %fd2440, %fd2435;
	fma.rn.f64 	%fd2443, %fd5669, %fd2440, %fd2436;
	fma.rn.f64 	%fd2444, %fd5668, %fd2440, 0d0000000000000000;
	fma.rn.f64 	%fd2445, %fd5667, %fd2440, 0d0000000000000000;
	fma.rn.f64 	%fd2446, %fd5666, %fd2440, 0d0000000000000000;
	add.f64 	%fd2447, %fd5674, %fd5674;
	add.f64 	%fd2448, %fd5673, %fd5673;
	add.f64 	%fd2449, %fd5672, %fd5672;
	fma.rn.f64 	%fd2450, %fd2447, %fd2422, %fd2437;
	fma.rn.f64 	%fd2451, %fd2448, %fd2422, %fd2438;
	fma.rn.f64 	%fd2452, %fd2449, %fd2422, %fd2439;
	sub.f64 	%fd2453, %fd5916, %fd2429;
	fma.rn.f64 	%fd2454, %fd5674, %fd2453, %fd2444;
	fma.rn.f64 	%fd2455, %fd5673, %fd2453, %fd2445;
	fma.rn.f64 	%fd2456, %fd5672, %fd2453, %fd2446;
	fma.rn.f64 	%fd2457, %fd5671, %fd2453, %fd2450;
	fma.rn.f64 	%fd2458, %fd5670, %fd2453, %fd2451;
	fma.rn.f64 	%fd2459, %fd5669, %fd2453, %fd2452;
	add.f64 	%fd2460, %fd5671, %fd5671;
	add.f64 	%fd2461, %fd5670, %fd5670;
	add.f64 	%fd2462, %fd5669, %fd5669;
	fma.rn.f64 	%fd2463, %fd2460, %fd2433, %fd2454;
	fma.rn.f64 	%fd2464, %fd2461, %fd2433, %fd2455;
	fma.rn.f64 	%fd2465, %fd2462, %fd2433, %fd2456;
	add.f64 	%fd5956, %fd5956, %fd2457;
	add.f64 	%fd5955, %fd5955, %fd2458;
	add.f64 	%fd5954, %fd5954, %fd2459;
	sub.f64 	%fd2466, %fd5940, %fd2457;
	sub.f64 	%fd2467, %fd5939, %fd2458;
	sub.f64 	%fd2468, %fd5938, %fd2459;
	add.f64 	%fd5937, %fd5937, %fd2463;
	add.f64 	%fd5936, %fd5936, %fd2464;
	add.f64 	%fd5935, %fd5935, %fd2465;
	sub.f64 	%fd2469, %fd5934, %fd2463;
	sub.f64 	%fd2470, %fd5933, %fd2464;
	sub.f64 	%fd2471, %fd5932, %fd2465;
	add.f64 	%fd5934, %fd2441, %fd2469;
	add.f64 	%fd5933, %fd2442, %fd2470;
	add.f64 	%fd5932, %fd2443, %fd2471;
	sub.f64 	%fd5940, %fd2466, %fd2441;
	sub.f64 	%fd5939, %fd2467, %fd2442;
	sub.f64 	%fd5938, %fd2468, %fd2443;
	mov.f64 	%fd5917, %fd5916;

$L__BB5_116:
	selp.f64 	%fd2472, 0d0000000000000000, %fd5931, %p11;
	add.f64 	%fd5931, %fd2472, %fd5917;
	selp.f64 	%fd2473, 0d0000000000000000, %fd5944, %p11;
	add.f64 	%fd5930, %fd2473, %fd5916;
	setp.eq.s16 	%p111, %rs288, 0;
	@%p111 bra 	$L__BB5_118;

	add.f64 	%fd2476, %fd5917, 0d0000000000000000;
	mov.f64 	%fd5930, 0d0000000000000000;
	div.rn.f64 	%fd2477, %fd2476, %fd34;
	add.f64 	%fd2478, %fd2477, 0d0000000000000000;
	mul.f64 	%fd2479, %fd5677, %fd2476;
	div.rn.f64 	%fd2480, %fd2479, %fd34;
	sub.f64 	%fd2481, %fd5930, %fd2480;
	add.f64 	%fd2482, %fd5665, %fd5665;
	add.f64 	%fd2483, %fd5664, %fd5664;
	add.f64 	%fd2484, %fd5663, %fd5663;
	fma.rn.f64 	%fd2485, %fd2482, %fd2481, 0d0000000000000000;
	fma.rn.f64 	%fd2486, %fd2483, %fd2481, 0d0000000000000000;
	fma.rn.f64 	%fd2487, %fd2484, %fd2481, 0d0000000000000000;
	fma.rn.f64 	%fd2488, %fd5665, %fd2478, 0d0000000000000000;
	fma.rn.f64 	%fd2489, %fd5664, %fd2478, 0d0000000000000000;
	fma.rn.f64 	%fd2490, %fd5663, %fd2478, 0d0000000000000000;
	fma.rn.f64 	%fd2491, %fd5662, %fd2478, %fd2485;
	fma.rn.f64 	%fd2492, %fd5661, %fd2478, %fd2486;
	fma.rn.f64 	%fd2493, %fd5660, %fd2478, %fd2487;
	add.f64 	%fd5937, %fd5937, %fd2491;
	add.f64 	%fd5936, %fd5936, %fd2492;
	add.f64 	%fd5935, %fd5935, %fd2493;
	sub.f64 	%fd2494, %fd5934, %fd2491;
	sub.f64 	%fd2495, %fd5933, %fd2492;
	sub.f64 	%fd2496, %fd5932, %fd2493;
	add.f64 	%fd5956, %fd5956, %fd2488;
	add.f64 	%fd5955, %fd5955, %fd2489;
	add.f64 	%fd5954, %fd5954, %fd2490;
	sub.f64 	%fd5934, %fd2494, %fd2488;
	sub.f64 	%fd5933, %fd2495, %fd2489;
	sub.f64 	%fd5932, %fd2496, %fd2490;
	mov.f64 	%fd5931, %fd5930;

$L__BB5_118:
	selp.f64 	%fd2497, 0d0000000000000000, %fd5944, %p10;
	add.f64 	%fd5944, %fd2497, %fd5930;
	@%p11 bra 	$L__BB5_120;

	add.f64 	%fd2499, %fd5931, 0d0000000000000000;
	mov.f64 	%fd5944, 0d0000000000000000;
	div.rn.f64 	%fd2500, %fd2499, %fd34;
	add.f64 	%fd2501, %fd2500, 0d0000000000000000;
	mul.f64 	%fd2502, %fd5705, %fd2499;
	div.rn.f64 	%fd2503, %fd2502, %fd34;
	sub.f64 	%fd2504, %fd5944, %fd2503;
	add.f64 	%fd2505, %fd5687, %fd5687;
	add.f64 	%fd2506, %fd5686, %fd5686;
	add.f64 	%fd2507, %fd5685, %fd5685;
	fma.rn.f64 	%fd2508, %fd2505, %fd2504, 0d0000000000000000;
	fma.rn.f64 	%fd2509, %fd2506, %fd2504, 0d0000000000000000;
	fma.rn.f64 	%fd2510, %fd2507, %fd2504, 0d0000000000000000;
	fma.rn.f64 	%fd2511, %fd5687, %fd2501, 0d0000000000000000;
	fma.rn.f64 	%fd2512, %fd5686, %fd2501, 0d0000000000000000;
	fma.rn.f64 	%fd2513, %fd5685, %fd2501, 0d0000000000000000;
	fma.rn.f64 	%fd2514, %fd5684, %fd2501, %fd2508;
	fma.rn.f64 	%fd2515, %fd5683, %fd2501, %fd2509;
	fma.rn.f64 	%fd2516, %fd5682, %fd2501, %fd2510;
	add.f64 	%fd5937, %fd5937, %fd2514;
	add.f64 	%fd5936, %fd5936, %fd2515;
	add.f64 	%fd5935, %fd5935, %fd2516;
	sub.f64 	%fd2517, %fd5934, %fd2514;
	sub.f64 	%fd2518, %fd5933, %fd2515;
	sub.f64 	%fd2519, %fd5932, %fd2516;
	add.f64 	%fd5940, %fd5940, %fd2511;
	add.f64 	%fd5939, %fd5939, %fd2512;
	add.f64 	%fd5938, %fd5938, %fd2513;
	sub.f64 	%fd5934, %fd2517, %fd2511;
	sub.f64 	%fd5933, %fd2518, %fd2512;
	sub.f64 	%fd5932, %fd2519, %fd2513;

$L__BB5_120:
	@%p10 bra 	$L__BB5_122;

	add.f64 	%fd2520, %fd5944, 0d0000000000000000;
	mov.f64 	%fd2521, 0d0000000000000000;
	div.rn.f64 	%fd2522, %fd2520, %fd36;
	add.f64 	%fd2523, %fd2522, 0d0000000000000000;
	mul.f64 	%fd2524, %fd5740, %fd2520;
	div.rn.f64 	%fd2525, %fd2524, %fd36;
	sub.f64 	%fd2526, %fd2521, %fd2525;
	add.f64 	%fd2527, %fd5716, %fd5716;
	add.f64 	%fd2528, %fd5715, %fd5715;
	add.f64 	%fd2529, %fd5714, %fd5714;
	fma.rn.f64 	%fd2530, %fd2527, %fd2526, 0d0000000000000000;
	fma.rn.f64 	%fd2531, %fd2528, %fd2526, 0d0000000000000000;
	fma.rn.f64 	%fd2532, %fd2529, %fd2526, 0d0000000000000000;
	fma.rn.f64 	%fd2533, %fd5716, %fd2523, 0d0000000000000000;
	fma.rn.f64 	%fd2534, %fd5715, %fd2523, 0d0000000000000000;
	fma.rn.f64 	%fd2535, %fd5714, %fd2523, 0d0000000000000000;
	fma.rn.f64 	%fd2536, %fd5713, %fd2523, %fd2530;
	fma.rn.f64 	%fd2537, %fd5712, %fd2523, %fd2531;
	fma.rn.f64 	%fd2538, %fd5711, %fd2523, %fd2532;
	add.f64 	%fd5956, %fd5956, %fd2536;
	add.f64 	%fd5955, %fd5955, %fd2537;
	add.f64 	%fd5954, %fd5954, %fd2538;
	sub.f64 	%fd2539, %fd5940, %fd2536;
	sub.f64 	%fd2540, %fd5939, %fd2537;
	sub.f64 	%fd2541, %fd5938, %fd2538;
	add.f64 	%fd5937, %fd5937, %fd2533;
	add.f64 	%fd5936, %fd5936, %fd2534;
	add.f64 	%fd5935, %fd5935, %fd2535;
	sub.f64 	%fd5940, %fd2539, %fd2533;
	sub.f64 	%fd5939, %fd2540, %fd2534;
	sub.f64 	%fd5938, %fd2541, %fd2535;

$L__BB5_122:
	mov.f64 	%fd5957, 0d0000000000000000;
	mov.f64 	%fd2546, 0d7FF8000000000000;
	add.f64 	%fd5960, %fd2546, 0d0000000000000000;
	mov.f64 	%fd5958, 0d0000000000000000;
	mov.f64 	%fd5959, 0d0000000000000000;
	@%p31 bra 	$L__BB5_125;

	mov.f64 	%fd5957, 0d0000000000000000;
	setp.ge.f64 	%p117, %fd40, %fd39;
	mov.pred 	%p323, 0;
	@%p117 bra 	$L__BB5_125;

	mul.f64 	%fd2552, %fd18, %fd30;
	mul.f64 	%fd2553, %fd21, %fd27;
	sub.f64 	%fd5957, %fd2552, %fd2553;
	mul.f64 	%fd2554, %fd15, %fd30;
	mul.f64 	%fd2555, %fd21, %fd24;
	sub.f64 	%fd5958, %fd2555, %fd2554;
	mul.f64 	%fd2556, %fd18, %fd24;
	mul.f64 	%fd2557, %fd15, %fd27;
	sub.f64 	%fd5959, %fd2557, %fd2556;
	mul.f64 	%fd2558, %fd34, 0d3BC79CA100000000;
	fma.rn.f64 	%fd5960, %fd2558, 0d0000000000000000, 0d0000000000000000;
	mov.pred 	%p323, -1;

$L__BB5_125:
	setp.gtu.f64 	%p13, %fd40, 0d0000000000000000;
	and.pred  	%p119, %p323, %p13;
	@%p119 bra 	$L__BB5_127;
	bra.uni 	$L__BB5_126;

$L__BB5_127:
	mul.f64 	%fd5964, %fd36, 0d0000000000000000;
	add.f64 	%fd2573, %fd5964, 0d0000000000000000;
	fma.rn.f64 	%fd2574, %fd2573, 0d3BC79CA100000000, 0d0000000000000000;
	add.f64 	%fd2575, %fd5957, %fd5957;
	add.f64 	%fd2576, %fd5958, %fd5958;
	add.f64 	%fd2577, %fd5959, %fd5959;
	fma.rn.f64 	%fd2578, %fd2575, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2579, %fd2576, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2580, %fd2577, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2581, %fd31, 0d0000000000000000, %fd2578;
	fma.rn.f64 	%fd2582, %fd32, 0d0000000000000000, %fd2579;
	fma.rn.f64 	%fd2583, %fd33, 0d0000000000000000, %fd2580;
	fma.rn.f64 	%fd5971, %fd5957, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd5972, %fd5958, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd5973, %fd5959, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd2584, %fd27, %fd2583;
	mul.f64 	%fd2585, %fd30, %fd2582;
	sub.f64 	%fd2586, %fd2584, %fd2585;
	mul.f64 	%fd2587, %fd30, %fd2581;
	mul.f64 	%fd2588, %fd24, %fd2583;
	sub.f64 	%fd2589, %fd2587, %fd2588;
	mul.f64 	%fd2590, %fd24, %fd2582;
	mul.f64 	%fd2591, %fd27, %fd2581;
	sub.f64 	%fd2592, %fd2590, %fd2591;
	add.f64 	%fd5977, %fd2586, 0d0000000000000000;
	add.f64 	%fd5978, %fd2589, 0d0000000000000000;
	add.f64 	%fd5979, %fd2592, 0d0000000000000000;
	mul.f64 	%fd2593, %fd18, %fd2583;
	mul.f64 	%fd2594, %fd21, %fd2582;
	mul.f64 	%fd2595, %fd21, %fd2581;
	mul.f64 	%fd2596, %fd15, %fd2583;
	mul.f64 	%fd2597, %fd15, %fd2582;
	mul.f64 	%fd2598, %fd18, %fd2581;
	sub.f64 	%fd2599, %fd2594, %fd2593;
	add.f64 	%fd5974, %fd2599, 0d0000000000000000;
	sub.f64 	%fd2600, %fd2596, %fd2595;
	add.f64 	%fd5975, %fd2600, 0d0000000000000000;
	sub.f64 	%fd2601, %fd2598, %fd2597;
	add.f64 	%fd5976, %fd2601, 0d0000000000000000;
	mul.f64 	%fd5965, %fd37, 0d0000000000000000;
	add.f64 	%fd5969, %fd5965, 0d0000000000000000;
	mul.f64 	%fd5962, %fd35, 0d0000000000000000;
	add.f64 	%fd5967, %fd5962, 0d0000000000000000;
	mul.f64 	%fd5963, %fd38, 0d0000000000000000;
	add.f64 	%fd5970, %fd5963, %fd2574;
	mul.f64 	%fd5961, %fd34, 0d0000000000000000;
	add.f64 	%fd5966, %fd5961, 0d0000000000000000;
	bra.uni 	$L__BB5_128;

$L__BB5_126:
	mul.f64 	%fd5965, %fd37, 0d0000000000000000;
	mov.f64 	%fd5966, 0d0000000000000000;
	mul.f64 	%fd5964, %fd36, 0d0000000000000000;
	mul.f64 	%fd5963, %fd38, 0d0000000000000000;
	mul.f64 	%fd5962, %fd35, 0d0000000000000000;
	mul.f64 	%fd5961, %fd34, 0d0000000000000000;
	mov.f64 	%fd5967, %fd5966;
	mov.f64 	%fd5960, %fd5966;
	mov.f64 	%fd5969, %fd5966;
	mov.f64 	%fd5970, %fd5966;
	mov.f64 	%fd5971, %fd5966;
	mov.f64 	%fd5972, %fd5966;
	mov.f64 	%fd5973, %fd5966;
	mov.f64 	%fd5974, %fd5966;
	mov.f64 	%fd5975, %fd5966;
	mov.f64 	%fd5976, %fd5966;
	mov.f64 	%fd5977, %fd5966;
	mov.f64 	%fd5978, %fd5966;
	mov.f64 	%fd5979, %fd5966;

$L__BB5_128:
	add.f64 	%fd2604, %fd5966, 0d0000000000000000;
	mov.f64 	%fd2603, 0d0000000000000000;
	selp.f64 	%fd2605, %fd2604, %fd5966, %p31;
	add.f64 	%fd2606, %fd5960, 0d0000000000000000;
	selp.f64 	%fd2607, %fd2606, %fd5960, %p31;
	add.f64 	%fd2608, %fd5965, %fd2607;
	add.f64 	%fd2609, %fd5962, %fd2605;
	add.f64 	%fd2610, %fd5963, %fd5969;
	add.f64 	%fd2611, %fd5962, %fd2610;
	add.f64 	%fd2612, %fd5962, %fd2611;
	add.f64 	%fd2613, %fd5961, %fd2608;
	fma.rn.f64 	%fd2614, %fd31, %fd2609, %fd5974;
	fma.rn.f64 	%fd2615, %fd32, %fd2609, %fd5975;
	fma.rn.f64 	%fd2616, %fd33, %fd2609, %fd5976;
	fma.rn.f64 	%fd2617, %fd24, %fd2609, %fd5971;
	fma.rn.f64 	%fd2618, %fd27, %fd2609, %fd5972;
	fma.rn.f64 	%fd2619, %fd30, %fd2609, %fd5973;
	add.f64 	%fd2620, %fd5964, %fd5967;
	fma.rn.f64 	%fd2621, %fd31, %fd2620, %fd5977;
	fma.rn.f64 	%fd2622, %fd32, %fd2620, %fd5978;
	fma.rn.f64 	%fd2623, %fd33, %fd2620, %fd5979;
	fma.rn.f64 	%fd2624, %fd15, %fd2620, %fd2617;
	fma.rn.f64 	%fd2625, %fd18, %fd2620, %fd2618;
	fma.rn.f64 	%fd2626, %fd21, %fd2620, %fd2619;
	add.f64 	%fd687, %fd24, %fd24;
	add.f64 	%fd688, %fd27, %fd27;
	add.f64 	%fd689, %fd30, %fd30;
	fma.rn.f64 	%fd2627, %fd687, %fd2613, %fd2614;
	fma.rn.f64 	%fd2628, %fd688, %fd2613, %fd2615;
	fma.rn.f64 	%fd2629, %fd689, %fd2613, %fd2616;
	fma.rn.f64 	%fd2630, %fd24, %fd2612, %fd2621;
	fma.rn.f64 	%fd2631, %fd27, %fd2612, %fd2622;
	fma.rn.f64 	%fd2632, %fd30, %fd2612, %fd2623;
	fma.rn.f64 	%fd2633, %fd15, %fd2612, %fd2627;
	fma.rn.f64 	%fd2634, %fd18, %fd2612, %fd2628;
	fma.rn.f64 	%fd2635, %fd21, %fd2612, %fd2629;
	add.f64 	%fd690, %fd15, %fd15;
	add.f64 	%fd2636, %fd5964, %fd5970;
	add.f64 	%fd691, %fd18, %fd18;
	add.f64 	%fd692, %fd21, %fd21;
	fma.rn.f64 	%fd2637, %fd690, %fd2636, %fd2630;
	fma.rn.f64 	%fd2638, %fd691, %fd2636, %fd2631;
	fma.rn.f64 	%fd2639, %fd692, %fd2636, %fd2632;
	add.f64 	%fd2640, %fd5934, %fd2624;
	add.f64 	%fd2641, %fd5933, %fd2625;
	add.f64 	%fd2642, %fd5932, %fd2626;
	sub.f64 	%fd2643, %fd5940, %fd2624;
	sub.f64 	%fd2644, %fd5939, %fd2625;
	sub.f64 	%fd2645, %fd5938, %fd2626;
	add.f64 	%fd768, %fd5956, %fd2633;
	add.f64 	%fd767, %fd5955, %fd2634;
	add.f64 	%fd766, %fd5954, %fd2635;
	sub.f64 	%fd765, %fd2643, %fd2633;
	sub.f64 	%fd764, %fd2644, %fd2634;
	sub.f64 	%fd763, %fd2645, %fd2635;
	add.f64 	%fd762, %fd5937, %fd2637;
	add.f64 	%fd761, %fd5936, %fd2638;
	add.f64 	%fd760, %fd5935, %fd2639;
	sub.f64 	%fd759, %fd2640, %fd2637;
	sub.f64 	%fd758, %fd2641, %fd2638;
	sub.f64 	%fd757, %fd2642, %fd2639;
	mov.f64 	%fd6000, %fd2603;
	mov.f64 	%fd6001, %fd2603;
	@%p56 bra 	$L__BB5_163;

	and.b16  	%rs292, %rs366, 255;
	setp.eq.s16 	%p122, %rs292, 0;
	mov.f64 	%fd5986, 0d0000000000000000;
	mov.f64 	%fd6000, %fd5986;
	mov.f64 	%fd6001, %fd5986;
	@%p122 bra 	$L__BB5_145;

	setp.eq.s64 	%p123, %rd135, 0;
	@%p123 bra 	$L__BB5_132;

	cvta.to.global.u64 	%rd203, %rd135;
	mul.lo.s64 	%rd204, %rd75, %rd56;
	add.s64 	%rd205, %rd203, %rd204;
	ld.global.f64 	%fd2648, [%rd205];
	add.f64 	%fd5980, %fd2648, 0d0000000000000000;
	bra.uni 	$L__BB5_134;

$L__BB5_132:
	setp.eq.s64 	%p124, %rd104, 0;
	mov.f64 	%fd5980, 0d0000000000000000;
	@%p124 bra 	$L__BB5_134;

	cvta.to.global.u64 	%rd206, %rd104;
	mul.lo.s64 	%rd207, %rd75, %rd57;
	add.s64 	%rd208, %rd206, %rd207;
	ld.global.f64 	%fd2650, [%rd208];
	add.f64 	%fd5980, %fd2650, 0d0000000000000000;

$L__BB5_134:
	mov.f64 	%fd2651, 0d0000000000000000;
	sub.f64 	%fd2652, %fd2651, %fd5980;
	fma.rn.f64 	%fd2653, %fd6411, %fd2652, 0d0000000000000000;
	fma.rn.f64 	%fd2654, %fd6412, %fd2652, 0d0000000000000000;
	rcp.rn.f64 	%fd2655, %fd6411;
	mul.f64 	%fd2656, %fd2655, 0d3FE0000000000000;
	fma.rn.f64 	%fd708, %fd2656, %fd2654, 0d0000000000000000;
	fma.rn.f64 	%fd2657, %fd2653, 0d4000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd709, %fd6413, %fd2657, 0d0000000000000000;
	fma.rn.f64 	%fd710, %fd6414, %fd2657, 0d0000000000000000;
	div.rn.f64 	%fd711, %fd64, %fd65;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r659}, %fd711;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r660, %temp}, %fd711;
	}
	setp.gt.s32 	%p125, %r659, 1048575;
	mov.u32 	%r661, -1023;
	mov.f64 	%fd5981, %fd711;
	@%p125 bra 	$L__BB5_136;

	mul.f64 	%fd5981, %fd711, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r659}, %fd5981;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r660, %temp}, %fd5981;
	}
	mov.u32 	%r661, -1077;

$L__BB5_136:
	add.s32 	%r578, %r659, -1;
	setp.lt.u32 	%p126, %r578, 2146435071;
	@%p126 bra 	$L__BB5_138;
	bra.uni 	$L__BB5_137;

$L__BB5_138:
	shr.u32 	%r580, %r659, 20;
	add.s32 	%r662, %r661, %r580;
	and.b32  	%r581, %r659, -2146435073;
	or.b32  	%r582, %r581, 1072693248;
	mov.b64 	%fd5982, {%r660, %r582};
	setp.lt.s32 	%p128, %r582, 1073127583;
	@%p128 bra 	$L__BB5_140;

	{
	.reg .b32 %temp;
	mov.b64 	{%r583, %temp}, %fd5982;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r584}, %fd5982;
	}
	add.s32 	%r585, %r584, -1048576;
	mov.b64 	%fd5982, {%r583, %r585};
	add.s32 	%r662, %r662, 1;

$L__BB5_140:
	add.f64 	%fd2660, %fd5982, 0d3FF0000000000000;
	mov.f64 	%fd2661, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd2662, %fd2660;
	neg.f64 	%fd2663, %fd2660;
	fma.rn.f64 	%fd2664, %fd2663, %fd2662, %fd2661;
	fma.rn.f64 	%fd2665, %fd2664, %fd2664, %fd2664;
	fma.rn.f64 	%fd2666, %fd2665, %fd2662, %fd2662;
	add.f64 	%fd2667, %fd5982, 0dBFF0000000000000;
	mul.f64 	%fd2668, %fd2667, %fd2666;
	fma.rn.f64 	%fd2669, %fd2667, %fd2666, %fd2668;
	mul.f64 	%fd2670, %fd2669, %fd2669;
	mov.f64 	%fd2671, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd2672, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd2673, %fd2672, %fd2670, %fd2671;
	mov.f64 	%fd2674, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd2675, %fd2673, %fd2670, %fd2674;
	mov.f64 	%fd2676, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd2677, %fd2675, %fd2670, %fd2676;
	mov.f64 	%fd2678, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd2679, %fd2677, %fd2670, %fd2678;
	mov.f64 	%fd2680, 0d3F624924923BE72D;
	fma.rn.f64 	%fd2681, %fd2679, %fd2670, %fd2680;
	mov.f64 	%fd2682, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd2683, %fd2681, %fd2670, %fd2682;
	mov.f64 	%fd2684, 0d3FB5555555555554;
	fma.rn.f64 	%fd2685, %fd2683, %fd2670, %fd2684;
	sub.f64 	%fd2686, %fd2667, %fd2669;
	add.f64 	%fd2687, %fd2686, %fd2686;
	neg.f64 	%fd2688, %fd2669;
	fma.rn.f64 	%fd2689, %fd2688, %fd2667, %fd2687;
	mul.f64 	%fd2690, %fd2666, %fd2689;
	mul.f64 	%fd2691, %fd2670, %fd2685;
	fma.rn.f64 	%fd2692, %fd2691, %fd2669, %fd2690;
	xor.b32  	%r586, %r662, -2147483648;
	mov.u32 	%r587, -2147483648;
	mov.u32 	%r588, 1127219200;
	mov.b64 	%fd2693, {%r586, %r588};
	mov.b64 	%fd2694, {%r587, %r588};
	sub.f64 	%fd2695, %fd2693, %fd2694;
	mov.f64 	%fd2696, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd2697, %fd2695, %fd2696, %fd2669;
	neg.f64 	%fd2698, %fd2695;
	fma.rn.f64 	%fd2699, %fd2698, %fd2696, %fd2697;
	sub.f64 	%fd2700, %fd2699, %fd2669;
	sub.f64 	%fd2701, %fd2692, %fd2700;
	mov.f64 	%fd2702, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd2703, %fd2695, %fd2702, %fd2701;
	add.f64 	%fd5983, %fd2697, %fd2703;
	bra.uni 	$L__BB5_141;

$L__BB5_137:
	mov.f64 	%fd2658, 0d7FF0000000000000;
	fma.rn.f64 	%fd2659, %fd5981, %fd2658, %fd2658;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r579}, %fd5981;
	}
	mov.b32 	%f2, %r579;
	setp.eq.f32 	%p127, %f2, 0f00000000;
	selp.f64 	%fd5983, 0dFFF0000000000000, %fd2659, %p127;

$L__BB5_141:
	sub.f64 	%fd2704, %fd64, %fd65;
	div.rn.f64 	%fd2705, %fd2704, %fd65;
	mul.f64 	%fd2706, %fd2705, %fd5983;
	fma.rn.f64 	%fd2707, %fd2705, %fd5983, %fd2706;
	div.rn.f64 	%fd2708, %fd2707, %fd65;
	mul.f64 	%fd2709, %fd2705, %fd2705;
	div.rn.f64 	%fd2710, %fd2709, %fd64;
	fma.rn.f64 	%fd2711, %fd710, %fd1737, 0d0000000000000000;
	mov.f64 	%fd2712, 0d0000000000000000;
	sub.f64 	%fd2713, %fd2712, %fd2711;
	div.rn.f64 	%fd2714, %fd2713, %fd64;
	add.f64 	%fd2715, %fd2714, 0d0000000000000000;
	mul.f64 	%fd2716, %fd2713, %fd2710;
	div.rn.f64 	%fd2717, %fd2716, %fd64;
	sub.f64 	%fd2718, %fd708, %fd2717;
	fma.rn.f64 	%fd2719, %fd2715, %fd2705, 0d0000000000000000;
	div.rn.f64 	%fd2720, %fd2719, %fd65;
	add.f64 	%fd2721, %fd2720, 0d0000000000000000;
	mul.f64 	%fd2722, %fd2705, %fd2719;
	div.rn.f64 	%fd2723, %fd2722, %fd65;
	sub.f64 	%fd2724, %fd2712, %fd2723;
	add.f64 	%fd2725, %fd2720, %fd2721;
	sub.f64 	%fd2726, %fd2724, %fd2723;
	div.rn.f64 	%fd2727, %fd2711, %fd65;
	add.f64 	%fd2728, %fd2727, 0d0000000000000000;
	mul.f64 	%fd2729, %fd2711, %fd2708;
	div.rn.f64 	%fd2730, %fd2729, %fd65;
	add.f64 	%fd2731, %fd2726, %fd2730;
	add.f64 	%fd2732, %fd2728, %fd2728;
	sub.f64 	%fd2733, %fd2712, %fd2732;
	fma.rn.f64 	%fd2734, %fd2733, %fd5983, 0d0000000000000000;
	fma.rn.f64 	%fd2735, %fd2733, %fd2705, 0d0000000000000000;
	rcp.rn.f64 	%fd2736, %fd711;
	fma.rn.f64 	%fd2737, %fd2736, %fd2735, 0d0000000000000000;
	div.rn.f64 	%fd2738, %fd2737, %fd65;
	add.f64 	%fd2739, %fd2738, %fd2718;
	mul.f64 	%fd2740, %fd711, %fd2737;
	div.rn.f64 	%fd2741, %fd2740, %fd65;
	sub.f64 	%fd2742, %fd2731, %fd2741;
	div.rn.f64 	%fd2743, %fd2734, %fd65;
	add.f64 	%fd2744, %fd2725, %fd2743;
	mul.f64 	%fd2745, %fd2705, %fd2734;
	div.rn.f64 	%fd2746, %fd2745, %fd65;
	sub.f64 	%fd2747, %fd2742, %fd2746;
	add.f64 	%fd6000, %fd2744, %fd2739;
	sub.f64 	%fd6001, %fd2747, %fd2744;
	fma.rn.f64 	%fd2748, %fd709, %fd1736, 0d0000000000000000;
	fma.rn.f64 	%fd722, %fd2748, %fd1738, 0d0000000000000000;
	setp.eq.s64 	%p129, %rd153, 0;
	@%p129 bra 	$L__BB5_143;

	mul.lo.s64 	%rd211, %rd78, %rd63;
	add.s64 	%rd209, %rd153, %rd211;
	// begin inline asm
	{ atom.add.f64 %fd2749,[%rd209],%fd722; }

	// end inline asm
	mul.lo.s64 	%rd212, %rd77, %rd63;
	add.s64 	%rd210, %rd153, %rd212;
	// begin inline asm
	{ atom.add.f64 %fd2751,[%rd210],%fd722; }

	// end inline asm
	bra.uni 	$L__BB5_145;

$L__BB5_143:
	setp.eq.s64 	%p130, %rd130, 0;
	@%p130 bra 	$L__BB5_145;

	mul.lo.s64 	%rd215, %rd78, %rd55;
	add.s64 	%rd213, %rd130, %rd215;
	// begin inline asm
	{ atom.add.f64 %fd2753,[%rd213],%fd722; }

	// end inline asm
	mul.lo.s64 	%rd216, %rd77, %rd55;
	add.s64 	%rd214, %rd130, %rd216;
	// begin inline asm
	{ atom.add.f64 %fd2755,[%rd214],%fd722; }

	// end inline asm

$L__BB5_145:
	mul.f64 	%fd2759, %fd21, %fd27;
	mul.f64 	%fd2760, %fd18, %fd30;
	sub.f64 	%fd725, %fd2760, %fd2759;
	mul.f64 	%fd2761, %fd15, %fd30;
	mul.f64 	%fd2762, %fd21, %fd24;
	sub.f64 	%fd726, %fd2762, %fd2761;
	mul.f64 	%fd2763, %fd18, %fd24;
	mul.f64 	%fd2764, %fd15, %fd27;
	sub.f64 	%fd727, %fd2764, %fd2763;
	mul.f64 	%fd2765, %fd726, %fd726;
	fma.rn.f64 	%fd2766, %fd725, %fd725, %fd2765;
	fma.rn.f64 	%fd728, %fd727, %fd727, %fd2766;
	setp.geu.f64 	%p131, %fd728, %fd6410;
	mov.f64 	%fd5987, %fd5986;
	@%p131 bra 	$L__BB5_147;

	div.rn.f64 	%fd2767, %fd728, %fd6410;
	mov.f64 	%fd2768, 0d0000000000000000;
	sub.f64 	%fd2769, %fd2768, %fd2767;
	add.f64 	%fd2770, %fd2769, 0d4000000000000000;
	fma.rn.f64 	%fd2771, %fd2767, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd2772, %fd2770, 0d0000000000000000, 0d0000000000000000;
	sub.f64 	%fd2773, %fd2772, %fd2771;
	div.rn.f64 	%fd2774, %fd2773, %fd6410;
	add.f64 	%fd5987, %fd2774, 0d0000000000000000;
	mul.f64 	%fd2775, %fd2767, %fd2773;
	div.rn.f64 	%fd2776, %fd2775, %fd6410;
	sub.f64 	%fd5986, %fd2768, %fd2776;

$L__BB5_147:
	add.f64 	%fd2777, %fd725, %fd725;
	add.f64 	%fd2778, %fd726, %fd726;
	add.f64 	%fd2779, %fd727, %fd727;
	fma.rn.f64 	%fd2780, %fd2777, %fd5987, 0d0000000000000000;
	fma.rn.f64 	%fd2781, %fd2778, %fd5987, 0d0000000000000000;
	fma.rn.f64 	%fd2782, %fd2779, %fd5987, 0d0000000000000000;
	mul.f64 	%fd2783, %fd27, %fd2782;
	mul.f64 	%fd2784, %fd30, %fd2781;
	sub.f64 	%fd2785, %fd2783, %fd2784;
	mul.f64 	%fd2786, %fd30, %fd2780;
	mul.f64 	%fd2787, %fd24, %fd2782;
	sub.f64 	%fd2788, %fd2786, %fd2787;
	mul.f64 	%fd2789, %fd24, %fd2781;
	mul.f64 	%fd2790, %fd27, %fd2780;
	sub.f64 	%fd2791, %fd2789, %fd2790;
	add.f64 	%fd2792, %fd2785, 0d0000000000000000;
	add.f64 	%fd2793, %fd2788, 0d0000000000000000;
	add.f64 	%fd2794, %fd2791, 0d0000000000000000;
	mul.f64 	%fd2795, %fd18, %fd2782;
	mul.f64 	%fd2796, %fd21, %fd2781;
	mul.f64 	%fd2797, %fd21, %fd2780;
	mul.f64 	%fd2798, %fd15, %fd2782;
	mul.f64 	%fd2799, %fd15, %fd2781;
	mul.f64 	%fd2800, %fd18, %fd2780;
	sub.f64 	%fd2801, %fd2796, %fd2795;
	add.f64 	%fd2802, %fd2801, 0d0000000000000000;
	sub.f64 	%fd2803, %fd2798, %fd2797;
	add.f64 	%fd2804, %fd2803, 0d0000000000000000;
	sub.f64 	%fd2805, %fd2800, %fd2799;
	add.f64 	%fd2806, %fd2805, 0d0000000000000000;
	add.f64 	%fd768, %fd768, %fd2802;
	add.f64 	%fd767, %fd767, %fd2804;
	add.f64 	%fd766, %fd766, %fd2806;
	sub.f64 	%fd765, %fd765, %fd2802;
	sub.f64 	%fd764, %fd764, %fd2804;
	sub.f64 	%fd763, %fd763, %fd2806;
	add.f64 	%fd762, %fd762, %fd2792;
	add.f64 	%fd761, %fd761, %fd2793;
	add.f64 	%fd760, %fd760, %fd2794;
	sub.f64 	%fd759, %fd759, %fd2792;
	sub.f64 	%fd758, %fd758, %fd2793;
	sub.f64 	%fd757, %fd757, %fd2794;
	sub.f64 	%fd2807, %fd5590, %fd5587;
	mul.f64 	%fd2808, %fd2807, %fd2807;
	sub.f64 	%fd2809, %fd5589, %fd5586;
	fma.rn.f64 	%fd2810, %fd2809, %fd2809, %fd2808;
	sub.f64 	%fd2811, %fd5588, %fd5585;
	fma.rn.f64 	%fd2812, %fd2811, %fd2811, %fd2810;
	mul.f64 	%fd2813, %fd2812, 0d3F50624DE0000000;
	sub.f64 	%fd2814, %fd5596, %fd5593;
	mul.f64 	%fd2815, %fd2814, %fd2814;
	sub.f64 	%fd2816, %fd5595, %fd5592;
	fma.rn.f64 	%fd2817, %fd2816, %fd2816, %fd2815;
	sub.f64 	%fd2818, %fd5594, %fd5591;
	fma.rn.f64 	%fd2819, %fd2818, %fd2818, %fd2817;
	add.f64 	%fd2820, %fd5986, 0d0000000000000000;
	fma.rn.f64 	%fd2821, %fd2819, %fd2820, 0d0000000000000000;
	fma.rn.f64 	%fd2822, %fd2813, %fd2820, 0d0000000000000000;
	add.f64 	%fd2823, %fd2814, %fd2814;
	add.f64 	%fd2824, %fd2816, %fd2816;
	add.f64 	%fd2825, %fd2818, %fd2818;
	fma.rn.f64 	%fd745, %fd2823, %fd2822, 0d0000000000000000;
	fma.rn.f64 	%fd746, %fd2824, %fd2822, 0d0000000000000000;
	fma.rn.f64 	%fd747, %fd2825, %fd2822, 0d0000000000000000;
	fma.rn.f64 	%fd2826, %fd2821, 0d3F50624DE0000000, 0d0000000000000000;
	add.f64 	%fd2827, %fd2807, %fd2807;
	add.f64 	%fd2828, %fd2809, %fd2809;
	add.f64 	%fd2829, %fd2811, %fd2811;
	fma.rn.f64 	%fd748, %fd2827, %fd2826, 0d0000000000000000;
	fma.rn.f64 	%fd749, %fd2828, %fd2826, 0d0000000000000000;
	fma.rn.f64 	%fd750, %fd2829, %fd2826, 0d0000000000000000;
	setp.eq.s64 	%p132, %rd143, 0;
	@%p132 bra 	$L__BB5_149;

	cvt.s64.s32 	%rd220, %r676;
	mul.lo.s64 	%rd221, %rd220, %rd62;
	add.s64 	%rd217, %rd143, %rd221;
	// begin inline asm
	{ atom.add.f64 %fd2830,[%rd217],%fd745; }

	// end inline asm
	add.s64 	%rd218, %rd217, 8;
	// begin inline asm
	{ atom.add.f64 %fd2832,[%rd218],%fd746; }

	// end inline asm
	add.s64 	%rd219, %rd217, 16;
	// begin inline asm
	{ atom.add.f64 %fd2834,[%rd219],%fd747; }

	// end inline asm
	bra.uni 	$L__BB5_151;

$L__BB5_149:
	setp.eq.s64 	%p133, %rd120, 0;
	@%p133 bra 	$L__BB5_151;

	cvt.s64.s32 	%rd225, %r676;
	mul.lo.s64 	%rd226, %rd225, %rd54;
	add.s64 	%rd222, %rd120, %rd226;
	// begin inline asm
	{ atom.add.f64 %fd2836,[%rd222],%fd745; }

	// end inline asm
	add.s64 	%rd223, %rd222, 8;
	// begin inline asm
	{ atom.add.f64 %fd2838,[%rd223],%fd746; }

	// end inline asm
	add.s64 	%rd224, %rd222, 16;
	// begin inline asm
	{ atom.add.f64 %fd2840,[%rd224],%fd747; }

	// end inline asm

$L__BB5_151:
	mov.f64 	%fd2842, 0d0000000000000000;
	sub.f64 	%fd2843, %fd2842, %fd745;
	add.f64 	%fd751, %fd2843, 0d0000000000000000;
	sub.f64 	%fd2844, %fd2842, %fd746;
	add.f64 	%fd752, %fd2844, 0d0000000000000000;
	sub.f64 	%fd2845, %fd2842, %fd747;
	add.f64 	%fd753, %fd2845, 0d0000000000000000;
	@%p132 bra 	$L__BB5_153;

	cvt.s64.s32 	%rd230, %r677;
	mul.lo.s64 	%rd231, %rd230, %rd62;
	add.s64 	%rd227, %rd143, %rd231;
	// begin inline asm
	{ atom.add.f64 %fd2846,[%rd227],%fd751; }

	// end inline asm
	add.s64 	%rd228, %rd227, 8;
	// begin inline asm
	{ atom.add.f64 %fd2848,[%rd228],%fd752; }

	// end inline asm
	add.s64 	%rd229, %rd227, 16;
	// begin inline asm
	{ atom.add.f64 %fd2850,[%rd229],%fd753; }

	// end inline asm
	bra.uni 	$L__BB5_155;

$L__BB5_153:
	setp.eq.s64 	%p135, %rd120, 0;
	@%p135 bra 	$L__BB5_159;

	cvt.s64.s32 	%rd235, %r677;
	mul.lo.s64 	%rd236, %rd235, %rd54;
	add.s64 	%rd232, %rd120, %rd236;
	// begin inline asm
	{ atom.add.f64 %fd2852,[%rd232],%fd751; }

	// end inline asm
	add.s64 	%rd233, %rd232, 8;
	// begin inline asm
	{ atom.add.f64 %fd2854,[%rd233],%fd752; }

	// end inline asm
	add.s64 	%rd234, %rd232, 16;
	// begin inline asm
	{ atom.add.f64 %fd2856,[%rd234],%fd753; }

	// end inline asm

$L__BB5_155:
	@%p132 bra 	$L__BB5_157;

	cvt.s64.s32 	%rd240, %r678;
	mul.lo.s64 	%rd241, %rd240, %rd62;
	add.s64 	%rd237, %rd143, %rd241;
	// begin inline asm
	{ atom.add.f64 %fd2858,[%rd237],%fd748; }

	// end inline asm
	add.s64 	%rd238, %rd237, 8;
	// begin inline asm
	{ atom.add.f64 %fd2860,[%rd238],%fd749; }

	// end inline asm
	add.s64 	%rd239, %rd237, 16;
	// begin inline asm
	{ atom.add.f64 %fd2862,[%rd239],%fd750; }

	// end inline asm
	bra.uni 	$L__BB5_159;

$L__BB5_157:
	setp.eq.s64 	%p137, %rd120, 0;
	@%p137 bra 	$L__BB5_159;

	cvt.s64.s32 	%rd245, %r678;
	mul.lo.s64 	%rd246, %rd245, %rd54;
	add.s64 	%rd242, %rd120, %rd246;
	// begin inline asm
	{ atom.add.f64 %fd2864,[%rd242],%fd748; }

	// end inline asm
	add.s64 	%rd243, %rd242, 8;
	// begin inline asm
	{ atom.add.f64 %fd2866,[%rd243],%fd749; }

	// end inline asm
	add.s64 	%rd244, %rd242, 16;
	// begin inline asm
	{ atom.add.f64 %fd2868,[%rd244],%fd750; }

	// end inline asm

$L__BB5_159:
	mov.f64 	%fd2870, 0d0000000000000000;
	sub.f64 	%fd2871, %fd2870, %fd748;
	add.f64 	%fd754, %fd2871, 0d0000000000000000;
	sub.f64 	%fd2872, %fd2870, %fd749;
	add.f64 	%fd755, %fd2872, 0d0000000000000000;
	sub.f64 	%fd2873, %fd2870, %fd750;
	add.f64 	%fd756, %fd2873, 0d0000000000000000;
	@%p132 bra 	$L__BB5_161;

	cvt.s64.s32 	%rd250, %r679;
	mul.lo.s64 	%rd251, %rd250, %rd62;
	add.s64 	%rd247, %rd143, %rd251;
	// begin inline asm
	{ atom.add.f64 %fd2874,[%rd247],%fd754; }

	// end inline asm
	add.s64 	%rd248, %rd247, 8;
	// begin inline asm
	{ atom.add.f64 %fd2876,[%rd248],%fd755; }

	// end inline asm
	add.s64 	%rd249, %rd247, 16;
	// begin inline asm
	{ atom.add.f64 %fd2878,[%rd249],%fd756; }

	// end inline asm
	bra.uni 	$L__BB5_163;

$L__BB5_161:
	setp.eq.s64 	%p139, %rd120, 0;
	@%p139 bra 	$L__BB5_163;

	cvt.s64.s32 	%rd255, %r679;
	mul.lo.s64 	%rd256, %rd255, %rd54;
	add.s64 	%rd252, %rd120, %rd256;
	// begin inline asm
	{ atom.add.f64 %fd2880,[%rd252],%fd754; }

	// end inline asm
	add.s64 	%rd253, %rd252, 8;
	// begin inline asm
	{ atom.add.f64 %fd2882,[%rd253],%fd755; }

	// end inline asm
	add.s64 	%rd254, %rd252, 16;
	// begin inline asm
	{ atom.add.f64 %fd2884,[%rd254],%fd756; }

	// end inline asm

$L__BB5_163:
	add.f64 	%fd2887, %fd6001, 0d0000000000000000;
	fma.rn.f64 	%fd2889, %fd2, %fd2887, 0d0000000000000000;
	add.f64 	%fd771, %fd6000, 0d0000000000000000;
	sub.f64 	%fd2890, %fd2603, %fd6000;
	fma.rn.f64 	%fd2891, %fd12, %fd2890, %fd2889;
	fma.rn.f64 	%fd772, %fd12, %fd2890, %fd2891;
	@%p31 bra 	$L__BB5_167;

	setp.ge.f64 	%p14, %fd40, %fd39;
	add.f64 	%fd773, %fd38, %fd35;
	@%p14 bra 	$L__BB5_166;

	selp.f64 	%fd2893, %fd36, %fd39, %p14;
	mul.f64 	%fd2894, %fd34, %fd38;
	mul.f64 	%fd2895, %fd37, %fd35;
	sub.f64 	%fd2896, %fd2894, %fd2895;
	mul.f64 	%fd2897, %fd21, %fd27;
	mul.f64 	%fd2898, %fd18, %fd30;
	sub.f64 	%fd2899, %fd2898, %fd2897;
	mul.f64 	%fd2900, %fd15, %fd30;
	mul.f64 	%fd2901, %fd21, %fd24;
	sub.f64 	%fd2902, %fd2901, %fd2900;
	mul.f64 	%fd2903, %fd18, %fd24;
	mul.f64 	%fd2904, %fd15, %fd27;
	sub.f64 	%fd2905, %fd2904, %fd2903;
	setp.gt.f64 	%p141, %fd2896, 0d0000000000000000;
	setp.lt.f64 	%p142, %fd2896, %fd2893;
	mul.f64 	%fd2906, %fd32, %fd2902;
	fma.rn.f64 	%fd2907, %fd31, %fd2899, %fd2906;
	fma.rn.f64 	%fd2908, %fd33, %fd2905, %fd2907;
	setp.eq.f64 	%p143, %fd2908, 0d0000000000000000;
	mul.f64 	%fd2909, %fd2902, %fd2902;
	fma.rn.f64 	%fd2910, %fd2899, %fd2899, %fd2909;
	fma.rn.f64 	%fd2911, %fd2905, %fd2905, %fd2910;
	mul.f64 	%fd2912, %fd34, 0d3BC79CA100000000;
	mul.f64 	%fd2913, %fd2912, %fd36;
	setp.lt.f64 	%p144, %fd2911, %fd2913;
	or.pred  	%p145, %p143, %p144;
	and.pred  	%p146, %p141, %p142;
	and.pred  	%p147, %p145, %p146;
	mul.f64 	%fd2914, %fd39, 0d3FE0000000000000;
	setp.lt.f64 	%p148, %fd40, %fd2914;
	selp.b32 	%r591, 2, 5, %p148;
	selp.f64 	%fd2915, %fd38, %fd773, %p148;
	selp.f64 	%fd6003, %fd36, %fd2893, %p147;
	selp.b32 	%r663, %r591, 8, %p147;
	selp.f64 	%fd6002, %fd2915, %fd2896, %p147;

$L__BB5_166:
	selp.f64 	%fd6005, %fd36, %fd6003, %p14;
	selp.b32 	%r664, 5, %r663, %p14;
	selp.f64 	%fd6004, %fd773, %fd6002, %p14;

$L__BB5_167:
	selp.f64 	%fd782, %fd36, %fd6005, %p31;
	selp.b32 	%r665, 2, %r664, %p31;
	selp.f64 	%fd783, %fd38, %fd6004, %p31;
	setp.gtu.f64 	%p151, %fd783, 0d0000000000000000;
	@%p151 bra 	$L__BB5_171;
	bra.uni 	$L__BB5_168;

$L__BB5_171:
	setp.ltu.f64 	%p154, %fd783, %fd782;
	@%p154 bra 	$L__BB5_175;

	mov.f64 	%fd2917, 0d0000000000000000;
	sub.f64 	%fd2918, %fd2917, %fd37;
	add.f64 	%fd785, %fd2918, %fd35;
	setp.le.f64 	%p155, %fd785, 0d0000000000000000;
	mov.u32 	%r665, 1;
	@%p155 bra 	$L__BB5_175;

	setp.ge.f64 	%p156, %fd785, %fd34;
	mov.u32 	%r665, 4;
	@%p156 bra 	$L__BB5_175;

	mov.u32 	%r665, 7;
	bra.uni 	$L__BB5_175;

$L__BB5_168:
	mov.f64 	%fd2916, 0d0000000000000000;
	sub.f64 	%fd784, %fd2916, %fd37;
	setp.le.f64 	%p152, %fd784, 0d0000000000000000;
	mov.u32 	%r665, 0;
	@%p152 bra 	$L__BB5_175;

	setp.ge.f64 	%p153, %fd784, %fd34;
	mov.u32 	%r665, 3;
	@%p153 bra 	$L__BB5_175;

	mov.u32 	%r665, 6;

$L__BB5_175:
	setp.eq.s32 	%p157, %r665, 0;
	@%p157 bra 	$L__BB5_191;

	setp.eq.s32 	%p158, %r665, 1;
	@%p158 bra 	$L__BB5_190;
	bra.uni 	$L__BB5_177;

$L__BB5_190:
	sub.f64 	%fd3272, %fd14, %fd22;
	add.f64 	%fd3273, %fd3272, %fd3272;
	sub.f64 	%fd3274, %fd17, %fd25;
	add.f64 	%fd3275, %fd3274, %fd3274;
	sub.f64 	%fd3276, %fd20, %fd28;
	add.f64 	%fd3277, %fd3276, %fd3276;
	fma.rn.f64 	%fd3278, %fd3273, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3279, %fd3275, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3280, %fd3277, %fd771, 0d0000000000000000;
	add.f64 	%fd759, %fd759, %fd3278;
	add.f64 	%fd758, %fd758, %fd3279;
	add.f64 	%fd757, %fd757, %fd3280;
	sub.f64 	%fd768, %fd768, %fd3278;
	sub.f64 	%fd767, %fd767, %fd3279;
	sub.f64 	%fd766, %fd766, %fd3280;
	bra.uni 	$L__BB5_192;

$L__BB5_191:
	add.f64 	%fd3281, %fd31, %fd31;
	add.f64 	%fd3282, %fd32, %fd32;
	add.f64 	%fd3283, %fd33, %fd33;
	fma.rn.f64 	%fd3284, %fd3281, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3285, %fd3282, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3286, %fd3283, %fd771, 0d0000000000000000;
	add.f64 	%fd759, %fd759, %fd3284;
	add.f64 	%fd758, %fd758, %fd3285;
	add.f64 	%fd757, %fd757, %fd3286;
	sub.f64 	%fd765, %fd765, %fd3284;
	sub.f64 	%fd764, %fd764, %fd3285;
	sub.f64 	%fd763, %fd763, %fd3286;
	bra.uni 	$L__BB5_192;

$L__BB5_177:
	setp.eq.s32 	%p159, %r665, 2;
	@%p159 bra 	$L__BB5_189;
	bra.uni 	$L__BB5_178;

$L__BB5_189:
	sub.f64 	%fd3205, %fd23, %fd14;
	sub.f64 	%fd3206, %fd28, %fd20;
	sub.f64 	%fd3207, %fd26, %fd17;
	mul.f64 	%fd3208, %fd3207, %fd3206;
	sub.f64 	%fd3209, %fd25, %fd17;
	sub.f64 	%fd3210, %fd29, %fd20;
	mul.f64 	%fd3211, %fd3210, %fd3209;
	sub.f64 	%fd3212, %fd3208, %fd3211;
	sub.f64 	%fd3213, %fd22, %fd14;
	mul.f64 	%fd3214, %fd3210, %fd3213;
	mul.f64 	%fd3215, %fd3205, %fd3206;
	sub.f64 	%fd3216, %fd3214, %fd3215;
	mul.f64 	%fd3217, %fd3205, %fd3209;
	mul.f64 	%fd3218, %fd3207, %fd3213;
	sub.f64 	%fd3219, %fd3217, %fd3218;
	mul.f64 	%fd3220, %fd3216, %fd3216;
	fma.rn.f64 	%fd3221, %fd3212, %fd3212, %fd3220;
	fma.rn.f64 	%fd3222, %fd3219, %fd3219, %fd3221;
	div.rn.f64 	%fd3223, %fd3222, %fd36;
	div.rn.f64 	%fd3224, %fd771, %fd36;
	add.f64 	%fd3225, %fd3224, 0d0000000000000000;
	mov.f64 	%fd3226, 0d0000000000000000;
	mul.f64 	%fd3227, %fd3223, %fd771;
	div.rn.f64 	%fd3228, %fd3227, %fd36;
	sub.f64 	%fd3229, %fd3226, %fd3228;
	fma.rn.f64 	%fd3230, %fd687, %fd3229, 0d0000000000000000;
	fma.rn.f64 	%fd3231, %fd688, %fd3229, 0d0000000000000000;
	fma.rn.f64 	%fd3232, %fd689, %fd3229, 0d0000000000000000;
	add.f64 	%fd3233, %fd3212, %fd3212;
	add.f64 	%fd3234, %fd3216, %fd3216;
	add.f64 	%fd3235, %fd3219, %fd3219;
	fma.rn.f64 	%fd3236, %fd3233, %fd3225, 0d0000000000000000;
	fma.rn.f64 	%fd3237, %fd3234, %fd3225, 0d0000000000000000;
	fma.rn.f64 	%fd3238, %fd3235, %fd3225, 0d0000000000000000;
	mul.f64 	%fd3239, %fd3209, %fd3238;
	mul.f64 	%fd3240, %fd3206, %fd3237;
	sub.f64 	%fd3241, %fd3239, %fd3240;
	mul.f64 	%fd3242, %fd3206, %fd3236;
	mul.f64 	%fd3243, %fd3213, %fd3238;
	sub.f64 	%fd3244, %fd3242, %fd3243;
	mul.f64 	%fd3245, %fd3213, %fd3237;
	mul.f64 	%fd3246, %fd3209, %fd3236;
	sub.f64 	%fd3247, %fd3245, %fd3246;
	add.f64 	%fd3248, %fd3241, 0d0000000000000000;
	add.f64 	%fd3249, %fd3244, 0d0000000000000000;
	add.f64 	%fd3250, %fd3247, 0d0000000000000000;
	mul.f64 	%fd3251, %fd3207, %fd3238;
	mul.f64 	%fd3252, %fd3210, %fd3237;
	mul.f64 	%fd3253, %fd3210, %fd3236;
	mul.f64 	%fd3254, %fd3205, %fd3238;
	mul.f64 	%fd3255, %fd3205, %fd3237;
	mul.f64 	%fd3256, %fd3207, %fd3236;
	sub.f64 	%fd3257, %fd3252, %fd3251;
	add.f64 	%fd3258, %fd3257, 0d0000000000000000;
	sub.f64 	%fd3259, %fd3254, %fd3253;
	add.f64 	%fd3260, %fd3259, 0d0000000000000000;
	sub.f64 	%fd3261, %fd3256, %fd3255;
	add.f64 	%fd3262, %fd3261, 0d0000000000000000;
	add.f64 	%fd3263, %fd768, %fd3230;
	add.f64 	%fd3264, %fd767, %fd3231;
	add.f64 	%fd3265, %fd766, %fd3232;
	sub.f64 	%fd3266, %fd765, %fd3230;
	sub.f64 	%fd3267, %fd764, %fd3231;
	sub.f64 	%fd3268, %fd763, %fd3232;
	add.f64 	%fd768, %fd3263, %fd3258;
	add.f64 	%fd767, %fd3264, %fd3260;
	add.f64 	%fd766, %fd3265, %fd3262;
	sub.f64 	%fd3269, %fd759, %fd3258;
	sub.f64 	%fd3270, %fd758, %fd3260;
	sub.f64 	%fd3271, %fd757, %fd3262;
	add.f64 	%fd765, %fd3266, %fd3248;
	add.f64 	%fd764, %fd3267, %fd3249;
	add.f64 	%fd763, %fd3268, %fd3250;
	sub.f64 	%fd759, %fd3269, %fd3248;
	sub.f64 	%fd758, %fd3270, %fd3249;
	sub.f64 	%fd757, %fd3271, %fd3250;
	bra.uni 	$L__BB5_192;

$L__BB5_178:
	setp.eq.s32 	%p160, %r665, 3;
	@%p160 bra 	$L__BB5_188;
	bra.uni 	$L__BB5_179;

$L__BB5_188:
	sub.f64 	%fd3196, %fd13, %fd23;
	add.f64 	%fd3197, %fd3196, %fd3196;
	sub.f64 	%fd3198, %fd16, %fd26;
	add.f64 	%fd3199, %fd3198, %fd3198;
	sub.f64 	%fd3200, %fd19, %fd29;
	add.f64 	%fd3201, %fd3200, %fd3200;
	fma.rn.f64 	%fd3202, %fd3197, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3203, %fd3199, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3204, %fd3201, %fd771, 0d0000000000000000;
	add.f64 	%fd762, %fd762, %fd3202;
	add.f64 	%fd761, %fd761, %fd3203;
	add.f64 	%fd760, %fd760, %fd3204;
	sub.f64 	%fd765, %fd765, %fd3202;
	sub.f64 	%fd764, %fd764, %fd3203;
	sub.f64 	%fd763, %fd763, %fd3204;
	bra.uni 	$L__BB5_192;

$L__BB5_179:
	setp.eq.s32 	%p161, %r665, 4;
	@%p161 bra 	$L__BB5_187;
	bra.uni 	$L__BB5_180;

$L__BB5_187:
	sub.f64 	%fd3187, %fd13, %fd22;
	add.f64 	%fd3188, %fd3187, %fd3187;
	sub.f64 	%fd3189, %fd16, %fd25;
	add.f64 	%fd3190, %fd3189, %fd3189;
	sub.f64 	%fd3191, %fd19, %fd28;
	add.f64 	%fd3192, %fd3191, %fd3191;
	fma.rn.f64 	%fd3193, %fd3188, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3194, %fd3190, %fd771, 0d0000000000000000;
	fma.rn.f64 	%fd3195, %fd3192, %fd771, 0d0000000000000000;
	add.f64 	%fd762, %fd762, %fd3193;
	add.f64 	%fd761, %fd761, %fd3194;
	add.f64 	%fd760, %fd760, %fd3195;
	sub.f64 	%fd768, %fd768, %fd3193;
	sub.f64 	%fd767, %fd767, %fd3194;
	sub.f64 	%fd766, %fd766, %fd3195;
	bra.uni 	$L__BB5_192;

$L__BB5_180:
	setp.eq.s32 	%p162, %r665, 5;
	@%p162 bra 	$L__BB5_186;
	bra.uni 	$L__BB5_181;

$L__BB5_186:
	sub.f64 	%fd3120, %fd23, %fd13;
	sub.f64 	%fd3121, %fd28, %fd19;
	sub.f64 	%fd3122, %fd26, %fd16;
	mul.f64 	%fd3123, %fd3122, %fd3121;
	sub.f64 	%fd3124, %fd25, %fd16;
	sub.f64 	%fd3125, %fd29, %fd19;
	mul.f64 	%fd3126, %fd3125, %fd3124;
	sub.f64 	%fd3127, %fd3123, %fd3126;
	sub.f64 	%fd3128, %fd22, %fd13;
	mul.f64 	%fd3129, %fd3125, %fd3128;
	mul.f64 	%fd3130, %fd3120, %fd3121;
	sub.f64 	%fd3131, %fd3129, %fd3130;
	mul.f64 	%fd3132, %fd3120, %fd3124;
	mul.f64 	%fd3133, %fd3122, %fd3128;
	sub.f64 	%fd3134, %fd3132, %fd3133;
	mul.f64 	%fd3135, %fd3131, %fd3131;
	fma.rn.f64 	%fd3136, %fd3127, %fd3127, %fd3135;
	fma.rn.f64 	%fd3137, %fd3134, %fd3134, %fd3136;
	div.rn.f64 	%fd3138, %fd3137, %fd36;
	div.rn.f64 	%fd3139, %fd771, %fd36;
	add.f64 	%fd3140, %fd3139, 0d0000000000000000;
	mov.f64 	%fd3141, 0d0000000000000000;
	mul.f64 	%fd3142, %fd3138, %fd771;
	div.rn.f64 	%fd3143, %fd3142, %fd36;
	sub.f64 	%fd3144, %fd3141, %fd3143;
	fma.rn.f64 	%fd3145, %fd687, %fd3144, 0d0000000000000000;
	fma.rn.f64 	%fd3146, %fd688, %fd3144, 0d0000000000000000;
	fma.rn.f64 	%fd3147, %fd689, %fd3144, 0d0000000000000000;
	add.f64 	%fd3148, %fd3127, %fd3127;
	add.f64 	%fd3149, %fd3131, %fd3131;
	add.f64 	%fd3150, %fd3134, %fd3134;
	fma.rn.f64 	%fd3151, %fd3148, %fd3140, 0d0000000000000000;
	fma.rn.f64 	%fd3152, %fd3149, %fd3140, 0d0000000000000000;
	fma.rn.f64 	%fd3153, %fd3150, %fd3140, 0d0000000000000000;
	mul.f64 	%fd3154, %fd3124, %fd3153;
	mul.f64 	%fd3155, %fd3121, %fd3152;
	sub.f64 	%fd3156, %fd3154, %fd3155;
	mul.f64 	%fd3157, %fd3121, %fd3151;
	mul.f64 	%fd3158, %fd3128, %fd3153;
	sub.f64 	%fd3159, %fd3157, %fd3158;
	mul.f64 	%fd3160, %fd3128, %fd3152;
	mul.f64 	%fd3161, %fd3124, %fd3151;
	sub.f64 	%fd3162, %fd3160, %fd3161;
	add.f64 	%fd3163, %fd3156, 0d0000000000000000;
	add.f64 	%fd3164, %fd3159, 0d0000000000000000;
	add.f64 	%fd3165, %fd3162, 0d0000000000000000;
	mul.f64 	%fd3166, %fd3122, %fd3153;
	mul.f64 	%fd3167, %fd3125, %fd3152;
	mul.f64 	%fd3168, %fd3125, %fd3151;
	mul.f64 	%fd3169, %fd3120, %fd3153;
	mul.f64 	%fd3170, %fd3120, %fd3152;
	mul.f64 	%fd3171, %fd3122, %fd3151;
	sub.f64 	%fd3172, %fd3167, %fd3166;
	add.f64 	%fd3173, %fd3172, 0d0000000000000000;
	sub.f64 	%fd3174, %fd3169, %fd3168;
	add.f64 	%fd3175, %fd3174, 0d0000000000000000;
	sub.f64 	%fd3176, %fd3171, %fd3170;
	add.f64 	%fd3177, %fd3176, 0d0000000000000000;
	add.f64 	%fd3178, %fd768, %fd3145;
	add.f64 	%fd3179, %fd767, %fd3146;
	add.f64 	%fd3180, %fd766, %fd3147;
	sub.f64 	%fd3181, %fd765, %fd3145;
	sub.f64 	%fd3182, %fd764, %fd3146;
	sub.f64 	%fd3183, %fd763, %fd3147;
	add.f64 	%fd768, %fd3178, %fd3173;
	add.f64 	%fd767, %fd3179, %fd3175;
	add.f64 	%fd766, %fd3180, %fd3177;
	sub.f64 	%fd3184, %fd762, %fd3173;
	sub.f64 	%fd3185, %fd761, %fd3175;
	sub.f64 	%fd3186, %fd760, %fd3177;
	add.f64 	%fd765, %fd3181, %fd3163;
	add.f64 	%fd764, %fd3182, %fd3164;
	add.f64 	%fd763, %fd3183, %fd3165;
	sub.f64 	%fd762, %fd3184, %fd3163;
	sub.f64 	%fd761, %fd3185, %fd3164;
	sub.f64 	%fd760, %fd3186, %fd3165;
	bra.uni 	$L__BB5_192;

$L__BB5_181:
	setp.eq.s32 	%p163, %r665, 6;
	@%p163 bra 	$L__BB5_185;
	bra.uni 	$L__BB5_182;

$L__BB5_185:
	sub.f64 	%fd3056, %fd13, %fd23;
	sub.f64 	%fd3057, %fd19, %fd29;
	mul.f64 	%fd3058, %fd32, %fd3057;
	sub.f64 	%fd3059, %fd16, %fd26;
	mul.f64 	%fd3060, %fd3059, %fd33;
	sub.f64 	%fd3061, %fd3058, %fd3060;
	mul.f64 	%fd3062, %fd3056, %fd33;
	mul.f64 	%fd3063, %fd31, %fd3057;
	sub.f64 	%fd3064, %fd3062, %fd3063;
	mul.f64 	%fd3065, %fd31, %fd3059;
	mul.f64 	%fd3066, %fd3056, %fd32;
	sub.f64 	%fd3067, %fd3065, %fd3066;
	mul.f64 	%fd3068, %fd3064, %fd3064;
	fma.rn.f64 	%fd3069, %fd3061, %fd3061, %fd3068;
	fma.rn.f64 	%fd3070, %fd3067, %fd3067, %fd3069;
	div.rn.f64 	%fd3071, %fd3070, %fd34;
	div.rn.f64 	%fd3072, %fd771, %fd34;
	add.f64 	%fd3073, %fd3072, 0d0000000000000000;
	mov.f64 	%fd3074, 0d0000000000000000;
	mul.f64 	%fd3075, %fd3071, %fd771;
	div.rn.f64 	%fd3076, %fd3075, %fd34;
	sub.f64 	%fd3077, %fd3074, %fd3076;
	fma.rn.f64 	%fd3078, %fd690, %fd3077, 0d0000000000000000;
	fma.rn.f64 	%fd3079, %fd691, %fd3077, 0d0000000000000000;
	fma.rn.f64 	%fd3080, %fd692, %fd3077, 0d0000000000000000;
	add.f64 	%fd3081, %fd3061, %fd3061;
	add.f64 	%fd3082, %fd3064, %fd3064;
	add.f64 	%fd3083, %fd3067, %fd3067;
	fma.rn.f64 	%fd3084, %fd3081, %fd3073, 0d0000000000000000;
	fma.rn.f64 	%fd3085, %fd3082, %fd3073, 0d0000000000000000;
	fma.rn.f64 	%fd3086, %fd3083, %fd3073, 0d0000000000000000;
	mul.f64 	%fd3087, %fd3059, %fd3086;
	mul.f64 	%fd3088, %fd3057, %fd3085;
	sub.f64 	%fd3089, %fd3087, %fd3088;
	mul.f64 	%fd3090, %fd3057, %fd3084;
	mul.f64 	%fd3091, %fd3056, %fd3086;
	sub.f64 	%fd3092, %fd3090, %fd3091;
	mul.f64 	%fd3093, %fd3056, %fd3085;
	mul.f64 	%fd3094, %fd3059, %fd3084;
	sub.f64 	%fd3095, %fd3093, %fd3094;
	add.f64 	%fd3096, %fd3089, 0d0000000000000000;
	add.f64 	%fd3097, %fd3092, 0d0000000000000000;
	add.f64 	%fd3098, %fd3095, 0d0000000000000000;
	mul.f64 	%fd3099, %fd32, %fd3086;
	mul.f64 	%fd3100, %fd33, %fd3085;
	mul.f64 	%fd3101, %fd33, %fd3084;
	mul.f64 	%fd3102, %fd31, %fd3086;
	mul.f64 	%fd3103, %fd31, %fd3085;
	mul.f64 	%fd3104, %fd32, %fd3084;
	sub.f64 	%fd3105, %fd3100, %fd3099;
	add.f64 	%fd3106, %fd3105, 0d0000000000000000;
	sub.f64 	%fd3107, %fd3102, %fd3101;
	add.f64 	%fd3108, %fd3107, 0d0000000000000000;
	sub.f64 	%fd3109, %fd3104, %fd3103;
	add.f64 	%fd3110, %fd3109, 0d0000000000000000;
	add.f64 	%fd3111, %fd762, %fd3078;
	add.f64 	%fd3112, %fd761, %fd3079;
	add.f64 	%fd3113, %fd760, %fd3080;
	sub.f64 	%fd3114, %fd759, %fd3078;
	sub.f64 	%fd3115, %fd758, %fd3079;
	sub.f64 	%fd3116, %fd757, %fd3080;
	add.f64 	%fd762, %fd3111, %fd3106;
	add.f64 	%fd761, %fd3112, %fd3108;
	add.f64 	%fd760, %fd3113, %fd3110;
	sub.f64 	%fd3117, %fd765, %fd3106;
	sub.f64 	%fd3118, %fd764, %fd3108;
	sub.f64 	%fd3119, %fd763, %fd3110;
	add.f64 	%fd759, %fd3114, %fd3096;
	add.f64 	%fd758, %fd3115, %fd3097;
	add.f64 	%fd757, %fd3116, %fd3098;
	sub.f64 	%fd765, %fd3117, %fd3096;
	sub.f64 	%fd764, %fd3118, %fd3097;
	sub.f64 	%fd763, %fd3119, %fd3098;
	bra.uni 	$L__BB5_192;

$L__BB5_182:
	setp.eq.s32 	%p164, %r665, 7;
	@%p164 bra 	$L__BB5_184;
	bra.uni 	$L__BB5_183;

$L__BB5_184:
	sub.f64 	%fd2989, %fd14, %fd22;
	sub.f64 	%fd2990, %fd19, %fd28;
	sub.f64 	%fd2991, %fd17, %fd25;
	mul.f64 	%fd2992, %fd2991, %fd2990;
	sub.f64 	%fd2993, %fd16, %fd25;
	sub.f64 	%fd2994, %fd20, %fd28;
	mul.f64 	%fd2995, %fd2993, %fd2994;
	sub.f64 	%fd2996, %fd2992, %fd2995;
	sub.f64 	%fd2997, %fd13, %fd22;
	mul.f64 	%fd2998, %fd2997, %fd2994;
	mul.f64 	%fd2999, %fd2989, %fd2990;
	sub.f64 	%fd3000, %fd2998, %fd2999;
	mul.f64 	%fd3001, %fd2989, %fd2993;
	mul.f64 	%fd3002, %fd2997, %fd2991;
	sub.f64 	%fd3003, %fd3001, %fd3002;
	mul.f64 	%fd3004, %fd3000, %fd3000;
	fma.rn.f64 	%fd3005, %fd2996, %fd2996, %fd3004;
	fma.rn.f64 	%fd3006, %fd3003, %fd3003, %fd3005;
	div.rn.f64 	%fd3007, %fd3006, %fd34;
	div.rn.f64 	%fd3008, %fd771, %fd34;
	add.f64 	%fd3009, %fd3008, 0d0000000000000000;
	mov.f64 	%fd3010, 0d0000000000000000;
	mul.f64 	%fd3011, %fd3007, %fd771;
	div.rn.f64 	%fd3012, %fd3011, %fd34;
	sub.f64 	%fd3013, %fd3010, %fd3012;
	fma.rn.f64 	%fd3014, %fd690, %fd3013, 0d0000000000000000;
	fma.rn.f64 	%fd3015, %fd691, %fd3013, 0d0000000000000000;
	fma.rn.f64 	%fd3016, %fd692, %fd3013, 0d0000000000000000;
	add.f64 	%fd3017, %fd2996, %fd2996;
	add.f64 	%fd3018, %fd3000, %fd3000;
	add.f64 	%fd3019, %fd3003, %fd3003;
	fma.rn.f64 	%fd3020, %fd3017, %fd3009, 0d0000000000000000;
	fma.rn.f64 	%fd3021, %fd3018, %fd3009, 0d0000000000000000;
	fma.rn.f64 	%fd3022, %fd3019, %fd3009, 0d0000000000000000;
	mul.f64 	%fd3023, %fd2993, %fd3022;
	mul.f64 	%fd3024, %fd2990, %fd3021;
	sub.f64 	%fd3025, %fd3023, %fd3024;
	mul.f64 	%fd3026, %fd2990, %fd3020;
	mul.f64 	%fd3027, %fd2997, %fd3022;
	sub.f64 	%fd3028, %fd3026, %fd3027;
	mul.f64 	%fd3029, %fd2997, %fd3021;
	mul.f64 	%fd3030, %fd2993, %fd3020;
	sub.f64 	%fd3031, %fd3029, %fd3030;
	add.f64 	%fd3032, %fd3025, 0d0000000000000000;
	add.f64 	%fd3033, %fd3028, 0d0000000000000000;
	add.f64 	%fd3034, %fd3031, 0d0000000000000000;
	mul.f64 	%fd3035, %fd2991, %fd3022;
	mul.f64 	%fd3036, %fd2994, %fd3021;
	mul.f64 	%fd3037, %fd2994, %fd3020;
	mul.f64 	%fd3038, %fd2989, %fd3022;
	mul.f64 	%fd3039, %fd2989, %fd3021;
	mul.f64 	%fd3040, %fd2991, %fd3020;
	sub.f64 	%fd3041, %fd3036, %fd3035;
	add.f64 	%fd3042, %fd3041, 0d0000000000000000;
	sub.f64 	%fd3043, %fd3038, %fd3037;
	add.f64 	%fd3044, %fd3043, 0d0000000000000000;
	sub.f64 	%fd3045, %fd3040, %fd3039;
	add.f64 	%fd3046, %fd3045, 0d0000000000000000;
	add.f64 	%fd3047, %fd762, %fd3014;
	add.f64 	%fd3048, %fd761, %fd3015;
	add.f64 	%fd3049, %fd760, %fd3016;
	sub.f64 	%fd3050, %fd759, %fd3014;
	sub.f64 	%fd3051, %fd758, %fd3015;
	sub.f64 	%fd3052, %fd757, %fd3016;
	add.f64 	%fd762, %fd3047, %fd3042;
	add.f64 	%fd761, %fd3048, %fd3044;
	add.f64 	%fd760, %fd3049, %fd3046;
	sub.f64 	%fd3053, %fd768, %fd3042;
	sub.f64 	%fd3054, %fd767, %fd3044;
	sub.f64 	%fd3055, %fd766, %fd3046;
	add.f64 	%fd759, %fd3050, %fd3032;
	add.f64 	%fd758, %fd3051, %fd3033;
	add.f64 	%fd757, %fd3052, %fd3034;
	sub.f64 	%fd768, %fd3053, %fd3032;
	sub.f64 	%fd767, %fd3054, %fd3033;
	sub.f64 	%fd766, %fd3055, %fd3034;
	bra.uni 	$L__BB5_192;

$L__BB5_183:
	sub.f64 	%fd2919, %fd23, %fd14;
	mul.f64 	%fd2920, %fd21, %fd27;
	mul.f64 	%fd2921, %fd18, %fd30;
	sub.f64 	%fd2922, %fd2921, %fd2920;
	mul.f64 	%fd2923, %fd15, %fd30;
	mul.f64 	%fd2924, %fd21, %fd24;
	sub.f64 	%fd2925, %fd2924, %fd2923;
	mul.f64 	%fd2926, %fd18, %fd24;
	mul.f64 	%fd2927, %fd15, %fd27;
	sub.f64 	%fd2928, %fd2927, %fd2926;
	sub.f64 	%fd2929, %fd26, %fd17;
	mul.f64 	%fd2930, %fd2929, %fd2925;
	fma.rn.f64 	%fd2931, %fd2919, %fd2922, %fd2930;
	sub.f64 	%fd2932, %fd29, %fd20;
	fma.rn.f64 	%fd2933, %fd2932, %fd2928, %fd2931;
	mul.f64 	%fd2934, %fd2933, %fd2933;
	mul.f64 	%fd2935, %fd2925, %fd2925;
	fma.rn.f64 	%fd2936, %fd2922, %fd2922, %fd2935;
	fma.rn.f64 	%fd2937, %fd2928, %fd2928, %fd2936;
	div.rn.f64 	%fd2938, %fd2934, %fd2937;
	div.rn.f64 	%fd2939, %fd771, %fd2937;
	add.f64 	%fd2940, %fd2939, 0d0000000000000000;
	mov.f64 	%fd2941, 0d0000000000000000;
	mul.f64 	%fd2942, %fd2938, %fd771;
	div.rn.f64 	%fd2943, %fd2942, %fd2937;
	sub.f64 	%fd2944, %fd2941, %fd2943;
	add.f64 	%fd2945, %fd2922, %fd2922;
	add.f64 	%fd2946, %fd2925, %fd2925;
	add.f64 	%fd2947, %fd2928, %fd2928;
	fma.rn.f64 	%fd2948, %fd2945, %fd2944, 0d0000000000000000;
	fma.rn.f64 	%fd2949, %fd2946, %fd2944, 0d0000000000000000;
	fma.rn.f64 	%fd2950, %fd2947, %fd2944, 0d0000000000000000;
	fma.rn.f64 	%fd2951, %fd2933, %fd2940, 0d0000000000000000;
	fma.rn.f64 	%fd2952, %fd2933, %fd2940, %fd2951;
	fma.rn.f64 	%fd2953, %fd2922, %fd2952, 0d0000000000000000;
	fma.rn.f64 	%fd2954, %fd2925, %fd2952, 0d0000000000000000;
	fma.rn.f64 	%fd2955, %fd2928, %fd2952, 0d0000000000000000;
	fma.rn.f64 	%fd2956, %fd2919, %fd2952, %fd2948;
	fma.rn.f64 	%fd2957, %fd2929, %fd2952, %fd2949;
	fma.rn.f64 	%fd2958, %fd2932, %fd2952, %fd2950;
	mul.f64 	%fd2959, %fd27, %fd2958;
	mul.f64 	%fd2960, %fd30, %fd2957;
	sub.f64 	%fd2961, %fd2959, %fd2960;
	mul.f64 	%fd2962, %fd30, %fd2956;
	mul.f64 	%fd2963, %fd24, %fd2958;
	sub.f64 	%fd2964, %fd2962, %fd2963;
	mul.f64 	%fd2965, %fd24, %fd2957;
	mul.f64 	%fd2966, %fd27, %fd2956;
	sub.f64 	%fd2967, %fd2965, %fd2966;
	add.f64 	%fd2968, %fd2961, 0d0000000000000000;
	add.f64 	%fd2969, %fd2964, 0d0000000000000000;
	add.f64 	%fd2970, %fd2967, 0d0000000000000000;
	mul.f64 	%fd2971, %fd18, %fd2958;
	mul.f64 	%fd2972, %fd21, %fd2957;
	mul.f64 	%fd2973, %fd21, %fd2956;
	mul.f64 	%fd2974, %fd15, %fd2958;
	mul.f64 	%fd2975, %fd15, %fd2957;
	mul.f64 	%fd2976, %fd18, %fd2956;
	sub.f64 	%fd2977, %fd2972, %fd2971;
	add.f64 	%fd2978, %fd2977, 0d0000000000000000;
	sub.f64 	%fd2979, %fd2974, %fd2973;
	add.f64 	%fd2980, %fd2979, 0d0000000000000000;
	sub.f64 	%fd2981, %fd2976, %fd2975;
	add.f64 	%fd2982, %fd2981, 0d0000000000000000;
	add.f64 	%fd2983, %fd765, %fd2953;
	add.f64 	%fd2984, %fd764, %fd2954;
	add.f64 	%fd2985, %fd763, %fd2955;
	sub.f64 	%fd2986, %fd759, %fd2953;
	sub.f64 	%fd2987, %fd758, %fd2954;
	sub.f64 	%fd2988, %fd757, %fd2955;
	add.f64 	%fd768, %fd768, %fd2978;
	add.f64 	%fd767, %fd767, %fd2980;
	add.f64 	%fd766, %fd766, %fd2982;
	sub.f64 	%fd765, %fd2983, %fd2978;
	sub.f64 	%fd764, %fd2984, %fd2980;
	sub.f64 	%fd763, %fd2985, %fd2982;
	add.f64 	%fd762, %fd762, %fd2968;
	add.f64 	%fd761, %fd761, %fd2969;
	add.f64 	%fd760, %fd760, %fd2970;
	sub.f64 	%fd759, %fd2986, %fd2968;
	sub.f64 	%fd758, %fd2987, %fd2969;
	sub.f64 	%fd757, %fd2988, %fd2970;

$L__BB5_192:
	mov.f64 	%fd6018, 0d0000000000000000;
	mov.f64 	%fd6019, 0d0000000000000000;
	mov.f64 	%fd6020, 0d0000000000000000;
	@%p31 bra 	$L__BB5_195;

	setp.ge.f64 	%p166, %fd40, %fd39;
	mov.u16 	%rs365, 1;
	@%p166 bra 	$L__BB5_195;

	mul.f64 	%fd3295, %fd18, %fd30;
	mul.f64 	%fd3296, %fd21, %fd27;
	sub.f64 	%fd6018, %fd3295, %fd3296;
	mul.f64 	%fd3297, %fd15, %fd30;
	mul.f64 	%fd3298, %fd21, %fd24;
	sub.f64 	%fd6019, %fd3298, %fd3297;
	mul.f64 	%fd3299, %fd18, %fd24;
	mul.f64 	%fd3300, %fd15, %fd27;
	sub.f64 	%fd6020, %fd3300, %fd3299;
	mul.f64 	%fd6021, %fd34, 0d3BC79CA100000000;
	mov.u16 	%rs365, 0;

$L__BB5_195:
	and.b16  	%rs295, %rs365, 255;
	setp.eq.s16 	%p167, %rs295, 0;
	and.pred  	%p168, %p167, %p13;
	@%p168 bra 	$L__BB5_197;
	bra.uni 	$L__BB5_196;

$L__BB5_197:
	mul.f64 	%fd6025, %fd36, 0d0000000000000000;
	add.f64 	%fd3315, %fd6025, 0d0000000000000000;
	fma.rn.f64 	%fd6029, %fd6021, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3316, %fd3315, 0d3BC79CA100000000, 0d0000000000000000;
	add.f64 	%fd3317, %fd6018, %fd6018;
	add.f64 	%fd3318, %fd6019, %fd6019;
	add.f64 	%fd3319, %fd6020, %fd6020;
	fma.rn.f64 	%fd3320, %fd3317, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3321, %fd3318, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3322, %fd3319, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd3323, %fd31, 0d0000000000000000, %fd3320;
	fma.rn.f64 	%fd3324, %fd32, 0d0000000000000000, %fd3321;
	fma.rn.f64 	%fd3325, %fd33, 0d0000000000000000, %fd3322;
	fma.rn.f64 	%fd6032, %fd6018, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6033, %fd6019, 0d0000000000000000, 0d0000000000000000;
	fma.rn.f64 	%fd6034, %fd6020, 0d0000000000000000, 0d0000000000000000;
	mul.f64 	%fd3326, %fd27, %fd3325;
	mul.f64 	%fd3327, %fd30, %fd3324;
	sub.f64 	%fd3328, %fd3326, %fd3327;
	mul.f64 	%fd3329, %fd30, %fd3323;
	mul.f64 	%fd3330, %fd24, %fd3325;
	sub.f64 	%fd3331, %fd3329, %fd3330;
	mul.f64 	%fd3332, %fd24, %fd3324;
	mul.f64 	%fd3333, %fd27, %fd3323;
	sub.f64 	%fd3334, %fd3332, %fd3333;
	add.f64 	%fd6038, %fd3328, 0d0000000000000000;
	add.f64 	%fd6039, %fd3331, 0d0000000000000000;
	add.f64 	%fd6040, %fd3334, 0d0000000000000000;
	mul.f64 	%fd3335, %fd18, %fd3325;
	mul.f64 	%fd3336, %fd21, %fd3324;
	mul.f64 	%fd3337, %fd21, %fd3323;
	mul.f64 	%fd3338, %fd15, %fd3325;
	mul.f64 	%fd3339, %fd15, %fd3324;
	mul.f64 	%fd3340, %fd18, %fd3323;
	sub.f64 	%fd3341, %fd3336, %fd3335;
	add.f64 	%fd6035, %fd3341, 0d0000000000000000;
	sub.f64 	%fd3342, %fd3338, %fd3337;
	add.f64 	%fd6036, %fd3342, 0d0000000000000000;
	sub.f64 	%fd3343, %fd3340, %fd3339;
	add.f64 	%fd6037, %fd3343, 0d0000000000000000;
	mul.f64 	%fd6026, %fd37, 0d0000000000000000;
	add.f64 	%fd6030, %fd6026, 0d0000000000000000;
	mul.f64 	%fd6023, %fd35, 0d0000000000000000;
	add.f64 	%fd6028, %fd6023, 0d0000000000000000;
	mul.f64 	%fd6024, %fd38, 0d0000000000000000;
	add.f64 	%fd6031, %fd6024, %fd3316;
	mul.f64 	%fd6022, %fd34, 0d0000000000000000;
	add.f64 	%fd6027, %fd6022, 0d0000000000000000;
	bra.uni 	$L__BB5_198;

$L__BB5_196:
	mul.f64 	%fd6026, %fd37, 0d0000000000000000;
	mov.f64 	%fd6027, 0d0000000000000000;
	mul.f64 	%fd6025, %fd36, 0d0000000000000000;
	mul.f64 	%fd6024, %fd38, 0d0000000000000000;
	mul.f64 	%fd6023, %fd35, 0d0000000000000000;
	mul.f64 	%fd6022, %fd34, 0d0000000000000000;
	mov.f64 	%fd6028, %fd6027;
	mov.f64 	%fd6029, %fd6027;
	mov.f64 	%fd6030, %fd6027;
	mov.f64 	%fd6031, %fd6027;
	mov.f64 	%fd6032, %fd6027;
	mov.f64 	%fd6033, %fd6027;
	mov.f64 	%fd6034, %fd6027;
	mov.f64 	%fd6035, %fd6027;
	mov.f64 	%fd6036, %fd6027;
	mov.f64 	%fd6037, %fd6027;
	mov.f64 	%fd6038, %fd6027;
	mov.f64 	%fd6039, %fd6027;
	mov.f64 	%fd6040, %fd6027;

$L__BB5_198:
	add.f64 	%fd3344, %fd6027, 0d0000000000000000;
	selp.f64 	%fd3345, %fd3344, %fd6027, %p31;
	add.f64 	%fd3346, %fd6029, 0d0000000000000000;
	selp.f64 	%fd3347, %fd3346, %fd6029, %p31;
	add.f64 	%fd3348, %fd6026, %fd3347;
	add.f64 	%fd3349, %fd6023, %fd3345;
	add.f64 	%fd3350, %fd6024, %fd6030;
	add.f64 	%fd3351, %fd6023, %fd3350;
	add.f64 	%fd3352, %fd6023, %fd3351;
	add.f64 	%fd3353, %fd6022, %fd3348;
	fma.rn.f64 	%fd3354, %fd31, %fd3349, %fd6035;
	fma.rn.f64 	%fd3355, %fd32, %fd3349, %fd6036;
	fma.rn.f64 	%fd3356, %fd33, %fd3349, %fd6037;
	fma.rn.f64 	%fd3357, %fd24, %fd3349, %fd6032;
	fma.rn.f64 	%fd3358, %fd27, %fd3349, %fd6033;
	fma.rn.f64 	%fd3359, %fd30, %fd3349, %fd6034;
	add.f64 	%fd3360, %fd6025, %fd6028;
	fma.rn.f64 	%fd3361, %fd31, %fd3360, %fd6038;
	fma.rn.f64 	%fd3362, %fd32, %fd3360, %fd6039;
	fma.rn.f64 	%fd3363, %fd33, %fd3360, %fd6040;
	fma.rn.f64 	%fd3364, %fd15, %fd3360, %fd3357;
	fma.rn.f64 	%fd3365, %fd18, %fd3360, %fd3358;
	fma.rn.f64 	%fd3366, %fd21, %fd3360, %fd3359;
	fma.rn.f64 	%fd3367, %fd687, %fd3353, %fd3354;
	fma.rn.f64 	%fd3368, %fd688, %fd3353, %fd3355;
	fma.rn.f64 	%fd3369, %fd689, %fd3353, %fd3356;
	fma.rn.f64 	%fd3370, %fd24, %fd3352, %fd3361;
	fma.rn.f64 	%fd3371, %fd27, %fd3352, %fd3362;
	fma.rn.f64 	%fd3372, %fd30, %fd3352, %fd3363;
	fma.rn.f64 	%fd3373, %fd15, %fd3352, %fd3367;
	fma.rn.f64 	%fd3374, %fd18, %fd3352, %fd3368;
	fma.rn.f64 	%fd3375, %fd21, %fd3352, %fd3369;
	add.f64 	%fd3376, %fd6025, %fd6031;
	fma.rn.f64 	%fd3377, %fd690, %fd3376, %fd3370;
	fma.rn.f64 	%fd3378, %fd691, %fd3376, %fd3371;
	fma.rn.f64 	%fd3379, %fd692, %fd3376, %fd3372;
	add.f64 	%fd3380, %fd759, %fd3364;
	add.f64 	%fd3381, %fd758, %fd3365;
	add.f64 	%fd3382, %fd757, %fd3366;
	sub.f64 	%fd3383, %fd765, %fd3364;
	sub.f64 	%fd3384, %fd764, %fd3365;
	sub.f64 	%fd3385, %fd763, %fd3366;
	add.f64 	%fd3386, %fd768, %fd3373;
	add.f64 	%fd3387, %fd767, %fd3374;
	add.f64 	%fd3388, %fd766, %fd3375;
	sub.f64 	%fd921, %fd3383, %fd3373;
	sub.f64 	%fd922, %fd3384, %fd3374;
	sub.f64 	%fd923, %fd3385, %fd3375;
	add.f64 	%fd924, %fd762, %fd3377;
	add.f64 	%fd925, %fd761, %fd3378;
	add.f64 	%fd926, %fd760, %fd3379;
	sub.f64 	%fd927, %fd3380, %fd3377;
	sub.f64 	%fd928, %fd3381, %fd3378;
	sub.f64 	%fd929, %fd3382, %fd3379;
	add.f64 	%fd930, %fd3386, 0d0000000000000000;
	add.f64 	%fd931, %fd3387, 0d0000000000000000;
	add.f64 	%fd932, %fd3388, 0d0000000000000000;
	setp.eq.s64 	%p170, %rd141, 0;
	@%p170 bra 	$L__BB5_200;

	mul.lo.s64 	%rd260, %rd87, %rd58;
	add.s64 	%rd257, %rd141, %rd260;
	// begin inline asm
	{ atom.add.f64 %fd3389,[%rd257],%fd930; }

	// end inline asm
	add.s64 	%rd258, %rd257, 8;
	// begin inline asm
	{ atom.add.f64 %fd3391,[%rd258],%fd931; }

	// end inline asm
	add.s64 	%rd259, %rd257, 16;
	// begin inline asm
	{ atom.add.f64 %fd3393,[%rd259],%fd932; }

	// end inline asm
	bra.uni 	$L__BB5_202;

$L__BB5_200:
	setp.eq.s64 	%p171, %rd116, 0;
	@%p171 bra 	$L__BB5_202;

	add.s64 	%rd261, %rd116, %rd88;
	// begin inline asm
	{ atom.add.f64 %fd3395,[%rd261],%fd930; }

	// end inline asm
	add.s64 	%rd262, %rd261, 8;
	// begin inline asm
	{ atom.add.f64 %fd3397,[%rd262],%fd931; }

	// end inline asm
	add.s64 	%rd263, %rd261, 16;
	// begin inline asm
	{ atom.add.f64 %fd3399,[%rd263],%fd932; }

	// end inline asm

$L__BB5_202:
	add.f64 	%fd933, %fd921, 0d0000000000000000;
	add.f64 	%fd934, %fd922, 0d0000000000000000;
	add.f64 	%fd935, %fd923, 0d0000000000000000;
	@%p170 bra 	$L__BB5_204;

	mul.lo.s64 	%rd267, %rd85, %rd58;
	add.s64 	%rd264, %rd141, %rd267;
	// begin inline asm
	{ atom.add.f64 %fd3401,[%rd264],%fd933; }

	// end inline asm
	add.s64 	%rd265, %rd264, 8;
	// begin inline asm
	{ atom.add.f64 %fd3403,[%rd265],%fd934; }

	// end inline asm
	add.s64 	%rd266, %rd264, 16;
	// begin inline asm
	{ atom.add.f64 %fd3405,[%rd266],%fd935; }

	// end inline asm
	bra.uni 	$L__BB5_206;

$L__BB5_204:
	setp.eq.s64 	%p173, %rd116, 0;
	@%p173 bra 	$L__BB5_206;

	add.s64 	%rd268, %rd116, %rd86;
	// begin inline asm
	{ atom.add.f64 %fd3407,[%rd268],%fd933; }

	// end inline asm
	add.s64 	%rd269, %rd268, 8;
	// begin inline asm
	{ atom.add.f64 %fd3409,[%rd269],%fd934; }

	// end inline asm
	add.s64 	%rd270, %rd268, 16;
	// begin inline asm
	{ atom.add.f64 %fd3411,[%rd270],%fd935; }

	// end inline asm

$L__BB5_206:
	add.f64 	%fd936, %fd924, 0d0000000000000000;
	add.f64 	%fd937, %fd925, 0d0000000000000000;
	add.f64 	%fd938, %fd926, 0d0000000000000000;
	@%p170 bra 	$L__BB5_208;

	mul.lo.s64 	%rd274, %rd83, %rd58;
	add.s64 	%rd271, %rd141, %rd274;
	// begin inline asm
	{ atom.add.f64 %fd3413,[%rd271],%fd936; }

	// end inline asm
	add.s64 	%rd272, %rd271, 8;
	// begin inline asm
	{ atom.add.f64 %fd3415,[%rd272],%fd937; }

	// end inline asm
	add.s64 	%rd273, %rd271, 16;
	// begin inline asm
	{ atom.add.f64 %fd3417,[%rd273],%fd938; }

	// end inline asm
	bra.uni 	$L__BB5_210;

$L__BB5_208:
	setp.eq.s64 	%p175, %rd116, 0;
	@%p175 bra 	$L__BB5_210;

	add.s64 	%rd275, %rd116, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd3419,[%rd275],%fd936; }

	// end inline asm
	add.s64 	%rd276, %rd275, 8;
	// begin inline asm
	{ atom.add.f64 %fd3421,[%rd276],%fd937; }

	// end inline asm
	add.s64 	%rd277, %rd275, 16;
	// begin inline asm
	{ atom.add.f64 %fd3423,[%rd277],%fd938; }

	// end inline asm

$L__BB5_210:
	add.f64 	%fd939, %fd927, 0d0000000000000000;
	add.f64 	%fd940, %fd928, 0d0000000000000000;
	add.f64 	%fd941, %fd929, 0d0000000000000000;
	@%p170 bra 	$L__BB5_212;

	mul.lo.s64 	%rd281, %rd81, %rd58;
	add.s64 	%rd278, %rd141, %rd281;
	// begin inline asm
	{ atom.add.f64 %fd3425,[%rd278],%fd939; }

	// end inline asm
	add.s64 	%rd279, %rd278, 8;
	// begin inline asm
	{ atom.add.f64 %fd3427,[%rd279],%fd940; }

	// end inline asm
	add.s64 	%rd280, %rd278, 16;
	// begin inline asm
	{ atom.add.f64 %fd3429,[%rd280],%fd941; }

	// end inline asm
	bra.uni 	$L__BB5_214;

$L__BB5_212:
	setp.eq.s64 	%p177, %rd116, 0;
	@%p177 bra 	$L__BB5_214;

	add.s64 	%rd282, %rd116, %rd82;
	// begin inline asm
	{ atom.add.f64 %fd3431,[%rd282],%fd939; }

	// end inline asm
	add.s64 	%rd283, %rd282, 8;
	// begin inline asm
	{ atom.add.f64 %fd3433,[%rd283],%fd940; }

	// end inline asm
	add.s64 	%rd284, %rd282, 16;
	// begin inline asm
	{ atom.add.f64 %fd3435,[%rd284],%fd941; }

	// end inline asm

$L__BB5_214:
	setp.eq.s64 	%p178, %rd147, 0;
	add.f64 	%fd942, %fd772, 0d0000000000000000;
	@%p178 bra 	$L__BB5_216;

	mul.lo.s64 	%rd287, %rd78, %rd64;
	add.s64 	%rd285, %rd147, %rd287;
	// begin inline asm
	{ atom.add.f64 %fd3437,[%rd285],%fd942; }

	// end inline asm
	mul.lo.s64 	%rd288, %rd77, %rd64;
	add.s64 	%rd286, %rd147, %rd288;
	// begin inline asm
	{ atom.add.f64 %fd3439,[%rd286],%fd942; }

	// end inline asm
	bra.uni 	$L__BB5_368;

$L__BB5_216:
	setp.eq.s64 	%p179, %rd124, 0;
	@%p179 bra 	$L__BB5_368;

	mul.lo.s64 	%rd365, %rd77, %rd363;
	mul.lo.s64 	%rd364, %rd78, %rd363;
	add.s64 	%rd289, %rd124, %rd364;
	// begin inline asm
	{ atom.add.f64 %fd3441,[%rd289],%fd942; }

	// end inline asm
	add.s64 	%rd290, %rd124, %rd365;
	// begin inline asm
	{ atom.add.f64 %fd3443,[%rd290],%fd942; }

	// end inline asm

$L__BB5_368:
	ld.param.u64 	%rd362, [initialize_friction_collisions_cuda_kernel_backward_param_0+24];
	mov.u32 	%r640, %ntid.x;
	mov.u32 	%r639, %nctaid.x;
	mul.wide.u32 	%rd361, %r640, %r639;
	add.s64 	%rd370, %rd370, %rd361;
	setp.lt.u64 	%p322, %rd370, %rd362;
	@%p322 bra 	$L__BB5_2;

$L__BB5_369:
	ret;

}
	// .globl	initialize_friction_hs_cuda_kernel_forward
.visible .entry initialize_friction_hs_cuda_kernel_forward(
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_forward_param_9[56],
	.param .f64 initialize_friction_hs_cuda_kernel_forward_param_10,
	.param .f64 initialize_friction_hs_cuda_kernel_forward_param_11
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<73>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<204>;
	.reg .f64 	%fd<93>;
	.reg .b64 	%rd<94>;


	ld.param.v2.u32 	{%r98, %r99}, [initialize_friction_hs_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r100, %r101}, [initialize_friction_hs_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r106, %r107}, [initialize_friction_hs_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r114, %r115}, [initialize_friction_hs_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r122, %r123}, [initialize_friction_hs_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r130, %r131}, [initialize_friction_hs_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r138, %r139}, [initialize_friction_hs_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r146, %r147}, [initialize_friction_hs_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r154, %r155}, [initialize_friction_hs_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r162, %r163}, [initialize_friction_hs_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r170, %r171}, [initialize_friction_hs_cuda_kernel_forward_param_9+32];
	ld.param.f64 	%fd12, [initialize_friction_hs_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd56, [initialize_friction_hs_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd54, [initialize_friction_hs_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd52, [initialize_friction_hs_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd50, [initialize_friction_hs_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [initialize_friction_hs_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd46, [initialize_friction_hs_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd44, [initialize_friction_hs_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd42, [initialize_friction_hs_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd40, [initialize_friction_hs_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd39, [initialize_friction_hs_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r16, [initialize_friction_hs_cuda_kernel_forward_param_0+16];
	mov.u32 	%r174, %ntid.x;
	cvt.u64.u32 	%rd1, %r174;
	mov.u32 	%r175, %ctaid.x;
	mul.wide.u32 	%rd58, %r174, %r175;
	mov.u32 	%r176, %tid.x;
	cvt.u64.u32 	%rd59, %r176;
	add.s64 	%rd90, %rd58, %rd59;
	setp.ge.u64 	%p1, %rd90, %rd39;
	@%p1 bra 	$L__BB6_24;

	cvta.to.global.u64 	%rd4, %rd56;
	cvta.to.global.u64 	%rd5, %rd54;
	cvta.to.global.u64 	%rd6, %rd52;
	cvta.to.global.u64 	%rd7, %rd50;
	cvta.to.global.u64 	%rd8, %rd48;
	cvta.to.global.u64 	%rd9, %rd46;
	cvta.to.global.u64 	%rd10, %rd44;
	cvta.to.global.u64 	%rd11, %rd42;
	cvta.to.global.u64 	%rd12, %rd40;
	cvt.s64.s32 	%rd13, %r101;
	cvt.s64.s32 	%rd14, %r100;
	cvt.s64.s32 	%rd15, %r99;
	cvt.s64.s32 	%rd16, %r106;
	cvt.s64.s32 	%rd17, %r114;
	cvt.s64.s32 	%rd18, %r146;
	cvt.s64.s32 	%rd19, %r138;
	cvt.s64.s32 	%rd20, %r122;
	cvt.s64.s32 	%rd21, %r162;
	cvt.s64.s32 	%rd22, %r170;
	cvt.s64.s32 	%rd23, %r130;
	cvt.s64.s32 	%rd24, %r154;
	mov.u32 	%r177, %nctaid.x;
	cvt.u64.u32 	%rd60, %r177;
	mul.lo.s64 	%rd25, %rd1, %rd60;

$L__BB6_2:
	setp.lt.s32 	%p2, %r16, 4;
	mov.u64 	%rd91, %rd90;
	@%p2 bra 	$L__BB6_6;

	or.b64  	%rd61, %rd90, %rd13;
	and.b64  	%rd62, %rd61, -4294967296;
	setp.eq.s64 	%p3, %rd62, 0;
	@%p3 bra 	$L__BB6_5;

	div.u64 	%rd91, %rd90, %rd13;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r178, %rd13;
	cvt.u32.u64 	%r179, %rd90;
	div.u32 	%r180, %r179, %r178;
	cvt.u64.u32 	%rd91, %r180;

$L__BB6_6:
	setp.lt.s32 	%p4, %r16, 3;
	@%p4 bra 	$L__BB6_10;

	or.b64  	%rd63, %rd91, %rd14;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.eq.s64 	%p5, %rd64, 0;
	@%p5 bra 	$L__BB6_9;

	div.u64 	%rd91, %rd91, %rd14;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r181, %rd14;
	cvt.u32.u64 	%r182, %rd91;
	div.u32 	%r183, %r182, %r181;
	cvt.u64.u32 	%rd91, %r183;

$L__BB6_10:
	setp.lt.s32 	%p6, %r16, 2;
	@%p6 bra 	$L__BB6_14;

	or.b64  	%rd65, %rd91, %rd15;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.eq.s64 	%p7, %rd66, 0;
	@%p7 bra 	$L__BB6_13;

	div.u64 	%rd91, %rd91, %rd15;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r184, %rd15;
	cvt.u32.u64 	%r185, %rd91;
	div.u32 	%r186, %r185, %r184;
	cvt.u64.u32 	%rd91, %r186;

$L__BB6_14:
	cvt.s64.s32 	%rd67, %rd91;
	setp.gt.s32 	%p8, %r16, 0;
	selp.b64 	%rd68, %rd67, 0, %p8;
	mov.u64 	%rd69, 0;
	mul.lo.s64 	%rd70, %rd68, %rd16;
	add.s64 	%rd36, %rd12, %rd70;
	st.global.u64 	[%rd36], %rd69;
	mul.lo.s64 	%rd71, %rd68, %rd17;
	add.s64 	%rd72, %rd11, %rd71;
	ld.global.s32 	%rd37, [%rd72];
	mul.lo.s64 	%rd73, %rd37, %rd18;
	add.s64 	%rd74, %rd7, %rd73;
	mul.lo.s64 	%rd75, %rd37, %rd19;
	add.s64 	%rd76, %rd8, %rd75;
	mul.lo.s64 	%rd77, %rd68, %rd20;
	add.s64 	%rd78, %rd10, %rd77;
	ld.global.s32 	%rd79, [%rd78];
	mul.lo.s64 	%rd80, %rd79, %rd21;
	add.s64 	%rd81, %rd5, %rd80;
	mul.lo.s64 	%rd82, %rd79, %rd22;
	add.s64 	%rd83, %rd4, %rd82;
	ld.global.s32 	%rd84, [%rd76];
	mul.lo.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd86, %rd9, %rd85;
	ld.global.f64 	%fd14, [%rd86];
	ld.global.f64 	%fd15, [%rd83];
	sub.f64 	%fd16, %fd14, %fd15;
	ld.global.f64 	%fd17, [%rd86+8];
	ld.global.f64 	%fd18, [%rd83+8];
	sub.f64 	%fd19, %fd17, %fd18;
	ld.global.f64 	%fd20, [%rd86+16];
	ld.global.f64 	%fd21, [%rd83+16];
	sub.f64 	%fd22, %fd20, %fd21;
	ld.global.f64 	%fd23, [%rd81];
	ld.global.f64 	%fd24, [%rd81+8];
	mul.f64 	%fd25, %fd24, %fd19;
	fma.rn.f64 	%fd26, %fd23, %fd16, %fd25;
	ld.global.f64 	%fd27, [%rd81+16];
	fma.rn.f64 	%fd28, %fd27, %fd22, %fd26;
	ld.global.f64 	%fd29, [%rd74];
	sub.f64 	%fd1, %fd28, %fd29;
	setp.geu.f64 	%p9, %fd1, %fd12;
	@%p9 bra 	$L__BB6_23;

	mul.lo.s64 	%rd87, %rd37, %rd24;
	add.s64 	%rd88, %rd6, %rd87;
	ld.global.f64 	%fd2, [%rd88];
	div.rn.f64 	%fd90, %fd1, %fd12;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r200}, %fd90;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r201, %temp}, %fd90;
	}
	setp.gt.s32 	%p10, %r200, 1048575;
	mov.u32 	%r202, -1023;
	@%p10 bra 	$L__BB6_17;

	mul.f64 	%fd90, %fd90, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r200}, %fd90;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r201, %temp}, %fd90;
	}
	mov.u32 	%r202, -1077;

$L__BB6_17:
	add.s32 	%r189, %r200, -1;
	setp.lt.u32 	%p11, %r189, 2146435071;
	@%p11 bra 	$L__BB6_19;
	bra.uni 	$L__BB6_18;

$L__BB6_19:
	shr.u32 	%r191, %r200, 20;
	add.s32 	%r203, %r202, %r191;
	and.b32  	%r192, %r200, -2146435073;
	or.b32  	%r193, %r192, 1072693248;
	mov.b64 	%fd91, {%r201, %r193};
	setp.lt.s32 	%p13, %r193, 1073127583;
	@%p13 bra 	$L__BB6_21;

	{
	.reg .b32 %temp;
	mov.b64 	{%r194, %temp}, %fd91;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r195}, %fd91;
	}
	add.s32 	%r196, %r195, -1048576;
	mov.b64 	%fd91, {%r194, %r196};
	add.s32 	%r203, %r203, 1;

$L__BB6_21:
	add.f64 	%fd32, %fd91, 0d3FF0000000000000;
	mov.f64 	%fd33, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd34, %fd32;
	neg.f64 	%fd35, %fd32;
	fma.rn.f64 	%fd36, %fd35, %fd34, %fd33;
	fma.rn.f64 	%fd37, %fd36, %fd36, %fd36;
	fma.rn.f64 	%fd38, %fd37, %fd34, %fd34;
	add.f64 	%fd39, %fd91, 0dBFF0000000000000;
	mul.f64 	%fd40, %fd39, %fd38;
	fma.rn.f64 	%fd41, %fd39, %fd38, %fd40;
	mul.f64 	%fd42, %fd41, %fd41;
	mov.f64 	%fd43, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd44, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd45, %fd44, %fd42, %fd43;
	mov.f64 	%fd46, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd47, %fd45, %fd42, %fd46;
	mov.f64 	%fd48, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd49, %fd47, %fd42, %fd48;
	mov.f64 	%fd50, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd51, %fd49, %fd42, %fd50;
	mov.f64 	%fd52, 0d3F624924923BE72D;
	fma.rn.f64 	%fd53, %fd51, %fd42, %fd52;
	mov.f64 	%fd54, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd55, %fd53, %fd42, %fd54;
	mov.f64 	%fd56, 0d3FB5555555555554;
	fma.rn.f64 	%fd57, %fd55, %fd42, %fd56;
	sub.f64 	%fd58, %fd39, %fd41;
	add.f64 	%fd59, %fd58, %fd58;
	neg.f64 	%fd60, %fd41;
	fma.rn.f64 	%fd61, %fd60, %fd39, %fd59;
	mul.f64 	%fd62, %fd38, %fd61;
	mul.f64 	%fd63, %fd42, %fd57;
	fma.rn.f64 	%fd64, %fd63, %fd41, %fd62;
	xor.b32  	%r197, %r203, -2147483648;
	mov.u32 	%r198, -2147483648;
	mov.u32 	%r199, 1127219200;
	mov.b64 	%fd65, {%r197, %r199};
	mov.b64 	%fd66, {%r198, %r199};
	sub.f64 	%fd67, %fd65, %fd66;
	mov.f64 	%fd68, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd69, %fd67, %fd68, %fd41;
	neg.f64 	%fd70, %fd67;
	fma.rn.f64 	%fd71, %fd70, %fd68, %fd69;
	sub.f64 	%fd72, %fd71, %fd41;
	sub.f64 	%fd73, %fd64, %fd72;
	mov.f64 	%fd74, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd75, %fd67, %fd74, %fd73;
	add.f64 	%fd92, %fd69, %fd75;
	bra.uni 	$L__BB6_22;

$L__BB6_18:
	mov.f64 	%fd30, 0d7FF0000000000000;
	fma.rn.f64 	%fd31, %fd90, %fd30, %fd30;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r190}, %fd90;
	}
	mov.b32 	%f1, %r190;
	setp.eq.f32 	%p12, %f1, 0f00000000;
	selp.f64 	%fd92, 0dFFF0000000000000, %fd31, %p12;

$L__BB6_22:
	ld.param.f64 	%fd89, [initialize_friction_hs_cuda_kernel_forward_param_11];
	sub.f64 	%fd76, %fd1, %fd12;
	div.rn.f64 	%fd77, %fd76, %fd12;
	mul.f64 	%fd78, %fd77, %fd92;
	mul.f64 	%fd79, %fd78, 0dC000000000000000;
	div.rn.f64 	%fd80, %fd79, %fd12;
	mul.f64 	%fd81, %fd77, %fd77;
	div.rn.f64 	%fd82, %fd81, %fd1;
	sub.f64 	%fd83, %fd80, %fd82;
	mul.f64 	%fd84, %fd83, %fd89;
	mul.f64 	%fd85, %fd2, %fd12;
	mul.f64 	%fd86, %fd85, %fd84;
	mov.f64 	%fd87, 0d0000000000000000;
	sub.f64 	%fd88, %fd87, %fd86;
	st.global.f64 	[%rd36], %fd88;

$L__BB6_23:
	ld.param.u64 	%rd89, [initialize_friction_hs_cuda_kernel_forward_param_0+24];
	add.s64 	%rd90, %rd90, %rd25;
	setp.lt.u64 	%p14, %rd90, %rd89;
	@%p14 bra 	$L__BB6_2;

$L__BB6_24:
	ret;

}
	// .globl	initialize_friction_hs_cuda_kernel_backward
.visible .entry initialize_friction_hs_cuda_kernel_backward(
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_9[56],
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_10,
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_11,
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 initialize_friction_hs_cuda_kernel_backward_param_20[56],
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_21,
	.param .f64 initialize_friction_hs_cuda_kernel_backward_param_22
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<121>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<334>;
	.reg .f64 	%fd<247>;
	.reg .b64 	%rd<165>;


	ld.param.v2.u32 	{%r160, %r161}, [initialize_friction_hs_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r162, %r163}, [initialize_friction_hs_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r168, %r169}, [initialize_friction_hs_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r176, %r177}, [initialize_friction_hs_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r184, %r185}, [initialize_friction_hs_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r192, %r193}, [initialize_friction_hs_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r200, %r201}, [initialize_friction_hs_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r208, %r209}, [initialize_friction_hs_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r216, %r217}, [initialize_friction_hs_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r224, %r225}, [initialize_friction_hs_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r232, %r233}, [initialize_friction_hs_cuda_kernel_backward_param_9+32];
	ld.param.f64 	%fd46, [initialize_friction_hs_cuda_kernel_backward_param_10];
	ld.param.f64 	%fd47, [initialize_friction_hs_cuda_kernel_backward_param_11];
	ld.param.v2.u32 	{%r240, %r241}, [initialize_friction_hs_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r248, %r249}, [initialize_friction_hs_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r256, %r257}, [initialize_friction_hs_cuda_kernel_backward_param_17+32];
	ld.param.v2.u32 	{%r264, %r265}, [initialize_friction_hs_cuda_kernel_backward_param_18+32];
	ld.param.v2.u32 	{%r272, %r273}, [initialize_friction_hs_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r280, %r281}, [initialize_friction_hs_cuda_kernel_backward_param_20+32];
	ld.param.u64 	%rd94, [initialize_friction_hs_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd92, [initialize_friction_hs_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd90, [initialize_friction_hs_cuda_kernel_backward_param_18];
	ld.param.u64 	%rd88, [initialize_friction_hs_cuda_kernel_backward_param_17];
	ld.param.u64 	%rd86, [initialize_friction_hs_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd84, [initialize_friction_hs_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd83, [initialize_friction_hs_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd82, [initialize_friction_hs_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd81, [initialize_friction_hs_cuda_kernel_backward_param_8+8];
	ld.param.u64 	%rd80, [initialize_friction_hs_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd79, [initialize_friction_hs_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd78, [initialize_friction_hs_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd77, [initialize_friction_hs_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd76, [initialize_friction_hs_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd74, [initialize_friction_hs_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd73, [initialize_friction_hs_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd72, [initialize_friction_hs_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd70, [initialize_friction_hs_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd68, [initialize_friction_hs_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd67, [initialize_friction_hs_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd65, [initialize_friction_hs_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r24, [initialize_friction_hs_cuda_kernel_backward_param_0+16];
	mov.u32 	%r284, %ntid.x;
	cvt.u64.u32 	%rd1, %r284;
	mov.u32 	%r285, %ctaid.x;
	mul.wide.u32 	%rd96, %r284, %r285;
	mov.u32 	%r286, %tid.x;
	cvt.u64.u32 	%rd97, %r286;
	add.s64 	%rd161, %rd96, %rd97;
	setp.ge.u64 	%p1, %rd161, %rd65;
	@%p1 bra 	$L__BB7_54;

	cvta.to.global.u64 	%rd16, %rd84;
	cvta.to.global.u64 	%rd17, %rd82;
	cvta.to.global.u64 	%rd18, %rd80;
	cvta.to.global.u64 	%rd19, %rd78;
	cvta.to.global.u64 	%rd20, %rd76;
	cvta.to.global.u64 	%rd21, %rd74;
	cvta.to.global.u64 	%rd22, %rd72;
	cvta.to.global.u64 	%rd23, %rd70;
	cvta.to.global.u64 	%rd24, %rd68;
	cvta.to.global.u64 	%rd25, %rd67;
	cvt.s64.s32 	%rd26, %r163;
	cvt.s64.s32 	%rd27, %r162;
	cvt.s64.s32 	%rd28, %r161;
	cvt.s64.s32 	%rd29, %r176;
	cvt.s64.s32 	%rd30, %r208;
	cvt.s64.s32 	%rd31, %r200;
	cvt.s64.s32 	%rd32, %r184;
	cvt.s64.s32 	%rd33, %r224;
	cvt.s64.s32 	%rd34, %r232;
	cvt.s64.s32 	%rd35, %r192;
	cvt.s64.s32 	%rd36, %r216;
	cvt.s64.s32 	%rd37, %r248;
	cvt.s64.s32 	%rd38, %r280;
	cvt.s64.s32 	%rd39, %r240;
	cvt.s64.s32 	%rd40, %r272;
	cvt.s64.s32 	%rd41, %r168;
	mov.u32 	%r287, %nctaid.x;
	cvt.u64.u32 	%rd98, %r287;
	mul.lo.s64 	%rd42, %rd1, %rd98;
	cvt.s64.s32 	%rd43, %r256;
	cvt.s64.s32 	%rd44, %r264;

$L__BB7_2:
	setp.lt.s32 	%p2, %r24, 4;
	mov.u64 	%rd162, %rd161;
	@%p2 bra 	$L__BB7_6;

	or.b64  	%rd99, %rd161, %rd26;
	and.b64  	%rd100, %rd99, -4294967296;
	setp.eq.s64 	%p3, %rd100, 0;
	@%p3 bra 	$L__BB7_5;

	div.u64 	%rd162, %rd161, %rd26;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r288, %rd26;
	cvt.u32.u64 	%r289, %rd161;
	div.u32 	%r290, %r289, %r288;
	cvt.u64.u32 	%rd162, %r290;

$L__BB7_6:
	setp.lt.s32 	%p4, %r24, 3;
	@%p4 bra 	$L__BB7_10;

	or.b64  	%rd101, %rd162, %rd27;
	and.b64  	%rd102, %rd101, -4294967296;
	setp.eq.s64 	%p5, %rd102, 0;
	@%p5 bra 	$L__BB7_9;

	div.u64 	%rd162, %rd162, %rd27;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r291, %rd27;
	cvt.u32.u64 	%r292, %rd162;
	div.u32 	%r293, %r292, %r291;
	cvt.u64.u32 	%rd162, %r293;

$L__BB7_10:
	setp.lt.s32 	%p6, %r24, 2;
	@%p6 bra 	$L__BB7_14;

	or.b64  	%rd103, %rd162, %rd28;
	and.b64  	%rd104, %rd103, -4294967296;
	setp.eq.s64 	%p7, %rd104, 0;
	@%p7 bra 	$L__BB7_13;

	div.u64 	%rd162, %rd162, %rd28;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r294, %rd28;
	cvt.u32.u64 	%r295, %rd162;
	div.u32 	%r296, %r295, %r294;
	cvt.u64.u32 	%rd162, %r296;

$L__BB7_14:
	cvt.s64.s32 	%rd153, %r208;
	cvt.s64.s32 	%rd152, %r224;
	cvt.s64.s32 	%rd151, %r232;
	cvt.s64.s32 	%rd150, %r192;
	cvt.s64.s32 	%rd105, %rd162;
	setp.gt.s32 	%p8, %r24, 0;
	selp.b64 	%rd55, %rd105, 0, %p8;
	mul.lo.s64 	%rd106, %rd55, %rd29;
	add.s64 	%rd107, %rd24, %rd106;
	ld.global.s32 	%rd56, [%rd107];
	mul.lo.s64 	%rd57, %rd56, %rd153;
	add.s64 	%rd108, %rd20, %rd57;
	mul.lo.s64 	%rd109, %rd56, %rd31;
	add.s64 	%rd110, %rd21, %rd109;
	mul.lo.s64 	%rd111, %rd55, %rd32;
	add.s64 	%rd112, %rd23, %rd111;
	ld.global.s32 	%rd58, [%rd112];
	mul.lo.s64 	%rd59, %rd58, %rd152;
	add.s64 	%rd113, %rd18, %rd59;
	mul.lo.s64 	%rd60, %rd58, %rd151;
	add.s64 	%rd114, %rd17, %rd60;
	ld.global.s32 	%rd61, [%rd110];
	mul.lo.s64 	%rd62, %rd61, %rd150;
	add.s64 	%rd115, %rd22, %rd62;
	ld.global.f64 	%fd49, [%rd115];
	ld.global.f64 	%fd50, [%rd114];
	sub.f64 	%fd1, %fd49, %fd50;
	ld.global.f64 	%fd51, [%rd115+8];
	ld.global.f64 	%fd52, [%rd114+8];
	sub.f64 	%fd2, %fd51, %fd52;
	ld.global.f64 	%fd53, [%rd115+16];
	ld.global.f64 	%fd54, [%rd114+16];
	sub.f64 	%fd3, %fd53, %fd54;
	ld.global.f64 	%fd4, [%rd113];
	ld.global.f64 	%fd5, [%rd113+8];
	mul.f64 	%fd55, %fd5, %fd2;
	fma.rn.f64 	%fd56, %fd4, %fd1, %fd55;
	ld.global.f64 	%fd6, [%rd113+16];
	fma.rn.f64 	%fd57, %fd6, %fd3, %fd56;
	ld.global.f64 	%fd58, [%rd108];
	sub.f64 	%fd7, %fd57, %fd58;
	setp.geu.f64 	%p9, %fd7, %fd46;
	mov.f64 	%fd246, 0d0000000000000000;
	@%p9 bra 	$L__BB7_37;

	cvt.s64.s32 	%rd154, %r216;
	mul.lo.s64 	%rd63, %rd56, %rd154;
	add.s64 	%rd116, %rd19, %rd63;
	ld.global.f64 	%fd8, [%rd116];
	sub.f64 	%fd59, %fd7, %fd46;
	div.rn.f64 	%fd9, %fd59, %fd46;
	div.rn.f64 	%fd10, %fd7, %fd46;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r326}, %fd10;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r327, %temp}, %fd10;
	}
	setp.gt.s32 	%p10, %r326, 1048575;
	mov.u32 	%r328, -1023;
	mov.f64 	%fd239, %fd10;
	@%p10 bra 	$L__BB7_17;

	mul.f64 	%fd239, %fd10, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r326}, %fd239;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r327, %temp}, %fd239;
	}
	mov.u32 	%r328, -1077;

$L__BB7_17:
	add.s32 	%r299, %r326, -1;
	setp.lt.u32 	%p11, %r299, 2146435071;
	@%p11 bra 	$L__BB7_19;
	bra.uni 	$L__BB7_18;

$L__BB7_19:
	shr.u32 	%r301, %r326, 20;
	add.s32 	%r329, %r328, %r301;
	and.b32  	%r302, %r326, -2146435073;
	or.b32  	%r303, %r302, 1072693248;
	mov.b64 	%fd240, {%r327, %r303};
	setp.lt.s32 	%p13, %r303, 1073127583;
	@%p13 bra 	$L__BB7_21;

	{
	.reg .b32 %temp;
	mov.b64 	{%r304, %temp}, %fd240;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r305}, %fd240;
	}
	add.s32 	%r306, %r305, -1048576;
	mov.b64 	%fd240, {%r304, %r306};
	add.s32 	%r329, %r329, 1;

$L__BB7_21:
	add.f64 	%fd62, %fd240, 0d3FF0000000000000;
	mov.f64 	%fd63, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd64, %fd62;
	neg.f64 	%fd65, %fd62;
	fma.rn.f64 	%fd66, %fd65, %fd64, %fd63;
	fma.rn.f64 	%fd67, %fd66, %fd66, %fd66;
	fma.rn.f64 	%fd68, %fd67, %fd64, %fd64;
	add.f64 	%fd69, %fd240, 0dBFF0000000000000;
	mul.f64 	%fd70, %fd69, %fd68;
	fma.rn.f64 	%fd71, %fd69, %fd68, %fd70;
	mul.f64 	%fd72, %fd71, %fd71;
	mov.f64 	%fd73, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd74, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd75, %fd74, %fd72, %fd73;
	mov.f64 	%fd76, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd77, %fd75, %fd72, %fd76;
	mov.f64 	%fd78, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd79, %fd77, %fd72, %fd78;
	mov.f64 	%fd80, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd81, %fd79, %fd72, %fd80;
	mov.f64 	%fd82, 0d3F624924923BE72D;
	fma.rn.f64 	%fd83, %fd81, %fd72, %fd82;
	mov.f64 	%fd84, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd85, %fd83, %fd72, %fd84;
	mov.f64 	%fd86, 0d3FB5555555555554;
	fma.rn.f64 	%fd87, %fd85, %fd72, %fd86;
	sub.f64 	%fd88, %fd69, %fd71;
	add.f64 	%fd89, %fd88, %fd88;
	neg.f64 	%fd90, %fd71;
	fma.rn.f64 	%fd91, %fd90, %fd69, %fd89;
	mul.f64 	%fd92, %fd68, %fd91;
	mul.f64 	%fd93, %fd72, %fd87;
	fma.rn.f64 	%fd94, %fd93, %fd71, %fd92;
	xor.b32  	%r307, %r329, -2147483648;
	mov.u32 	%r308, -2147483648;
	mov.u32 	%r309, 1127219200;
	mov.b64 	%fd95, {%r307, %r309};
	mov.b64 	%fd96, {%r308, %r309};
	sub.f64 	%fd97, %fd95, %fd96;
	mov.f64 	%fd98, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd99, %fd97, %fd98, %fd71;
	neg.f64 	%fd100, %fd97;
	fma.rn.f64 	%fd101, %fd100, %fd98, %fd99;
	sub.f64 	%fd102, %fd101, %fd71;
	sub.f64 	%fd103, %fd94, %fd102;
	mov.f64 	%fd104, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd105, %fd97, %fd104, %fd103;
	add.f64 	%fd241, %fd99, %fd105;
	bra.uni 	$L__BB7_22;

$L__BB7_18:
	mov.f64 	%fd60, 0d7FF0000000000000;
	fma.rn.f64 	%fd61, %fd239, %fd60, %fd60;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r300}, %fd239;
	}
	mov.b32 	%f1, %r300;
	setp.eq.f32 	%p12, %f1, 0f00000000;
	selp.f64 	%fd241, 0dFFF0000000000000, %fd61, %p12;

$L__BB7_22:
	ld.param.u64 	%rd148, [initialize_friction_hs_cuda_kernel_backward_param_12];
	mul.f64 	%fd106, %fd9, %fd9;
	div.rn.f64 	%fd19, %fd106, %fd7;
	setp.eq.s64 	%p14, %rd148, 0;
	@%p14 bra 	$L__BB7_24;

	mul.lo.s64 	%rd117, %rd55, %rd39;
	add.s64 	%rd118, %rd16, %rd117;
	ld.global.f64 	%fd107, [%rd118];
	add.f64 	%fd242, %fd107, 0d0000000000000000;
	bra.uni 	$L__BB7_26;

$L__BB7_24:
	ld.param.u64 	%rd160, [initialize_friction_hs_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p15, %rd160, 0;
	mov.f64 	%fd242, 0d0000000000000000;
	@%p15 bra 	$L__BB7_26;

	mul.lo.s64 	%rd119, %rd55, %rd41;
	add.s64 	%rd120, %rd25, %rd119;
	ld.global.f64 	%fd109, [%rd120];
	add.f64 	%fd242, %fd109, 0d0000000000000000;

$L__BB7_26:
	{
	.reg .b32 %temp;
	mov.b64 	{%r331, %temp}, %fd10;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r324}, %fd10;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r330}, %fd10;
	}
	setp.gt.s32 	%p31, %r330, 1048575;
	mov.f64 	%fd110, 0d0000000000000000;
	sub.f64 	%fd111, %fd110, %fd242;
	mul.f64 	%fd112, %fd9, %fd241;
	mul.f64 	%fd113, %fd112, 0dC000000000000000;
	div.rn.f64 	%fd114, %fd113, %fd46;
	sub.f64 	%fd115, %fd114, %fd19;
	mul.f64 	%fd116, %fd115, %fd47;
	fma.rn.f64 	%fd23, %fd116, %fd111, 0d0000000000000000;
	mul.f64 	%fd117, %fd8, %fd46;
	fma.rn.f64 	%fd24, %fd117, %fd111, 0d0000000000000000;
	mov.u32 	%r332, -1023;
	mov.f64 	%fd243, %fd10;
	@%p31 bra 	$L__BB7_28;

	mul.f64 	%fd243, %fd10, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r330}, %fd243;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r331, %temp}, %fd243;
	}
	mov.u32 	%r332, -1077;

$L__BB7_28:
	add.s32 	%r312, %r330, -1;
	setp.lt.u32 	%p17, %r312, 2146435071;
	@%p17 bra 	$L__BB7_30;
	bra.uni 	$L__BB7_29;

$L__BB7_30:
	shr.u32 	%r314, %r330, 20;
	add.s32 	%r333, %r332, %r314;
	and.b32  	%r315, %r330, -2146435073;
	or.b32  	%r316, %r315, 1072693248;
	mov.b64 	%fd244, {%r331, %r316};
	setp.lt.s32 	%p19, %r316, 1073127583;
	@%p19 bra 	$L__BB7_32;

	{
	.reg .b32 %temp;
	mov.b64 	{%r317, %temp}, %fd244;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r318}, %fd244;
	}
	add.s32 	%r319, %r318, -1048576;
	mov.b64 	%fd244, {%r317, %r319};
	add.s32 	%r333, %r333, 1;

$L__BB7_32:
	add.f64 	%fd120, %fd244, 0d3FF0000000000000;
	mov.f64 	%fd121, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd122, %fd120;
	neg.f64 	%fd123, %fd120;
	fma.rn.f64 	%fd124, %fd123, %fd122, %fd121;
	fma.rn.f64 	%fd125, %fd124, %fd124, %fd124;
	fma.rn.f64 	%fd126, %fd125, %fd122, %fd122;
	add.f64 	%fd127, %fd244, 0dBFF0000000000000;
	mul.f64 	%fd128, %fd127, %fd126;
	fma.rn.f64 	%fd129, %fd127, %fd126, %fd128;
	mul.f64 	%fd130, %fd129, %fd129;
	mov.f64 	%fd131, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd132, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd133, %fd132, %fd130, %fd131;
	mov.f64 	%fd134, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd135, %fd133, %fd130, %fd134;
	mov.f64 	%fd136, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd137, %fd135, %fd130, %fd136;
	mov.f64 	%fd138, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd139, %fd137, %fd130, %fd138;
	mov.f64 	%fd140, 0d3F624924923BE72D;
	fma.rn.f64 	%fd141, %fd139, %fd130, %fd140;
	mov.f64 	%fd142, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd143, %fd141, %fd130, %fd142;
	mov.f64 	%fd144, 0d3FB5555555555554;
	fma.rn.f64 	%fd145, %fd143, %fd130, %fd144;
	sub.f64 	%fd146, %fd127, %fd129;
	add.f64 	%fd147, %fd146, %fd146;
	neg.f64 	%fd148, %fd129;
	fma.rn.f64 	%fd149, %fd148, %fd127, %fd147;
	mul.f64 	%fd150, %fd126, %fd149;
	mul.f64 	%fd151, %fd130, %fd145;
	fma.rn.f64 	%fd152, %fd151, %fd129, %fd150;
	xor.b32  	%r320, %r333, -2147483648;
	mov.u32 	%r321, -2147483648;
	mov.u32 	%r322, 1127219200;
	mov.b64 	%fd153, {%r320, %r322};
	mov.b64 	%fd154, {%r321, %r322};
	sub.f64 	%fd155, %fd153, %fd154;
	mov.f64 	%fd156, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd157, %fd155, %fd156, %fd129;
	neg.f64 	%fd158, %fd155;
	fma.rn.f64 	%fd159, %fd158, %fd156, %fd157;
	sub.f64 	%fd160, %fd159, %fd129;
	sub.f64 	%fd161, %fd152, %fd160;
	mov.f64 	%fd162, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd163, %fd155, %fd162, %fd161;
	add.f64 	%fd245, %fd157, %fd163;
	bra.uni 	$L__BB7_33;

$L__BB7_29:
	mov.f64 	%fd118, 0d7FF0000000000000;
	fma.rn.f64 	%fd119, %fd243, %fd118, %fd118;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r313}, %fd243;
	}
	mov.b32 	%f2, %r313;
	setp.eq.f32 	%p18, %f2, 0f00000000;
	selp.f64 	%fd245, 0dFFF0000000000000, %fd119, %p18;

$L__BB7_33:
	fma.rn.f64 	%fd164, %fd24, %fd47, 0d0000000000000000;
	mov.f64 	%fd165, 0d0000000000000000;
	sub.f64 	%fd166, %fd165, %fd164;
	div.rn.f64 	%fd167, %fd166, %fd7;
	add.f64 	%fd168, %fd167, 0d0000000000000000;
	mul.f64 	%fd169, %fd19, %fd166;
	div.rn.f64 	%fd170, %fd169, %fd7;
	sub.f64 	%fd171, %fd165, %fd170;
	fma.rn.f64 	%fd172, %fd9, %fd168, 0d0000000000000000;
	div.rn.f64 	%fd173, %fd172, %fd46;
	add.f64 	%fd174, %fd173, 0d0000000000000000;
	add.f64 	%fd175, %fd173, %fd174;
	div.rn.f64 	%fd176, %fd164, %fd46;
	add.f64 	%fd177, %fd176, 0d0000000000000000;
	add.f64 	%fd178, %fd177, %fd177;
	sub.f64 	%fd179, %fd165, %fd178;
	fma.rn.f64 	%fd180, %fd179, %fd245, 0d0000000000000000;
	fma.rn.f64 	%fd181, %fd9, %fd179, 0d0000000000000000;
	rcp.rn.f64 	%fd182, %fd10;
	fma.rn.f64 	%fd183, %fd182, %fd181, 0d0000000000000000;
	div.rn.f64 	%fd184, %fd183, %fd46;
	add.f64 	%fd185, %fd171, %fd184;
	div.rn.f64 	%fd186, %fd180, %fd46;
	add.f64 	%fd187, %fd175, %fd186;
	add.f64 	%fd246, %fd185, %fd187;
	fma.rn.f64 	%fd34, %fd23, %fd46, 0d0000000000000000;
	setp.eq.s64 	%p20, %rd90, 0;
	@%p20 bra 	$L__BB7_35;

	mul.lo.s64 	%rd122, %rd56, %rd44;
	add.s64 	%rd121, %rd90, %rd122;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd121],%fd34; }

	// end inline asm
	bra.uni 	$L__BB7_37;

$L__BB7_35:
	setp.eq.s64 	%p21, %rd79, 0;
	@%p21 bra 	$L__BB7_37;

	mul.lo.s64 	%rd159, %rd56, %rd154;
	add.s64 	%rd123, %rd79, %rd159;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd123],%fd34; }

	// end inline asm

$L__BB7_37:
	add.f64 	%fd192, %fd246, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd4, %fd192, 0d0000000000000000;
	fma.rn.f64 	%fd37, %fd5, %fd192, 0d0000000000000000;
	fma.rn.f64 	%fd38, %fd6, %fd192, 0d0000000000000000;
	fma.rn.f64 	%fd39, %fd1, %fd192, 0d0000000000000000;
	fma.rn.f64 	%fd40, %fd2, %fd192, 0d0000000000000000;
	fma.rn.f64 	%fd41, %fd3, %fd192, 0d0000000000000000;
	setp.eq.s64 	%p22, %rd86, 0;
	@%p22 bra 	$L__BB7_39;

	mul.lo.s64 	%rd127, %rd61, %rd37;
	add.s64 	%rd124, %rd86, %rd127;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd124],%fd36; }

	// end inline asm
	add.s64 	%rd125, %rd124, 8;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd125],%fd37; }

	// end inline asm
	add.s64 	%rd126, %rd124, 16;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd126],%fd38; }

	// end inline asm
	bra.uni 	$L__BB7_41;

$L__BB7_39:
	setp.eq.s64 	%p23, %rd73, 0;
	@%p23 bra 	$L__BB7_41;

	mul.lo.s64 	%rd158, %rd61, %rd150;
	add.s64 	%rd128, %rd73, %rd158;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd128],%fd36; }

	// end inline asm
	add.s64 	%rd129, %rd128, 8;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd129],%fd37; }

	// end inline asm
	add.s64 	%rd130, %rd128, 16;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd130],%fd38; }

	// end inline asm

$L__BB7_41:
	setp.eq.s64 	%p24, %rd94, 0;
	mov.f64 	%fd205, 0d0000000000000000;
	sub.f64 	%fd206, %fd205, %fd36;
	add.f64 	%fd42, %fd206, 0d0000000000000000;
	sub.f64 	%fd207, %fd205, %fd37;
	add.f64 	%fd43, %fd207, 0d0000000000000000;
	sub.f64 	%fd208, %fd205, %fd38;
	add.f64 	%fd44, %fd208, 0d0000000000000000;
	@%p24 bra 	$L__BB7_43;

	mul.lo.s64 	%rd134, %rd58, %rd38;
	add.s64 	%rd131, %rd94, %rd134;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd131],%fd42; }

	// end inline asm
	add.s64 	%rd132, %rd131, 8;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd132],%fd43; }

	// end inline asm
	add.s64 	%rd133, %rd131, 16;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd133],%fd44; }

	// end inline asm
	bra.uni 	$L__BB7_45;

$L__BB7_43:
	setp.eq.s64 	%p25, %rd83, 0;
	@%p25 bra 	$L__BB7_45;

	mul.lo.s64 	%rd157, %rd58, %rd151;
	add.s64 	%rd135, %rd83, %rd157;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd135],%fd42; }

	// end inline asm
	add.s64 	%rd136, %rd135, 8;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd136],%fd43; }

	// end inline asm
	add.s64 	%rd137, %rd135, 16;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd137],%fd44; }

	// end inline asm

$L__BB7_45:
	setp.eq.s64 	%p26, %rd92, 0;
	@%p26 bra 	$L__BB7_47;

	mul.lo.s64 	%rd141, %rd58, %rd40;
	add.s64 	%rd138, %rd92, %rd141;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd138],%fd39; }

	// end inline asm
	add.s64 	%rd139, %rd138, 8;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd139],%fd40; }

	// end inline asm
	add.s64 	%rd140, %rd138, 16;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd140],%fd41; }

	// end inline asm
	bra.uni 	$L__BB7_49;

$L__BB7_47:
	setp.eq.s64 	%p27, %rd81, 0;
	@%p27 bra 	$L__BB7_49;

	mul.lo.s64 	%rd156, %rd58, %rd152;
	add.s64 	%rd142, %rd81, %rd156;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd142],%fd39; }

	// end inline asm
	add.s64 	%rd143, %rd142, 8;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd143],%fd40; }

	// end inline asm
	add.s64 	%rd144, %rd142, 16;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd144],%fd41; }

	// end inline asm

$L__BB7_49:
	setp.eq.s64 	%p28, %rd88, 0;
	mov.f64 	%fd233, 0d0000000000000000;
	sub.f64 	%fd234, %fd233, %fd246;
	add.f64 	%fd45, %fd234, 0d0000000000000000;
	@%p28 bra 	$L__BB7_51;

	mul.lo.s64 	%rd146, %rd56, %rd43;
	add.s64 	%rd145, %rd88, %rd146;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd145],%fd45; }

	// end inline asm
	bra.uni 	$L__BB7_53;

$L__BB7_51:
	setp.eq.s64 	%p29, %rd77, 0;
	@%p29 bra 	$L__BB7_53;

	mul.lo.s64 	%rd155, %rd56, %rd153;
	add.s64 	%rd147, %rd77, %rd155;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd147],%fd45; }

	// end inline asm

$L__BB7_53:
	ld.param.u64 	%rd149, [initialize_friction_hs_cuda_kernel_backward_param_0+24];
	add.s64 	%rd161, %rd161, %rd42;
	setp.lt.u64 	%p30, %rd161, %rd149;
	@%p30 bra 	$L__BB7_2;

$L__BB7_54:
	ret;

}
	// .globl	add_x_to_soft_x_cuda_kernel_forward
.visible .entry add_x_to_soft_x_cuda_kernel_forward(
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_forward_param_2[56],
	.param .u32 add_x_to_soft_x_cuda_kernel_forward_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<62>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<49>;


	ld.param.v2.u32 	{%r26, %r27}, [add_x_to_soft_x_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r28, %r29}, [add_x_to_soft_x_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r34, %r35}, [add_x_to_soft_x_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r42, %r43}, [add_x_to_soft_x_cuda_kernel_forward_param_2+32];
	ld.param.u32 	%r25, [add_x_to_soft_x_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd26, [add_x_to_soft_x_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd24, [add_x_to_soft_x_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd23, [add_x_to_soft_x_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [add_x_to_soft_x_cuda_kernel_forward_param_0+16];
	mov.u32 	%r46, %ntid.x;
	cvt.u64.u32 	%rd1, %r46;
	mov.u32 	%r47, %ctaid.x;
	mul.wide.u32 	%rd28, %r46, %r47;
	mov.u32 	%r48, %tid.x;
	cvt.u64.u32 	%rd29, %r48;
	add.s64 	%rd45, %rd28, %rd29;
	setp.ge.u64 	%p1, %rd45, %rd23;
	@%p1 bra 	$L__BB8_15;

	cvta.to.global.u64 	%rd5, %rd24;
	cvt.s64.s32 	%rd6, %r29;
	cvt.s64.s32 	%rd7, %r28;
	cvt.s64.s32 	%rd8, %r27;
	cvt.s64.s32 	%rd9, %r34;
	cvt.s64.s32 	%rd10, %r42;
	mov.u32 	%r49, %nctaid.x;
	cvt.u64.u32 	%rd30, %r49;
	mul.lo.s64 	%rd11, %rd1, %rd30;

$L__BB8_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd46, %rd45;
	@%p2 bra 	$L__BB8_6;

	or.b64  	%rd31, %rd45, %rd6;
	and.b64  	%rd32, %rd31, -4294967296;
	setp.eq.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB8_5;

	div.u64 	%rd46, %rd45, %rd6;
	bra.uni 	$L__BB8_6;

$L__BB8_5:
	cvt.u32.u64 	%r50, %rd6;
	cvt.u32.u64 	%r51, %rd45;
	div.u32 	%r52, %r51, %r50;
	cvt.u64.u32 	%rd46, %r52;

$L__BB8_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB8_10;

	or.b64  	%rd33, %rd46, %rd7;
	and.b64  	%rd34, %rd33, -4294967296;
	setp.eq.s64 	%p5, %rd34, 0;
	@%p5 bra 	$L__BB8_9;

	div.u64 	%rd46, %rd46, %rd7;
	bra.uni 	$L__BB8_10;

$L__BB8_9:
	cvt.u32.u64 	%r53, %rd7;
	cvt.u32.u64 	%r54, %rd46;
	div.u32 	%r55, %r54, %r53;
	cvt.u64.u32 	%rd46, %r55;

$L__BB8_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB8_14;

	or.b64  	%rd35, %rd46, %rd8;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p7, %rd36, 0;
	@%p7 bra 	$L__BB8_13;

	div.u64 	%rd46, %rd46, %rd8;
	bra.uni 	$L__BB8_14;

$L__BB8_13:
	cvt.u32.u64 	%r56, %rd8;
	cvt.u32.u64 	%r57, %rd46;
	div.u32 	%r58, %r57, %r56;
	cvt.u64.u32 	%rd46, %r58;

$L__BB8_14:
	cvt.u32.u64 	%r59, %rd46;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r60, %r59, 0, %p8;
	add.s32 	%r61, %r60, %r25;
	cvt.s64.s32 	%rd40, %r61;
	mul.lo.s64 	%rd41, %rd40, %rd9;
	add.s64 	%rd42, %rd5, %rd41;
	ld.global.f64 	%fd2, [%rd42];
	ld.global.f64 	%fd4, [%rd42+8];
	ld.global.f64 	%fd6, [%rd42+16];
	cvt.s64.s32 	%rd43, %r60;
	mul.lo.s64 	%rd44, %rd43, %rd10;
	add.s64 	%rd37, %rd26, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd1,[%rd37],%fd2; }

	// end inline asm
	add.s64 	%rd38, %rd37, 8;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd38],%fd4; }

	// end inline asm
	add.s64 	%rd39, %rd37, 16;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd39],%fd6; }

	// end inline asm
	add.s64 	%rd45, %rd45, %rd11;
	setp.lt.u64 	%p9, %rd45, %rd23;
	@%p9 bra 	$L__BB8_2;

$L__BB8_15:
	ret;

}
	// .globl	add_x_to_soft_x_cuda_kernel_backward
.visible .entry add_x_to_soft_x_cuda_kernel_backward(
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_2[56],
	.param .u32 add_x_to_soft_x_cuda_kernel_backward_param_3,
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 add_x_to_soft_x_cuda_kernel_backward_param_5[56],
	.param .u32 add_x_to_soft_x_cuda_kernel_backward_param_6
)
{
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<33>;
	.reg .b32 	%r<96>;
	.reg .f64 	%fd<34>;
	.reg .b64 	%rd<67>;


	ld.param.v2.u32 	{%r46, %r47}, [add_x_to_soft_x_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [add_x_to_soft_x_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [add_x_to_soft_x_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r62, %r63}, [add_x_to_soft_x_cuda_kernel_backward_param_2+32];
	ld.param.u32 	%r27, [add_x_to_soft_x_cuda_kernel_backward_param_3];
	ld.param.v2.u32 	{%r70, %r71}, [add_x_to_soft_x_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r78, %r79}, [add_x_to_soft_x_cuda_kernel_backward_param_5+32];
	ld.param.u64 	%rd36, [add_x_to_soft_x_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd34, [add_x_to_soft_x_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd33, [add_x_to_soft_x_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd31, [add_x_to_soft_x_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [add_x_to_soft_x_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r8, [add_x_to_soft_x_cuda_kernel_backward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd38, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd39, %r84;
	add.s64 	%rd63, %rd38, %rd39;
	setp.ge.u64 	%p1, %rd63, %rd29;
	@%p1 bra 	$L__BB9_23;

	cvta.to.global.u64 	%rd8, %rd36;
	cvta.to.global.u64 	%rd9, %rd33;
	cvt.s64.s32 	%rd10, %r49;
	cvt.s64.s32 	%rd11, %r48;
	cvt.s64.s32 	%rd12, %r47;
	cvt.s64.s32 	%rd13, %r78;
	cvt.s64.s32 	%rd14, %r62;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd40, %r85;
	mul.lo.s64 	%rd15, %rd1, %rd40;
	cvt.s64.s32 	%rd16, %r70;
	cvt.s64.s32 	%rd17, %r54;

$L__BB9_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd64, %rd63;
	@%p2 bra 	$L__BB9_6;

	or.b64  	%rd41, %rd63, %rd10;
	and.b64  	%rd42, %rd41, -4294967296;
	setp.eq.s64 	%p3, %rd42, 0;
	@%p3 bra 	$L__BB9_5;

	div.u64 	%rd64, %rd63, %rd10;
	bra.uni 	$L__BB9_6;

$L__BB9_5:
	cvt.u32.u64 	%r86, %rd10;
	cvt.u32.u64 	%r87, %rd63;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd64, %r88;

$L__BB9_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB9_10;

	or.b64  	%rd43, %rd64, %rd11;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p5, %rd44, 0;
	@%p5 bra 	$L__BB9_9;

	div.u64 	%rd64, %rd64, %rd11;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r89, %rd11;
	cvt.u32.u64 	%r90, %rd64;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd64, %r91;

$L__BB9_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB9_14;

	or.b64  	%rd45, %rd64, %rd12;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p7, %rd46, 0;
	@%p7 bra 	$L__BB9_13;

	div.u64 	%rd64, %rd64, %rd12;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r92, %rd12;
	cvt.u32.u64 	%r93, %rd64;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd64, %r94;

$L__BB9_14:
	cvt.u32.u64 	%r95, %rd64;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r95, 0, %p8;
	add.s32 	%r3, %r2, %r27;
	setp.eq.s64 	%p9, %rd36, 0;
	@%p9 bra 	$L__BB9_16;

	cvt.s64.s32 	%rd47, %r2;
	mul.lo.s64 	%rd48, %rd47, %rd13;
	add.s64 	%rd49, %rd8, %rd48;
	ld.global.f64 	%fd10, [%rd49];
	add.f64 	%fd33, %fd10, 0d0000000000000000;
	ld.global.f64 	%fd11, [%rd49+8];
	add.f64 	%fd32, %fd11, 0d0000000000000000;
	ld.global.f64 	%fd12, [%rd49+16];
	add.f64 	%fd31, %fd12, 0d0000000000000000;
	bra.uni 	$L__BB9_18;

$L__BB9_16:
	setp.eq.s64 	%p10, %rd33, 0;
	mov.f64 	%fd31, 0d0000000000000000;
	mov.f64 	%fd32, %fd31;
	mov.f64 	%fd33, %fd31;
	@%p10 bra 	$L__BB9_18;

	cvt.s64.s32 	%rd50, %r2;
	mul.lo.s64 	%rd51, %rd50, %rd14;
	add.s64 	%rd52, %rd9, %rd51;
	ld.global.f64 	%fd16, [%rd52];
	add.f64 	%fd33, %fd16, 0d0000000000000000;
	ld.global.f64 	%fd17, [%rd52+8];
	add.f64 	%fd32, %fd17, 0d0000000000000000;
	ld.global.f64 	%fd18, [%rd52+16];
	add.f64 	%fd31, %fd18, 0d0000000000000000;

$L__BB9_18:
	setp.eq.s64 	%p11, %rd34, 0;
	@%p11 bra 	$L__BB9_20;

	cvt.s64.s32 	%rd56, %r3;
	mul.lo.s64 	%rd57, %rd56, %rd16;
	add.s64 	%rd53, %rd34, %rd57;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd53],%fd33; }

	// end inline asm
	add.s64 	%rd54, %rd53, 8;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd54],%fd32; }

	// end inline asm
	add.s64 	%rd55, %rd53, 16;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd55],%fd31; }

	// end inline asm
	bra.uni 	$L__BB9_22;

$L__BB9_20:
	setp.eq.s64 	%p12, %rd31, 0;
	@%p12 bra 	$L__BB9_22;

	cvt.s64.s32 	%rd61, %r3;
	mul.lo.s64 	%rd62, %rd61, %rd17;
	add.s64 	%rd58, %rd31, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd58],%fd33; }

	// end inline asm
	add.s64 	%rd59, %rd58, 8;
	// begin inline asm
	{ atom.add.f64 %fd27,[%rd59],%fd32; }

	// end inline asm
	add.s64 	%rd60, %rd58, 16;
	// begin inline asm
	{ atom.add.f64 %fd29,[%rd60],%fd31; }

	// end inline asm

$L__BB9_22:
	add.s64 	%rd63, %rd63, %rd15;
	setp.lt.u64 	%p13, %rd63, %rd29;
	@%p13 bra 	$L__BB9_2;

$L__BB9_23:
	ret;

}
	// .globl	val_IPC_hs_cuda_kernel_forward
.visible .entry val_IPC_hs_cuda_kernel_forward(
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_9[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_10[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_11[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_forward_param_12[56],
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_13,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_14,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_15,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_16,
	.param .f64 val_IPC_hs_cuda_kernel_forward_param_17,
	.param .u32 val_IPC_hs_cuda_kernel_forward_param_18
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<97>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<257>;
	.reg .f64 	%fd<155>;
	.reg .b64 	%rd<113>;


	ld.param.v2.u32 	{%r126, %r127}, [val_IPC_hs_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r128, %r129}, [val_IPC_hs_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r134, %r135}, [val_IPC_hs_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r142, %r143}, [val_IPC_hs_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r150, %r151}, [val_IPC_hs_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r158, %r159}, [val_IPC_hs_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r166, %r167}, [val_IPC_hs_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r174, %r175}, [val_IPC_hs_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r182, %r183}, [val_IPC_hs_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r190, %r191}, [val_IPC_hs_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r198, %r199}, [val_IPC_hs_cuda_kernel_forward_param_9+32];
	ld.param.v2.u32 	{%r206, %r207}, [val_IPC_hs_cuda_kernel_forward_param_10+32];
	ld.param.v2.u32 	{%r214, %r215}, [val_IPC_hs_cuda_kernel_forward_param_11+32];
	ld.param.v2.u32 	{%r222, %r223}, [val_IPC_hs_cuda_kernel_forward_param_12+32];
	ld.param.f64 	%fd28, [val_IPC_hs_cuda_kernel_forward_param_13];
	ld.param.f64 	%fd29, [val_IPC_hs_cuda_kernel_forward_param_14];
	ld.param.f64 	%fd30, [val_IPC_hs_cuda_kernel_forward_param_15];
	ld.param.f64 	%fd31, [val_IPC_hs_cuda_kernel_forward_param_16];
	ld.param.f64 	%fd32, [val_IPC_hs_cuda_kernel_forward_param_17];
	ld.param.u64 	%rd70, [val_IPC_hs_cuda_kernel_forward_param_12];
	ld.param.u64 	%rd68, [val_IPC_hs_cuda_kernel_forward_param_11];
	ld.param.u64 	%rd66, [val_IPC_hs_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd64, [val_IPC_hs_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd62, [val_IPC_hs_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd60, [val_IPC_hs_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd58, [val_IPC_hs_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd56, [val_IPC_hs_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd54, [val_IPC_hs_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd52, [val_IPC_hs_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd50, [val_IPC_hs_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd48, [val_IPC_hs_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd47, [val_IPC_hs_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r16, [val_IPC_hs_cuda_kernel_forward_param_0+16];
	mov.u32 	%r226, %ntid.x;
	cvt.u64.u32 	%rd1, %r226;
	mov.u32 	%r227, %ctaid.x;
	mul.wide.u32 	%rd72, %r226, %r227;
	mov.u32 	%r228, %tid.x;
	cvt.u64.u32 	%rd73, %r228;
	add.s64 	%rd109, %rd72, %rd73;
	setp.ge.u64 	%p1, %rd109, %rd47;
	@%p1 bra 	$L__BB10_29;

	cvta.to.global.u64 	%rd5, %rd70;
	cvta.to.global.u64 	%rd6, %rd68;
	cvta.to.global.u64 	%rd7, %rd66;
	cvta.to.global.u64 	%rd8, %rd64;
	cvta.to.global.u64 	%rd9, %rd62;
	cvta.to.global.u64 	%rd10, %rd60;
	cvta.to.global.u64 	%rd11, %rd58;
	cvta.to.global.u64 	%rd12, %rd56;
	cvta.to.global.u64 	%rd13, %rd54;
	cvta.to.global.u64 	%rd14, %rd52;
	cvta.to.global.u64 	%rd15, %rd50;
	cvt.s64.s32 	%rd16, %r129;
	cvt.s64.s32 	%rd17, %r128;
	cvt.s64.s32 	%rd18, %r127;
	cvt.s64.s32 	%rd19, %r150;
	cvt.s64.s32 	%rd20, %r190;
	cvt.s64.s32 	%rd21, %r174;
	cvt.s64.s32 	%rd22, %r158;
	cvt.s64.s32 	%rd23, %r206;
	cvt.s64.s32 	%rd24, %r214;
	cvt.s64.s32 	%rd25, %r166;
	cvt.s64.s32 	%rd26, %r182;
	cvt.s64.s32 	%rd27, %r198;
	mov.u32 	%r229, %nctaid.x;
	cvt.u64.u32 	%rd74, %r229;
	mul.lo.s64 	%rd28, %rd1, %rd74;
	cvt.s64.s32 	%rd29, %r222;
	cvt.s64.s32 	%rd30, %r142;
	mov.f64 	%fd33, 0d0000000000000000;
	sub.f64 	%fd1, %fd33, %fd30;
	cvt.s64.s32 	%rd31, %r134;
	mul.f64 	%fd2, %fd29, %fd32;
	mul.f64 	%fd3, %fd2, %fd2;
	div.rn.f64 	%fd4, %fd2, 0d4008000000000000;

$L__BB10_2:
	setp.lt.s32 	%p2, %r16, 4;
	mov.u64 	%rd110, %rd109;
	@%p2 bra 	$L__BB10_6;

	or.b64  	%rd75, %rd109, %rd16;
	and.b64  	%rd76, %rd75, -4294967296;
	setp.eq.s64 	%p3, %rd76, 0;
	@%p3 bra 	$L__BB10_5;

	div.u64 	%rd110, %rd109, %rd16;
	bra.uni 	$L__BB10_6;

$L__BB10_5:
	cvt.u32.u64 	%r230, %rd16;
	cvt.u32.u64 	%r231, %rd109;
	div.u32 	%r232, %r231, %r230;
	cvt.u64.u32 	%rd110, %r232;

$L__BB10_6:
	setp.lt.s32 	%p4, %r16, 3;
	@%p4 bra 	$L__BB10_10;

	or.b64  	%rd77, %rd110, %rd17;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p5, %rd78, 0;
	@%p5 bra 	$L__BB10_9;

	div.u64 	%rd110, %rd110, %rd17;
	bra.uni 	$L__BB10_10;

$L__BB10_9:
	cvt.u32.u64 	%r233, %rd17;
	cvt.u32.u64 	%r234, %rd110;
	div.u32 	%r235, %r234, %r233;
	cvt.u64.u32 	%rd110, %r235;

$L__BB10_10:
	setp.lt.s32 	%p6, %r16, 2;
	@%p6 bra 	$L__BB10_14;

	or.b64  	%rd79, %rd110, %rd18;
	and.b64  	%rd80, %rd79, -4294967296;
	setp.eq.s64 	%p7, %rd80, 0;
	@%p7 bra 	$L__BB10_13;

	div.u64 	%rd110, %rd110, %rd18;
	bra.uni 	$L__BB10_14;

$L__BB10_13:
	cvt.u32.u64 	%r236, %rd18;
	cvt.u32.u64 	%r237, %rd110;
	div.u32 	%r238, %r237, %r236;
	cvt.u64.u32 	%rd110, %r238;

$L__BB10_14:
	ld.param.u32 	%r252, [val_IPC_hs_cuda_kernel_forward_param_18];
	cvt.s64.s32 	%rd81, %rd110;
	setp.gt.s32 	%p8, %r16, 0;
	selp.b64 	%rd42, %rd81, 0, %p8;
	mul.lo.s64 	%rd82, %rd42, %rd19;
	add.s64 	%rd83, %rd14, %rd82;
	ld.global.s32 	%rd43, [%rd83];
	mul.lo.s64 	%rd84, %rd43, %rd20;
	add.s64 	%rd85, %rd9, %rd84;
	mul.lo.s64 	%rd86, %rd43, %rd21;
	add.s64 	%rd87, %rd11, %rd86;
	mul.lo.s64 	%rd88, %rd42, %rd22;
	add.s64 	%rd89, %rd13, %rd88;
	ld.global.s32 	%rd44, [%rd89];
	mul.lo.s64 	%rd90, %rd44, %rd23;
	add.s64 	%rd91, %rd7, %rd90;
	mul.lo.s64 	%rd92, %rd44, %rd24;
	add.s64 	%rd93, %rd6, %rd92;
	ld.global.s32 	%rd45, [%rd87];
	mul.lo.s64 	%rd94, %rd45, %rd25;
	add.s64 	%rd95, %rd12, %rd94;
	ld.global.f64 	%fd5, [%rd95];
	ld.global.f64 	%fd34, [%rd93];
	sub.f64 	%fd35, %fd5, %fd34;
	ld.global.f64 	%fd6, [%rd95+8];
	ld.global.f64 	%fd36, [%rd93+8];
	sub.f64 	%fd37, %fd6, %fd36;
	ld.global.f64 	%fd7, [%rd95+16];
	ld.global.f64 	%fd38, [%rd93+16];
	sub.f64 	%fd39, %fd7, %fd38;
	ld.global.f64 	%fd8, [%rd91];
	ld.global.f64 	%fd9, [%rd91+8];
	mul.f64 	%fd40, %fd9, %fd37;
	fma.rn.f64 	%fd41, %fd8, %fd35, %fd40;
	ld.global.f64 	%fd10, [%rd91+16];
	fma.rn.f64 	%fd42, %fd10, %fd39, %fd41;
	ld.global.f64 	%fd43, [%rd85];
	sub.f64 	%fd11, %fd42, %fd43;
	setp.eq.s32 	%p9, %r252, 0;
	@%p9 bra 	$L__BB10_18;

	ld.param.f64 	%fd149, [val_IPC_hs_cuda_kernel_forward_param_17];
	ld.param.f64 	%fd148, [val_IPC_hs_cuda_kernel_forward_param_14];
	mul.lo.s64 	%rd96, %rd45, %rd26;
	add.s64 	%rd97, %rd10, %rd96;
	mul.lo.s64 	%rd98, %rd44, %rd29;
	add.s64 	%rd99, %rd5, %rd98;
	mul.lo.s64 	%rd100, %rd42, %rd30;
	add.s64 	%rd101, %rd15, %rd100;
	ld.global.f64 	%fd44, [%rd101];
	ld.global.f64 	%fd45, [%rd99];
	mul.f64 	%fd12, %fd45, %fd44;
	mul.f64 	%fd46, %fd8, %fd8;
	mov.f64 	%fd47, 0d3FF0000000000000;
	sub.f64 	%fd48, %fd47, %fd46;
	mul.f64 	%fd49, %fd8, %fd9;
	mov.f64 	%fd50, 0d0000000000000000;
	sub.f64 	%fd51, %fd50, %fd49;
	mul.f64 	%fd52, %fd8, %fd10;
	sub.f64 	%fd53, %fd50, %fd52;
	mul.f64 	%fd54, %fd9, %fd9;
	sub.f64 	%fd55, %fd47, %fd54;
	mul.f64 	%fd56, %fd9, %fd10;
	sub.f64 	%fd57, %fd50, %fd56;
	mul.f64 	%fd58, %fd10, %fd10;
	sub.f64 	%fd59, %fd47, %fd58;
	ld.global.f64 	%fd60, [%rd97];
	sub.f64 	%fd61, %fd5, %fd60;
	ld.global.f64 	%fd62, [%rd97+8];
	sub.f64 	%fd63, %fd6, %fd62;
	ld.global.f64 	%fd64, [%rd97+16];
	sub.f64 	%fd65, %fd7, %fd64;
	mul.f64 	%fd66, %fd51, %fd63;
	mul.f64 	%fd67, %fd55, %fd63;
	mul.f64 	%fd68, %fd57, %fd63;
	fma.rn.f64 	%fd69, %fd48, %fd61, %fd66;
	fma.rn.f64 	%fd70, %fd51, %fd61, %fd67;
	fma.rn.f64 	%fd71, %fd53, %fd61, %fd68;
	fma.rn.f64 	%fd72, %fd53, %fd65, %fd69;
	fma.rn.f64 	%fd73, %fd57, %fd65, %fd70;
	fma.rn.f64 	%fd74, %fd59, %fd65, %fd71;
	div.rn.f64 	%fd75, %fd72, %fd148;
	div.rn.f64 	%fd76, %fd73, %fd148;
	div.rn.f64 	%fd77, %fd74, %fd148;
	mul.f64 	%fd78, %fd76, %fd76;
	fma.rn.f64 	%fd79, %fd75, %fd75, %fd78;
	fma.rn.f64 	%fd80, %fd77, %fd77, %fd79;
	sqrt.rn.f64 	%fd81, %fd80;
	setp.ge.f64 	%p10, %fd81, %fd149;
	mul.f64 	%fd150, %fd81, %fd148;
	@%p10 bra 	$L__BB10_17;

	mul.f64 	%fd82, %fd150, %fd150;
	sub.f64 	%fd84, %fd50, %fd150;
	div.rn.f64 	%fd85, %fd84, 0d4008000000000000;
	add.f64 	%fd86, %fd2, %fd85;
	mul.f64 	%fd87, %fd82, %fd86;
	div.rn.f64 	%fd88, %fd87, %fd3;
	add.f64 	%fd150, %fd4, %fd88;

$L__BB10_17:
	mul.lo.s64 	%rd103, %rd45, %rd31;
	add.s64 	%rd102, %rd48, %rd103;
	mul.f64 	%fd91, %fd12, %fd31;
	mul.f64 	%fd90, %fd91, %fd150;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd102],%fd90; }

	// end inline asm
	bra.uni 	$L__BB10_28;

$L__BB10_18:
	mul.lo.s64 	%rd104, %rd43, %rd27;
	add.s64 	%rd105, %rd8, %rd104;
	ld.global.f64 	%fd16, [%rd105];
	setp.geu.f64 	%p11, %fd11, %fd28;
	@%p11 bra 	$L__BB10_27;

	div.rn.f64 	%fd151, %fd11, %fd28;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r253}, %fd151;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r254, %temp}, %fd151;
	}
	setp.gt.s32 	%p12, %r253, 1048575;
	mov.u32 	%r255, -1023;
	@%p12 bra 	$L__BB10_21;

	mul.f64 	%fd151, %fd151, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r253}, %fd151;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r254, %temp}, %fd151;
	}
	mov.u32 	%r255, -1077;

$L__BB10_21:
	add.s32 	%r241, %r253, -1;
	setp.lt.u32 	%p13, %r241, 2146435071;
	@%p13 bra 	$L__BB10_23;
	bra.uni 	$L__BB10_22;

$L__BB10_23:
	shr.u32 	%r243, %r253, 20;
	add.s32 	%r256, %r255, %r243;
	and.b32  	%r244, %r253, -2146435073;
	or.b32  	%r245, %r244, 1072693248;
	mov.b64 	%fd152, {%r254, %r245};
	setp.lt.s32 	%p15, %r245, 1073127583;
	@%p15 bra 	$L__BB10_25;

	{
	.reg .b32 %temp;
	mov.b64 	{%r246, %temp}, %fd152;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r247}, %fd152;
	}
	add.s32 	%r248, %r247, -1048576;
	mov.b64 	%fd152, {%r246, %r248};
	add.s32 	%r256, %r256, 1;

$L__BB10_25:
	add.f64 	%fd95, %fd152, 0d3FF0000000000000;
	mov.f64 	%fd96, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd97, %fd95;
	neg.f64 	%fd98, %fd95;
	fma.rn.f64 	%fd99, %fd98, %fd97, %fd96;
	fma.rn.f64 	%fd100, %fd99, %fd99, %fd99;
	fma.rn.f64 	%fd101, %fd100, %fd97, %fd97;
	add.f64 	%fd102, %fd152, 0dBFF0000000000000;
	mul.f64 	%fd103, %fd102, %fd101;
	fma.rn.f64 	%fd104, %fd102, %fd101, %fd103;
	mul.f64 	%fd105, %fd104, %fd104;
	mov.f64 	%fd106, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd107, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd108, %fd107, %fd105, %fd106;
	mov.f64 	%fd109, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd110, %fd108, %fd105, %fd109;
	mov.f64 	%fd111, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd112, %fd110, %fd105, %fd111;
	mov.f64 	%fd113, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd114, %fd112, %fd105, %fd113;
	mov.f64 	%fd115, 0d3F624924923BE72D;
	fma.rn.f64 	%fd116, %fd114, %fd105, %fd115;
	mov.f64 	%fd117, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd118, %fd116, %fd105, %fd117;
	mov.f64 	%fd119, 0d3FB5555555555554;
	fma.rn.f64 	%fd120, %fd118, %fd105, %fd119;
	sub.f64 	%fd121, %fd102, %fd104;
	add.f64 	%fd122, %fd121, %fd121;
	neg.f64 	%fd123, %fd104;
	fma.rn.f64 	%fd124, %fd123, %fd102, %fd122;
	mul.f64 	%fd125, %fd101, %fd124;
	mul.f64 	%fd126, %fd105, %fd120;
	fma.rn.f64 	%fd127, %fd126, %fd104, %fd125;
	xor.b32  	%r249, %r256, -2147483648;
	mov.u32 	%r250, -2147483648;
	mov.u32 	%r251, 1127219200;
	mov.b64 	%fd128, {%r249, %r251};
	mov.b64 	%fd129, {%r250, %r251};
	sub.f64 	%fd130, %fd128, %fd129;
	mov.f64 	%fd131, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd132, %fd130, %fd131, %fd104;
	neg.f64 	%fd133, %fd130;
	fma.rn.f64 	%fd134, %fd133, %fd131, %fd132;
	sub.f64 	%fd135, %fd134, %fd104;
	sub.f64 	%fd136, %fd127, %fd135;
	mov.f64 	%fd137, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd138, %fd130, %fd137, %fd136;
	add.f64 	%fd153, %fd132, %fd138;
	bra.uni 	$L__BB10_26;

$L__BB10_22:
	mov.f64 	%fd93, 0d7FF0000000000000;
	fma.rn.f64 	%fd94, %fd151, %fd93, %fd93;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r242}, %fd151;
	}
	mov.b32 	%f1, %r242;
	setp.eq.f32 	%p14, %f1, 0f00000000;
	selp.f64 	%fd153, 0dFFF0000000000000, %fd94, %p14;

$L__BB10_26:
	sub.f64 	%fd139, %fd11, %fd28;
	div.rn.f64 	%fd140, %fd139, %fd28;
	mul.f64 	%fd141, %fd1, %fd140;
	mul.f64 	%fd142, %fd140, %fd141;
	mul.f64 	%fd154, %fd142, %fd153;

$L__BB10_27:
	setp.lt.f64 	%p16, %fd11, %fd28;
	selp.f64 	%fd145, %fd154, 0d0000000000000000, %p16;
	mul.f64 	%fd146, %fd16, %fd28;
	mul.f64 	%fd147, %fd146, %fd145;
	mul.f64 	%fd144, %fd147, %fd31;
	mul.lo.s64 	%rd107, %rd45, %rd31;
	add.s64 	%rd106, %rd48, %rd107;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd106],%fd144; }

	// end inline asm

$L__BB10_28:
	ld.param.u64 	%rd108, [val_IPC_hs_cuda_kernel_forward_param_0+24];
	add.s64 	%rd109, %rd109, %rd28;
	setp.lt.u64 	%p17, %rd109, %rd108;
	@%p17 bra 	$L__BB10_2;

$L__BB10_29:
	ret;

}
	// .globl	val_IPC_hs_cuda_kernel_backward
.visible .entry val_IPC_hs_cuda_kernel_backward(
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_12[56],
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_13,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_14,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_15,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_16,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_17,
	.param .u32 val_IPC_hs_cuda_kernel_backward_param_18,
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_20[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_21[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_22[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_23[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_24[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_25[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_26[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_27[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_28[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_29[56],
	.param .align 8 .b8 val_IPC_hs_cuda_kernel_backward_param_30[56],
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_31,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_32,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_33,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_34,
	.param .f64 val_IPC_hs_cuda_kernel_backward_param_35,
	.param .u32 val_IPC_hs_cuda_kernel_backward_param_36
)
{
	.reg .pred 	%p<49>;
	.reg .b16 	%rs<169>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<437>;
	.reg .f64 	%fd<527>;
	.reg .b64 	%rd<213>;


	ld.param.v2.u32 	{%r217, %r218}, [val_IPC_hs_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r219, %r220}, [val_IPC_hs_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r225, %r226}, [val_IPC_hs_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r233, %r234}, [val_IPC_hs_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r241, %r242}, [val_IPC_hs_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r249, %r250}, [val_IPC_hs_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r257, %r258}, [val_IPC_hs_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r265, %r266}, [val_IPC_hs_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r273, %r274}, [val_IPC_hs_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r281, %r282}, [val_IPC_hs_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r289, %r290}, [val_IPC_hs_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r297, %r298}, [val_IPC_hs_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r305, %r306}, [val_IPC_hs_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r313, %r314}, [val_IPC_hs_cuda_kernel_backward_param_12+32];
	ld.param.f64 	%fd133, [val_IPC_hs_cuda_kernel_backward_param_13];
	ld.param.f64 	%fd134, [val_IPC_hs_cuda_kernel_backward_param_14];
	ld.param.f64 	%fd135, [val_IPC_hs_cuda_kernel_backward_param_15];
	ld.param.f64 	%fd136, [val_IPC_hs_cuda_kernel_backward_param_16];
	ld.param.f64 	%fd137, [val_IPC_hs_cuda_kernel_backward_param_17];
	ld.param.v2.u32 	{%r321, %r322}, [val_IPC_hs_cuda_kernel_backward_param_19+32];
	ld.param.v2.u32 	{%r329, %r330}, [val_IPC_hs_cuda_kernel_backward_param_20+32];
	ld.param.v2.u32 	{%r337, %r338}, [val_IPC_hs_cuda_kernel_backward_param_23+32];
	ld.param.v2.u32 	{%r345, %r346}, [val_IPC_hs_cuda_kernel_backward_param_25+32];
	ld.param.v2.u32 	{%r353, %r354}, [val_IPC_hs_cuda_kernel_backward_param_26+32];
	ld.param.v2.u32 	{%r361, %r362}, [val_IPC_hs_cuda_kernel_backward_param_27+32];
	ld.param.v2.u32 	{%r369, %r370}, [val_IPC_hs_cuda_kernel_backward_param_28+32];
	ld.param.v2.u32 	{%r377, %r378}, [val_IPC_hs_cuda_kernel_backward_param_29+32];
	ld.param.v2.u32 	{%r385, %r386}, [val_IPC_hs_cuda_kernel_backward_param_30+32];
	ld.param.u64 	%rd126, [val_IPC_hs_cuda_kernel_backward_param_30];
	ld.param.u64 	%rd124, [val_IPC_hs_cuda_kernel_backward_param_29];
	ld.param.u64 	%rd122, [val_IPC_hs_cuda_kernel_backward_param_28];
	ld.param.u64 	%rd120, [val_IPC_hs_cuda_kernel_backward_param_27];
	ld.param.u64 	%rd118, [val_IPC_hs_cuda_kernel_backward_param_26];
	ld.param.u64 	%rd116, [val_IPC_hs_cuda_kernel_backward_param_25];
	ld.param.u64 	%rd114, [val_IPC_hs_cuda_kernel_backward_param_23];
	ld.param.u64 	%rd112, [val_IPC_hs_cuda_kernel_backward_param_20];
	ld.param.u64 	%rd110, [val_IPC_hs_cuda_kernel_backward_param_19];
	ld.param.u64 	%rd109, [val_IPC_hs_cuda_kernel_backward_param_12+8];
	ld.param.u64 	%rd108, [val_IPC_hs_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd107, [val_IPC_hs_cuda_kernel_backward_param_11+8];
	ld.param.u64 	%rd106, [val_IPC_hs_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd105, [val_IPC_hs_cuda_kernel_backward_param_10+8];
	ld.param.u64 	%rd104, [val_IPC_hs_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd103, [val_IPC_hs_cuda_kernel_backward_param_9+8];
	ld.param.u64 	%rd102, [val_IPC_hs_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd101, [val_IPC_hs_cuda_kernel_backward_param_8+8];
	ld.param.u64 	%rd100, [val_IPC_hs_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd99, [val_IPC_hs_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd98, [val_IPC_hs_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd96, [val_IPC_hs_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd95, [val_IPC_hs_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd94, [val_IPC_hs_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd92, [val_IPC_hs_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd90, [val_IPC_hs_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd89, [val_IPC_hs_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd88, [val_IPC_hs_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd87, [val_IPC_hs_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd85, [val_IPC_hs_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r26, [val_IPC_hs_cuda_kernel_backward_param_0+16];
	mov.u32 	%r389, %ntid.x;
	cvt.u64.u32 	%rd1, %r389;
	mov.u32 	%r390, %ctaid.x;
	mul.wide.u32 	%rd128, %r389, %r390;
	mov.u32 	%r391, %tid.x;
	cvt.u64.u32 	%rd129, %r391;
	add.s64 	%rd209, %rd128, %rd129;
	setp.ge.u64 	%p1, %rd209, %rd85;
	@%p1 bra 	$L__BB11_81;

	cvta.to.global.u64 	%rd24, %rd108;
	cvta.to.global.u64 	%rd25, %rd106;
	cvta.to.global.u64 	%rd26, %rd104;
	cvta.to.global.u64 	%rd27, %rd102;
	cvta.to.global.u64 	%rd28, %rd100;
	cvta.to.global.u64 	%rd29, %rd98;
	cvta.to.global.u64 	%rd30, %rd96;
	cvta.to.global.u64 	%rd31, %rd94;
	cvta.to.global.u64 	%rd32, %rd92;
	cvta.to.global.u64 	%rd33, %rd90;
	cvta.to.global.u64 	%rd34, %rd88;
	cvt.s64.s32 	%rd35, %r220;
	cvt.s64.s32 	%rd36, %r219;
	cvt.s64.s32 	%rd37, %r218;
	cvt.s64.s32 	%rd38, %r241;
	cvt.s64.s32 	%rd39, %r281;
	cvt.s64.s32 	%rd40, %r265;
	cvt.s64.s32 	%rd41, %r249;
	cvt.s64.s32 	%rd42, %r297;
	cvt.s64.s32 	%rd43, %r305;
	cvt.s64.s32 	%rd44, %r257;
	cvt.s64.s32 	%rd45, %r273;
	cvt.s64.s32 	%rd46, %r289;
	cvt.s64.s32 	%rd47, %r313;
	cvt.s64.s32 	%rd48, %r233;
	mov.f64 	%fd138, 0d0000000000000000;
	sub.f64 	%fd1, %fd138, %fd135;
	cvt.s64.s32 	%rd49, %r321;
	cvt.s64.s32 	%rd50, %r345;
	cvt.s64.s32 	%rd51, %r225;
	cvt.s64.s32 	%rd52, %r337;
	cvt.s64.s32 	%rd53, %r377;
	cvt.s64.s32 	%rd54, %r361;
	cvt.s64.s32 	%rd55, %r369;
	mov.u32 	%r392, %nctaid.x;
	cvt.u64.u32 	%rd130, %r392;
	mul.lo.s64 	%rd56, %rd1, %rd130;
	cvt.s64.s32 	%rd57, %r353;
	mul.f64 	%fd2, %fd134, %fd137;
	mul.f64 	%fd3, %fd2, %fd2;
	div.rn.f64 	%fd4, %fd2, 0d4008000000000000;
	cvt.s64.s32 	%rd58, %r329;
	cvt.s64.s32 	%rd59, %r385;

$L__BB11_2:
	setp.lt.s32 	%p2, %r26, 4;
	mov.u64 	%rd210, %rd209;
	@%p2 bra 	$L__BB11_6;

	or.b64  	%rd131, %rd209, %rd35;
	and.b64  	%rd132, %rd131, -4294967296;
	setp.eq.s64 	%p3, %rd132, 0;
	@%p3 bra 	$L__BB11_5;

	div.u64 	%rd210, %rd209, %rd35;
	bra.uni 	$L__BB11_6;

$L__BB11_5:
	cvt.u32.u64 	%r393, %rd35;
	cvt.u32.u64 	%r394, %rd209;
	div.u32 	%r395, %r394, %r393;
	cvt.u64.u32 	%rd210, %r395;

$L__BB11_6:
	setp.lt.s32 	%p4, %r26, 3;
	@%p4 bra 	$L__BB11_10;

	or.b64  	%rd133, %rd210, %rd36;
	and.b64  	%rd134, %rd133, -4294967296;
	setp.eq.s64 	%p5, %rd134, 0;
	@%p5 bra 	$L__BB11_9;

	div.u64 	%rd210, %rd210, %rd36;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r396, %rd36;
	cvt.u32.u64 	%r397, %rd210;
	div.u32 	%r398, %r397, %r396;
	cvt.u64.u32 	%rd210, %r398;

$L__BB11_10:
	setp.lt.s32 	%p6, %r26, 2;
	@%p6 bra 	$L__BB11_14;

	or.b64  	%rd135, %rd210, %rd37;
	and.b64  	%rd136, %rd135, -4294967296;
	setp.eq.s64 	%p7, %rd136, 0;
	@%p7 bra 	$L__BB11_13;

	div.u64 	%rd210, %rd210, %rd37;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r399, %rd37;
	cvt.u32.u64 	%r400, %rd210;
	div.u32 	%r401, %r400, %r399;
	cvt.u64.u32 	%rd210, %r401;

$L__BB11_14:
	cvt.s64.s32 	%rd200, %r281;
	cvt.s64.s32 	%rd199, %r297;
	cvt.s64.s32 	%rd198, %r305;
	cvt.s64.s32 	%rd197, %r257;
	ld.param.u32 	%r428, [val_IPC_hs_cuda_kernel_backward_param_18];
	cvta.to.global.u64 	%rd196, %rd87;
	cvta.to.global.u64 	%rd195, %rd110;
	cvt.s64.s32 	%rd137, %rd210;
	setp.gt.s32 	%p8, %r26, 0;
	selp.b64 	%rd70, %rd137, 0, %p8;
	mul.lo.s64 	%rd138, %rd70, %rd38;
	add.s64 	%rd139, %rd33, %rd138;
	ld.global.s32 	%rd71, [%rd139];
	mul.lo.s64 	%rd72, %rd71, %rd200;
	add.s64 	%rd140, %rd28, %rd72;
	mul.lo.s64 	%rd141, %rd71, %rd40;
	add.s64 	%rd142, %rd30, %rd141;
	mul.lo.s64 	%rd143, %rd70, %rd41;
	add.s64 	%rd144, %rd32, %rd143;
	ld.global.s32 	%rd73, [%rd144];
	mul.lo.s64 	%rd74, %rd73, %rd199;
	add.s64 	%rd145, %rd26, %rd74;
	mul.lo.s64 	%rd75, %rd73, %rd198;
	add.s64 	%rd146, %rd25, %rd75;
	ld.global.s32 	%rd76, [%rd142];
	mul.lo.s64 	%rd77, %rd76, %rd197;
	add.s64 	%rd147, %rd31, %rd77;
	mul.lo.s64 	%rd78, %rd76, %rd45;
	ld.global.f64 	%fd5, [%rd147];
	ld.global.f64 	%fd139, [%rd146];
	sub.f64 	%fd6, %fd5, %fd139;
	ld.global.f64 	%fd7, [%rd147+8];
	ld.global.f64 	%fd140, [%rd146+8];
	sub.f64 	%fd8, %fd7, %fd140;
	ld.global.f64 	%fd9, [%rd147+16];
	ld.global.f64 	%fd141, [%rd146+16];
	sub.f64 	%fd10, %fd9, %fd141;
	ld.global.f64 	%fd11, [%rd145];
	ld.global.f64 	%fd12, [%rd145+8];
	mul.f64 	%fd142, %fd12, %fd8;
	fma.rn.f64 	%fd143, %fd11, %fd6, %fd142;
	ld.global.f64 	%fd13, [%rd145+16];
	fma.rn.f64 	%fd144, %fd13, %fd10, %fd143;
	ld.global.f64 	%fd145, [%rd140];
	sub.f64 	%fd14, %fd144, %fd145;
	mul.lo.s64 	%rd148, %rd76, %rd49;
	add.s64 	%rd79, %rd195, %rd148;
	mul.lo.s64 	%rd149, %rd76, %rd51;
	add.s64 	%rd80, %rd196, %rd149;
	setp.eq.s32 	%p9, %r428, 0;
	@%p9 bra 	$L__BB11_33;

	cvt.s64.s32 	%rd201, %r313;
	add.s64 	%rd150, %rd29, %rd78;
	ld.global.f64 	%fd15, [%rd150];
	ld.global.f64 	%fd16, [%rd150+8];
	ld.global.f64 	%fd17, [%rd150+16];
	mul.lo.s64 	%rd81, %rd73, %rd201;
	add.s64 	%rd151, %rd24, %rd81;
	mul.lo.s64 	%rd82, %rd70, %rd48;
	add.s64 	%rd152, %rd34, %rd82;
	ld.global.f64 	%fd18, [%rd152];
	ld.global.f64 	%fd19, [%rd151];
	setp.eq.s64 	%p10, %rd110, 0;
	@%p10 bra 	$L__BB11_17;

	ld.global.f64 	%fd146, [%rd79];
	add.f64 	%fd498, %fd146, 0d0000000000000000;
	bra.uni 	$L__BB11_19;

$L__BB11_33:
	mul.lo.s64 	%rd83, %rd71, %rd46;
	add.s64 	%rd159, %rd27, %rd83;
	ld.global.f64 	%fd67, [%rd159];
	setp.geu.f64 	%p19, %fd14, %fd133;
	@%p19 bra 	$L__BB11_42;

	div.rn.f64 	%fd504, %fd14, %fd133;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r429}, %fd504;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r430, %temp}, %fd504;
	}
	setp.gt.s32 	%p20, %r429, 1048575;
	mov.u32 	%r431, -1023;
	@%p20 bra 	$L__BB11_36;

	mul.f64 	%fd504, %fd504, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r429}, %fd504;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r430, %temp}, %fd504;
	}
	mov.u32 	%r431, -1077;

$L__BB11_36:
	add.s32 	%r404, %r429, -1;
	setp.lt.u32 	%p21, %r404, 2146435071;
	@%p21 bra 	$L__BB11_38;
	bra.uni 	$L__BB11_37;

$L__BB11_38:
	shr.u32 	%r406, %r429, 20;
	add.s32 	%r432, %r431, %r406;
	and.b32  	%r407, %r429, -2146435073;
	or.b32  	%r408, %r407, 1072693248;
	mov.b64 	%fd505, {%r430, %r408};
	setp.lt.s32 	%p23, %r408, 1073127583;
	@%p23 bra 	$L__BB11_40;

	{
	.reg .b32 %temp;
	mov.b64 	{%r409, %temp}, %fd505;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r410}, %fd505;
	}
	add.s32 	%r411, %r410, -1048576;
	mov.b64 	%fd505, {%r409, %r411};
	add.s32 	%r432, %r432, 1;

$L__BB11_40:
	add.f64 	%fd262, %fd505, 0d3FF0000000000000;
	mov.f64 	%fd263, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd264, %fd262;
	neg.f64 	%fd265, %fd262;
	fma.rn.f64 	%fd266, %fd265, %fd264, %fd263;
	fma.rn.f64 	%fd267, %fd266, %fd266, %fd266;
	fma.rn.f64 	%fd268, %fd267, %fd264, %fd264;
	add.f64 	%fd269, %fd505, 0dBFF0000000000000;
	mul.f64 	%fd270, %fd269, %fd268;
	fma.rn.f64 	%fd271, %fd269, %fd268, %fd270;
	mul.f64 	%fd272, %fd271, %fd271;
	mov.f64 	%fd273, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd274, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd275, %fd274, %fd272, %fd273;
	mov.f64 	%fd276, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd277, %fd275, %fd272, %fd276;
	mov.f64 	%fd278, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd279, %fd277, %fd272, %fd278;
	mov.f64 	%fd280, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd281, %fd279, %fd272, %fd280;
	mov.f64 	%fd282, 0d3F624924923BE72D;
	fma.rn.f64 	%fd283, %fd281, %fd272, %fd282;
	mov.f64 	%fd284, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd285, %fd283, %fd272, %fd284;
	mov.f64 	%fd286, 0d3FB5555555555554;
	fma.rn.f64 	%fd287, %fd285, %fd272, %fd286;
	sub.f64 	%fd288, %fd269, %fd271;
	add.f64 	%fd289, %fd288, %fd288;
	neg.f64 	%fd290, %fd271;
	fma.rn.f64 	%fd291, %fd290, %fd269, %fd289;
	mul.f64 	%fd292, %fd268, %fd291;
	mul.f64 	%fd293, %fd272, %fd287;
	fma.rn.f64 	%fd294, %fd293, %fd271, %fd292;
	xor.b32  	%r412, %r432, -2147483648;
	mov.u32 	%r413, -2147483648;
	mov.u32 	%r414, 1127219200;
	mov.b64 	%fd295, {%r412, %r414};
	mov.b64 	%fd296, {%r413, %r414};
	sub.f64 	%fd297, %fd295, %fd296;
	mov.f64 	%fd298, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd299, %fd297, %fd298, %fd271;
	neg.f64 	%fd300, %fd297;
	fma.rn.f64 	%fd301, %fd300, %fd298, %fd299;
	sub.f64 	%fd302, %fd301, %fd271;
	sub.f64 	%fd303, %fd294, %fd302;
	mov.f64 	%fd304, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd305, %fd297, %fd304, %fd303;
	add.f64 	%fd506, %fd299, %fd305;
	bra.uni 	$L__BB11_41;

$L__BB11_17:
	setp.eq.s64 	%p11, %rd87, 0;
	mov.f64 	%fd498, 0d0000000000000000;
	@%p11 bra 	$L__BB11_19;

	ld.global.f64 	%fd148, [%rd80];
	add.f64 	%fd498, %fd148, 0d0000000000000000;

$L__BB11_19:
	ld.param.f64 	%fd494, [val_IPC_hs_cuda_kernel_backward_param_17];
	mul.f64 	%fd149, %fd11, %fd11;
	mov.f64 	%fd150, 0d3FF0000000000000;
	sub.f64 	%fd23, %fd150, %fd149;
	mul.f64 	%fd151, %fd11, %fd12;
	mov.f64 	%fd152, 0d0000000000000000;
	sub.f64 	%fd24, %fd152, %fd151;
	mul.f64 	%fd153, %fd11, %fd13;
	sub.f64 	%fd25, %fd152, %fd153;
	mul.f64 	%fd154, %fd12, %fd12;
	sub.f64 	%fd26, %fd150, %fd154;
	mul.f64 	%fd155, %fd12, %fd13;
	sub.f64 	%fd27, %fd152, %fd155;
	mul.f64 	%fd156, %fd13, %fd13;
	sub.f64 	%fd28, %fd150, %fd156;
	sub.f64 	%fd29, %fd5, %fd15;
	sub.f64 	%fd30, %fd7, %fd16;
	mul.f64 	%fd157, %fd24, %fd30;
	mul.f64 	%fd158, %fd26, %fd30;
	mul.f64 	%fd159, %fd27, %fd30;
	fma.rn.f64 	%fd160, %fd23, %fd29, %fd157;
	fma.rn.f64 	%fd161, %fd24, %fd29, %fd158;
	fma.rn.f64 	%fd162, %fd25, %fd29, %fd159;
	sub.f64 	%fd31, %fd9, %fd17;
	fma.rn.f64 	%fd163, %fd25, %fd31, %fd160;
	fma.rn.f64 	%fd164, %fd27, %fd31, %fd161;
	fma.rn.f64 	%fd165, %fd28, %fd31, %fd162;
	div.rn.f64 	%fd32, %fd163, %fd134;
	div.rn.f64 	%fd33, %fd164, %fd134;
	div.rn.f64 	%fd34, %fd165, %fd134;
	mul.f64 	%fd166, %fd33, %fd33;
	fma.rn.f64 	%fd167, %fd32, %fd32, %fd166;
	fma.rn.f64 	%fd168, %fd34, %fd34, %fd167;
	sqrt.rn.f64 	%fd35, %fd168;
	setp.ge.f64 	%p12, %fd35, %fd494;
	mul.f64 	%fd499, %fd35, %fd134;
	@%p12 bra 	$L__BB11_21;

	mov.f64 	%fd497, 0d0000000000000000;
	mul.f64 	%fd496, %fd35, %fd134;
	mul.f64 	%fd169, %fd496, %fd496;
	sub.f64 	%fd171, %fd497, %fd496;
	div.rn.f64 	%fd172, %fd171, 0d4008000000000000;
	add.f64 	%fd173, %fd2, %fd172;
	mul.f64 	%fd174, %fd169, %fd173;
	div.rn.f64 	%fd175, %fd174, %fd3;
	add.f64 	%fd499, %fd4, %fd175;

$L__BB11_21:
	add.f64 	%fd176, %fd498, 0d0000000000000000;
	fma.rn.f64 	%fd39, %fd176, %fd499, 0d0000000000000000;
	mul.f64 	%fd177, %fd19, %fd18;
	mul.f64 	%fd178, %fd177, %fd136;
	fma.rn.f64 	%fd500, %fd178, %fd176, 0d0000000000000000;
	@%p12 bra 	$L__BB11_23;

	mul.f64 	%fd495, %fd35, %fd134;
	mul.f64 	%fd179, %fd495, %fd495;
	mov.f64 	%fd180, 0d0000000000000000;
	sub.f64 	%fd181, %fd180, %fd495;
	div.rn.f64 	%fd182, %fd181, 0d4008000000000000;
	add.f64 	%fd183, %fd2, %fd182;
	div.rn.f64 	%fd184, %fd500, %fd3;
	add.f64 	%fd185, %fd184, 0d0000000000000000;
	fma.rn.f64 	%fd186, %fd185, %fd183, 0d0000000000000000;
	fma.rn.f64 	%fd187, %fd185, %fd179, 0d0000000000000000;
	div.rn.f64 	%fd188, %fd187, 0d4008000000000000;
	add.f64 	%fd189, %fd188, 0d0000000000000000;
	sub.f64 	%fd190, %fd180, %fd189;
	fma.rn.f64 	%fd191, %fd495, %fd186, %fd190;
	fma.rn.f64 	%fd500, %fd495, %fd186, %fd191;

$L__BB11_23:
	fma.rn.f64 	%fd43, %fd500, %fd134, 0d0000000000000000;
	mov.f64 	%fd194, 0d0000000000000000;
	setp.leu.f64 	%p14, %fd35, 0d0000000000000000;
	mov.f64 	%fd501, %fd194;
	mov.f64 	%fd502, %fd194;
	mov.f64 	%fd503, %fd194;
	@%p14 bra 	$L__BB11_25;

	div.rn.f64 	%fd195, %fd32, %fd35;
	div.rn.f64 	%fd196, %fd33, %fd35;
	div.rn.f64 	%fd197, %fd34, %fd35;
	fma.rn.f64 	%fd503, %fd195, %fd43, 0d0000000000000000;
	fma.rn.f64 	%fd502, %fd196, %fd43, 0d0000000000000000;
	fma.rn.f64 	%fd501, %fd197, %fd43, 0d0000000000000000;

$L__BB11_25:
	mov.f64 	%fd493, 0d3FF0000000000000;
	mul.f64 	%fd492, %fd13, %fd13;
	sub.f64 	%fd491, %fd493, %fd492;
	mul.f64 	%fd490, %fd11, %fd13;
	sub.f64 	%fd489, %fd152, %fd490;
	mul.f64 	%fd488, %fd11, %fd11;
	sub.f64 	%fd487, %fd493, %fd488;
	mul.f64 	%fd486, %fd12, %fd13;
	sub.f64 	%fd485, %fd152, %fd486;
	mul.f64 	%fd484, %fd12, %fd12;
	sub.f64 	%fd483, %fd493, %fd484;
	mul.f64 	%fd482, %fd11, %fd12;
	sub.f64 	%fd481, %fd152, %fd482;
	div.rn.f64 	%fd198, %fd503, %fd134;
	add.f64 	%fd199, %fd198, 0d0000000000000000;
	div.rn.f64 	%fd201, %fd502, %fd134;
	add.f64 	%fd202, %fd201, 0d0000000000000000;
	div.rn.f64 	%fd203, %fd501, %fd134;
	add.f64 	%fd204, %fd203, 0d0000000000000000;
	fma.rn.f64 	%fd205, %fd199, %fd29, 0d0000000000000000;
	fma.rn.f64 	%fd206, %fd199, %fd30, 0d0000000000000000;
	fma.rn.f64 	%fd207, %fd199, %fd31, 0d0000000000000000;
	fma.rn.f64 	%fd208, %fd202, %fd29, 0d0000000000000000;
	fma.rn.f64 	%fd209, %fd202, %fd30, 0d0000000000000000;
	fma.rn.f64 	%fd210, %fd202, %fd31, 0d0000000000000000;
	fma.rn.f64 	%fd211, %fd204, %fd29, 0d0000000000000000;
	fma.rn.f64 	%fd212, %fd204, %fd30, 0d0000000000000000;
	fma.rn.f64 	%fd213, %fd204, %fd31, 0d0000000000000000;
	mul.f64 	%fd214, %fd481, %fd202;
	mul.f64 	%fd215, %fd483, %fd202;
	mul.f64 	%fd216, %fd485, %fd202;
	fma.rn.f64 	%fd217, %fd487, %fd199, %fd214;
	fma.rn.f64 	%fd218, %fd481, %fd199, %fd215;
	fma.rn.f64 	%fd219, %fd489, %fd199, %fd216;
	fma.rn.f64 	%fd220, %fd485, %fd204, %fd218;
	fma.rn.f64 	%fd221, %fd491, %fd204, %fd219;
	add.f64 	%fd521, %fd220, 0d0000000000000000;
	fma.rn.f64 	%fd222, %fd489, %fd204, %fd217;
	add.f64 	%fd520, %fd222, 0d0000000000000000;
	add.f64 	%fd522, %fd221, 0d0000000000000000;
	sub.f64 	%fd223, %fd194, %fd205;
	sub.f64 	%fd224, %fd194, %fd208;
	sub.f64 	%fd225, %fd194, %fd211;
	sub.f64 	%fd226, %fd194, %fd206;
	sub.f64 	%fd227, %fd194, %fd209;
	sub.f64 	%fd228, %fd194, %fd212;
	sub.f64 	%fd229, %fd194, %fd207;
	sub.f64 	%fd230, %fd194, %fd210;
	sub.f64 	%fd231, %fd194, %fd213;
	mul.f64 	%fd232, %fd224, %fd12;
	mul.f64 	%fd233, %fd227, %fd12;
	mul.f64 	%fd234, %fd230, %fd12;
	fma.rn.f64 	%fd235, %fd223, %fd11, %fd232;
	fma.rn.f64 	%fd236, %fd226, %fd11, %fd233;
	fma.rn.f64 	%fd237, %fd229, %fd11, %fd234;
	fma.rn.f64 	%fd53, %fd228, %fd13, %fd236;
	fma.rn.f64 	%fd54, %fd231, %fd13, %fd237;
	fma.rn.f64 	%fd55, %fd225, %fd13, %fd235;
	mul.f64 	%fd238, %fd226, %fd12;
	mul.f64 	%fd239, %fd228, %fd12;
	fma.rn.f64 	%fd240, %fd223, %fd11, %fd238;
	fma.rn.f64 	%fd241, %fd224, %fd11, %fd233;
	fma.rn.f64 	%fd242, %fd225, %fd11, %fd239;
	fma.rn.f64 	%fd56, %fd229, %fd13, %fd240;
	fma.rn.f64 	%fd57, %fd230, %fd13, %fd241;
	fma.rn.f64 	%fd58, %fd231, %fd13, %fd242;
	fma.rn.f64 	%fd243, %fd39, %fd136, 0d0000000000000000;
	fma.rn.f64 	%fd59, %fd18, %fd243, 0d0000000000000000;
	fma.rn.f64 	%fd60, %fd19, %fd243, 0d0000000000000000;
	setp.eq.s64 	%p15, %rd112, 0;
	@%p15 bra 	$L__BB11_27;

	mul.lo.s64 	%rd154, %rd70, %rd58;
	add.s64 	%rd153, %rd112, %rd154;
	// begin inline asm
	{ atom.add.f64 %fd244,[%rd153],%fd60; }

	// end inline asm
	bra.uni 	$L__BB11_29;

$L__BB11_27:
	setp.eq.s64 	%p16, %rd89, 0;
	@%p16 bra 	$L__BB11_29;

	add.s64 	%rd155, %rd89, %rd82;
	// begin inline asm
	{ atom.add.f64 %fd246,[%rd155],%fd60; }

	// end inline asm

$L__BB11_29:
	mov.f64 	%fd526, 0d0000000000000000;
	sub.f64 	%fd518, %fd526, %fd521;
	sub.f64 	%fd517, %fd526, %fd520;
	sub.f64 	%fd519, %fd526, %fd522;
	add.f64 	%fd249, %fd55, 0d0000000000000000;
	add.f64 	%fd523, %fd249, %fd56;
	add.f64 	%fd250, %fd53, 0d0000000000000000;
	add.f64 	%fd524, %fd250, %fd57;
	add.f64 	%fd251, %fd54, 0d0000000000000000;
	add.f64 	%fd525, %fd251, %fd58;
	setp.eq.s64 	%p17, %rd126, 0;
	@%p17 bra 	$L__BB11_31;

	mul.lo.s64 	%rd157, %rd73, %rd59;
	add.s64 	%rd156, %rd126, %rd157;
	// begin inline asm
	{ atom.add.f64 %fd252,[%rd156],%fd59; }

	// end inline asm
	bra.uni 	$L__BB11_60;

$L__BB11_31:
	setp.eq.s64 	%p18, %rd109, 0;
	@%p18 bra 	$L__BB11_60;

	mul.lo.s64 	%rd206, %rd73, %rd201;
	add.s64 	%rd158, %rd109, %rd206;
	// begin inline asm
	{ atom.add.f64 %fd256,[%rd158],%fd59; }

	// end inline asm
	bra.uni 	$L__BB11_60;

$L__BB11_37:
	mov.f64 	%fd260, 0d7FF0000000000000;
	fma.rn.f64 	%fd261, %fd504, %fd260, %fd260;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r405}, %fd504;
	}
	mov.b32 	%f1, %r405;
	setp.eq.f32 	%p22, %f1, 0f00000000;
	selp.f64 	%fd506, 0dFFF0000000000000, %fd261, %p22;

$L__BB11_41:
	sub.f64 	%fd306, %fd14, %fd133;
	div.rn.f64 	%fd307, %fd306, %fd133;
	mul.f64 	%fd308, %fd1, %fd307;
	mul.f64 	%fd309, %fd307, %fd308;
	mul.f64 	%fd507, %fd309, %fd506;

$L__BB11_42:
	setp.eq.s64 	%p24, %rd110, 0;
	@%p24 bra 	$L__BB11_44;

	ld.global.f64 	%fd310, [%rd79];
	add.f64 	%fd508, %fd310, 0d0000000000000000;
	bra.uni 	$L__BB11_46;

$L__BB11_44:
	setp.eq.s64 	%p25, %rd87, 0;
	mov.f64 	%fd508, 0d0000000000000000;
	@%p25 bra 	$L__BB11_46;

	ld.global.f64 	%fd312, [%rd80];
	add.f64 	%fd508, %fd312, 0d0000000000000000;

$L__BB11_46:
	setp.geu.f64 	%p47, %fd14, %fd133;
	fma.rn.f64 	%fd314, %fd508, %fd136, 0d0000000000000000;
	setp.lt.f64 	%p27, %fd14, %fd133;
	selp.f64 	%fd315, %fd507, 0d0000000000000000, %p27;
	fma.rn.f64 	%fd82, %fd315, %fd314, 0d0000000000000000;
	mul.f64 	%fd316, %fd67, %fd133;
	fma.rn.f64 	%fd83, %fd316, %fd314, 0d0000000000000000;
	@%p47 bra 	$L__BB11_54;

	sub.f64 	%fd317, %fd14, %fd133;
	div.rn.f64 	%fd511, %fd317, %fd133;
	mul.f64 	%fd512, %fd1, %fd511;
	div.rn.f64 	%fd514, %fd14, %fd133;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r433}, %fd514;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r434, %temp}, %fd514;
	}
	setp.gt.s32 	%p28, %r433, 1048575;
	mov.u32 	%r435, -1023;
	mov.f64 	%fd509, %fd514;
	@%p28 bra 	$L__BB11_49;

	mul.f64 	%fd509, %fd514, 0d4350000000000000;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r433}, %fd509;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%r434, %temp}, %fd509;
	}
	mov.u32 	%r435, -1077;

$L__BB11_49:
	mul.f64 	%fd513, %fd511, %fd512;
	add.s32 	%r417, %r433, -1;
	setp.lt.u32 	%p29, %r417, 2146435071;
	@%p29 bra 	$L__BB11_51;
	bra.uni 	$L__BB11_50;

$L__BB11_51:
	shr.u32 	%r419, %r433, 20;
	add.s32 	%r436, %r435, %r419;
	and.b32  	%r420, %r433, -2146435073;
	or.b32  	%r421, %r420, 1072693248;
	mov.b64 	%fd510, {%r434, %r421};
	setp.lt.s32 	%p31, %r421, 1073127583;
	@%p31 bra 	$L__BB11_53;

	{
	.reg .b32 %temp;
	mov.b64 	{%r422, %temp}, %fd510;
	}
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r423}, %fd510;
	}
	add.s32 	%r424, %r423, -1048576;
	mov.b64 	%fd510, {%r422, %r424};
	add.s32 	%r436, %r436, 1;

$L__BB11_53:
	add.f64 	%fd320, %fd510, 0d3FF0000000000000;
	mov.f64 	%fd321, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd322, %fd320;
	neg.f64 	%fd323, %fd320;
	fma.rn.f64 	%fd324, %fd323, %fd322, %fd321;
	fma.rn.f64 	%fd325, %fd324, %fd324, %fd324;
	fma.rn.f64 	%fd326, %fd325, %fd322, %fd322;
	add.f64 	%fd327, %fd510, 0dBFF0000000000000;
	mul.f64 	%fd328, %fd327, %fd326;
	fma.rn.f64 	%fd329, %fd327, %fd326, %fd328;
	mul.f64 	%fd330, %fd329, %fd329;
	mov.f64 	%fd331, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd332, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd333, %fd332, %fd330, %fd331;
	mov.f64 	%fd334, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd335, %fd333, %fd330, %fd334;
	mov.f64 	%fd336, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd337, %fd335, %fd330, %fd336;
	mov.f64 	%fd338, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd339, %fd337, %fd330, %fd338;
	mov.f64 	%fd340, 0d3F624924923BE72D;
	fma.rn.f64 	%fd341, %fd339, %fd330, %fd340;
	mov.f64 	%fd342, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd343, %fd341, %fd330, %fd342;
	mov.f64 	%fd344, 0d3FB5555555555554;
	fma.rn.f64 	%fd345, %fd343, %fd330, %fd344;
	sub.f64 	%fd346, %fd327, %fd329;
	add.f64 	%fd347, %fd346, %fd346;
	neg.f64 	%fd348, %fd329;
	fma.rn.f64 	%fd349, %fd348, %fd327, %fd347;
	mul.f64 	%fd350, %fd326, %fd349;
	mul.f64 	%fd351, %fd330, %fd345;
	fma.rn.f64 	%fd352, %fd351, %fd329, %fd350;
	xor.b32  	%r425, %r436, -2147483648;
	mov.u32 	%r426, -2147483648;
	mov.u32 	%r427, 1127219200;
	mov.b64 	%fd353, {%r425, %r427};
	mov.b64 	%fd354, {%r426, %r427};
	sub.f64 	%fd355, %fd353, %fd354;
	mov.f64 	%fd356, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd357, %fd355, %fd356, %fd329;
	neg.f64 	%fd358, %fd355;
	fma.rn.f64 	%fd359, %fd358, %fd356, %fd357;
	sub.f64 	%fd360, %fd359, %fd329;
	sub.f64 	%fd361, %fd352, %fd360;
	mov.f64 	%fd362, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd363, %fd355, %fd362, %fd361;
	add.f64 	%fd515, %fd357, %fd363;
	bra.uni 	$L__BB11_54;

$L__BB11_50:
	mov.f64 	%fd318, 0d7FF0000000000000;
	fma.rn.f64 	%fd319, %fd509, %fd318, %fd318;
	{
	.reg .b32 %temp;
	mov.b64 	{%temp, %r418}, %fd509;
	}
	mov.b32 	%f2, %r418;
	setp.eq.f32 	%p30, %f2, 0f00000000;
	selp.f64 	%fd515, 0dFFF0000000000000, %fd319, %p30;

$L__BB11_54:
	setp.geu.f64 	%p48, %fd14, %fd133;
	selp.f64 	%fd100, %fd83, 0d0000000000000000, %p27;
	mov.f64 	%fd517, 0d0000000000000000;
	mov.f64 	%fd526, %fd517;
	@%p48 bra 	$L__BB11_56;

	fma.rn.f64 	%fd365, %fd100, %fd515, 0d0000000000000000;
	fma.rn.f64 	%fd366, %fd100, %fd513, 0d0000000000000000;
	rcp.rn.f64 	%fd367, %fd514;
	fma.rn.f64 	%fd368, %fd367, %fd366, 0d0000000000000000;
	div.rn.f64 	%fd369, %fd368, %fd133;
	add.f64 	%fd370, %fd369, 0d0000000000000000;
	fma.rn.f64 	%fd371, %fd511, %fd365, 0d0000000000000000;
	fma.rn.f64 	%fd372, %fd512, %fd365, 0d0000000000000000;
	div.rn.f64 	%fd373, %fd372, %fd133;
	add.f64 	%fd374, %fd373, 0d0000000000000000;
	fma.rn.f64 	%fd375, %fd1, %fd371, 0d0000000000000000;
	div.rn.f64 	%fd376, %fd375, %fd133;
	add.f64 	%fd377, %fd374, %fd376;
	add.f64 	%fd526, %fd370, %fd377;

$L__BB11_56:
	fma.rn.f64 	%fd103, %fd82, %fd133, 0d0000000000000000;
	setp.eq.s64 	%p34, %rd120, 0;
	@%p34 bra 	$L__BB11_58;

	mul.lo.s64 	%rd161, %rd71, %rd54;
	add.s64 	%rd160, %rd120, %rd161;
	// begin inline asm
	{ atom.add.f64 %fd378,[%rd160],%fd103; }

	// end inline asm
	mov.f64 	%fd518, %fd517;
	mov.f64 	%fd519, %fd517;
	mov.f64 	%fd520, %fd517;
	mov.f64 	%fd521, %fd517;
	mov.f64 	%fd522, %fd517;
	mov.f64 	%fd523, %fd517;
	mov.f64 	%fd524, %fd517;
	mov.f64 	%fd525, %fd517;
	bra.uni 	$L__BB11_60;

$L__BB11_58:
	setp.eq.s64 	%p35, %rd103, 0;
	mov.f64 	%fd518, %fd517;
	mov.f64 	%fd519, %fd517;
	mov.f64 	%fd520, %fd517;
	mov.f64 	%fd521, %fd517;
	mov.f64 	%fd522, %fd517;
	mov.f64 	%fd523, %fd517;
	mov.f64 	%fd524, %fd517;
	mov.f64 	%fd525, %fd517;
	@%p35 bra 	$L__BB11_60;

	cvt.s64.s32 	%rd208, %r289;
	mul.lo.s64 	%rd207, %rd71, %rd208;
	add.s64 	%rd162, %rd103, %rd207;
	// begin inline asm
	{ atom.add.f64 %fd398,[%rd162],%fd103; }

	// end inline asm
	mov.f64 	%fd518, %fd517;
	mov.f64 	%fd519, %fd517;
	mov.f64 	%fd520, %fd517;
	mov.f64 	%fd521, %fd517;
	mov.f64 	%fd522, %fd517;
	mov.f64 	%fd523, %fd517;
	mov.f64 	%fd524, %fd517;
	mov.f64 	%fd525, %fd517;

$L__BB11_60:
	add.f64 	%fd409, %fd526, 0d0000000000000000;
	fma.rn.f64 	%fd114, %fd11, %fd409, 0d0000000000000000;
	fma.rn.f64 	%fd115, %fd12, %fd409, 0d0000000000000000;
	fma.rn.f64 	%fd116, %fd13, %fd409, 0d0000000000000000;
	fma.rn.f64 	%fd117, %fd6, %fd409, %fd523;
	fma.rn.f64 	%fd118, %fd8, %fd409, %fd524;
	fma.rn.f64 	%fd119, %fd10, %fd409, %fd525;
	add.f64 	%fd120, %fd517, 0d0000000000000000;
	add.f64 	%fd121, %fd518, 0d0000000000000000;
	add.f64 	%fd122, %fd519, 0d0000000000000000;
	setp.eq.s64 	%p36, %rd116, 0;
	@%p36 bra 	$L__BB11_62;

	mul.lo.s64 	%rd166, %rd76, %rd50;
	add.s64 	%rd163, %rd116, %rd166;
	// begin inline asm
	{ atom.add.f64 %fd410,[%rd163],%fd120; }

	// end inline asm
	add.s64 	%rd164, %rd163, 8;
	// begin inline asm
	{ atom.add.f64 %fd412,[%rd164],%fd121; }

	// end inline asm
	add.s64 	%rd165, %rd163, 16;
	// begin inline asm
	{ atom.add.f64 %fd414,[%rd165],%fd122; }

	// end inline asm
	bra.uni 	$L__BB11_64;

$L__BB11_62:
	setp.eq.s64 	%p37, %rd99, 0;
	@%p37 bra 	$L__BB11_64;

	add.s64 	%rd167, %rd99, %rd78;
	// begin inline asm
	{ atom.add.f64 %fd416,[%rd167],%fd120; }

	// end inline asm
	add.s64 	%rd168, %rd167, 8;
	// begin inline asm
	{ atom.add.f64 %fd418,[%rd168],%fd121; }

	// end inline asm
	add.s64 	%rd169, %rd167, 16;
	// begin inline asm
	{ atom.add.f64 %fd420,[%rd169],%fd122; }

	// end inline asm

$L__BB11_64:
	setp.eq.s64 	%p38, %rd114, 0;
	add.f64 	%fd422, %fd520, %fd114;
	add.f64 	%fd123, %fd422, 0d0000000000000000;
	add.f64 	%fd423, %fd521, %fd115;
	add.f64 	%fd124, %fd423, 0d0000000000000000;
	add.f64 	%fd424, %fd522, %fd116;
	add.f64 	%fd125, %fd424, 0d0000000000000000;
	@%p38 bra 	$L__BB11_66;

	mul.lo.s64 	%rd173, %rd76, %rd52;
	add.s64 	%rd170, %rd114, %rd173;
	// begin inline asm
	{ atom.add.f64 %fd425,[%rd170],%fd123; }

	// end inline asm
	add.s64 	%rd171, %rd170, 8;
	// begin inline asm
	{ atom.add.f64 %fd427,[%rd171],%fd124; }

	// end inline asm
	add.s64 	%rd172, %rd170, 16;
	// begin inline asm
	{ atom.add.f64 %fd429,[%rd172],%fd125; }

	// end inline asm
	bra.uni 	$L__BB11_68;

$L__BB11_66:
	setp.eq.s64 	%p39, %rd95, 0;
	@%p39 bra 	$L__BB11_68;

	mul.lo.s64 	%rd205, %rd76, %rd197;
	add.s64 	%rd174, %rd95, %rd205;
	// begin inline asm
	{ atom.add.f64 %fd431,[%rd174],%fd123; }

	// end inline asm
	add.s64 	%rd175, %rd174, 8;
	// begin inline asm
	{ atom.add.f64 %fd433,[%rd175],%fd124; }

	// end inline asm
	add.s64 	%rd176, %rd174, 16;
	// begin inline asm
	{ atom.add.f64 %fd435,[%rd176],%fd125; }

	// end inline asm

$L__BB11_68:
	setp.eq.s64 	%p40, %rd124, 0;
	mov.f64 	%fd437, 0d0000000000000000;
	sub.f64 	%fd438, %fd437, %fd114;
	add.f64 	%fd126, %fd438, 0d0000000000000000;
	sub.f64 	%fd439, %fd437, %fd115;
	add.f64 	%fd127, %fd439, 0d0000000000000000;
	sub.f64 	%fd440, %fd437, %fd116;
	add.f64 	%fd128, %fd440, 0d0000000000000000;
	@%p40 bra 	$L__BB11_70;

	mul.lo.s64 	%rd180, %rd73, %rd53;
	add.s64 	%rd177, %rd124, %rd180;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd177],%fd126; }

	// end inline asm
	add.s64 	%rd178, %rd177, 8;
	// begin inline asm
	{ atom.add.f64 %fd443,[%rd178],%fd127; }

	// end inline asm
	add.s64 	%rd179, %rd177, 16;
	// begin inline asm
	{ atom.add.f64 %fd445,[%rd179],%fd128; }

	// end inline asm
	bra.uni 	$L__BB11_72;

$L__BB11_70:
	setp.eq.s64 	%p41, %rd107, 0;
	@%p41 bra 	$L__BB11_72;

	mul.lo.s64 	%rd204, %rd73, %rd198;
	add.s64 	%rd181, %rd107, %rd204;
	// begin inline asm
	{ atom.add.f64 %fd447,[%rd181],%fd126; }

	// end inline asm
	add.s64 	%rd182, %rd181, 8;
	// begin inline asm
	{ atom.add.f64 %fd449,[%rd182],%fd127; }

	// end inline asm
	add.s64 	%rd183, %rd181, 16;
	// begin inline asm
	{ atom.add.f64 %fd451,[%rd183],%fd128; }

	// end inline asm

$L__BB11_72:
	setp.eq.s64 	%p42, %rd122, 0;
	add.f64 	%fd129, %fd117, 0d0000000000000000;
	add.f64 	%fd130, %fd118, 0d0000000000000000;
	add.f64 	%fd131, %fd119, 0d0000000000000000;
	@%p42 bra 	$L__BB11_74;

	mul.lo.s64 	%rd187, %rd73, %rd55;
	add.s64 	%rd184, %rd122, %rd187;
	// begin inline asm
	{ atom.add.f64 %fd453,[%rd184],%fd129; }

	// end inline asm
	add.s64 	%rd185, %rd184, 8;
	// begin inline asm
	{ atom.add.f64 %fd455,[%rd185],%fd130; }

	// end inline asm
	add.s64 	%rd186, %rd184, 16;
	// begin inline asm
	{ atom.add.f64 %fd457,[%rd186],%fd131; }

	// end inline asm
	bra.uni 	$L__BB11_76;

$L__BB11_74:
	setp.eq.s64 	%p43, %rd105, 0;
	@%p43 bra 	$L__BB11_76;

	mul.lo.s64 	%rd203, %rd73, %rd199;
	add.s64 	%rd188, %rd105, %rd203;
	// begin inline asm
	{ atom.add.f64 %fd459,[%rd188],%fd129; }

	// end inline asm
	add.s64 	%rd189, %rd188, 8;
	// begin inline asm
	{ atom.add.f64 %fd461,[%rd189],%fd130; }

	// end inline asm
	add.s64 	%rd190, %rd188, 16;
	// begin inline asm
	{ atom.add.f64 %fd463,[%rd190],%fd131; }

	// end inline asm

$L__BB11_76:
	setp.eq.s64 	%p44, %rd118, 0;
	mov.f64 	%fd465, 0d0000000000000000;
	sub.f64 	%fd466, %fd465, %fd526;
	add.f64 	%fd132, %fd466, 0d0000000000000000;
	@%p44 bra 	$L__BB11_78;

	mul.lo.s64 	%rd192, %rd71, %rd57;
	add.s64 	%rd191, %rd118, %rd192;
	// begin inline asm
	{ atom.add.f64 %fd467,[%rd191],%fd132; }

	// end inline asm
	bra.uni 	$L__BB11_80;

$L__BB11_78:
	setp.eq.s64 	%p45, %rd101, 0;
	@%p45 bra 	$L__BB11_80;

	mul.lo.s64 	%rd202, %rd71, %rd200;
	add.s64 	%rd193, %rd101, %rd202;
	// begin inline asm
	{ atom.add.f64 %fd469,[%rd193],%fd132; }

	// end inline asm

$L__BB11_80:
	ld.param.u64 	%rd194, [val_IPC_hs_cuda_kernel_backward_param_0+24];
	add.s64 	%rd209, %rd209, %rd56;
	setp.lt.u64 	%p46, %rd209, %rd194;
	@%p46 bra 	$L__BB11_2;

$L__BB11_81:
	ret;

}
	// .globl	assemble_matrix_cuda_kernel_forward
.visible .entry assemble_matrix_cuda_kernel_forward(
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_1[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_2[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_forward_param_5[56],
	.param .u32 assemble_matrix_cuda_kernel_forward_param_6,
	.param .u32 assemble_matrix_cuda_kernel_forward_param_7
)
{
	.local .align 8 .b8 	__local_depot12[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<301>;
	.reg .b16 	%rs<123>;
	.reg .b32 	%r<822>;
	.reg .f64 	%fd<1729>;
	.reg .b64 	%rd<1216>;


	mov.u64 	%SPL, __local_depot12;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r404, %r405}, [assemble_matrix_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r406, %r407}, [assemble_matrix_cuda_kernel_forward_param_0+8];
	mov.b64 	%rd159, assemble_matrix_cuda_kernel_forward_param_2;
	ld.param.v2.u32 	{%r412, %r413}, [assemble_matrix_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r420, %r421}, [assemble_matrix_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r428, %r429}, [assemble_matrix_cuda_kernel_forward_param_5+32];
	ld.param.u32 	%r403, [assemble_matrix_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd164, [assemble_matrix_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd162, [assemble_matrix_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd160, [assemble_matrix_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd157, [assemble_matrix_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r375, [assemble_matrix_cuda_kernel_forward_param_0+16];
	mov.u32 	%r432, %ntid.x;
	mov.u32 	%r433, %ctaid.x;
	mul.wide.u32 	%rd166, %r432, %r433;
	mov.u32 	%r434, %tid.x;
	cvt.u64.u32 	%rd167, %r434;
	add.s64 	%rd1212, %rd166, %rd167;
	setp.ge.u64 	%p1, %rd1212, %rd157;
	@%p1 bra 	$L__BB12_208;

	cvta.to.global.u64 	%rd3, %rd164;
	cvta.to.global.u64 	%rd4, %rd162;
	cvta.to.global.u64 	%rd5, %rd160;
	cvt.s64.s32 	%rd6, %r407;
	cvt.s64.s32 	%rd7, %r406;
	cvt.s64.s32 	%rd8, %r405;
	cvt.s64.s32 	%rd9, %r428;
	cvt.s64.s32 	%rd10, %r412;
	cvt.s64.s32 	%rd11, %r420;

$L__BB12_2:
	setp.lt.s32 	%p2, %r375, 4;
	mov.u64 	%rd1213, %rd1212;
	@%p2 bra 	$L__BB12_6;

	or.b64  	%rd168, %rd1212, %rd6;
	and.b64  	%rd169, %rd168, -4294967296;
	setp.eq.s64 	%p3, %rd169, 0;
	@%p3 bra 	$L__BB12_5;

	div.u64 	%rd1213, %rd1212, %rd6;
	bra.uni 	$L__BB12_6;

$L__BB12_5:
	cvt.u32.u64 	%r435, %rd6;
	cvt.u32.u64 	%r436, %rd1212;
	div.u32 	%r437, %r436, %r435;
	cvt.u64.u32 	%rd1213, %r437;

$L__BB12_6:
	setp.lt.s32 	%p4, %r375, 3;
	@%p4 bra 	$L__BB12_10;

	or.b64  	%rd170, %rd1213, %rd7;
	and.b64  	%rd171, %rd170, -4294967296;
	setp.eq.s64 	%p5, %rd171, 0;
	@%p5 bra 	$L__BB12_9;

	div.u64 	%rd1213, %rd1213, %rd7;
	bra.uni 	$L__BB12_10;

$L__BB12_9:
	cvt.u32.u64 	%r438, %rd7;
	cvt.u32.u64 	%r439, %rd1213;
	div.u32 	%r440, %r439, %r438;
	cvt.u64.u32 	%rd1213, %r440;

$L__BB12_10:
	setp.lt.s32 	%p6, %r375, 2;
	@%p6 bra 	$L__BB12_14;

	or.b64  	%rd172, %rd1213, %rd8;
	and.b64  	%rd173, %rd172, -4294967296;
	setp.eq.s64 	%p7, %rd173, 0;
	@%p7 bra 	$L__BB12_13;

	div.u64 	%rd1213, %rd1213, %rd8;
	bra.uni 	$L__BB12_14;

$L__BB12_13:
	cvt.u32.u64 	%r441, %rd8;
	cvt.u32.u64 	%r442, %rd1213;
	div.u32 	%r443, %r442, %r441;
	cvt.u64.u32 	%rd1213, %r443;

$L__BB12_14:
	cvt.u32.u64 	%r444, %rd1213;
	setp.gt.s32 	%p8, %r375, 0;
	selp.b32 	%r2, %r444, 0, %p8;
	setp.ge.s32 	%p9, %r2, %r403;
	@%p9 bra 	$L__BB12_207;

	setp.gt.s32 	%p300, %r375, 0;
	cvt.u32.u64 	%r820, %rd1213;
	selp.b32 	%r819, %r820, 0, %p300;
	mov.b64 	%rd1210, assemble_matrix_cuda_kernel_forward_param_1;
	cvt.s64.s32 	%rd174, %r819;
	mul.lo.s64 	%rd175, %rd174, %rd9;
	add.s64 	%rd176, %rd3, %rd175;
	ld.global.f64 	%fd1, [%rd176+4600];
	ld.global.f64 	%fd2, [%rd176+4592];
	ld.global.f64 	%fd3, [%rd176+4584];
	ld.global.f64 	%fd4, [%rd176+4576];
	ld.global.f64 	%fd5, [%rd176+4568];
	ld.global.f64 	%fd6, [%rd176+4560];
	ld.global.f64 	%fd7, [%rd176+4552];
	ld.global.f64 	%fd8, [%rd176+4544];
	ld.global.f64 	%fd9, [%rd176+4536];
	ld.global.f64 	%fd10, [%rd176+4528];
	ld.global.f64 	%fd11, [%rd176+4520];
	ld.global.f64 	%fd12, [%rd176+4512];
	ld.global.f64 	%fd13, [%rd176+4504];
	ld.global.f64 	%fd14, [%rd176+4496];
	ld.global.f64 	%fd15, [%rd176+4488];
	ld.global.f64 	%fd16, [%rd176+4480];
	ld.global.f64 	%fd17, [%rd176+4472];
	ld.global.f64 	%fd18, [%rd176+4464];
	ld.global.f64 	%fd19, [%rd176+4456];
	ld.global.f64 	%fd20, [%rd176+4448];
	ld.global.f64 	%fd21, [%rd176+4440];
	ld.global.f64 	%fd22, [%rd176+4432];
	ld.global.f64 	%fd23, [%rd176+4424];
	ld.global.f64 	%fd24, [%rd176+4416];
	ld.global.f64 	%fd25, [%rd176+4408];
	ld.global.f64 	%fd26, [%rd176+4400];
	ld.global.f64 	%fd27, [%rd176+4392];
	ld.global.f64 	%fd28, [%rd176+4384];
	ld.global.f64 	%fd29, [%rd176+4376];
	ld.global.f64 	%fd30, [%rd176+4368];
	ld.global.f64 	%fd31, [%rd176+4360];
	ld.global.f64 	%fd32, [%rd176+4352];
	ld.global.f64 	%fd33, [%rd176+4344];
	ld.global.f64 	%fd34, [%rd176+4336];
	ld.global.f64 	%fd35, [%rd176+4328];
	ld.global.f64 	%fd36, [%rd176+4320];
	ld.global.f64 	%fd37, [%rd176+4312];
	ld.global.f64 	%fd38, [%rd176+4304];
	ld.global.f64 	%fd39, [%rd176+4296];
	ld.global.f64 	%fd40, [%rd176+4288];
	ld.global.f64 	%fd41, [%rd176+4280];
	ld.global.f64 	%fd42, [%rd176+4272];
	ld.global.f64 	%fd43, [%rd176+4264];
	ld.global.f64 	%fd44, [%rd176+4256];
	ld.global.f64 	%fd45, [%rd176+4248];
	ld.global.f64 	%fd46, [%rd176+4240];
	ld.global.f64 	%fd47, [%rd176+4232];
	ld.global.f64 	%fd48, [%rd176+4224];
	ld.global.f64 	%fd49, [%rd176+4216];
	ld.global.f64 	%fd50, [%rd176+4208];
	ld.global.f64 	%fd51, [%rd176+4200];
	ld.global.f64 	%fd52, [%rd176+4192];
	ld.global.f64 	%fd53, [%rd176+4184];
	ld.global.f64 	%fd54, [%rd176+4176];
	ld.global.f64 	%fd55, [%rd176+4168];
	ld.global.f64 	%fd56, [%rd176+4160];
	ld.global.f64 	%fd57, [%rd176+4152];
	ld.global.f64 	%fd58, [%rd176+4144];
	ld.global.f64 	%fd59, [%rd176+4136];
	ld.global.f64 	%fd60, [%rd176+4128];
	ld.global.f64 	%fd61, [%rd176+4120];
	ld.global.f64 	%fd62, [%rd176+4112];
	ld.global.f64 	%fd63, [%rd176+4104];
	ld.global.f64 	%fd64, [%rd176+4096];
	ld.global.f64 	%fd65, [%rd176+4088];
	ld.global.f64 	%fd66, [%rd176+4080];
	ld.global.f64 	%fd67, [%rd176+4072];
	ld.global.f64 	%fd68, [%rd176+4064];
	ld.global.f64 	%fd69, [%rd176+4056];
	ld.global.f64 	%fd70, [%rd176+4048];
	ld.global.f64 	%fd71, [%rd176+4040];
	ld.global.f64 	%fd72, [%rd176+4032];
	ld.global.f64 	%fd73, [%rd176+4024];
	ld.global.f64 	%fd74, [%rd176+4016];
	ld.global.f64 	%fd75, [%rd176+4008];
	ld.global.f64 	%fd76, [%rd176+4000];
	ld.global.f64 	%fd77, [%rd176+3992];
	ld.global.f64 	%fd78, [%rd176+3984];
	ld.global.f64 	%fd79, [%rd176+3976];
	ld.global.f64 	%fd80, [%rd176+3968];
	ld.global.f64 	%fd81, [%rd176+3960];
	ld.global.f64 	%fd82, [%rd176+3952];
	ld.global.f64 	%fd83, [%rd176+3944];
	ld.global.f64 	%fd84, [%rd176+3936];
	ld.global.f64 	%fd85, [%rd176+3928];
	ld.global.f64 	%fd86, [%rd176+3920];
	ld.global.f64 	%fd87, [%rd176+3912];
	ld.global.f64 	%fd88, [%rd176+3904];
	ld.global.f64 	%fd89, [%rd176+3896];
	ld.global.f64 	%fd90, [%rd176+3888];
	ld.global.f64 	%fd91, [%rd176+3880];
	ld.global.f64 	%fd92, [%rd176+3872];
	ld.global.f64 	%fd93, [%rd176+3864];
	ld.global.f64 	%fd94, [%rd176+3856];
	ld.global.f64 	%fd95, [%rd176+3848];
	ld.global.f64 	%fd96, [%rd176+3840];
	ld.global.f64 	%fd97, [%rd176+3832];
	ld.global.f64 	%fd98, [%rd176+3824];
	ld.global.f64 	%fd99, [%rd176+3816];
	ld.global.f64 	%fd100, [%rd176+3808];
	ld.global.f64 	%fd101, [%rd176+3800];
	ld.global.f64 	%fd102, [%rd176+3792];
	ld.global.f64 	%fd103, [%rd176+3784];
	ld.global.f64 	%fd104, [%rd176+3776];
	ld.global.f64 	%fd105, [%rd176+3768];
	ld.global.f64 	%fd106, [%rd176+3760];
	ld.global.f64 	%fd107, [%rd176+3752];
	ld.global.f64 	%fd108, [%rd176+3744];
	ld.global.f64 	%fd109, [%rd176+3736];
	ld.global.f64 	%fd110, [%rd176+3728];
	ld.global.f64 	%fd111, [%rd176+3720];
	ld.global.f64 	%fd112, [%rd176+3712];
	ld.global.f64 	%fd113, [%rd176+3704];
	ld.global.f64 	%fd114, [%rd176+3696];
	ld.global.f64 	%fd115, [%rd176+3688];
	ld.global.f64 	%fd116, [%rd176+3680];
	ld.global.f64 	%fd117, [%rd176+3672];
	ld.global.f64 	%fd118, [%rd176+3664];
	ld.global.f64 	%fd119, [%rd176+3656];
	ld.global.f64 	%fd120, [%rd176+3648];
	ld.global.f64 	%fd121, [%rd176+3640];
	ld.global.f64 	%fd122, [%rd176+3632];
	ld.global.f64 	%fd123, [%rd176+3624];
	ld.global.f64 	%fd124, [%rd176+3616];
	ld.global.f64 	%fd125, [%rd176+3608];
	ld.global.f64 	%fd126, [%rd176+3600];
	ld.global.f64 	%fd127, [%rd176+3592];
	ld.global.f64 	%fd128, [%rd176+3584];
	ld.global.f64 	%fd129, [%rd176+3576];
	ld.global.f64 	%fd130, [%rd176+3568];
	ld.global.f64 	%fd131, [%rd176+3560];
	ld.global.f64 	%fd132, [%rd176+3552];
	ld.global.f64 	%fd133, [%rd176+3544];
	ld.global.f64 	%fd134, [%rd176+3536];
	ld.global.f64 	%fd135, [%rd176+3528];
	ld.global.f64 	%fd136, [%rd176+3520];
	ld.global.f64 	%fd137, [%rd176+3512];
	ld.global.f64 	%fd138, [%rd176+3504];
	ld.global.f64 	%fd139, [%rd176+3496];
	ld.global.f64 	%fd140, [%rd176+3488];
	ld.global.f64 	%fd141, [%rd176+3480];
	ld.global.f64 	%fd142, [%rd176+3472];
	ld.global.f64 	%fd143, [%rd176+3464];
	ld.global.f64 	%fd144, [%rd176+3456];
	ld.global.f64 	%fd145, [%rd176+3448];
	ld.global.f64 	%fd146, [%rd176+3440];
	ld.global.f64 	%fd147, [%rd176+3432];
	ld.global.f64 	%fd148, [%rd176+3424];
	ld.global.f64 	%fd149, [%rd176+3416];
	ld.global.f64 	%fd150, [%rd176+3408];
	ld.global.f64 	%fd151, [%rd176+3400];
	ld.global.f64 	%fd152, [%rd176+3392];
	ld.global.f64 	%fd153, [%rd176+3384];
	ld.global.f64 	%fd154, [%rd176+3376];
	ld.global.f64 	%fd155, [%rd176+3368];
	ld.global.f64 	%fd156, [%rd176+3360];
	ld.global.f64 	%fd157, [%rd176+3352];
	ld.global.f64 	%fd158, [%rd176+3344];
	ld.global.f64 	%fd159, [%rd176+3336];
	ld.global.f64 	%fd160, [%rd176+3328];
	ld.global.f64 	%fd161, [%rd176+3320];
	ld.global.f64 	%fd162, [%rd176+3312];
	ld.global.f64 	%fd163, [%rd176+3304];
	ld.global.f64 	%fd164, [%rd176+3296];
	ld.global.f64 	%fd165, [%rd176+3288];
	ld.global.f64 	%fd166, [%rd176+3280];
	ld.global.f64 	%fd167, [%rd176+3272];
	ld.global.f64 	%fd168, [%rd176+3264];
	ld.global.f64 	%fd169, [%rd176+3256];
	ld.global.f64 	%fd170, [%rd176+3248];
	ld.global.f64 	%fd171, [%rd176+3240];
	ld.global.f64 	%fd172, [%rd176+3232];
	ld.global.f64 	%fd173, [%rd176+3224];
	ld.global.f64 	%fd174, [%rd176+3216];
	ld.global.f64 	%fd175, [%rd176+3208];
	ld.global.f64 	%fd176, [%rd176+3200];
	ld.global.f64 	%fd177, [%rd176+3192];
	ld.global.f64 	%fd178, [%rd176+3184];
	ld.global.f64 	%fd179, [%rd176+3176];
	ld.global.f64 	%fd180, [%rd176+3168];
	ld.global.f64 	%fd181, [%rd176+3160];
	ld.global.f64 	%fd182, [%rd176+3152];
	ld.global.f64 	%fd183, [%rd176+3144];
	ld.global.f64 	%fd184, [%rd176+3136];
	ld.global.f64 	%fd185, [%rd176+3128];
	ld.global.f64 	%fd186, [%rd176+3120];
	ld.global.f64 	%fd187, [%rd176+3112];
	ld.global.f64 	%fd188, [%rd176+3104];
	ld.global.f64 	%fd189, [%rd176+3096];
	ld.global.f64 	%fd190, [%rd176+3088];
	ld.global.f64 	%fd191, [%rd176+3080];
	ld.global.f64 	%fd192, [%rd176+3072];
	ld.global.f64 	%fd193, [%rd176+3064];
	ld.global.f64 	%fd194, [%rd176+3056];
	ld.global.f64 	%fd195, [%rd176+3048];
	ld.global.f64 	%fd196, [%rd176+3040];
	ld.global.f64 	%fd197, [%rd176+3032];
	ld.global.f64 	%fd198, [%rd176+3024];
	ld.global.f64 	%fd199, [%rd176+3016];
	ld.global.f64 	%fd200, [%rd176+3008];
	ld.global.f64 	%fd201, [%rd176+3000];
	ld.global.f64 	%fd202, [%rd176+2992];
	ld.global.f64 	%fd203, [%rd176+2984];
	ld.global.f64 	%fd204, [%rd176+2976];
	ld.global.f64 	%fd205, [%rd176+2968];
	ld.global.f64 	%fd206, [%rd176+2960];
	ld.global.f64 	%fd207, [%rd176+2952];
	ld.global.f64 	%fd208, [%rd176+2944];
	ld.global.f64 	%fd209, [%rd176+2936];
	ld.global.f64 	%fd210, [%rd176+2928];
	ld.global.f64 	%fd211, [%rd176+2920];
	ld.global.f64 	%fd212, [%rd176+2912];
	ld.global.f64 	%fd213, [%rd176+2904];
	ld.global.f64 	%fd214, [%rd176+2896];
	ld.global.f64 	%fd215, [%rd176+2888];
	ld.global.f64 	%fd216, [%rd176+2880];
	ld.global.f64 	%fd217, [%rd176+2872];
	ld.global.f64 	%fd218, [%rd176+2864];
	ld.global.f64 	%fd219, [%rd176+2856];
	ld.global.f64 	%fd220, [%rd176+2848];
	ld.global.f64 	%fd221, [%rd176+2840];
	ld.global.f64 	%fd222, [%rd176+2832];
	ld.global.f64 	%fd223, [%rd176+2824];
	ld.global.f64 	%fd224, [%rd176+2816];
	ld.global.f64 	%fd225, [%rd176+2808];
	ld.global.f64 	%fd226, [%rd176+2800];
	ld.global.f64 	%fd227, [%rd176+2792];
	ld.global.f64 	%fd228, [%rd176+2784];
	ld.global.f64 	%fd229, [%rd176+2776];
	ld.global.f64 	%fd230, [%rd176+2768];
	ld.global.f64 	%fd231, [%rd176+2760];
	ld.global.f64 	%fd232, [%rd176+2752];
	ld.global.f64 	%fd233, [%rd176+2744];
	ld.global.f64 	%fd234, [%rd176+2736];
	ld.global.f64 	%fd235, [%rd176+2728];
	ld.global.f64 	%fd236, [%rd176+2720];
	ld.global.f64 	%fd237, [%rd176+2712];
	ld.global.f64 	%fd238, [%rd176+2704];
	ld.global.f64 	%fd239, [%rd176+2696];
	ld.global.f64 	%fd240, [%rd176+2688];
	ld.global.f64 	%fd241, [%rd176+2680];
	ld.global.f64 	%fd242, [%rd176+2672];
	ld.global.f64 	%fd243, [%rd176+2664];
	ld.global.f64 	%fd244, [%rd176+2656];
	ld.global.f64 	%fd245, [%rd176+2648];
	ld.global.f64 	%fd246, [%rd176+2640];
	ld.global.f64 	%fd247, [%rd176+2632];
	ld.global.f64 	%fd248, [%rd176+2624];
	ld.global.f64 	%fd249, [%rd176+2616];
	ld.global.f64 	%fd250, [%rd176+2608];
	ld.global.f64 	%fd251, [%rd176+2600];
	ld.global.f64 	%fd252, [%rd176+2592];
	ld.global.f64 	%fd253, [%rd176+2584];
	ld.global.f64 	%fd254, [%rd176+2576];
	ld.global.f64 	%fd255, [%rd176+2568];
	ld.global.f64 	%fd256, [%rd176+2560];
	ld.global.f64 	%fd257, [%rd176+2552];
	ld.global.f64 	%fd258, [%rd176+2544];
	ld.global.f64 	%fd259, [%rd176+2536];
	ld.global.f64 	%fd260, [%rd176+2528];
	ld.global.f64 	%fd261, [%rd176+2520];
	ld.global.f64 	%fd262, [%rd176+2512];
	ld.global.f64 	%fd263, [%rd176+2504];
	ld.global.f64 	%fd264, [%rd176+2496];
	ld.global.f64 	%fd265, [%rd176+2488];
	ld.global.f64 	%fd266, [%rd176+2480];
	ld.global.f64 	%fd267, [%rd176+2472];
	ld.global.f64 	%fd268, [%rd176+2464];
	ld.global.f64 	%fd269, [%rd176+2456];
	ld.global.f64 	%fd270, [%rd176+2448];
	ld.global.f64 	%fd271, [%rd176+2440];
	ld.global.f64 	%fd272, [%rd176+2432];
	ld.global.f64 	%fd273, [%rd176+2424];
	ld.global.f64 	%fd274, [%rd176+2416];
	ld.global.f64 	%fd275, [%rd176+2408];
	ld.global.f64 	%fd276, [%rd176+2400];
	ld.global.f64 	%fd277, [%rd176+2392];
	ld.global.f64 	%fd278, [%rd176+2384];
	ld.global.f64 	%fd279, [%rd176+2376];
	ld.global.f64 	%fd280, [%rd176+2368];
	ld.global.f64 	%fd281, [%rd176+2360];
	ld.global.f64 	%fd282, [%rd176+2352];
	ld.global.f64 	%fd283, [%rd176+2344];
	ld.global.f64 	%fd284, [%rd176+2336];
	ld.global.f64 	%fd285, [%rd176+2328];
	ld.global.f64 	%fd286, [%rd176+2320];
	ld.global.f64 	%fd287, [%rd176+2312];
	ld.global.f64 	%fd288, [%rd176+2304];
	ld.global.f64 	%fd289, [%rd176+2296];
	ld.global.f64 	%fd290, [%rd176+2288];
	ld.global.f64 	%fd291, [%rd176+2280];
	ld.global.f64 	%fd292, [%rd176+2272];
	ld.global.f64 	%fd293, [%rd176+2264];
	ld.global.f64 	%fd294, [%rd176+2256];
	ld.global.f64 	%fd295, [%rd176+2248];
	ld.global.f64 	%fd296, [%rd176+2240];
	ld.global.f64 	%fd297, [%rd176+2232];
	ld.global.f64 	%fd298, [%rd176+2224];
	ld.global.f64 	%fd299, [%rd176+2216];
	ld.global.f64 	%fd300, [%rd176+2208];
	ld.global.f64 	%fd301, [%rd176+2200];
	ld.global.f64 	%fd302, [%rd176+2192];
	ld.global.f64 	%fd303, [%rd176+2184];
	ld.global.f64 	%fd304, [%rd176+2176];
	ld.global.f64 	%fd305, [%rd176+2168];
	ld.global.f64 	%fd306, [%rd176+2160];
	ld.global.f64 	%fd307, [%rd176+2152];
	ld.global.f64 	%fd308, [%rd176+2144];
	ld.global.f64 	%fd309, [%rd176+2136];
	ld.global.f64 	%fd310, [%rd176+2128];
	ld.global.f64 	%fd311, [%rd176+2120];
	ld.global.f64 	%fd312, [%rd176+2112];
	ld.global.f64 	%fd313, [%rd176+2104];
	ld.global.f64 	%fd314, [%rd176+2096];
	ld.global.f64 	%fd315, [%rd176+2088];
	ld.global.f64 	%fd316, [%rd176+2080];
	ld.global.f64 	%fd317, [%rd176+2072];
	ld.global.f64 	%fd318, [%rd176+2064];
	ld.global.f64 	%fd319, [%rd176+2056];
	ld.global.f64 	%fd320, [%rd176+2048];
	ld.global.f64 	%fd321, [%rd176+2040];
	ld.global.f64 	%fd322, [%rd176+2032];
	ld.global.f64 	%fd323, [%rd176+2024];
	ld.global.f64 	%fd324, [%rd176+2016];
	ld.global.f64 	%fd325, [%rd176+2008];
	ld.global.f64 	%fd326, [%rd176+2000];
	ld.global.f64 	%fd327, [%rd176+1992];
	ld.global.f64 	%fd328, [%rd176+1984];
	ld.global.f64 	%fd329, [%rd176+1976];
	ld.global.f64 	%fd330, [%rd176+1968];
	ld.global.f64 	%fd331, [%rd176+1960];
	ld.global.f64 	%fd332, [%rd176+1952];
	ld.global.f64 	%fd333, [%rd176+1944];
	ld.global.f64 	%fd334, [%rd176+1936];
	ld.global.f64 	%fd335, [%rd176+1928];
	ld.global.f64 	%fd336, [%rd176+1920];
	ld.global.f64 	%fd337, [%rd176+1912];
	ld.global.f64 	%fd338, [%rd176+1904];
	ld.global.f64 	%fd339, [%rd176+1896];
	ld.global.f64 	%fd340, [%rd176+1888];
	ld.global.f64 	%fd341, [%rd176+1880];
	ld.global.f64 	%fd342, [%rd176+1872];
	ld.global.f64 	%fd343, [%rd176+1864];
	ld.global.f64 	%fd344, [%rd176+1856];
	ld.global.f64 	%fd345, [%rd176+1848];
	ld.global.f64 	%fd346, [%rd176+1840];
	ld.global.f64 	%fd347, [%rd176+1832];
	ld.global.f64 	%fd348, [%rd176+1824];
	ld.global.f64 	%fd349, [%rd176+1816];
	ld.global.f64 	%fd350, [%rd176+1808];
	ld.global.f64 	%fd351, [%rd176+1800];
	ld.global.f64 	%fd352, [%rd176+1792];
	ld.global.f64 	%fd353, [%rd176+1784];
	ld.global.f64 	%fd354, [%rd176+1776];
	ld.global.f64 	%fd355, [%rd176+1768];
	ld.global.f64 	%fd356, [%rd176+1760];
	ld.global.f64 	%fd357, [%rd176+1752];
	ld.global.f64 	%fd358, [%rd176+1744];
	ld.global.f64 	%fd359, [%rd176+1736];
	ld.global.f64 	%fd360, [%rd176+1728];
	ld.global.f64 	%fd361, [%rd176+1720];
	ld.global.f64 	%fd362, [%rd176+1712];
	ld.global.f64 	%fd363, [%rd176+1704];
	ld.global.f64 	%fd364, [%rd176+1696];
	ld.global.f64 	%fd365, [%rd176+1688];
	ld.global.f64 	%fd366, [%rd176+1680];
	ld.global.f64 	%fd367, [%rd176+1672];
	ld.global.f64 	%fd368, [%rd176+1664];
	ld.global.f64 	%fd369, [%rd176+1656];
	ld.global.f64 	%fd370, [%rd176+1648];
	ld.global.f64 	%fd371, [%rd176+1640];
	ld.global.f64 	%fd372, [%rd176+1632];
	ld.global.f64 	%fd373, [%rd176+1624];
	ld.global.f64 	%fd374, [%rd176+1616];
	ld.global.f64 	%fd375, [%rd176+1608];
	ld.global.f64 	%fd376, [%rd176+1600];
	ld.global.f64 	%fd377, [%rd176+1592];
	ld.global.f64 	%fd378, [%rd176+1584];
	ld.global.f64 	%fd379, [%rd176+1576];
	ld.global.f64 	%fd380, [%rd176+1568];
	ld.global.f64 	%fd381, [%rd176+1560];
	ld.global.f64 	%fd382, [%rd176+1552];
	ld.global.f64 	%fd383, [%rd176+1544];
	ld.global.f64 	%fd384, [%rd176+1536];
	ld.global.f64 	%fd385, [%rd176+1528];
	ld.global.f64 	%fd386, [%rd176+1520];
	ld.global.f64 	%fd387, [%rd176+1512];
	ld.global.f64 	%fd388, [%rd176+1504];
	ld.global.f64 	%fd389, [%rd176+1496];
	ld.global.f64 	%fd390, [%rd176+1488];
	ld.global.f64 	%fd391, [%rd176+1480];
	ld.global.f64 	%fd392, [%rd176+1472];
	ld.global.f64 	%fd393, [%rd176+1464];
	ld.global.f64 	%fd394, [%rd176+1456];
	ld.global.f64 	%fd395, [%rd176+1448];
	ld.global.f64 	%fd396, [%rd176+1440];
	ld.global.f64 	%fd397, [%rd176+1432];
	ld.global.f64 	%fd398, [%rd176+1424];
	ld.global.f64 	%fd399, [%rd176+1416];
	ld.global.f64 	%fd400, [%rd176+1408];
	ld.global.f64 	%fd401, [%rd176+1400];
	ld.global.f64 	%fd402, [%rd176+1392];
	ld.global.f64 	%fd403, [%rd176+1384];
	ld.global.f64 	%fd404, [%rd176+1376];
	ld.global.f64 	%fd405, [%rd176+1368];
	ld.global.f64 	%fd406, [%rd176+1360];
	ld.global.f64 	%fd407, [%rd176+1352];
	ld.global.f64 	%fd408, [%rd176+1344];
	ld.global.f64 	%fd409, [%rd176+1336];
	ld.global.f64 	%fd410, [%rd176+1328];
	ld.global.f64 	%fd411, [%rd176+1320];
	ld.global.f64 	%fd412, [%rd176+1312];
	ld.global.f64 	%fd413, [%rd176+1304];
	ld.global.f64 	%fd414, [%rd176+1296];
	ld.global.f64 	%fd415, [%rd176+1288];
	ld.global.f64 	%fd416, [%rd176+1280];
	ld.global.f64 	%fd417, [%rd176+1272];
	ld.global.f64 	%fd418, [%rd176+1264];
	ld.global.f64 	%fd419, [%rd176+1256];
	ld.global.f64 	%fd420, [%rd176+1248];
	ld.global.f64 	%fd421, [%rd176+1240];
	ld.global.f64 	%fd422, [%rd176+1232];
	ld.global.f64 	%fd423, [%rd176+1224];
	ld.global.f64 	%fd424, [%rd176+1216];
	ld.global.f64 	%fd425, [%rd176+1208];
	ld.global.f64 	%fd426, [%rd176+1200];
	ld.global.f64 	%fd427, [%rd176+1192];
	ld.global.f64 	%fd428, [%rd176+1184];
	ld.global.f64 	%fd429, [%rd176+1176];
	ld.global.f64 	%fd430, [%rd176+1168];
	ld.global.f64 	%fd431, [%rd176+1160];
	ld.global.f64 	%fd432, [%rd176+1152];
	ld.global.f64 	%fd433, [%rd176+1144];
	ld.global.f64 	%fd434, [%rd176+1136];
	ld.global.f64 	%fd435, [%rd176+1128];
	ld.global.f64 	%fd436, [%rd176+1120];
	ld.global.f64 	%fd437, [%rd176+1112];
	ld.global.f64 	%fd438, [%rd176+1104];
	ld.global.f64 	%fd439, [%rd176+1096];
	ld.global.f64 	%fd440, [%rd176+1088];
	ld.global.f64 	%fd441, [%rd176+1080];
	ld.global.f64 	%fd442, [%rd176+1072];
	ld.global.f64 	%fd443, [%rd176+1064];
	ld.global.f64 	%fd444, [%rd176+1056];
	ld.global.f64 	%fd445, [%rd176+1048];
	ld.global.f64 	%fd446, [%rd176+1040];
	ld.global.f64 	%fd447, [%rd176+1032];
	ld.global.f64 	%fd448, [%rd176+1024];
	ld.global.f64 	%fd449, [%rd176+1016];
	ld.global.f64 	%fd450, [%rd176+1008];
	ld.global.f64 	%fd451, [%rd176+1000];
	ld.global.f64 	%fd452, [%rd176+992];
	ld.global.f64 	%fd453, [%rd176+984];
	ld.global.f64 	%fd454, [%rd176+976];
	ld.global.f64 	%fd455, [%rd176+968];
	ld.global.f64 	%fd456, [%rd176+960];
	ld.global.f64 	%fd457, [%rd176+952];
	ld.global.f64 	%fd458, [%rd176+944];
	ld.global.f64 	%fd459, [%rd176+936];
	ld.global.f64 	%fd460, [%rd176+928];
	ld.global.f64 	%fd461, [%rd176+920];
	ld.global.f64 	%fd462, [%rd176+912];
	ld.global.f64 	%fd463, [%rd176+904];
	ld.global.f64 	%fd464, [%rd176+896];
	ld.global.f64 	%fd465, [%rd176+888];
	ld.global.f64 	%fd466, [%rd176+880];
	ld.global.f64 	%fd467, [%rd176+872];
	ld.global.f64 	%fd468, [%rd176+864];
	ld.global.f64 	%fd469, [%rd176+856];
	ld.global.f64 	%fd470, [%rd176+848];
	ld.global.f64 	%fd471, [%rd176+840];
	ld.global.f64 	%fd472, [%rd176+832];
	ld.global.f64 	%fd473, [%rd176+824];
	ld.global.f64 	%fd474, [%rd176+816];
	ld.global.f64 	%fd475, [%rd176+808];
	ld.global.f64 	%fd476, [%rd176+800];
	ld.global.f64 	%fd477, [%rd176+792];
	ld.global.f64 	%fd478, [%rd176+784];
	ld.global.f64 	%fd479, [%rd176+776];
	ld.global.f64 	%fd480, [%rd176+768];
	ld.global.f64 	%fd481, [%rd176+760];
	ld.global.f64 	%fd482, [%rd176+752];
	ld.global.f64 	%fd483, [%rd176+744];
	ld.global.f64 	%fd484, [%rd176+736];
	ld.global.f64 	%fd485, [%rd176+728];
	ld.global.f64 	%fd486, [%rd176+720];
	ld.global.f64 	%fd487, [%rd176+712];
	ld.global.f64 	%fd488, [%rd176+704];
	ld.global.f64 	%fd489, [%rd176+696];
	ld.global.f64 	%fd490, [%rd176+688];
	ld.global.f64 	%fd491, [%rd176+680];
	ld.global.f64 	%fd492, [%rd176+672];
	ld.global.f64 	%fd493, [%rd176+664];
	ld.global.f64 	%fd494, [%rd176+656];
	ld.global.f64 	%fd495, [%rd176+648];
	ld.global.f64 	%fd496, [%rd176+640];
	ld.global.f64 	%fd497, [%rd176+632];
	ld.global.f64 	%fd498, [%rd176+624];
	ld.global.f64 	%fd499, [%rd176+616];
	ld.global.f64 	%fd500, [%rd176+608];
	ld.global.f64 	%fd501, [%rd176+600];
	ld.global.f64 	%fd502, [%rd176+592];
	ld.global.f64 	%fd503, [%rd176+584];
	ld.global.f64 	%fd504, [%rd176+576];
	ld.global.f64 	%fd505, [%rd176+568];
	ld.global.f64 	%fd506, [%rd176+560];
	ld.global.f64 	%fd507, [%rd176+552];
	ld.global.f64 	%fd508, [%rd176+544];
	ld.global.f64 	%fd509, [%rd176+536];
	ld.global.f64 	%fd510, [%rd176+528];
	ld.global.f64 	%fd511, [%rd176+520];
	ld.global.f64 	%fd512, [%rd176+512];
	ld.global.f64 	%fd513, [%rd176+504];
	ld.global.f64 	%fd514, [%rd176+496];
	ld.global.f64 	%fd515, [%rd176+488];
	ld.global.f64 	%fd516, [%rd176+480];
	ld.global.f64 	%fd517, [%rd176+472];
	ld.global.f64 	%fd518, [%rd176+464];
	ld.global.f64 	%fd519, [%rd176+456];
	ld.global.f64 	%fd520, [%rd176+448];
	ld.global.f64 	%fd521, [%rd176+440];
	ld.global.f64 	%fd522, [%rd176+432];
	ld.global.f64 	%fd523, [%rd176+424];
	ld.global.f64 	%fd524, [%rd176+416];
	ld.global.f64 	%fd525, [%rd176+408];
	ld.global.f64 	%fd526, [%rd176+400];
	ld.global.f64 	%fd527, [%rd176+392];
	ld.global.f64 	%fd528, [%rd176+384];
	ld.global.f64 	%fd529, [%rd176+376];
	ld.global.f64 	%fd530, [%rd176+368];
	ld.global.f64 	%fd531, [%rd176+360];
	ld.global.f64 	%fd532, [%rd176+352];
	ld.global.f64 	%fd533, [%rd176+344];
	ld.global.f64 	%fd534, [%rd176+336];
	ld.global.f64 	%fd535, [%rd176+328];
	ld.global.f64 	%fd536, [%rd176+320];
	ld.global.f64 	%fd537, [%rd176+312];
	ld.global.f64 	%fd538, [%rd176+304];
	ld.global.f64 	%fd539, [%rd176+296];
	ld.global.f64 	%fd540, [%rd176+288];
	ld.global.f64 	%fd541, [%rd176+280];
	ld.global.f64 	%fd542, [%rd176+272];
	ld.global.f64 	%fd543, [%rd176+264];
	ld.global.f64 	%fd544, [%rd176+256];
	ld.global.f64 	%fd545, [%rd176+248];
	ld.global.f64 	%fd546, [%rd176+240];
	ld.global.f64 	%fd547, [%rd176+232];
	ld.global.f64 	%fd548, [%rd176+224];
	ld.global.f64 	%fd549, [%rd176+216];
	ld.global.f64 	%fd550, [%rd176+208];
	ld.global.f64 	%fd551, [%rd176+200];
	ld.global.f64 	%fd552, [%rd176+192];
	ld.global.f64 	%fd553, [%rd176+184];
	ld.global.f64 	%fd554, [%rd176+176];
	ld.global.f64 	%fd555, [%rd176+168];
	ld.global.f64 	%fd556, [%rd176+160];
	ld.global.f64 	%fd557, [%rd176+152];
	ld.global.f64 	%fd558, [%rd176+144];
	ld.global.f64 	%fd559, [%rd176+136];
	ld.global.f64 	%fd560, [%rd176+128];
	ld.global.f64 	%fd561, [%rd176+120];
	ld.global.f64 	%fd562, [%rd176+112];
	ld.global.f64 	%fd563, [%rd176+104];
	ld.global.f64 	%fd564, [%rd176+96];
	ld.global.f64 	%fd565, [%rd176+88];
	ld.global.f64 	%fd566, [%rd176+80];
	ld.global.f64 	%fd567, [%rd176+72];
	ld.global.f64 	%fd568, [%rd176+64];
	ld.global.f64 	%fd569, [%rd176+56];
	ld.global.f64 	%fd570, [%rd176+48];
	ld.global.f64 	%fd571, [%rd176+40];
	ld.global.f64 	%fd572, [%rd176+32];
	ld.global.f64 	%fd573, [%rd176+24];
	ld.global.f64 	%fd574, [%rd176+16];
	ld.global.f64 	%fd575, [%rd176+8];
	ld.global.f64 	%fd576, [%rd176];
	mul.lo.s64 	%rd177, %rd174, %rd10;
	add.s64 	%rd22, %rd5, %rd177;
	ld.global.u32 	%r445, [%rd22];
	shl.b32 	%r3, %r445, 4;
	add.s64 	%rd23, %rd1210, 112;
	ld.param.u32 	%r5, [%rd1210+172];
	setp.le.s32 	%p10, %r5, %r3;
	selp.u16 	%rs25, 1, 0, %p10;
	shr.u32 	%r446, %r445, 27;
	cvt.u16.u32 	%rs26, %r446;
	and.b16  	%rs27, %rs26, 1;
	or.b16  	%rs28, %rs27, %rs25;
	setp.eq.s16 	%p11, %rs28, 0;
	add.u64 	%rd179, %SP, 0;
	add.u64 	%rd25, %SPL, 0;
	@%p11 bra 	$L__BB12_17;

	st.local.v2.u32 	[%rd25], {%r3, %r5};
	mov.u64 	%rd180, $str$1;
	cvta.global.u64 	%rd181, %rd180;
	{ // callseq 257, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd181;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r447, [retval0+0];
	} // callseq 257
	bra.uni 	$L__BB12_18;

$L__BB12_17:
	ld.param.u64 	%rd1211, [%rd1210+112];
	ld.param.u32 	%r821, [%rd1210+144];
	mul.wide.s32 	%rd192, %r821, %r3;
	add.s64 	%rd183, %rd1211, %rd192;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd183],%fd576; }

	// end inline asm
	add.s64 	%rd184, %rd183, 8;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd184],%fd575; }

	// end inline asm
	add.s64 	%rd185, %rd183, 16;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd185],%fd574; }

	// end inline asm
	add.s64 	%rd186, %rd183, 24;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd186],%fd552; }

	// end inline asm
	add.s64 	%rd187, %rd183, 32;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd187],%fd551; }

	// end inline asm
	add.s64 	%rd188, %rd183, 40;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd188],%fd550; }

	// end inline asm
	add.s64 	%rd189, %rd183, 48;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd189],%fd528; }

	// end inline asm
	add.s64 	%rd190, %rd183, 56;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd190],%fd527; }

	// end inline asm
	add.s64 	%rd191, %rd183, 64;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd191],%fd526; }

	// end inline asm

$L__BB12_18:
	ld.param.u64 	%rd26, [%rd23];
	ld.param.u32 	%r6, [%rd23+32];
	ld.param.u32 	%r7, [%rd23+60];
	add.s32 	%r8, %r3, 1;
	setp.le.s32 	%p12, %r7, %r8;
	selp.u16 	%rs29, 1, 0, %p12;
	shr.u32 	%r448, %r8, 31;
	cvt.u16.u32 	%rs30, %r448;
	or.b16  	%rs31, %rs29, %rs30;
	setp.eq.s16 	%p13, %rs31, 0;
	@%p13 bra 	$L__BB12_20;

	add.s32 	%r678, %r3, 1;
	st.local.v2.u32 	[%rd25], {%r678, %r7};
	mov.u64 	%rd193, $str$1;
	cvta.global.u64 	%rd194, %rd193;
	{ // callseq 258, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd194;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r449, [retval0+0];
	} // callseq 258
	bra.uni 	$L__BB12_21;

$L__BB12_20:
	mul.wide.s32 	%rd205, %r6, %r8;
	add.s64 	%rd196, %rd26, %rd205;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd196],%fd573; }

	// end inline asm
	add.s64 	%rd197, %rd196, 8;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd197],%fd572; }

	// end inline asm
	add.s64 	%rd198, %rd196, 16;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd198],%fd571; }

	// end inline asm
	add.s64 	%rd199, %rd196, 24;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd199],%fd549; }

	// end inline asm
	add.s64 	%rd200, %rd196, 32;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd200],%fd548; }

	// end inline asm
	add.s64 	%rd201, %rd196, 40;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd201],%fd547; }

	// end inline asm
	add.s64 	%rd202, %rd196, 48;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd202],%fd525; }

	// end inline asm
	add.s64 	%rd203, %rd196, 56;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd203],%fd524; }

	// end inline asm
	add.s64 	%rd204, %rd196, 64;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd204],%fd523; }

	// end inline asm

$L__BB12_21:
	ld.param.u64 	%rd27, [%rd23];
	ld.param.u32 	%r9, [%rd23+32];
	ld.param.u32 	%r10, [%rd23+60];
	add.s32 	%r11, %r3, 2;
	setp.le.s32 	%p14, %r10, %r11;
	selp.u16 	%rs32, 1, 0, %p14;
	shr.u32 	%r450, %r11, 31;
	cvt.u16.u32 	%rs33, %r450;
	or.b16  	%rs34, %rs32, %rs33;
	setp.eq.s16 	%p15, %rs34, 0;
	@%p15 bra 	$L__BB12_23;

	add.s32 	%r679, %r3, 2;
	st.local.v2.u32 	[%rd25], {%r679, %r10};
	mov.u64 	%rd206, $str$1;
	cvta.global.u64 	%rd207, %rd206;
	{ // callseq 259, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd207;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r451, [retval0+0];
	} // callseq 259
	bra.uni 	$L__BB12_24;

$L__BB12_23:
	mul.wide.s32 	%rd218, %r9, %r11;
	add.s64 	%rd209, %rd27, %rd218;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd209],%fd570; }

	// end inline asm
	add.s64 	%rd210, %rd209, 8;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd210],%fd569; }

	// end inline asm
	add.s64 	%rd211, %rd209, 16;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd211],%fd568; }

	// end inline asm
	add.s64 	%rd212, %rd209, 24;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd212],%fd546; }

	// end inline asm
	add.s64 	%rd213, %rd209, 32;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd213],%fd545; }

	// end inline asm
	add.s64 	%rd214, %rd209, 40;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd214],%fd544; }

	// end inline asm
	add.s64 	%rd215, %rd209, 48;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd215],%fd522; }

	// end inline asm
	add.s64 	%rd216, %rd209, 56;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd216],%fd521; }

	// end inline asm
	add.s64 	%rd217, %rd209, 64;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd217],%fd520; }

	// end inline asm

$L__BB12_24:
	ld.param.u64 	%rd28, [%rd23];
	ld.param.u32 	%r12, [%rd23+32];
	ld.param.u32 	%r13, [%rd23+60];
	add.s32 	%r14, %r3, 3;
	setp.le.s32 	%p16, %r13, %r14;
	selp.u16 	%rs35, 1, 0, %p16;
	shr.u32 	%r452, %r14, 31;
	cvt.u16.u32 	%rs36, %r452;
	or.b16  	%rs37, %rs35, %rs36;
	setp.eq.s16 	%p17, %rs37, 0;
	@%p17 bra 	$L__BB12_26;

	add.s32 	%r680, %r3, 3;
	st.local.v2.u32 	[%rd25], {%r680, %r13};
	mov.u64 	%rd219, $str$1;
	cvta.global.u64 	%rd220, %rd219;
	{ // callseq 260, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd220;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r453, [retval0+0];
	} // callseq 260
	bra.uni 	$L__BB12_27;

$L__BB12_26:
	mul.wide.s32 	%rd231, %r12, %r14;
	add.s64 	%rd222, %rd28, %rd231;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd222],%fd567; }

	// end inline asm
	add.s64 	%rd223, %rd222, 8;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd223],%fd566; }

	// end inline asm
	add.s64 	%rd224, %rd222, 16;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd224],%fd565; }

	// end inline asm
	add.s64 	%rd225, %rd222, 24;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd225],%fd543; }

	// end inline asm
	add.s64 	%rd226, %rd222, 32;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd226],%fd542; }

	// end inline asm
	add.s64 	%rd227, %rd222, 40;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd227],%fd541; }

	// end inline asm
	add.s64 	%rd228, %rd222, 48;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd228],%fd519; }

	// end inline asm
	add.s64 	%rd229, %rd222, 56;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd229],%fd518; }

	// end inline asm
	add.s64 	%rd230, %rd222, 64;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd230],%fd517; }

	// end inline asm

$L__BB12_27:
	ld.param.u64 	%rd29, [%rd23];
	ld.param.u32 	%r15, [%rd23+32];
	ld.param.u32 	%r16, [%rd23+60];
	add.s32 	%r17, %r3, 4;
	setp.le.s32 	%p18, %r16, %r17;
	selp.u16 	%rs38, 1, 0, %p18;
	shr.u32 	%r454, %r17, 31;
	cvt.u16.u32 	%rs39, %r454;
	or.b16  	%rs40, %rs38, %rs39;
	setp.eq.s16 	%p19, %rs40, 0;
	@%p19 bra 	$L__BB12_29;

	add.s32 	%r681, %r3, 4;
	st.local.v2.u32 	[%rd25], {%r681, %r16};
	mov.u64 	%rd232, $str$1;
	cvta.global.u64 	%rd233, %rd232;
	{ // callseq 261, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd233;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r455, [retval0+0];
	} // callseq 261
	bra.uni 	$L__BB12_30;

$L__BB12_29:
	mul.wide.s32 	%rd244, %r15, %r17;
	add.s64 	%rd235, %rd29, %rd244;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd235],%fd504; }

	// end inline asm
	add.s64 	%rd236, %rd235, 8;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd236],%fd503; }

	// end inline asm
	add.s64 	%rd237, %rd235, 16;
	// begin inline asm
	{ atom.add.f64 %fd653,[%rd237],%fd502; }

	// end inline asm
	add.s64 	%rd238, %rd235, 24;
	// begin inline asm
	{ atom.add.f64 %fd655,[%rd238],%fd480; }

	// end inline asm
	add.s64 	%rd239, %rd235, 32;
	// begin inline asm
	{ atom.add.f64 %fd657,[%rd239],%fd479; }

	// end inline asm
	add.s64 	%rd240, %rd235, 40;
	// begin inline asm
	{ atom.add.f64 %fd659,[%rd240],%fd478; }

	// end inline asm
	add.s64 	%rd241, %rd235, 48;
	// begin inline asm
	{ atom.add.f64 %fd661,[%rd241],%fd456; }

	// end inline asm
	add.s64 	%rd242, %rd235, 56;
	// begin inline asm
	{ atom.add.f64 %fd663,[%rd242],%fd455; }

	// end inline asm
	add.s64 	%rd243, %rd235, 64;
	// begin inline asm
	{ atom.add.f64 %fd665,[%rd243],%fd454; }

	// end inline asm

$L__BB12_30:
	ld.param.u64 	%rd30, [%rd23];
	ld.param.u32 	%r18, [%rd23+32];
	ld.param.u32 	%r19, [%rd23+60];
	add.s32 	%r20, %r3, 5;
	setp.le.s32 	%p20, %r19, %r20;
	selp.u16 	%rs41, 1, 0, %p20;
	shr.u32 	%r456, %r20, 31;
	cvt.u16.u32 	%rs42, %r456;
	or.b16  	%rs43, %rs41, %rs42;
	setp.eq.s16 	%p21, %rs43, 0;
	@%p21 bra 	$L__BB12_32;

	add.s32 	%r682, %r3, 5;
	st.local.v2.u32 	[%rd25], {%r682, %r19};
	mov.u64 	%rd245, $str$1;
	cvta.global.u64 	%rd246, %rd245;
	{ // callseq 262, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd246;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r457, [retval0+0];
	} // callseq 262
	bra.uni 	$L__BB12_33;

$L__BB12_32:
	mul.wide.s32 	%rd257, %r18, %r20;
	add.s64 	%rd248, %rd30, %rd257;
	// begin inline asm
	{ atom.add.f64 %fd667,[%rd248],%fd501; }

	// end inline asm
	add.s64 	%rd249, %rd248, 8;
	// begin inline asm
	{ atom.add.f64 %fd669,[%rd249],%fd500; }

	// end inline asm
	add.s64 	%rd250, %rd248, 16;
	// begin inline asm
	{ atom.add.f64 %fd671,[%rd250],%fd499; }

	// end inline asm
	add.s64 	%rd251, %rd248, 24;
	// begin inline asm
	{ atom.add.f64 %fd673,[%rd251],%fd477; }

	// end inline asm
	add.s64 	%rd252, %rd248, 32;
	// begin inline asm
	{ atom.add.f64 %fd675,[%rd252],%fd476; }

	// end inline asm
	add.s64 	%rd253, %rd248, 40;
	// begin inline asm
	{ atom.add.f64 %fd677,[%rd253],%fd475; }

	// end inline asm
	add.s64 	%rd254, %rd248, 48;
	// begin inline asm
	{ atom.add.f64 %fd679,[%rd254],%fd453; }

	// end inline asm
	add.s64 	%rd255, %rd248, 56;
	// begin inline asm
	{ atom.add.f64 %fd681,[%rd255],%fd452; }

	// end inline asm
	add.s64 	%rd256, %rd248, 64;
	// begin inline asm
	{ atom.add.f64 %fd683,[%rd256],%fd451; }

	// end inline asm

$L__BB12_33:
	ld.param.u64 	%rd31, [%rd23];
	ld.param.u32 	%r21, [%rd23+32];
	ld.param.u32 	%r22, [%rd23+60];
	add.s32 	%r23, %r3, 6;
	setp.le.s32 	%p22, %r22, %r23;
	selp.u16 	%rs44, 1, 0, %p22;
	shr.u32 	%r458, %r23, 31;
	cvt.u16.u32 	%rs45, %r458;
	or.b16  	%rs46, %rs44, %rs45;
	setp.eq.s16 	%p23, %rs46, 0;
	@%p23 bra 	$L__BB12_35;

	add.s32 	%r683, %r3, 6;
	st.local.v2.u32 	[%rd25], {%r683, %r22};
	mov.u64 	%rd258, $str$1;
	cvta.global.u64 	%rd259, %rd258;
	{ // callseq 263, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd259;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r459, [retval0+0];
	} // callseq 263
	bra.uni 	$L__BB12_36;

$L__BB12_35:
	mul.wide.s32 	%rd270, %r21, %r23;
	add.s64 	%rd261, %rd31, %rd270;
	// begin inline asm
	{ atom.add.f64 %fd685,[%rd261],%fd498; }

	// end inline asm
	add.s64 	%rd262, %rd261, 8;
	// begin inline asm
	{ atom.add.f64 %fd687,[%rd262],%fd497; }

	// end inline asm
	add.s64 	%rd263, %rd261, 16;
	// begin inline asm
	{ atom.add.f64 %fd689,[%rd263],%fd496; }

	// end inline asm
	add.s64 	%rd264, %rd261, 24;
	// begin inline asm
	{ atom.add.f64 %fd691,[%rd264],%fd474; }

	// end inline asm
	add.s64 	%rd265, %rd261, 32;
	// begin inline asm
	{ atom.add.f64 %fd693,[%rd265],%fd473; }

	// end inline asm
	add.s64 	%rd266, %rd261, 40;
	// begin inline asm
	{ atom.add.f64 %fd695,[%rd266],%fd472; }

	// end inline asm
	add.s64 	%rd267, %rd261, 48;
	// begin inline asm
	{ atom.add.f64 %fd697,[%rd267],%fd450; }

	// end inline asm
	add.s64 	%rd268, %rd261, 56;
	// begin inline asm
	{ atom.add.f64 %fd699,[%rd268],%fd449; }

	// end inline asm
	add.s64 	%rd269, %rd261, 64;
	// begin inline asm
	{ atom.add.f64 %fd701,[%rd269],%fd448; }

	// end inline asm

$L__BB12_36:
	ld.param.u64 	%rd32, [%rd23];
	ld.param.u32 	%r24, [%rd23+32];
	ld.param.u32 	%r25, [%rd23+60];
	add.s32 	%r26, %r3, 7;
	setp.le.s32 	%p24, %r25, %r26;
	selp.u16 	%rs47, 1, 0, %p24;
	shr.u32 	%r460, %r26, 31;
	cvt.u16.u32 	%rs48, %r460;
	or.b16  	%rs49, %rs47, %rs48;
	setp.eq.s16 	%p25, %rs49, 0;
	@%p25 bra 	$L__BB12_38;

	add.s32 	%r684, %r3, 7;
	st.local.v2.u32 	[%rd25], {%r684, %r25};
	mov.u64 	%rd271, $str$1;
	cvta.global.u64 	%rd272, %rd271;
	{ // callseq 264, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd272;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r461, [retval0+0];
	} // callseq 264
	bra.uni 	$L__BB12_39;

$L__BB12_38:
	mul.wide.s32 	%rd283, %r24, %r26;
	add.s64 	%rd274, %rd32, %rd283;
	// begin inline asm
	{ atom.add.f64 %fd703,[%rd274],%fd495; }

	// end inline asm
	add.s64 	%rd275, %rd274, 8;
	// begin inline asm
	{ atom.add.f64 %fd705,[%rd275],%fd494; }

	// end inline asm
	add.s64 	%rd276, %rd274, 16;
	// begin inline asm
	{ atom.add.f64 %fd707,[%rd276],%fd493; }

	// end inline asm
	add.s64 	%rd277, %rd274, 24;
	// begin inline asm
	{ atom.add.f64 %fd709,[%rd277],%fd471; }

	// end inline asm
	add.s64 	%rd278, %rd274, 32;
	// begin inline asm
	{ atom.add.f64 %fd711,[%rd278],%fd470; }

	// end inline asm
	add.s64 	%rd279, %rd274, 40;
	// begin inline asm
	{ atom.add.f64 %fd713,[%rd279],%fd469; }

	// end inline asm
	add.s64 	%rd280, %rd274, 48;
	// begin inline asm
	{ atom.add.f64 %fd715,[%rd280],%fd447; }

	// end inline asm
	add.s64 	%rd281, %rd274, 56;
	// begin inline asm
	{ atom.add.f64 %fd717,[%rd281],%fd446; }

	// end inline asm
	add.s64 	%rd282, %rd274, 64;
	// begin inline asm
	{ atom.add.f64 %fd719,[%rd282],%fd445; }

	// end inline asm

$L__BB12_39:
	ld.param.u64 	%rd33, [%rd23];
	ld.param.u32 	%r27, [%rd23+32];
	ld.param.u32 	%r28, [%rd23+60];
	add.s32 	%r29, %r3, 8;
	setp.le.s32 	%p26, %r28, %r29;
	selp.u16 	%rs50, 1, 0, %p26;
	shr.u32 	%r462, %r29, 31;
	cvt.u16.u32 	%rs51, %r462;
	or.b16  	%rs52, %rs50, %rs51;
	setp.eq.s16 	%p27, %rs52, 0;
	@%p27 bra 	$L__BB12_41;

	add.s32 	%r685, %r3, 8;
	st.local.v2.u32 	[%rd25], {%r685, %r28};
	mov.u64 	%rd284, $str$1;
	cvta.global.u64 	%rd285, %rd284;
	{ // callseq 265, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd285;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r463, [retval0+0];
	} // callseq 265
	bra.uni 	$L__BB12_42;

$L__BB12_41:
	mul.wide.s32 	%rd296, %r27, %r29;
	add.s64 	%rd287, %rd33, %rd296;
	// begin inline asm
	{ atom.add.f64 %fd721,[%rd287],%fd432; }

	// end inline asm
	add.s64 	%rd288, %rd287, 8;
	// begin inline asm
	{ atom.add.f64 %fd723,[%rd288],%fd431; }

	// end inline asm
	add.s64 	%rd289, %rd287, 16;
	// begin inline asm
	{ atom.add.f64 %fd725,[%rd289],%fd430; }

	// end inline asm
	add.s64 	%rd290, %rd287, 24;
	// begin inline asm
	{ atom.add.f64 %fd727,[%rd290],%fd408; }

	// end inline asm
	add.s64 	%rd291, %rd287, 32;
	// begin inline asm
	{ atom.add.f64 %fd729,[%rd291],%fd407; }

	// end inline asm
	add.s64 	%rd292, %rd287, 40;
	// begin inline asm
	{ atom.add.f64 %fd731,[%rd292],%fd406; }

	// end inline asm
	add.s64 	%rd293, %rd287, 48;
	// begin inline asm
	{ atom.add.f64 %fd733,[%rd293],%fd384; }

	// end inline asm
	add.s64 	%rd294, %rd287, 56;
	// begin inline asm
	{ atom.add.f64 %fd735,[%rd294],%fd383; }

	// end inline asm
	add.s64 	%rd295, %rd287, 64;
	// begin inline asm
	{ atom.add.f64 %fd737,[%rd295],%fd382; }

	// end inline asm

$L__BB12_42:
	ld.param.u64 	%rd34, [%rd23];
	ld.param.u32 	%r30, [%rd23+32];
	ld.param.u32 	%r31, [%rd23+60];
	add.s32 	%r32, %r3, 9;
	setp.le.s32 	%p28, %r31, %r32;
	selp.u16 	%rs53, 1, 0, %p28;
	shr.u32 	%r464, %r32, 31;
	cvt.u16.u32 	%rs54, %r464;
	or.b16  	%rs55, %rs53, %rs54;
	setp.eq.s16 	%p29, %rs55, 0;
	@%p29 bra 	$L__BB12_44;

	add.s32 	%r686, %r3, 9;
	st.local.v2.u32 	[%rd25], {%r686, %r31};
	mov.u64 	%rd297, $str$1;
	cvta.global.u64 	%rd298, %rd297;
	{ // callseq 266, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd298;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r465, [retval0+0];
	} // callseq 266
	bra.uni 	$L__BB12_45;

$L__BB12_44:
	mul.wide.s32 	%rd309, %r30, %r32;
	add.s64 	%rd300, %rd34, %rd309;
	// begin inline asm
	{ atom.add.f64 %fd739,[%rd300],%fd429; }

	// end inline asm
	add.s64 	%rd301, %rd300, 8;
	// begin inline asm
	{ atom.add.f64 %fd741,[%rd301],%fd428; }

	// end inline asm
	add.s64 	%rd302, %rd300, 16;
	// begin inline asm
	{ atom.add.f64 %fd743,[%rd302],%fd427; }

	// end inline asm
	add.s64 	%rd303, %rd300, 24;
	// begin inline asm
	{ atom.add.f64 %fd745,[%rd303],%fd405; }

	// end inline asm
	add.s64 	%rd304, %rd300, 32;
	// begin inline asm
	{ atom.add.f64 %fd747,[%rd304],%fd404; }

	// end inline asm
	add.s64 	%rd305, %rd300, 40;
	// begin inline asm
	{ atom.add.f64 %fd749,[%rd305],%fd403; }

	// end inline asm
	add.s64 	%rd306, %rd300, 48;
	// begin inline asm
	{ atom.add.f64 %fd751,[%rd306],%fd381; }

	// end inline asm
	add.s64 	%rd307, %rd300, 56;
	// begin inline asm
	{ atom.add.f64 %fd753,[%rd307],%fd380; }

	// end inline asm
	add.s64 	%rd308, %rd300, 64;
	// begin inline asm
	{ atom.add.f64 %fd755,[%rd308],%fd379; }

	// end inline asm

$L__BB12_45:
	ld.param.u64 	%rd35, [%rd23];
	ld.param.u32 	%r33, [%rd23+32];
	ld.param.u32 	%r34, [%rd23+60];
	add.s32 	%r35, %r3, 10;
	setp.le.s32 	%p30, %r34, %r35;
	selp.u16 	%rs56, 1, 0, %p30;
	shr.u32 	%r466, %r35, 31;
	cvt.u16.u32 	%rs57, %r466;
	or.b16  	%rs58, %rs56, %rs57;
	setp.eq.s16 	%p31, %rs58, 0;
	@%p31 bra 	$L__BB12_47;

	add.s32 	%r687, %r3, 10;
	st.local.v2.u32 	[%rd25], {%r687, %r34};
	mov.u64 	%rd310, $str$1;
	cvta.global.u64 	%rd311, %rd310;
	{ // callseq 267, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd311;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r467, [retval0+0];
	} // callseq 267
	bra.uni 	$L__BB12_48;

$L__BB12_47:
	mul.wide.s32 	%rd322, %r33, %r35;
	add.s64 	%rd313, %rd35, %rd322;
	// begin inline asm
	{ atom.add.f64 %fd757,[%rd313],%fd426; }

	// end inline asm
	add.s64 	%rd314, %rd313, 8;
	// begin inline asm
	{ atom.add.f64 %fd759,[%rd314],%fd425; }

	// end inline asm
	add.s64 	%rd315, %rd313, 16;
	// begin inline asm
	{ atom.add.f64 %fd761,[%rd315],%fd424; }

	// end inline asm
	add.s64 	%rd316, %rd313, 24;
	// begin inline asm
	{ atom.add.f64 %fd763,[%rd316],%fd402; }

	// end inline asm
	add.s64 	%rd317, %rd313, 32;
	// begin inline asm
	{ atom.add.f64 %fd765,[%rd317],%fd401; }

	// end inline asm
	add.s64 	%rd318, %rd313, 40;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd318],%fd400; }

	// end inline asm
	add.s64 	%rd319, %rd313, 48;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd319],%fd378; }

	// end inline asm
	add.s64 	%rd320, %rd313, 56;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd320],%fd377; }

	// end inline asm
	add.s64 	%rd321, %rd313, 64;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd321],%fd376; }

	// end inline asm

$L__BB12_48:
	ld.param.u64 	%rd36, [%rd23];
	ld.param.u32 	%r36, [%rd23+32];
	ld.param.u32 	%r37, [%rd23+60];
	add.s32 	%r38, %r3, 11;
	setp.le.s32 	%p32, %r37, %r38;
	selp.u16 	%rs59, 1, 0, %p32;
	shr.u32 	%r468, %r38, 31;
	cvt.u16.u32 	%rs60, %r468;
	or.b16  	%rs61, %rs59, %rs60;
	setp.eq.s16 	%p33, %rs61, 0;
	@%p33 bra 	$L__BB12_50;

	add.s32 	%r688, %r3, 11;
	st.local.v2.u32 	[%rd25], {%r688, %r37};
	mov.u64 	%rd323, $str$1;
	cvta.global.u64 	%rd324, %rd323;
	{ // callseq 268, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd324;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r469, [retval0+0];
	} // callseq 268
	bra.uni 	$L__BB12_51;

$L__BB12_50:
	mul.wide.s32 	%rd335, %r36, %r38;
	add.s64 	%rd326, %rd36, %rd335;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd326],%fd423; }

	// end inline asm
	add.s64 	%rd327, %rd326, 8;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd327],%fd422; }

	// end inline asm
	add.s64 	%rd328, %rd326, 16;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd328],%fd421; }

	// end inline asm
	add.s64 	%rd329, %rd326, 24;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd329],%fd399; }

	// end inline asm
	add.s64 	%rd330, %rd326, 32;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd330],%fd398; }

	// end inline asm
	add.s64 	%rd331, %rd326, 40;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd331],%fd397; }

	// end inline asm
	add.s64 	%rd332, %rd326, 48;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd332],%fd375; }

	// end inline asm
	add.s64 	%rd333, %rd326, 56;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd333],%fd374; }

	// end inline asm
	add.s64 	%rd334, %rd326, 64;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd334],%fd373; }

	// end inline asm

$L__BB12_51:
	ld.param.u64 	%rd37, [%rd23];
	ld.param.u32 	%r39, [%rd23+32];
	ld.param.u32 	%r40, [%rd23+60];
	add.s32 	%r41, %r3, 12;
	setp.le.s32 	%p34, %r40, %r41;
	selp.u16 	%rs62, 1, 0, %p34;
	shr.u32 	%r470, %r41, 31;
	cvt.u16.u32 	%rs63, %r470;
	or.b16  	%rs64, %rs62, %rs63;
	setp.eq.s16 	%p35, %rs64, 0;
	@%p35 bra 	$L__BB12_53;

	add.s32 	%r689, %r3, 12;
	st.local.v2.u32 	[%rd25], {%r689, %r40};
	mov.u64 	%rd336, $str$1;
	cvta.global.u64 	%rd337, %rd336;
	{ // callseq 269, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd337;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r471, [retval0+0];
	} // callseq 269
	bra.uni 	$L__BB12_54;

$L__BB12_53:
	mul.wide.s32 	%rd348, %r39, %r41;
	add.s64 	%rd339, %rd37, %rd348;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd339],%fd360; }

	// end inline asm
	add.s64 	%rd340, %rd339, 8;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd340],%fd359; }

	// end inline asm
	add.s64 	%rd341, %rd339, 16;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd341],%fd358; }

	// end inline asm
	add.s64 	%rd342, %rd339, 24;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd342],%fd336; }

	// end inline asm
	add.s64 	%rd343, %rd339, 32;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd343],%fd335; }

	// end inline asm
	add.s64 	%rd344, %rd339, 40;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd344],%fd334; }

	// end inline asm
	add.s64 	%rd345, %rd339, 48;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd345],%fd312; }

	// end inline asm
	add.s64 	%rd346, %rd339, 56;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd346],%fd311; }

	// end inline asm
	add.s64 	%rd347, %rd339, 64;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd347],%fd310; }

	// end inline asm

$L__BB12_54:
	ld.param.u64 	%rd38, [%rd23];
	ld.param.u32 	%r42, [%rd23+32];
	ld.param.u32 	%r43, [%rd23+60];
	add.s32 	%r44, %r3, 13;
	setp.le.s32 	%p36, %r43, %r44;
	selp.u16 	%rs65, 1, 0, %p36;
	shr.u32 	%r472, %r44, 31;
	cvt.u16.u32 	%rs66, %r472;
	or.b16  	%rs67, %rs65, %rs66;
	setp.eq.s16 	%p37, %rs67, 0;
	@%p37 bra 	$L__BB12_56;

	add.s32 	%r690, %r3, 13;
	st.local.v2.u32 	[%rd25], {%r690, %r43};
	mov.u64 	%rd349, $str$1;
	cvta.global.u64 	%rd350, %rd349;
	{ // callseq 270, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd350;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r473, [retval0+0];
	} // callseq 270
	bra.uni 	$L__BB12_57;

$L__BB12_56:
	mul.wide.s32 	%rd361, %r42, %r44;
	add.s64 	%rd352, %rd38, %rd361;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd352],%fd357; }

	// end inline asm
	add.s64 	%rd353, %rd352, 8;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd353],%fd356; }

	// end inline asm
	add.s64 	%rd354, %rd352, 16;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd354],%fd355; }

	// end inline asm
	add.s64 	%rd355, %rd352, 24;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd355],%fd333; }

	// end inline asm
	add.s64 	%rd356, %rd352, 32;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd356],%fd332; }

	// end inline asm
	add.s64 	%rd357, %rd352, 40;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd357],%fd331; }

	// end inline asm
	add.s64 	%rd358, %rd352, 48;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd358],%fd309; }

	// end inline asm
	add.s64 	%rd359, %rd352, 56;
	// begin inline asm
	{ atom.add.f64 %fd825,[%rd359],%fd308; }

	// end inline asm
	add.s64 	%rd360, %rd352, 64;
	// begin inline asm
	{ atom.add.f64 %fd827,[%rd360],%fd307; }

	// end inline asm

$L__BB12_57:
	ld.param.u64 	%rd39, [%rd23];
	ld.param.u32 	%r45, [%rd23+32];
	ld.param.u32 	%r46, [%rd23+60];
	add.s32 	%r47, %r3, 14;
	setp.le.s32 	%p38, %r46, %r47;
	selp.u16 	%rs68, 1, 0, %p38;
	shr.u32 	%r474, %r47, 31;
	cvt.u16.u32 	%rs69, %r474;
	or.b16  	%rs70, %rs68, %rs69;
	setp.eq.s16 	%p39, %rs70, 0;
	@%p39 bra 	$L__BB12_59;

	add.s32 	%r691, %r3, 14;
	st.local.v2.u32 	[%rd25], {%r691, %r46};
	mov.u64 	%rd362, $str$1;
	cvta.global.u64 	%rd363, %rd362;
	{ // callseq 271, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd363;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r475, [retval0+0];
	} // callseq 271
	bra.uni 	$L__BB12_60;

$L__BB12_59:
	mul.wide.s32 	%rd374, %r45, %r47;
	add.s64 	%rd365, %rd39, %rd374;
	// begin inline asm
	{ atom.add.f64 %fd829,[%rd365],%fd354; }

	// end inline asm
	add.s64 	%rd366, %rd365, 8;
	// begin inline asm
	{ atom.add.f64 %fd831,[%rd366],%fd353; }

	// end inline asm
	add.s64 	%rd367, %rd365, 16;
	// begin inline asm
	{ atom.add.f64 %fd833,[%rd367],%fd352; }

	// end inline asm
	add.s64 	%rd368, %rd365, 24;
	// begin inline asm
	{ atom.add.f64 %fd835,[%rd368],%fd330; }

	// end inline asm
	add.s64 	%rd369, %rd365, 32;
	// begin inline asm
	{ atom.add.f64 %fd837,[%rd369],%fd329; }

	// end inline asm
	add.s64 	%rd370, %rd365, 40;
	// begin inline asm
	{ atom.add.f64 %fd839,[%rd370],%fd328; }

	// end inline asm
	add.s64 	%rd371, %rd365, 48;
	// begin inline asm
	{ atom.add.f64 %fd841,[%rd371],%fd306; }

	// end inline asm
	add.s64 	%rd372, %rd365, 56;
	// begin inline asm
	{ atom.add.f64 %fd843,[%rd372],%fd305; }

	// end inline asm
	add.s64 	%rd373, %rd365, 64;
	// begin inline asm
	{ atom.add.f64 %fd845,[%rd373],%fd304; }

	// end inline asm

$L__BB12_60:
	ld.param.u64 	%rd40, [%rd23];
	ld.param.u32 	%r48, [%rd23+32];
	ld.param.u32 	%r49, [%rd23+60];
	add.s32 	%r50, %r3, 15;
	setp.le.s32 	%p40, %r49, %r50;
	selp.u16 	%rs71, 1, 0, %p40;
	shr.u32 	%r476, %r50, 31;
	cvt.u16.u32 	%rs72, %r476;
	or.b16  	%rs73, %rs71, %rs72;
	setp.eq.s16 	%p41, %rs73, 0;
	@%p41 bra 	$L__BB12_62;

	add.s32 	%r692, %r3, 15;
	st.local.v2.u32 	[%rd25], {%r692, %r49};
	mov.u64 	%rd375, $str$1;
	cvta.global.u64 	%rd376, %rd375;
	{ // callseq 272, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd376;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r477, [retval0+0];
	} // callseq 272
	bra.uni 	$L__BB12_63;

$L__BB12_62:
	mul.wide.s32 	%rd387, %r48, %r50;
	add.s64 	%rd378, %rd40, %rd387;
	// begin inline asm
	{ atom.add.f64 %fd847,[%rd378],%fd351; }

	// end inline asm
	add.s64 	%rd379, %rd378, 8;
	// begin inline asm
	{ atom.add.f64 %fd849,[%rd379],%fd350; }

	// end inline asm
	add.s64 	%rd380, %rd378, 16;
	// begin inline asm
	{ atom.add.f64 %fd851,[%rd380],%fd349; }

	// end inline asm
	add.s64 	%rd381, %rd378, 24;
	// begin inline asm
	{ atom.add.f64 %fd853,[%rd381],%fd327; }

	// end inline asm
	add.s64 	%rd382, %rd378, 32;
	// begin inline asm
	{ atom.add.f64 %fd855,[%rd382],%fd326; }

	// end inline asm
	add.s64 	%rd383, %rd378, 40;
	// begin inline asm
	{ atom.add.f64 %fd857,[%rd383],%fd325; }

	// end inline asm
	add.s64 	%rd384, %rd378, 48;
	// begin inline asm
	{ atom.add.f64 %fd859,[%rd384],%fd303; }

	// end inline asm
	add.s64 	%rd385, %rd378, 56;
	// begin inline asm
	{ atom.add.f64 %fd861,[%rd385],%fd302; }

	// end inline asm
	add.s64 	%rd386, %rd378, 64;
	// begin inline asm
	{ atom.add.f64 %fd863,[%rd386],%fd301; }

	// end inline asm

$L__BB12_63:
	setp.gt.s32 	%p299, %r375, 0;
	cvt.u32.u64 	%r817, %rd1213;
	selp.b32 	%r816, %r817, 0, %p299;
	cvt.s64.s32 	%rd1208, %r816;
	mul.lo.s64 	%rd389, %rd1208, %rd11;
	add.s64 	%rd41, %rd4, %rd389;
	ld.global.u32 	%r478, [%rd41];
	shl.b32 	%r51, %r478, 4;
	ld.param.u64 	%rd42, [%rd23];
	ld.param.u32 	%r52, [%rd23+32];
	ld.param.u32 	%r53, [%rd23+60];
	setp.le.s32 	%p42, %r53, %r51;
	selp.u16 	%rs74, 1, 0, %p42;
	shr.u32 	%r479, %r478, 27;
	cvt.u16.u32 	%rs75, %r479;
	and.b16  	%rs76, %rs75, 1;
	or.b16  	%rs77, %rs76, %rs74;
	setp.eq.s16 	%p43, %rs77, 0;
	@%p43 bra 	$L__BB12_65;

	st.local.v2.u32 	[%rd25], {%r51, %r53};
	mov.u64 	%rd390, $str$1;
	cvta.global.u64 	%rd391, %rd390;
	{ // callseq 273, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd391;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r480, [retval0+0];
	} // callseq 273
	bra.uni 	$L__BB12_66;

$L__BB12_65:
	mul.wide.s32 	%rd402, %r52, %r51;
	add.s64 	%rd393, %rd42, %rd402;
	// begin inline asm
	{ atom.add.f64 %fd865,[%rd393],%fd276; }

	// end inline asm
	add.s64 	%rd394, %rd393, 8;
	// begin inline asm
	{ atom.add.f64 %fd867,[%rd394],%fd275; }

	// end inline asm
	add.s64 	%rd395, %rd393, 16;
	// begin inline asm
	{ atom.add.f64 %fd869,[%rd395],%fd274; }

	// end inline asm
	add.s64 	%rd396, %rd393, 24;
	// begin inline asm
	{ atom.add.f64 %fd871,[%rd396],%fd252; }

	// end inline asm
	add.s64 	%rd397, %rd393, 32;
	// begin inline asm
	{ atom.add.f64 %fd873,[%rd397],%fd251; }

	// end inline asm
	add.s64 	%rd398, %rd393, 40;
	// begin inline asm
	{ atom.add.f64 %fd875,[%rd398],%fd250; }

	// end inline asm
	add.s64 	%rd399, %rd393, 48;
	// begin inline asm
	{ atom.add.f64 %fd877,[%rd399],%fd228; }

	// end inline asm
	add.s64 	%rd400, %rd393, 56;
	// begin inline asm
	{ atom.add.f64 %fd879,[%rd400],%fd227; }

	// end inline asm
	add.s64 	%rd401, %rd393, 64;
	// begin inline asm
	{ atom.add.f64 %fd881,[%rd401],%fd226; }

	// end inline asm

$L__BB12_66:
	ld.param.u64 	%rd43, [%rd23];
	ld.param.u32 	%r54, [%rd23+32];
	ld.param.u32 	%r55, [%rd23+60];
	add.s32 	%r56, %r51, 1;
	setp.le.s32 	%p44, %r55, %r56;
	selp.u16 	%rs78, 1, 0, %p44;
	shr.u32 	%r481, %r56, 31;
	cvt.u16.u32 	%rs79, %r481;
	or.b16  	%rs80, %rs78, %rs79;
	setp.eq.s16 	%p45, %rs80, 0;
	@%p45 bra 	$L__BB12_68;

	add.s32 	%r693, %r51, 1;
	st.local.v2.u32 	[%rd25], {%r693, %r55};
	mov.u64 	%rd403, $str$1;
	cvta.global.u64 	%rd404, %rd403;
	{ // callseq 274, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd404;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r482, [retval0+0];
	} // callseq 274
	bra.uni 	$L__BB12_69;

$L__BB12_68:
	mul.wide.s32 	%rd415, %r54, %r56;
	add.s64 	%rd406, %rd43, %rd415;
	// begin inline asm
	{ atom.add.f64 %fd883,[%rd406],%fd273; }

	// end inline asm
	add.s64 	%rd407, %rd406, 8;
	// begin inline asm
	{ atom.add.f64 %fd885,[%rd407],%fd272; }

	// end inline asm
	add.s64 	%rd408, %rd406, 16;
	// begin inline asm
	{ atom.add.f64 %fd887,[%rd408],%fd271; }

	// end inline asm
	add.s64 	%rd409, %rd406, 24;
	// begin inline asm
	{ atom.add.f64 %fd889,[%rd409],%fd249; }

	// end inline asm
	add.s64 	%rd410, %rd406, 32;
	// begin inline asm
	{ atom.add.f64 %fd891,[%rd410],%fd248; }

	// end inline asm
	add.s64 	%rd411, %rd406, 40;
	// begin inline asm
	{ atom.add.f64 %fd893,[%rd411],%fd247; }

	// end inline asm
	add.s64 	%rd412, %rd406, 48;
	// begin inline asm
	{ atom.add.f64 %fd895,[%rd412],%fd225; }

	// end inline asm
	add.s64 	%rd413, %rd406, 56;
	// begin inline asm
	{ atom.add.f64 %fd897,[%rd413],%fd224; }

	// end inline asm
	add.s64 	%rd414, %rd406, 64;
	// begin inline asm
	{ atom.add.f64 %fd899,[%rd414],%fd223; }

	// end inline asm

$L__BB12_69:
	ld.param.u64 	%rd44, [%rd23];
	ld.param.u32 	%r57, [%rd23+32];
	ld.param.u32 	%r58, [%rd23+60];
	add.s32 	%r59, %r51, 2;
	setp.le.s32 	%p46, %r58, %r59;
	selp.u16 	%rs81, 1, 0, %p46;
	shr.u32 	%r483, %r59, 31;
	cvt.u16.u32 	%rs82, %r483;
	or.b16  	%rs83, %rs81, %rs82;
	setp.eq.s16 	%p47, %rs83, 0;
	@%p47 bra 	$L__BB12_71;

	add.s32 	%r694, %r51, 2;
	st.local.v2.u32 	[%rd25], {%r694, %r58};
	mov.u64 	%rd416, $str$1;
	cvta.global.u64 	%rd417, %rd416;
	{ // callseq 275, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd417;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r484, [retval0+0];
	} // callseq 275
	bra.uni 	$L__BB12_72;

$L__BB12_71:
	mul.wide.s32 	%rd428, %r57, %r59;
	add.s64 	%rd419, %rd44, %rd428;
	// begin inline asm
	{ atom.add.f64 %fd901,[%rd419],%fd270; }

	// end inline asm
	add.s64 	%rd420, %rd419, 8;
	// begin inline asm
	{ atom.add.f64 %fd903,[%rd420],%fd269; }

	// end inline asm
	add.s64 	%rd421, %rd419, 16;
	// begin inline asm
	{ atom.add.f64 %fd905,[%rd421],%fd268; }

	// end inline asm
	add.s64 	%rd422, %rd419, 24;
	// begin inline asm
	{ atom.add.f64 %fd907,[%rd422],%fd246; }

	// end inline asm
	add.s64 	%rd423, %rd419, 32;
	// begin inline asm
	{ atom.add.f64 %fd909,[%rd423],%fd245; }

	// end inline asm
	add.s64 	%rd424, %rd419, 40;
	// begin inline asm
	{ atom.add.f64 %fd911,[%rd424],%fd244; }

	// end inline asm
	add.s64 	%rd425, %rd419, 48;
	// begin inline asm
	{ atom.add.f64 %fd913,[%rd425],%fd222; }

	// end inline asm
	add.s64 	%rd426, %rd419, 56;
	// begin inline asm
	{ atom.add.f64 %fd915,[%rd426],%fd221; }

	// end inline asm
	add.s64 	%rd427, %rd419, 64;
	// begin inline asm
	{ atom.add.f64 %fd917,[%rd427],%fd220; }

	// end inline asm

$L__BB12_72:
	ld.param.u64 	%rd45, [%rd23];
	ld.param.u32 	%r60, [%rd23+32];
	ld.param.u32 	%r61, [%rd23+60];
	add.s32 	%r62, %r51, 3;
	setp.le.s32 	%p48, %r61, %r62;
	selp.u16 	%rs84, 1, 0, %p48;
	shr.u32 	%r485, %r62, 31;
	cvt.u16.u32 	%rs85, %r485;
	or.b16  	%rs86, %rs84, %rs85;
	setp.eq.s16 	%p49, %rs86, 0;
	@%p49 bra 	$L__BB12_74;

	add.s32 	%r695, %r51, 3;
	st.local.v2.u32 	[%rd25], {%r695, %r61};
	mov.u64 	%rd429, $str$1;
	cvta.global.u64 	%rd430, %rd429;
	{ // callseq 276, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd430;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r486, [retval0+0];
	} // callseq 276
	bra.uni 	$L__BB12_75;

$L__BB12_74:
	mul.wide.s32 	%rd441, %r60, %r62;
	add.s64 	%rd432, %rd45, %rd441;
	// begin inline asm
	{ atom.add.f64 %fd919,[%rd432],%fd267; }

	// end inline asm
	add.s64 	%rd433, %rd432, 8;
	// begin inline asm
	{ atom.add.f64 %fd921,[%rd433],%fd266; }

	// end inline asm
	add.s64 	%rd434, %rd432, 16;
	// begin inline asm
	{ atom.add.f64 %fd923,[%rd434],%fd265; }

	// end inline asm
	add.s64 	%rd435, %rd432, 24;
	// begin inline asm
	{ atom.add.f64 %fd925,[%rd435],%fd243; }

	// end inline asm
	add.s64 	%rd436, %rd432, 32;
	// begin inline asm
	{ atom.add.f64 %fd927,[%rd436],%fd242; }

	// end inline asm
	add.s64 	%rd437, %rd432, 40;
	// begin inline asm
	{ atom.add.f64 %fd929,[%rd437],%fd241; }

	// end inline asm
	add.s64 	%rd438, %rd432, 48;
	// begin inline asm
	{ atom.add.f64 %fd931,[%rd438],%fd219; }

	// end inline asm
	add.s64 	%rd439, %rd432, 56;
	// begin inline asm
	{ atom.add.f64 %fd933,[%rd439],%fd218; }

	// end inline asm
	add.s64 	%rd440, %rd432, 64;
	// begin inline asm
	{ atom.add.f64 %fd935,[%rd440],%fd217; }

	// end inline asm

$L__BB12_75:
	ld.param.u64 	%rd46, [%rd23];
	ld.param.u32 	%r63, [%rd23+32];
	ld.param.u32 	%r64, [%rd23+60];
	add.s32 	%r65, %r51, 4;
	setp.le.s32 	%p50, %r64, %r65;
	selp.u16 	%rs87, 1, 0, %p50;
	shr.u32 	%r487, %r65, 31;
	cvt.u16.u32 	%rs88, %r487;
	or.b16  	%rs89, %rs87, %rs88;
	setp.eq.s16 	%p51, %rs89, 0;
	@%p51 bra 	$L__BB12_77;

	add.s32 	%r696, %r51, 4;
	st.local.v2.u32 	[%rd25], {%r696, %r64};
	mov.u64 	%rd442, $str$1;
	cvta.global.u64 	%rd443, %rd442;
	{ // callseq 277, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd443;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r488, [retval0+0];
	} // callseq 277
	bra.uni 	$L__BB12_78;

$L__BB12_77:
	mul.wide.s32 	%rd454, %r63, %r65;
	add.s64 	%rd445, %rd46, %rd454;
	// begin inline asm
	{ atom.add.f64 %fd937,[%rd445],%fd204; }

	// end inline asm
	add.s64 	%rd446, %rd445, 8;
	// begin inline asm
	{ atom.add.f64 %fd939,[%rd446],%fd203; }

	// end inline asm
	add.s64 	%rd447, %rd445, 16;
	// begin inline asm
	{ atom.add.f64 %fd941,[%rd447],%fd202; }

	// end inline asm
	add.s64 	%rd448, %rd445, 24;
	// begin inline asm
	{ atom.add.f64 %fd943,[%rd448],%fd180; }

	// end inline asm
	add.s64 	%rd449, %rd445, 32;
	// begin inline asm
	{ atom.add.f64 %fd945,[%rd449],%fd179; }

	// end inline asm
	add.s64 	%rd450, %rd445, 40;
	// begin inline asm
	{ atom.add.f64 %fd947,[%rd450],%fd178; }

	// end inline asm
	add.s64 	%rd451, %rd445, 48;
	// begin inline asm
	{ atom.add.f64 %fd949,[%rd451],%fd156; }

	// end inline asm
	add.s64 	%rd452, %rd445, 56;
	// begin inline asm
	{ atom.add.f64 %fd951,[%rd452],%fd155; }

	// end inline asm
	add.s64 	%rd453, %rd445, 64;
	// begin inline asm
	{ atom.add.f64 %fd953,[%rd453],%fd154; }

	// end inline asm

$L__BB12_78:
	ld.param.u64 	%rd47, [%rd23];
	ld.param.u32 	%r66, [%rd23+32];
	ld.param.u32 	%r67, [%rd23+60];
	add.s32 	%r68, %r51, 5;
	setp.le.s32 	%p52, %r67, %r68;
	selp.u16 	%rs90, 1, 0, %p52;
	shr.u32 	%r489, %r68, 31;
	cvt.u16.u32 	%rs91, %r489;
	or.b16  	%rs92, %rs90, %rs91;
	setp.eq.s16 	%p53, %rs92, 0;
	@%p53 bra 	$L__BB12_80;

	add.s32 	%r697, %r51, 5;
	st.local.v2.u32 	[%rd25], {%r697, %r67};
	mov.u64 	%rd455, $str$1;
	cvta.global.u64 	%rd456, %rd455;
	{ // callseq 278, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd456;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r490, [retval0+0];
	} // callseq 278
	bra.uni 	$L__BB12_81;

$L__BB12_80:
	mul.wide.s32 	%rd467, %r66, %r68;
	add.s64 	%rd458, %rd47, %rd467;
	// begin inline asm
	{ atom.add.f64 %fd955,[%rd458],%fd201; }

	// end inline asm
	add.s64 	%rd459, %rd458, 8;
	// begin inline asm
	{ atom.add.f64 %fd957,[%rd459],%fd200; }

	// end inline asm
	add.s64 	%rd460, %rd458, 16;
	// begin inline asm
	{ atom.add.f64 %fd959,[%rd460],%fd199; }

	// end inline asm
	add.s64 	%rd461, %rd458, 24;
	// begin inline asm
	{ atom.add.f64 %fd961,[%rd461],%fd177; }

	// end inline asm
	add.s64 	%rd462, %rd458, 32;
	// begin inline asm
	{ atom.add.f64 %fd963,[%rd462],%fd176; }

	// end inline asm
	add.s64 	%rd463, %rd458, 40;
	// begin inline asm
	{ atom.add.f64 %fd965,[%rd463],%fd175; }

	// end inline asm
	add.s64 	%rd464, %rd458, 48;
	// begin inline asm
	{ atom.add.f64 %fd967,[%rd464],%fd153; }

	// end inline asm
	add.s64 	%rd465, %rd458, 56;
	// begin inline asm
	{ atom.add.f64 %fd969,[%rd465],%fd152; }

	// end inline asm
	add.s64 	%rd466, %rd458, 64;
	// begin inline asm
	{ atom.add.f64 %fd971,[%rd466],%fd151; }

	// end inline asm

$L__BB12_81:
	ld.param.u64 	%rd48, [%rd23];
	ld.param.u32 	%r69, [%rd23+32];
	ld.param.u32 	%r70, [%rd23+60];
	add.s32 	%r71, %r51, 6;
	setp.le.s32 	%p54, %r70, %r71;
	selp.u16 	%rs93, 1, 0, %p54;
	shr.u32 	%r491, %r71, 31;
	cvt.u16.u32 	%rs94, %r491;
	or.b16  	%rs95, %rs93, %rs94;
	setp.eq.s16 	%p55, %rs95, 0;
	@%p55 bra 	$L__BB12_83;

	add.s32 	%r698, %r51, 6;
	st.local.v2.u32 	[%rd25], {%r698, %r70};
	mov.u64 	%rd468, $str$1;
	cvta.global.u64 	%rd469, %rd468;
	{ // callseq 279, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd469;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r492, [retval0+0];
	} // callseq 279
	bra.uni 	$L__BB12_84;

$L__BB12_83:
	mul.wide.s32 	%rd480, %r69, %r71;
	add.s64 	%rd471, %rd48, %rd480;
	// begin inline asm
	{ atom.add.f64 %fd973,[%rd471],%fd198; }

	// end inline asm
	add.s64 	%rd472, %rd471, 8;
	// begin inline asm
	{ atom.add.f64 %fd975,[%rd472],%fd197; }

	// end inline asm
	add.s64 	%rd473, %rd471, 16;
	// begin inline asm
	{ atom.add.f64 %fd977,[%rd473],%fd196; }

	// end inline asm
	add.s64 	%rd474, %rd471, 24;
	// begin inline asm
	{ atom.add.f64 %fd979,[%rd474],%fd174; }

	// end inline asm
	add.s64 	%rd475, %rd471, 32;
	// begin inline asm
	{ atom.add.f64 %fd981,[%rd475],%fd173; }

	// end inline asm
	add.s64 	%rd476, %rd471, 40;
	// begin inline asm
	{ atom.add.f64 %fd983,[%rd476],%fd172; }

	// end inline asm
	add.s64 	%rd477, %rd471, 48;
	// begin inline asm
	{ atom.add.f64 %fd985,[%rd477],%fd150; }

	// end inline asm
	add.s64 	%rd478, %rd471, 56;
	// begin inline asm
	{ atom.add.f64 %fd987,[%rd478],%fd149; }

	// end inline asm
	add.s64 	%rd479, %rd471, 64;
	// begin inline asm
	{ atom.add.f64 %fd989,[%rd479],%fd148; }

	// end inline asm

$L__BB12_84:
	ld.param.u64 	%rd49, [%rd23];
	ld.param.u32 	%r72, [%rd23+32];
	ld.param.u32 	%r73, [%rd23+60];
	add.s32 	%r74, %r51, 7;
	setp.le.s32 	%p56, %r73, %r74;
	selp.u16 	%rs96, 1, 0, %p56;
	shr.u32 	%r493, %r74, 31;
	cvt.u16.u32 	%rs97, %r493;
	or.b16  	%rs98, %rs96, %rs97;
	setp.eq.s16 	%p57, %rs98, 0;
	@%p57 bra 	$L__BB12_86;

	add.s32 	%r699, %r51, 7;
	st.local.v2.u32 	[%rd25], {%r699, %r73};
	mov.u64 	%rd481, $str$1;
	cvta.global.u64 	%rd482, %rd481;
	{ // callseq 280, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd482;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r494, [retval0+0];
	} // callseq 280
	bra.uni 	$L__BB12_87;

$L__BB12_86:
	mul.wide.s32 	%rd493, %r72, %r74;
	add.s64 	%rd484, %rd49, %rd493;
	// begin inline asm
	{ atom.add.f64 %fd991,[%rd484],%fd195; }

	// end inline asm
	add.s64 	%rd485, %rd484, 8;
	// begin inline asm
	{ atom.add.f64 %fd993,[%rd485],%fd194; }

	// end inline asm
	add.s64 	%rd486, %rd484, 16;
	// begin inline asm
	{ atom.add.f64 %fd995,[%rd486],%fd193; }

	// end inline asm
	add.s64 	%rd487, %rd484, 24;
	// begin inline asm
	{ atom.add.f64 %fd997,[%rd487],%fd171; }

	// end inline asm
	add.s64 	%rd488, %rd484, 32;
	// begin inline asm
	{ atom.add.f64 %fd999,[%rd488],%fd170; }

	// end inline asm
	add.s64 	%rd489, %rd484, 40;
	// begin inline asm
	{ atom.add.f64 %fd1001,[%rd489],%fd169; }

	// end inline asm
	add.s64 	%rd490, %rd484, 48;
	// begin inline asm
	{ atom.add.f64 %fd1003,[%rd490],%fd147; }

	// end inline asm
	add.s64 	%rd491, %rd484, 56;
	// begin inline asm
	{ atom.add.f64 %fd1005,[%rd491],%fd146; }

	// end inline asm
	add.s64 	%rd492, %rd484, 64;
	// begin inline asm
	{ atom.add.f64 %fd1007,[%rd492],%fd145; }

	// end inline asm

$L__BB12_87:
	ld.param.u64 	%rd50, [%rd23];
	ld.param.u32 	%r75, [%rd23+32];
	ld.param.u32 	%r76, [%rd23+60];
	add.s32 	%r77, %r51, 8;
	setp.le.s32 	%p58, %r76, %r77;
	selp.u16 	%rs99, 1, 0, %p58;
	shr.u32 	%r495, %r77, 31;
	cvt.u16.u32 	%rs100, %r495;
	or.b16  	%rs101, %rs99, %rs100;
	setp.eq.s16 	%p59, %rs101, 0;
	@%p59 bra 	$L__BB12_89;

	add.s32 	%r700, %r51, 8;
	st.local.v2.u32 	[%rd25], {%r700, %r76};
	mov.u64 	%rd494, $str$1;
	cvta.global.u64 	%rd495, %rd494;
	{ // callseq 281, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd495;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r496, [retval0+0];
	} // callseq 281
	bra.uni 	$L__BB12_90;

$L__BB12_89:
	mul.wide.s32 	%rd506, %r75, %r77;
	add.s64 	%rd497, %rd50, %rd506;
	// begin inline asm
	{ atom.add.f64 %fd1009,[%rd497],%fd132; }

	// end inline asm
	add.s64 	%rd498, %rd497, 8;
	// begin inline asm
	{ atom.add.f64 %fd1011,[%rd498],%fd131; }

	// end inline asm
	add.s64 	%rd499, %rd497, 16;
	// begin inline asm
	{ atom.add.f64 %fd1013,[%rd499],%fd130; }

	// end inline asm
	add.s64 	%rd500, %rd497, 24;
	// begin inline asm
	{ atom.add.f64 %fd1015,[%rd500],%fd108; }

	// end inline asm
	add.s64 	%rd501, %rd497, 32;
	// begin inline asm
	{ atom.add.f64 %fd1017,[%rd501],%fd107; }

	// end inline asm
	add.s64 	%rd502, %rd497, 40;
	// begin inline asm
	{ atom.add.f64 %fd1019,[%rd502],%fd106; }

	// end inline asm
	add.s64 	%rd503, %rd497, 48;
	// begin inline asm
	{ atom.add.f64 %fd1021,[%rd503],%fd84; }

	// end inline asm
	add.s64 	%rd504, %rd497, 56;
	// begin inline asm
	{ atom.add.f64 %fd1023,[%rd504],%fd83; }

	// end inline asm
	add.s64 	%rd505, %rd497, 64;
	// begin inline asm
	{ atom.add.f64 %fd1025,[%rd505],%fd82; }

	// end inline asm

$L__BB12_90:
	ld.param.u64 	%rd51, [%rd23];
	ld.param.u32 	%r78, [%rd23+32];
	ld.param.u32 	%r79, [%rd23+60];
	add.s32 	%r80, %r51, 9;
	setp.le.s32 	%p60, %r79, %r80;
	selp.u16 	%rs102, 1, 0, %p60;
	shr.u32 	%r497, %r80, 31;
	cvt.u16.u32 	%rs103, %r497;
	or.b16  	%rs104, %rs102, %rs103;
	setp.eq.s16 	%p61, %rs104, 0;
	@%p61 bra 	$L__BB12_92;

	add.s32 	%r701, %r51, 9;
	st.local.v2.u32 	[%rd25], {%r701, %r79};
	mov.u64 	%rd507, $str$1;
	cvta.global.u64 	%rd508, %rd507;
	{ // callseq 282, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd508;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r498, [retval0+0];
	} // callseq 282
	bra.uni 	$L__BB12_93;

$L__BB12_92:
	mul.wide.s32 	%rd519, %r78, %r80;
	add.s64 	%rd510, %rd51, %rd519;
	// begin inline asm
	{ atom.add.f64 %fd1027,[%rd510],%fd129; }

	// end inline asm
	add.s64 	%rd511, %rd510, 8;
	// begin inline asm
	{ atom.add.f64 %fd1029,[%rd511],%fd128; }

	// end inline asm
	add.s64 	%rd512, %rd510, 16;
	// begin inline asm
	{ atom.add.f64 %fd1031,[%rd512],%fd127; }

	// end inline asm
	add.s64 	%rd513, %rd510, 24;
	// begin inline asm
	{ atom.add.f64 %fd1033,[%rd513],%fd105; }

	// end inline asm
	add.s64 	%rd514, %rd510, 32;
	// begin inline asm
	{ atom.add.f64 %fd1035,[%rd514],%fd104; }

	// end inline asm
	add.s64 	%rd515, %rd510, 40;
	// begin inline asm
	{ atom.add.f64 %fd1037,[%rd515],%fd103; }

	// end inline asm
	add.s64 	%rd516, %rd510, 48;
	// begin inline asm
	{ atom.add.f64 %fd1039,[%rd516],%fd81; }

	// end inline asm
	add.s64 	%rd517, %rd510, 56;
	// begin inline asm
	{ atom.add.f64 %fd1041,[%rd517],%fd80; }

	// end inline asm
	add.s64 	%rd518, %rd510, 64;
	// begin inline asm
	{ atom.add.f64 %fd1043,[%rd518],%fd79; }

	// end inline asm

$L__BB12_93:
	ld.param.u64 	%rd52, [%rd23];
	ld.param.u32 	%r81, [%rd23+32];
	ld.param.u32 	%r82, [%rd23+60];
	add.s32 	%r83, %r51, 10;
	setp.le.s32 	%p62, %r82, %r83;
	selp.u16 	%rs105, 1, 0, %p62;
	shr.u32 	%r499, %r83, 31;
	cvt.u16.u32 	%rs106, %r499;
	or.b16  	%rs107, %rs105, %rs106;
	setp.eq.s16 	%p63, %rs107, 0;
	@%p63 bra 	$L__BB12_95;

	add.s32 	%r702, %r51, 10;
	st.local.v2.u32 	[%rd25], {%r702, %r82};
	mov.u64 	%rd520, $str$1;
	cvta.global.u64 	%rd521, %rd520;
	{ // callseq 283, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd521;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r500, [retval0+0];
	} // callseq 283
	bra.uni 	$L__BB12_96;

$L__BB12_95:
	mul.wide.s32 	%rd532, %r81, %r83;
	add.s64 	%rd523, %rd52, %rd532;
	// begin inline asm
	{ atom.add.f64 %fd1045,[%rd523],%fd126; }

	// end inline asm
	add.s64 	%rd524, %rd523, 8;
	// begin inline asm
	{ atom.add.f64 %fd1047,[%rd524],%fd125; }

	// end inline asm
	add.s64 	%rd525, %rd523, 16;
	// begin inline asm
	{ atom.add.f64 %fd1049,[%rd525],%fd124; }

	// end inline asm
	add.s64 	%rd526, %rd523, 24;
	// begin inline asm
	{ atom.add.f64 %fd1051,[%rd526],%fd102; }

	// end inline asm
	add.s64 	%rd527, %rd523, 32;
	// begin inline asm
	{ atom.add.f64 %fd1053,[%rd527],%fd101; }

	// end inline asm
	add.s64 	%rd528, %rd523, 40;
	// begin inline asm
	{ atom.add.f64 %fd1055,[%rd528],%fd100; }

	// end inline asm
	add.s64 	%rd529, %rd523, 48;
	// begin inline asm
	{ atom.add.f64 %fd1057,[%rd529],%fd78; }

	// end inline asm
	add.s64 	%rd530, %rd523, 56;
	// begin inline asm
	{ atom.add.f64 %fd1059,[%rd530],%fd77; }

	// end inline asm
	add.s64 	%rd531, %rd523, 64;
	// begin inline asm
	{ atom.add.f64 %fd1061,[%rd531],%fd76; }

	// end inline asm

$L__BB12_96:
	ld.param.u64 	%rd53, [%rd23];
	ld.param.u32 	%r84, [%rd23+32];
	ld.param.u32 	%r85, [%rd23+60];
	add.s32 	%r86, %r51, 11;
	setp.le.s32 	%p64, %r85, %r86;
	selp.u16 	%rs108, 1, 0, %p64;
	shr.u32 	%r501, %r86, 31;
	cvt.u16.u32 	%rs109, %r501;
	or.b16  	%rs110, %rs108, %rs109;
	setp.eq.s16 	%p65, %rs110, 0;
	@%p65 bra 	$L__BB12_98;

	add.s32 	%r703, %r51, 11;
	st.local.v2.u32 	[%rd25], {%r703, %r85};
	mov.u64 	%rd533, $str$1;
	cvta.global.u64 	%rd534, %rd533;
	{ // callseq 284, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd534;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r502, [retval0+0];
	} // callseq 284
	bra.uni 	$L__BB12_99;

$L__BB12_98:
	mul.wide.s32 	%rd545, %r84, %r86;
	add.s64 	%rd536, %rd53, %rd545;
	// begin inline asm
	{ atom.add.f64 %fd1063,[%rd536],%fd123; }

	// end inline asm
	add.s64 	%rd537, %rd536, 8;
	// begin inline asm
	{ atom.add.f64 %fd1065,[%rd537],%fd122; }

	// end inline asm
	add.s64 	%rd538, %rd536, 16;
	// begin inline asm
	{ atom.add.f64 %fd1067,[%rd538],%fd121; }

	// end inline asm
	add.s64 	%rd539, %rd536, 24;
	// begin inline asm
	{ atom.add.f64 %fd1069,[%rd539],%fd99; }

	// end inline asm
	add.s64 	%rd540, %rd536, 32;
	// begin inline asm
	{ atom.add.f64 %fd1071,[%rd540],%fd98; }

	// end inline asm
	add.s64 	%rd541, %rd536, 40;
	// begin inline asm
	{ atom.add.f64 %fd1073,[%rd541],%fd97; }

	// end inline asm
	add.s64 	%rd542, %rd536, 48;
	// begin inline asm
	{ atom.add.f64 %fd1075,[%rd542],%fd75; }

	// end inline asm
	add.s64 	%rd543, %rd536, 56;
	// begin inline asm
	{ atom.add.f64 %fd1077,[%rd543],%fd74; }

	// end inline asm
	add.s64 	%rd544, %rd536, 64;
	// begin inline asm
	{ atom.add.f64 %fd1079,[%rd544],%fd73; }

	// end inline asm

$L__BB12_99:
	ld.param.u64 	%rd54, [%rd23];
	ld.param.u32 	%r87, [%rd23+32];
	ld.param.u32 	%r88, [%rd23+60];
	add.s32 	%r89, %r51, 12;
	setp.le.s32 	%p66, %r88, %r89;
	selp.u16 	%rs111, 1, 0, %p66;
	shr.u32 	%r503, %r89, 31;
	cvt.u16.u32 	%rs112, %r503;
	or.b16  	%rs113, %rs111, %rs112;
	setp.eq.s16 	%p67, %rs113, 0;
	@%p67 bra 	$L__BB12_101;

	add.s32 	%r704, %r51, 12;
	st.local.v2.u32 	[%rd25], {%r704, %r88};
	mov.u64 	%rd546, $str$1;
	cvta.global.u64 	%rd547, %rd546;
	{ // callseq 285, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd547;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r504, [retval0+0];
	} // callseq 285
	bra.uni 	$L__BB12_102;

$L__BB12_101:
	mul.wide.s32 	%rd558, %r87, %r89;
	add.s64 	%rd549, %rd54, %rd558;
	// begin inline asm
	{ atom.add.f64 %fd1081,[%rd549],%fd60; }

	// end inline asm
	add.s64 	%rd550, %rd549, 8;
	// begin inline asm
	{ atom.add.f64 %fd1083,[%rd550],%fd59; }

	// end inline asm
	add.s64 	%rd551, %rd549, 16;
	// begin inline asm
	{ atom.add.f64 %fd1085,[%rd551],%fd58; }

	// end inline asm
	add.s64 	%rd552, %rd549, 24;
	// begin inline asm
	{ atom.add.f64 %fd1087,[%rd552],%fd36; }

	// end inline asm
	add.s64 	%rd553, %rd549, 32;
	// begin inline asm
	{ atom.add.f64 %fd1089,[%rd553],%fd35; }

	// end inline asm
	add.s64 	%rd554, %rd549, 40;
	// begin inline asm
	{ atom.add.f64 %fd1091,[%rd554],%fd34; }

	// end inline asm
	add.s64 	%rd555, %rd549, 48;
	// begin inline asm
	{ atom.add.f64 %fd1093,[%rd555],%fd12; }

	// end inline asm
	add.s64 	%rd556, %rd549, 56;
	// begin inline asm
	{ atom.add.f64 %fd1095,[%rd556],%fd11; }

	// end inline asm
	add.s64 	%rd557, %rd549, 64;
	// begin inline asm
	{ atom.add.f64 %fd1097,[%rd557],%fd10; }

	// end inline asm

$L__BB12_102:
	ld.param.u64 	%rd55, [%rd23];
	ld.param.u32 	%r90, [%rd23+32];
	ld.param.u32 	%r91, [%rd23+60];
	add.s32 	%r92, %r51, 13;
	setp.le.s32 	%p68, %r91, %r92;
	selp.u16 	%rs114, 1, 0, %p68;
	shr.u32 	%r505, %r92, 31;
	cvt.u16.u32 	%rs115, %r505;
	or.b16  	%rs116, %rs114, %rs115;
	setp.eq.s16 	%p69, %rs116, 0;
	@%p69 bra 	$L__BB12_104;

	add.s32 	%r705, %r51, 13;
	st.local.v2.u32 	[%rd25], {%r705, %r91};
	mov.u64 	%rd559, $str$1;
	cvta.global.u64 	%rd560, %rd559;
	{ // callseq 286, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd560;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r506, [retval0+0];
	} // callseq 286
	bra.uni 	$L__BB12_105;

$L__BB12_104:
	mul.wide.s32 	%rd571, %r90, %r92;
	add.s64 	%rd562, %rd55, %rd571;
	// begin inline asm
	{ atom.add.f64 %fd1099,[%rd562],%fd57; }

	// end inline asm
	add.s64 	%rd563, %rd562, 8;
	// begin inline asm
	{ atom.add.f64 %fd1101,[%rd563],%fd56; }

	// end inline asm
	add.s64 	%rd564, %rd562, 16;
	// begin inline asm
	{ atom.add.f64 %fd1103,[%rd564],%fd55; }

	// end inline asm
	add.s64 	%rd565, %rd562, 24;
	// begin inline asm
	{ atom.add.f64 %fd1105,[%rd565],%fd33; }

	// end inline asm
	add.s64 	%rd566, %rd562, 32;
	// begin inline asm
	{ atom.add.f64 %fd1107,[%rd566],%fd32; }

	// end inline asm
	add.s64 	%rd567, %rd562, 40;
	// begin inline asm
	{ atom.add.f64 %fd1109,[%rd567],%fd31; }

	// end inline asm
	add.s64 	%rd568, %rd562, 48;
	// begin inline asm
	{ atom.add.f64 %fd1111,[%rd568],%fd9; }

	// end inline asm
	add.s64 	%rd569, %rd562, 56;
	// begin inline asm
	{ atom.add.f64 %fd1113,[%rd569],%fd8; }

	// end inline asm
	add.s64 	%rd570, %rd562, 64;
	// begin inline asm
	{ atom.add.f64 %fd1115,[%rd570],%fd7; }

	// end inline asm

$L__BB12_105:
	ld.param.u64 	%rd56, [%rd23];
	ld.param.u32 	%r93, [%rd23+32];
	ld.param.u32 	%r94, [%rd23+60];
	add.s32 	%r95, %r51, 14;
	setp.le.s32 	%p70, %r94, %r95;
	selp.u16 	%rs117, 1, 0, %p70;
	shr.u32 	%r507, %r95, 31;
	cvt.u16.u32 	%rs118, %r507;
	or.b16  	%rs119, %rs117, %rs118;
	setp.eq.s16 	%p71, %rs119, 0;
	@%p71 bra 	$L__BB12_107;

	add.s32 	%r706, %r51, 14;
	st.local.v2.u32 	[%rd25], {%r706, %r94};
	mov.u64 	%rd572, $str$1;
	cvta.global.u64 	%rd573, %rd572;
	{ // callseq 287, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd573;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r508, [retval0+0];
	} // callseq 287
	bra.uni 	$L__BB12_108;

$L__BB12_107:
	mul.wide.s32 	%rd584, %r93, %r95;
	add.s64 	%rd575, %rd56, %rd584;
	// begin inline asm
	{ atom.add.f64 %fd1117,[%rd575],%fd54; }

	// end inline asm
	add.s64 	%rd576, %rd575, 8;
	// begin inline asm
	{ atom.add.f64 %fd1119,[%rd576],%fd53; }

	// end inline asm
	add.s64 	%rd577, %rd575, 16;
	// begin inline asm
	{ atom.add.f64 %fd1121,[%rd577],%fd52; }

	// end inline asm
	add.s64 	%rd578, %rd575, 24;
	// begin inline asm
	{ atom.add.f64 %fd1123,[%rd578],%fd30; }

	// end inline asm
	add.s64 	%rd579, %rd575, 32;
	// begin inline asm
	{ atom.add.f64 %fd1125,[%rd579],%fd29; }

	// end inline asm
	add.s64 	%rd580, %rd575, 40;
	// begin inline asm
	{ atom.add.f64 %fd1127,[%rd580],%fd28; }

	// end inline asm
	add.s64 	%rd581, %rd575, 48;
	// begin inline asm
	{ atom.add.f64 %fd1129,[%rd581],%fd6; }

	// end inline asm
	add.s64 	%rd582, %rd575, 56;
	// begin inline asm
	{ atom.add.f64 %fd1131,[%rd582],%fd5; }

	// end inline asm
	add.s64 	%rd583, %rd575, 64;
	// begin inline asm
	{ atom.add.f64 %fd1133,[%rd583],%fd4; }

	// end inline asm

$L__BB12_108:
	ld.param.u64 	%rd57, [%rd23];
	ld.param.u32 	%r96, [%rd23+32];
	ld.param.u32 	%r97, [%rd23+60];
	add.s32 	%r98, %r51, 15;
	setp.le.s32 	%p72, %r97, %r98;
	selp.u16 	%rs120, 1, 0, %p72;
	shr.u32 	%r509, %r98, 31;
	cvt.u16.u32 	%rs121, %r509;
	or.b16  	%rs122, %rs120, %rs121;
	setp.eq.s16 	%p73, %rs122, 0;
	@%p73 bra 	$L__BB12_110;

	add.s32 	%r707, %r51, 15;
	st.local.v2.u32 	[%rd25], {%r707, %r97};
	mov.u64 	%rd585, $str$1;
	cvta.global.u64 	%rd586, %rd585;
	{ // callseq 288, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd586;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r510, [retval0+0];
	} // callseq 288
	bra.uni 	$L__BB12_111;

$L__BB12_110:
	mul.wide.s32 	%rd597, %r96, %r98;
	add.s64 	%rd588, %rd57, %rd597;
	// begin inline asm
	{ atom.add.f64 %fd1135,[%rd588],%fd51; }

	// end inline asm
	add.s64 	%rd589, %rd588, 8;
	// begin inline asm
	{ atom.add.f64 %fd1137,[%rd589],%fd50; }

	// end inline asm
	add.s64 	%rd590, %rd588, 16;
	// begin inline asm
	{ atom.add.f64 %fd1139,[%rd590],%fd49; }

	// end inline asm
	add.s64 	%rd591, %rd588, 24;
	// begin inline asm
	{ atom.add.f64 %fd1141,[%rd591],%fd27; }

	// end inline asm
	add.s64 	%rd592, %rd588, 32;
	// begin inline asm
	{ atom.add.f64 %fd1143,[%rd592],%fd26; }

	// end inline asm
	add.s64 	%rd593, %rd588, 40;
	// begin inline asm
	{ atom.add.f64 %fd1145,[%rd593],%fd25; }

	// end inline asm
	add.s64 	%rd594, %rd588, 48;
	// begin inline asm
	{ atom.add.f64 %fd1147,[%rd594],%fd3; }

	// end inline asm
	add.s64 	%rd595, %rd588, 56;
	// begin inline asm
	{ atom.add.f64 %fd1149,[%rd595],%fd2; }

	// end inline asm
	add.s64 	%rd596, %rd588, 64;
	// begin inline asm
	{ atom.add.f64 %fd1151,[%rd596],%fd1; }

	// end inline asm

$L__BB12_111:
	ld.global.u32 	%r511, [%rd22];
	shl.b32 	%r99, %r511, 2;
	ld.global.u32 	%r512, [%rd41];
	shl.b32 	%r100, %r512, 2;
	ld.param.u64 	%rd59, [%rd159];
	ld.param.u32 	%r101, [%rd159+32];
	ld.param.u64 	%rd60, [%rd159+56];
	ld.param.u32 	%r102, [%rd159+88];
	ld.param.u64 	%rd61, [%rd159+112];
	ld.param.u32 	%r103, [%rd159+144];
	ld.param.u32 	%r104, [%rd159+172];
	ld.param.v2.u32 	{%r513, %r514}, [%rd159+176];
	setp.le.s32 	%p74, %r513, %r99;
	setp.le.s32 	%p75, %r514, %r100;
	shl.b32 	%r108, %r2, 4;
	setp.le.s32 	%p76, %r104, %r108;
	or.pred  	%p77, %p74, %p75;
	or.b32  	%r515, %r99, %r108;
	or.b32  	%r516, %r515, %r100;
	setp.lt.s32 	%p78, %r516, 0;
	or.pred  	%p79, %p78, %p77;
	or.pred  	%p80, %p76, %p79;
	@%p80 bra 	$L__BB12_113;
	bra.uni 	$L__BB12_112;

$L__BB12_113:
	st.local.v2.u32 	[%rd25], {%r99, %r100};
	st.local.v2.u32 	[%rd25+8], {%r108, %r513};
	st.local.v2.u32 	[%rd25+16], {%r514, %r104};
	mov.u64 	%rd615, $str$2;
	cvta.global.u64 	%rd616, %rd615;
	{ // callseq 289, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd616;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r517, [retval0+0];
	} // callseq 289
	bra.uni 	$L__BB12_114;

$L__BB12_112:
	cvta.to.global.u64 	%rd608, %rd60;
	mul.wide.s32 	%rd609, %r103, %r108;
	add.s64 	%rd599, %rd61, %rd609;
	// begin inline asm
	{ atom.add.f64 %fd1153,[%rd599],%fd564; }

	// end inline asm
	add.s64 	%rd600, %rd599, 8;
	// begin inline asm
	{ atom.add.f64 %fd1155,[%rd600],%fd563; }

	// end inline asm
	add.s64 	%rd601, %rd599, 16;
	// begin inline asm
	{ atom.add.f64 %fd1157,[%rd601],%fd562; }

	// end inline asm
	add.s64 	%rd602, %rd599, 24;
	// begin inline asm
	{ atom.add.f64 %fd1159,[%rd602],%fd540; }

	// end inline asm
	add.s64 	%rd603, %rd599, 32;
	// begin inline asm
	{ atom.add.f64 %fd1161,[%rd603],%fd539; }

	// end inline asm
	add.s64 	%rd604, %rd599, 40;
	// begin inline asm
	{ atom.add.f64 %fd1163,[%rd604],%fd538; }

	// end inline asm
	add.s64 	%rd605, %rd599, 48;
	// begin inline asm
	{ atom.add.f64 %fd1165,[%rd605],%fd516; }

	// end inline asm
	add.s64 	%rd606, %rd599, 56;
	// begin inline asm
	{ atom.add.f64 %fd1167,[%rd606],%fd515; }

	// end inline asm
	add.s64 	%rd607, %rd599, 64;
	// begin inline asm
	{ atom.add.f64 %fd1169,[%rd607],%fd514; }

	// end inline asm
	mul.wide.s32 	%rd610, %r101, %r108;
	cvta.to.global.u64 	%rd611, %rd59;
	add.s64 	%rd612, %rd611, %rd610;
	mul.wide.s32 	%rd613, %r102, %r108;
	add.s64 	%rd614, %rd608, %rd613;
	st.global.u32 	[%rd612], %r99;
	st.global.u32 	[%rd614], %r100;

$L__BB12_114:
	ld.param.u64 	%rd63, [%rd159];
	ld.param.u32 	%r109, [%rd159+32];
	ld.param.u64 	%rd64, [%rd159+56];
	ld.param.u32 	%r110, [%rd159+88];
	ld.param.u64 	%rd65, [%rd159+112];
	ld.param.u32 	%r111, [%rd159+144];
	ld.param.u32 	%r112, [%rd159+172];
	ld.param.v2.u32 	{%r518, %r519}, [%rd159+176];
	setp.le.s32 	%p81, %r518, %r99;
	add.s32 	%r116, %r100, 1;
	setp.le.s32 	%p82, %r519, %r116;
	add.s32 	%r117, %r108, 1;
	setp.le.s32 	%p83, %r112, %r117;
	or.pred  	%p84, %p81, %p82;
	or.b32  	%r520, %r99, %r117;
	or.b32  	%r521, %r520, %r116;
	setp.lt.s32 	%p85, %r521, 0;
	or.pred  	%p86, %p85, %p84;
	or.pred  	%p87, %p83, %p86;
	@%p87 bra 	$L__BB12_116;
	bra.uni 	$L__BB12_115;

$L__BB12_116:
	add.s32 	%r709, %r100, 1;
	st.local.v2.u32 	[%rd25], {%r99, %r709};
	add.s32 	%r710, %r108, 1;
	st.local.v2.u32 	[%rd25+8], {%r710, %r518};
	st.local.v2.u32 	[%rd25+16], {%r519, %r112};
	mov.u64 	%rd634, $str$2;
	cvta.global.u64 	%rd635, %rd634;
	{ // callseq 290, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd635;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r522, [retval0+0];
	} // callseq 290
	bra.uni 	$L__BB12_117;

$L__BB12_115:
	cvta.to.global.u64 	%rd627, %rd64;
	mul.wide.s32 	%rd628, %r111, %r117;
	add.s64 	%rd618, %rd65, %rd628;
	// begin inline asm
	{ atom.add.f64 %fd1171,[%rd618],%fd561; }

	// end inline asm
	add.s64 	%rd619, %rd618, 8;
	// begin inline asm
	{ atom.add.f64 %fd1173,[%rd619],%fd560; }

	// end inline asm
	add.s64 	%rd620, %rd618, 16;
	// begin inline asm
	{ atom.add.f64 %fd1175,[%rd620],%fd559; }

	// end inline asm
	add.s64 	%rd621, %rd618, 24;
	// begin inline asm
	{ atom.add.f64 %fd1177,[%rd621],%fd537; }

	// end inline asm
	add.s64 	%rd622, %rd618, 32;
	// begin inline asm
	{ atom.add.f64 %fd1179,[%rd622],%fd536; }

	// end inline asm
	add.s64 	%rd623, %rd618, 40;
	// begin inline asm
	{ atom.add.f64 %fd1181,[%rd623],%fd535; }

	// end inline asm
	add.s64 	%rd624, %rd618, 48;
	// begin inline asm
	{ atom.add.f64 %fd1183,[%rd624],%fd513; }

	// end inline asm
	add.s64 	%rd625, %rd618, 56;
	// begin inline asm
	{ atom.add.f64 %fd1185,[%rd625],%fd512; }

	// end inline asm
	add.s64 	%rd626, %rd618, 64;
	// begin inline asm
	{ atom.add.f64 %fd1187,[%rd626],%fd511; }

	// end inline asm
	mul.wide.s32 	%rd629, %r109, %r117;
	cvta.to.global.u64 	%rd630, %rd63;
	add.s64 	%rd631, %rd630, %rd629;
	mul.wide.s32 	%rd632, %r110, %r117;
	add.s64 	%rd633, %rd627, %rd632;
	st.global.u32 	[%rd631], %r99;
	add.s32 	%r708, %r100, 1;
	st.global.u32 	[%rd633], %r708;

$L__BB12_117:
	ld.param.u64 	%rd66, [%rd159];
	ld.param.u32 	%r118, [%rd159+32];
	ld.param.u64 	%rd67, [%rd159+56];
	ld.param.u32 	%r119, [%rd159+88];
	ld.param.u64 	%rd68, [%rd159+112];
	ld.param.u32 	%r120, [%rd159+144];
	ld.param.u32 	%r121, [%rd159+172];
	ld.param.v2.u32 	{%r523, %r524}, [%rd159+176];
	setp.le.s32 	%p88, %r523, %r99;
	add.s32 	%r125, %r100, 2;
	setp.le.s32 	%p89, %r524, %r125;
	add.s32 	%r126, %r108, 2;
	setp.le.s32 	%p90, %r121, %r126;
	or.pred  	%p91, %p88, %p89;
	or.b32  	%r525, %r99, %r126;
	or.b32  	%r526, %r525, %r125;
	setp.lt.s32 	%p92, %r526, 0;
	or.pred  	%p93, %p92, %p91;
	or.pred  	%p94, %p90, %p93;
	@%p94 bra 	$L__BB12_119;
	bra.uni 	$L__BB12_118;

$L__BB12_119:
	add.s32 	%r712, %r100, 2;
	st.local.v2.u32 	[%rd25], {%r99, %r712};
	add.s32 	%r713, %r108, 2;
	st.local.v2.u32 	[%rd25+8], {%r713, %r523};
	st.local.v2.u32 	[%rd25+16], {%r524, %r121};
	mov.u64 	%rd653, $str$2;
	cvta.global.u64 	%rd654, %rd653;
	{ // callseq 291, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd654;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r527, [retval0+0];
	} // callseq 291
	bra.uni 	$L__BB12_120;

$L__BB12_118:
	cvta.to.global.u64 	%rd646, %rd67;
	mul.wide.s32 	%rd647, %r120, %r126;
	add.s64 	%rd637, %rd68, %rd647;
	// begin inline asm
	{ atom.add.f64 %fd1189,[%rd637],%fd558; }

	// end inline asm
	add.s64 	%rd638, %rd637, 8;
	// begin inline asm
	{ atom.add.f64 %fd1191,[%rd638],%fd557; }

	// end inline asm
	add.s64 	%rd639, %rd637, 16;
	// begin inline asm
	{ atom.add.f64 %fd1193,[%rd639],%fd556; }

	// end inline asm
	add.s64 	%rd640, %rd637, 24;
	// begin inline asm
	{ atom.add.f64 %fd1195,[%rd640],%fd534; }

	// end inline asm
	add.s64 	%rd641, %rd637, 32;
	// begin inline asm
	{ atom.add.f64 %fd1197,[%rd641],%fd533; }

	// end inline asm
	add.s64 	%rd642, %rd637, 40;
	// begin inline asm
	{ atom.add.f64 %fd1199,[%rd642],%fd532; }

	// end inline asm
	add.s64 	%rd643, %rd637, 48;
	// begin inline asm
	{ atom.add.f64 %fd1201,[%rd643],%fd510; }

	// end inline asm
	add.s64 	%rd644, %rd637, 56;
	// begin inline asm
	{ atom.add.f64 %fd1203,[%rd644],%fd509; }

	// end inline asm
	add.s64 	%rd645, %rd637, 64;
	// begin inline asm
	{ atom.add.f64 %fd1205,[%rd645],%fd508; }

	// end inline asm
	mul.wide.s32 	%rd648, %r118, %r126;
	cvta.to.global.u64 	%rd649, %rd66;
	add.s64 	%rd650, %rd649, %rd648;
	mul.wide.s32 	%rd651, %r119, %r126;
	add.s64 	%rd652, %rd646, %rd651;
	st.global.u32 	[%rd650], %r99;
	add.s32 	%r711, %r100, 2;
	st.global.u32 	[%rd652], %r711;

$L__BB12_120:
	ld.param.u64 	%rd69, [%rd159];
	ld.param.u32 	%r127, [%rd159+32];
	ld.param.u64 	%rd70, [%rd159+56];
	ld.param.u32 	%r128, [%rd159+88];
	ld.param.u64 	%rd71, [%rd159+112];
	ld.param.u32 	%r129, [%rd159+144];
	ld.param.u32 	%r130, [%rd159+172];
	ld.param.v2.u32 	{%r528, %r529}, [%rd159+176];
	setp.le.s32 	%p95, %r528, %r99;
	add.s32 	%r134, %r100, 3;
	setp.le.s32 	%p96, %r529, %r134;
	add.s32 	%r135, %r108, 3;
	setp.le.s32 	%p97, %r130, %r135;
	or.pred  	%p98, %p95, %p96;
	or.b32  	%r530, %r99, %r135;
	or.b32  	%r531, %r530, %r134;
	setp.lt.s32 	%p99, %r531, 0;
	or.pred  	%p100, %p99, %p98;
	or.pred  	%p101, %p97, %p100;
	@%p101 bra 	$L__BB12_122;
	bra.uni 	$L__BB12_121;

$L__BB12_122:
	add.s32 	%r715, %r100, 3;
	st.local.v2.u32 	[%rd25], {%r99, %r715};
	add.s32 	%r716, %r108, 3;
	st.local.v2.u32 	[%rd25+8], {%r716, %r528};
	st.local.v2.u32 	[%rd25+16], {%r529, %r130};
	mov.u64 	%rd672, $str$2;
	cvta.global.u64 	%rd673, %rd672;
	{ // callseq 292, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd673;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r532, [retval0+0];
	} // callseq 292
	bra.uni 	$L__BB12_123;

$L__BB12_121:
	cvta.to.global.u64 	%rd665, %rd70;
	mul.wide.s32 	%rd666, %r129, %r135;
	add.s64 	%rd656, %rd71, %rd666;
	// begin inline asm
	{ atom.add.f64 %fd1207,[%rd656],%fd555; }

	// end inline asm
	add.s64 	%rd657, %rd656, 8;
	// begin inline asm
	{ atom.add.f64 %fd1209,[%rd657],%fd554; }

	// end inline asm
	add.s64 	%rd658, %rd656, 16;
	// begin inline asm
	{ atom.add.f64 %fd1211,[%rd658],%fd553; }

	// end inline asm
	add.s64 	%rd659, %rd656, 24;
	// begin inline asm
	{ atom.add.f64 %fd1213,[%rd659],%fd531; }

	// end inline asm
	add.s64 	%rd660, %rd656, 32;
	// begin inline asm
	{ atom.add.f64 %fd1215,[%rd660],%fd530; }

	// end inline asm
	add.s64 	%rd661, %rd656, 40;
	// begin inline asm
	{ atom.add.f64 %fd1217,[%rd661],%fd529; }

	// end inline asm
	add.s64 	%rd662, %rd656, 48;
	// begin inline asm
	{ atom.add.f64 %fd1219,[%rd662],%fd507; }

	// end inline asm
	add.s64 	%rd663, %rd656, 56;
	// begin inline asm
	{ atom.add.f64 %fd1221,[%rd663],%fd506; }

	// end inline asm
	add.s64 	%rd664, %rd656, 64;
	// begin inline asm
	{ atom.add.f64 %fd1223,[%rd664],%fd505; }

	// end inline asm
	mul.wide.s32 	%rd667, %r127, %r135;
	cvta.to.global.u64 	%rd668, %rd69;
	add.s64 	%rd669, %rd668, %rd667;
	mul.wide.s32 	%rd670, %r128, %r135;
	add.s64 	%rd671, %rd665, %rd670;
	st.global.u32 	[%rd669], %r99;
	add.s32 	%r714, %r100, 3;
	st.global.u32 	[%rd671], %r714;

$L__BB12_123:
	ld.param.u64 	%rd72, [%rd159];
	ld.param.u32 	%r136, [%rd159+32];
	ld.param.u64 	%rd73, [%rd159+56];
	ld.param.u32 	%r137, [%rd159+88];
	ld.param.u64 	%rd74, [%rd159+112];
	ld.param.u32 	%r138, [%rd159+144];
	ld.param.u32 	%r139, [%rd159+172];
	ld.param.v2.u32 	{%r533, %r534}, [%rd159+176];
	add.s32 	%r143, %r99, 1;
	setp.le.s32 	%p102, %r533, %r143;
	setp.le.s32 	%p103, %r534, %r100;
	add.s32 	%r144, %r108, 4;
	setp.le.s32 	%p104, %r139, %r144;
	or.pred  	%p105, %p102, %p103;
	or.b32  	%r535, %r100, %r144;
	or.b32  	%r536, %r535, %r143;
	setp.lt.s32 	%p106, %r536, 0;
	or.pred  	%p107, %p106, %p105;
	or.pred  	%p108, %p104, %p107;
	@%p108 bra 	$L__BB12_125;
	bra.uni 	$L__BB12_124;

$L__BB12_125:
	add.s32 	%r718, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r718, %r100};
	add.s32 	%r719, %r108, 4;
	st.local.v2.u32 	[%rd25+8], {%r719, %r533};
	st.local.v2.u32 	[%rd25+16], {%r534, %r139};
	mov.u64 	%rd691, $str$2;
	cvta.global.u64 	%rd692, %rd691;
	{ // callseq 293, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd692;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r537, [retval0+0];
	} // callseq 293
	bra.uni 	$L__BB12_126;

$L__BB12_124:
	cvta.to.global.u64 	%rd684, %rd73;
	mul.wide.s32 	%rd685, %r138, %r144;
	add.s64 	%rd675, %rd74, %rd685;
	// begin inline asm
	{ atom.add.f64 %fd1225,[%rd675],%fd492; }

	// end inline asm
	add.s64 	%rd676, %rd675, 8;
	// begin inline asm
	{ atom.add.f64 %fd1227,[%rd676],%fd491; }

	// end inline asm
	add.s64 	%rd677, %rd675, 16;
	// begin inline asm
	{ atom.add.f64 %fd1229,[%rd677],%fd490; }

	// end inline asm
	add.s64 	%rd678, %rd675, 24;
	// begin inline asm
	{ atom.add.f64 %fd1231,[%rd678],%fd468; }

	// end inline asm
	add.s64 	%rd679, %rd675, 32;
	// begin inline asm
	{ atom.add.f64 %fd1233,[%rd679],%fd467; }

	// end inline asm
	add.s64 	%rd680, %rd675, 40;
	// begin inline asm
	{ atom.add.f64 %fd1235,[%rd680],%fd466; }

	// end inline asm
	add.s64 	%rd681, %rd675, 48;
	// begin inline asm
	{ atom.add.f64 %fd1237,[%rd681],%fd444; }

	// end inline asm
	add.s64 	%rd682, %rd675, 56;
	// begin inline asm
	{ atom.add.f64 %fd1239,[%rd682],%fd443; }

	// end inline asm
	add.s64 	%rd683, %rd675, 64;
	// begin inline asm
	{ atom.add.f64 %fd1241,[%rd683],%fd442; }

	// end inline asm
	mul.wide.s32 	%rd686, %r136, %r144;
	cvta.to.global.u64 	%rd687, %rd72;
	add.s64 	%rd688, %rd687, %rd686;
	mul.wide.s32 	%rd689, %r137, %r144;
	add.s64 	%rd690, %rd684, %rd689;
	add.s32 	%r717, %r99, 1;
	st.global.u32 	[%rd688], %r717;
	st.global.u32 	[%rd690], %r100;

$L__BB12_126:
	ld.param.u64 	%rd75, [%rd159];
	ld.param.u32 	%r145, [%rd159+32];
	ld.param.u64 	%rd76, [%rd159+56];
	ld.param.u32 	%r146, [%rd159+88];
	ld.param.u64 	%rd77, [%rd159+112];
	ld.param.u32 	%r147, [%rd159+144];
	ld.param.u32 	%r148, [%rd159+172];
	ld.param.v2.u32 	{%r538, %r539}, [%rd159+176];
	setp.le.s32 	%p109, %r538, %r143;
	setp.le.s32 	%p110, %r539, %r116;
	add.s32 	%r152, %r108, 5;
	setp.le.s32 	%p111, %r148, %r152;
	or.pred  	%p112, %p109, %p110;
	or.b32  	%r540, %r143, %r152;
	or.b32  	%r541, %r540, %r116;
	setp.lt.s32 	%p113, %r541, 0;
	or.pred  	%p114, %p113, %p112;
	or.pred  	%p115, %p111, %p114;
	@%p115 bra 	$L__BB12_128;
	bra.uni 	$L__BB12_127;

$L__BB12_128:
	add.s32 	%r722, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r722, %r116};
	add.s32 	%r723, %r108, 5;
	st.local.v2.u32 	[%rd25+8], {%r723, %r538};
	st.local.v2.u32 	[%rd25+16], {%r539, %r148};
	mov.u64 	%rd710, $str$2;
	cvta.global.u64 	%rd711, %rd710;
	{ // callseq 294, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd711;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r542, [retval0+0];
	} // callseq 294
	bra.uni 	$L__BB12_129;

$L__BB12_127:
	cvta.to.global.u64 	%rd703, %rd76;
	mul.wide.s32 	%rd704, %r147, %r152;
	add.s64 	%rd694, %rd77, %rd704;
	// begin inline asm
	{ atom.add.f64 %fd1243,[%rd694],%fd489; }

	// end inline asm
	add.s64 	%rd695, %rd694, 8;
	// begin inline asm
	{ atom.add.f64 %fd1245,[%rd695],%fd488; }

	// end inline asm
	add.s64 	%rd696, %rd694, 16;
	// begin inline asm
	{ atom.add.f64 %fd1247,[%rd696],%fd487; }

	// end inline asm
	add.s64 	%rd697, %rd694, 24;
	// begin inline asm
	{ atom.add.f64 %fd1249,[%rd697],%fd465; }

	// end inline asm
	add.s64 	%rd698, %rd694, 32;
	// begin inline asm
	{ atom.add.f64 %fd1251,[%rd698],%fd464; }

	// end inline asm
	add.s64 	%rd699, %rd694, 40;
	// begin inline asm
	{ atom.add.f64 %fd1253,[%rd699],%fd463; }

	// end inline asm
	add.s64 	%rd700, %rd694, 48;
	// begin inline asm
	{ atom.add.f64 %fd1255,[%rd700],%fd441; }

	// end inline asm
	add.s64 	%rd701, %rd694, 56;
	// begin inline asm
	{ atom.add.f64 %fd1257,[%rd701],%fd440; }

	// end inline asm
	add.s64 	%rd702, %rd694, 64;
	// begin inline asm
	{ atom.add.f64 %fd1259,[%rd702],%fd439; }

	// end inline asm
	mul.wide.s32 	%rd705, %r145, %r152;
	cvta.to.global.u64 	%rd706, %rd75;
	add.s64 	%rd707, %rd706, %rd705;
	mul.wide.s32 	%rd708, %r146, %r152;
	add.s64 	%rd709, %rd703, %rd708;
	add.s32 	%r720, %r99, 1;
	st.global.u32 	[%rd707], %r720;
	add.s32 	%r721, %r100, 1;
	st.global.u32 	[%rd709], %r721;

$L__BB12_129:
	ld.param.u64 	%rd78, [%rd159];
	ld.param.u32 	%r153, [%rd159+32];
	ld.param.u64 	%rd79, [%rd159+56];
	ld.param.u32 	%r154, [%rd159+88];
	ld.param.u64 	%rd80, [%rd159+112];
	ld.param.u32 	%r155, [%rd159+144];
	ld.param.u32 	%r156, [%rd159+172];
	ld.param.v2.u32 	{%r543, %r544}, [%rd159+176];
	setp.le.s32 	%p116, %r543, %r143;
	setp.le.s32 	%p117, %r544, %r125;
	add.s32 	%r160, %r108, 6;
	setp.le.s32 	%p118, %r156, %r160;
	or.pred  	%p119, %p116, %p117;
	or.b32  	%r545, %r143, %r160;
	or.b32  	%r546, %r545, %r125;
	setp.lt.s32 	%p120, %r546, 0;
	or.pred  	%p121, %p120, %p119;
	or.pred  	%p122, %p118, %p121;
	@%p122 bra 	$L__BB12_131;
	bra.uni 	$L__BB12_130;

$L__BB12_131:
	add.s32 	%r726, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r726, %r125};
	add.s32 	%r727, %r108, 6;
	st.local.v2.u32 	[%rd25+8], {%r727, %r543};
	st.local.v2.u32 	[%rd25+16], {%r544, %r156};
	mov.u64 	%rd729, $str$2;
	cvta.global.u64 	%rd730, %rd729;
	{ // callseq 295, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd730;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r547, [retval0+0];
	} // callseq 295
	bra.uni 	$L__BB12_132;

$L__BB12_130:
	cvta.to.global.u64 	%rd722, %rd79;
	mul.wide.s32 	%rd723, %r155, %r160;
	add.s64 	%rd713, %rd80, %rd723;
	// begin inline asm
	{ atom.add.f64 %fd1261,[%rd713],%fd486; }

	// end inline asm
	add.s64 	%rd714, %rd713, 8;
	// begin inline asm
	{ atom.add.f64 %fd1263,[%rd714],%fd485; }

	// end inline asm
	add.s64 	%rd715, %rd713, 16;
	// begin inline asm
	{ atom.add.f64 %fd1265,[%rd715],%fd484; }

	// end inline asm
	add.s64 	%rd716, %rd713, 24;
	// begin inline asm
	{ atom.add.f64 %fd1267,[%rd716],%fd462; }

	// end inline asm
	add.s64 	%rd717, %rd713, 32;
	// begin inline asm
	{ atom.add.f64 %fd1269,[%rd717],%fd461; }

	// end inline asm
	add.s64 	%rd718, %rd713, 40;
	// begin inline asm
	{ atom.add.f64 %fd1271,[%rd718],%fd460; }

	// end inline asm
	add.s64 	%rd719, %rd713, 48;
	// begin inline asm
	{ atom.add.f64 %fd1273,[%rd719],%fd438; }

	// end inline asm
	add.s64 	%rd720, %rd713, 56;
	// begin inline asm
	{ atom.add.f64 %fd1275,[%rd720],%fd437; }

	// end inline asm
	add.s64 	%rd721, %rd713, 64;
	// begin inline asm
	{ atom.add.f64 %fd1277,[%rd721],%fd436; }

	// end inline asm
	mul.wide.s32 	%rd724, %r153, %r160;
	cvta.to.global.u64 	%rd725, %rd78;
	add.s64 	%rd726, %rd725, %rd724;
	mul.wide.s32 	%rd727, %r154, %r160;
	add.s64 	%rd728, %rd722, %rd727;
	add.s32 	%r724, %r99, 1;
	st.global.u32 	[%rd726], %r724;
	add.s32 	%r725, %r100, 2;
	st.global.u32 	[%rd728], %r725;

$L__BB12_132:
	ld.param.u64 	%rd81, [%rd159];
	ld.param.u32 	%r161, [%rd159+32];
	ld.param.u64 	%rd82, [%rd159+56];
	ld.param.u32 	%r162, [%rd159+88];
	ld.param.u64 	%rd83, [%rd159+112];
	ld.param.u32 	%r163, [%rd159+144];
	ld.param.u32 	%r164, [%rd159+172];
	ld.param.v2.u32 	{%r548, %r549}, [%rd159+176];
	setp.le.s32 	%p123, %r548, %r143;
	setp.le.s32 	%p124, %r549, %r134;
	add.s32 	%r168, %r108, 7;
	setp.le.s32 	%p125, %r164, %r168;
	or.pred  	%p126, %p123, %p124;
	or.b32  	%r550, %r143, %r168;
	or.b32  	%r551, %r550, %r134;
	setp.lt.s32 	%p127, %r551, 0;
	or.pred  	%p128, %p127, %p126;
	or.pred  	%p129, %p125, %p128;
	@%p129 bra 	$L__BB12_134;
	bra.uni 	$L__BB12_133;

$L__BB12_134:
	add.s32 	%r730, %r99, 1;
	st.local.v2.u32 	[%rd25], {%r730, %r134};
	add.s32 	%r731, %r108, 7;
	st.local.v2.u32 	[%rd25+8], {%r731, %r548};
	st.local.v2.u32 	[%rd25+16], {%r549, %r164};
	mov.u64 	%rd748, $str$2;
	cvta.global.u64 	%rd749, %rd748;
	{ // callseq 296, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd749;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r552, [retval0+0];
	} // callseq 296
	bra.uni 	$L__BB12_135;

$L__BB12_133:
	cvta.to.global.u64 	%rd741, %rd82;
	mul.wide.s32 	%rd742, %r163, %r168;
	add.s64 	%rd732, %rd83, %rd742;
	// begin inline asm
	{ atom.add.f64 %fd1279,[%rd732],%fd483; }

	// end inline asm
	add.s64 	%rd733, %rd732, 8;
	// begin inline asm
	{ atom.add.f64 %fd1281,[%rd733],%fd482; }

	// end inline asm
	add.s64 	%rd734, %rd732, 16;
	// begin inline asm
	{ atom.add.f64 %fd1283,[%rd734],%fd481; }

	// end inline asm
	add.s64 	%rd735, %rd732, 24;
	// begin inline asm
	{ atom.add.f64 %fd1285,[%rd735],%fd459; }

	// end inline asm
	add.s64 	%rd736, %rd732, 32;
	// begin inline asm
	{ atom.add.f64 %fd1287,[%rd736],%fd458; }

	// end inline asm
	add.s64 	%rd737, %rd732, 40;
	// begin inline asm
	{ atom.add.f64 %fd1289,[%rd737],%fd457; }

	// end inline asm
	add.s64 	%rd738, %rd732, 48;
	// begin inline asm
	{ atom.add.f64 %fd1291,[%rd738],%fd435; }

	// end inline asm
	add.s64 	%rd739, %rd732, 56;
	// begin inline asm
	{ atom.add.f64 %fd1293,[%rd739],%fd434; }

	// end inline asm
	add.s64 	%rd740, %rd732, 64;
	// begin inline asm
	{ atom.add.f64 %fd1295,[%rd740],%fd433; }

	// end inline asm
	mul.wide.s32 	%rd743, %r161, %r168;
	cvta.to.global.u64 	%rd744, %rd81;
	add.s64 	%rd745, %rd744, %rd743;
	mul.wide.s32 	%rd746, %r162, %r168;
	add.s64 	%rd747, %rd741, %rd746;
	add.s32 	%r728, %r99, 1;
	st.global.u32 	[%rd745], %r728;
	add.s32 	%r729, %r100, 3;
	st.global.u32 	[%rd747], %r729;

$L__BB12_135:
	ld.param.u64 	%rd84, [%rd159];
	ld.param.u32 	%r169, [%rd159+32];
	ld.param.u64 	%rd85, [%rd159+56];
	ld.param.u32 	%r170, [%rd159+88];
	ld.param.u64 	%rd86, [%rd159+112];
	ld.param.u32 	%r171, [%rd159+144];
	ld.param.u32 	%r172, [%rd159+172];
	ld.param.v2.u32 	{%r553, %r554}, [%rd159+176];
	add.s32 	%r176, %r99, 2;
	setp.le.s32 	%p130, %r553, %r176;
	setp.le.s32 	%p131, %r554, %r100;
	add.s32 	%r177, %r108, 8;
	setp.le.s32 	%p132, %r172, %r177;
	or.pred  	%p133, %p130, %p131;
	or.b32  	%r555, %r100, %r177;
	or.b32  	%r556, %r555, %r176;
	setp.lt.s32 	%p134, %r556, 0;
	or.pred  	%p135, %p134, %p133;
	or.pred  	%p136, %p132, %p135;
	@%p136 bra 	$L__BB12_137;
	bra.uni 	$L__BB12_136;

$L__BB12_137:
	add.s32 	%r733, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r733, %r100};
	add.s32 	%r734, %r108, 8;
	st.local.v2.u32 	[%rd25+8], {%r734, %r553};
	st.local.v2.u32 	[%rd25+16], {%r554, %r172};
	mov.u64 	%rd767, $str$2;
	cvta.global.u64 	%rd768, %rd767;
	{ // callseq 297, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd768;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r557, [retval0+0];
	} // callseq 297
	bra.uni 	$L__BB12_138;

$L__BB12_136:
	cvta.to.global.u64 	%rd760, %rd85;
	mul.wide.s32 	%rd761, %r171, %r177;
	add.s64 	%rd751, %rd86, %rd761;
	// begin inline asm
	{ atom.add.f64 %fd1297,[%rd751],%fd420; }

	// end inline asm
	add.s64 	%rd752, %rd751, 8;
	// begin inline asm
	{ atom.add.f64 %fd1299,[%rd752],%fd419; }

	// end inline asm
	add.s64 	%rd753, %rd751, 16;
	// begin inline asm
	{ atom.add.f64 %fd1301,[%rd753],%fd418; }

	// end inline asm
	add.s64 	%rd754, %rd751, 24;
	// begin inline asm
	{ atom.add.f64 %fd1303,[%rd754],%fd396; }

	// end inline asm
	add.s64 	%rd755, %rd751, 32;
	// begin inline asm
	{ atom.add.f64 %fd1305,[%rd755],%fd395; }

	// end inline asm
	add.s64 	%rd756, %rd751, 40;
	// begin inline asm
	{ atom.add.f64 %fd1307,[%rd756],%fd394; }

	// end inline asm
	add.s64 	%rd757, %rd751, 48;
	// begin inline asm
	{ atom.add.f64 %fd1309,[%rd757],%fd372; }

	// end inline asm
	add.s64 	%rd758, %rd751, 56;
	// begin inline asm
	{ atom.add.f64 %fd1311,[%rd758],%fd371; }

	// end inline asm
	add.s64 	%rd759, %rd751, 64;
	// begin inline asm
	{ atom.add.f64 %fd1313,[%rd759],%fd370; }

	// end inline asm
	mul.wide.s32 	%rd762, %r169, %r177;
	cvta.to.global.u64 	%rd763, %rd84;
	add.s64 	%rd764, %rd763, %rd762;
	mul.wide.s32 	%rd765, %r170, %r177;
	add.s64 	%rd766, %rd760, %rd765;
	add.s32 	%r732, %r99, 2;
	st.global.u32 	[%rd764], %r732;
	st.global.u32 	[%rd766], %r100;

$L__BB12_138:
	ld.param.u64 	%rd87, [%rd159];
	ld.param.u32 	%r178, [%rd159+32];
	ld.param.u64 	%rd88, [%rd159+56];
	ld.param.u32 	%r179, [%rd159+88];
	ld.param.u64 	%rd89, [%rd159+112];
	ld.param.u32 	%r180, [%rd159+144];
	ld.param.u32 	%r181, [%rd159+172];
	ld.param.v2.u32 	{%r558, %r559}, [%rd159+176];
	setp.le.s32 	%p137, %r558, %r176;
	setp.le.s32 	%p138, %r559, %r116;
	add.s32 	%r185, %r108, 9;
	setp.le.s32 	%p139, %r181, %r185;
	or.pred  	%p140, %p137, %p138;
	or.b32  	%r560, %r176, %r185;
	or.b32  	%r561, %r560, %r116;
	setp.lt.s32 	%p141, %r561, 0;
	or.pred  	%p142, %p141, %p140;
	or.pred  	%p143, %p139, %p142;
	@%p143 bra 	$L__BB12_140;
	bra.uni 	$L__BB12_139;

$L__BB12_140:
	add.s32 	%r737, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r737, %r116};
	add.s32 	%r738, %r108, 9;
	st.local.v2.u32 	[%rd25+8], {%r738, %r558};
	st.local.v2.u32 	[%rd25+16], {%r559, %r181};
	mov.u64 	%rd786, $str$2;
	cvta.global.u64 	%rd787, %rd786;
	{ // callseq 298, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd787;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r562, [retval0+0];
	} // callseq 298
	bra.uni 	$L__BB12_141;

$L__BB12_139:
	cvta.to.global.u64 	%rd779, %rd88;
	mul.wide.s32 	%rd780, %r180, %r185;
	add.s64 	%rd770, %rd89, %rd780;
	// begin inline asm
	{ atom.add.f64 %fd1315,[%rd770],%fd417; }

	// end inline asm
	add.s64 	%rd771, %rd770, 8;
	// begin inline asm
	{ atom.add.f64 %fd1317,[%rd771],%fd416; }

	// end inline asm
	add.s64 	%rd772, %rd770, 16;
	// begin inline asm
	{ atom.add.f64 %fd1319,[%rd772],%fd415; }

	// end inline asm
	add.s64 	%rd773, %rd770, 24;
	// begin inline asm
	{ atom.add.f64 %fd1321,[%rd773],%fd393; }

	// end inline asm
	add.s64 	%rd774, %rd770, 32;
	// begin inline asm
	{ atom.add.f64 %fd1323,[%rd774],%fd392; }

	// end inline asm
	add.s64 	%rd775, %rd770, 40;
	// begin inline asm
	{ atom.add.f64 %fd1325,[%rd775],%fd391; }

	// end inline asm
	add.s64 	%rd776, %rd770, 48;
	// begin inline asm
	{ atom.add.f64 %fd1327,[%rd776],%fd369; }

	// end inline asm
	add.s64 	%rd777, %rd770, 56;
	// begin inline asm
	{ atom.add.f64 %fd1329,[%rd777],%fd368; }

	// end inline asm
	add.s64 	%rd778, %rd770, 64;
	// begin inline asm
	{ atom.add.f64 %fd1331,[%rd778],%fd367; }

	// end inline asm
	mul.wide.s32 	%rd781, %r178, %r185;
	cvta.to.global.u64 	%rd782, %rd87;
	add.s64 	%rd783, %rd782, %rd781;
	mul.wide.s32 	%rd784, %r179, %r185;
	add.s64 	%rd785, %rd779, %rd784;
	add.s32 	%r735, %r99, 2;
	st.global.u32 	[%rd783], %r735;
	add.s32 	%r736, %r100, 1;
	st.global.u32 	[%rd785], %r736;

$L__BB12_141:
	ld.param.u64 	%rd90, [%rd159];
	ld.param.u32 	%r186, [%rd159+32];
	ld.param.u64 	%rd91, [%rd159+56];
	ld.param.u32 	%r187, [%rd159+88];
	ld.param.u64 	%rd92, [%rd159+112];
	ld.param.u32 	%r188, [%rd159+144];
	ld.param.u32 	%r189, [%rd159+172];
	ld.param.v2.u32 	{%r563, %r564}, [%rd159+176];
	setp.le.s32 	%p144, %r563, %r176;
	setp.le.s32 	%p145, %r564, %r125;
	add.s32 	%r193, %r108, 10;
	setp.le.s32 	%p146, %r189, %r193;
	or.pred  	%p147, %p144, %p145;
	or.b32  	%r565, %r176, %r193;
	or.b32  	%r566, %r565, %r125;
	setp.lt.s32 	%p148, %r566, 0;
	or.pred  	%p149, %p148, %p147;
	or.pred  	%p150, %p146, %p149;
	@%p150 bra 	$L__BB12_143;
	bra.uni 	$L__BB12_142;

$L__BB12_143:
	add.s32 	%r741, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r741, %r125};
	add.s32 	%r742, %r108, 10;
	st.local.v2.u32 	[%rd25+8], {%r742, %r563};
	st.local.v2.u32 	[%rd25+16], {%r564, %r189};
	mov.u64 	%rd805, $str$2;
	cvta.global.u64 	%rd806, %rd805;
	{ // callseq 299, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd806;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r567, [retval0+0];
	} // callseq 299
	bra.uni 	$L__BB12_144;

$L__BB12_142:
	cvta.to.global.u64 	%rd798, %rd91;
	mul.wide.s32 	%rd799, %r188, %r193;
	add.s64 	%rd789, %rd92, %rd799;
	// begin inline asm
	{ atom.add.f64 %fd1333,[%rd789],%fd414; }

	// end inline asm
	add.s64 	%rd790, %rd789, 8;
	// begin inline asm
	{ atom.add.f64 %fd1335,[%rd790],%fd413; }

	// end inline asm
	add.s64 	%rd791, %rd789, 16;
	// begin inline asm
	{ atom.add.f64 %fd1337,[%rd791],%fd412; }

	// end inline asm
	add.s64 	%rd792, %rd789, 24;
	// begin inline asm
	{ atom.add.f64 %fd1339,[%rd792],%fd390; }

	// end inline asm
	add.s64 	%rd793, %rd789, 32;
	// begin inline asm
	{ atom.add.f64 %fd1341,[%rd793],%fd389; }

	// end inline asm
	add.s64 	%rd794, %rd789, 40;
	// begin inline asm
	{ atom.add.f64 %fd1343,[%rd794],%fd388; }

	// end inline asm
	add.s64 	%rd795, %rd789, 48;
	// begin inline asm
	{ atom.add.f64 %fd1345,[%rd795],%fd366; }

	// end inline asm
	add.s64 	%rd796, %rd789, 56;
	// begin inline asm
	{ atom.add.f64 %fd1347,[%rd796],%fd365; }

	// end inline asm
	add.s64 	%rd797, %rd789, 64;
	// begin inline asm
	{ atom.add.f64 %fd1349,[%rd797],%fd364; }

	// end inline asm
	mul.wide.s32 	%rd800, %r186, %r193;
	cvta.to.global.u64 	%rd801, %rd90;
	add.s64 	%rd802, %rd801, %rd800;
	mul.wide.s32 	%rd803, %r187, %r193;
	add.s64 	%rd804, %rd798, %rd803;
	add.s32 	%r739, %r99, 2;
	st.global.u32 	[%rd802], %r739;
	add.s32 	%r740, %r100, 2;
	st.global.u32 	[%rd804], %r740;

$L__BB12_144:
	ld.param.u64 	%rd93, [%rd159];
	ld.param.u32 	%r194, [%rd159+32];
	ld.param.u64 	%rd94, [%rd159+56];
	ld.param.u32 	%r195, [%rd159+88];
	ld.param.u64 	%rd95, [%rd159+112];
	ld.param.u32 	%r196, [%rd159+144];
	ld.param.u32 	%r197, [%rd159+172];
	ld.param.v2.u32 	{%r568, %r569}, [%rd159+176];
	setp.le.s32 	%p151, %r568, %r176;
	setp.le.s32 	%p152, %r569, %r134;
	add.s32 	%r201, %r108, 11;
	setp.le.s32 	%p153, %r197, %r201;
	or.pred  	%p154, %p151, %p152;
	or.b32  	%r570, %r176, %r201;
	or.b32  	%r571, %r570, %r134;
	setp.lt.s32 	%p155, %r571, 0;
	or.pred  	%p156, %p155, %p154;
	or.pred  	%p157, %p153, %p156;
	@%p157 bra 	$L__BB12_146;
	bra.uni 	$L__BB12_145;

$L__BB12_146:
	add.s32 	%r745, %r99, 2;
	st.local.v2.u32 	[%rd25], {%r745, %r134};
	add.s32 	%r746, %r108, 11;
	st.local.v2.u32 	[%rd25+8], {%r746, %r568};
	st.local.v2.u32 	[%rd25+16], {%r569, %r197};
	mov.u64 	%rd824, $str$2;
	cvta.global.u64 	%rd825, %rd824;
	{ // callseq 300, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd825;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r572, [retval0+0];
	} // callseq 300
	bra.uni 	$L__BB12_147;

$L__BB12_145:
	cvta.to.global.u64 	%rd817, %rd94;
	mul.wide.s32 	%rd818, %r196, %r201;
	add.s64 	%rd808, %rd95, %rd818;
	// begin inline asm
	{ atom.add.f64 %fd1351,[%rd808],%fd411; }

	// end inline asm
	add.s64 	%rd809, %rd808, 8;
	// begin inline asm
	{ atom.add.f64 %fd1353,[%rd809],%fd410; }

	// end inline asm
	add.s64 	%rd810, %rd808, 16;
	// begin inline asm
	{ atom.add.f64 %fd1355,[%rd810],%fd409; }

	// end inline asm
	add.s64 	%rd811, %rd808, 24;
	// begin inline asm
	{ atom.add.f64 %fd1357,[%rd811],%fd387; }

	// end inline asm
	add.s64 	%rd812, %rd808, 32;
	// begin inline asm
	{ atom.add.f64 %fd1359,[%rd812],%fd386; }

	// end inline asm
	add.s64 	%rd813, %rd808, 40;
	// begin inline asm
	{ atom.add.f64 %fd1361,[%rd813],%fd385; }

	// end inline asm
	add.s64 	%rd814, %rd808, 48;
	// begin inline asm
	{ atom.add.f64 %fd1363,[%rd814],%fd363; }

	// end inline asm
	add.s64 	%rd815, %rd808, 56;
	// begin inline asm
	{ atom.add.f64 %fd1365,[%rd815],%fd362; }

	// end inline asm
	add.s64 	%rd816, %rd808, 64;
	// begin inline asm
	{ atom.add.f64 %fd1367,[%rd816],%fd361; }

	// end inline asm
	mul.wide.s32 	%rd819, %r194, %r201;
	cvta.to.global.u64 	%rd820, %rd93;
	add.s64 	%rd821, %rd820, %rd819;
	mul.wide.s32 	%rd822, %r195, %r201;
	add.s64 	%rd823, %rd817, %rd822;
	add.s32 	%r743, %r99, 2;
	st.global.u32 	[%rd821], %r743;
	add.s32 	%r744, %r100, 3;
	st.global.u32 	[%rd823], %r744;

$L__BB12_147:
	ld.param.u64 	%rd96, [%rd159];
	ld.param.u32 	%r202, [%rd159+32];
	ld.param.u64 	%rd97, [%rd159+56];
	ld.param.u32 	%r203, [%rd159+88];
	ld.param.u64 	%rd98, [%rd159+112];
	ld.param.u32 	%r204, [%rd159+144];
	ld.param.u32 	%r205, [%rd159+172];
	ld.param.v2.u32 	{%r573, %r574}, [%rd159+176];
	add.s32 	%r209, %r99, 3;
	setp.le.s32 	%p158, %r573, %r209;
	setp.le.s32 	%p159, %r574, %r100;
	add.s32 	%r210, %r108, 12;
	setp.le.s32 	%p160, %r205, %r210;
	or.pred  	%p161, %p158, %p159;
	or.b32  	%r575, %r100, %r210;
	or.b32  	%r576, %r575, %r209;
	setp.lt.s32 	%p162, %r576, 0;
	or.pred  	%p163, %p162, %p161;
	or.pred  	%p164, %p160, %p163;
	@%p164 bra 	$L__BB12_149;
	bra.uni 	$L__BB12_148;

$L__BB12_149:
	add.s32 	%r748, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r748, %r100};
	add.s32 	%r749, %r108, 12;
	st.local.v2.u32 	[%rd25+8], {%r749, %r573};
	st.local.v2.u32 	[%rd25+16], {%r574, %r205};
	mov.u64 	%rd843, $str$2;
	cvta.global.u64 	%rd844, %rd843;
	{ // callseq 301, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd844;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r577, [retval0+0];
	} // callseq 301
	bra.uni 	$L__BB12_150;

$L__BB12_148:
	cvta.to.global.u64 	%rd836, %rd97;
	mul.wide.s32 	%rd837, %r204, %r210;
	add.s64 	%rd827, %rd98, %rd837;
	// begin inline asm
	{ atom.add.f64 %fd1369,[%rd827],%fd348; }

	// end inline asm
	add.s64 	%rd828, %rd827, 8;
	// begin inline asm
	{ atom.add.f64 %fd1371,[%rd828],%fd347; }

	// end inline asm
	add.s64 	%rd829, %rd827, 16;
	// begin inline asm
	{ atom.add.f64 %fd1373,[%rd829],%fd346; }

	// end inline asm
	add.s64 	%rd830, %rd827, 24;
	// begin inline asm
	{ atom.add.f64 %fd1375,[%rd830],%fd324; }

	// end inline asm
	add.s64 	%rd831, %rd827, 32;
	// begin inline asm
	{ atom.add.f64 %fd1377,[%rd831],%fd323; }

	// end inline asm
	add.s64 	%rd832, %rd827, 40;
	// begin inline asm
	{ atom.add.f64 %fd1379,[%rd832],%fd322; }

	// end inline asm
	add.s64 	%rd833, %rd827, 48;
	// begin inline asm
	{ atom.add.f64 %fd1381,[%rd833],%fd300; }

	// end inline asm
	add.s64 	%rd834, %rd827, 56;
	// begin inline asm
	{ atom.add.f64 %fd1383,[%rd834],%fd299; }

	// end inline asm
	add.s64 	%rd835, %rd827, 64;
	// begin inline asm
	{ atom.add.f64 %fd1385,[%rd835],%fd298; }

	// end inline asm
	mul.wide.s32 	%rd838, %r202, %r210;
	cvta.to.global.u64 	%rd839, %rd96;
	add.s64 	%rd840, %rd839, %rd838;
	mul.wide.s32 	%rd841, %r203, %r210;
	add.s64 	%rd842, %rd836, %rd841;
	add.s32 	%r747, %r99, 3;
	st.global.u32 	[%rd840], %r747;
	st.global.u32 	[%rd842], %r100;

$L__BB12_150:
	ld.param.u64 	%rd99, [%rd159];
	ld.param.u32 	%r211, [%rd159+32];
	ld.param.u64 	%rd100, [%rd159+56];
	ld.param.u32 	%r212, [%rd159+88];
	ld.param.u64 	%rd101, [%rd159+112];
	ld.param.u32 	%r213, [%rd159+144];
	ld.param.u32 	%r214, [%rd159+172];
	ld.param.v2.u32 	{%r578, %r579}, [%rd159+176];
	setp.le.s32 	%p165, %r578, %r209;
	setp.le.s32 	%p166, %r579, %r116;
	add.s32 	%r218, %r108, 13;
	setp.le.s32 	%p167, %r214, %r218;
	or.pred  	%p168, %p165, %p166;
	or.b32  	%r580, %r209, %r218;
	or.b32  	%r581, %r580, %r116;
	setp.lt.s32 	%p169, %r581, 0;
	or.pred  	%p170, %p169, %p168;
	or.pred  	%p171, %p167, %p170;
	@%p171 bra 	$L__BB12_152;
	bra.uni 	$L__BB12_151;

$L__BB12_152:
	add.s32 	%r752, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r752, %r116};
	add.s32 	%r753, %r108, 13;
	st.local.v2.u32 	[%rd25+8], {%r753, %r578};
	st.local.v2.u32 	[%rd25+16], {%r579, %r214};
	mov.u64 	%rd862, $str$2;
	cvta.global.u64 	%rd863, %rd862;
	{ // callseq 302, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd863;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r582, [retval0+0];
	} // callseq 302
	bra.uni 	$L__BB12_153;

$L__BB12_151:
	cvta.to.global.u64 	%rd855, %rd100;
	mul.wide.s32 	%rd856, %r213, %r218;
	add.s64 	%rd846, %rd101, %rd856;
	// begin inline asm
	{ atom.add.f64 %fd1387,[%rd846],%fd345; }

	// end inline asm
	add.s64 	%rd847, %rd846, 8;
	// begin inline asm
	{ atom.add.f64 %fd1389,[%rd847],%fd344; }

	// end inline asm
	add.s64 	%rd848, %rd846, 16;
	// begin inline asm
	{ atom.add.f64 %fd1391,[%rd848],%fd343; }

	// end inline asm
	add.s64 	%rd849, %rd846, 24;
	// begin inline asm
	{ atom.add.f64 %fd1393,[%rd849],%fd321; }

	// end inline asm
	add.s64 	%rd850, %rd846, 32;
	// begin inline asm
	{ atom.add.f64 %fd1395,[%rd850],%fd320; }

	// end inline asm
	add.s64 	%rd851, %rd846, 40;
	// begin inline asm
	{ atom.add.f64 %fd1397,[%rd851],%fd319; }

	// end inline asm
	add.s64 	%rd852, %rd846, 48;
	// begin inline asm
	{ atom.add.f64 %fd1399,[%rd852],%fd297; }

	// end inline asm
	add.s64 	%rd853, %rd846, 56;
	// begin inline asm
	{ atom.add.f64 %fd1401,[%rd853],%fd296; }

	// end inline asm
	add.s64 	%rd854, %rd846, 64;
	// begin inline asm
	{ atom.add.f64 %fd1403,[%rd854],%fd295; }

	// end inline asm
	mul.wide.s32 	%rd857, %r211, %r218;
	cvta.to.global.u64 	%rd858, %rd99;
	add.s64 	%rd859, %rd858, %rd857;
	mul.wide.s32 	%rd860, %r212, %r218;
	add.s64 	%rd861, %rd855, %rd860;
	add.s32 	%r750, %r99, 3;
	st.global.u32 	[%rd859], %r750;
	add.s32 	%r751, %r100, 1;
	st.global.u32 	[%rd861], %r751;

$L__BB12_153:
	ld.param.u64 	%rd102, [%rd159];
	ld.param.u32 	%r219, [%rd159+32];
	ld.param.u64 	%rd103, [%rd159+56];
	ld.param.u32 	%r220, [%rd159+88];
	ld.param.u64 	%rd104, [%rd159+112];
	ld.param.u32 	%r221, [%rd159+144];
	ld.param.u32 	%r222, [%rd159+172];
	ld.param.v2.u32 	{%r583, %r584}, [%rd159+176];
	setp.le.s32 	%p172, %r583, %r209;
	setp.le.s32 	%p173, %r584, %r125;
	add.s32 	%r226, %r108, 14;
	setp.le.s32 	%p174, %r222, %r226;
	or.pred  	%p175, %p172, %p173;
	or.b32  	%r585, %r209, %r226;
	or.b32  	%r586, %r585, %r125;
	setp.lt.s32 	%p176, %r586, 0;
	or.pred  	%p177, %p176, %p175;
	or.pred  	%p178, %p174, %p177;
	@%p178 bra 	$L__BB12_155;
	bra.uni 	$L__BB12_154;

$L__BB12_155:
	add.s32 	%r756, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r756, %r125};
	add.s32 	%r757, %r108, 14;
	st.local.v2.u32 	[%rd25+8], {%r757, %r583};
	st.local.v2.u32 	[%rd25+16], {%r584, %r222};
	mov.u64 	%rd881, $str$2;
	cvta.global.u64 	%rd882, %rd881;
	{ // callseq 303, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd882;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r587, [retval0+0];
	} // callseq 303
	bra.uni 	$L__BB12_156;

$L__BB12_154:
	cvta.to.global.u64 	%rd874, %rd103;
	mul.wide.s32 	%rd875, %r221, %r226;
	add.s64 	%rd865, %rd104, %rd875;
	// begin inline asm
	{ atom.add.f64 %fd1405,[%rd865],%fd342; }

	// end inline asm
	add.s64 	%rd866, %rd865, 8;
	// begin inline asm
	{ atom.add.f64 %fd1407,[%rd866],%fd341; }

	// end inline asm
	add.s64 	%rd867, %rd865, 16;
	// begin inline asm
	{ atom.add.f64 %fd1409,[%rd867],%fd340; }

	// end inline asm
	add.s64 	%rd868, %rd865, 24;
	// begin inline asm
	{ atom.add.f64 %fd1411,[%rd868],%fd318; }

	// end inline asm
	add.s64 	%rd869, %rd865, 32;
	// begin inline asm
	{ atom.add.f64 %fd1413,[%rd869],%fd317; }

	// end inline asm
	add.s64 	%rd870, %rd865, 40;
	// begin inline asm
	{ atom.add.f64 %fd1415,[%rd870],%fd316; }

	// end inline asm
	add.s64 	%rd871, %rd865, 48;
	// begin inline asm
	{ atom.add.f64 %fd1417,[%rd871],%fd294; }

	// end inline asm
	add.s64 	%rd872, %rd865, 56;
	// begin inline asm
	{ atom.add.f64 %fd1419,[%rd872],%fd293; }

	// end inline asm
	add.s64 	%rd873, %rd865, 64;
	// begin inline asm
	{ atom.add.f64 %fd1421,[%rd873],%fd292; }

	// end inline asm
	mul.wide.s32 	%rd876, %r219, %r226;
	cvta.to.global.u64 	%rd877, %rd102;
	add.s64 	%rd878, %rd877, %rd876;
	mul.wide.s32 	%rd879, %r220, %r226;
	add.s64 	%rd880, %rd874, %rd879;
	add.s32 	%r754, %r99, 3;
	st.global.u32 	[%rd878], %r754;
	add.s32 	%r755, %r100, 2;
	st.global.u32 	[%rd880], %r755;

$L__BB12_156:
	ld.param.u64 	%rd105, [%rd159];
	ld.param.u32 	%r227, [%rd159+32];
	ld.param.u64 	%rd106, [%rd159+56];
	ld.param.u32 	%r228, [%rd159+88];
	ld.param.u64 	%rd107, [%rd159+112];
	ld.param.u32 	%r229, [%rd159+144];
	ld.param.u32 	%r230, [%rd159+172];
	ld.param.v2.u32 	{%r588, %r589}, [%rd159+176];
	setp.le.s32 	%p179, %r588, %r209;
	setp.le.s32 	%p180, %r589, %r134;
	add.s32 	%r234, %r108, 15;
	setp.le.s32 	%p181, %r230, %r234;
	or.pred  	%p182, %p179, %p180;
	or.b32  	%r590, %r209, %r234;
	or.b32  	%r591, %r590, %r134;
	setp.lt.s32 	%p183, %r591, 0;
	or.pred  	%p184, %p183, %p182;
	or.pred  	%p185, %p181, %p184;
	@%p185 bra 	$L__BB12_158;
	bra.uni 	$L__BB12_157;

$L__BB12_158:
	add.s32 	%r760, %r99, 3;
	st.local.v2.u32 	[%rd25], {%r760, %r134};
	add.s32 	%r761, %r108, 15;
	st.local.v2.u32 	[%rd25+8], {%r761, %r588};
	st.local.v2.u32 	[%rd25+16], {%r589, %r230};
	mov.u64 	%rd900, $str$2;
	cvta.global.u64 	%rd901, %rd900;
	{ // callseq 304, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd901;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r592, [retval0+0];
	} // callseq 304
	bra.uni 	$L__BB12_159;

$L__BB12_157:
	cvta.to.global.u64 	%rd893, %rd106;
	mul.wide.s32 	%rd894, %r229, %r234;
	add.s64 	%rd884, %rd107, %rd894;
	// begin inline asm
	{ atom.add.f64 %fd1423,[%rd884],%fd339; }

	// end inline asm
	add.s64 	%rd885, %rd884, 8;
	// begin inline asm
	{ atom.add.f64 %fd1425,[%rd885],%fd338; }

	// end inline asm
	add.s64 	%rd886, %rd884, 16;
	// begin inline asm
	{ atom.add.f64 %fd1427,[%rd886],%fd337; }

	// end inline asm
	add.s64 	%rd887, %rd884, 24;
	// begin inline asm
	{ atom.add.f64 %fd1429,[%rd887],%fd315; }

	// end inline asm
	add.s64 	%rd888, %rd884, 32;
	// begin inline asm
	{ atom.add.f64 %fd1431,[%rd888],%fd314; }

	// end inline asm
	add.s64 	%rd889, %rd884, 40;
	// begin inline asm
	{ atom.add.f64 %fd1433,[%rd889],%fd313; }

	// end inline asm
	add.s64 	%rd890, %rd884, 48;
	// begin inline asm
	{ atom.add.f64 %fd1435,[%rd890],%fd291; }

	// end inline asm
	add.s64 	%rd891, %rd884, 56;
	// begin inline asm
	{ atom.add.f64 %fd1437,[%rd891],%fd290; }

	// end inline asm
	add.s64 	%rd892, %rd884, 64;
	// begin inline asm
	{ atom.add.f64 %fd1439,[%rd892],%fd289; }

	// end inline asm
	mul.wide.s32 	%rd895, %r227, %r234;
	cvta.to.global.u64 	%rd896, %rd105;
	add.s64 	%rd897, %rd896, %rd895;
	mul.wide.s32 	%rd898, %r228, %r234;
	add.s64 	%rd899, %rd893, %rd898;
	add.s32 	%r758, %r99, 3;
	st.global.u32 	[%rd897], %r758;
	add.s32 	%r759, %r100, 3;
	st.global.u32 	[%rd899], %r759;

$L__BB12_159:
	add.s32 	%r593, %r2, %r403;
	shl.b32 	%r235, %r593, 4;
	ld.global.u32 	%r594, [%rd41];
	shl.b32 	%r236, %r594, 2;
	ld.global.u32 	%r595, [%rd22];
	shl.b32 	%r237, %r595, 2;
	ld.param.u64 	%rd108, [%rd159];
	ld.param.u32 	%r238, [%rd159+32];
	ld.param.u64 	%rd109, [%rd159+56];
	ld.param.u32 	%r239, [%rd159+88];
	ld.param.u64 	%rd110, [%rd159+112];
	ld.param.u32 	%r240, [%rd159+144];
	ld.param.u32 	%r241, [%rd159+172];
	ld.param.v2.u32 	{%r596, %r597}, [%rd159+176];
	setp.le.s32 	%p186, %r596, %r236;
	setp.le.s32 	%p187, %r597, %r237;
	setp.le.s32 	%p188, %r241, %r235;
	or.pred  	%p189, %p186, %p187;
	or.b32  	%r598, %r236, %r235;
	or.b32  	%r599, %r598, %r237;
	setp.lt.s32 	%p190, %r599, 0;
	or.pred  	%p191, %p190, %p189;
	or.pred  	%p192, %p188, %p191;
	@%p192 bra 	$L__BB12_161;
	bra.uni 	$L__BB12_160;

$L__BB12_161:
	st.local.v2.u32 	[%rd25], {%r236, %r237};
	st.local.v2.u32 	[%rd25+8], {%r235, %r596};
	st.local.v2.u32 	[%rd25+16], {%r597, %r241};
	mov.u64 	%rd919, $str$2;
	cvta.global.u64 	%rd920, %rd919;
	{ // callseq 305, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd920;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r600, [retval0+0];
	} // callseq 305
	bra.uni 	$L__BB12_162;

$L__BB12_160:
	cvta.to.global.u64 	%rd912, %rd109;
	mul.wide.s32 	%rd913, %r240, %r235;
	add.s64 	%rd903, %rd110, %rd913;
	// begin inline asm
	{ atom.add.f64 %fd1441,[%rd903],%fd288; }

	// end inline asm
	add.s64 	%rd904, %rd903, 8;
	// begin inline asm
	{ atom.add.f64 %fd1443,[%rd904],%fd287; }

	// end inline asm
	add.s64 	%rd905, %rd903, 16;
	// begin inline asm
	{ atom.add.f64 %fd1445,[%rd905],%fd286; }

	// end inline asm
	add.s64 	%rd906, %rd903, 24;
	// begin inline asm
	{ atom.add.f64 %fd1447,[%rd906],%fd264; }

	// end inline asm
	add.s64 	%rd907, %rd903, 32;
	// begin inline asm
	{ atom.add.f64 %fd1449,[%rd907],%fd263; }

	// end inline asm
	add.s64 	%rd908, %rd903, 40;
	// begin inline asm
	{ atom.add.f64 %fd1451,[%rd908],%fd262; }

	// end inline asm
	add.s64 	%rd909, %rd903, 48;
	// begin inline asm
	{ atom.add.f64 %fd1453,[%rd909],%fd240; }

	// end inline asm
	add.s64 	%rd910, %rd903, 56;
	// begin inline asm
	{ atom.add.f64 %fd1455,[%rd910],%fd239; }

	// end inline asm
	add.s64 	%rd911, %rd903, 64;
	// begin inline asm
	{ atom.add.f64 %fd1457,[%rd911],%fd238; }

	// end inline asm
	mul.wide.s32 	%rd914, %r238, %r235;
	cvta.to.global.u64 	%rd915, %rd108;
	add.s64 	%rd916, %rd915, %rd914;
	mul.wide.s32 	%rd917, %r239, %r235;
	add.s64 	%rd918, %rd912, %rd917;
	st.global.u32 	[%rd916], %r236;
	st.global.u32 	[%rd918], %r237;

$L__BB12_162:
	ld.param.u64 	%rd111, [%rd159];
	ld.param.u32 	%r245, [%rd159+32];
	ld.param.u64 	%rd112, [%rd159+56];
	ld.param.u32 	%r246, [%rd159+88];
	ld.param.u64 	%rd113, [%rd159+112];
	ld.param.u32 	%r247, [%rd159+144];
	ld.param.u32 	%r248, [%rd159+172];
	ld.param.v2.u32 	{%r601, %r602}, [%rd159+176];
	setp.le.s32 	%p193, %r601, %r236;
	add.s32 	%r252, %r237, 1;
	setp.le.s32 	%p194, %r602, %r252;
	add.s32 	%r253, %r235, 1;
	setp.le.s32 	%p195, %r248, %r253;
	or.pred  	%p196, %p193, %p194;
	or.b32  	%r603, %r236, %r253;
	or.b32  	%r604, %r603, %r252;
	setp.lt.s32 	%p197, %r604, 0;
	or.pred  	%p198, %p197, %p196;
	or.pred  	%p199, %p195, %p198;
	@%p199 bra 	$L__BB12_164;
	bra.uni 	$L__BB12_163;

$L__BB12_164:
	add.s32 	%r763, %r237, 1;
	st.local.v2.u32 	[%rd25], {%r236, %r763};
	add.s32 	%r764, %r235, 1;
	st.local.v2.u32 	[%rd25+8], {%r764, %r601};
	st.local.v2.u32 	[%rd25+16], {%r602, %r248};
	mov.u64 	%rd938, $str$2;
	cvta.global.u64 	%rd939, %rd938;
	{ // callseq 306, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd939;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r605, [retval0+0];
	} // callseq 306
	bra.uni 	$L__BB12_165;

$L__BB12_163:
	cvta.to.global.u64 	%rd931, %rd112;
	mul.wide.s32 	%rd932, %r247, %r253;
	add.s64 	%rd922, %rd113, %rd932;
	// begin inline asm
	{ atom.add.f64 %fd1459,[%rd922],%fd285; }

	// end inline asm
	add.s64 	%rd923, %rd922, 8;
	// begin inline asm
	{ atom.add.f64 %fd1461,[%rd923],%fd284; }

	// end inline asm
	add.s64 	%rd924, %rd922, 16;
	// begin inline asm
	{ atom.add.f64 %fd1463,[%rd924],%fd283; }

	// end inline asm
	add.s64 	%rd925, %rd922, 24;
	// begin inline asm
	{ atom.add.f64 %fd1465,[%rd925],%fd261; }

	// end inline asm
	add.s64 	%rd926, %rd922, 32;
	// begin inline asm
	{ atom.add.f64 %fd1467,[%rd926],%fd260; }

	// end inline asm
	add.s64 	%rd927, %rd922, 40;
	// begin inline asm
	{ atom.add.f64 %fd1469,[%rd927],%fd259; }

	// end inline asm
	add.s64 	%rd928, %rd922, 48;
	// begin inline asm
	{ atom.add.f64 %fd1471,[%rd928],%fd237; }

	// end inline asm
	add.s64 	%rd929, %rd922, 56;
	// begin inline asm
	{ atom.add.f64 %fd1473,[%rd929],%fd236; }

	// end inline asm
	add.s64 	%rd930, %rd922, 64;
	// begin inline asm
	{ atom.add.f64 %fd1475,[%rd930],%fd235; }

	// end inline asm
	mul.wide.s32 	%rd933, %r245, %r253;
	cvta.to.global.u64 	%rd934, %rd111;
	add.s64 	%rd935, %rd934, %rd933;
	mul.wide.s32 	%rd936, %r246, %r253;
	add.s64 	%rd937, %rd931, %rd936;
	st.global.u32 	[%rd935], %r236;
	add.s32 	%r762, %r237, 1;
	st.global.u32 	[%rd937], %r762;

$L__BB12_165:
	ld.param.u64 	%rd114, [%rd159];
	ld.param.u32 	%r254, [%rd159+32];
	ld.param.u64 	%rd115, [%rd159+56];
	ld.param.u32 	%r255, [%rd159+88];
	ld.param.u64 	%rd116, [%rd159+112];
	ld.param.u32 	%r256, [%rd159+144];
	ld.param.u32 	%r257, [%rd159+172];
	ld.param.v2.u32 	{%r606, %r607}, [%rd159+176];
	setp.le.s32 	%p200, %r606, %r236;
	add.s32 	%r261, %r237, 2;
	setp.le.s32 	%p201, %r607, %r261;
	add.s32 	%r262, %r235, 2;
	setp.le.s32 	%p202, %r257, %r262;
	or.pred  	%p203, %p200, %p201;
	or.b32  	%r608, %r236, %r262;
	or.b32  	%r609, %r608, %r261;
	setp.lt.s32 	%p204, %r609, 0;
	or.pred  	%p205, %p204, %p203;
	or.pred  	%p206, %p202, %p205;
	@%p206 bra 	$L__BB12_167;
	bra.uni 	$L__BB12_166;

$L__BB12_167:
	add.s32 	%r766, %r237, 2;
	st.local.v2.u32 	[%rd25], {%r236, %r766};
	add.s32 	%r767, %r235, 2;
	st.local.v2.u32 	[%rd25+8], {%r767, %r606};
	st.local.v2.u32 	[%rd25+16], {%r607, %r257};
	mov.u64 	%rd957, $str$2;
	cvta.global.u64 	%rd958, %rd957;
	{ // callseq 307, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd958;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r610, [retval0+0];
	} // callseq 307
	bra.uni 	$L__BB12_168;

$L__BB12_166:
	cvta.to.global.u64 	%rd950, %rd115;
	mul.wide.s32 	%rd951, %r256, %r262;
	add.s64 	%rd941, %rd116, %rd951;
	// begin inline asm
	{ atom.add.f64 %fd1477,[%rd941],%fd282; }

	// end inline asm
	add.s64 	%rd942, %rd941, 8;
	// begin inline asm
	{ atom.add.f64 %fd1479,[%rd942],%fd281; }

	// end inline asm
	add.s64 	%rd943, %rd941, 16;
	// begin inline asm
	{ atom.add.f64 %fd1481,[%rd943],%fd280; }

	// end inline asm
	add.s64 	%rd944, %rd941, 24;
	// begin inline asm
	{ atom.add.f64 %fd1483,[%rd944],%fd258; }

	// end inline asm
	add.s64 	%rd945, %rd941, 32;
	// begin inline asm
	{ atom.add.f64 %fd1485,[%rd945],%fd257; }

	// end inline asm
	add.s64 	%rd946, %rd941, 40;
	// begin inline asm
	{ atom.add.f64 %fd1487,[%rd946],%fd256; }

	// end inline asm
	add.s64 	%rd947, %rd941, 48;
	// begin inline asm
	{ atom.add.f64 %fd1489,[%rd947],%fd234; }

	// end inline asm
	add.s64 	%rd948, %rd941, 56;
	// begin inline asm
	{ atom.add.f64 %fd1491,[%rd948],%fd233; }

	// end inline asm
	add.s64 	%rd949, %rd941, 64;
	// begin inline asm
	{ atom.add.f64 %fd1493,[%rd949],%fd232; }

	// end inline asm
	mul.wide.s32 	%rd952, %r254, %r262;
	cvta.to.global.u64 	%rd953, %rd114;
	add.s64 	%rd954, %rd953, %rd952;
	mul.wide.s32 	%rd955, %r255, %r262;
	add.s64 	%rd956, %rd950, %rd955;
	st.global.u32 	[%rd954], %r236;
	add.s32 	%r765, %r237, 2;
	st.global.u32 	[%rd956], %r765;

$L__BB12_168:
	ld.param.u64 	%rd117, [%rd159];
	ld.param.u32 	%r263, [%rd159+32];
	ld.param.u64 	%rd118, [%rd159+56];
	ld.param.u32 	%r264, [%rd159+88];
	ld.param.u64 	%rd119, [%rd159+112];
	ld.param.u32 	%r265, [%rd159+144];
	ld.param.u32 	%r266, [%rd159+172];
	ld.param.v2.u32 	{%r611, %r612}, [%rd159+176];
	setp.le.s32 	%p207, %r611, %r236;
	add.s32 	%r270, %r237, 3;
	setp.le.s32 	%p208, %r612, %r270;
	add.s32 	%r271, %r235, 3;
	setp.le.s32 	%p209, %r266, %r271;
	or.pred  	%p210, %p207, %p208;
	or.b32  	%r613, %r236, %r271;
	or.b32  	%r614, %r613, %r270;
	setp.lt.s32 	%p211, %r614, 0;
	or.pred  	%p212, %p211, %p210;
	or.pred  	%p213, %p209, %p212;
	@%p213 bra 	$L__BB12_170;
	bra.uni 	$L__BB12_169;

$L__BB12_170:
	add.s32 	%r769, %r237, 3;
	st.local.v2.u32 	[%rd25], {%r236, %r769};
	add.s32 	%r770, %r235, 3;
	st.local.v2.u32 	[%rd25+8], {%r770, %r611};
	st.local.v2.u32 	[%rd25+16], {%r612, %r266};
	mov.u64 	%rd976, $str$2;
	cvta.global.u64 	%rd977, %rd976;
	{ // callseq 308, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd977;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r615, [retval0+0];
	} // callseq 308
	bra.uni 	$L__BB12_171;

$L__BB12_169:
	cvta.to.global.u64 	%rd969, %rd118;
	mul.wide.s32 	%rd970, %r265, %r271;
	add.s64 	%rd960, %rd119, %rd970;
	// begin inline asm
	{ atom.add.f64 %fd1495,[%rd960],%fd279; }

	// end inline asm
	add.s64 	%rd961, %rd960, 8;
	// begin inline asm
	{ atom.add.f64 %fd1497,[%rd961],%fd278; }

	// end inline asm
	add.s64 	%rd962, %rd960, 16;
	// begin inline asm
	{ atom.add.f64 %fd1499,[%rd962],%fd277; }

	// end inline asm
	add.s64 	%rd963, %rd960, 24;
	// begin inline asm
	{ atom.add.f64 %fd1501,[%rd963],%fd255; }

	// end inline asm
	add.s64 	%rd964, %rd960, 32;
	// begin inline asm
	{ atom.add.f64 %fd1503,[%rd964],%fd254; }

	// end inline asm
	add.s64 	%rd965, %rd960, 40;
	// begin inline asm
	{ atom.add.f64 %fd1505,[%rd965],%fd253; }

	// end inline asm
	add.s64 	%rd966, %rd960, 48;
	// begin inline asm
	{ atom.add.f64 %fd1507,[%rd966],%fd231; }

	// end inline asm
	add.s64 	%rd967, %rd960, 56;
	// begin inline asm
	{ atom.add.f64 %fd1509,[%rd967],%fd230; }

	// end inline asm
	add.s64 	%rd968, %rd960, 64;
	// begin inline asm
	{ atom.add.f64 %fd1511,[%rd968],%fd229; }

	// end inline asm
	mul.wide.s32 	%rd971, %r263, %r271;
	cvta.to.global.u64 	%rd972, %rd117;
	add.s64 	%rd973, %rd972, %rd971;
	mul.wide.s32 	%rd974, %r264, %r271;
	add.s64 	%rd975, %rd969, %rd974;
	st.global.u32 	[%rd973], %r236;
	add.s32 	%r768, %r237, 3;
	st.global.u32 	[%rd975], %r768;

$L__BB12_171:
	ld.param.u64 	%rd120, [%rd159];
	ld.param.u32 	%r272, [%rd159+32];
	ld.param.u64 	%rd121, [%rd159+56];
	ld.param.u32 	%r273, [%rd159+88];
	ld.param.u64 	%rd122, [%rd159+112];
	ld.param.u32 	%r274, [%rd159+144];
	ld.param.u32 	%r275, [%rd159+172];
	ld.param.v2.u32 	{%r616, %r617}, [%rd159+176];
	add.s32 	%r279, %r236, 1;
	setp.le.s32 	%p214, %r616, %r279;
	setp.le.s32 	%p215, %r617, %r237;
	add.s32 	%r280, %r235, 4;
	setp.le.s32 	%p216, %r275, %r280;
	or.pred  	%p217, %p214, %p215;
	or.b32  	%r618, %r237, %r280;
	or.b32  	%r619, %r618, %r279;
	setp.lt.s32 	%p218, %r619, 0;
	or.pred  	%p219, %p218, %p217;
	or.pred  	%p220, %p216, %p219;
	@%p220 bra 	$L__BB12_173;
	bra.uni 	$L__BB12_172;

$L__BB12_173:
	add.s32 	%r772, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r772, %r237};
	add.s32 	%r773, %r235, 4;
	st.local.v2.u32 	[%rd25+8], {%r773, %r616};
	st.local.v2.u32 	[%rd25+16], {%r617, %r275};
	mov.u64 	%rd995, $str$2;
	cvta.global.u64 	%rd996, %rd995;
	{ // callseq 309, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd996;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r620, [retval0+0];
	} // callseq 309
	bra.uni 	$L__BB12_174;

$L__BB12_172:
	cvta.to.global.u64 	%rd988, %rd121;
	mul.wide.s32 	%rd989, %r274, %r280;
	add.s64 	%rd979, %rd122, %rd989;
	// begin inline asm
	{ atom.add.f64 %fd1513,[%rd979],%fd216; }

	// end inline asm
	add.s64 	%rd980, %rd979, 8;
	// begin inline asm
	{ atom.add.f64 %fd1515,[%rd980],%fd215; }

	// end inline asm
	add.s64 	%rd981, %rd979, 16;
	// begin inline asm
	{ atom.add.f64 %fd1517,[%rd981],%fd214; }

	// end inline asm
	add.s64 	%rd982, %rd979, 24;
	// begin inline asm
	{ atom.add.f64 %fd1519,[%rd982],%fd192; }

	// end inline asm
	add.s64 	%rd983, %rd979, 32;
	// begin inline asm
	{ atom.add.f64 %fd1521,[%rd983],%fd191; }

	// end inline asm
	add.s64 	%rd984, %rd979, 40;
	// begin inline asm
	{ atom.add.f64 %fd1523,[%rd984],%fd190; }

	// end inline asm
	add.s64 	%rd985, %rd979, 48;
	// begin inline asm
	{ atom.add.f64 %fd1525,[%rd985],%fd168; }

	// end inline asm
	add.s64 	%rd986, %rd979, 56;
	// begin inline asm
	{ atom.add.f64 %fd1527,[%rd986],%fd167; }

	// end inline asm
	add.s64 	%rd987, %rd979, 64;
	// begin inline asm
	{ atom.add.f64 %fd1529,[%rd987],%fd166; }

	// end inline asm
	mul.wide.s32 	%rd990, %r272, %r280;
	cvta.to.global.u64 	%rd991, %rd120;
	add.s64 	%rd992, %rd991, %rd990;
	mul.wide.s32 	%rd993, %r273, %r280;
	add.s64 	%rd994, %rd988, %rd993;
	add.s32 	%r771, %r236, 1;
	st.global.u32 	[%rd992], %r771;
	st.global.u32 	[%rd994], %r237;

$L__BB12_174:
	ld.param.u64 	%rd123, [%rd159];
	ld.param.u32 	%r281, [%rd159+32];
	ld.param.u64 	%rd124, [%rd159+56];
	ld.param.u32 	%r282, [%rd159+88];
	ld.param.u64 	%rd125, [%rd159+112];
	ld.param.u32 	%r283, [%rd159+144];
	ld.param.u32 	%r284, [%rd159+172];
	ld.param.v2.u32 	{%r621, %r622}, [%rd159+176];
	setp.le.s32 	%p221, %r621, %r279;
	setp.le.s32 	%p222, %r622, %r252;
	add.s32 	%r288, %r235, 5;
	setp.le.s32 	%p223, %r284, %r288;
	or.pred  	%p224, %p221, %p222;
	or.b32  	%r623, %r279, %r288;
	or.b32  	%r624, %r623, %r252;
	setp.lt.s32 	%p225, %r624, 0;
	or.pred  	%p226, %p225, %p224;
	or.pred  	%p227, %p223, %p226;
	@%p227 bra 	$L__BB12_176;
	bra.uni 	$L__BB12_175;

$L__BB12_176:
	add.s32 	%r776, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r776, %r252};
	add.s32 	%r777, %r235, 5;
	st.local.v2.u32 	[%rd25+8], {%r777, %r621};
	st.local.v2.u32 	[%rd25+16], {%r622, %r284};
	mov.u64 	%rd1014, $str$2;
	cvta.global.u64 	%rd1015, %rd1014;
	{ // callseq 310, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1015;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r625, [retval0+0];
	} // callseq 310
	bra.uni 	$L__BB12_177;

$L__BB12_175:
	cvta.to.global.u64 	%rd1007, %rd124;
	mul.wide.s32 	%rd1008, %r283, %r288;
	add.s64 	%rd998, %rd125, %rd1008;
	// begin inline asm
	{ atom.add.f64 %fd1531,[%rd998],%fd213; }

	// end inline asm
	add.s64 	%rd999, %rd998, 8;
	// begin inline asm
	{ atom.add.f64 %fd1533,[%rd999],%fd212; }

	// end inline asm
	add.s64 	%rd1000, %rd998, 16;
	// begin inline asm
	{ atom.add.f64 %fd1535,[%rd1000],%fd211; }

	// end inline asm
	add.s64 	%rd1001, %rd998, 24;
	// begin inline asm
	{ atom.add.f64 %fd1537,[%rd1001],%fd189; }

	// end inline asm
	add.s64 	%rd1002, %rd998, 32;
	// begin inline asm
	{ atom.add.f64 %fd1539,[%rd1002],%fd188; }

	// end inline asm
	add.s64 	%rd1003, %rd998, 40;
	// begin inline asm
	{ atom.add.f64 %fd1541,[%rd1003],%fd187; }

	// end inline asm
	add.s64 	%rd1004, %rd998, 48;
	// begin inline asm
	{ atom.add.f64 %fd1543,[%rd1004],%fd165; }

	// end inline asm
	add.s64 	%rd1005, %rd998, 56;
	// begin inline asm
	{ atom.add.f64 %fd1545,[%rd1005],%fd164; }

	// end inline asm
	add.s64 	%rd1006, %rd998, 64;
	// begin inline asm
	{ atom.add.f64 %fd1547,[%rd1006],%fd163; }

	// end inline asm
	mul.wide.s32 	%rd1009, %r281, %r288;
	cvta.to.global.u64 	%rd1010, %rd123;
	add.s64 	%rd1011, %rd1010, %rd1009;
	mul.wide.s32 	%rd1012, %r282, %r288;
	add.s64 	%rd1013, %rd1007, %rd1012;
	add.s32 	%r774, %r236, 1;
	st.global.u32 	[%rd1011], %r774;
	add.s32 	%r775, %r237, 1;
	st.global.u32 	[%rd1013], %r775;

$L__BB12_177:
	ld.param.u64 	%rd126, [%rd159];
	ld.param.u32 	%r289, [%rd159+32];
	ld.param.u64 	%rd127, [%rd159+56];
	ld.param.u32 	%r290, [%rd159+88];
	ld.param.u64 	%rd128, [%rd159+112];
	ld.param.u32 	%r291, [%rd159+144];
	ld.param.u32 	%r292, [%rd159+172];
	ld.param.v2.u32 	{%r626, %r627}, [%rd159+176];
	setp.le.s32 	%p228, %r626, %r279;
	setp.le.s32 	%p229, %r627, %r261;
	add.s32 	%r296, %r235, 6;
	setp.le.s32 	%p230, %r292, %r296;
	or.pred  	%p231, %p228, %p229;
	or.b32  	%r628, %r279, %r296;
	or.b32  	%r629, %r628, %r261;
	setp.lt.s32 	%p232, %r629, 0;
	or.pred  	%p233, %p232, %p231;
	or.pred  	%p234, %p230, %p233;
	@%p234 bra 	$L__BB12_179;
	bra.uni 	$L__BB12_178;

$L__BB12_179:
	add.s32 	%r780, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r780, %r261};
	add.s32 	%r781, %r235, 6;
	st.local.v2.u32 	[%rd25+8], {%r781, %r626};
	st.local.v2.u32 	[%rd25+16], {%r627, %r292};
	mov.u64 	%rd1033, $str$2;
	cvta.global.u64 	%rd1034, %rd1033;
	{ // callseq 311, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1034;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r630, [retval0+0];
	} // callseq 311
	bra.uni 	$L__BB12_180;

$L__BB12_178:
	cvta.to.global.u64 	%rd1026, %rd127;
	mul.wide.s32 	%rd1027, %r291, %r296;
	add.s64 	%rd1017, %rd128, %rd1027;
	// begin inline asm
	{ atom.add.f64 %fd1549,[%rd1017],%fd210; }

	// end inline asm
	add.s64 	%rd1018, %rd1017, 8;
	// begin inline asm
	{ atom.add.f64 %fd1551,[%rd1018],%fd209; }

	// end inline asm
	add.s64 	%rd1019, %rd1017, 16;
	// begin inline asm
	{ atom.add.f64 %fd1553,[%rd1019],%fd208; }

	// end inline asm
	add.s64 	%rd1020, %rd1017, 24;
	// begin inline asm
	{ atom.add.f64 %fd1555,[%rd1020],%fd186; }

	// end inline asm
	add.s64 	%rd1021, %rd1017, 32;
	// begin inline asm
	{ atom.add.f64 %fd1557,[%rd1021],%fd185; }

	// end inline asm
	add.s64 	%rd1022, %rd1017, 40;
	// begin inline asm
	{ atom.add.f64 %fd1559,[%rd1022],%fd184; }

	// end inline asm
	add.s64 	%rd1023, %rd1017, 48;
	// begin inline asm
	{ atom.add.f64 %fd1561,[%rd1023],%fd162; }

	// end inline asm
	add.s64 	%rd1024, %rd1017, 56;
	// begin inline asm
	{ atom.add.f64 %fd1563,[%rd1024],%fd161; }

	// end inline asm
	add.s64 	%rd1025, %rd1017, 64;
	// begin inline asm
	{ atom.add.f64 %fd1565,[%rd1025],%fd160; }

	// end inline asm
	mul.wide.s32 	%rd1028, %r289, %r296;
	cvta.to.global.u64 	%rd1029, %rd126;
	add.s64 	%rd1030, %rd1029, %rd1028;
	mul.wide.s32 	%rd1031, %r290, %r296;
	add.s64 	%rd1032, %rd1026, %rd1031;
	add.s32 	%r778, %r236, 1;
	st.global.u32 	[%rd1030], %r778;
	add.s32 	%r779, %r237, 2;
	st.global.u32 	[%rd1032], %r779;

$L__BB12_180:
	ld.param.u64 	%rd129, [%rd159];
	ld.param.u32 	%r297, [%rd159+32];
	ld.param.u64 	%rd130, [%rd159+56];
	ld.param.u32 	%r298, [%rd159+88];
	ld.param.u64 	%rd131, [%rd159+112];
	ld.param.u32 	%r299, [%rd159+144];
	ld.param.u32 	%r300, [%rd159+172];
	ld.param.v2.u32 	{%r631, %r632}, [%rd159+176];
	setp.le.s32 	%p235, %r631, %r279;
	setp.le.s32 	%p236, %r632, %r270;
	add.s32 	%r304, %r235, 7;
	setp.le.s32 	%p237, %r300, %r304;
	or.pred  	%p238, %p235, %p236;
	or.b32  	%r633, %r279, %r304;
	or.b32  	%r634, %r633, %r270;
	setp.lt.s32 	%p239, %r634, 0;
	or.pred  	%p240, %p239, %p238;
	or.pred  	%p241, %p237, %p240;
	@%p241 bra 	$L__BB12_182;
	bra.uni 	$L__BB12_181;

$L__BB12_182:
	add.s32 	%r784, %r236, 1;
	st.local.v2.u32 	[%rd25], {%r784, %r270};
	add.s32 	%r785, %r235, 7;
	st.local.v2.u32 	[%rd25+8], {%r785, %r631};
	st.local.v2.u32 	[%rd25+16], {%r632, %r300};
	mov.u64 	%rd1052, $str$2;
	cvta.global.u64 	%rd1053, %rd1052;
	{ // callseq 312, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1053;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r635, [retval0+0];
	} // callseq 312
	bra.uni 	$L__BB12_183;

$L__BB12_181:
	cvta.to.global.u64 	%rd1045, %rd130;
	mul.wide.s32 	%rd1046, %r299, %r304;
	add.s64 	%rd1036, %rd131, %rd1046;
	// begin inline asm
	{ atom.add.f64 %fd1567,[%rd1036],%fd207; }

	// end inline asm
	add.s64 	%rd1037, %rd1036, 8;
	// begin inline asm
	{ atom.add.f64 %fd1569,[%rd1037],%fd206; }

	// end inline asm
	add.s64 	%rd1038, %rd1036, 16;
	// begin inline asm
	{ atom.add.f64 %fd1571,[%rd1038],%fd205; }

	// end inline asm
	add.s64 	%rd1039, %rd1036, 24;
	// begin inline asm
	{ atom.add.f64 %fd1573,[%rd1039],%fd183; }

	// end inline asm
	add.s64 	%rd1040, %rd1036, 32;
	// begin inline asm
	{ atom.add.f64 %fd1575,[%rd1040],%fd182; }

	// end inline asm
	add.s64 	%rd1041, %rd1036, 40;
	// begin inline asm
	{ atom.add.f64 %fd1577,[%rd1041],%fd181; }

	// end inline asm
	add.s64 	%rd1042, %rd1036, 48;
	// begin inline asm
	{ atom.add.f64 %fd1579,[%rd1042],%fd159; }

	// end inline asm
	add.s64 	%rd1043, %rd1036, 56;
	// begin inline asm
	{ atom.add.f64 %fd1581,[%rd1043],%fd158; }

	// end inline asm
	add.s64 	%rd1044, %rd1036, 64;
	// begin inline asm
	{ atom.add.f64 %fd1583,[%rd1044],%fd157; }

	// end inline asm
	mul.wide.s32 	%rd1047, %r297, %r304;
	cvta.to.global.u64 	%rd1048, %rd129;
	add.s64 	%rd1049, %rd1048, %rd1047;
	mul.wide.s32 	%rd1050, %r298, %r304;
	add.s64 	%rd1051, %rd1045, %rd1050;
	add.s32 	%r782, %r236, 1;
	st.global.u32 	[%rd1049], %r782;
	add.s32 	%r783, %r237, 3;
	st.global.u32 	[%rd1051], %r783;

$L__BB12_183:
	ld.param.u64 	%rd132, [%rd159];
	ld.param.u32 	%r305, [%rd159+32];
	ld.param.u64 	%rd133, [%rd159+56];
	ld.param.u32 	%r306, [%rd159+88];
	ld.param.u64 	%rd134, [%rd159+112];
	ld.param.u32 	%r307, [%rd159+144];
	ld.param.u32 	%r308, [%rd159+172];
	ld.param.v2.u32 	{%r636, %r637}, [%rd159+176];
	add.s32 	%r312, %r236, 2;
	setp.le.s32 	%p242, %r636, %r312;
	setp.le.s32 	%p243, %r637, %r237;
	add.s32 	%r313, %r235, 8;
	setp.le.s32 	%p244, %r308, %r313;
	or.pred  	%p245, %p242, %p243;
	or.b32  	%r638, %r237, %r313;
	or.b32  	%r639, %r638, %r312;
	setp.lt.s32 	%p246, %r639, 0;
	or.pred  	%p247, %p246, %p245;
	or.pred  	%p248, %p244, %p247;
	@%p248 bra 	$L__BB12_185;
	bra.uni 	$L__BB12_184;

$L__BB12_185:
	add.s32 	%r787, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r787, %r237};
	add.s32 	%r788, %r235, 8;
	st.local.v2.u32 	[%rd25+8], {%r788, %r636};
	st.local.v2.u32 	[%rd25+16], {%r637, %r308};
	mov.u64 	%rd1071, $str$2;
	cvta.global.u64 	%rd1072, %rd1071;
	{ // callseq 313, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1072;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r640, [retval0+0];
	} // callseq 313
	bra.uni 	$L__BB12_186;

$L__BB12_184:
	cvta.to.global.u64 	%rd1064, %rd133;
	mul.wide.s32 	%rd1065, %r307, %r313;
	add.s64 	%rd1055, %rd134, %rd1065;
	// begin inline asm
	{ atom.add.f64 %fd1585,[%rd1055],%fd144; }

	// end inline asm
	add.s64 	%rd1056, %rd1055, 8;
	// begin inline asm
	{ atom.add.f64 %fd1587,[%rd1056],%fd143; }

	// end inline asm
	add.s64 	%rd1057, %rd1055, 16;
	// begin inline asm
	{ atom.add.f64 %fd1589,[%rd1057],%fd142; }

	// end inline asm
	add.s64 	%rd1058, %rd1055, 24;
	// begin inline asm
	{ atom.add.f64 %fd1591,[%rd1058],%fd120; }

	// end inline asm
	add.s64 	%rd1059, %rd1055, 32;
	// begin inline asm
	{ atom.add.f64 %fd1593,[%rd1059],%fd119; }

	// end inline asm
	add.s64 	%rd1060, %rd1055, 40;
	// begin inline asm
	{ atom.add.f64 %fd1595,[%rd1060],%fd118; }

	// end inline asm
	add.s64 	%rd1061, %rd1055, 48;
	// begin inline asm
	{ atom.add.f64 %fd1597,[%rd1061],%fd96; }

	// end inline asm
	add.s64 	%rd1062, %rd1055, 56;
	// begin inline asm
	{ atom.add.f64 %fd1599,[%rd1062],%fd95; }

	// end inline asm
	add.s64 	%rd1063, %rd1055, 64;
	// begin inline asm
	{ atom.add.f64 %fd1601,[%rd1063],%fd94; }

	// end inline asm
	mul.wide.s32 	%rd1066, %r305, %r313;
	cvta.to.global.u64 	%rd1067, %rd132;
	add.s64 	%rd1068, %rd1067, %rd1066;
	mul.wide.s32 	%rd1069, %r306, %r313;
	add.s64 	%rd1070, %rd1064, %rd1069;
	add.s32 	%r786, %r236, 2;
	st.global.u32 	[%rd1068], %r786;
	st.global.u32 	[%rd1070], %r237;

$L__BB12_186:
	ld.param.u64 	%rd135, [%rd159];
	ld.param.u32 	%r314, [%rd159+32];
	ld.param.u64 	%rd136, [%rd159+56];
	ld.param.u32 	%r315, [%rd159+88];
	ld.param.u64 	%rd137, [%rd159+112];
	ld.param.u32 	%r316, [%rd159+144];
	ld.param.u32 	%r317, [%rd159+172];
	ld.param.v2.u32 	{%r641, %r642}, [%rd159+176];
	setp.le.s32 	%p249, %r641, %r312;
	setp.le.s32 	%p250, %r642, %r252;
	add.s32 	%r321, %r235, 9;
	setp.le.s32 	%p251, %r317, %r321;
	or.pred  	%p252, %p249, %p250;
	or.b32  	%r643, %r312, %r321;
	or.b32  	%r644, %r643, %r252;
	setp.lt.s32 	%p253, %r644, 0;
	or.pred  	%p254, %p253, %p252;
	or.pred  	%p255, %p251, %p254;
	@%p255 bra 	$L__BB12_188;
	bra.uni 	$L__BB12_187;

$L__BB12_188:
	add.s32 	%r791, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r791, %r252};
	add.s32 	%r792, %r235, 9;
	st.local.v2.u32 	[%rd25+8], {%r792, %r641};
	st.local.v2.u32 	[%rd25+16], {%r642, %r317};
	mov.u64 	%rd1090, $str$2;
	cvta.global.u64 	%rd1091, %rd1090;
	{ // callseq 314, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1091;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r645, [retval0+0];
	} // callseq 314
	bra.uni 	$L__BB12_189;

$L__BB12_187:
	cvta.to.global.u64 	%rd1083, %rd136;
	mul.wide.s32 	%rd1084, %r316, %r321;
	add.s64 	%rd1074, %rd137, %rd1084;
	// begin inline asm
	{ atom.add.f64 %fd1603,[%rd1074],%fd141; }

	// end inline asm
	add.s64 	%rd1075, %rd1074, 8;
	// begin inline asm
	{ atom.add.f64 %fd1605,[%rd1075],%fd140; }

	// end inline asm
	add.s64 	%rd1076, %rd1074, 16;
	// begin inline asm
	{ atom.add.f64 %fd1607,[%rd1076],%fd139; }

	// end inline asm
	add.s64 	%rd1077, %rd1074, 24;
	// begin inline asm
	{ atom.add.f64 %fd1609,[%rd1077],%fd117; }

	// end inline asm
	add.s64 	%rd1078, %rd1074, 32;
	// begin inline asm
	{ atom.add.f64 %fd1611,[%rd1078],%fd116; }

	// end inline asm
	add.s64 	%rd1079, %rd1074, 40;
	// begin inline asm
	{ atom.add.f64 %fd1613,[%rd1079],%fd115; }

	// end inline asm
	add.s64 	%rd1080, %rd1074, 48;
	// begin inline asm
	{ atom.add.f64 %fd1615,[%rd1080],%fd93; }

	// end inline asm
	add.s64 	%rd1081, %rd1074, 56;
	// begin inline asm
	{ atom.add.f64 %fd1617,[%rd1081],%fd92; }

	// end inline asm
	add.s64 	%rd1082, %rd1074, 64;
	// begin inline asm
	{ atom.add.f64 %fd1619,[%rd1082],%fd91; }

	// end inline asm
	mul.wide.s32 	%rd1085, %r314, %r321;
	cvta.to.global.u64 	%rd1086, %rd135;
	add.s64 	%rd1087, %rd1086, %rd1085;
	mul.wide.s32 	%rd1088, %r315, %r321;
	add.s64 	%rd1089, %rd1083, %rd1088;
	add.s32 	%r789, %r236, 2;
	st.global.u32 	[%rd1087], %r789;
	add.s32 	%r790, %r237, 1;
	st.global.u32 	[%rd1089], %r790;

$L__BB12_189:
	ld.param.u64 	%rd138, [%rd159];
	ld.param.u32 	%r322, [%rd159+32];
	ld.param.u64 	%rd139, [%rd159+56];
	ld.param.u32 	%r323, [%rd159+88];
	ld.param.u64 	%rd140, [%rd159+112];
	ld.param.u32 	%r324, [%rd159+144];
	ld.param.u32 	%r325, [%rd159+172];
	ld.param.v2.u32 	{%r646, %r647}, [%rd159+176];
	setp.le.s32 	%p256, %r646, %r312;
	setp.le.s32 	%p257, %r647, %r261;
	add.s32 	%r329, %r235, 10;
	setp.le.s32 	%p258, %r325, %r329;
	or.pred  	%p259, %p256, %p257;
	or.b32  	%r648, %r312, %r329;
	or.b32  	%r649, %r648, %r261;
	setp.lt.s32 	%p260, %r649, 0;
	or.pred  	%p261, %p260, %p259;
	or.pred  	%p262, %p258, %p261;
	@%p262 bra 	$L__BB12_191;
	bra.uni 	$L__BB12_190;

$L__BB12_191:
	add.s32 	%r795, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r795, %r261};
	add.s32 	%r796, %r235, 10;
	st.local.v2.u32 	[%rd25+8], {%r796, %r646};
	st.local.v2.u32 	[%rd25+16], {%r647, %r325};
	mov.u64 	%rd1109, $str$2;
	cvta.global.u64 	%rd1110, %rd1109;
	{ // callseq 315, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1110;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r650, [retval0+0];
	} // callseq 315
	bra.uni 	$L__BB12_192;

$L__BB12_190:
	cvta.to.global.u64 	%rd1102, %rd139;
	mul.wide.s32 	%rd1103, %r324, %r329;
	add.s64 	%rd1093, %rd140, %rd1103;
	// begin inline asm
	{ atom.add.f64 %fd1621,[%rd1093],%fd138; }

	// end inline asm
	add.s64 	%rd1094, %rd1093, 8;
	// begin inline asm
	{ atom.add.f64 %fd1623,[%rd1094],%fd137; }

	// end inline asm
	add.s64 	%rd1095, %rd1093, 16;
	// begin inline asm
	{ atom.add.f64 %fd1625,[%rd1095],%fd136; }

	// end inline asm
	add.s64 	%rd1096, %rd1093, 24;
	// begin inline asm
	{ atom.add.f64 %fd1627,[%rd1096],%fd114; }

	// end inline asm
	add.s64 	%rd1097, %rd1093, 32;
	// begin inline asm
	{ atom.add.f64 %fd1629,[%rd1097],%fd113; }

	// end inline asm
	add.s64 	%rd1098, %rd1093, 40;
	// begin inline asm
	{ atom.add.f64 %fd1631,[%rd1098],%fd112; }

	// end inline asm
	add.s64 	%rd1099, %rd1093, 48;
	// begin inline asm
	{ atom.add.f64 %fd1633,[%rd1099],%fd90; }

	// end inline asm
	add.s64 	%rd1100, %rd1093, 56;
	// begin inline asm
	{ atom.add.f64 %fd1635,[%rd1100],%fd89; }

	// end inline asm
	add.s64 	%rd1101, %rd1093, 64;
	// begin inline asm
	{ atom.add.f64 %fd1637,[%rd1101],%fd88; }

	// end inline asm
	mul.wide.s32 	%rd1104, %r322, %r329;
	cvta.to.global.u64 	%rd1105, %rd138;
	add.s64 	%rd1106, %rd1105, %rd1104;
	mul.wide.s32 	%rd1107, %r323, %r329;
	add.s64 	%rd1108, %rd1102, %rd1107;
	add.s32 	%r793, %r236, 2;
	st.global.u32 	[%rd1106], %r793;
	add.s32 	%r794, %r237, 2;
	st.global.u32 	[%rd1108], %r794;

$L__BB12_192:
	ld.param.u64 	%rd141, [%rd159];
	ld.param.u32 	%r330, [%rd159+32];
	ld.param.u64 	%rd142, [%rd159+56];
	ld.param.u32 	%r331, [%rd159+88];
	ld.param.u64 	%rd143, [%rd159+112];
	ld.param.u32 	%r332, [%rd159+144];
	ld.param.u32 	%r333, [%rd159+172];
	ld.param.v2.u32 	{%r651, %r652}, [%rd159+176];
	setp.le.s32 	%p263, %r651, %r312;
	setp.le.s32 	%p264, %r652, %r270;
	add.s32 	%r337, %r235, 11;
	setp.le.s32 	%p265, %r333, %r337;
	or.pred  	%p266, %p263, %p264;
	or.b32  	%r653, %r312, %r337;
	or.b32  	%r654, %r653, %r270;
	setp.lt.s32 	%p267, %r654, 0;
	or.pred  	%p268, %p267, %p266;
	or.pred  	%p269, %p265, %p268;
	@%p269 bra 	$L__BB12_194;
	bra.uni 	$L__BB12_193;

$L__BB12_194:
	add.s32 	%r799, %r236, 2;
	st.local.v2.u32 	[%rd25], {%r799, %r270};
	add.s32 	%r800, %r235, 11;
	st.local.v2.u32 	[%rd25+8], {%r800, %r651};
	st.local.v2.u32 	[%rd25+16], {%r652, %r333};
	mov.u64 	%rd1128, $str$2;
	cvta.global.u64 	%rd1129, %rd1128;
	{ // callseq 316, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1129;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r655, [retval0+0];
	} // callseq 316
	bra.uni 	$L__BB12_195;

$L__BB12_193:
	cvta.to.global.u64 	%rd1121, %rd142;
	mul.wide.s32 	%rd1122, %r332, %r337;
	add.s64 	%rd1112, %rd143, %rd1122;
	// begin inline asm
	{ atom.add.f64 %fd1639,[%rd1112],%fd135; }

	// end inline asm
	add.s64 	%rd1113, %rd1112, 8;
	// begin inline asm
	{ atom.add.f64 %fd1641,[%rd1113],%fd134; }

	// end inline asm
	add.s64 	%rd1114, %rd1112, 16;
	// begin inline asm
	{ atom.add.f64 %fd1643,[%rd1114],%fd133; }

	// end inline asm
	add.s64 	%rd1115, %rd1112, 24;
	// begin inline asm
	{ atom.add.f64 %fd1645,[%rd1115],%fd111; }

	// end inline asm
	add.s64 	%rd1116, %rd1112, 32;
	// begin inline asm
	{ atom.add.f64 %fd1647,[%rd1116],%fd110; }

	// end inline asm
	add.s64 	%rd1117, %rd1112, 40;
	// begin inline asm
	{ atom.add.f64 %fd1649,[%rd1117],%fd109; }

	// end inline asm
	add.s64 	%rd1118, %rd1112, 48;
	// begin inline asm
	{ atom.add.f64 %fd1651,[%rd1118],%fd87; }

	// end inline asm
	add.s64 	%rd1119, %rd1112, 56;
	// begin inline asm
	{ atom.add.f64 %fd1653,[%rd1119],%fd86; }

	// end inline asm
	add.s64 	%rd1120, %rd1112, 64;
	// begin inline asm
	{ atom.add.f64 %fd1655,[%rd1120],%fd85; }

	// end inline asm
	mul.wide.s32 	%rd1123, %r330, %r337;
	cvta.to.global.u64 	%rd1124, %rd141;
	add.s64 	%rd1125, %rd1124, %rd1123;
	mul.wide.s32 	%rd1126, %r331, %r337;
	add.s64 	%rd1127, %rd1121, %rd1126;
	add.s32 	%r797, %r236, 2;
	st.global.u32 	[%rd1125], %r797;
	add.s32 	%r798, %r237, 3;
	st.global.u32 	[%rd1127], %r798;

$L__BB12_195:
	ld.param.u64 	%rd144, [%rd159];
	ld.param.u32 	%r338, [%rd159+32];
	ld.param.u64 	%rd145, [%rd159+56];
	ld.param.u32 	%r339, [%rd159+88];
	ld.param.u64 	%rd146, [%rd159+112];
	ld.param.u32 	%r340, [%rd159+144];
	ld.param.u32 	%r341, [%rd159+172];
	ld.param.v2.u32 	{%r656, %r657}, [%rd159+176];
	add.s32 	%r345, %r236, 3;
	setp.le.s32 	%p270, %r656, %r345;
	setp.le.s32 	%p271, %r657, %r237;
	add.s32 	%r346, %r235, 12;
	setp.le.s32 	%p272, %r341, %r346;
	or.pred  	%p273, %p270, %p271;
	or.b32  	%r658, %r237, %r346;
	or.b32  	%r659, %r658, %r345;
	setp.lt.s32 	%p274, %r659, 0;
	or.pred  	%p275, %p274, %p273;
	or.pred  	%p276, %p272, %p275;
	@%p276 bra 	$L__BB12_197;
	bra.uni 	$L__BB12_196;

$L__BB12_197:
	add.s32 	%r802, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r802, %r237};
	add.s32 	%r803, %r235, 12;
	st.local.v2.u32 	[%rd25+8], {%r803, %r656};
	st.local.v2.u32 	[%rd25+16], {%r657, %r341};
	mov.u64 	%rd1147, $str$2;
	cvta.global.u64 	%rd1148, %rd1147;
	{ // callseq 317, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1148;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r660, [retval0+0];
	} // callseq 317
	bra.uni 	$L__BB12_198;

$L__BB12_196:
	cvta.to.global.u64 	%rd1140, %rd145;
	mul.wide.s32 	%rd1141, %r340, %r346;
	add.s64 	%rd1131, %rd146, %rd1141;
	// begin inline asm
	{ atom.add.f64 %fd1657,[%rd1131],%fd72; }

	// end inline asm
	add.s64 	%rd1132, %rd1131, 8;
	// begin inline asm
	{ atom.add.f64 %fd1659,[%rd1132],%fd71; }

	// end inline asm
	add.s64 	%rd1133, %rd1131, 16;
	// begin inline asm
	{ atom.add.f64 %fd1661,[%rd1133],%fd70; }

	// end inline asm
	add.s64 	%rd1134, %rd1131, 24;
	// begin inline asm
	{ atom.add.f64 %fd1663,[%rd1134],%fd48; }

	// end inline asm
	add.s64 	%rd1135, %rd1131, 32;
	// begin inline asm
	{ atom.add.f64 %fd1665,[%rd1135],%fd47; }

	// end inline asm
	add.s64 	%rd1136, %rd1131, 40;
	// begin inline asm
	{ atom.add.f64 %fd1667,[%rd1136],%fd46; }

	// end inline asm
	add.s64 	%rd1137, %rd1131, 48;
	// begin inline asm
	{ atom.add.f64 %fd1669,[%rd1137],%fd24; }

	// end inline asm
	add.s64 	%rd1138, %rd1131, 56;
	// begin inline asm
	{ atom.add.f64 %fd1671,[%rd1138],%fd23; }

	// end inline asm
	add.s64 	%rd1139, %rd1131, 64;
	// begin inline asm
	{ atom.add.f64 %fd1673,[%rd1139],%fd22; }

	// end inline asm
	mul.wide.s32 	%rd1142, %r338, %r346;
	cvta.to.global.u64 	%rd1143, %rd144;
	add.s64 	%rd1144, %rd1143, %rd1142;
	mul.wide.s32 	%rd1145, %r339, %r346;
	add.s64 	%rd1146, %rd1140, %rd1145;
	add.s32 	%r801, %r236, 3;
	st.global.u32 	[%rd1144], %r801;
	st.global.u32 	[%rd1146], %r237;

$L__BB12_198:
	ld.param.u64 	%rd147, [%rd159];
	ld.param.u32 	%r347, [%rd159+32];
	ld.param.u64 	%rd148, [%rd159+56];
	ld.param.u32 	%r348, [%rd159+88];
	ld.param.u64 	%rd149, [%rd159+112];
	ld.param.u32 	%r349, [%rd159+144];
	ld.param.u32 	%r350, [%rd159+172];
	ld.param.v2.u32 	{%r661, %r662}, [%rd159+176];
	setp.le.s32 	%p277, %r661, %r345;
	setp.le.s32 	%p278, %r662, %r252;
	add.s32 	%r354, %r235, 13;
	setp.le.s32 	%p279, %r350, %r354;
	or.pred  	%p280, %p277, %p278;
	or.b32  	%r663, %r345, %r354;
	or.b32  	%r664, %r663, %r252;
	setp.lt.s32 	%p281, %r664, 0;
	or.pred  	%p282, %p281, %p280;
	or.pred  	%p283, %p279, %p282;
	@%p283 bra 	$L__BB12_200;
	bra.uni 	$L__BB12_199;

$L__BB12_200:
	add.s32 	%r806, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r806, %r252};
	add.s32 	%r807, %r235, 13;
	st.local.v2.u32 	[%rd25+8], {%r807, %r661};
	st.local.v2.u32 	[%rd25+16], {%r662, %r350};
	mov.u64 	%rd1166, $str$2;
	cvta.global.u64 	%rd1167, %rd1166;
	{ // callseq 318, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1167;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r665, [retval0+0];
	} // callseq 318
	bra.uni 	$L__BB12_201;

$L__BB12_199:
	cvta.to.global.u64 	%rd1159, %rd148;
	mul.wide.s32 	%rd1160, %r349, %r354;
	add.s64 	%rd1150, %rd149, %rd1160;
	// begin inline asm
	{ atom.add.f64 %fd1675,[%rd1150],%fd69; }

	// end inline asm
	add.s64 	%rd1151, %rd1150, 8;
	// begin inline asm
	{ atom.add.f64 %fd1677,[%rd1151],%fd68; }

	// end inline asm
	add.s64 	%rd1152, %rd1150, 16;
	// begin inline asm
	{ atom.add.f64 %fd1679,[%rd1152],%fd67; }

	// end inline asm
	add.s64 	%rd1153, %rd1150, 24;
	// begin inline asm
	{ atom.add.f64 %fd1681,[%rd1153],%fd45; }

	// end inline asm
	add.s64 	%rd1154, %rd1150, 32;
	// begin inline asm
	{ atom.add.f64 %fd1683,[%rd1154],%fd44; }

	// end inline asm
	add.s64 	%rd1155, %rd1150, 40;
	// begin inline asm
	{ atom.add.f64 %fd1685,[%rd1155],%fd43; }

	// end inline asm
	add.s64 	%rd1156, %rd1150, 48;
	// begin inline asm
	{ atom.add.f64 %fd1687,[%rd1156],%fd21; }

	// end inline asm
	add.s64 	%rd1157, %rd1150, 56;
	// begin inline asm
	{ atom.add.f64 %fd1689,[%rd1157],%fd20; }

	// end inline asm
	add.s64 	%rd1158, %rd1150, 64;
	// begin inline asm
	{ atom.add.f64 %fd1691,[%rd1158],%fd19; }

	// end inline asm
	mul.wide.s32 	%rd1161, %r347, %r354;
	cvta.to.global.u64 	%rd1162, %rd147;
	add.s64 	%rd1163, %rd1162, %rd1161;
	mul.wide.s32 	%rd1164, %r348, %r354;
	add.s64 	%rd1165, %rd1159, %rd1164;
	add.s32 	%r804, %r236, 3;
	st.global.u32 	[%rd1163], %r804;
	add.s32 	%r805, %r237, 1;
	st.global.u32 	[%rd1165], %r805;

$L__BB12_201:
	ld.param.u64 	%rd150, [%rd159];
	ld.param.u32 	%r355, [%rd159+32];
	ld.param.u64 	%rd151, [%rd159+56];
	ld.param.u32 	%r356, [%rd159+88];
	ld.param.u64 	%rd152, [%rd159+112];
	ld.param.u32 	%r357, [%rd159+144];
	ld.param.u32 	%r358, [%rd159+172];
	ld.param.v2.u32 	{%r666, %r667}, [%rd159+176];
	setp.le.s32 	%p284, %r666, %r345;
	setp.le.s32 	%p285, %r667, %r261;
	add.s32 	%r362, %r235, 14;
	setp.le.s32 	%p286, %r358, %r362;
	or.pred  	%p287, %p284, %p285;
	or.b32  	%r668, %r345, %r362;
	or.b32  	%r669, %r668, %r261;
	setp.lt.s32 	%p288, %r669, 0;
	or.pred  	%p289, %p288, %p287;
	or.pred  	%p290, %p286, %p289;
	@%p290 bra 	$L__BB12_203;
	bra.uni 	$L__BB12_202;

$L__BB12_203:
	add.s32 	%r810, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r810, %r261};
	add.s32 	%r811, %r235, 14;
	st.local.v2.u32 	[%rd25+8], {%r811, %r666};
	st.local.v2.u32 	[%rd25+16], {%r667, %r358};
	mov.u64 	%rd1185, $str$2;
	cvta.global.u64 	%rd1186, %rd1185;
	{ // callseq 319, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1186;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r670, [retval0+0];
	} // callseq 319
	bra.uni 	$L__BB12_204;

$L__BB12_202:
	cvta.to.global.u64 	%rd1178, %rd151;
	mul.wide.s32 	%rd1179, %r357, %r362;
	add.s64 	%rd1169, %rd152, %rd1179;
	// begin inline asm
	{ atom.add.f64 %fd1693,[%rd1169],%fd66; }

	// end inline asm
	add.s64 	%rd1170, %rd1169, 8;
	// begin inline asm
	{ atom.add.f64 %fd1695,[%rd1170],%fd65; }

	// end inline asm
	add.s64 	%rd1171, %rd1169, 16;
	// begin inline asm
	{ atom.add.f64 %fd1697,[%rd1171],%fd64; }

	// end inline asm
	add.s64 	%rd1172, %rd1169, 24;
	// begin inline asm
	{ atom.add.f64 %fd1699,[%rd1172],%fd42; }

	// end inline asm
	add.s64 	%rd1173, %rd1169, 32;
	// begin inline asm
	{ atom.add.f64 %fd1701,[%rd1173],%fd41; }

	// end inline asm
	add.s64 	%rd1174, %rd1169, 40;
	// begin inline asm
	{ atom.add.f64 %fd1703,[%rd1174],%fd40; }

	// end inline asm
	add.s64 	%rd1175, %rd1169, 48;
	// begin inline asm
	{ atom.add.f64 %fd1705,[%rd1175],%fd18; }

	// end inline asm
	add.s64 	%rd1176, %rd1169, 56;
	// begin inline asm
	{ atom.add.f64 %fd1707,[%rd1176],%fd17; }

	// end inline asm
	add.s64 	%rd1177, %rd1169, 64;
	// begin inline asm
	{ atom.add.f64 %fd1709,[%rd1177],%fd16; }

	// end inline asm
	mul.wide.s32 	%rd1180, %r355, %r362;
	cvta.to.global.u64 	%rd1181, %rd150;
	add.s64 	%rd1182, %rd1181, %rd1180;
	mul.wide.s32 	%rd1183, %r356, %r362;
	add.s64 	%rd1184, %rd1178, %rd1183;
	add.s32 	%r808, %r236, 3;
	st.global.u32 	[%rd1182], %r808;
	add.s32 	%r809, %r237, 2;
	st.global.u32 	[%rd1184], %r809;

$L__BB12_204:
	ld.param.u64 	%rd153, [%rd159];
	ld.param.u32 	%r363, [%rd159+32];
	ld.param.u64 	%rd154, [%rd159+56];
	ld.param.u32 	%r364, [%rd159+88];
	ld.param.u64 	%rd155, [%rd159+112];
	ld.param.u32 	%r365, [%rd159+144];
	ld.param.u32 	%r366, [%rd159+172];
	ld.param.v2.u32 	{%r671, %r672}, [%rd159+176];
	setp.le.s32 	%p291, %r671, %r345;
	setp.le.s32 	%p292, %r672, %r270;
	add.s32 	%r370, %r235, 15;
	setp.le.s32 	%p293, %r366, %r370;
	or.pred  	%p294, %p291, %p292;
	or.b32  	%r673, %r345, %r370;
	or.b32  	%r674, %r673, %r270;
	setp.lt.s32 	%p295, %r674, 0;
	or.pred  	%p296, %p295, %p294;
	or.pred  	%p297, %p293, %p296;
	@%p297 bra 	$L__BB12_206;
	bra.uni 	$L__BB12_205;

$L__BB12_206:
	add.s32 	%r814, %r236, 3;
	st.local.v2.u32 	[%rd25], {%r814, %r270};
	add.s32 	%r815, %r235, 15;
	st.local.v2.u32 	[%rd25+8], {%r815, %r671};
	st.local.v2.u32 	[%rd25+16], {%r672, %r366};
	mov.u64 	%rd1204, $str$2;
	cvta.global.u64 	%rd1205, %rd1204;
	{ // callseq 320, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1205;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd179;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r675, [retval0+0];
	} // callseq 320
	bra.uni 	$L__BB12_207;

$L__BB12_205:
	cvta.to.global.u64 	%rd1197, %rd154;
	mul.wide.s32 	%rd1198, %r365, %r370;
	add.s64 	%rd1188, %rd155, %rd1198;
	// begin inline asm
	{ atom.add.f64 %fd1711,[%rd1188],%fd63; }

	// end inline asm
	add.s64 	%rd1189, %rd1188, 8;
	// begin inline asm
	{ atom.add.f64 %fd1713,[%rd1189],%fd62; }

	// end inline asm
	add.s64 	%rd1190, %rd1188, 16;
	// begin inline asm
	{ atom.add.f64 %fd1715,[%rd1190],%fd61; }

	// end inline asm
	add.s64 	%rd1191, %rd1188, 24;
	// begin inline asm
	{ atom.add.f64 %fd1717,[%rd1191],%fd39; }

	// end inline asm
	add.s64 	%rd1192, %rd1188, 32;
	// begin inline asm
	{ atom.add.f64 %fd1719,[%rd1192],%fd38; }

	// end inline asm
	add.s64 	%rd1193, %rd1188, 40;
	// begin inline asm
	{ atom.add.f64 %fd1721,[%rd1193],%fd37; }

	// end inline asm
	add.s64 	%rd1194, %rd1188, 48;
	// begin inline asm
	{ atom.add.f64 %fd1723,[%rd1194],%fd15; }

	// end inline asm
	add.s64 	%rd1195, %rd1188, 56;
	// begin inline asm
	{ atom.add.f64 %fd1725,[%rd1195],%fd14; }

	// end inline asm
	add.s64 	%rd1196, %rd1188, 64;
	// begin inline asm
	{ atom.add.f64 %fd1727,[%rd1196],%fd13; }

	// end inline asm
	mul.wide.s32 	%rd1199, %r363, %r370;
	cvta.to.global.u64 	%rd1200, %rd153;
	add.s64 	%rd1201, %rd1200, %rd1199;
	mul.wide.s32 	%rd1202, %r364, %r370;
	add.s64 	%rd1203, %rd1197, %rd1202;
	add.s32 	%r812, %r236, 3;
	st.global.u32 	[%rd1201], %r812;
	add.s32 	%r813, %r237, 3;
	st.global.u32 	[%rd1203], %r813;

$L__BB12_207:
	ld.param.u64 	%rd1209, [assemble_matrix_cuda_kernel_forward_param_0+24];
	mov.u32 	%r818, %ntid.x;
	mov.u32 	%r677, %nctaid.x;
	mul.wide.u32 	%rd1207, %r818, %r677;
	add.s64 	%rd1212, %rd1212, %rd1207;
	setp.lt.u64 	%p298, %rd1212, %rd1209;
	@%p298 bra 	$L__BB12_2;

$L__BB12_208:
	ret;

}
	// .globl	assemble_matrix_cuda_kernel_backward
.visible .entry assemble_matrix_cuda_kernel_backward(
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_1[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_2[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_5[56],
	.param .u32 assemble_matrix_cuda_kernel_backward_param_6,
	.param .u32 assemble_matrix_cuda_kernel_backward_param_7,
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_8[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_9[184],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 assemble_matrix_cuda_kernel_backward_param_12[56],
	.param .u32 assemble_matrix_cuda_kernel_backward_param_13,
	.param .u32 assemble_matrix_cuda_kernel_backward_param_14
)
{
	.local .align 8 .b8 	__local_depot13[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<941>;
	.reg .b16 	%rs<395>;
	.reg .b32 	%r<2806>;
	.reg .f64 	%fd<9793>;
	.reg .b64 	%rd<4038>;


	mov.u64 	%SPL, __local_depot13;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r899, %r900}, [assemble_matrix_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r901, %r902}, [assemble_matrix_cuda_kernel_backward_param_0+8];
	mov.b64 	%rd357, assemble_matrix_cuda_kernel_backward_param_2;
	ld.param.v2.u32 	{%r907, %r908}, [assemble_matrix_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r915, %r916}, [assemble_matrix_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r923, %r924}, [assemble_matrix_cuda_kernel_backward_param_5+32];
	ld.param.u32 	%r889, [assemble_matrix_cuda_kernel_backward_param_7];
	ld.param.v2.u32 	{%r931, %r932}, [assemble_matrix_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd364, [assemble_matrix_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd363, [assemble_matrix_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd362, [assemble_matrix_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd360, [assemble_matrix_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd358, [assemble_matrix_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd355, [assemble_matrix_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r861, [assemble_matrix_cuda_kernel_backward_param_0+16];
	mov.u32 	%r935, %ntid.x;
	mov.u32 	%r936, %ctaid.x;
	mul.wide.u32 	%rd366, %r935, %r936;
	mov.u32 	%r937, %tid.x;
	cvt.u64.u32 	%rd367, %r937;
	add.s64 	%rd4034, %rd366, %rd367;
	setp.ge.u64 	%p1, %rd4034, %rd355;
	@%p1 bra 	$L__BB13_660;

	cvta.to.global.u64 	%rd5, %rd362;
	cvta.to.global.u64 	%rd6, %rd360;
	cvta.to.global.u64 	%rd7, %rd358;
	cvt.s64.s32 	%rd8, %r902;
	cvt.s64.s32 	%rd9, %r901;
	cvt.s64.s32 	%rd10, %r900;
	cvt.s64.s32 	%rd11, %r923;
	cvt.s64.s32 	%rd12, %r907;
	cvt.s64.s32 	%rd13, %r915;
	cvt.s64.s32 	%rd14, %r931;

$L__BB13_2:
	setp.lt.s32 	%p2, %r861, 4;
	mov.u64 	%rd4035, %rd4034;
	@%p2 bra 	$L__BB13_6;

	or.b64  	%rd368, %rd4034, %rd8;
	and.b64  	%rd369, %rd368, -4294967296;
	setp.eq.s64 	%p3, %rd369, 0;
	@%p3 bra 	$L__BB13_5;

	div.u64 	%rd4035, %rd4034, %rd8;
	bra.uni 	$L__BB13_6;

$L__BB13_5:
	cvt.u32.u64 	%r938, %rd8;
	cvt.u32.u64 	%r939, %rd4034;
	div.u32 	%r940, %r939, %r938;
	cvt.u64.u32 	%rd4035, %r940;

$L__BB13_6:
	setp.lt.s32 	%p4, %r861, 3;
	@%p4 bra 	$L__BB13_10;

	or.b64  	%rd370, %rd4035, %rd9;
	and.b64  	%rd371, %rd370, -4294967296;
	setp.eq.s64 	%p5, %rd371, 0;
	@%p5 bra 	$L__BB13_9;

	div.u64 	%rd4035, %rd4035, %rd9;
	bra.uni 	$L__BB13_10;

$L__BB13_9:
	cvt.u32.u64 	%r941, %rd9;
	cvt.u32.u64 	%r942, %rd4035;
	div.u32 	%r943, %r942, %r941;
	cvt.u64.u32 	%rd4035, %r943;

$L__BB13_10:
	setp.lt.s32 	%p6, %r861, 2;
	@%p6 bra 	$L__BB13_14;

	or.b64  	%rd372, %rd4035, %rd10;
	and.b64  	%rd373, %rd372, -4294967296;
	setp.eq.s64 	%p7, %rd373, 0;
	@%p7 bra 	$L__BB13_13;

	div.u64 	%rd4035, %rd4035, %rd10;
	bra.uni 	$L__BB13_14;

$L__BB13_13:
	cvt.u32.u64 	%r944, %rd10;
	cvt.u32.u64 	%r945, %rd4035;
	div.u32 	%r946, %r945, %r944;
	cvt.u64.u32 	%rd4035, %r946;

$L__BB13_14:
	cvt.u32.u64 	%r947, %rd4035;
	setp.gt.s32 	%p8, %r861, 0;
	selp.b32 	%r2, %r947, 0, %p8;
	setp.ge.s32 	%p9, %r2, %r889;
	@%p9 bra 	$L__BB13_659;

	mov.b64 	%rd3970, assemble_matrix_cuda_kernel_backward_param_1;
	cvt.s64.s32 	%rd25, %r2;
	mul.lo.s64 	%rd26, %rd25, %rd11;
	add.s64 	%rd374, %rd5, %rd26;
	ld.global.f64 	%fd1, [%rd374+4600];
	ld.global.f64 	%fd2, [%rd374+4592];
	ld.global.f64 	%fd3, [%rd374+4584];
	ld.global.f64 	%fd4, [%rd374+4576];
	ld.global.f64 	%fd5, [%rd374+4568];
	ld.global.f64 	%fd6, [%rd374+4560];
	ld.global.f64 	%fd7, [%rd374+4552];
	ld.global.f64 	%fd8, [%rd374+4544];
	ld.global.f64 	%fd9, [%rd374+4536];
	ld.global.f64 	%fd10, [%rd374+4528];
	ld.global.f64 	%fd11, [%rd374+4520];
	ld.global.f64 	%fd12, [%rd374+4512];
	ld.global.f64 	%fd13, [%rd374+4504];
	ld.global.f64 	%fd14, [%rd374+4496];
	ld.global.f64 	%fd15, [%rd374+4488];
	ld.global.f64 	%fd16, [%rd374+4480];
	ld.global.f64 	%fd17, [%rd374+4472];
	ld.global.f64 	%fd18, [%rd374+4464];
	ld.global.f64 	%fd19, [%rd374+4456];
	ld.global.f64 	%fd20, [%rd374+4448];
	ld.global.f64 	%fd21, [%rd374+4440];
	ld.global.f64 	%fd22, [%rd374+4432];
	ld.global.f64 	%fd23, [%rd374+4424];
	ld.global.f64 	%fd24, [%rd374+4416];
	ld.global.f64 	%fd25, [%rd374+4408];
	ld.global.f64 	%fd26, [%rd374+4400];
	ld.global.f64 	%fd27, [%rd374+4392];
	ld.global.f64 	%fd28, [%rd374+4384];
	ld.global.f64 	%fd29, [%rd374+4376];
	ld.global.f64 	%fd30, [%rd374+4368];
	ld.global.f64 	%fd31, [%rd374+4360];
	ld.global.f64 	%fd32, [%rd374+4352];
	ld.global.f64 	%fd33, [%rd374+4344];
	ld.global.f64 	%fd34, [%rd374+4336];
	ld.global.f64 	%fd35, [%rd374+4328];
	ld.global.f64 	%fd36, [%rd374+4320];
	ld.global.f64 	%fd37, [%rd374+4312];
	ld.global.f64 	%fd38, [%rd374+4304];
	ld.global.f64 	%fd39, [%rd374+4296];
	ld.global.f64 	%fd40, [%rd374+4288];
	ld.global.f64 	%fd41, [%rd374+4280];
	ld.global.f64 	%fd42, [%rd374+4272];
	ld.global.f64 	%fd43, [%rd374+4264];
	ld.global.f64 	%fd44, [%rd374+4256];
	ld.global.f64 	%fd45, [%rd374+4248];
	ld.global.f64 	%fd46, [%rd374+4240];
	ld.global.f64 	%fd47, [%rd374+4232];
	ld.global.f64 	%fd48, [%rd374+4224];
	ld.global.f64 	%fd49, [%rd374+4216];
	ld.global.f64 	%fd50, [%rd374+4208];
	ld.global.f64 	%fd51, [%rd374+4200];
	ld.global.f64 	%fd52, [%rd374+4192];
	ld.global.f64 	%fd53, [%rd374+4184];
	ld.global.f64 	%fd54, [%rd374+4176];
	ld.global.f64 	%fd55, [%rd374+4168];
	ld.global.f64 	%fd56, [%rd374+4160];
	ld.global.f64 	%fd57, [%rd374+4152];
	ld.global.f64 	%fd58, [%rd374+4144];
	ld.global.f64 	%fd59, [%rd374+4136];
	ld.global.f64 	%fd60, [%rd374+4128];
	ld.global.f64 	%fd61, [%rd374+4120];
	ld.global.f64 	%fd62, [%rd374+4112];
	ld.global.f64 	%fd63, [%rd374+4104];
	ld.global.f64 	%fd64, [%rd374+4096];
	ld.global.f64 	%fd65, [%rd374+4088];
	ld.global.f64 	%fd66, [%rd374+4080];
	ld.global.f64 	%fd67, [%rd374+4072];
	ld.global.f64 	%fd68, [%rd374+4064];
	ld.global.f64 	%fd69, [%rd374+4056];
	ld.global.f64 	%fd70, [%rd374+4048];
	ld.global.f64 	%fd71, [%rd374+4040];
	ld.global.f64 	%fd72, [%rd374+4032];
	ld.global.f64 	%fd73, [%rd374+4024];
	ld.global.f64 	%fd74, [%rd374+4016];
	ld.global.f64 	%fd75, [%rd374+4008];
	ld.global.f64 	%fd76, [%rd374+4000];
	ld.global.f64 	%fd77, [%rd374+3992];
	ld.global.f64 	%fd78, [%rd374+3984];
	ld.global.f64 	%fd79, [%rd374+3976];
	ld.global.f64 	%fd80, [%rd374+3968];
	ld.global.f64 	%fd81, [%rd374+3960];
	ld.global.f64 	%fd82, [%rd374+3952];
	ld.global.f64 	%fd83, [%rd374+3944];
	ld.global.f64 	%fd84, [%rd374+3936];
	ld.global.f64 	%fd85, [%rd374+3928];
	ld.global.f64 	%fd86, [%rd374+3920];
	ld.global.f64 	%fd87, [%rd374+3912];
	ld.global.f64 	%fd88, [%rd374+3904];
	ld.global.f64 	%fd89, [%rd374+3896];
	ld.global.f64 	%fd90, [%rd374+3888];
	ld.global.f64 	%fd91, [%rd374+3880];
	ld.global.f64 	%fd92, [%rd374+3872];
	ld.global.f64 	%fd93, [%rd374+3864];
	ld.global.f64 	%fd94, [%rd374+3856];
	ld.global.f64 	%fd95, [%rd374+3848];
	ld.global.f64 	%fd96, [%rd374+3840];
	ld.global.f64 	%fd97, [%rd374+3832];
	ld.global.f64 	%fd98, [%rd374+3824];
	ld.global.f64 	%fd99, [%rd374+3816];
	ld.global.f64 	%fd100, [%rd374+3808];
	ld.global.f64 	%fd101, [%rd374+3800];
	ld.global.f64 	%fd102, [%rd374+3792];
	ld.global.f64 	%fd103, [%rd374+3784];
	ld.global.f64 	%fd104, [%rd374+3776];
	ld.global.f64 	%fd105, [%rd374+3768];
	ld.global.f64 	%fd106, [%rd374+3760];
	ld.global.f64 	%fd107, [%rd374+3752];
	ld.global.f64 	%fd108, [%rd374+3744];
	ld.global.f64 	%fd109, [%rd374+3736];
	ld.global.f64 	%fd110, [%rd374+3728];
	ld.global.f64 	%fd111, [%rd374+3720];
	ld.global.f64 	%fd112, [%rd374+3712];
	ld.global.f64 	%fd113, [%rd374+3704];
	ld.global.f64 	%fd114, [%rd374+3696];
	ld.global.f64 	%fd115, [%rd374+3688];
	ld.global.f64 	%fd116, [%rd374+3680];
	ld.global.f64 	%fd117, [%rd374+3672];
	ld.global.f64 	%fd118, [%rd374+3664];
	ld.global.f64 	%fd119, [%rd374+3656];
	ld.global.f64 	%fd120, [%rd374+3648];
	ld.global.f64 	%fd121, [%rd374+3640];
	ld.global.f64 	%fd122, [%rd374+3632];
	ld.global.f64 	%fd123, [%rd374+3624];
	ld.global.f64 	%fd124, [%rd374+3616];
	ld.global.f64 	%fd125, [%rd374+3608];
	ld.global.f64 	%fd126, [%rd374+3600];
	ld.global.f64 	%fd127, [%rd374+3592];
	ld.global.f64 	%fd128, [%rd374+3584];
	ld.global.f64 	%fd129, [%rd374+3576];
	ld.global.f64 	%fd130, [%rd374+3568];
	ld.global.f64 	%fd131, [%rd374+3560];
	ld.global.f64 	%fd132, [%rd374+3552];
	ld.global.f64 	%fd133, [%rd374+3544];
	ld.global.f64 	%fd134, [%rd374+3536];
	ld.global.f64 	%fd135, [%rd374+3528];
	ld.global.f64 	%fd136, [%rd374+3520];
	ld.global.f64 	%fd137, [%rd374+3512];
	ld.global.f64 	%fd138, [%rd374+3504];
	ld.global.f64 	%fd139, [%rd374+3496];
	ld.global.f64 	%fd140, [%rd374+3488];
	ld.global.f64 	%fd141, [%rd374+3480];
	ld.global.f64 	%fd142, [%rd374+3472];
	ld.global.f64 	%fd143, [%rd374+3464];
	ld.global.f64 	%fd144, [%rd374+3456];
	ld.global.f64 	%fd145, [%rd374+3448];
	ld.global.f64 	%fd146, [%rd374+3440];
	ld.global.f64 	%fd147, [%rd374+3432];
	ld.global.f64 	%fd148, [%rd374+3424];
	ld.global.f64 	%fd149, [%rd374+3416];
	ld.global.f64 	%fd150, [%rd374+3408];
	ld.global.f64 	%fd151, [%rd374+3400];
	ld.global.f64 	%fd152, [%rd374+3392];
	ld.global.f64 	%fd153, [%rd374+3384];
	ld.global.f64 	%fd154, [%rd374+3376];
	ld.global.f64 	%fd155, [%rd374+3368];
	ld.global.f64 	%fd156, [%rd374+3360];
	ld.global.f64 	%fd157, [%rd374+3352];
	ld.global.f64 	%fd158, [%rd374+3344];
	ld.global.f64 	%fd159, [%rd374+3336];
	ld.global.f64 	%fd160, [%rd374+3328];
	ld.global.f64 	%fd161, [%rd374+3320];
	ld.global.f64 	%fd162, [%rd374+3312];
	ld.global.f64 	%fd163, [%rd374+3304];
	ld.global.f64 	%fd164, [%rd374+3296];
	ld.global.f64 	%fd165, [%rd374+3288];
	ld.global.f64 	%fd166, [%rd374+3280];
	ld.global.f64 	%fd167, [%rd374+3272];
	ld.global.f64 	%fd168, [%rd374+3264];
	ld.global.f64 	%fd169, [%rd374+3256];
	ld.global.f64 	%fd170, [%rd374+3248];
	ld.global.f64 	%fd171, [%rd374+3240];
	ld.global.f64 	%fd172, [%rd374+3232];
	ld.global.f64 	%fd173, [%rd374+3224];
	ld.global.f64 	%fd174, [%rd374+3216];
	ld.global.f64 	%fd175, [%rd374+3208];
	ld.global.f64 	%fd176, [%rd374+3200];
	ld.global.f64 	%fd177, [%rd374+3192];
	ld.global.f64 	%fd178, [%rd374+3184];
	ld.global.f64 	%fd179, [%rd374+3176];
	ld.global.f64 	%fd180, [%rd374+3168];
	ld.global.f64 	%fd181, [%rd374+3160];
	ld.global.f64 	%fd182, [%rd374+3152];
	ld.global.f64 	%fd183, [%rd374+3144];
	ld.global.f64 	%fd184, [%rd374+3136];
	ld.global.f64 	%fd185, [%rd374+3128];
	ld.global.f64 	%fd186, [%rd374+3120];
	ld.global.f64 	%fd187, [%rd374+3112];
	ld.global.f64 	%fd188, [%rd374+3104];
	ld.global.f64 	%fd189, [%rd374+3096];
	ld.global.f64 	%fd190, [%rd374+3088];
	ld.global.f64 	%fd191, [%rd374+3080];
	ld.global.f64 	%fd192, [%rd374+3072];
	ld.global.f64 	%fd193, [%rd374+3064];
	ld.global.f64 	%fd194, [%rd374+3056];
	ld.global.f64 	%fd195, [%rd374+3048];
	ld.global.f64 	%fd196, [%rd374+3040];
	ld.global.f64 	%fd197, [%rd374+3032];
	ld.global.f64 	%fd198, [%rd374+3024];
	ld.global.f64 	%fd199, [%rd374+3016];
	ld.global.f64 	%fd200, [%rd374+3008];
	ld.global.f64 	%fd201, [%rd374+3000];
	ld.global.f64 	%fd202, [%rd374+2992];
	ld.global.f64 	%fd203, [%rd374+2984];
	ld.global.f64 	%fd204, [%rd374+2976];
	ld.global.f64 	%fd205, [%rd374+2968];
	ld.global.f64 	%fd206, [%rd374+2960];
	ld.global.f64 	%fd207, [%rd374+2952];
	ld.global.f64 	%fd208, [%rd374+2944];
	ld.global.f64 	%fd209, [%rd374+2936];
	ld.global.f64 	%fd210, [%rd374+2928];
	ld.global.f64 	%fd211, [%rd374+2920];
	ld.global.f64 	%fd212, [%rd374+2912];
	ld.global.f64 	%fd213, [%rd374+2904];
	ld.global.f64 	%fd214, [%rd374+2896];
	ld.global.f64 	%fd215, [%rd374+2888];
	ld.global.f64 	%fd216, [%rd374+2880];
	ld.global.f64 	%fd217, [%rd374+2872];
	ld.global.f64 	%fd218, [%rd374+2864];
	ld.global.f64 	%fd219, [%rd374+2856];
	ld.global.f64 	%fd220, [%rd374+2848];
	ld.global.f64 	%fd221, [%rd374+2840];
	ld.global.f64 	%fd222, [%rd374+2832];
	ld.global.f64 	%fd223, [%rd374+2824];
	ld.global.f64 	%fd224, [%rd374+2816];
	ld.global.f64 	%fd225, [%rd374+2808];
	ld.global.f64 	%fd226, [%rd374+2800];
	ld.global.f64 	%fd227, [%rd374+2792];
	ld.global.f64 	%fd228, [%rd374+2784];
	ld.global.f64 	%fd229, [%rd374+2776];
	ld.global.f64 	%fd230, [%rd374+2768];
	ld.global.f64 	%fd231, [%rd374+2760];
	ld.global.f64 	%fd232, [%rd374+2752];
	ld.global.f64 	%fd233, [%rd374+2744];
	ld.global.f64 	%fd234, [%rd374+2736];
	ld.global.f64 	%fd235, [%rd374+2728];
	ld.global.f64 	%fd236, [%rd374+2720];
	ld.global.f64 	%fd237, [%rd374+2712];
	ld.global.f64 	%fd238, [%rd374+2704];
	ld.global.f64 	%fd239, [%rd374+2696];
	ld.global.f64 	%fd240, [%rd374+2688];
	ld.global.f64 	%fd241, [%rd374+2680];
	ld.global.f64 	%fd242, [%rd374+2672];
	ld.global.f64 	%fd243, [%rd374+2664];
	ld.global.f64 	%fd244, [%rd374+2656];
	ld.global.f64 	%fd245, [%rd374+2648];
	ld.global.f64 	%fd246, [%rd374+2640];
	ld.global.f64 	%fd247, [%rd374+2632];
	ld.global.f64 	%fd248, [%rd374+2624];
	ld.global.f64 	%fd249, [%rd374+2616];
	ld.global.f64 	%fd250, [%rd374+2608];
	ld.global.f64 	%fd251, [%rd374+2600];
	ld.global.f64 	%fd252, [%rd374+2592];
	ld.global.f64 	%fd253, [%rd374+2584];
	ld.global.f64 	%fd254, [%rd374+2576];
	ld.global.f64 	%fd255, [%rd374+2568];
	ld.global.f64 	%fd256, [%rd374+2560];
	ld.global.f64 	%fd257, [%rd374+2552];
	ld.global.f64 	%fd258, [%rd374+2544];
	ld.global.f64 	%fd259, [%rd374+2536];
	ld.global.f64 	%fd260, [%rd374+2528];
	ld.global.f64 	%fd261, [%rd374+2520];
	ld.global.f64 	%fd262, [%rd374+2512];
	ld.global.f64 	%fd263, [%rd374+2504];
	ld.global.f64 	%fd264, [%rd374+2496];
	ld.global.f64 	%fd265, [%rd374+2488];
	ld.global.f64 	%fd266, [%rd374+2480];
	ld.global.f64 	%fd267, [%rd374+2472];
	ld.global.f64 	%fd268, [%rd374+2464];
	ld.global.f64 	%fd269, [%rd374+2456];
	ld.global.f64 	%fd270, [%rd374+2448];
	ld.global.f64 	%fd271, [%rd374+2440];
	ld.global.f64 	%fd272, [%rd374+2432];
	ld.global.f64 	%fd273, [%rd374+2424];
	ld.global.f64 	%fd274, [%rd374+2416];
	ld.global.f64 	%fd275, [%rd374+2408];
	ld.global.f64 	%fd276, [%rd374+2400];
	ld.global.f64 	%fd277, [%rd374+2392];
	ld.global.f64 	%fd278, [%rd374+2384];
	ld.global.f64 	%fd279, [%rd374+2376];
	ld.global.f64 	%fd280, [%rd374+2368];
	ld.global.f64 	%fd281, [%rd374+2360];
	ld.global.f64 	%fd282, [%rd374+2352];
	ld.global.f64 	%fd283, [%rd374+2344];
	ld.global.f64 	%fd284, [%rd374+2336];
	ld.global.f64 	%fd285, [%rd374+2328];
	ld.global.f64 	%fd286, [%rd374+2320];
	ld.global.f64 	%fd287, [%rd374+2312];
	ld.global.f64 	%fd288, [%rd374+2304];
	ld.global.f64 	%fd289, [%rd374+2296];
	ld.global.f64 	%fd290, [%rd374+2288];
	ld.global.f64 	%fd291, [%rd374+2280];
	ld.global.f64 	%fd292, [%rd374+2272];
	ld.global.f64 	%fd293, [%rd374+2264];
	ld.global.f64 	%fd294, [%rd374+2256];
	ld.global.f64 	%fd295, [%rd374+2248];
	ld.global.f64 	%fd296, [%rd374+2240];
	ld.global.f64 	%fd297, [%rd374+2232];
	ld.global.f64 	%fd298, [%rd374+2224];
	ld.global.f64 	%fd299, [%rd374+2216];
	ld.global.f64 	%fd300, [%rd374+2208];
	ld.global.f64 	%fd301, [%rd374+2200];
	ld.global.f64 	%fd302, [%rd374+2192];
	ld.global.f64 	%fd303, [%rd374+2184];
	ld.global.f64 	%fd304, [%rd374+2176];
	ld.global.f64 	%fd305, [%rd374+2168];
	ld.global.f64 	%fd306, [%rd374+2160];
	ld.global.f64 	%fd307, [%rd374+2152];
	ld.global.f64 	%fd308, [%rd374+2144];
	ld.global.f64 	%fd309, [%rd374+2136];
	ld.global.f64 	%fd310, [%rd374+2128];
	ld.global.f64 	%fd311, [%rd374+2120];
	ld.global.f64 	%fd312, [%rd374+2112];
	ld.global.f64 	%fd313, [%rd374+2104];
	ld.global.f64 	%fd314, [%rd374+2096];
	ld.global.f64 	%fd315, [%rd374+2088];
	ld.global.f64 	%fd316, [%rd374+2080];
	ld.global.f64 	%fd317, [%rd374+2072];
	ld.global.f64 	%fd318, [%rd374+2064];
	ld.global.f64 	%fd319, [%rd374+2056];
	ld.global.f64 	%fd320, [%rd374+2048];
	ld.global.f64 	%fd321, [%rd374+2040];
	ld.global.f64 	%fd322, [%rd374+2032];
	ld.global.f64 	%fd323, [%rd374+2024];
	ld.global.f64 	%fd324, [%rd374+2016];
	ld.global.f64 	%fd325, [%rd374+2008];
	ld.global.f64 	%fd326, [%rd374+2000];
	ld.global.f64 	%fd327, [%rd374+1992];
	ld.global.f64 	%fd328, [%rd374+1984];
	ld.global.f64 	%fd329, [%rd374+1976];
	ld.global.f64 	%fd330, [%rd374+1968];
	ld.global.f64 	%fd331, [%rd374+1960];
	ld.global.f64 	%fd332, [%rd374+1952];
	ld.global.f64 	%fd333, [%rd374+1944];
	ld.global.f64 	%fd334, [%rd374+1936];
	ld.global.f64 	%fd335, [%rd374+1928];
	ld.global.f64 	%fd336, [%rd374+1920];
	ld.global.f64 	%fd337, [%rd374+1912];
	ld.global.f64 	%fd338, [%rd374+1904];
	ld.global.f64 	%fd339, [%rd374+1896];
	ld.global.f64 	%fd340, [%rd374+1888];
	ld.global.f64 	%fd341, [%rd374+1880];
	ld.global.f64 	%fd342, [%rd374+1872];
	ld.global.f64 	%fd343, [%rd374+1864];
	ld.global.f64 	%fd344, [%rd374+1856];
	ld.global.f64 	%fd345, [%rd374+1848];
	ld.global.f64 	%fd346, [%rd374+1840];
	ld.global.f64 	%fd347, [%rd374+1832];
	ld.global.f64 	%fd348, [%rd374+1824];
	ld.global.f64 	%fd349, [%rd374+1816];
	ld.global.f64 	%fd350, [%rd374+1808];
	ld.global.f64 	%fd351, [%rd374+1800];
	ld.global.f64 	%fd352, [%rd374+1792];
	ld.global.f64 	%fd353, [%rd374+1784];
	ld.global.f64 	%fd354, [%rd374+1776];
	ld.global.f64 	%fd355, [%rd374+1768];
	ld.global.f64 	%fd356, [%rd374+1760];
	ld.global.f64 	%fd357, [%rd374+1752];
	ld.global.f64 	%fd358, [%rd374+1744];
	ld.global.f64 	%fd359, [%rd374+1736];
	ld.global.f64 	%fd360, [%rd374+1728];
	ld.global.f64 	%fd361, [%rd374+1720];
	ld.global.f64 	%fd362, [%rd374+1712];
	ld.global.f64 	%fd363, [%rd374+1704];
	ld.global.f64 	%fd364, [%rd374+1696];
	ld.global.f64 	%fd365, [%rd374+1688];
	ld.global.f64 	%fd366, [%rd374+1680];
	ld.global.f64 	%fd367, [%rd374+1672];
	ld.global.f64 	%fd368, [%rd374+1664];
	ld.global.f64 	%fd369, [%rd374+1656];
	ld.global.f64 	%fd370, [%rd374+1648];
	ld.global.f64 	%fd371, [%rd374+1640];
	ld.global.f64 	%fd372, [%rd374+1632];
	ld.global.f64 	%fd373, [%rd374+1624];
	ld.global.f64 	%fd374, [%rd374+1616];
	ld.global.f64 	%fd375, [%rd374+1608];
	ld.global.f64 	%fd376, [%rd374+1600];
	ld.global.f64 	%fd377, [%rd374+1592];
	ld.global.f64 	%fd378, [%rd374+1584];
	ld.global.f64 	%fd379, [%rd374+1576];
	ld.global.f64 	%fd380, [%rd374+1568];
	ld.global.f64 	%fd381, [%rd374+1560];
	ld.global.f64 	%fd382, [%rd374+1552];
	ld.global.f64 	%fd383, [%rd374+1544];
	ld.global.f64 	%fd384, [%rd374+1536];
	ld.global.f64 	%fd385, [%rd374+1528];
	ld.global.f64 	%fd386, [%rd374+1520];
	ld.global.f64 	%fd387, [%rd374+1512];
	ld.global.f64 	%fd388, [%rd374+1504];
	ld.global.f64 	%fd389, [%rd374+1496];
	ld.global.f64 	%fd390, [%rd374+1488];
	ld.global.f64 	%fd391, [%rd374+1480];
	ld.global.f64 	%fd392, [%rd374+1472];
	ld.global.f64 	%fd393, [%rd374+1464];
	ld.global.f64 	%fd394, [%rd374+1456];
	ld.global.f64 	%fd395, [%rd374+1448];
	ld.global.f64 	%fd396, [%rd374+1440];
	ld.global.f64 	%fd397, [%rd374+1432];
	ld.global.f64 	%fd398, [%rd374+1424];
	ld.global.f64 	%fd399, [%rd374+1416];
	ld.global.f64 	%fd400, [%rd374+1408];
	ld.global.f64 	%fd401, [%rd374+1400];
	ld.global.f64 	%fd402, [%rd374+1392];
	ld.global.f64 	%fd403, [%rd374+1384];
	ld.global.f64 	%fd404, [%rd374+1376];
	ld.global.f64 	%fd405, [%rd374+1368];
	ld.global.f64 	%fd406, [%rd374+1360];
	ld.global.f64 	%fd407, [%rd374+1352];
	ld.global.f64 	%fd408, [%rd374+1344];
	ld.global.f64 	%fd409, [%rd374+1336];
	ld.global.f64 	%fd410, [%rd374+1328];
	ld.global.f64 	%fd411, [%rd374+1320];
	ld.global.f64 	%fd412, [%rd374+1312];
	ld.global.f64 	%fd413, [%rd374+1304];
	ld.global.f64 	%fd414, [%rd374+1296];
	ld.global.f64 	%fd415, [%rd374+1288];
	ld.global.f64 	%fd416, [%rd374+1280];
	ld.global.f64 	%fd417, [%rd374+1272];
	ld.global.f64 	%fd418, [%rd374+1264];
	ld.global.f64 	%fd419, [%rd374+1256];
	ld.global.f64 	%fd420, [%rd374+1248];
	ld.global.f64 	%fd421, [%rd374+1240];
	ld.global.f64 	%fd422, [%rd374+1232];
	ld.global.f64 	%fd423, [%rd374+1224];
	ld.global.f64 	%fd424, [%rd374+1216];
	ld.global.f64 	%fd425, [%rd374+1208];
	ld.global.f64 	%fd426, [%rd374+1200];
	ld.global.f64 	%fd427, [%rd374+1192];
	ld.global.f64 	%fd428, [%rd374+1184];
	ld.global.f64 	%fd429, [%rd374+1176];
	ld.global.f64 	%fd430, [%rd374+1168];
	ld.global.f64 	%fd431, [%rd374+1160];
	ld.global.f64 	%fd432, [%rd374+1152];
	ld.global.f64 	%fd433, [%rd374+1144];
	ld.global.f64 	%fd434, [%rd374+1136];
	ld.global.f64 	%fd435, [%rd374+1128];
	ld.global.f64 	%fd436, [%rd374+1120];
	ld.global.f64 	%fd437, [%rd374+1112];
	ld.global.f64 	%fd438, [%rd374+1104];
	ld.global.f64 	%fd439, [%rd374+1096];
	ld.global.f64 	%fd440, [%rd374+1088];
	ld.global.f64 	%fd441, [%rd374+1080];
	ld.global.f64 	%fd442, [%rd374+1072];
	ld.global.f64 	%fd443, [%rd374+1064];
	ld.global.f64 	%fd444, [%rd374+1056];
	ld.global.f64 	%fd445, [%rd374+1048];
	ld.global.f64 	%fd446, [%rd374+1040];
	ld.global.f64 	%fd447, [%rd374+1032];
	ld.global.f64 	%fd448, [%rd374+1024];
	ld.global.f64 	%fd449, [%rd374+1016];
	ld.global.f64 	%fd450, [%rd374+1008];
	ld.global.f64 	%fd451, [%rd374+1000];
	ld.global.f64 	%fd452, [%rd374+992];
	ld.global.f64 	%fd453, [%rd374+984];
	ld.global.f64 	%fd454, [%rd374+976];
	ld.global.f64 	%fd455, [%rd374+968];
	ld.global.f64 	%fd456, [%rd374+960];
	ld.global.f64 	%fd457, [%rd374+952];
	ld.global.f64 	%fd458, [%rd374+944];
	ld.global.f64 	%fd459, [%rd374+936];
	ld.global.f64 	%fd460, [%rd374+928];
	ld.global.f64 	%fd461, [%rd374+920];
	ld.global.f64 	%fd462, [%rd374+912];
	ld.global.f64 	%fd463, [%rd374+904];
	ld.global.f64 	%fd464, [%rd374+896];
	ld.global.f64 	%fd465, [%rd374+888];
	ld.global.f64 	%fd466, [%rd374+880];
	ld.global.f64 	%fd467, [%rd374+872];
	ld.global.f64 	%fd468, [%rd374+864];
	ld.global.f64 	%fd469, [%rd374+856];
	ld.global.f64 	%fd470, [%rd374+848];
	ld.global.f64 	%fd471, [%rd374+840];
	ld.global.f64 	%fd472, [%rd374+832];
	ld.global.f64 	%fd473, [%rd374+824];
	ld.global.f64 	%fd474, [%rd374+816];
	ld.global.f64 	%fd475, [%rd374+808];
	ld.global.f64 	%fd476, [%rd374+800];
	ld.global.f64 	%fd477, [%rd374+792];
	ld.global.f64 	%fd478, [%rd374+784];
	ld.global.f64 	%fd479, [%rd374+776];
	ld.global.f64 	%fd480, [%rd374+768];
	ld.global.f64 	%fd481, [%rd374+760];
	ld.global.f64 	%fd482, [%rd374+752];
	ld.global.f64 	%fd483, [%rd374+744];
	ld.global.f64 	%fd484, [%rd374+736];
	ld.global.f64 	%fd485, [%rd374+728];
	ld.global.f64 	%fd486, [%rd374+720];
	ld.global.f64 	%fd487, [%rd374+712];
	ld.global.f64 	%fd488, [%rd374+704];
	ld.global.f64 	%fd489, [%rd374+696];
	ld.global.f64 	%fd490, [%rd374+688];
	ld.global.f64 	%fd491, [%rd374+680];
	ld.global.f64 	%fd492, [%rd374+672];
	ld.global.f64 	%fd493, [%rd374+664];
	ld.global.f64 	%fd494, [%rd374+656];
	ld.global.f64 	%fd495, [%rd374+648];
	ld.global.f64 	%fd496, [%rd374+640];
	ld.global.f64 	%fd497, [%rd374+632];
	ld.global.f64 	%fd498, [%rd374+624];
	ld.global.f64 	%fd499, [%rd374+616];
	ld.global.f64 	%fd500, [%rd374+608];
	ld.global.f64 	%fd501, [%rd374+600];
	ld.global.f64 	%fd502, [%rd374+592];
	ld.global.f64 	%fd503, [%rd374+584];
	ld.global.f64 	%fd504, [%rd374+576];
	ld.global.f64 	%fd505, [%rd374+568];
	ld.global.f64 	%fd506, [%rd374+560];
	ld.global.f64 	%fd507, [%rd374+552];
	ld.global.f64 	%fd508, [%rd374+544];
	ld.global.f64 	%fd509, [%rd374+536];
	ld.global.f64 	%fd510, [%rd374+528];
	ld.global.f64 	%fd511, [%rd374+520];
	ld.global.f64 	%fd512, [%rd374+512];
	ld.global.f64 	%fd513, [%rd374+504];
	ld.global.f64 	%fd514, [%rd374+496];
	ld.global.f64 	%fd515, [%rd374+488];
	ld.global.f64 	%fd516, [%rd374+480];
	ld.global.f64 	%fd517, [%rd374+472];
	ld.global.f64 	%fd518, [%rd374+464];
	ld.global.f64 	%fd519, [%rd374+456];
	ld.global.f64 	%fd520, [%rd374+448];
	ld.global.f64 	%fd521, [%rd374+440];
	ld.global.f64 	%fd522, [%rd374+432];
	ld.global.f64 	%fd523, [%rd374+424];
	ld.global.f64 	%fd524, [%rd374+416];
	ld.global.f64 	%fd525, [%rd374+408];
	ld.global.f64 	%fd526, [%rd374+400];
	ld.global.f64 	%fd527, [%rd374+392];
	ld.global.f64 	%fd528, [%rd374+384];
	ld.global.f64 	%fd529, [%rd374+376];
	ld.global.f64 	%fd530, [%rd374+368];
	ld.global.f64 	%fd531, [%rd374+360];
	ld.global.f64 	%fd532, [%rd374+352];
	ld.global.f64 	%fd533, [%rd374+344];
	ld.global.f64 	%fd534, [%rd374+336];
	ld.global.f64 	%fd535, [%rd374+328];
	ld.global.f64 	%fd536, [%rd374+320];
	ld.global.f64 	%fd537, [%rd374+312];
	ld.global.f64 	%fd538, [%rd374+304];
	ld.global.f64 	%fd539, [%rd374+296];
	ld.global.f64 	%fd540, [%rd374+288];
	ld.global.f64 	%fd541, [%rd374+280];
	ld.global.f64 	%fd542, [%rd374+272];
	ld.global.f64 	%fd543, [%rd374+264];
	ld.global.f64 	%fd544, [%rd374+256];
	ld.global.f64 	%fd545, [%rd374+248];
	ld.global.f64 	%fd546, [%rd374+240];
	ld.global.f64 	%fd547, [%rd374+232];
	ld.global.f64 	%fd548, [%rd374+224];
	ld.global.f64 	%fd549, [%rd374+216];
	ld.global.f64 	%fd550, [%rd374+208];
	ld.global.f64 	%fd551, [%rd374+200];
	ld.global.f64 	%fd552, [%rd374+192];
	ld.global.f64 	%fd553, [%rd374+184];
	ld.global.f64 	%fd554, [%rd374+176];
	ld.global.f64 	%fd555, [%rd374+168];
	ld.global.f64 	%fd556, [%rd374+160];
	ld.global.f64 	%fd557, [%rd374+152];
	ld.global.f64 	%fd558, [%rd374+144];
	ld.global.f64 	%fd559, [%rd374+136];
	ld.global.f64 	%fd560, [%rd374+128];
	ld.global.f64 	%fd561, [%rd374+120];
	ld.global.f64 	%fd562, [%rd374+112];
	ld.global.f64 	%fd563, [%rd374+104];
	ld.global.f64 	%fd564, [%rd374+96];
	ld.global.f64 	%fd565, [%rd374+88];
	ld.global.f64 	%fd566, [%rd374+80];
	ld.global.f64 	%fd567, [%rd374+72];
	ld.global.f64 	%fd568, [%rd374+64];
	ld.global.f64 	%fd569, [%rd374+56];
	ld.global.f64 	%fd570, [%rd374+48];
	ld.global.f64 	%fd571, [%rd374+40];
	ld.global.f64 	%fd572, [%rd374+32];
	ld.global.f64 	%fd573, [%rd374+24];
	ld.global.f64 	%fd574, [%rd374+16];
	ld.global.f64 	%fd575, [%rd374+8];
	ld.global.f64 	%fd576, [%rd374];
	mul.lo.s64 	%rd375, %rd25, %rd12;
	add.s64 	%rd27, %rd7, %rd375;
	ld.global.u32 	%r3, [%rd27];
	shl.b32 	%r4, %r3, 4;
	add.s64 	%rd28, %rd3970, 112;
	ld.param.u64 	%rd29, [%rd3970+112];
	ld.param.u32 	%r5, [%rd3970+144];
	ld.param.u32 	%r6, [%rd3970+172];
	setp.le.s32 	%p10, %r6, %r4;
	selp.u16 	%rs33, 1, 0, %p10;
	shr.u32 	%r948, %r3, 27;
	cvt.u16.u32 	%rs34, %r948;
	and.b16  	%rs35, %rs34, 1;
	or.b16  	%rs36, %rs35, %rs33;
	setp.eq.s16 	%p11, %rs36, 0;
	add.u64 	%rd377, %SP, 0;
	add.u64 	%rd30, %SPL, 0;
	@%p11 bra 	$L__BB13_17;

	st.local.v2.u32 	[%rd30], {%r4, %r6};
	mov.u64 	%rd378, $str$1;
	cvta.global.u64 	%rd379, %rd378;
	{ // callseq 321, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd379;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r949, [retval0+0];
	} // callseq 321
	bra.uni 	$L__BB13_18;

$L__BB13_17:
	mul.wide.s32 	%rd390, %r5, %r4;
	add.s64 	%rd381, %rd29, %rd390;
	// begin inline asm
	{ atom.add.f64 %fd2305,[%rd381],%fd576; }

	// end inline asm
	add.s64 	%rd382, %rd381, 8;
	// begin inline asm
	{ atom.add.f64 %fd2307,[%rd382],%fd575; }

	// end inline asm
	add.s64 	%rd383, %rd381, 16;
	// begin inline asm
	{ atom.add.f64 %fd2309,[%rd383],%fd574; }

	// end inline asm
	add.s64 	%rd384, %rd381, 24;
	// begin inline asm
	{ atom.add.f64 %fd2311,[%rd384],%fd552; }

	// end inline asm
	add.s64 	%rd385, %rd381, 32;
	// begin inline asm
	{ atom.add.f64 %fd2313,[%rd385],%fd551; }

	// end inline asm
	add.s64 	%rd386, %rd381, 40;
	// begin inline asm
	{ atom.add.f64 %fd2315,[%rd386],%fd550; }

	// end inline asm
	add.s64 	%rd387, %rd381, 48;
	// begin inline asm
	{ atom.add.f64 %fd2317,[%rd387],%fd528; }

	// end inline asm
	add.s64 	%rd388, %rd381, 56;
	// begin inline asm
	{ atom.add.f64 %fd2319,[%rd388],%fd527; }

	// end inline asm
	add.s64 	%rd389, %rd381, 64;
	// begin inline asm
	{ atom.add.f64 %fd2321,[%rd389],%fd526; }

	// end inline asm

$L__BB13_18:
	ld.param.u64 	%rd31, [%rd28];
	ld.param.u32 	%r7, [%rd28+32];
	ld.param.u32 	%r8, [%rd28+60];
	add.s32 	%r950, %r4, 1;
	setp.le.s32 	%p12, %r8, %r950;
	selp.u16 	%rs37, 1, 0, %p12;
	shr.u32 	%r951, %r950, 31;
	cvt.u16.u32 	%rs38, %r951;
	or.b16  	%rs39, %rs37, %rs38;
	setp.eq.s16 	%p13, %rs39, 0;
	@%p13 bra 	$L__BB13_20;

	add.s32 	%r2007, %r4, 1;
	st.local.v2.u32 	[%rd30], {%r2007, %r8};
	mov.u64 	%rd391, $str$1;
	cvta.global.u64 	%rd392, %rd391;
	{ // callseq 322, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd392;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r953, [retval0+0];
	} // callseq 322
	bra.uni 	$L__BB13_21;

$L__BB13_20:
	add.s32 	%r2678, %r4, 1;
	mul.wide.s32 	%rd403, %r7, %r2678;
	add.s64 	%rd394, %rd31, %rd403;
	// begin inline asm
	{ atom.add.f64 %fd2323,[%rd394],%fd573; }

	// end inline asm
	add.s64 	%rd395, %rd394, 8;
	// begin inline asm
	{ atom.add.f64 %fd2325,[%rd395],%fd572; }

	// end inline asm
	add.s64 	%rd396, %rd394, 16;
	// begin inline asm
	{ atom.add.f64 %fd2327,[%rd396],%fd571; }

	// end inline asm
	add.s64 	%rd397, %rd394, 24;
	// begin inline asm
	{ atom.add.f64 %fd2329,[%rd397],%fd549; }

	// end inline asm
	add.s64 	%rd398, %rd394, 32;
	// begin inline asm
	{ atom.add.f64 %fd2331,[%rd398],%fd548; }

	// end inline asm
	add.s64 	%rd399, %rd394, 40;
	// begin inline asm
	{ atom.add.f64 %fd2333,[%rd399],%fd547; }

	// end inline asm
	add.s64 	%rd400, %rd394, 48;
	// begin inline asm
	{ atom.add.f64 %fd2335,[%rd400],%fd525; }

	// end inline asm
	add.s64 	%rd401, %rd394, 56;
	// begin inline asm
	{ atom.add.f64 %fd2337,[%rd401],%fd524; }

	// end inline asm
	add.s64 	%rd402, %rd394, 64;
	// begin inline asm
	{ atom.add.f64 %fd2339,[%rd402],%fd523; }

	// end inline asm

$L__BB13_21:
	ld.param.u64 	%rd32, [%rd28];
	ld.param.u32 	%r9, [%rd28+32];
	ld.param.u32 	%r10, [%rd28+60];
	add.s32 	%r955, %r4, 2;
	setp.le.s32 	%p14, %r10, %r955;
	selp.u16 	%rs40, 1, 0, %p14;
	shr.u32 	%r956, %r955, 31;
	cvt.u16.u32 	%rs41, %r956;
	or.b16  	%rs42, %rs40, %rs41;
	setp.eq.s16 	%p15, %rs42, 0;
	@%p15 bra 	$L__BB13_23;

	add.s32 	%r2008, %r4, 2;
	st.local.v2.u32 	[%rd30], {%r2008, %r10};
	mov.u64 	%rd404, $str$1;
	cvta.global.u64 	%rd405, %rd404;
	{ // callseq 323, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd405;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r958, [retval0+0];
	} // callseq 323
	bra.uni 	$L__BB13_24;

$L__BB13_23:
	add.s32 	%r2677, %r4, 2;
	mul.wide.s32 	%rd416, %r9, %r2677;
	add.s64 	%rd407, %rd32, %rd416;
	// begin inline asm
	{ atom.add.f64 %fd2341,[%rd407],%fd570; }

	// end inline asm
	add.s64 	%rd408, %rd407, 8;
	// begin inline asm
	{ atom.add.f64 %fd2343,[%rd408],%fd569; }

	// end inline asm
	add.s64 	%rd409, %rd407, 16;
	// begin inline asm
	{ atom.add.f64 %fd2345,[%rd409],%fd568; }

	// end inline asm
	add.s64 	%rd410, %rd407, 24;
	// begin inline asm
	{ atom.add.f64 %fd2347,[%rd410],%fd546; }

	// end inline asm
	add.s64 	%rd411, %rd407, 32;
	// begin inline asm
	{ atom.add.f64 %fd2349,[%rd411],%fd545; }

	// end inline asm
	add.s64 	%rd412, %rd407, 40;
	// begin inline asm
	{ atom.add.f64 %fd2351,[%rd412],%fd544; }

	// end inline asm
	add.s64 	%rd413, %rd407, 48;
	// begin inline asm
	{ atom.add.f64 %fd2353,[%rd413],%fd522; }

	// end inline asm
	add.s64 	%rd414, %rd407, 56;
	// begin inline asm
	{ atom.add.f64 %fd2355,[%rd414],%fd521; }

	// end inline asm
	add.s64 	%rd415, %rd407, 64;
	// begin inline asm
	{ atom.add.f64 %fd2357,[%rd415],%fd520; }

	// end inline asm

$L__BB13_24:
	ld.param.u64 	%rd33, [%rd28];
	ld.param.u32 	%r11, [%rd28+32];
	ld.param.u32 	%r12, [%rd28+60];
	add.s32 	%r960, %r4, 3;
	setp.le.s32 	%p16, %r12, %r960;
	selp.u16 	%rs43, 1, 0, %p16;
	shr.u32 	%r961, %r960, 31;
	cvt.u16.u32 	%rs44, %r961;
	or.b16  	%rs45, %rs43, %rs44;
	setp.eq.s16 	%p17, %rs45, 0;
	@%p17 bra 	$L__BB13_26;

	add.s32 	%r2009, %r4, 3;
	st.local.v2.u32 	[%rd30], {%r2009, %r12};
	mov.u64 	%rd417, $str$1;
	cvta.global.u64 	%rd418, %rd417;
	{ // callseq 324, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd418;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r963, [retval0+0];
	} // callseq 324
	bra.uni 	$L__BB13_27;

$L__BB13_26:
	add.s32 	%r2676, %r4, 3;
	mul.wide.s32 	%rd429, %r11, %r2676;
	add.s64 	%rd420, %rd33, %rd429;
	// begin inline asm
	{ atom.add.f64 %fd2359,[%rd420],%fd567; }

	// end inline asm
	add.s64 	%rd421, %rd420, 8;
	// begin inline asm
	{ atom.add.f64 %fd2361,[%rd421],%fd566; }

	// end inline asm
	add.s64 	%rd422, %rd420, 16;
	// begin inline asm
	{ atom.add.f64 %fd2363,[%rd422],%fd565; }

	// end inline asm
	add.s64 	%rd423, %rd420, 24;
	// begin inline asm
	{ atom.add.f64 %fd2365,[%rd423],%fd543; }

	// end inline asm
	add.s64 	%rd424, %rd420, 32;
	// begin inline asm
	{ atom.add.f64 %fd2367,[%rd424],%fd542; }

	// end inline asm
	add.s64 	%rd425, %rd420, 40;
	// begin inline asm
	{ atom.add.f64 %fd2369,[%rd425],%fd541; }

	// end inline asm
	add.s64 	%rd426, %rd420, 48;
	// begin inline asm
	{ atom.add.f64 %fd2371,[%rd426],%fd519; }

	// end inline asm
	add.s64 	%rd427, %rd420, 56;
	// begin inline asm
	{ atom.add.f64 %fd2373,[%rd427],%fd518; }

	// end inline asm
	add.s64 	%rd428, %rd420, 64;
	// begin inline asm
	{ atom.add.f64 %fd2375,[%rd428],%fd517; }

	// end inline asm

$L__BB13_27:
	ld.param.u64 	%rd34, [%rd28];
	ld.param.u32 	%r13, [%rd28+32];
	ld.param.u32 	%r14, [%rd28+60];
	add.s32 	%r965, %r4, 4;
	setp.le.s32 	%p18, %r14, %r965;
	selp.u16 	%rs46, 1, 0, %p18;
	shr.u32 	%r966, %r965, 31;
	cvt.u16.u32 	%rs47, %r966;
	or.b16  	%rs48, %rs46, %rs47;
	setp.eq.s16 	%p19, %rs48, 0;
	@%p19 bra 	$L__BB13_29;

	add.s32 	%r2010, %r4, 4;
	st.local.v2.u32 	[%rd30], {%r2010, %r14};
	mov.u64 	%rd430, $str$1;
	cvta.global.u64 	%rd431, %rd430;
	{ // callseq 325, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd431;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r968, [retval0+0];
	} // callseq 325
	bra.uni 	$L__BB13_30;

$L__BB13_29:
	add.s32 	%r2675, %r4, 4;
	mul.wide.s32 	%rd442, %r13, %r2675;
	add.s64 	%rd433, %rd34, %rd442;
	// begin inline asm
	{ atom.add.f64 %fd2377,[%rd433],%fd504; }

	// end inline asm
	add.s64 	%rd434, %rd433, 8;
	// begin inline asm
	{ atom.add.f64 %fd2379,[%rd434],%fd503; }

	// end inline asm
	add.s64 	%rd435, %rd433, 16;
	// begin inline asm
	{ atom.add.f64 %fd2381,[%rd435],%fd502; }

	// end inline asm
	add.s64 	%rd436, %rd433, 24;
	// begin inline asm
	{ atom.add.f64 %fd2383,[%rd436],%fd480; }

	// end inline asm
	add.s64 	%rd437, %rd433, 32;
	// begin inline asm
	{ atom.add.f64 %fd2385,[%rd437],%fd479; }

	// end inline asm
	add.s64 	%rd438, %rd433, 40;
	// begin inline asm
	{ atom.add.f64 %fd2387,[%rd438],%fd478; }

	// end inline asm
	add.s64 	%rd439, %rd433, 48;
	// begin inline asm
	{ atom.add.f64 %fd2389,[%rd439],%fd456; }

	// end inline asm
	add.s64 	%rd440, %rd433, 56;
	// begin inline asm
	{ atom.add.f64 %fd2391,[%rd440],%fd455; }

	// end inline asm
	add.s64 	%rd441, %rd433, 64;
	// begin inline asm
	{ atom.add.f64 %fd2393,[%rd441],%fd454; }

	// end inline asm

$L__BB13_30:
	ld.param.u64 	%rd35, [%rd28];
	ld.param.u32 	%r15, [%rd28+32];
	ld.param.u32 	%r16, [%rd28+60];
	add.s32 	%r970, %r4, 5;
	setp.le.s32 	%p20, %r16, %r970;
	selp.u16 	%rs49, 1, 0, %p20;
	shr.u32 	%r971, %r970, 31;
	cvt.u16.u32 	%rs50, %r971;
	or.b16  	%rs51, %rs49, %rs50;
	setp.eq.s16 	%p21, %rs51, 0;
	@%p21 bra 	$L__BB13_32;

	add.s32 	%r2011, %r4, 5;
	st.local.v2.u32 	[%rd30], {%r2011, %r16};
	mov.u64 	%rd443, $str$1;
	cvta.global.u64 	%rd444, %rd443;
	{ // callseq 326, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd444;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r973, [retval0+0];
	} // callseq 326
	bra.uni 	$L__BB13_33;

$L__BB13_32:
	add.s32 	%r2674, %r4, 5;
	mul.wide.s32 	%rd455, %r15, %r2674;
	add.s64 	%rd446, %rd35, %rd455;
	// begin inline asm
	{ atom.add.f64 %fd2395,[%rd446],%fd501; }

	// end inline asm
	add.s64 	%rd447, %rd446, 8;
	// begin inline asm
	{ atom.add.f64 %fd2397,[%rd447],%fd500; }

	// end inline asm
	add.s64 	%rd448, %rd446, 16;
	// begin inline asm
	{ atom.add.f64 %fd2399,[%rd448],%fd499; }

	// end inline asm
	add.s64 	%rd449, %rd446, 24;
	// begin inline asm
	{ atom.add.f64 %fd2401,[%rd449],%fd477; }

	// end inline asm
	add.s64 	%rd450, %rd446, 32;
	// begin inline asm
	{ atom.add.f64 %fd2403,[%rd450],%fd476; }

	// end inline asm
	add.s64 	%rd451, %rd446, 40;
	// begin inline asm
	{ atom.add.f64 %fd2405,[%rd451],%fd475; }

	// end inline asm
	add.s64 	%rd452, %rd446, 48;
	// begin inline asm
	{ atom.add.f64 %fd2407,[%rd452],%fd453; }

	// end inline asm
	add.s64 	%rd453, %rd446, 56;
	// begin inline asm
	{ atom.add.f64 %fd2409,[%rd453],%fd452; }

	// end inline asm
	add.s64 	%rd454, %rd446, 64;
	// begin inline asm
	{ atom.add.f64 %fd2411,[%rd454],%fd451; }

	// end inline asm

$L__BB13_33:
	ld.param.u64 	%rd36, [%rd28];
	ld.param.u32 	%r17, [%rd28+32];
	ld.param.u32 	%r18, [%rd28+60];
	add.s32 	%r975, %r4, 6;
	setp.le.s32 	%p22, %r18, %r975;
	selp.u16 	%rs52, 1, 0, %p22;
	shr.u32 	%r976, %r975, 31;
	cvt.u16.u32 	%rs53, %r976;
	or.b16  	%rs54, %rs52, %rs53;
	setp.eq.s16 	%p23, %rs54, 0;
	@%p23 bra 	$L__BB13_35;

	add.s32 	%r2012, %r4, 6;
	st.local.v2.u32 	[%rd30], {%r2012, %r18};
	mov.u64 	%rd456, $str$1;
	cvta.global.u64 	%rd457, %rd456;
	{ // callseq 327, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd457;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r978, [retval0+0];
	} // callseq 327
	bra.uni 	$L__BB13_36;

$L__BB13_35:
	add.s32 	%r2673, %r4, 6;
	mul.wide.s32 	%rd468, %r17, %r2673;
	add.s64 	%rd459, %rd36, %rd468;
	// begin inline asm
	{ atom.add.f64 %fd2413,[%rd459],%fd498; }

	// end inline asm
	add.s64 	%rd460, %rd459, 8;
	// begin inline asm
	{ atom.add.f64 %fd2415,[%rd460],%fd497; }

	// end inline asm
	add.s64 	%rd461, %rd459, 16;
	// begin inline asm
	{ atom.add.f64 %fd2417,[%rd461],%fd496; }

	// end inline asm
	add.s64 	%rd462, %rd459, 24;
	// begin inline asm
	{ atom.add.f64 %fd2419,[%rd462],%fd474; }

	// end inline asm
	add.s64 	%rd463, %rd459, 32;
	// begin inline asm
	{ atom.add.f64 %fd2421,[%rd463],%fd473; }

	// end inline asm
	add.s64 	%rd464, %rd459, 40;
	// begin inline asm
	{ atom.add.f64 %fd2423,[%rd464],%fd472; }

	// end inline asm
	add.s64 	%rd465, %rd459, 48;
	// begin inline asm
	{ atom.add.f64 %fd2425,[%rd465],%fd450; }

	// end inline asm
	add.s64 	%rd466, %rd459, 56;
	// begin inline asm
	{ atom.add.f64 %fd2427,[%rd466],%fd449; }

	// end inline asm
	add.s64 	%rd467, %rd459, 64;
	// begin inline asm
	{ atom.add.f64 %fd2429,[%rd467],%fd448; }

	// end inline asm

$L__BB13_36:
	ld.param.u64 	%rd37, [%rd28];
	ld.param.u32 	%r19, [%rd28+32];
	ld.param.u32 	%r20, [%rd28+60];
	add.s32 	%r980, %r4, 7;
	setp.le.s32 	%p24, %r20, %r980;
	selp.u16 	%rs55, 1, 0, %p24;
	shr.u32 	%r981, %r980, 31;
	cvt.u16.u32 	%rs56, %r981;
	or.b16  	%rs57, %rs55, %rs56;
	setp.eq.s16 	%p25, %rs57, 0;
	@%p25 bra 	$L__BB13_38;

	add.s32 	%r2013, %r4, 7;
	st.local.v2.u32 	[%rd30], {%r2013, %r20};
	mov.u64 	%rd469, $str$1;
	cvta.global.u64 	%rd470, %rd469;
	{ // callseq 328, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd470;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r983, [retval0+0];
	} // callseq 328
	bra.uni 	$L__BB13_39;

$L__BB13_38:
	add.s32 	%r2672, %r4, 7;
	mul.wide.s32 	%rd481, %r19, %r2672;
	add.s64 	%rd472, %rd37, %rd481;
	// begin inline asm
	{ atom.add.f64 %fd2431,[%rd472],%fd495; }

	// end inline asm
	add.s64 	%rd473, %rd472, 8;
	// begin inline asm
	{ atom.add.f64 %fd2433,[%rd473],%fd494; }

	// end inline asm
	add.s64 	%rd474, %rd472, 16;
	// begin inline asm
	{ atom.add.f64 %fd2435,[%rd474],%fd493; }

	// end inline asm
	add.s64 	%rd475, %rd472, 24;
	// begin inline asm
	{ atom.add.f64 %fd2437,[%rd475],%fd471; }

	// end inline asm
	add.s64 	%rd476, %rd472, 32;
	// begin inline asm
	{ atom.add.f64 %fd2439,[%rd476],%fd470; }

	// end inline asm
	add.s64 	%rd477, %rd472, 40;
	// begin inline asm
	{ atom.add.f64 %fd2441,[%rd477],%fd469; }

	// end inline asm
	add.s64 	%rd478, %rd472, 48;
	// begin inline asm
	{ atom.add.f64 %fd2443,[%rd478],%fd447; }

	// end inline asm
	add.s64 	%rd479, %rd472, 56;
	// begin inline asm
	{ atom.add.f64 %fd2445,[%rd479],%fd446; }

	// end inline asm
	add.s64 	%rd480, %rd472, 64;
	// begin inline asm
	{ atom.add.f64 %fd2447,[%rd480],%fd445; }

	// end inline asm

$L__BB13_39:
	ld.param.u64 	%rd38, [%rd28];
	ld.param.u32 	%r21, [%rd28+32];
	ld.param.u32 	%r22, [%rd28+60];
	add.s32 	%r985, %r4, 8;
	setp.le.s32 	%p26, %r22, %r985;
	selp.u16 	%rs58, 1, 0, %p26;
	shr.u32 	%r986, %r985, 31;
	cvt.u16.u32 	%rs59, %r986;
	or.b16  	%rs60, %rs58, %rs59;
	setp.eq.s16 	%p27, %rs60, 0;
	@%p27 bra 	$L__BB13_41;

	add.s32 	%r2014, %r4, 8;
	st.local.v2.u32 	[%rd30], {%r2014, %r22};
	mov.u64 	%rd482, $str$1;
	cvta.global.u64 	%rd483, %rd482;
	{ // callseq 329, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd483;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r988, [retval0+0];
	} // callseq 329
	bra.uni 	$L__BB13_42;

$L__BB13_41:
	add.s32 	%r2671, %r4, 8;
	mul.wide.s32 	%rd494, %r21, %r2671;
	add.s64 	%rd485, %rd38, %rd494;
	// begin inline asm
	{ atom.add.f64 %fd2449,[%rd485],%fd432; }

	// end inline asm
	add.s64 	%rd486, %rd485, 8;
	// begin inline asm
	{ atom.add.f64 %fd2451,[%rd486],%fd431; }

	// end inline asm
	add.s64 	%rd487, %rd485, 16;
	// begin inline asm
	{ atom.add.f64 %fd2453,[%rd487],%fd430; }

	// end inline asm
	add.s64 	%rd488, %rd485, 24;
	// begin inline asm
	{ atom.add.f64 %fd2455,[%rd488],%fd408; }

	// end inline asm
	add.s64 	%rd489, %rd485, 32;
	// begin inline asm
	{ atom.add.f64 %fd2457,[%rd489],%fd407; }

	// end inline asm
	add.s64 	%rd490, %rd485, 40;
	// begin inline asm
	{ atom.add.f64 %fd2459,[%rd490],%fd406; }

	// end inline asm
	add.s64 	%rd491, %rd485, 48;
	// begin inline asm
	{ atom.add.f64 %fd2461,[%rd491],%fd384; }

	// end inline asm
	add.s64 	%rd492, %rd485, 56;
	// begin inline asm
	{ atom.add.f64 %fd2463,[%rd492],%fd383; }

	// end inline asm
	add.s64 	%rd493, %rd485, 64;
	// begin inline asm
	{ atom.add.f64 %fd2465,[%rd493],%fd382; }

	// end inline asm

$L__BB13_42:
	ld.param.u64 	%rd39, [%rd28];
	ld.param.u32 	%r23, [%rd28+32];
	ld.param.u32 	%r24, [%rd28+60];
	add.s32 	%r990, %r4, 9;
	setp.le.s32 	%p28, %r24, %r990;
	selp.u16 	%rs61, 1, 0, %p28;
	shr.u32 	%r991, %r990, 31;
	cvt.u16.u32 	%rs62, %r991;
	or.b16  	%rs63, %rs61, %rs62;
	setp.eq.s16 	%p29, %rs63, 0;
	@%p29 bra 	$L__BB13_44;

	add.s32 	%r2015, %r4, 9;
	st.local.v2.u32 	[%rd30], {%r2015, %r24};
	mov.u64 	%rd495, $str$1;
	cvta.global.u64 	%rd496, %rd495;
	{ // callseq 330, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd496;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r993, [retval0+0];
	} // callseq 330
	bra.uni 	$L__BB13_45;

$L__BB13_44:
	add.s32 	%r2670, %r4, 9;
	mul.wide.s32 	%rd507, %r23, %r2670;
	add.s64 	%rd498, %rd39, %rd507;
	// begin inline asm
	{ atom.add.f64 %fd2467,[%rd498],%fd429; }

	// end inline asm
	add.s64 	%rd499, %rd498, 8;
	// begin inline asm
	{ atom.add.f64 %fd2469,[%rd499],%fd428; }

	// end inline asm
	add.s64 	%rd500, %rd498, 16;
	// begin inline asm
	{ atom.add.f64 %fd2471,[%rd500],%fd427; }

	// end inline asm
	add.s64 	%rd501, %rd498, 24;
	// begin inline asm
	{ atom.add.f64 %fd2473,[%rd501],%fd405; }

	// end inline asm
	add.s64 	%rd502, %rd498, 32;
	// begin inline asm
	{ atom.add.f64 %fd2475,[%rd502],%fd404; }

	// end inline asm
	add.s64 	%rd503, %rd498, 40;
	// begin inline asm
	{ atom.add.f64 %fd2477,[%rd503],%fd403; }

	// end inline asm
	add.s64 	%rd504, %rd498, 48;
	// begin inline asm
	{ atom.add.f64 %fd2479,[%rd504],%fd381; }

	// end inline asm
	add.s64 	%rd505, %rd498, 56;
	// begin inline asm
	{ atom.add.f64 %fd2481,[%rd505],%fd380; }

	// end inline asm
	add.s64 	%rd506, %rd498, 64;
	// begin inline asm
	{ atom.add.f64 %fd2483,[%rd506],%fd379; }

	// end inline asm

$L__BB13_45:
	ld.param.u64 	%rd40, [%rd28];
	ld.param.u32 	%r25, [%rd28+32];
	ld.param.u32 	%r26, [%rd28+60];
	add.s32 	%r995, %r4, 10;
	setp.le.s32 	%p30, %r26, %r995;
	selp.u16 	%rs64, 1, 0, %p30;
	shr.u32 	%r996, %r995, 31;
	cvt.u16.u32 	%rs65, %r996;
	or.b16  	%rs66, %rs64, %rs65;
	setp.eq.s16 	%p31, %rs66, 0;
	@%p31 bra 	$L__BB13_47;

	add.s32 	%r2016, %r4, 10;
	st.local.v2.u32 	[%rd30], {%r2016, %r26};
	mov.u64 	%rd508, $str$1;
	cvta.global.u64 	%rd509, %rd508;
	{ // callseq 331, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd509;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r998, [retval0+0];
	} // callseq 331
	bra.uni 	$L__BB13_48;

$L__BB13_47:
	add.s32 	%r2669, %r4, 10;
	mul.wide.s32 	%rd520, %r25, %r2669;
	add.s64 	%rd511, %rd40, %rd520;
	// begin inline asm
	{ atom.add.f64 %fd2485,[%rd511],%fd426; }

	// end inline asm
	add.s64 	%rd512, %rd511, 8;
	// begin inline asm
	{ atom.add.f64 %fd2487,[%rd512],%fd425; }

	// end inline asm
	add.s64 	%rd513, %rd511, 16;
	// begin inline asm
	{ atom.add.f64 %fd2489,[%rd513],%fd424; }

	// end inline asm
	add.s64 	%rd514, %rd511, 24;
	// begin inline asm
	{ atom.add.f64 %fd2491,[%rd514],%fd402; }

	// end inline asm
	add.s64 	%rd515, %rd511, 32;
	// begin inline asm
	{ atom.add.f64 %fd2493,[%rd515],%fd401; }

	// end inline asm
	add.s64 	%rd516, %rd511, 40;
	// begin inline asm
	{ atom.add.f64 %fd2495,[%rd516],%fd400; }

	// end inline asm
	add.s64 	%rd517, %rd511, 48;
	// begin inline asm
	{ atom.add.f64 %fd2497,[%rd517],%fd378; }

	// end inline asm
	add.s64 	%rd518, %rd511, 56;
	// begin inline asm
	{ atom.add.f64 %fd2499,[%rd518],%fd377; }

	// end inline asm
	add.s64 	%rd519, %rd511, 64;
	// begin inline asm
	{ atom.add.f64 %fd2501,[%rd519],%fd376; }

	// end inline asm

$L__BB13_48:
	ld.param.u64 	%rd41, [%rd28];
	ld.param.u32 	%r27, [%rd28+32];
	ld.param.u32 	%r28, [%rd28+60];
	add.s32 	%r1000, %r4, 11;
	setp.le.s32 	%p32, %r28, %r1000;
	selp.u16 	%rs67, 1, 0, %p32;
	shr.u32 	%r1001, %r1000, 31;
	cvt.u16.u32 	%rs68, %r1001;
	or.b16  	%rs69, %rs67, %rs68;
	setp.eq.s16 	%p33, %rs69, 0;
	@%p33 bra 	$L__BB13_50;

	add.s32 	%r2017, %r4, 11;
	st.local.v2.u32 	[%rd30], {%r2017, %r28};
	mov.u64 	%rd521, $str$1;
	cvta.global.u64 	%rd522, %rd521;
	{ // callseq 332, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd522;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1003, [retval0+0];
	} // callseq 332
	bra.uni 	$L__BB13_51;

$L__BB13_50:
	add.s32 	%r2668, %r4, 11;
	mul.wide.s32 	%rd533, %r27, %r2668;
	add.s64 	%rd524, %rd41, %rd533;
	// begin inline asm
	{ atom.add.f64 %fd2503,[%rd524],%fd423; }

	// end inline asm
	add.s64 	%rd525, %rd524, 8;
	// begin inline asm
	{ atom.add.f64 %fd2505,[%rd525],%fd422; }

	// end inline asm
	add.s64 	%rd526, %rd524, 16;
	// begin inline asm
	{ atom.add.f64 %fd2507,[%rd526],%fd421; }

	// end inline asm
	add.s64 	%rd527, %rd524, 24;
	// begin inline asm
	{ atom.add.f64 %fd2509,[%rd527],%fd399; }

	// end inline asm
	add.s64 	%rd528, %rd524, 32;
	// begin inline asm
	{ atom.add.f64 %fd2511,[%rd528],%fd398; }

	// end inline asm
	add.s64 	%rd529, %rd524, 40;
	// begin inline asm
	{ atom.add.f64 %fd2513,[%rd529],%fd397; }

	// end inline asm
	add.s64 	%rd530, %rd524, 48;
	// begin inline asm
	{ atom.add.f64 %fd2515,[%rd530],%fd375; }

	// end inline asm
	add.s64 	%rd531, %rd524, 56;
	// begin inline asm
	{ atom.add.f64 %fd2517,[%rd531],%fd374; }

	// end inline asm
	add.s64 	%rd532, %rd524, 64;
	// begin inline asm
	{ atom.add.f64 %fd2519,[%rd532],%fd373; }

	// end inline asm

$L__BB13_51:
	ld.param.u64 	%rd42, [%rd28];
	ld.param.u32 	%r29, [%rd28+32];
	ld.param.u32 	%r30, [%rd28+60];
	add.s32 	%r1005, %r4, 12;
	setp.le.s32 	%p34, %r30, %r1005;
	selp.u16 	%rs70, 1, 0, %p34;
	shr.u32 	%r1006, %r1005, 31;
	cvt.u16.u32 	%rs71, %r1006;
	or.b16  	%rs72, %rs70, %rs71;
	setp.eq.s16 	%p35, %rs72, 0;
	@%p35 bra 	$L__BB13_53;

	add.s32 	%r2018, %r4, 12;
	st.local.v2.u32 	[%rd30], {%r2018, %r30};
	mov.u64 	%rd534, $str$1;
	cvta.global.u64 	%rd535, %rd534;
	{ // callseq 333, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd535;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1008, [retval0+0];
	} // callseq 333
	bra.uni 	$L__BB13_54;

$L__BB13_53:
	add.s32 	%r2667, %r4, 12;
	mul.wide.s32 	%rd546, %r29, %r2667;
	add.s64 	%rd537, %rd42, %rd546;
	// begin inline asm
	{ atom.add.f64 %fd2521,[%rd537],%fd360; }

	// end inline asm
	add.s64 	%rd538, %rd537, 8;
	// begin inline asm
	{ atom.add.f64 %fd2523,[%rd538],%fd359; }

	// end inline asm
	add.s64 	%rd539, %rd537, 16;
	// begin inline asm
	{ atom.add.f64 %fd2525,[%rd539],%fd358; }

	// end inline asm
	add.s64 	%rd540, %rd537, 24;
	// begin inline asm
	{ atom.add.f64 %fd2527,[%rd540],%fd336; }

	// end inline asm
	add.s64 	%rd541, %rd537, 32;
	// begin inline asm
	{ atom.add.f64 %fd2529,[%rd541],%fd335; }

	// end inline asm
	add.s64 	%rd542, %rd537, 40;
	// begin inline asm
	{ atom.add.f64 %fd2531,[%rd542],%fd334; }

	// end inline asm
	add.s64 	%rd543, %rd537, 48;
	// begin inline asm
	{ atom.add.f64 %fd2533,[%rd543],%fd312; }

	// end inline asm
	add.s64 	%rd544, %rd537, 56;
	// begin inline asm
	{ atom.add.f64 %fd2535,[%rd544],%fd311; }

	// end inline asm
	add.s64 	%rd545, %rd537, 64;
	// begin inline asm
	{ atom.add.f64 %fd2537,[%rd545],%fd310; }

	// end inline asm

$L__BB13_54:
	ld.param.u64 	%rd43, [%rd28];
	ld.param.u32 	%r31, [%rd28+32];
	ld.param.u32 	%r32, [%rd28+60];
	add.s32 	%r1010, %r4, 13;
	setp.le.s32 	%p36, %r32, %r1010;
	selp.u16 	%rs73, 1, 0, %p36;
	shr.u32 	%r1011, %r1010, 31;
	cvt.u16.u32 	%rs74, %r1011;
	or.b16  	%rs75, %rs73, %rs74;
	setp.eq.s16 	%p37, %rs75, 0;
	@%p37 bra 	$L__BB13_56;

	add.s32 	%r2019, %r4, 13;
	st.local.v2.u32 	[%rd30], {%r2019, %r32};
	mov.u64 	%rd547, $str$1;
	cvta.global.u64 	%rd548, %rd547;
	{ // callseq 334, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd548;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1013, [retval0+0];
	} // callseq 334
	bra.uni 	$L__BB13_57;

$L__BB13_56:
	add.s32 	%r2666, %r4, 13;
	mul.wide.s32 	%rd559, %r31, %r2666;
	add.s64 	%rd550, %rd43, %rd559;
	// begin inline asm
	{ atom.add.f64 %fd2539,[%rd550],%fd357; }

	// end inline asm
	add.s64 	%rd551, %rd550, 8;
	// begin inline asm
	{ atom.add.f64 %fd2541,[%rd551],%fd356; }

	// end inline asm
	add.s64 	%rd552, %rd550, 16;
	// begin inline asm
	{ atom.add.f64 %fd2543,[%rd552],%fd355; }

	// end inline asm
	add.s64 	%rd553, %rd550, 24;
	// begin inline asm
	{ atom.add.f64 %fd2545,[%rd553],%fd333; }

	// end inline asm
	add.s64 	%rd554, %rd550, 32;
	// begin inline asm
	{ atom.add.f64 %fd2547,[%rd554],%fd332; }

	// end inline asm
	add.s64 	%rd555, %rd550, 40;
	// begin inline asm
	{ atom.add.f64 %fd2549,[%rd555],%fd331; }

	// end inline asm
	add.s64 	%rd556, %rd550, 48;
	// begin inline asm
	{ atom.add.f64 %fd2551,[%rd556],%fd309; }

	// end inline asm
	add.s64 	%rd557, %rd550, 56;
	// begin inline asm
	{ atom.add.f64 %fd2553,[%rd557],%fd308; }

	// end inline asm
	add.s64 	%rd558, %rd550, 64;
	// begin inline asm
	{ atom.add.f64 %fd2555,[%rd558],%fd307; }

	// end inline asm

$L__BB13_57:
	ld.param.u64 	%rd44, [%rd28];
	ld.param.u32 	%r33, [%rd28+32];
	ld.param.u32 	%r34, [%rd28+60];
	add.s32 	%r1015, %r4, 14;
	setp.le.s32 	%p38, %r34, %r1015;
	selp.u16 	%rs76, 1, 0, %p38;
	shr.u32 	%r1016, %r1015, 31;
	cvt.u16.u32 	%rs77, %r1016;
	or.b16  	%rs78, %rs76, %rs77;
	setp.eq.s16 	%p39, %rs78, 0;
	@%p39 bra 	$L__BB13_59;

	add.s32 	%r2020, %r4, 14;
	st.local.v2.u32 	[%rd30], {%r2020, %r34};
	mov.u64 	%rd560, $str$1;
	cvta.global.u64 	%rd561, %rd560;
	{ // callseq 335, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd561;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1018, [retval0+0];
	} // callseq 335
	bra.uni 	$L__BB13_60;

$L__BB13_59:
	add.s32 	%r2665, %r4, 14;
	mul.wide.s32 	%rd572, %r33, %r2665;
	add.s64 	%rd563, %rd44, %rd572;
	// begin inline asm
	{ atom.add.f64 %fd2557,[%rd563],%fd354; }

	// end inline asm
	add.s64 	%rd564, %rd563, 8;
	// begin inline asm
	{ atom.add.f64 %fd2559,[%rd564],%fd353; }

	// end inline asm
	add.s64 	%rd565, %rd563, 16;
	// begin inline asm
	{ atom.add.f64 %fd2561,[%rd565],%fd352; }

	// end inline asm
	add.s64 	%rd566, %rd563, 24;
	// begin inline asm
	{ atom.add.f64 %fd2563,[%rd566],%fd330; }

	// end inline asm
	add.s64 	%rd567, %rd563, 32;
	// begin inline asm
	{ atom.add.f64 %fd2565,[%rd567],%fd329; }

	// end inline asm
	add.s64 	%rd568, %rd563, 40;
	// begin inline asm
	{ atom.add.f64 %fd2567,[%rd568],%fd328; }

	// end inline asm
	add.s64 	%rd569, %rd563, 48;
	// begin inline asm
	{ atom.add.f64 %fd2569,[%rd569],%fd306; }

	// end inline asm
	add.s64 	%rd570, %rd563, 56;
	// begin inline asm
	{ atom.add.f64 %fd2571,[%rd570],%fd305; }

	// end inline asm
	add.s64 	%rd571, %rd563, 64;
	// begin inline asm
	{ atom.add.f64 %fd2573,[%rd571],%fd304; }

	// end inline asm

$L__BB13_60:
	ld.param.u64 	%rd45, [%rd28];
	ld.param.u32 	%r35, [%rd28+32];
	ld.param.u32 	%r36, [%rd28+60];
	add.s32 	%r1020, %r4, 15;
	setp.le.s32 	%p40, %r36, %r1020;
	selp.u16 	%rs79, 1, 0, %p40;
	shr.u32 	%r1021, %r1020, 31;
	cvt.u16.u32 	%rs80, %r1021;
	or.b16  	%rs81, %rs79, %rs80;
	setp.eq.s16 	%p41, %rs81, 0;
	@%p41 bra 	$L__BB13_62;

	add.s32 	%r2021, %r4, 15;
	st.local.v2.u32 	[%rd30], {%r2021, %r36};
	mov.u64 	%rd573, $str$1;
	cvta.global.u64 	%rd574, %rd573;
	{ // callseq 336, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd574;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1023, [retval0+0];
	} // callseq 336
	bra.uni 	$L__BB13_63;

$L__BB13_62:
	add.s32 	%r2664, %r4, 15;
	mul.wide.s32 	%rd585, %r35, %r2664;
	add.s64 	%rd576, %rd45, %rd585;
	// begin inline asm
	{ atom.add.f64 %fd2575,[%rd576],%fd351; }

	// end inline asm
	add.s64 	%rd577, %rd576, 8;
	// begin inline asm
	{ atom.add.f64 %fd2577,[%rd577],%fd350; }

	// end inline asm
	add.s64 	%rd578, %rd576, 16;
	// begin inline asm
	{ atom.add.f64 %fd2579,[%rd578],%fd349; }

	// end inline asm
	add.s64 	%rd579, %rd576, 24;
	// begin inline asm
	{ atom.add.f64 %fd2581,[%rd579],%fd327; }

	// end inline asm
	add.s64 	%rd580, %rd576, 32;
	// begin inline asm
	{ atom.add.f64 %fd2583,[%rd580],%fd326; }

	// end inline asm
	add.s64 	%rd581, %rd576, 40;
	// begin inline asm
	{ atom.add.f64 %fd2585,[%rd581],%fd325; }

	// end inline asm
	add.s64 	%rd582, %rd576, 48;
	// begin inline asm
	{ atom.add.f64 %fd2587,[%rd582],%fd303; }

	// end inline asm
	add.s64 	%rd583, %rd576, 56;
	// begin inline asm
	{ atom.add.f64 %fd2589,[%rd583],%fd302; }

	// end inline asm
	add.s64 	%rd584, %rd576, 64;
	// begin inline asm
	{ atom.add.f64 %fd2591,[%rd584],%fd301; }

	// end inline asm

$L__BB13_63:
	mul.lo.s64 	%rd586, %rd25, %rd13;
	add.s64 	%rd46, %rd6, %rd586;
	ld.global.u32 	%r37, [%rd46];
	shl.b32 	%r38, %r37, 4;
	ld.param.u64 	%rd47, [%rd28];
	ld.param.u32 	%r39, [%rd28+32];
	ld.param.u32 	%r40, [%rd28+60];
	setp.le.s32 	%p42, %r40, %r38;
	selp.u16 	%rs82, 1, 0, %p42;
	shr.u32 	%r1025, %r37, 27;
	cvt.u16.u32 	%rs83, %r1025;
	and.b16  	%rs84, %rs83, 1;
	or.b16  	%rs85, %rs84, %rs82;
	setp.eq.s16 	%p43, %rs85, 0;
	@%p43 bra 	$L__BB13_65;

	st.local.v2.u32 	[%rd30], {%r38, %r40};
	mov.u64 	%rd587, $str$1;
	cvta.global.u64 	%rd588, %rd587;
	{ // callseq 337, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd588;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1026, [retval0+0];
	} // callseq 337
	bra.uni 	$L__BB13_66;

$L__BB13_65:
	mul.wide.s32 	%rd599, %r39, %r38;
	add.s64 	%rd590, %rd47, %rd599;
	// begin inline asm
	{ atom.add.f64 %fd2593,[%rd590],%fd276; }

	// end inline asm
	add.s64 	%rd591, %rd590, 8;
	// begin inline asm
	{ atom.add.f64 %fd2595,[%rd591],%fd275; }

	// end inline asm
	add.s64 	%rd592, %rd590, 16;
	// begin inline asm
	{ atom.add.f64 %fd2597,[%rd592],%fd274; }

	// end inline asm
	add.s64 	%rd593, %rd590, 24;
	// begin inline asm
	{ atom.add.f64 %fd2599,[%rd593],%fd252; }

	// end inline asm
	add.s64 	%rd594, %rd590, 32;
	// begin inline asm
	{ atom.add.f64 %fd2601,[%rd594],%fd251; }

	// end inline asm
	add.s64 	%rd595, %rd590, 40;
	// begin inline asm
	{ atom.add.f64 %fd2603,[%rd595],%fd250; }

	// end inline asm
	add.s64 	%rd596, %rd590, 48;
	// begin inline asm
	{ atom.add.f64 %fd2605,[%rd596],%fd228; }

	// end inline asm
	add.s64 	%rd597, %rd590, 56;
	// begin inline asm
	{ atom.add.f64 %fd2607,[%rd597],%fd227; }

	// end inline asm
	add.s64 	%rd598, %rd590, 64;
	// begin inline asm
	{ atom.add.f64 %fd2609,[%rd598],%fd226; }

	// end inline asm

$L__BB13_66:
	ld.param.u64 	%rd48, [%rd28];
	ld.param.u32 	%r41, [%rd28+32];
	ld.param.u32 	%r42, [%rd28+60];
	add.s32 	%r1027, %r38, 1;
	setp.le.s32 	%p44, %r42, %r1027;
	selp.u16 	%rs86, 1, 0, %p44;
	shr.u32 	%r1028, %r1027, 31;
	cvt.u16.u32 	%rs87, %r1028;
	or.b16  	%rs88, %rs86, %rs87;
	setp.eq.s16 	%p45, %rs88, 0;
	@%p45 bra 	$L__BB13_68;

	add.s32 	%r2022, %r38, 1;
	st.local.v2.u32 	[%rd30], {%r2022, %r42};
	mov.u64 	%rd600, $str$1;
	cvta.global.u64 	%rd601, %rd600;
	{ // callseq 338, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd601;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1030, [retval0+0];
	} // callseq 338
	bra.uni 	$L__BB13_69;

$L__BB13_68:
	add.s32 	%r2663, %r38, 1;
	mul.wide.s32 	%rd612, %r41, %r2663;
	add.s64 	%rd603, %rd48, %rd612;
	// begin inline asm
	{ atom.add.f64 %fd2611,[%rd603],%fd273; }

	// end inline asm
	add.s64 	%rd604, %rd603, 8;
	// begin inline asm
	{ atom.add.f64 %fd2613,[%rd604],%fd272; }

	// end inline asm
	add.s64 	%rd605, %rd603, 16;
	// begin inline asm
	{ atom.add.f64 %fd2615,[%rd605],%fd271; }

	// end inline asm
	add.s64 	%rd606, %rd603, 24;
	// begin inline asm
	{ atom.add.f64 %fd2617,[%rd606],%fd249; }

	// end inline asm
	add.s64 	%rd607, %rd603, 32;
	// begin inline asm
	{ atom.add.f64 %fd2619,[%rd607],%fd248; }

	// end inline asm
	add.s64 	%rd608, %rd603, 40;
	// begin inline asm
	{ atom.add.f64 %fd2621,[%rd608],%fd247; }

	// end inline asm
	add.s64 	%rd609, %rd603, 48;
	// begin inline asm
	{ atom.add.f64 %fd2623,[%rd609],%fd225; }

	// end inline asm
	add.s64 	%rd610, %rd603, 56;
	// begin inline asm
	{ atom.add.f64 %fd2625,[%rd610],%fd224; }

	// end inline asm
	add.s64 	%rd611, %rd603, 64;
	// begin inline asm
	{ atom.add.f64 %fd2627,[%rd611],%fd223; }

	// end inline asm

$L__BB13_69:
	ld.param.u64 	%rd49, [%rd28];
	ld.param.u32 	%r43, [%rd28+32];
	ld.param.u32 	%r44, [%rd28+60];
	add.s32 	%r1032, %r38, 2;
	setp.le.s32 	%p46, %r44, %r1032;
	selp.u16 	%rs89, 1, 0, %p46;
	shr.u32 	%r1033, %r1032, 31;
	cvt.u16.u32 	%rs90, %r1033;
	or.b16  	%rs91, %rs89, %rs90;
	setp.eq.s16 	%p47, %rs91, 0;
	@%p47 bra 	$L__BB13_71;

	add.s32 	%r2023, %r38, 2;
	st.local.v2.u32 	[%rd30], {%r2023, %r44};
	mov.u64 	%rd613, $str$1;
	cvta.global.u64 	%rd614, %rd613;
	{ // callseq 339, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd614;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1035, [retval0+0];
	} // callseq 339
	bra.uni 	$L__BB13_72;

$L__BB13_71:
	add.s32 	%r2662, %r38, 2;
	mul.wide.s32 	%rd625, %r43, %r2662;
	add.s64 	%rd616, %rd49, %rd625;
	// begin inline asm
	{ atom.add.f64 %fd2629,[%rd616],%fd270; }

	// end inline asm
	add.s64 	%rd617, %rd616, 8;
	// begin inline asm
	{ atom.add.f64 %fd2631,[%rd617],%fd269; }

	// end inline asm
	add.s64 	%rd618, %rd616, 16;
	// begin inline asm
	{ atom.add.f64 %fd2633,[%rd618],%fd268; }

	// end inline asm
	add.s64 	%rd619, %rd616, 24;
	// begin inline asm
	{ atom.add.f64 %fd2635,[%rd619],%fd246; }

	// end inline asm
	add.s64 	%rd620, %rd616, 32;
	// begin inline asm
	{ atom.add.f64 %fd2637,[%rd620],%fd245; }

	// end inline asm
	add.s64 	%rd621, %rd616, 40;
	// begin inline asm
	{ atom.add.f64 %fd2639,[%rd621],%fd244; }

	// end inline asm
	add.s64 	%rd622, %rd616, 48;
	// begin inline asm
	{ atom.add.f64 %fd2641,[%rd622],%fd222; }

	// end inline asm
	add.s64 	%rd623, %rd616, 56;
	// begin inline asm
	{ atom.add.f64 %fd2643,[%rd623],%fd221; }

	// end inline asm
	add.s64 	%rd624, %rd616, 64;
	// begin inline asm
	{ atom.add.f64 %fd2645,[%rd624],%fd220; }

	// end inline asm

$L__BB13_72:
	ld.param.u64 	%rd50, [%rd28];
	ld.param.u32 	%r45, [%rd28+32];
	ld.param.u32 	%r46, [%rd28+60];
	add.s32 	%r1037, %r38, 3;
	setp.le.s32 	%p48, %r46, %r1037;
	selp.u16 	%rs92, 1, 0, %p48;
	shr.u32 	%r1038, %r1037, 31;
	cvt.u16.u32 	%rs93, %r1038;
	or.b16  	%rs94, %rs92, %rs93;
	setp.eq.s16 	%p49, %rs94, 0;
	@%p49 bra 	$L__BB13_74;

	add.s32 	%r2024, %r38, 3;
	st.local.v2.u32 	[%rd30], {%r2024, %r46};
	mov.u64 	%rd626, $str$1;
	cvta.global.u64 	%rd627, %rd626;
	{ // callseq 340, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd627;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1040, [retval0+0];
	} // callseq 340
	bra.uni 	$L__BB13_75;

$L__BB13_74:
	add.s32 	%r2661, %r38, 3;
	mul.wide.s32 	%rd638, %r45, %r2661;
	add.s64 	%rd629, %rd50, %rd638;
	// begin inline asm
	{ atom.add.f64 %fd2647,[%rd629],%fd267; }

	// end inline asm
	add.s64 	%rd630, %rd629, 8;
	// begin inline asm
	{ atom.add.f64 %fd2649,[%rd630],%fd266; }

	// end inline asm
	add.s64 	%rd631, %rd629, 16;
	// begin inline asm
	{ atom.add.f64 %fd2651,[%rd631],%fd265; }

	// end inline asm
	add.s64 	%rd632, %rd629, 24;
	// begin inline asm
	{ atom.add.f64 %fd2653,[%rd632],%fd243; }

	// end inline asm
	add.s64 	%rd633, %rd629, 32;
	// begin inline asm
	{ atom.add.f64 %fd2655,[%rd633],%fd242; }

	// end inline asm
	add.s64 	%rd634, %rd629, 40;
	// begin inline asm
	{ atom.add.f64 %fd2657,[%rd634],%fd241; }

	// end inline asm
	add.s64 	%rd635, %rd629, 48;
	// begin inline asm
	{ atom.add.f64 %fd2659,[%rd635],%fd219; }

	// end inline asm
	add.s64 	%rd636, %rd629, 56;
	// begin inline asm
	{ atom.add.f64 %fd2661,[%rd636],%fd218; }

	// end inline asm
	add.s64 	%rd637, %rd629, 64;
	// begin inline asm
	{ atom.add.f64 %fd2663,[%rd637],%fd217; }

	// end inline asm

$L__BB13_75:
	ld.param.u64 	%rd51, [%rd28];
	ld.param.u32 	%r47, [%rd28+32];
	ld.param.u32 	%r48, [%rd28+60];
	add.s32 	%r1042, %r38, 4;
	setp.le.s32 	%p50, %r48, %r1042;
	selp.u16 	%rs95, 1, 0, %p50;
	shr.u32 	%r1043, %r1042, 31;
	cvt.u16.u32 	%rs96, %r1043;
	or.b16  	%rs97, %rs95, %rs96;
	setp.eq.s16 	%p51, %rs97, 0;
	@%p51 bra 	$L__BB13_77;

	add.s32 	%r2025, %r38, 4;
	st.local.v2.u32 	[%rd30], {%r2025, %r48};
	mov.u64 	%rd639, $str$1;
	cvta.global.u64 	%rd640, %rd639;
	{ // callseq 341, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd640;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1045, [retval0+0];
	} // callseq 341
	bra.uni 	$L__BB13_78;

$L__BB13_77:
	add.s32 	%r2660, %r38, 4;
	mul.wide.s32 	%rd651, %r47, %r2660;
	add.s64 	%rd642, %rd51, %rd651;
	// begin inline asm
	{ atom.add.f64 %fd2665,[%rd642],%fd204; }

	// end inline asm
	add.s64 	%rd643, %rd642, 8;
	// begin inline asm
	{ atom.add.f64 %fd2667,[%rd643],%fd203; }

	// end inline asm
	add.s64 	%rd644, %rd642, 16;
	// begin inline asm
	{ atom.add.f64 %fd2669,[%rd644],%fd202; }

	// end inline asm
	add.s64 	%rd645, %rd642, 24;
	// begin inline asm
	{ atom.add.f64 %fd2671,[%rd645],%fd180; }

	// end inline asm
	add.s64 	%rd646, %rd642, 32;
	// begin inline asm
	{ atom.add.f64 %fd2673,[%rd646],%fd179; }

	// end inline asm
	add.s64 	%rd647, %rd642, 40;
	// begin inline asm
	{ atom.add.f64 %fd2675,[%rd647],%fd178; }

	// end inline asm
	add.s64 	%rd648, %rd642, 48;
	// begin inline asm
	{ atom.add.f64 %fd2677,[%rd648],%fd156; }

	// end inline asm
	add.s64 	%rd649, %rd642, 56;
	// begin inline asm
	{ atom.add.f64 %fd2679,[%rd649],%fd155; }

	// end inline asm
	add.s64 	%rd650, %rd642, 64;
	// begin inline asm
	{ atom.add.f64 %fd2681,[%rd650],%fd154; }

	// end inline asm

$L__BB13_78:
	ld.param.u64 	%rd52, [%rd28];
	ld.param.u32 	%r49, [%rd28+32];
	ld.param.u32 	%r50, [%rd28+60];
	add.s32 	%r1047, %r38, 5;
	setp.le.s32 	%p52, %r50, %r1047;
	selp.u16 	%rs98, 1, 0, %p52;
	shr.u32 	%r1048, %r1047, 31;
	cvt.u16.u32 	%rs99, %r1048;
	or.b16  	%rs100, %rs98, %rs99;
	setp.eq.s16 	%p53, %rs100, 0;
	@%p53 bra 	$L__BB13_80;

	add.s32 	%r2026, %r38, 5;
	st.local.v2.u32 	[%rd30], {%r2026, %r50};
	mov.u64 	%rd652, $str$1;
	cvta.global.u64 	%rd653, %rd652;
	{ // callseq 342, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd653;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1050, [retval0+0];
	} // callseq 342
	bra.uni 	$L__BB13_81;

$L__BB13_80:
	add.s32 	%r2659, %r38, 5;
	mul.wide.s32 	%rd664, %r49, %r2659;
	add.s64 	%rd655, %rd52, %rd664;
	// begin inline asm
	{ atom.add.f64 %fd2683,[%rd655],%fd201; }

	// end inline asm
	add.s64 	%rd656, %rd655, 8;
	// begin inline asm
	{ atom.add.f64 %fd2685,[%rd656],%fd200; }

	// end inline asm
	add.s64 	%rd657, %rd655, 16;
	// begin inline asm
	{ atom.add.f64 %fd2687,[%rd657],%fd199; }

	// end inline asm
	add.s64 	%rd658, %rd655, 24;
	// begin inline asm
	{ atom.add.f64 %fd2689,[%rd658],%fd177; }

	// end inline asm
	add.s64 	%rd659, %rd655, 32;
	// begin inline asm
	{ atom.add.f64 %fd2691,[%rd659],%fd176; }

	// end inline asm
	add.s64 	%rd660, %rd655, 40;
	// begin inline asm
	{ atom.add.f64 %fd2693,[%rd660],%fd175; }

	// end inline asm
	add.s64 	%rd661, %rd655, 48;
	// begin inline asm
	{ atom.add.f64 %fd2695,[%rd661],%fd153; }

	// end inline asm
	add.s64 	%rd662, %rd655, 56;
	// begin inline asm
	{ atom.add.f64 %fd2697,[%rd662],%fd152; }

	// end inline asm
	add.s64 	%rd663, %rd655, 64;
	// begin inline asm
	{ atom.add.f64 %fd2699,[%rd663],%fd151; }

	// end inline asm

$L__BB13_81:
	ld.param.u64 	%rd53, [%rd28];
	ld.param.u32 	%r51, [%rd28+32];
	ld.param.u32 	%r52, [%rd28+60];
	add.s32 	%r1052, %r38, 6;
	setp.le.s32 	%p54, %r52, %r1052;
	selp.u16 	%rs101, 1, 0, %p54;
	shr.u32 	%r1053, %r1052, 31;
	cvt.u16.u32 	%rs102, %r1053;
	or.b16  	%rs103, %rs101, %rs102;
	setp.eq.s16 	%p55, %rs103, 0;
	@%p55 bra 	$L__BB13_83;

	add.s32 	%r2027, %r38, 6;
	st.local.v2.u32 	[%rd30], {%r2027, %r52};
	mov.u64 	%rd665, $str$1;
	cvta.global.u64 	%rd666, %rd665;
	{ // callseq 343, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd666;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1055, [retval0+0];
	} // callseq 343
	bra.uni 	$L__BB13_84;

$L__BB13_83:
	add.s32 	%r2658, %r38, 6;
	mul.wide.s32 	%rd677, %r51, %r2658;
	add.s64 	%rd668, %rd53, %rd677;
	// begin inline asm
	{ atom.add.f64 %fd2701,[%rd668],%fd198; }

	// end inline asm
	add.s64 	%rd669, %rd668, 8;
	// begin inline asm
	{ atom.add.f64 %fd2703,[%rd669],%fd197; }

	// end inline asm
	add.s64 	%rd670, %rd668, 16;
	// begin inline asm
	{ atom.add.f64 %fd2705,[%rd670],%fd196; }

	// end inline asm
	add.s64 	%rd671, %rd668, 24;
	// begin inline asm
	{ atom.add.f64 %fd2707,[%rd671],%fd174; }

	// end inline asm
	add.s64 	%rd672, %rd668, 32;
	// begin inline asm
	{ atom.add.f64 %fd2709,[%rd672],%fd173; }

	// end inline asm
	add.s64 	%rd673, %rd668, 40;
	// begin inline asm
	{ atom.add.f64 %fd2711,[%rd673],%fd172; }

	// end inline asm
	add.s64 	%rd674, %rd668, 48;
	// begin inline asm
	{ atom.add.f64 %fd2713,[%rd674],%fd150; }

	// end inline asm
	add.s64 	%rd675, %rd668, 56;
	// begin inline asm
	{ atom.add.f64 %fd2715,[%rd675],%fd149; }

	// end inline asm
	add.s64 	%rd676, %rd668, 64;
	// begin inline asm
	{ atom.add.f64 %fd2717,[%rd676],%fd148; }

	// end inline asm

$L__BB13_84:
	ld.param.u64 	%rd54, [%rd28];
	ld.param.u32 	%r53, [%rd28+32];
	ld.param.u32 	%r54, [%rd28+60];
	add.s32 	%r1057, %r38, 7;
	setp.le.s32 	%p56, %r54, %r1057;
	selp.u16 	%rs104, 1, 0, %p56;
	shr.u32 	%r1058, %r1057, 31;
	cvt.u16.u32 	%rs105, %r1058;
	or.b16  	%rs106, %rs104, %rs105;
	setp.eq.s16 	%p57, %rs106, 0;
	@%p57 bra 	$L__BB13_86;

	add.s32 	%r2028, %r38, 7;
	st.local.v2.u32 	[%rd30], {%r2028, %r54};
	mov.u64 	%rd678, $str$1;
	cvta.global.u64 	%rd679, %rd678;
	{ // callseq 344, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd679;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1060, [retval0+0];
	} // callseq 344
	bra.uni 	$L__BB13_87;

$L__BB13_86:
	add.s32 	%r2657, %r38, 7;
	mul.wide.s32 	%rd690, %r53, %r2657;
	add.s64 	%rd681, %rd54, %rd690;
	// begin inline asm
	{ atom.add.f64 %fd2719,[%rd681],%fd195; }

	// end inline asm
	add.s64 	%rd682, %rd681, 8;
	// begin inline asm
	{ atom.add.f64 %fd2721,[%rd682],%fd194; }

	// end inline asm
	add.s64 	%rd683, %rd681, 16;
	// begin inline asm
	{ atom.add.f64 %fd2723,[%rd683],%fd193; }

	// end inline asm
	add.s64 	%rd684, %rd681, 24;
	// begin inline asm
	{ atom.add.f64 %fd2725,[%rd684],%fd171; }

	// end inline asm
	add.s64 	%rd685, %rd681, 32;
	// begin inline asm
	{ atom.add.f64 %fd2727,[%rd685],%fd170; }

	// end inline asm
	add.s64 	%rd686, %rd681, 40;
	// begin inline asm
	{ atom.add.f64 %fd2729,[%rd686],%fd169; }

	// end inline asm
	add.s64 	%rd687, %rd681, 48;
	// begin inline asm
	{ atom.add.f64 %fd2731,[%rd687],%fd147; }

	// end inline asm
	add.s64 	%rd688, %rd681, 56;
	// begin inline asm
	{ atom.add.f64 %fd2733,[%rd688],%fd146; }

	// end inline asm
	add.s64 	%rd689, %rd681, 64;
	// begin inline asm
	{ atom.add.f64 %fd2735,[%rd689],%fd145; }

	// end inline asm

$L__BB13_87:
	ld.param.u64 	%rd55, [%rd28];
	ld.param.u32 	%r55, [%rd28+32];
	ld.param.u32 	%r56, [%rd28+60];
	add.s32 	%r1062, %r38, 8;
	setp.le.s32 	%p58, %r56, %r1062;
	selp.u16 	%rs107, 1, 0, %p58;
	shr.u32 	%r1063, %r1062, 31;
	cvt.u16.u32 	%rs108, %r1063;
	or.b16  	%rs109, %rs107, %rs108;
	setp.eq.s16 	%p59, %rs109, 0;
	@%p59 bra 	$L__BB13_89;

	add.s32 	%r2029, %r38, 8;
	st.local.v2.u32 	[%rd30], {%r2029, %r56};
	mov.u64 	%rd691, $str$1;
	cvta.global.u64 	%rd692, %rd691;
	{ // callseq 345, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd692;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1065, [retval0+0];
	} // callseq 345
	bra.uni 	$L__BB13_90;

$L__BB13_89:
	add.s32 	%r2656, %r38, 8;
	mul.wide.s32 	%rd703, %r55, %r2656;
	add.s64 	%rd694, %rd55, %rd703;
	// begin inline asm
	{ atom.add.f64 %fd2737,[%rd694],%fd132; }

	// end inline asm
	add.s64 	%rd695, %rd694, 8;
	// begin inline asm
	{ atom.add.f64 %fd2739,[%rd695],%fd131; }

	// end inline asm
	add.s64 	%rd696, %rd694, 16;
	// begin inline asm
	{ atom.add.f64 %fd2741,[%rd696],%fd130; }

	// end inline asm
	add.s64 	%rd697, %rd694, 24;
	// begin inline asm
	{ atom.add.f64 %fd2743,[%rd697],%fd108; }

	// end inline asm
	add.s64 	%rd698, %rd694, 32;
	// begin inline asm
	{ atom.add.f64 %fd2745,[%rd698],%fd107; }

	// end inline asm
	add.s64 	%rd699, %rd694, 40;
	// begin inline asm
	{ atom.add.f64 %fd2747,[%rd699],%fd106; }

	// end inline asm
	add.s64 	%rd700, %rd694, 48;
	// begin inline asm
	{ atom.add.f64 %fd2749,[%rd700],%fd84; }

	// end inline asm
	add.s64 	%rd701, %rd694, 56;
	// begin inline asm
	{ atom.add.f64 %fd2751,[%rd701],%fd83; }

	// end inline asm
	add.s64 	%rd702, %rd694, 64;
	// begin inline asm
	{ atom.add.f64 %fd2753,[%rd702],%fd82; }

	// end inline asm

$L__BB13_90:
	ld.param.u64 	%rd56, [%rd28];
	ld.param.u32 	%r57, [%rd28+32];
	ld.param.u32 	%r58, [%rd28+60];
	add.s32 	%r1067, %r38, 9;
	setp.le.s32 	%p60, %r58, %r1067;
	selp.u16 	%rs110, 1, 0, %p60;
	shr.u32 	%r1068, %r1067, 31;
	cvt.u16.u32 	%rs111, %r1068;
	or.b16  	%rs112, %rs110, %rs111;
	setp.eq.s16 	%p61, %rs112, 0;
	@%p61 bra 	$L__BB13_92;

	add.s32 	%r2030, %r38, 9;
	st.local.v2.u32 	[%rd30], {%r2030, %r58};
	mov.u64 	%rd704, $str$1;
	cvta.global.u64 	%rd705, %rd704;
	{ // callseq 346, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd705;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1070, [retval0+0];
	} // callseq 346
	bra.uni 	$L__BB13_93;

$L__BB13_92:
	add.s32 	%r2655, %r38, 9;
	mul.wide.s32 	%rd716, %r57, %r2655;
	add.s64 	%rd707, %rd56, %rd716;
	// begin inline asm
	{ atom.add.f64 %fd2755,[%rd707],%fd129; }

	// end inline asm
	add.s64 	%rd708, %rd707, 8;
	// begin inline asm
	{ atom.add.f64 %fd2757,[%rd708],%fd128; }

	// end inline asm
	add.s64 	%rd709, %rd707, 16;
	// begin inline asm
	{ atom.add.f64 %fd2759,[%rd709],%fd127; }

	// end inline asm
	add.s64 	%rd710, %rd707, 24;
	// begin inline asm
	{ atom.add.f64 %fd2761,[%rd710],%fd105; }

	// end inline asm
	add.s64 	%rd711, %rd707, 32;
	// begin inline asm
	{ atom.add.f64 %fd2763,[%rd711],%fd104; }

	// end inline asm
	add.s64 	%rd712, %rd707, 40;
	// begin inline asm
	{ atom.add.f64 %fd2765,[%rd712],%fd103; }

	// end inline asm
	add.s64 	%rd713, %rd707, 48;
	// begin inline asm
	{ atom.add.f64 %fd2767,[%rd713],%fd81; }

	// end inline asm
	add.s64 	%rd714, %rd707, 56;
	// begin inline asm
	{ atom.add.f64 %fd2769,[%rd714],%fd80; }

	// end inline asm
	add.s64 	%rd715, %rd707, 64;
	// begin inline asm
	{ atom.add.f64 %fd2771,[%rd715],%fd79; }

	// end inline asm

$L__BB13_93:
	ld.param.u64 	%rd57, [%rd28];
	ld.param.u32 	%r59, [%rd28+32];
	ld.param.u32 	%r60, [%rd28+60];
	add.s32 	%r1072, %r38, 10;
	setp.le.s32 	%p62, %r60, %r1072;
	selp.u16 	%rs113, 1, 0, %p62;
	shr.u32 	%r1073, %r1072, 31;
	cvt.u16.u32 	%rs114, %r1073;
	or.b16  	%rs115, %rs113, %rs114;
	setp.eq.s16 	%p63, %rs115, 0;
	@%p63 bra 	$L__BB13_95;

	add.s32 	%r2031, %r38, 10;
	st.local.v2.u32 	[%rd30], {%r2031, %r60};
	mov.u64 	%rd717, $str$1;
	cvta.global.u64 	%rd718, %rd717;
	{ // callseq 347, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd718;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1075, [retval0+0];
	} // callseq 347
	bra.uni 	$L__BB13_96;

$L__BB13_95:
	add.s32 	%r2654, %r38, 10;
	mul.wide.s32 	%rd729, %r59, %r2654;
	add.s64 	%rd720, %rd57, %rd729;
	// begin inline asm
	{ atom.add.f64 %fd2773,[%rd720],%fd126; }

	// end inline asm
	add.s64 	%rd721, %rd720, 8;
	// begin inline asm
	{ atom.add.f64 %fd2775,[%rd721],%fd125; }

	// end inline asm
	add.s64 	%rd722, %rd720, 16;
	// begin inline asm
	{ atom.add.f64 %fd2777,[%rd722],%fd124; }

	// end inline asm
	add.s64 	%rd723, %rd720, 24;
	// begin inline asm
	{ atom.add.f64 %fd2779,[%rd723],%fd102; }

	// end inline asm
	add.s64 	%rd724, %rd720, 32;
	// begin inline asm
	{ atom.add.f64 %fd2781,[%rd724],%fd101; }

	// end inline asm
	add.s64 	%rd725, %rd720, 40;
	// begin inline asm
	{ atom.add.f64 %fd2783,[%rd725],%fd100; }

	// end inline asm
	add.s64 	%rd726, %rd720, 48;
	// begin inline asm
	{ atom.add.f64 %fd2785,[%rd726],%fd78; }

	// end inline asm
	add.s64 	%rd727, %rd720, 56;
	// begin inline asm
	{ atom.add.f64 %fd2787,[%rd727],%fd77; }

	// end inline asm
	add.s64 	%rd728, %rd720, 64;
	// begin inline asm
	{ atom.add.f64 %fd2789,[%rd728],%fd76; }

	// end inline asm

$L__BB13_96:
	ld.param.u64 	%rd58, [%rd28];
	ld.param.u32 	%r61, [%rd28+32];
	ld.param.u32 	%r62, [%rd28+60];
	add.s32 	%r1077, %r38, 11;
	setp.le.s32 	%p64, %r62, %r1077;
	selp.u16 	%rs116, 1, 0, %p64;
	shr.u32 	%r1078, %r1077, 31;
	cvt.u16.u32 	%rs117, %r1078;
	or.b16  	%rs118, %rs116, %rs117;
	setp.eq.s16 	%p65, %rs118, 0;
	@%p65 bra 	$L__BB13_98;

	add.s32 	%r2032, %r38, 11;
	st.local.v2.u32 	[%rd30], {%r2032, %r62};
	mov.u64 	%rd730, $str$1;
	cvta.global.u64 	%rd731, %rd730;
	{ // callseq 348, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd731;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1080, [retval0+0];
	} // callseq 348
	bra.uni 	$L__BB13_99;

$L__BB13_98:
	add.s32 	%r2653, %r38, 11;
	mul.wide.s32 	%rd742, %r61, %r2653;
	add.s64 	%rd733, %rd58, %rd742;
	// begin inline asm
	{ atom.add.f64 %fd2791,[%rd733],%fd123; }

	// end inline asm
	add.s64 	%rd734, %rd733, 8;
	// begin inline asm
	{ atom.add.f64 %fd2793,[%rd734],%fd122; }

	// end inline asm
	add.s64 	%rd735, %rd733, 16;
	// begin inline asm
	{ atom.add.f64 %fd2795,[%rd735],%fd121; }

	// end inline asm
	add.s64 	%rd736, %rd733, 24;
	// begin inline asm
	{ atom.add.f64 %fd2797,[%rd736],%fd99; }

	// end inline asm
	add.s64 	%rd737, %rd733, 32;
	// begin inline asm
	{ atom.add.f64 %fd2799,[%rd737],%fd98; }

	// end inline asm
	add.s64 	%rd738, %rd733, 40;
	// begin inline asm
	{ atom.add.f64 %fd2801,[%rd738],%fd97; }

	// end inline asm
	add.s64 	%rd739, %rd733, 48;
	// begin inline asm
	{ atom.add.f64 %fd2803,[%rd739],%fd75; }

	// end inline asm
	add.s64 	%rd740, %rd733, 56;
	// begin inline asm
	{ atom.add.f64 %fd2805,[%rd740],%fd74; }

	// end inline asm
	add.s64 	%rd741, %rd733, 64;
	// begin inline asm
	{ atom.add.f64 %fd2807,[%rd741],%fd73; }

	// end inline asm

$L__BB13_99:
	ld.param.u64 	%rd59, [%rd28];
	ld.param.u32 	%r63, [%rd28+32];
	ld.param.u32 	%r64, [%rd28+60];
	add.s32 	%r1082, %r38, 12;
	setp.le.s32 	%p66, %r64, %r1082;
	selp.u16 	%rs119, 1, 0, %p66;
	shr.u32 	%r1083, %r1082, 31;
	cvt.u16.u32 	%rs120, %r1083;
	or.b16  	%rs121, %rs119, %rs120;
	setp.eq.s16 	%p67, %rs121, 0;
	@%p67 bra 	$L__BB13_101;

	add.s32 	%r2033, %r38, 12;
	st.local.v2.u32 	[%rd30], {%r2033, %r64};
	mov.u64 	%rd743, $str$1;
	cvta.global.u64 	%rd744, %rd743;
	{ // callseq 349, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd744;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1085, [retval0+0];
	} // callseq 349
	bra.uni 	$L__BB13_102;

$L__BB13_101:
	add.s32 	%r2652, %r38, 12;
	mul.wide.s32 	%rd755, %r63, %r2652;
	add.s64 	%rd746, %rd59, %rd755;
	// begin inline asm
	{ atom.add.f64 %fd2809,[%rd746],%fd60; }

	// end inline asm
	add.s64 	%rd747, %rd746, 8;
	// begin inline asm
	{ atom.add.f64 %fd2811,[%rd747],%fd59; }

	// end inline asm
	add.s64 	%rd748, %rd746, 16;
	// begin inline asm
	{ atom.add.f64 %fd2813,[%rd748],%fd58; }

	// end inline asm
	add.s64 	%rd749, %rd746, 24;
	// begin inline asm
	{ atom.add.f64 %fd2815,[%rd749],%fd36; }

	// end inline asm
	add.s64 	%rd750, %rd746, 32;
	// begin inline asm
	{ atom.add.f64 %fd2817,[%rd750],%fd35; }

	// end inline asm
	add.s64 	%rd751, %rd746, 40;
	// begin inline asm
	{ atom.add.f64 %fd2819,[%rd751],%fd34; }

	// end inline asm
	add.s64 	%rd752, %rd746, 48;
	// begin inline asm
	{ atom.add.f64 %fd2821,[%rd752],%fd12; }

	// end inline asm
	add.s64 	%rd753, %rd746, 56;
	// begin inline asm
	{ atom.add.f64 %fd2823,[%rd753],%fd11; }

	// end inline asm
	add.s64 	%rd754, %rd746, 64;
	// begin inline asm
	{ atom.add.f64 %fd2825,[%rd754],%fd10; }

	// end inline asm

$L__BB13_102:
	ld.param.u64 	%rd60, [%rd28];
	ld.param.u32 	%r65, [%rd28+32];
	ld.param.u32 	%r66, [%rd28+60];
	add.s32 	%r1087, %r38, 13;
	setp.le.s32 	%p68, %r66, %r1087;
	selp.u16 	%rs122, 1, 0, %p68;
	shr.u32 	%r1088, %r1087, 31;
	cvt.u16.u32 	%rs123, %r1088;
	or.b16  	%rs124, %rs122, %rs123;
	setp.eq.s16 	%p69, %rs124, 0;
	@%p69 bra 	$L__BB13_104;

	add.s32 	%r2034, %r38, 13;
	st.local.v2.u32 	[%rd30], {%r2034, %r66};
	mov.u64 	%rd756, $str$1;
	cvta.global.u64 	%rd757, %rd756;
	{ // callseq 350, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd757;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1090, [retval0+0];
	} // callseq 350
	bra.uni 	$L__BB13_105;

$L__BB13_104:
	add.s32 	%r2651, %r38, 13;
	mul.wide.s32 	%rd768, %r65, %r2651;
	add.s64 	%rd759, %rd60, %rd768;
	// begin inline asm
	{ atom.add.f64 %fd2827,[%rd759],%fd57; }

	// end inline asm
	add.s64 	%rd760, %rd759, 8;
	// begin inline asm
	{ atom.add.f64 %fd2829,[%rd760],%fd56; }

	// end inline asm
	add.s64 	%rd761, %rd759, 16;
	// begin inline asm
	{ atom.add.f64 %fd2831,[%rd761],%fd55; }

	// end inline asm
	add.s64 	%rd762, %rd759, 24;
	// begin inline asm
	{ atom.add.f64 %fd2833,[%rd762],%fd33; }

	// end inline asm
	add.s64 	%rd763, %rd759, 32;
	// begin inline asm
	{ atom.add.f64 %fd2835,[%rd763],%fd32; }

	// end inline asm
	add.s64 	%rd764, %rd759, 40;
	// begin inline asm
	{ atom.add.f64 %fd2837,[%rd764],%fd31; }

	// end inline asm
	add.s64 	%rd765, %rd759, 48;
	// begin inline asm
	{ atom.add.f64 %fd2839,[%rd765],%fd9; }

	// end inline asm
	add.s64 	%rd766, %rd759, 56;
	// begin inline asm
	{ atom.add.f64 %fd2841,[%rd766],%fd8; }

	// end inline asm
	add.s64 	%rd767, %rd759, 64;
	// begin inline asm
	{ atom.add.f64 %fd2843,[%rd767],%fd7; }

	// end inline asm

$L__BB13_105:
	ld.param.u64 	%rd61, [%rd28];
	ld.param.u32 	%r67, [%rd28+32];
	ld.param.u32 	%r68, [%rd28+60];
	add.s32 	%r1092, %r38, 14;
	setp.le.s32 	%p70, %r68, %r1092;
	selp.u16 	%rs125, 1, 0, %p70;
	shr.u32 	%r1093, %r1092, 31;
	cvt.u16.u32 	%rs126, %r1093;
	or.b16  	%rs127, %rs125, %rs126;
	setp.eq.s16 	%p71, %rs127, 0;
	@%p71 bra 	$L__BB13_107;

	add.s32 	%r2035, %r38, 14;
	st.local.v2.u32 	[%rd30], {%r2035, %r68};
	mov.u64 	%rd769, $str$1;
	cvta.global.u64 	%rd770, %rd769;
	{ // callseq 351, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd770;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1095, [retval0+0];
	} // callseq 351
	bra.uni 	$L__BB13_108;

$L__BB13_107:
	add.s32 	%r2650, %r38, 14;
	mul.wide.s32 	%rd781, %r67, %r2650;
	add.s64 	%rd772, %rd61, %rd781;
	// begin inline asm
	{ atom.add.f64 %fd2845,[%rd772],%fd54; }

	// end inline asm
	add.s64 	%rd773, %rd772, 8;
	// begin inline asm
	{ atom.add.f64 %fd2847,[%rd773],%fd53; }

	// end inline asm
	add.s64 	%rd774, %rd772, 16;
	// begin inline asm
	{ atom.add.f64 %fd2849,[%rd774],%fd52; }

	// end inline asm
	add.s64 	%rd775, %rd772, 24;
	// begin inline asm
	{ atom.add.f64 %fd2851,[%rd775],%fd30; }

	// end inline asm
	add.s64 	%rd776, %rd772, 32;
	// begin inline asm
	{ atom.add.f64 %fd2853,[%rd776],%fd29; }

	// end inline asm
	add.s64 	%rd777, %rd772, 40;
	// begin inline asm
	{ atom.add.f64 %fd2855,[%rd777],%fd28; }

	// end inline asm
	add.s64 	%rd778, %rd772, 48;
	// begin inline asm
	{ atom.add.f64 %fd2857,[%rd778],%fd6; }

	// end inline asm
	add.s64 	%rd779, %rd772, 56;
	// begin inline asm
	{ atom.add.f64 %fd2859,[%rd779],%fd5; }

	// end inline asm
	add.s64 	%rd780, %rd772, 64;
	// begin inline asm
	{ atom.add.f64 %fd2861,[%rd780],%fd4; }

	// end inline asm

$L__BB13_108:
	ld.param.u64 	%rd62, [%rd28];
	ld.param.u32 	%r69, [%rd28+32];
	ld.param.u32 	%r70, [%rd28+60];
	add.s32 	%r1097, %r38, 15;
	setp.le.s32 	%p72, %r70, %r1097;
	selp.u16 	%rs128, 1, 0, %p72;
	shr.u32 	%r1098, %r1097, 31;
	cvt.u16.u32 	%rs129, %r1098;
	or.b16  	%rs130, %rs128, %rs129;
	setp.eq.s16 	%p73, %rs130, 0;
	@%p73 bra 	$L__BB13_110;

	add.s32 	%r2036, %r38, 15;
	st.local.v2.u32 	[%rd30], {%r2036, %r70};
	mov.u64 	%rd782, $str$1;
	cvta.global.u64 	%rd783, %rd782;
	{ // callseq 352, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd783;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1100, [retval0+0];
	} // callseq 352
	bra.uni 	$L__BB13_111;

$L__BB13_110:
	add.s32 	%r2649, %r38, 15;
	mul.wide.s32 	%rd794, %r69, %r2649;
	add.s64 	%rd785, %rd62, %rd794;
	// begin inline asm
	{ atom.add.f64 %fd2863,[%rd785],%fd51; }

	// end inline asm
	add.s64 	%rd786, %rd785, 8;
	// begin inline asm
	{ atom.add.f64 %fd2865,[%rd786],%fd50; }

	// end inline asm
	add.s64 	%rd787, %rd785, 16;
	// begin inline asm
	{ atom.add.f64 %fd2867,[%rd787],%fd49; }

	// end inline asm
	add.s64 	%rd788, %rd785, 24;
	// begin inline asm
	{ atom.add.f64 %fd2869,[%rd788],%fd27; }

	// end inline asm
	add.s64 	%rd789, %rd785, 32;
	// begin inline asm
	{ atom.add.f64 %fd2871,[%rd789],%fd26; }

	// end inline asm
	add.s64 	%rd790, %rd785, 40;
	// begin inline asm
	{ atom.add.f64 %fd2873,[%rd790],%fd25; }

	// end inline asm
	add.s64 	%rd791, %rd785, 48;
	// begin inline asm
	{ atom.add.f64 %fd2875,[%rd791],%fd3; }

	// end inline asm
	add.s64 	%rd792, %rd785, 56;
	// begin inline asm
	{ atom.add.f64 %fd2877,[%rd792],%fd2; }

	// end inline asm
	add.s64 	%rd793, %rd785, 64;
	// begin inline asm
	{ atom.add.f64 %fd2879,[%rd793],%fd1; }

	// end inline asm

$L__BB13_111:
	ld.global.u32 	%r1102, [%rd27];
	shl.b32 	%r71, %r1102, 2;
	ld.global.u32 	%r1103, [%rd46];
	shl.b32 	%r72, %r1103, 2;
	ld.param.u64 	%rd64, [%rd357];
	ld.param.u32 	%r73, [%rd357+32];
	ld.param.u64 	%rd65, [%rd357+56];
	ld.param.u32 	%r74, [%rd357+88];
	ld.param.u64 	%rd66, [%rd357+112];
	ld.param.u32 	%r75, [%rd357+144];
	ld.param.u32 	%r76, [%rd357+172];
	ld.param.v2.u32 	{%r1104, %r1105}, [%rd357+176];
	setp.le.s32 	%p74, %r1104, %r71;
	setp.le.s32 	%p75, %r1105, %r72;
	shl.b32 	%r80, %r2, 4;
	setp.le.s32 	%p76, %r76, %r80;
	or.pred  	%p77, %p74, %p75;
	or.b32  	%r1106, %r71, %r80;
	or.b32  	%r81, %r1106, %r72;
	setp.lt.s32 	%p78, %r81, 0;
	or.pred  	%p79, %p78, %p77;
	or.pred  	%p80, %p76, %p79;
	@%p80 bra 	$L__BB13_113;
	bra.uni 	$L__BB13_112;

$L__BB13_113:
	st.local.v2.u32 	[%rd30], {%r71, %r72};
	st.local.v2.u32 	[%rd30+8], {%r80, %r1104};
	st.local.v2.u32 	[%rd30+16], {%r1105, %r76};
	mov.u64 	%rd812, $str$2;
	cvta.global.u64 	%rd813, %rd812;
	{ // callseq 353, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd813;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1107, [retval0+0];
	} // callseq 353
	bra.uni 	$L__BB13_114;

$L__BB13_112:
	cvta.to.global.u64 	%rd805, %rd65;
	mul.wide.s32 	%rd806, %r75, %r80;
	add.s64 	%rd796, %rd66, %rd806;
	// begin inline asm
	{ atom.add.f64 %fd2881,[%rd796],%fd564; }

	// end inline asm
	add.s64 	%rd797, %rd796, 8;
	// begin inline asm
	{ atom.add.f64 %fd2883,[%rd797],%fd563; }

	// end inline asm
	add.s64 	%rd798, %rd796, 16;
	// begin inline asm
	{ atom.add.f64 %fd2885,[%rd798],%fd562; }

	// end inline asm
	add.s64 	%rd799, %rd796, 24;
	// begin inline asm
	{ atom.add.f64 %fd2887,[%rd799],%fd540; }

	// end inline asm
	add.s64 	%rd800, %rd796, 32;
	// begin inline asm
	{ atom.add.f64 %fd2889,[%rd800],%fd539; }

	// end inline asm
	add.s64 	%rd801, %rd796, 40;
	// begin inline asm
	{ atom.add.f64 %fd2891,[%rd801],%fd538; }

	// end inline asm
	add.s64 	%rd802, %rd796, 48;
	// begin inline asm
	{ atom.add.f64 %fd2893,[%rd802],%fd516; }

	// end inline asm
	add.s64 	%rd803, %rd796, 56;
	// begin inline asm
	{ atom.add.f64 %fd2895,[%rd803],%fd515; }

	// end inline asm
	add.s64 	%rd804, %rd796, 64;
	// begin inline asm
	{ atom.add.f64 %fd2897,[%rd804],%fd514; }

	// end inline asm
	mul.wide.s32 	%rd807, %r73, %r80;
	cvta.to.global.u64 	%rd808, %rd64;
	add.s64 	%rd809, %rd808, %rd807;
	mul.wide.s32 	%rd810, %r74, %r80;
	add.s64 	%rd811, %rd805, %rd810;
	st.global.u32 	[%rd809], %r71;
	st.global.u32 	[%rd811], %r72;

$L__BB13_114:
	ld.param.u64 	%rd68, [%rd357];
	ld.param.u32 	%r82, [%rd357+32];
	ld.param.u64 	%rd69, [%rd357+56];
	ld.param.u32 	%r83, [%rd357+88];
	ld.param.u64 	%rd70, [%rd357+112];
	ld.param.u32 	%r84, [%rd357+144];
	ld.param.u32 	%r85, [%rd357+172];
	ld.param.v2.u32 	{%r1108, %r1109}, [%rd357+176];
	setp.le.s32 	%p81, %r1108, %r71;
	add.s32 	%r89, %r72, 1;
	setp.le.s32 	%p82, %r1109, %r89;
	add.s32 	%r1110, %r80, 1;
	setp.le.s32 	%p83, %r85, %r1110;
	or.pred  	%p84, %p81, %p82;
	or.b32  	%r1111, %r71, %r1110;
	or.b32  	%r90, %r1111, %r89;
	setp.lt.s32 	%p85, %r90, 0;
	or.pred  	%p86, %p85, %p84;
	or.pred  	%p87, %p83, %p86;
	@%p87 bra 	$L__BB13_116;
	bra.uni 	$L__BB13_115;

$L__BB13_116:
	add.s32 	%r2038, %r72, 1;
	st.local.v2.u32 	[%rd30], {%r71, %r2038};
	add.s32 	%r2039, %r80, 1;
	st.local.v2.u32 	[%rd30+8], {%r2039, %r1108};
	st.local.v2.u32 	[%rd30+16], {%r1109, %r85};
	mov.u64 	%rd831, $str$2;
	cvta.global.u64 	%rd832, %rd831;
	{ // callseq 354, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd832;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1114, [retval0+0];
	} // callseq 354
	bra.uni 	$L__BB13_117;

$L__BB13_115:
	add.s32 	%r2620, %r80, 1;
	cvta.to.global.u64 	%rd824, %rd69;
	mul.wide.s32 	%rd825, %r84, %r2620;
	add.s64 	%rd815, %rd70, %rd825;
	// begin inline asm
	{ atom.add.f64 %fd2899,[%rd815],%fd561; }

	// end inline asm
	add.s64 	%rd816, %rd815, 8;
	// begin inline asm
	{ atom.add.f64 %fd2901,[%rd816],%fd560; }

	// end inline asm
	add.s64 	%rd817, %rd815, 16;
	// begin inline asm
	{ atom.add.f64 %fd2903,[%rd817],%fd559; }

	// end inline asm
	add.s64 	%rd818, %rd815, 24;
	// begin inline asm
	{ atom.add.f64 %fd2905,[%rd818],%fd537; }

	// end inline asm
	add.s64 	%rd819, %rd815, 32;
	// begin inline asm
	{ atom.add.f64 %fd2907,[%rd819],%fd536; }

	// end inline asm
	add.s64 	%rd820, %rd815, 40;
	// begin inline asm
	{ atom.add.f64 %fd2909,[%rd820],%fd535; }

	// end inline asm
	add.s64 	%rd821, %rd815, 48;
	// begin inline asm
	{ atom.add.f64 %fd2911,[%rd821],%fd513; }

	// end inline asm
	add.s64 	%rd822, %rd815, 56;
	// begin inline asm
	{ atom.add.f64 %fd2913,[%rd822],%fd512; }

	// end inline asm
	add.s64 	%rd823, %rd815, 64;
	// begin inline asm
	{ atom.add.f64 %fd2915,[%rd823],%fd511; }

	// end inline asm
	mul.wide.s32 	%rd826, %r82, %r2620;
	cvta.to.global.u64 	%rd827, %rd68;
	add.s64 	%rd828, %rd827, %rd826;
	mul.wide.s32 	%rd829, %r83, %r2620;
	add.s64 	%rd830, %rd824, %rd829;
	st.global.u32 	[%rd828], %r71;
	add.s32 	%r2037, %r72, 1;
	st.global.u32 	[%rd830], %r2037;

$L__BB13_117:
	ld.param.u64 	%rd71, [%rd357];
	ld.param.u32 	%r91, [%rd357+32];
	ld.param.u64 	%rd72, [%rd357+56];
	ld.param.u32 	%r92, [%rd357+88];
	ld.param.u64 	%rd73, [%rd357+112];
	ld.param.u32 	%r93, [%rd357+144];
	ld.param.u32 	%r94, [%rd357+172];
	ld.param.v2.u32 	{%r1115, %r1116}, [%rd357+176];
	setp.le.s32 	%p88, %r1115, %r71;
	add.s32 	%r98, %r72, 2;
	setp.le.s32 	%p89, %r1116, %r98;
	add.s32 	%r1117, %r80, 2;
	setp.le.s32 	%p90, %r94, %r1117;
	or.pred  	%p91, %p88, %p89;
	or.b32  	%r1118, %r71, %r1117;
	or.b32  	%r99, %r1118, %r98;
	setp.lt.s32 	%p92, %r99, 0;
	or.pred  	%p93, %p92, %p91;
	or.pred  	%p94, %p90, %p93;
	@%p94 bra 	$L__BB13_119;
	bra.uni 	$L__BB13_118;

$L__BB13_119:
	add.s32 	%r2041, %r72, 2;
	st.local.v2.u32 	[%rd30], {%r71, %r2041};
	add.s32 	%r2042, %r80, 2;
	st.local.v2.u32 	[%rd30+8], {%r2042, %r1115};
	st.local.v2.u32 	[%rd30+16], {%r1116, %r94};
	mov.u64 	%rd850, $str$2;
	cvta.global.u64 	%rd851, %rd850;
	{ // callseq 355, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd851;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1121, [retval0+0];
	} // callseq 355
	bra.uni 	$L__BB13_120;

$L__BB13_118:
	add.s32 	%r2621, %r80, 2;
	cvta.to.global.u64 	%rd843, %rd72;
	mul.wide.s32 	%rd844, %r93, %r2621;
	add.s64 	%rd834, %rd73, %rd844;
	// begin inline asm
	{ atom.add.f64 %fd2917,[%rd834],%fd558; }

	// end inline asm
	add.s64 	%rd835, %rd834, 8;
	// begin inline asm
	{ atom.add.f64 %fd2919,[%rd835],%fd557; }

	// end inline asm
	add.s64 	%rd836, %rd834, 16;
	// begin inline asm
	{ atom.add.f64 %fd2921,[%rd836],%fd556; }

	// end inline asm
	add.s64 	%rd837, %rd834, 24;
	// begin inline asm
	{ atom.add.f64 %fd2923,[%rd837],%fd534; }

	// end inline asm
	add.s64 	%rd838, %rd834, 32;
	// begin inline asm
	{ atom.add.f64 %fd2925,[%rd838],%fd533; }

	// end inline asm
	add.s64 	%rd839, %rd834, 40;
	// begin inline asm
	{ atom.add.f64 %fd2927,[%rd839],%fd532; }

	// end inline asm
	add.s64 	%rd840, %rd834, 48;
	// begin inline asm
	{ atom.add.f64 %fd2929,[%rd840],%fd510; }

	// end inline asm
	add.s64 	%rd841, %rd834, 56;
	// begin inline asm
	{ atom.add.f64 %fd2931,[%rd841],%fd509; }

	// end inline asm
	add.s64 	%rd842, %rd834, 64;
	// begin inline asm
	{ atom.add.f64 %fd2933,[%rd842],%fd508; }

	// end inline asm
	mul.wide.s32 	%rd845, %r91, %r2621;
	cvta.to.global.u64 	%rd846, %rd71;
	add.s64 	%rd847, %rd846, %rd845;
	mul.wide.s32 	%rd848, %r92, %r2621;
	add.s64 	%rd849, %rd843, %rd848;
	st.global.u32 	[%rd847], %r71;
	add.s32 	%r2040, %r72, 2;
	st.global.u32 	[%rd849], %r2040;

$L__BB13_120:
	ld.param.u64 	%rd74, [%rd357];
	ld.param.u32 	%r100, [%rd357+32];
	ld.param.u64 	%rd75, [%rd357+56];
	ld.param.u32 	%r101, [%rd357+88];
	ld.param.u64 	%rd76, [%rd357+112];
	ld.param.u32 	%r102, [%rd357+144];
	ld.param.u32 	%r103, [%rd357+172];
	ld.param.v2.u32 	{%r1122, %r1123}, [%rd357+176];
	setp.le.s32 	%p95, %r1122, %r71;
	add.s32 	%r107, %r72, 3;
	setp.le.s32 	%p96, %r1123, %r107;
	add.s32 	%r1124, %r80, 3;
	setp.le.s32 	%p97, %r103, %r1124;
	or.pred  	%p98, %p95, %p96;
	or.b32  	%r1125, %r71, %r1124;
	or.b32  	%r108, %r1125, %r107;
	setp.lt.s32 	%p99, %r108, 0;
	or.pred  	%p100, %p99, %p98;
	or.pred  	%p101, %p97, %p100;
	@%p101 bra 	$L__BB13_122;
	bra.uni 	$L__BB13_121;

$L__BB13_122:
	add.s32 	%r2044, %r72, 3;
	st.local.v2.u32 	[%rd30], {%r71, %r2044};
	add.s32 	%r2045, %r80, 3;
	st.local.v2.u32 	[%rd30+8], {%r2045, %r1122};
	st.local.v2.u32 	[%rd30+16], {%r1123, %r103};
	mov.u64 	%rd869, $str$2;
	cvta.global.u64 	%rd870, %rd869;
	{ // callseq 356, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd870;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1128, [retval0+0];
	} // callseq 356
	bra.uni 	$L__BB13_123;

$L__BB13_121:
	add.s32 	%r2622, %r80, 3;
	cvta.to.global.u64 	%rd862, %rd75;
	mul.wide.s32 	%rd863, %r102, %r2622;
	add.s64 	%rd853, %rd76, %rd863;
	// begin inline asm
	{ atom.add.f64 %fd2935,[%rd853],%fd555; }

	// end inline asm
	add.s64 	%rd854, %rd853, 8;
	// begin inline asm
	{ atom.add.f64 %fd2937,[%rd854],%fd554; }

	// end inline asm
	add.s64 	%rd855, %rd853, 16;
	// begin inline asm
	{ atom.add.f64 %fd2939,[%rd855],%fd553; }

	// end inline asm
	add.s64 	%rd856, %rd853, 24;
	// begin inline asm
	{ atom.add.f64 %fd2941,[%rd856],%fd531; }

	// end inline asm
	add.s64 	%rd857, %rd853, 32;
	// begin inline asm
	{ atom.add.f64 %fd2943,[%rd857],%fd530; }

	// end inline asm
	add.s64 	%rd858, %rd853, 40;
	// begin inline asm
	{ atom.add.f64 %fd2945,[%rd858],%fd529; }

	// end inline asm
	add.s64 	%rd859, %rd853, 48;
	// begin inline asm
	{ atom.add.f64 %fd2947,[%rd859],%fd507; }

	// end inline asm
	add.s64 	%rd860, %rd853, 56;
	// begin inline asm
	{ atom.add.f64 %fd2949,[%rd860],%fd506; }

	// end inline asm
	add.s64 	%rd861, %rd853, 64;
	// begin inline asm
	{ atom.add.f64 %fd2951,[%rd861],%fd505; }

	// end inline asm
	mul.wide.s32 	%rd864, %r100, %r2622;
	cvta.to.global.u64 	%rd865, %rd74;
	add.s64 	%rd866, %rd865, %rd864;
	mul.wide.s32 	%rd867, %r101, %r2622;
	add.s64 	%rd868, %rd862, %rd867;
	st.global.u32 	[%rd866], %r71;
	add.s32 	%r2043, %r72, 3;
	st.global.u32 	[%rd868], %r2043;

$L__BB13_123:
	ld.param.u64 	%rd77, [%rd357];
	ld.param.u32 	%r109, [%rd357+32];
	ld.param.u64 	%rd78, [%rd357+56];
	ld.param.u32 	%r110, [%rd357+88];
	ld.param.u64 	%rd79, [%rd357+112];
	ld.param.u32 	%r111, [%rd357+144];
	ld.param.u32 	%r112, [%rd357+172];
	ld.param.v2.u32 	{%r1129, %r1130}, [%rd357+176];
	add.s32 	%r116, %r71, 1;
	setp.le.s32 	%p102, %r1129, %r116;
	setp.le.s32 	%p103, %r1130, %r72;
	add.s32 	%r1131, %r80, 4;
	setp.le.s32 	%p104, %r112, %r1131;
	or.pred  	%p105, %p102, %p103;
	or.b32  	%r1132, %r72, %r1131;
	or.b32  	%r117, %r1132, %r116;
	setp.lt.s32 	%p106, %r117, 0;
	or.pred  	%p107, %p106, %p105;
	or.pred  	%p108, %p104, %p107;
	@%p108 bra 	$L__BB13_125;
	bra.uni 	$L__BB13_124;

$L__BB13_125:
	add.s32 	%r2047, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2047, %r72};
	add.s32 	%r2048, %r80, 4;
	st.local.v2.u32 	[%rd30+8], {%r2048, %r1129};
	st.local.v2.u32 	[%rd30+16], {%r1130, %r112};
	mov.u64 	%rd888, $str$2;
	cvta.global.u64 	%rd889, %rd888;
	{ // callseq 357, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd889;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1135, [retval0+0];
	} // callseq 357
	bra.uni 	$L__BB13_126;

$L__BB13_124:
	add.s32 	%r2623, %r80, 4;
	cvta.to.global.u64 	%rd881, %rd78;
	mul.wide.s32 	%rd882, %r111, %r2623;
	add.s64 	%rd872, %rd79, %rd882;
	// begin inline asm
	{ atom.add.f64 %fd2953,[%rd872],%fd492; }

	// end inline asm
	add.s64 	%rd873, %rd872, 8;
	// begin inline asm
	{ atom.add.f64 %fd2955,[%rd873],%fd491; }

	// end inline asm
	add.s64 	%rd874, %rd872, 16;
	// begin inline asm
	{ atom.add.f64 %fd2957,[%rd874],%fd490; }

	// end inline asm
	add.s64 	%rd875, %rd872, 24;
	// begin inline asm
	{ atom.add.f64 %fd2959,[%rd875],%fd468; }

	// end inline asm
	add.s64 	%rd876, %rd872, 32;
	// begin inline asm
	{ atom.add.f64 %fd2961,[%rd876],%fd467; }

	// end inline asm
	add.s64 	%rd877, %rd872, 40;
	// begin inline asm
	{ atom.add.f64 %fd2963,[%rd877],%fd466; }

	// end inline asm
	add.s64 	%rd878, %rd872, 48;
	// begin inline asm
	{ atom.add.f64 %fd2965,[%rd878],%fd444; }

	// end inline asm
	add.s64 	%rd879, %rd872, 56;
	// begin inline asm
	{ atom.add.f64 %fd2967,[%rd879],%fd443; }

	// end inline asm
	add.s64 	%rd880, %rd872, 64;
	// begin inline asm
	{ atom.add.f64 %fd2969,[%rd880],%fd442; }

	// end inline asm
	mul.wide.s32 	%rd883, %r109, %r2623;
	cvta.to.global.u64 	%rd884, %rd77;
	add.s64 	%rd885, %rd884, %rd883;
	mul.wide.s32 	%rd886, %r110, %r2623;
	add.s64 	%rd887, %rd881, %rd886;
	add.s32 	%r2046, %r71, 1;
	st.global.u32 	[%rd885], %r2046;
	st.global.u32 	[%rd887], %r72;

$L__BB13_126:
	ld.param.u64 	%rd80, [%rd357];
	ld.param.u32 	%r118, [%rd357+32];
	ld.param.u64 	%rd81, [%rd357+56];
	ld.param.u32 	%r119, [%rd357+88];
	ld.param.u64 	%rd82, [%rd357+112];
	ld.param.u32 	%r120, [%rd357+144];
	ld.param.u32 	%r121, [%rd357+172];
	ld.param.v2.u32 	{%r1136, %r1137}, [%rd357+176];
	setp.le.s32 	%p109, %r1136, %r116;
	setp.le.s32 	%p110, %r1137, %r89;
	add.s32 	%r1138, %r80, 5;
	setp.le.s32 	%p111, %r121, %r1138;
	or.pred  	%p112, %p109, %p110;
	or.b32  	%r1139, %r116, %r1138;
	or.b32  	%r125, %r1139, %r89;
	setp.lt.s32 	%p113, %r125, 0;
	or.pred  	%p114, %p113, %p112;
	or.pred  	%p115, %p111, %p114;
	@%p115 bra 	$L__BB13_128;
	bra.uni 	$L__BB13_127;

$L__BB13_128:
	add.s32 	%r2051, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2051, %r89};
	add.s32 	%r2052, %r80, 5;
	st.local.v2.u32 	[%rd30+8], {%r2052, %r1136};
	st.local.v2.u32 	[%rd30+16], {%r1137, %r121};
	mov.u64 	%rd907, $str$2;
	cvta.global.u64 	%rd908, %rd907;
	{ // callseq 358, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd908;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1142, [retval0+0];
	} // callseq 358
	bra.uni 	$L__BB13_129;

$L__BB13_127:
	add.s32 	%r2624, %r80, 5;
	cvta.to.global.u64 	%rd900, %rd81;
	mul.wide.s32 	%rd901, %r120, %r2624;
	add.s64 	%rd891, %rd82, %rd901;
	// begin inline asm
	{ atom.add.f64 %fd2971,[%rd891],%fd489; }

	// end inline asm
	add.s64 	%rd892, %rd891, 8;
	// begin inline asm
	{ atom.add.f64 %fd2973,[%rd892],%fd488; }

	// end inline asm
	add.s64 	%rd893, %rd891, 16;
	// begin inline asm
	{ atom.add.f64 %fd2975,[%rd893],%fd487; }

	// end inline asm
	add.s64 	%rd894, %rd891, 24;
	// begin inline asm
	{ atom.add.f64 %fd2977,[%rd894],%fd465; }

	// end inline asm
	add.s64 	%rd895, %rd891, 32;
	// begin inline asm
	{ atom.add.f64 %fd2979,[%rd895],%fd464; }

	// end inline asm
	add.s64 	%rd896, %rd891, 40;
	// begin inline asm
	{ atom.add.f64 %fd2981,[%rd896],%fd463; }

	// end inline asm
	add.s64 	%rd897, %rd891, 48;
	// begin inline asm
	{ atom.add.f64 %fd2983,[%rd897],%fd441; }

	// end inline asm
	add.s64 	%rd898, %rd891, 56;
	// begin inline asm
	{ atom.add.f64 %fd2985,[%rd898],%fd440; }

	// end inline asm
	add.s64 	%rd899, %rd891, 64;
	// begin inline asm
	{ atom.add.f64 %fd2987,[%rd899],%fd439; }

	// end inline asm
	mul.wide.s32 	%rd902, %r118, %r2624;
	cvta.to.global.u64 	%rd903, %rd80;
	add.s64 	%rd904, %rd903, %rd902;
	mul.wide.s32 	%rd905, %r119, %r2624;
	add.s64 	%rd906, %rd900, %rd905;
	add.s32 	%r2049, %r71, 1;
	st.global.u32 	[%rd904], %r2049;
	add.s32 	%r2050, %r72, 1;
	st.global.u32 	[%rd906], %r2050;

$L__BB13_129:
	ld.param.u64 	%rd83, [%rd357];
	ld.param.u32 	%r126, [%rd357+32];
	ld.param.u64 	%rd84, [%rd357+56];
	ld.param.u32 	%r127, [%rd357+88];
	ld.param.u64 	%rd85, [%rd357+112];
	ld.param.u32 	%r128, [%rd357+144];
	ld.param.u32 	%r129, [%rd357+172];
	ld.param.v2.u32 	{%r1143, %r1144}, [%rd357+176];
	setp.le.s32 	%p116, %r1143, %r116;
	setp.le.s32 	%p117, %r1144, %r98;
	add.s32 	%r1145, %r80, 6;
	setp.le.s32 	%p118, %r129, %r1145;
	or.pred  	%p119, %p116, %p117;
	or.b32  	%r1146, %r116, %r1145;
	or.b32  	%r133, %r1146, %r98;
	setp.lt.s32 	%p120, %r133, 0;
	or.pred  	%p121, %p120, %p119;
	or.pred  	%p122, %p118, %p121;
	@%p122 bra 	$L__BB13_131;
	bra.uni 	$L__BB13_130;

$L__BB13_131:
	add.s32 	%r2055, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2055, %r98};
	add.s32 	%r2056, %r80, 6;
	st.local.v2.u32 	[%rd30+8], {%r2056, %r1143};
	st.local.v2.u32 	[%rd30+16], {%r1144, %r129};
	mov.u64 	%rd926, $str$2;
	cvta.global.u64 	%rd927, %rd926;
	{ // callseq 359, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd927;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1149, [retval0+0];
	} // callseq 359
	bra.uni 	$L__BB13_132;

$L__BB13_130:
	add.s32 	%r2625, %r80, 6;
	cvta.to.global.u64 	%rd919, %rd84;
	mul.wide.s32 	%rd920, %r128, %r2625;
	add.s64 	%rd910, %rd85, %rd920;
	// begin inline asm
	{ atom.add.f64 %fd2989,[%rd910],%fd486; }

	// end inline asm
	add.s64 	%rd911, %rd910, 8;
	// begin inline asm
	{ atom.add.f64 %fd2991,[%rd911],%fd485; }

	// end inline asm
	add.s64 	%rd912, %rd910, 16;
	// begin inline asm
	{ atom.add.f64 %fd2993,[%rd912],%fd484; }

	// end inline asm
	add.s64 	%rd913, %rd910, 24;
	// begin inline asm
	{ atom.add.f64 %fd2995,[%rd913],%fd462; }

	// end inline asm
	add.s64 	%rd914, %rd910, 32;
	// begin inline asm
	{ atom.add.f64 %fd2997,[%rd914],%fd461; }

	// end inline asm
	add.s64 	%rd915, %rd910, 40;
	// begin inline asm
	{ atom.add.f64 %fd2999,[%rd915],%fd460; }

	// end inline asm
	add.s64 	%rd916, %rd910, 48;
	// begin inline asm
	{ atom.add.f64 %fd3001,[%rd916],%fd438; }

	// end inline asm
	add.s64 	%rd917, %rd910, 56;
	// begin inline asm
	{ atom.add.f64 %fd3003,[%rd917],%fd437; }

	// end inline asm
	add.s64 	%rd918, %rd910, 64;
	// begin inline asm
	{ atom.add.f64 %fd3005,[%rd918],%fd436; }

	// end inline asm
	mul.wide.s32 	%rd921, %r126, %r2625;
	cvta.to.global.u64 	%rd922, %rd83;
	add.s64 	%rd923, %rd922, %rd921;
	mul.wide.s32 	%rd924, %r127, %r2625;
	add.s64 	%rd925, %rd919, %rd924;
	add.s32 	%r2053, %r71, 1;
	st.global.u32 	[%rd923], %r2053;
	add.s32 	%r2054, %r72, 2;
	st.global.u32 	[%rd925], %r2054;

$L__BB13_132:
	ld.param.u64 	%rd86, [%rd357];
	ld.param.u32 	%r134, [%rd357+32];
	ld.param.u64 	%rd87, [%rd357+56];
	ld.param.u32 	%r135, [%rd357+88];
	ld.param.u64 	%rd88, [%rd357+112];
	ld.param.u32 	%r136, [%rd357+144];
	ld.param.u32 	%r137, [%rd357+172];
	ld.param.v2.u32 	{%r1150, %r1151}, [%rd357+176];
	setp.le.s32 	%p123, %r1150, %r116;
	setp.le.s32 	%p124, %r1151, %r107;
	add.s32 	%r1152, %r80, 7;
	setp.le.s32 	%p125, %r137, %r1152;
	or.pred  	%p126, %p123, %p124;
	or.b32  	%r1153, %r116, %r1152;
	or.b32  	%r141, %r1153, %r107;
	setp.lt.s32 	%p127, %r141, 0;
	or.pred  	%p128, %p127, %p126;
	or.pred  	%p129, %p125, %p128;
	@%p129 bra 	$L__BB13_134;
	bra.uni 	$L__BB13_133;

$L__BB13_134:
	add.s32 	%r2059, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2059, %r107};
	add.s32 	%r2060, %r80, 7;
	st.local.v2.u32 	[%rd30+8], {%r2060, %r1150};
	st.local.v2.u32 	[%rd30+16], {%r1151, %r137};
	mov.u64 	%rd945, $str$2;
	cvta.global.u64 	%rd946, %rd945;
	{ // callseq 360, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd946;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1156, [retval0+0];
	} // callseq 360
	bra.uni 	$L__BB13_135;

$L__BB13_133:
	add.s32 	%r2626, %r80, 7;
	cvta.to.global.u64 	%rd938, %rd87;
	mul.wide.s32 	%rd939, %r136, %r2626;
	add.s64 	%rd929, %rd88, %rd939;
	// begin inline asm
	{ atom.add.f64 %fd3007,[%rd929],%fd483; }

	// end inline asm
	add.s64 	%rd930, %rd929, 8;
	// begin inline asm
	{ atom.add.f64 %fd3009,[%rd930],%fd482; }

	// end inline asm
	add.s64 	%rd931, %rd929, 16;
	// begin inline asm
	{ atom.add.f64 %fd3011,[%rd931],%fd481; }

	// end inline asm
	add.s64 	%rd932, %rd929, 24;
	// begin inline asm
	{ atom.add.f64 %fd3013,[%rd932],%fd459; }

	// end inline asm
	add.s64 	%rd933, %rd929, 32;
	// begin inline asm
	{ atom.add.f64 %fd3015,[%rd933],%fd458; }

	// end inline asm
	add.s64 	%rd934, %rd929, 40;
	// begin inline asm
	{ atom.add.f64 %fd3017,[%rd934],%fd457; }

	// end inline asm
	add.s64 	%rd935, %rd929, 48;
	// begin inline asm
	{ atom.add.f64 %fd3019,[%rd935],%fd435; }

	// end inline asm
	add.s64 	%rd936, %rd929, 56;
	// begin inline asm
	{ atom.add.f64 %fd3021,[%rd936],%fd434; }

	// end inline asm
	add.s64 	%rd937, %rd929, 64;
	// begin inline asm
	{ atom.add.f64 %fd3023,[%rd937],%fd433; }

	// end inline asm
	mul.wide.s32 	%rd940, %r134, %r2626;
	cvta.to.global.u64 	%rd941, %rd86;
	add.s64 	%rd942, %rd941, %rd940;
	mul.wide.s32 	%rd943, %r135, %r2626;
	add.s64 	%rd944, %rd938, %rd943;
	add.s32 	%r2057, %r71, 1;
	st.global.u32 	[%rd942], %r2057;
	add.s32 	%r2058, %r72, 3;
	st.global.u32 	[%rd944], %r2058;

$L__BB13_135:
	ld.param.u64 	%rd89, [%rd357];
	ld.param.u32 	%r142, [%rd357+32];
	ld.param.u64 	%rd90, [%rd357+56];
	ld.param.u32 	%r143, [%rd357+88];
	ld.param.u64 	%rd91, [%rd357+112];
	ld.param.u32 	%r144, [%rd357+144];
	ld.param.u32 	%r145, [%rd357+172];
	ld.param.v2.u32 	{%r1157, %r1158}, [%rd357+176];
	add.s32 	%r149, %r71, 2;
	setp.le.s32 	%p130, %r1157, %r149;
	setp.le.s32 	%p131, %r1158, %r72;
	add.s32 	%r1159, %r80, 8;
	setp.le.s32 	%p132, %r145, %r1159;
	or.pred  	%p133, %p130, %p131;
	or.b32  	%r1160, %r72, %r1159;
	or.b32  	%r150, %r1160, %r149;
	setp.lt.s32 	%p134, %r150, 0;
	or.pred  	%p135, %p134, %p133;
	or.pred  	%p136, %p132, %p135;
	@%p136 bra 	$L__BB13_137;
	bra.uni 	$L__BB13_136;

$L__BB13_137:
	add.s32 	%r2062, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2062, %r72};
	add.s32 	%r2063, %r80, 8;
	st.local.v2.u32 	[%rd30+8], {%r2063, %r1157};
	st.local.v2.u32 	[%rd30+16], {%r1158, %r145};
	mov.u64 	%rd964, $str$2;
	cvta.global.u64 	%rd965, %rd964;
	{ // callseq 361, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd965;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1163, [retval0+0];
	} // callseq 361
	bra.uni 	$L__BB13_138;

$L__BB13_136:
	add.s32 	%r2627, %r80, 8;
	cvta.to.global.u64 	%rd957, %rd90;
	mul.wide.s32 	%rd958, %r144, %r2627;
	add.s64 	%rd948, %rd91, %rd958;
	// begin inline asm
	{ atom.add.f64 %fd3025,[%rd948],%fd420; }

	// end inline asm
	add.s64 	%rd949, %rd948, 8;
	// begin inline asm
	{ atom.add.f64 %fd3027,[%rd949],%fd419; }

	// end inline asm
	add.s64 	%rd950, %rd948, 16;
	// begin inline asm
	{ atom.add.f64 %fd3029,[%rd950],%fd418; }

	// end inline asm
	add.s64 	%rd951, %rd948, 24;
	// begin inline asm
	{ atom.add.f64 %fd3031,[%rd951],%fd396; }

	// end inline asm
	add.s64 	%rd952, %rd948, 32;
	// begin inline asm
	{ atom.add.f64 %fd3033,[%rd952],%fd395; }

	// end inline asm
	add.s64 	%rd953, %rd948, 40;
	// begin inline asm
	{ atom.add.f64 %fd3035,[%rd953],%fd394; }

	// end inline asm
	add.s64 	%rd954, %rd948, 48;
	// begin inline asm
	{ atom.add.f64 %fd3037,[%rd954],%fd372; }

	// end inline asm
	add.s64 	%rd955, %rd948, 56;
	// begin inline asm
	{ atom.add.f64 %fd3039,[%rd955],%fd371; }

	// end inline asm
	add.s64 	%rd956, %rd948, 64;
	// begin inline asm
	{ atom.add.f64 %fd3041,[%rd956],%fd370; }

	// end inline asm
	mul.wide.s32 	%rd959, %r142, %r2627;
	cvta.to.global.u64 	%rd960, %rd89;
	add.s64 	%rd961, %rd960, %rd959;
	mul.wide.s32 	%rd962, %r143, %r2627;
	add.s64 	%rd963, %rd957, %rd962;
	add.s32 	%r2061, %r71, 2;
	st.global.u32 	[%rd961], %r2061;
	st.global.u32 	[%rd963], %r72;

$L__BB13_138:
	ld.param.u64 	%rd92, [%rd357];
	ld.param.u32 	%r151, [%rd357+32];
	ld.param.u64 	%rd93, [%rd357+56];
	ld.param.u32 	%r152, [%rd357+88];
	ld.param.u64 	%rd94, [%rd357+112];
	ld.param.u32 	%r153, [%rd357+144];
	ld.param.u32 	%r154, [%rd357+172];
	ld.param.v2.u32 	{%r1164, %r1165}, [%rd357+176];
	setp.le.s32 	%p137, %r1164, %r149;
	setp.le.s32 	%p138, %r1165, %r89;
	add.s32 	%r1166, %r80, 9;
	setp.le.s32 	%p139, %r154, %r1166;
	or.pred  	%p140, %p137, %p138;
	or.b32  	%r1167, %r149, %r1166;
	or.b32  	%r158, %r1167, %r89;
	setp.lt.s32 	%p141, %r158, 0;
	or.pred  	%p142, %p141, %p140;
	or.pred  	%p143, %p139, %p142;
	@%p143 bra 	$L__BB13_140;
	bra.uni 	$L__BB13_139;

$L__BB13_140:
	add.s32 	%r2066, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2066, %r89};
	add.s32 	%r2067, %r80, 9;
	st.local.v2.u32 	[%rd30+8], {%r2067, %r1164};
	st.local.v2.u32 	[%rd30+16], {%r1165, %r154};
	mov.u64 	%rd983, $str$2;
	cvta.global.u64 	%rd984, %rd983;
	{ // callseq 362, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd984;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1170, [retval0+0];
	} // callseq 362
	bra.uni 	$L__BB13_141;

$L__BB13_139:
	add.s32 	%r2628, %r80, 9;
	cvta.to.global.u64 	%rd976, %rd93;
	mul.wide.s32 	%rd977, %r153, %r2628;
	add.s64 	%rd967, %rd94, %rd977;
	// begin inline asm
	{ atom.add.f64 %fd3043,[%rd967],%fd417; }

	// end inline asm
	add.s64 	%rd968, %rd967, 8;
	// begin inline asm
	{ atom.add.f64 %fd3045,[%rd968],%fd416; }

	// end inline asm
	add.s64 	%rd969, %rd967, 16;
	// begin inline asm
	{ atom.add.f64 %fd3047,[%rd969],%fd415; }

	// end inline asm
	add.s64 	%rd970, %rd967, 24;
	// begin inline asm
	{ atom.add.f64 %fd3049,[%rd970],%fd393; }

	// end inline asm
	add.s64 	%rd971, %rd967, 32;
	// begin inline asm
	{ atom.add.f64 %fd3051,[%rd971],%fd392; }

	// end inline asm
	add.s64 	%rd972, %rd967, 40;
	// begin inline asm
	{ atom.add.f64 %fd3053,[%rd972],%fd391; }

	// end inline asm
	add.s64 	%rd973, %rd967, 48;
	// begin inline asm
	{ atom.add.f64 %fd3055,[%rd973],%fd369; }

	// end inline asm
	add.s64 	%rd974, %rd967, 56;
	// begin inline asm
	{ atom.add.f64 %fd3057,[%rd974],%fd368; }

	// end inline asm
	add.s64 	%rd975, %rd967, 64;
	// begin inline asm
	{ atom.add.f64 %fd3059,[%rd975],%fd367; }

	// end inline asm
	mul.wide.s32 	%rd978, %r151, %r2628;
	cvta.to.global.u64 	%rd979, %rd92;
	add.s64 	%rd980, %rd979, %rd978;
	mul.wide.s32 	%rd981, %r152, %r2628;
	add.s64 	%rd982, %rd976, %rd981;
	add.s32 	%r2064, %r71, 2;
	st.global.u32 	[%rd980], %r2064;
	add.s32 	%r2065, %r72, 1;
	st.global.u32 	[%rd982], %r2065;

$L__BB13_141:
	ld.param.u64 	%rd95, [%rd357];
	ld.param.u32 	%r159, [%rd357+32];
	ld.param.u64 	%rd96, [%rd357+56];
	ld.param.u32 	%r160, [%rd357+88];
	ld.param.u64 	%rd97, [%rd357+112];
	ld.param.u32 	%r161, [%rd357+144];
	ld.param.u32 	%r162, [%rd357+172];
	ld.param.v2.u32 	{%r1171, %r1172}, [%rd357+176];
	setp.le.s32 	%p144, %r1171, %r149;
	setp.le.s32 	%p145, %r1172, %r98;
	add.s32 	%r1173, %r80, 10;
	setp.le.s32 	%p146, %r162, %r1173;
	or.pred  	%p147, %p144, %p145;
	or.b32  	%r1174, %r149, %r1173;
	or.b32  	%r166, %r1174, %r98;
	setp.lt.s32 	%p148, %r166, 0;
	or.pred  	%p149, %p148, %p147;
	or.pred  	%p150, %p146, %p149;
	@%p150 bra 	$L__BB13_143;
	bra.uni 	$L__BB13_142;

$L__BB13_143:
	add.s32 	%r2070, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2070, %r98};
	add.s32 	%r2071, %r80, 10;
	st.local.v2.u32 	[%rd30+8], {%r2071, %r1171};
	st.local.v2.u32 	[%rd30+16], {%r1172, %r162};
	mov.u64 	%rd1002, $str$2;
	cvta.global.u64 	%rd1003, %rd1002;
	{ // callseq 363, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1003;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1177, [retval0+0];
	} // callseq 363
	bra.uni 	$L__BB13_144;

$L__BB13_142:
	add.s32 	%r2629, %r80, 10;
	cvta.to.global.u64 	%rd995, %rd96;
	mul.wide.s32 	%rd996, %r161, %r2629;
	add.s64 	%rd986, %rd97, %rd996;
	// begin inline asm
	{ atom.add.f64 %fd3061,[%rd986],%fd414; }

	// end inline asm
	add.s64 	%rd987, %rd986, 8;
	// begin inline asm
	{ atom.add.f64 %fd3063,[%rd987],%fd413; }

	// end inline asm
	add.s64 	%rd988, %rd986, 16;
	// begin inline asm
	{ atom.add.f64 %fd3065,[%rd988],%fd412; }

	// end inline asm
	add.s64 	%rd989, %rd986, 24;
	// begin inline asm
	{ atom.add.f64 %fd3067,[%rd989],%fd390; }

	// end inline asm
	add.s64 	%rd990, %rd986, 32;
	// begin inline asm
	{ atom.add.f64 %fd3069,[%rd990],%fd389; }

	// end inline asm
	add.s64 	%rd991, %rd986, 40;
	// begin inline asm
	{ atom.add.f64 %fd3071,[%rd991],%fd388; }

	// end inline asm
	add.s64 	%rd992, %rd986, 48;
	// begin inline asm
	{ atom.add.f64 %fd3073,[%rd992],%fd366; }

	// end inline asm
	add.s64 	%rd993, %rd986, 56;
	// begin inline asm
	{ atom.add.f64 %fd3075,[%rd993],%fd365; }

	// end inline asm
	add.s64 	%rd994, %rd986, 64;
	// begin inline asm
	{ atom.add.f64 %fd3077,[%rd994],%fd364; }

	// end inline asm
	mul.wide.s32 	%rd997, %r159, %r2629;
	cvta.to.global.u64 	%rd998, %rd95;
	add.s64 	%rd999, %rd998, %rd997;
	mul.wide.s32 	%rd1000, %r160, %r2629;
	add.s64 	%rd1001, %rd995, %rd1000;
	add.s32 	%r2068, %r71, 2;
	st.global.u32 	[%rd999], %r2068;
	add.s32 	%r2069, %r72, 2;
	st.global.u32 	[%rd1001], %r2069;

$L__BB13_144:
	ld.param.u64 	%rd98, [%rd357];
	ld.param.u32 	%r167, [%rd357+32];
	ld.param.u64 	%rd99, [%rd357+56];
	ld.param.u32 	%r168, [%rd357+88];
	ld.param.u64 	%rd100, [%rd357+112];
	ld.param.u32 	%r169, [%rd357+144];
	ld.param.u32 	%r170, [%rd357+172];
	ld.param.v2.u32 	{%r1178, %r1179}, [%rd357+176];
	setp.le.s32 	%p151, %r1178, %r149;
	setp.le.s32 	%p152, %r1179, %r107;
	add.s32 	%r1180, %r80, 11;
	setp.le.s32 	%p153, %r170, %r1180;
	or.pred  	%p154, %p151, %p152;
	or.b32  	%r1181, %r149, %r1180;
	or.b32  	%r174, %r1181, %r107;
	setp.lt.s32 	%p155, %r174, 0;
	or.pred  	%p156, %p155, %p154;
	or.pred  	%p157, %p153, %p156;
	@%p157 bra 	$L__BB13_146;
	bra.uni 	$L__BB13_145;

$L__BB13_146:
	add.s32 	%r2074, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2074, %r107};
	add.s32 	%r2075, %r80, 11;
	st.local.v2.u32 	[%rd30+8], {%r2075, %r1178};
	st.local.v2.u32 	[%rd30+16], {%r1179, %r170};
	mov.u64 	%rd1021, $str$2;
	cvta.global.u64 	%rd1022, %rd1021;
	{ // callseq 364, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1022;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1184, [retval0+0];
	} // callseq 364
	bra.uni 	$L__BB13_147;

$L__BB13_145:
	add.s32 	%r2630, %r80, 11;
	cvta.to.global.u64 	%rd1014, %rd99;
	mul.wide.s32 	%rd1015, %r169, %r2630;
	add.s64 	%rd1005, %rd100, %rd1015;
	// begin inline asm
	{ atom.add.f64 %fd3079,[%rd1005],%fd411; }

	// end inline asm
	add.s64 	%rd1006, %rd1005, 8;
	// begin inline asm
	{ atom.add.f64 %fd3081,[%rd1006],%fd410; }

	// end inline asm
	add.s64 	%rd1007, %rd1005, 16;
	// begin inline asm
	{ atom.add.f64 %fd3083,[%rd1007],%fd409; }

	// end inline asm
	add.s64 	%rd1008, %rd1005, 24;
	// begin inline asm
	{ atom.add.f64 %fd3085,[%rd1008],%fd387; }

	// end inline asm
	add.s64 	%rd1009, %rd1005, 32;
	// begin inline asm
	{ atom.add.f64 %fd3087,[%rd1009],%fd386; }

	// end inline asm
	add.s64 	%rd1010, %rd1005, 40;
	// begin inline asm
	{ atom.add.f64 %fd3089,[%rd1010],%fd385; }

	// end inline asm
	add.s64 	%rd1011, %rd1005, 48;
	// begin inline asm
	{ atom.add.f64 %fd3091,[%rd1011],%fd363; }

	// end inline asm
	add.s64 	%rd1012, %rd1005, 56;
	// begin inline asm
	{ atom.add.f64 %fd3093,[%rd1012],%fd362; }

	// end inline asm
	add.s64 	%rd1013, %rd1005, 64;
	// begin inline asm
	{ atom.add.f64 %fd3095,[%rd1013],%fd361; }

	// end inline asm
	mul.wide.s32 	%rd1016, %r167, %r2630;
	cvta.to.global.u64 	%rd1017, %rd98;
	add.s64 	%rd1018, %rd1017, %rd1016;
	mul.wide.s32 	%rd1019, %r168, %r2630;
	add.s64 	%rd1020, %rd1014, %rd1019;
	add.s32 	%r2072, %r71, 2;
	st.global.u32 	[%rd1018], %r2072;
	add.s32 	%r2073, %r72, 3;
	st.global.u32 	[%rd1020], %r2073;

$L__BB13_147:
	ld.param.u32 	%r178, [%rd357+172];
	ld.param.v2.u32 	{%r1185, %r1186}, [%rd357+176];
	add.s32 	%r182, %r71, 3;
	setp.le.s32 	%p158, %r1185, %r182;
	setp.le.s32 	%p159, %r1186, %r72;
	add.s32 	%r1187, %r80, 12;
	setp.le.s32 	%p160, %r178, %r1187;
	or.pred  	%p161, %p158, %p159;
	or.b32  	%r1188, %r72, %r1187;
	or.b32  	%r183, %r1188, %r182;
	setp.lt.s32 	%p162, %r183, 0;
	or.pred  	%p163, %p162, %p161;
	or.pred  	%p164, %p160, %p163;
	@%p164 bra 	$L__BB13_149;
	bra.uni 	$L__BB13_148;

$L__BB13_149:
	add.s32 	%r2077, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2077, %r72};
	add.s32 	%r2078, %r80, 12;
	st.local.v2.u32 	[%rd30+8], {%r2078, %r1185};
	st.local.v2.u32 	[%rd30+16], {%r1186, %r178};
	mov.u64 	%rd1040, $str$2;
	cvta.global.u64 	%rd1041, %rd1040;
	{ // callseq 365, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1041;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1191, [retval0+0];
	} // callseq 365
	bra.uni 	$L__BB13_150;

$L__BB13_148:
	ld.param.u32 	%r2805, [%rd357+88];
	ld.param.u64 	%rd4033, [%rd357];
	ld.param.u32 	%r2804, [%rd357+32];
	ld.param.u64 	%rd4032, [%rd357+112];
	ld.param.u32 	%r2803, [%rd357+144];
	ld.param.u64 	%rd4031, [%rd357+56];
	add.s32 	%r2631, %r80, 12;
	cvta.to.global.u64 	%rd1033, %rd4031;
	mul.wide.s32 	%rd1034, %r2803, %r2631;
	add.s64 	%rd1024, %rd4032, %rd1034;
	// begin inline asm
	{ atom.add.f64 %fd3097,[%rd1024],%fd348; }

	// end inline asm
	add.s64 	%rd1025, %rd1024, 8;
	// begin inline asm
	{ atom.add.f64 %fd3099,[%rd1025],%fd347; }

	// end inline asm
	add.s64 	%rd1026, %rd1024, 16;
	// begin inline asm
	{ atom.add.f64 %fd3101,[%rd1026],%fd346; }

	// end inline asm
	add.s64 	%rd1027, %rd1024, 24;
	// begin inline asm
	{ atom.add.f64 %fd3103,[%rd1027],%fd324; }

	// end inline asm
	add.s64 	%rd1028, %rd1024, 32;
	// begin inline asm
	{ atom.add.f64 %fd3105,[%rd1028],%fd323; }

	// end inline asm
	add.s64 	%rd1029, %rd1024, 40;
	// begin inline asm
	{ atom.add.f64 %fd3107,[%rd1029],%fd322; }

	// end inline asm
	add.s64 	%rd1030, %rd1024, 48;
	// begin inline asm
	{ atom.add.f64 %fd3109,[%rd1030],%fd300; }

	// end inline asm
	add.s64 	%rd1031, %rd1024, 56;
	// begin inline asm
	{ atom.add.f64 %fd3111,[%rd1031],%fd299; }

	// end inline asm
	add.s64 	%rd1032, %rd1024, 64;
	// begin inline asm
	{ atom.add.f64 %fd3113,[%rd1032],%fd298; }

	// end inline asm
	mul.wide.s32 	%rd1035, %r2804, %r2631;
	cvta.to.global.u64 	%rd1036, %rd4033;
	add.s64 	%rd1037, %rd1036, %rd1035;
	mul.wide.s32 	%rd1038, %r2805, %r2631;
	add.s64 	%rd1039, %rd1033, %rd1038;
	add.s32 	%r2076, %r71, 3;
	st.global.u32 	[%rd1037], %r2076;
	st.global.u32 	[%rd1039], %r72;

$L__BB13_150:
	ld.param.u32 	%r187, [%rd357+172];
	ld.param.v2.u32 	{%r1192, %r1193}, [%rd357+176];
	setp.le.s32 	%p165, %r1192, %r182;
	setp.le.s32 	%p166, %r1193, %r89;
	add.s32 	%r1194, %r80, 13;
	setp.le.s32 	%p167, %r187, %r1194;
	or.pred  	%p168, %p165, %p166;
	or.b32  	%r1195, %r182, %r1194;
	or.b32  	%r191, %r1195, %r89;
	setp.lt.s32 	%p169, %r191, 0;
	or.pred  	%p170, %p169, %p168;
	or.pred  	%p171, %p167, %p170;
	@%p171 bra 	$L__BB13_152;
	bra.uni 	$L__BB13_151;

$L__BB13_152:
	add.s32 	%r2081, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2081, %r89};
	add.s32 	%r2082, %r80, 13;
	st.local.v2.u32 	[%rd30+8], {%r2082, %r1192};
	st.local.v2.u32 	[%rd30+16], {%r1193, %r187};
	mov.u64 	%rd1059, $str$2;
	cvta.global.u64 	%rd1060, %rd1059;
	{ // callseq 366, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1060;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1198, [retval0+0];
	} // callseq 366
	bra.uni 	$L__BB13_153;

$L__BB13_151:
	ld.param.u32 	%r2769, [%rd357+88];
	ld.param.u64 	%rd3997, [%rd357];
	ld.param.u32 	%r2768, [%rd357+32];
	ld.param.u64 	%rd3996, [%rd357+112];
	ld.param.u32 	%r2767, [%rd357+144];
	ld.param.u64 	%rd3995, [%rd357+56];
	add.s32 	%r2632, %r80, 13;
	cvta.to.global.u64 	%rd1052, %rd3995;
	mul.wide.s32 	%rd1053, %r2767, %r2632;
	add.s64 	%rd1043, %rd3996, %rd1053;
	// begin inline asm
	{ atom.add.f64 %fd3115,[%rd1043],%fd345; }

	// end inline asm
	add.s64 	%rd1044, %rd1043, 8;
	// begin inline asm
	{ atom.add.f64 %fd3117,[%rd1044],%fd344; }

	// end inline asm
	add.s64 	%rd1045, %rd1043, 16;
	// begin inline asm
	{ atom.add.f64 %fd3119,[%rd1045],%fd343; }

	// end inline asm
	add.s64 	%rd1046, %rd1043, 24;
	// begin inline asm
	{ atom.add.f64 %fd3121,[%rd1046],%fd321; }

	// end inline asm
	add.s64 	%rd1047, %rd1043, 32;
	// begin inline asm
	{ atom.add.f64 %fd3123,[%rd1047],%fd320; }

	// end inline asm
	add.s64 	%rd1048, %rd1043, 40;
	// begin inline asm
	{ atom.add.f64 %fd3125,[%rd1048],%fd319; }

	// end inline asm
	add.s64 	%rd1049, %rd1043, 48;
	// begin inline asm
	{ atom.add.f64 %fd3127,[%rd1049],%fd297; }

	// end inline asm
	add.s64 	%rd1050, %rd1043, 56;
	// begin inline asm
	{ atom.add.f64 %fd3129,[%rd1050],%fd296; }

	// end inline asm
	add.s64 	%rd1051, %rd1043, 64;
	// begin inline asm
	{ atom.add.f64 %fd3131,[%rd1051],%fd295; }

	// end inline asm
	mul.wide.s32 	%rd1054, %r2768, %r2632;
	cvta.to.global.u64 	%rd1055, %rd3997;
	add.s64 	%rd1056, %rd1055, %rd1054;
	mul.wide.s32 	%rd1057, %r2769, %r2632;
	add.s64 	%rd1058, %rd1052, %rd1057;
	add.s32 	%r2079, %r71, 3;
	st.global.u32 	[%rd1056], %r2079;
	add.s32 	%r2080, %r72, 1;
	st.global.u32 	[%rd1058], %r2080;

$L__BB13_153:
	ld.param.u32 	%r195, [%rd357+172];
	ld.param.v2.u32 	{%r1199, %r1200}, [%rd357+176];
	setp.le.s32 	%p172, %r1199, %r182;
	setp.le.s32 	%p173, %r1200, %r98;
	add.s32 	%r1201, %r80, 14;
	setp.le.s32 	%p174, %r195, %r1201;
	or.pred  	%p175, %p172, %p173;
	or.b32  	%r1202, %r182, %r1201;
	or.b32  	%r199, %r1202, %r98;
	setp.lt.s32 	%p176, %r199, 0;
	or.pred  	%p177, %p176, %p175;
	or.pred  	%p178, %p174, %p177;
	@%p178 bra 	$L__BB13_155;
	bra.uni 	$L__BB13_154;

$L__BB13_155:
	add.s32 	%r2085, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2085, %r98};
	add.s32 	%r2086, %r80, 14;
	st.local.v2.u32 	[%rd30+8], {%r2086, %r1199};
	st.local.v2.u32 	[%rd30+16], {%r1200, %r195};
	mov.u64 	%rd1078, $str$2;
	cvta.global.u64 	%rd1079, %rd1078;
	{ // callseq 367, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1079;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1205, [retval0+0];
	} // callseq 367
	bra.uni 	$L__BB13_156;

$L__BB13_154:
	ld.param.u32 	%r2772, [%rd357+88];
	ld.param.u64 	%rd4000, [%rd357];
	ld.param.u32 	%r2771, [%rd357+32];
	ld.param.u64 	%rd3999, [%rd357+112];
	ld.param.u32 	%r2770, [%rd357+144];
	ld.param.u64 	%rd3998, [%rd357+56];
	add.s32 	%r2633, %r80, 14;
	cvta.to.global.u64 	%rd1071, %rd3998;
	mul.wide.s32 	%rd1072, %r2770, %r2633;
	add.s64 	%rd1062, %rd3999, %rd1072;
	// begin inline asm
	{ atom.add.f64 %fd3133,[%rd1062],%fd342; }

	// end inline asm
	add.s64 	%rd1063, %rd1062, 8;
	// begin inline asm
	{ atom.add.f64 %fd3135,[%rd1063],%fd341; }

	// end inline asm
	add.s64 	%rd1064, %rd1062, 16;
	// begin inline asm
	{ atom.add.f64 %fd3137,[%rd1064],%fd340; }

	// end inline asm
	add.s64 	%rd1065, %rd1062, 24;
	// begin inline asm
	{ atom.add.f64 %fd3139,[%rd1065],%fd318; }

	// end inline asm
	add.s64 	%rd1066, %rd1062, 32;
	// begin inline asm
	{ atom.add.f64 %fd3141,[%rd1066],%fd317; }

	// end inline asm
	add.s64 	%rd1067, %rd1062, 40;
	// begin inline asm
	{ atom.add.f64 %fd3143,[%rd1067],%fd316; }

	// end inline asm
	add.s64 	%rd1068, %rd1062, 48;
	// begin inline asm
	{ atom.add.f64 %fd3145,[%rd1068],%fd294; }

	// end inline asm
	add.s64 	%rd1069, %rd1062, 56;
	// begin inline asm
	{ atom.add.f64 %fd3147,[%rd1069],%fd293; }

	// end inline asm
	add.s64 	%rd1070, %rd1062, 64;
	// begin inline asm
	{ atom.add.f64 %fd3149,[%rd1070],%fd292; }

	// end inline asm
	mul.wide.s32 	%rd1073, %r2771, %r2633;
	cvta.to.global.u64 	%rd1074, %rd4000;
	add.s64 	%rd1075, %rd1074, %rd1073;
	mul.wide.s32 	%rd1076, %r2772, %r2633;
	add.s64 	%rd1077, %rd1071, %rd1076;
	add.s32 	%r2083, %r71, 3;
	st.global.u32 	[%rd1075], %r2083;
	add.s32 	%r2084, %r72, 2;
	st.global.u32 	[%rd1077], %r2084;

$L__BB13_156:
	ld.param.u32 	%r203, [%rd357+172];
	ld.param.v2.u32 	{%r1206, %r1207}, [%rd357+176];
	setp.le.s32 	%p179, %r1206, %r182;
	setp.le.s32 	%p180, %r1207, %r107;
	add.s32 	%r1208, %r80, 15;
	setp.le.s32 	%p181, %r203, %r1208;
	or.pred  	%p182, %p179, %p180;
	or.b32  	%r1209, %r182, %r1208;
	or.b32  	%r207, %r1209, %r107;
	setp.lt.s32 	%p183, %r207, 0;
	or.pred  	%p184, %p183, %p182;
	or.pred  	%p185, %p181, %p184;
	@%p185 bra 	$L__BB13_158;
	bra.uni 	$L__BB13_157;

$L__BB13_158:
	add.s32 	%r2089, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2089, %r107};
	add.s32 	%r2090, %r80, 15;
	st.local.v2.u32 	[%rd30+8], {%r2090, %r1206};
	st.local.v2.u32 	[%rd30+16], {%r1207, %r203};
	mov.u64 	%rd1097, $str$2;
	cvta.global.u64 	%rd1098, %rd1097;
	{ // callseq 368, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1098;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1212, [retval0+0];
	} // callseq 368
	bra.uni 	$L__BB13_159;

$L__BB13_157:
	ld.param.u32 	%r2775, [%rd357+88];
	ld.param.u64 	%rd4003, [%rd357];
	ld.param.u32 	%r2774, [%rd357+32];
	ld.param.u64 	%rd4002, [%rd357+112];
	ld.param.u32 	%r2773, [%rd357+144];
	ld.param.u64 	%rd4001, [%rd357+56];
	add.s32 	%r2634, %r80, 15;
	cvta.to.global.u64 	%rd1090, %rd4001;
	mul.wide.s32 	%rd1091, %r2773, %r2634;
	add.s64 	%rd1081, %rd4002, %rd1091;
	// begin inline asm
	{ atom.add.f64 %fd3151,[%rd1081],%fd339; }

	// end inline asm
	add.s64 	%rd1082, %rd1081, 8;
	// begin inline asm
	{ atom.add.f64 %fd3153,[%rd1082],%fd338; }

	// end inline asm
	add.s64 	%rd1083, %rd1081, 16;
	// begin inline asm
	{ atom.add.f64 %fd3155,[%rd1083],%fd337; }

	// end inline asm
	add.s64 	%rd1084, %rd1081, 24;
	// begin inline asm
	{ atom.add.f64 %fd3157,[%rd1084],%fd315; }

	// end inline asm
	add.s64 	%rd1085, %rd1081, 32;
	// begin inline asm
	{ atom.add.f64 %fd3159,[%rd1085],%fd314; }

	// end inline asm
	add.s64 	%rd1086, %rd1081, 40;
	// begin inline asm
	{ atom.add.f64 %fd3161,[%rd1086],%fd313; }

	// end inline asm
	add.s64 	%rd1087, %rd1081, 48;
	// begin inline asm
	{ atom.add.f64 %fd3163,[%rd1087],%fd291; }

	// end inline asm
	add.s64 	%rd1088, %rd1081, 56;
	// begin inline asm
	{ atom.add.f64 %fd3165,[%rd1088],%fd290; }

	// end inline asm
	add.s64 	%rd1089, %rd1081, 64;
	// begin inline asm
	{ atom.add.f64 %fd3167,[%rd1089],%fd289; }

	// end inline asm
	mul.wide.s32 	%rd1092, %r2774, %r2634;
	cvta.to.global.u64 	%rd1093, %rd4003;
	add.s64 	%rd1094, %rd1093, %rd1092;
	mul.wide.s32 	%rd1095, %r2775, %r2634;
	add.s64 	%rd1096, %rd1090, %rd1095;
	add.s32 	%r2087, %r71, 3;
	st.global.u32 	[%rd1094], %r2087;
	add.s32 	%r2088, %r72, 3;
	st.global.u32 	[%rd1096], %r2088;

$L__BB13_159:
	add.s32 	%r1213, %r2, %r889;
	shl.b32 	%r208, %r1213, 4;
	ld.global.u32 	%r1214, [%rd46];
	shl.b32 	%r209, %r1214, 2;
	ld.global.u32 	%r1215, [%rd27];
	shl.b32 	%r210, %r1215, 2;
	ld.param.u32 	%r214, [%rd357+172];
	ld.param.v2.u32 	{%r1216, %r1217}, [%rd357+176];
	setp.le.s32 	%p186, %r1216, %r209;
	setp.le.s32 	%p187, %r1217, %r210;
	setp.le.s32 	%p188, %r214, %r208;
	or.pred  	%p189, %p186, %p187;
	or.b32  	%r1218, %r209, %r208;
	or.b32  	%r218, %r1218, %r210;
	setp.lt.s32 	%p190, %r218, 0;
	or.pred  	%p191, %p190, %p189;
	or.pred  	%p192, %p188, %p191;
	@%p192 bra 	$L__BB13_161;
	bra.uni 	$L__BB13_160;

$L__BB13_161:
	st.local.v2.u32 	[%rd30], {%r209, %r210};
	st.local.v2.u32 	[%rd30+8], {%r208, %r1216};
	st.local.v2.u32 	[%rd30+16], {%r1217, %r214};
	mov.u64 	%rd1116, $str$2;
	cvta.global.u64 	%rd1117, %rd1116;
	{ // callseq 369, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1117;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1219, [retval0+0];
	} // callseq 369
	bra.uni 	$L__BB13_162;

$L__BB13_160:
	ld.param.u32 	%r2778, [%rd357+88];
	ld.param.u64 	%rd4006, [%rd357];
	ld.param.u32 	%r2777, [%rd357+32];
	ld.param.u64 	%rd4005, [%rd357+112];
	ld.param.u32 	%r2776, [%rd357+144];
	ld.param.u64 	%rd4004, [%rd357+56];
	cvta.to.global.u64 	%rd1109, %rd4004;
	mul.wide.s32 	%rd1110, %r2776, %r208;
	add.s64 	%rd1100, %rd4005, %rd1110;
	// begin inline asm
	{ atom.add.f64 %fd3169,[%rd1100],%fd288; }

	// end inline asm
	add.s64 	%rd1101, %rd1100, 8;
	// begin inline asm
	{ atom.add.f64 %fd3171,[%rd1101],%fd287; }

	// end inline asm
	add.s64 	%rd1102, %rd1100, 16;
	// begin inline asm
	{ atom.add.f64 %fd3173,[%rd1102],%fd286; }

	// end inline asm
	add.s64 	%rd1103, %rd1100, 24;
	// begin inline asm
	{ atom.add.f64 %fd3175,[%rd1103],%fd264; }

	// end inline asm
	add.s64 	%rd1104, %rd1100, 32;
	// begin inline asm
	{ atom.add.f64 %fd3177,[%rd1104],%fd263; }

	// end inline asm
	add.s64 	%rd1105, %rd1100, 40;
	// begin inline asm
	{ atom.add.f64 %fd3179,[%rd1105],%fd262; }

	// end inline asm
	add.s64 	%rd1106, %rd1100, 48;
	// begin inline asm
	{ atom.add.f64 %fd3181,[%rd1106],%fd240; }

	// end inline asm
	add.s64 	%rd1107, %rd1100, 56;
	// begin inline asm
	{ atom.add.f64 %fd3183,[%rd1107],%fd239; }

	// end inline asm
	add.s64 	%rd1108, %rd1100, 64;
	// begin inline asm
	{ atom.add.f64 %fd3185,[%rd1108],%fd238; }

	// end inline asm
	mul.wide.s32 	%rd1111, %r2777, %r208;
	cvta.to.global.u64 	%rd1112, %rd4006;
	add.s64 	%rd1113, %rd1112, %rd1111;
	mul.wide.s32 	%rd1114, %r2778, %r208;
	add.s64 	%rd1115, %rd1109, %rd1114;
	st.global.u32 	[%rd1113], %r209;
	st.global.u32 	[%rd1115], %r210;

$L__BB13_162:
	ld.param.u32 	%r222, [%rd357+172];
	ld.param.v2.u32 	{%r1220, %r1221}, [%rd357+176];
	setp.le.s32 	%p193, %r1220, %r209;
	add.s32 	%r226, %r210, 1;
	setp.le.s32 	%p194, %r1221, %r226;
	add.s32 	%r1222, %r208, 1;
	setp.le.s32 	%p195, %r222, %r1222;
	or.pred  	%p196, %p193, %p194;
	or.b32  	%r1223, %r209, %r1222;
	or.b32  	%r227, %r1223, %r226;
	setp.lt.s32 	%p197, %r227, 0;
	or.pred  	%p198, %p197, %p196;
	or.pred  	%p199, %p195, %p198;
	@%p199 bra 	$L__BB13_164;
	bra.uni 	$L__BB13_163;

$L__BB13_164:
	add.s32 	%r2092, %r210, 1;
	st.local.v2.u32 	[%rd30], {%r209, %r2092};
	add.s32 	%r2093, %r208, 1;
	st.local.v2.u32 	[%rd30+8], {%r2093, %r1220};
	st.local.v2.u32 	[%rd30+16], {%r1221, %r222};
	mov.u64 	%rd1135, $str$2;
	cvta.global.u64 	%rd1136, %rd1135;
	{ // callseq 370, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1136;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1226, [retval0+0];
	} // callseq 370
	bra.uni 	$L__BB13_165;

$L__BB13_163:
	ld.param.u32 	%r2781, [%rd357+88];
	ld.param.u64 	%rd4009, [%rd357];
	ld.param.u32 	%r2780, [%rd357+32];
	ld.param.u64 	%rd4008, [%rd357+112];
	ld.param.u32 	%r2779, [%rd357+144];
	ld.param.u64 	%rd4007, [%rd357+56];
	add.s32 	%r2635, %r208, 1;
	cvta.to.global.u64 	%rd1128, %rd4007;
	mul.wide.s32 	%rd1129, %r2779, %r2635;
	add.s64 	%rd1119, %rd4008, %rd1129;
	// begin inline asm
	{ atom.add.f64 %fd3187,[%rd1119],%fd285; }

	// end inline asm
	add.s64 	%rd1120, %rd1119, 8;
	// begin inline asm
	{ atom.add.f64 %fd3189,[%rd1120],%fd284; }

	// end inline asm
	add.s64 	%rd1121, %rd1119, 16;
	// begin inline asm
	{ atom.add.f64 %fd3191,[%rd1121],%fd283; }

	// end inline asm
	add.s64 	%rd1122, %rd1119, 24;
	// begin inline asm
	{ atom.add.f64 %fd3193,[%rd1122],%fd261; }

	// end inline asm
	add.s64 	%rd1123, %rd1119, 32;
	// begin inline asm
	{ atom.add.f64 %fd3195,[%rd1123],%fd260; }

	// end inline asm
	add.s64 	%rd1124, %rd1119, 40;
	// begin inline asm
	{ atom.add.f64 %fd3197,[%rd1124],%fd259; }

	// end inline asm
	add.s64 	%rd1125, %rd1119, 48;
	// begin inline asm
	{ atom.add.f64 %fd3199,[%rd1125],%fd237; }

	// end inline asm
	add.s64 	%rd1126, %rd1119, 56;
	// begin inline asm
	{ atom.add.f64 %fd3201,[%rd1126],%fd236; }

	// end inline asm
	add.s64 	%rd1127, %rd1119, 64;
	// begin inline asm
	{ atom.add.f64 %fd3203,[%rd1127],%fd235; }

	// end inline asm
	mul.wide.s32 	%rd1130, %r2780, %r2635;
	cvta.to.global.u64 	%rd1131, %rd4009;
	add.s64 	%rd1132, %rd1131, %rd1130;
	mul.wide.s32 	%rd1133, %r2781, %r2635;
	add.s64 	%rd1134, %rd1128, %rd1133;
	st.global.u32 	[%rd1132], %r209;
	add.s32 	%r2091, %r210, 1;
	st.global.u32 	[%rd1134], %r2091;

$L__BB13_165:
	ld.param.u32 	%r231, [%rd357+172];
	ld.param.v2.u32 	{%r1227, %r1228}, [%rd357+176];
	setp.le.s32 	%p200, %r1227, %r209;
	add.s32 	%r235, %r210, 2;
	setp.le.s32 	%p201, %r1228, %r235;
	add.s32 	%r1229, %r208, 2;
	setp.le.s32 	%p202, %r231, %r1229;
	or.pred  	%p203, %p200, %p201;
	or.b32  	%r1230, %r209, %r1229;
	or.b32  	%r236, %r1230, %r235;
	setp.lt.s32 	%p204, %r236, 0;
	or.pred  	%p205, %p204, %p203;
	or.pred  	%p206, %p202, %p205;
	@%p206 bra 	$L__BB13_167;
	bra.uni 	$L__BB13_166;

$L__BB13_167:
	add.s32 	%r2095, %r210, 2;
	st.local.v2.u32 	[%rd30], {%r209, %r2095};
	add.s32 	%r2096, %r208, 2;
	st.local.v2.u32 	[%rd30+8], {%r2096, %r1227};
	st.local.v2.u32 	[%rd30+16], {%r1228, %r231};
	mov.u64 	%rd1154, $str$2;
	cvta.global.u64 	%rd1155, %rd1154;
	{ // callseq 371, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1155;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1233, [retval0+0];
	} // callseq 371
	bra.uni 	$L__BB13_168;

$L__BB13_166:
	ld.param.u32 	%r2784, [%rd357+88];
	ld.param.u64 	%rd4012, [%rd357];
	ld.param.u32 	%r2783, [%rd357+32];
	ld.param.u64 	%rd4011, [%rd357+112];
	ld.param.u32 	%r2782, [%rd357+144];
	ld.param.u64 	%rd4010, [%rd357+56];
	add.s32 	%r2636, %r208, 2;
	cvta.to.global.u64 	%rd1147, %rd4010;
	mul.wide.s32 	%rd1148, %r2782, %r2636;
	add.s64 	%rd1138, %rd4011, %rd1148;
	// begin inline asm
	{ atom.add.f64 %fd3205,[%rd1138],%fd282; }

	// end inline asm
	add.s64 	%rd1139, %rd1138, 8;
	// begin inline asm
	{ atom.add.f64 %fd3207,[%rd1139],%fd281; }

	// end inline asm
	add.s64 	%rd1140, %rd1138, 16;
	// begin inline asm
	{ atom.add.f64 %fd3209,[%rd1140],%fd280; }

	// end inline asm
	add.s64 	%rd1141, %rd1138, 24;
	// begin inline asm
	{ atom.add.f64 %fd3211,[%rd1141],%fd258; }

	// end inline asm
	add.s64 	%rd1142, %rd1138, 32;
	// begin inline asm
	{ atom.add.f64 %fd3213,[%rd1142],%fd257; }

	// end inline asm
	add.s64 	%rd1143, %rd1138, 40;
	// begin inline asm
	{ atom.add.f64 %fd3215,[%rd1143],%fd256; }

	// end inline asm
	add.s64 	%rd1144, %rd1138, 48;
	// begin inline asm
	{ atom.add.f64 %fd3217,[%rd1144],%fd234; }

	// end inline asm
	add.s64 	%rd1145, %rd1138, 56;
	// begin inline asm
	{ atom.add.f64 %fd3219,[%rd1145],%fd233; }

	// end inline asm
	add.s64 	%rd1146, %rd1138, 64;
	// begin inline asm
	{ atom.add.f64 %fd3221,[%rd1146],%fd232; }

	// end inline asm
	mul.wide.s32 	%rd1149, %r2783, %r2636;
	cvta.to.global.u64 	%rd1150, %rd4012;
	add.s64 	%rd1151, %rd1150, %rd1149;
	mul.wide.s32 	%rd1152, %r2784, %r2636;
	add.s64 	%rd1153, %rd1147, %rd1152;
	st.global.u32 	[%rd1151], %r209;
	add.s32 	%r2094, %r210, 2;
	st.global.u32 	[%rd1153], %r2094;

$L__BB13_168:
	ld.param.u32 	%r240, [%rd357+172];
	ld.param.v2.u32 	{%r1234, %r1235}, [%rd357+176];
	setp.le.s32 	%p207, %r1234, %r209;
	add.s32 	%r244, %r210, 3;
	setp.le.s32 	%p208, %r1235, %r244;
	add.s32 	%r1236, %r208, 3;
	setp.le.s32 	%p209, %r240, %r1236;
	or.pred  	%p210, %p207, %p208;
	or.b32  	%r1237, %r209, %r1236;
	or.b32  	%r245, %r1237, %r244;
	setp.lt.s32 	%p211, %r245, 0;
	or.pred  	%p212, %p211, %p210;
	or.pred  	%p213, %p209, %p212;
	@%p213 bra 	$L__BB13_170;
	bra.uni 	$L__BB13_169;

$L__BB13_170:
	add.s32 	%r2098, %r210, 3;
	st.local.v2.u32 	[%rd30], {%r209, %r2098};
	add.s32 	%r2099, %r208, 3;
	st.local.v2.u32 	[%rd30+8], {%r2099, %r1234};
	st.local.v2.u32 	[%rd30+16], {%r1235, %r240};
	mov.u64 	%rd1173, $str$2;
	cvta.global.u64 	%rd1174, %rd1173;
	{ // callseq 372, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1174;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1240, [retval0+0];
	} // callseq 372
	bra.uni 	$L__BB13_171;

$L__BB13_169:
	ld.param.u32 	%r2787, [%rd357+88];
	ld.param.u64 	%rd4015, [%rd357];
	ld.param.u32 	%r2786, [%rd357+32];
	ld.param.u64 	%rd4014, [%rd357+112];
	ld.param.u32 	%r2785, [%rd357+144];
	ld.param.u64 	%rd4013, [%rd357+56];
	add.s32 	%r2637, %r208, 3;
	cvta.to.global.u64 	%rd1166, %rd4013;
	mul.wide.s32 	%rd1167, %r2785, %r2637;
	add.s64 	%rd1157, %rd4014, %rd1167;
	// begin inline asm
	{ atom.add.f64 %fd3223,[%rd1157],%fd279; }

	// end inline asm
	add.s64 	%rd1158, %rd1157, 8;
	// begin inline asm
	{ atom.add.f64 %fd3225,[%rd1158],%fd278; }

	// end inline asm
	add.s64 	%rd1159, %rd1157, 16;
	// begin inline asm
	{ atom.add.f64 %fd3227,[%rd1159],%fd277; }

	// end inline asm
	add.s64 	%rd1160, %rd1157, 24;
	// begin inline asm
	{ atom.add.f64 %fd3229,[%rd1160],%fd255; }

	// end inline asm
	add.s64 	%rd1161, %rd1157, 32;
	// begin inline asm
	{ atom.add.f64 %fd3231,[%rd1161],%fd254; }

	// end inline asm
	add.s64 	%rd1162, %rd1157, 40;
	// begin inline asm
	{ atom.add.f64 %fd3233,[%rd1162],%fd253; }

	// end inline asm
	add.s64 	%rd1163, %rd1157, 48;
	// begin inline asm
	{ atom.add.f64 %fd3235,[%rd1163],%fd231; }

	// end inline asm
	add.s64 	%rd1164, %rd1157, 56;
	// begin inline asm
	{ atom.add.f64 %fd3237,[%rd1164],%fd230; }

	// end inline asm
	add.s64 	%rd1165, %rd1157, 64;
	// begin inline asm
	{ atom.add.f64 %fd3239,[%rd1165],%fd229; }

	// end inline asm
	mul.wide.s32 	%rd1168, %r2786, %r2637;
	cvta.to.global.u64 	%rd1169, %rd4015;
	add.s64 	%rd1170, %rd1169, %rd1168;
	mul.wide.s32 	%rd1171, %r2787, %r2637;
	add.s64 	%rd1172, %rd1166, %rd1171;
	st.global.u32 	[%rd1170], %r209;
	add.s32 	%r2097, %r210, 3;
	st.global.u32 	[%rd1172], %r2097;

$L__BB13_171:
	ld.param.u32 	%r249, [%rd357+172];
	ld.param.v2.u32 	{%r1241, %r1242}, [%rd357+176];
	add.s32 	%r253, %r209, 1;
	setp.le.s32 	%p214, %r1241, %r253;
	setp.le.s32 	%p215, %r1242, %r210;
	add.s32 	%r1243, %r208, 4;
	setp.le.s32 	%p216, %r249, %r1243;
	or.pred  	%p217, %p214, %p215;
	or.b32  	%r1244, %r210, %r1243;
	or.b32  	%r254, %r1244, %r253;
	setp.lt.s32 	%p218, %r254, 0;
	or.pred  	%p219, %p218, %p217;
	or.pred  	%p220, %p216, %p219;
	@%p220 bra 	$L__BB13_173;
	bra.uni 	$L__BB13_172;

$L__BB13_173:
	add.s32 	%r2101, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2101, %r210};
	add.s32 	%r2102, %r208, 4;
	st.local.v2.u32 	[%rd30+8], {%r2102, %r1241};
	st.local.v2.u32 	[%rd30+16], {%r1242, %r249};
	mov.u64 	%rd1192, $str$2;
	cvta.global.u64 	%rd1193, %rd1192;
	{ // callseq 373, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1193;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1247, [retval0+0];
	} // callseq 373
	bra.uni 	$L__BB13_174;

$L__BB13_172:
	ld.param.u32 	%r2790, [%rd357+88];
	ld.param.u64 	%rd4018, [%rd357];
	ld.param.u32 	%r2789, [%rd357+32];
	ld.param.u64 	%rd4017, [%rd357+112];
	ld.param.u32 	%r2788, [%rd357+144];
	ld.param.u64 	%rd4016, [%rd357+56];
	add.s32 	%r2638, %r208, 4;
	cvta.to.global.u64 	%rd1185, %rd4016;
	mul.wide.s32 	%rd1186, %r2788, %r2638;
	add.s64 	%rd1176, %rd4017, %rd1186;
	// begin inline asm
	{ atom.add.f64 %fd3241,[%rd1176],%fd216; }

	// end inline asm
	add.s64 	%rd1177, %rd1176, 8;
	// begin inline asm
	{ atom.add.f64 %fd3243,[%rd1177],%fd215; }

	// end inline asm
	add.s64 	%rd1178, %rd1176, 16;
	// begin inline asm
	{ atom.add.f64 %fd3245,[%rd1178],%fd214; }

	// end inline asm
	add.s64 	%rd1179, %rd1176, 24;
	// begin inline asm
	{ atom.add.f64 %fd3247,[%rd1179],%fd192; }

	// end inline asm
	add.s64 	%rd1180, %rd1176, 32;
	// begin inline asm
	{ atom.add.f64 %fd3249,[%rd1180],%fd191; }

	// end inline asm
	add.s64 	%rd1181, %rd1176, 40;
	// begin inline asm
	{ atom.add.f64 %fd3251,[%rd1181],%fd190; }

	// end inline asm
	add.s64 	%rd1182, %rd1176, 48;
	// begin inline asm
	{ atom.add.f64 %fd3253,[%rd1182],%fd168; }

	// end inline asm
	add.s64 	%rd1183, %rd1176, 56;
	// begin inline asm
	{ atom.add.f64 %fd3255,[%rd1183],%fd167; }

	// end inline asm
	add.s64 	%rd1184, %rd1176, 64;
	// begin inline asm
	{ atom.add.f64 %fd3257,[%rd1184],%fd166; }

	// end inline asm
	mul.wide.s32 	%rd1187, %r2789, %r2638;
	cvta.to.global.u64 	%rd1188, %rd4018;
	add.s64 	%rd1189, %rd1188, %rd1187;
	mul.wide.s32 	%rd1190, %r2790, %r2638;
	add.s64 	%rd1191, %rd1185, %rd1190;
	add.s32 	%r2100, %r209, 1;
	st.global.u32 	[%rd1189], %r2100;
	st.global.u32 	[%rd1191], %r210;

$L__BB13_174:
	ld.param.u32 	%r258, [%rd357+172];
	ld.param.v2.u32 	{%r1248, %r1249}, [%rd357+176];
	setp.le.s32 	%p221, %r1248, %r253;
	setp.le.s32 	%p222, %r1249, %r226;
	add.s32 	%r1250, %r208, 5;
	setp.le.s32 	%p223, %r258, %r1250;
	or.pred  	%p224, %p221, %p222;
	or.b32  	%r1251, %r253, %r1250;
	or.b32  	%r262, %r1251, %r226;
	setp.lt.s32 	%p225, %r262, 0;
	or.pred  	%p226, %p225, %p224;
	or.pred  	%p227, %p223, %p226;
	@%p227 bra 	$L__BB13_176;
	bra.uni 	$L__BB13_175;

$L__BB13_176:
	add.s32 	%r2105, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2105, %r226};
	add.s32 	%r2106, %r208, 5;
	st.local.v2.u32 	[%rd30+8], {%r2106, %r1248};
	st.local.v2.u32 	[%rd30+16], {%r1249, %r258};
	mov.u64 	%rd1211, $str$2;
	cvta.global.u64 	%rd1212, %rd1211;
	{ // callseq 374, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1212;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1254, [retval0+0];
	} // callseq 374
	bra.uni 	$L__BB13_177;

$L__BB13_175:
	ld.param.u32 	%r2793, [%rd357+88];
	ld.param.u64 	%rd4021, [%rd357];
	ld.param.u32 	%r2792, [%rd357+32];
	ld.param.u64 	%rd4020, [%rd357+112];
	ld.param.u32 	%r2791, [%rd357+144];
	ld.param.u64 	%rd4019, [%rd357+56];
	add.s32 	%r2639, %r208, 5;
	cvta.to.global.u64 	%rd1204, %rd4019;
	mul.wide.s32 	%rd1205, %r2791, %r2639;
	add.s64 	%rd1195, %rd4020, %rd1205;
	// begin inline asm
	{ atom.add.f64 %fd3259,[%rd1195],%fd213; }

	// end inline asm
	add.s64 	%rd1196, %rd1195, 8;
	// begin inline asm
	{ atom.add.f64 %fd3261,[%rd1196],%fd212; }

	// end inline asm
	add.s64 	%rd1197, %rd1195, 16;
	// begin inline asm
	{ atom.add.f64 %fd3263,[%rd1197],%fd211; }

	// end inline asm
	add.s64 	%rd1198, %rd1195, 24;
	// begin inline asm
	{ atom.add.f64 %fd3265,[%rd1198],%fd189; }

	// end inline asm
	add.s64 	%rd1199, %rd1195, 32;
	// begin inline asm
	{ atom.add.f64 %fd3267,[%rd1199],%fd188; }

	// end inline asm
	add.s64 	%rd1200, %rd1195, 40;
	// begin inline asm
	{ atom.add.f64 %fd3269,[%rd1200],%fd187; }

	// end inline asm
	add.s64 	%rd1201, %rd1195, 48;
	// begin inline asm
	{ atom.add.f64 %fd3271,[%rd1201],%fd165; }

	// end inline asm
	add.s64 	%rd1202, %rd1195, 56;
	// begin inline asm
	{ atom.add.f64 %fd3273,[%rd1202],%fd164; }

	// end inline asm
	add.s64 	%rd1203, %rd1195, 64;
	// begin inline asm
	{ atom.add.f64 %fd3275,[%rd1203],%fd163; }

	// end inline asm
	mul.wide.s32 	%rd1206, %r2792, %r2639;
	cvta.to.global.u64 	%rd1207, %rd4021;
	add.s64 	%rd1208, %rd1207, %rd1206;
	mul.wide.s32 	%rd1209, %r2793, %r2639;
	add.s64 	%rd1210, %rd1204, %rd1209;
	add.s32 	%r2103, %r209, 1;
	st.global.u32 	[%rd1208], %r2103;
	add.s32 	%r2104, %r210, 1;
	st.global.u32 	[%rd1210], %r2104;

$L__BB13_177:
	ld.param.u32 	%r266, [%rd357+172];
	ld.param.v2.u32 	{%r1255, %r1256}, [%rd357+176];
	setp.le.s32 	%p228, %r1255, %r253;
	setp.le.s32 	%p229, %r1256, %r235;
	add.s32 	%r1257, %r208, 6;
	setp.le.s32 	%p230, %r266, %r1257;
	or.pred  	%p231, %p228, %p229;
	or.b32  	%r1258, %r253, %r1257;
	or.b32  	%r270, %r1258, %r235;
	setp.lt.s32 	%p232, %r270, 0;
	or.pred  	%p233, %p232, %p231;
	or.pred  	%p234, %p230, %p233;
	@%p234 bra 	$L__BB13_179;
	bra.uni 	$L__BB13_178;

$L__BB13_179:
	add.s32 	%r2109, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2109, %r235};
	add.s32 	%r2110, %r208, 6;
	st.local.v2.u32 	[%rd30+8], {%r2110, %r1255};
	st.local.v2.u32 	[%rd30+16], {%r1256, %r266};
	mov.u64 	%rd1230, $str$2;
	cvta.global.u64 	%rd1231, %rd1230;
	{ // callseq 375, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1231;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1261, [retval0+0];
	} // callseq 375
	bra.uni 	$L__BB13_180;

$L__BB13_178:
	ld.param.u32 	%r2796, [%rd357+88];
	ld.param.u64 	%rd4024, [%rd357];
	ld.param.u32 	%r2795, [%rd357+32];
	ld.param.u64 	%rd4023, [%rd357+112];
	ld.param.u32 	%r2794, [%rd357+144];
	ld.param.u64 	%rd4022, [%rd357+56];
	add.s32 	%r2640, %r208, 6;
	cvta.to.global.u64 	%rd1223, %rd4022;
	mul.wide.s32 	%rd1224, %r2794, %r2640;
	add.s64 	%rd1214, %rd4023, %rd1224;
	// begin inline asm
	{ atom.add.f64 %fd3277,[%rd1214],%fd210; }

	// end inline asm
	add.s64 	%rd1215, %rd1214, 8;
	// begin inline asm
	{ atom.add.f64 %fd3279,[%rd1215],%fd209; }

	// end inline asm
	add.s64 	%rd1216, %rd1214, 16;
	// begin inline asm
	{ atom.add.f64 %fd3281,[%rd1216],%fd208; }

	// end inline asm
	add.s64 	%rd1217, %rd1214, 24;
	// begin inline asm
	{ atom.add.f64 %fd3283,[%rd1217],%fd186; }

	// end inline asm
	add.s64 	%rd1218, %rd1214, 32;
	// begin inline asm
	{ atom.add.f64 %fd3285,[%rd1218],%fd185; }

	// end inline asm
	add.s64 	%rd1219, %rd1214, 40;
	// begin inline asm
	{ atom.add.f64 %fd3287,[%rd1219],%fd184; }

	// end inline asm
	add.s64 	%rd1220, %rd1214, 48;
	// begin inline asm
	{ atom.add.f64 %fd3289,[%rd1220],%fd162; }

	// end inline asm
	add.s64 	%rd1221, %rd1214, 56;
	// begin inline asm
	{ atom.add.f64 %fd3291,[%rd1221],%fd161; }

	// end inline asm
	add.s64 	%rd1222, %rd1214, 64;
	// begin inline asm
	{ atom.add.f64 %fd3293,[%rd1222],%fd160; }

	// end inline asm
	mul.wide.s32 	%rd1225, %r2795, %r2640;
	cvta.to.global.u64 	%rd1226, %rd4024;
	add.s64 	%rd1227, %rd1226, %rd1225;
	mul.wide.s32 	%rd1228, %r2796, %r2640;
	add.s64 	%rd1229, %rd1223, %rd1228;
	add.s32 	%r2107, %r209, 1;
	st.global.u32 	[%rd1227], %r2107;
	add.s32 	%r2108, %r210, 2;
	st.global.u32 	[%rd1229], %r2108;

$L__BB13_180:
	ld.param.u32 	%r274, [%rd357+172];
	ld.param.v2.u32 	{%r1262, %r1263}, [%rd357+176];
	setp.le.s32 	%p235, %r1262, %r253;
	setp.le.s32 	%p236, %r1263, %r244;
	add.s32 	%r1264, %r208, 7;
	setp.le.s32 	%p237, %r274, %r1264;
	or.pred  	%p238, %p235, %p236;
	or.b32  	%r1265, %r253, %r1264;
	or.b32  	%r278, %r1265, %r244;
	setp.lt.s32 	%p239, %r278, 0;
	or.pred  	%p240, %p239, %p238;
	or.pred  	%p241, %p237, %p240;
	@%p241 bra 	$L__BB13_182;
	bra.uni 	$L__BB13_181;

$L__BB13_182:
	add.s32 	%r2113, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2113, %r244};
	add.s32 	%r2114, %r208, 7;
	st.local.v2.u32 	[%rd30+8], {%r2114, %r1262};
	st.local.v2.u32 	[%rd30+16], {%r1263, %r274};
	mov.u64 	%rd1249, $str$2;
	cvta.global.u64 	%rd1250, %rd1249;
	{ // callseq 376, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1250;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1268, [retval0+0];
	} // callseq 376
	bra.uni 	$L__BB13_183;

$L__BB13_181:
	ld.param.u32 	%r2799, [%rd357+88];
	ld.param.u64 	%rd4027, [%rd357];
	ld.param.u32 	%r2798, [%rd357+32];
	ld.param.u64 	%rd4026, [%rd357+112];
	ld.param.u32 	%r2797, [%rd357+144];
	ld.param.u64 	%rd4025, [%rd357+56];
	add.s32 	%r2641, %r208, 7;
	cvta.to.global.u64 	%rd1242, %rd4025;
	mul.wide.s32 	%rd1243, %r2797, %r2641;
	add.s64 	%rd1233, %rd4026, %rd1243;
	// begin inline asm
	{ atom.add.f64 %fd3295,[%rd1233],%fd207; }

	// end inline asm
	add.s64 	%rd1234, %rd1233, 8;
	// begin inline asm
	{ atom.add.f64 %fd3297,[%rd1234],%fd206; }

	// end inline asm
	add.s64 	%rd1235, %rd1233, 16;
	// begin inline asm
	{ atom.add.f64 %fd3299,[%rd1235],%fd205; }

	// end inline asm
	add.s64 	%rd1236, %rd1233, 24;
	// begin inline asm
	{ atom.add.f64 %fd3301,[%rd1236],%fd183; }

	// end inline asm
	add.s64 	%rd1237, %rd1233, 32;
	// begin inline asm
	{ atom.add.f64 %fd3303,[%rd1237],%fd182; }

	// end inline asm
	add.s64 	%rd1238, %rd1233, 40;
	// begin inline asm
	{ atom.add.f64 %fd3305,[%rd1238],%fd181; }

	// end inline asm
	add.s64 	%rd1239, %rd1233, 48;
	// begin inline asm
	{ atom.add.f64 %fd3307,[%rd1239],%fd159; }

	// end inline asm
	add.s64 	%rd1240, %rd1233, 56;
	// begin inline asm
	{ atom.add.f64 %fd3309,[%rd1240],%fd158; }

	// end inline asm
	add.s64 	%rd1241, %rd1233, 64;
	// begin inline asm
	{ atom.add.f64 %fd3311,[%rd1241],%fd157; }

	// end inline asm
	mul.wide.s32 	%rd1244, %r2798, %r2641;
	cvta.to.global.u64 	%rd1245, %rd4027;
	add.s64 	%rd1246, %rd1245, %rd1244;
	mul.wide.s32 	%rd1247, %r2799, %r2641;
	add.s64 	%rd1248, %rd1242, %rd1247;
	add.s32 	%r2111, %r209, 1;
	st.global.u32 	[%rd1246], %r2111;
	add.s32 	%r2112, %r210, 3;
	st.global.u32 	[%rd1248], %r2112;

$L__BB13_183:
	ld.param.u32 	%r282, [%rd357+172];
	ld.param.v2.u32 	{%r1269, %r1270}, [%rd357+176];
	add.s32 	%r286, %r209, 2;
	setp.le.s32 	%p242, %r1269, %r286;
	setp.le.s32 	%p243, %r1270, %r210;
	add.s32 	%r1271, %r208, 8;
	setp.le.s32 	%p244, %r282, %r1271;
	or.pred  	%p245, %p242, %p243;
	or.b32  	%r1272, %r210, %r1271;
	or.b32  	%r287, %r1272, %r286;
	setp.lt.s32 	%p246, %r287, 0;
	or.pred  	%p247, %p246, %p245;
	or.pred  	%p248, %p244, %p247;
	@%p248 bra 	$L__BB13_185;
	bra.uni 	$L__BB13_184;

$L__BB13_185:
	add.s32 	%r2116, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2116, %r210};
	add.s32 	%r2117, %r208, 8;
	st.local.v2.u32 	[%rd30+8], {%r2117, %r1269};
	st.local.v2.u32 	[%rd30+16], {%r1270, %r282};
	mov.u64 	%rd1268, $str$2;
	cvta.global.u64 	%rd1269, %rd1268;
	{ // callseq 377, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1269;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1275, [retval0+0];
	} // callseq 377
	bra.uni 	$L__BB13_186;

$L__BB13_184:
	ld.param.u32 	%r2766, [%rd357+88];
	ld.param.u64 	%rd3994, [%rd357];
	ld.param.u32 	%r2765, [%rd357+32];
	ld.param.u64 	%rd3993, [%rd357+112];
	ld.param.u32 	%r2764, [%rd357+144];
	ld.param.u64 	%rd3992, [%rd357+56];
	add.s32 	%r2642, %r208, 8;
	cvta.to.global.u64 	%rd1261, %rd3992;
	mul.wide.s32 	%rd1262, %r2764, %r2642;
	add.s64 	%rd1252, %rd3993, %rd1262;
	// begin inline asm
	{ atom.add.f64 %fd3313,[%rd1252],%fd144; }

	// end inline asm
	add.s64 	%rd1253, %rd1252, 8;
	// begin inline asm
	{ atom.add.f64 %fd3315,[%rd1253],%fd143; }

	// end inline asm
	add.s64 	%rd1254, %rd1252, 16;
	// begin inline asm
	{ atom.add.f64 %fd3317,[%rd1254],%fd142; }

	// end inline asm
	add.s64 	%rd1255, %rd1252, 24;
	// begin inline asm
	{ atom.add.f64 %fd3319,[%rd1255],%fd120; }

	// end inline asm
	add.s64 	%rd1256, %rd1252, 32;
	// begin inline asm
	{ atom.add.f64 %fd3321,[%rd1256],%fd119; }

	// end inline asm
	add.s64 	%rd1257, %rd1252, 40;
	// begin inline asm
	{ atom.add.f64 %fd3323,[%rd1257],%fd118; }

	// end inline asm
	add.s64 	%rd1258, %rd1252, 48;
	// begin inline asm
	{ atom.add.f64 %fd3325,[%rd1258],%fd96; }

	// end inline asm
	add.s64 	%rd1259, %rd1252, 56;
	// begin inline asm
	{ atom.add.f64 %fd3327,[%rd1259],%fd95; }

	// end inline asm
	add.s64 	%rd1260, %rd1252, 64;
	// begin inline asm
	{ atom.add.f64 %fd3329,[%rd1260],%fd94; }

	// end inline asm
	mul.wide.s32 	%rd1263, %r2765, %r2642;
	cvta.to.global.u64 	%rd1264, %rd3994;
	add.s64 	%rd1265, %rd1264, %rd1263;
	mul.wide.s32 	%rd1266, %r2766, %r2642;
	add.s64 	%rd1267, %rd1261, %rd1266;
	add.s32 	%r2115, %r209, 2;
	st.global.u32 	[%rd1265], %r2115;
	st.global.u32 	[%rd1267], %r210;

$L__BB13_186:
	ld.param.u32 	%r291, [%rd357+172];
	ld.param.v2.u32 	{%r1276, %r1277}, [%rd357+176];
	setp.le.s32 	%p249, %r1276, %r286;
	setp.le.s32 	%p250, %r1277, %r226;
	add.s32 	%r1278, %r208, 9;
	setp.le.s32 	%p251, %r291, %r1278;
	or.pred  	%p252, %p249, %p250;
	or.b32  	%r1279, %r286, %r1278;
	or.b32  	%r295, %r1279, %r226;
	setp.lt.s32 	%p253, %r295, 0;
	or.pred  	%p254, %p253, %p252;
	or.pred  	%p255, %p251, %p254;
	@%p255 bra 	$L__BB13_188;
	bra.uni 	$L__BB13_187;

$L__BB13_188:
	add.s32 	%r2120, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2120, %r226};
	add.s32 	%r2121, %r208, 9;
	st.local.v2.u32 	[%rd30+8], {%r2121, %r1276};
	st.local.v2.u32 	[%rd30+16], {%r1277, %r291};
	mov.u64 	%rd1287, $str$2;
	cvta.global.u64 	%rd1288, %rd1287;
	{ // callseq 378, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1288;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1282, [retval0+0];
	} // callseq 378
	bra.uni 	$L__BB13_189;

$L__BB13_187:
	ld.param.u32 	%r2748, [%rd357+88];
	ld.param.u64 	%rd3976, [%rd357];
	ld.param.u32 	%r2747, [%rd357+32];
	ld.param.u64 	%rd3975, [%rd357+112];
	ld.param.u32 	%r2746, [%rd357+144];
	ld.param.u64 	%rd3974, [%rd357+56];
	add.s32 	%r2643, %r208, 9;
	cvta.to.global.u64 	%rd1280, %rd3974;
	mul.wide.s32 	%rd1281, %r2746, %r2643;
	add.s64 	%rd1271, %rd3975, %rd1281;
	// begin inline asm
	{ atom.add.f64 %fd3331,[%rd1271],%fd141; }

	// end inline asm
	add.s64 	%rd1272, %rd1271, 8;
	// begin inline asm
	{ atom.add.f64 %fd3333,[%rd1272],%fd140; }

	// end inline asm
	add.s64 	%rd1273, %rd1271, 16;
	// begin inline asm
	{ atom.add.f64 %fd3335,[%rd1273],%fd139; }

	// end inline asm
	add.s64 	%rd1274, %rd1271, 24;
	// begin inline asm
	{ atom.add.f64 %fd3337,[%rd1274],%fd117; }

	// end inline asm
	add.s64 	%rd1275, %rd1271, 32;
	// begin inline asm
	{ atom.add.f64 %fd3339,[%rd1275],%fd116; }

	// end inline asm
	add.s64 	%rd1276, %rd1271, 40;
	// begin inline asm
	{ atom.add.f64 %fd3341,[%rd1276],%fd115; }

	// end inline asm
	add.s64 	%rd1277, %rd1271, 48;
	// begin inline asm
	{ atom.add.f64 %fd3343,[%rd1277],%fd93; }

	// end inline asm
	add.s64 	%rd1278, %rd1271, 56;
	// begin inline asm
	{ atom.add.f64 %fd3345,[%rd1278],%fd92; }

	// end inline asm
	add.s64 	%rd1279, %rd1271, 64;
	// begin inline asm
	{ atom.add.f64 %fd3347,[%rd1279],%fd91; }

	// end inline asm
	mul.wide.s32 	%rd1282, %r2747, %r2643;
	cvta.to.global.u64 	%rd1283, %rd3976;
	add.s64 	%rd1284, %rd1283, %rd1282;
	mul.wide.s32 	%rd1285, %r2748, %r2643;
	add.s64 	%rd1286, %rd1280, %rd1285;
	add.s32 	%r2118, %r209, 2;
	st.global.u32 	[%rd1284], %r2118;
	add.s32 	%r2119, %r210, 1;
	st.global.u32 	[%rd1286], %r2119;

$L__BB13_189:
	ld.param.u32 	%r299, [%rd357+172];
	ld.param.v2.u32 	{%r1283, %r1284}, [%rd357+176];
	setp.le.s32 	%p256, %r1283, %r286;
	setp.le.s32 	%p257, %r1284, %r235;
	add.s32 	%r1285, %r208, 10;
	setp.le.s32 	%p258, %r299, %r1285;
	or.pred  	%p259, %p256, %p257;
	or.b32  	%r1286, %r286, %r1285;
	or.b32  	%r303, %r1286, %r235;
	setp.lt.s32 	%p260, %r303, 0;
	or.pred  	%p261, %p260, %p259;
	or.pred  	%p262, %p258, %p261;
	@%p262 bra 	$L__BB13_191;
	bra.uni 	$L__BB13_190;

$L__BB13_191:
	add.s32 	%r2124, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2124, %r235};
	add.s32 	%r2125, %r208, 10;
	st.local.v2.u32 	[%rd30+8], {%r2125, %r1283};
	st.local.v2.u32 	[%rd30+16], {%r1284, %r299};
	mov.u64 	%rd1306, $str$2;
	cvta.global.u64 	%rd1307, %rd1306;
	{ // callseq 379, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1307;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1289, [retval0+0];
	} // callseq 379
	bra.uni 	$L__BB13_192;

$L__BB13_190:
	ld.param.u32 	%r2751, [%rd357+88];
	ld.param.u64 	%rd3979, [%rd357];
	ld.param.u32 	%r2750, [%rd357+32];
	ld.param.u64 	%rd3978, [%rd357+112];
	ld.param.u32 	%r2749, [%rd357+144];
	ld.param.u64 	%rd3977, [%rd357+56];
	add.s32 	%r2644, %r208, 10;
	cvta.to.global.u64 	%rd1299, %rd3977;
	mul.wide.s32 	%rd1300, %r2749, %r2644;
	add.s64 	%rd1290, %rd3978, %rd1300;
	// begin inline asm
	{ atom.add.f64 %fd3349,[%rd1290],%fd138; }

	// end inline asm
	add.s64 	%rd1291, %rd1290, 8;
	// begin inline asm
	{ atom.add.f64 %fd3351,[%rd1291],%fd137; }

	// end inline asm
	add.s64 	%rd1292, %rd1290, 16;
	// begin inline asm
	{ atom.add.f64 %fd3353,[%rd1292],%fd136; }

	// end inline asm
	add.s64 	%rd1293, %rd1290, 24;
	// begin inline asm
	{ atom.add.f64 %fd3355,[%rd1293],%fd114; }

	// end inline asm
	add.s64 	%rd1294, %rd1290, 32;
	// begin inline asm
	{ atom.add.f64 %fd3357,[%rd1294],%fd113; }

	// end inline asm
	add.s64 	%rd1295, %rd1290, 40;
	// begin inline asm
	{ atom.add.f64 %fd3359,[%rd1295],%fd112; }

	// end inline asm
	add.s64 	%rd1296, %rd1290, 48;
	// begin inline asm
	{ atom.add.f64 %fd3361,[%rd1296],%fd90; }

	// end inline asm
	add.s64 	%rd1297, %rd1290, 56;
	// begin inline asm
	{ atom.add.f64 %fd3363,[%rd1297],%fd89; }

	// end inline asm
	add.s64 	%rd1298, %rd1290, 64;
	// begin inline asm
	{ atom.add.f64 %fd3365,[%rd1298],%fd88; }

	// end inline asm
	mul.wide.s32 	%rd1301, %r2750, %r2644;
	cvta.to.global.u64 	%rd1302, %rd3979;
	add.s64 	%rd1303, %rd1302, %rd1301;
	mul.wide.s32 	%rd1304, %r2751, %r2644;
	add.s64 	%rd1305, %rd1299, %rd1304;
	add.s32 	%r2122, %r209, 2;
	st.global.u32 	[%rd1303], %r2122;
	add.s32 	%r2123, %r210, 2;
	st.global.u32 	[%rd1305], %r2123;

$L__BB13_192:
	ld.param.u32 	%r307, [%rd357+172];
	ld.param.v2.u32 	{%r1290, %r1291}, [%rd357+176];
	setp.le.s32 	%p263, %r1290, %r286;
	setp.le.s32 	%p264, %r1291, %r244;
	add.s32 	%r1292, %r208, 11;
	setp.le.s32 	%p265, %r307, %r1292;
	or.pred  	%p266, %p263, %p264;
	or.b32  	%r1293, %r286, %r1292;
	or.b32  	%r311, %r1293, %r244;
	setp.lt.s32 	%p267, %r311, 0;
	or.pred  	%p268, %p267, %p266;
	or.pred  	%p269, %p265, %p268;
	@%p269 bra 	$L__BB13_194;
	bra.uni 	$L__BB13_193;

$L__BB13_194:
	add.s32 	%r2128, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2128, %r244};
	add.s32 	%r2129, %r208, 11;
	st.local.v2.u32 	[%rd30+8], {%r2129, %r1290};
	st.local.v2.u32 	[%rd30+16], {%r1291, %r307};
	mov.u64 	%rd1325, $str$2;
	cvta.global.u64 	%rd1326, %rd1325;
	{ // callseq 380, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1326;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1296, [retval0+0];
	} // callseq 380
	bra.uni 	$L__BB13_195;

$L__BB13_193:
	ld.param.u32 	%r2754, [%rd357+88];
	ld.param.u64 	%rd3982, [%rd357];
	ld.param.u32 	%r2753, [%rd357+32];
	ld.param.u64 	%rd3981, [%rd357+112];
	ld.param.u32 	%r2752, [%rd357+144];
	ld.param.u64 	%rd3980, [%rd357+56];
	add.s32 	%r2645, %r208, 11;
	cvta.to.global.u64 	%rd1318, %rd3980;
	mul.wide.s32 	%rd1319, %r2752, %r2645;
	add.s64 	%rd1309, %rd3981, %rd1319;
	// begin inline asm
	{ atom.add.f64 %fd3367,[%rd1309],%fd135; }

	// end inline asm
	add.s64 	%rd1310, %rd1309, 8;
	// begin inline asm
	{ atom.add.f64 %fd3369,[%rd1310],%fd134; }

	// end inline asm
	add.s64 	%rd1311, %rd1309, 16;
	// begin inline asm
	{ atom.add.f64 %fd3371,[%rd1311],%fd133; }

	// end inline asm
	add.s64 	%rd1312, %rd1309, 24;
	// begin inline asm
	{ atom.add.f64 %fd3373,[%rd1312],%fd111; }

	// end inline asm
	add.s64 	%rd1313, %rd1309, 32;
	// begin inline asm
	{ atom.add.f64 %fd3375,[%rd1313],%fd110; }

	// end inline asm
	add.s64 	%rd1314, %rd1309, 40;
	// begin inline asm
	{ atom.add.f64 %fd3377,[%rd1314],%fd109; }

	// end inline asm
	add.s64 	%rd1315, %rd1309, 48;
	// begin inline asm
	{ atom.add.f64 %fd3379,[%rd1315],%fd87; }

	// end inline asm
	add.s64 	%rd1316, %rd1309, 56;
	// begin inline asm
	{ atom.add.f64 %fd3381,[%rd1316],%fd86; }

	// end inline asm
	add.s64 	%rd1317, %rd1309, 64;
	// begin inline asm
	{ atom.add.f64 %fd3383,[%rd1317],%fd85; }

	// end inline asm
	mul.wide.s32 	%rd1320, %r2753, %r2645;
	cvta.to.global.u64 	%rd1321, %rd3982;
	add.s64 	%rd1322, %rd1321, %rd1320;
	mul.wide.s32 	%rd1323, %r2754, %r2645;
	add.s64 	%rd1324, %rd1318, %rd1323;
	add.s32 	%r2126, %r209, 2;
	st.global.u32 	[%rd1322], %r2126;
	add.s32 	%r2127, %r210, 3;
	st.global.u32 	[%rd1324], %r2127;

$L__BB13_195:
	ld.param.u32 	%r315, [%rd357+172];
	ld.param.v2.u32 	{%r1297, %r1298}, [%rd357+176];
	add.s32 	%r319, %r209, 3;
	setp.le.s32 	%p270, %r1297, %r319;
	setp.le.s32 	%p271, %r1298, %r210;
	add.s32 	%r1299, %r208, 12;
	setp.le.s32 	%p272, %r315, %r1299;
	or.pred  	%p273, %p270, %p271;
	or.b32  	%r1300, %r210, %r1299;
	or.b32  	%r320, %r1300, %r319;
	setp.lt.s32 	%p274, %r320, 0;
	or.pred  	%p275, %p274, %p273;
	or.pred  	%p276, %p272, %p275;
	@%p276 bra 	$L__BB13_197;
	bra.uni 	$L__BB13_196;

$L__BB13_197:
	add.s32 	%r2131, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2131, %r210};
	add.s32 	%r2132, %r208, 12;
	st.local.v2.u32 	[%rd30+8], {%r2132, %r1297};
	st.local.v2.u32 	[%rd30+16], {%r1298, %r315};
	mov.u64 	%rd1344, $str$2;
	cvta.global.u64 	%rd1345, %rd1344;
	{ // callseq 381, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1345;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1303, [retval0+0];
	} // callseq 381
	bra.uni 	$L__BB13_198;

$L__BB13_196:
	ld.param.u32 	%r2757, [%rd357+88];
	ld.param.u64 	%rd3985, [%rd357];
	ld.param.u32 	%r2756, [%rd357+32];
	ld.param.u64 	%rd3984, [%rd357+112];
	ld.param.u32 	%r2755, [%rd357+144];
	ld.param.u64 	%rd3983, [%rd357+56];
	add.s32 	%r2646, %r208, 12;
	cvta.to.global.u64 	%rd1337, %rd3983;
	mul.wide.s32 	%rd1338, %r2755, %r2646;
	add.s64 	%rd1328, %rd3984, %rd1338;
	// begin inline asm
	{ atom.add.f64 %fd3385,[%rd1328],%fd72; }

	// end inline asm
	add.s64 	%rd1329, %rd1328, 8;
	// begin inline asm
	{ atom.add.f64 %fd3387,[%rd1329],%fd71; }

	// end inline asm
	add.s64 	%rd1330, %rd1328, 16;
	// begin inline asm
	{ atom.add.f64 %fd3389,[%rd1330],%fd70; }

	// end inline asm
	add.s64 	%rd1331, %rd1328, 24;
	// begin inline asm
	{ atom.add.f64 %fd3391,[%rd1331],%fd48; }

	// end inline asm
	add.s64 	%rd1332, %rd1328, 32;
	// begin inline asm
	{ atom.add.f64 %fd3393,[%rd1332],%fd47; }

	// end inline asm
	add.s64 	%rd1333, %rd1328, 40;
	// begin inline asm
	{ atom.add.f64 %fd3395,[%rd1333],%fd46; }

	// end inline asm
	add.s64 	%rd1334, %rd1328, 48;
	// begin inline asm
	{ atom.add.f64 %fd3397,[%rd1334],%fd24; }

	// end inline asm
	add.s64 	%rd1335, %rd1328, 56;
	// begin inline asm
	{ atom.add.f64 %fd3399,[%rd1335],%fd23; }

	// end inline asm
	add.s64 	%rd1336, %rd1328, 64;
	// begin inline asm
	{ atom.add.f64 %fd3401,[%rd1336],%fd22; }

	// end inline asm
	mul.wide.s32 	%rd1339, %r2756, %r2646;
	cvta.to.global.u64 	%rd1340, %rd3985;
	add.s64 	%rd1341, %rd1340, %rd1339;
	mul.wide.s32 	%rd1342, %r2757, %r2646;
	add.s64 	%rd1343, %rd1337, %rd1342;
	add.s32 	%r2130, %r209, 3;
	st.global.u32 	[%rd1341], %r2130;
	st.global.u32 	[%rd1343], %r210;

$L__BB13_198:
	ld.param.u32 	%r324, [%rd357+172];
	ld.param.v2.u32 	{%r1304, %r1305}, [%rd357+176];
	setp.le.s32 	%p277, %r1304, %r319;
	setp.le.s32 	%p278, %r1305, %r226;
	add.s32 	%r1306, %r208, 13;
	setp.le.s32 	%p279, %r324, %r1306;
	or.pred  	%p280, %p277, %p278;
	or.b32  	%r1307, %r319, %r1306;
	or.b32  	%r328, %r1307, %r226;
	setp.lt.s32 	%p281, %r328, 0;
	or.pred  	%p282, %p281, %p280;
	or.pred  	%p283, %p279, %p282;
	@%p283 bra 	$L__BB13_200;
	bra.uni 	$L__BB13_199;

$L__BB13_200:
	add.s32 	%r2135, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2135, %r226};
	add.s32 	%r2136, %r208, 13;
	st.local.v2.u32 	[%rd30+8], {%r2136, %r1304};
	st.local.v2.u32 	[%rd30+16], {%r1305, %r324};
	mov.u64 	%rd1363, $str$2;
	cvta.global.u64 	%rd1364, %rd1363;
	{ // callseq 382, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1364;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1310, [retval0+0];
	} // callseq 382
	bra.uni 	$L__BB13_201;

$L__BB13_199:
	ld.param.u32 	%r2760, [%rd357+88];
	ld.param.u64 	%rd3988, [%rd357];
	ld.param.u32 	%r2759, [%rd357+32];
	ld.param.u64 	%rd3987, [%rd357+112];
	ld.param.u32 	%r2758, [%rd357+144];
	ld.param.u64 	%rd3986, [%rd357+56];
	add.s32 	%r2647, %r208, 13;
	cvta.to.global.u64 	%rd1356, %rd3986;
	mul.wide.s32 	%rd1357, %r2758, %r2647;
	add.s64 	%rd1347, %rd3987, %rd1357;
	// begin inline asm
	{ atom.add.f64 %fd3403,[%rd1347],%fd69; }

	// end inline asm
	add.s64 	%rd1348, %rd1347, 8;
	// begin inline asm
	{ atom.add.f64 %fd3405,[%rd1348],%fd68; }

	// end inline asm
	add.s64 	%rd1349, %rd1347, 16;
	// begin inline asm
	{ atom.add.f64 %fd3407,[%rd1349],%fd67; }

	// end inline asm
	add.s64 	%rd1350, %rd1347, 24;
	// begin inline asm
	{ atom.add.f64 %fd3409,[%rd1350],%fd45; }

	// end inline asm
	add.s64 	%rd1351, %rd1347, 32;
	// begin inline asm
	{ atom.add.f64 %fd3411,[%rd1351],%fd44; }

	// end inline asm
	add.s64 	%rd1352, %rd1347, 40;
	// begin inline asm
	{ atom.add.f64 %fd3413,[%rd1352],%fd43; }

	// end inline asm
	add.s64 	%rd1353, %rd1347, 48;
	// begin inline asm
	{ atom.add.f64 %fd3415,[%rd1353],%fd21; }

	// end inline asm
	add.s64 	%rd1354, %rd1347, 56;
	// begin inline asm
	{ atom.add.f64 %fd3417,[%rd1354],%fd20; }

	// end inline asm
	add.s64 	%rd1355, %rd1347, 64;
	// begin inline asm
	{ atom.add.f64 %fd3419,[%rd1355],%fd19; }

	// end inline asm
	mul.wide.s32 	%rd1358, %r2759, %r2647;
	cvta.to.global.u64 	%rd1359, %rd3988;
	add.s64 	%rd1360, %rd1359, %rd1358;
	mul.wide.s32 	%rd1361, %r2760, %r2647;
	add.s64 	%rd1362, %rd1356, %rd1361;
	add.s32 	%r2133, %r209, 3;
	st.global.u32 	[%rd1360], %r2133;
	add.s32 	%r2134, %r210, 1;
	st.global.u32 	[%rd1362], %r2134;

$L__BB13_201:
	ld.param.u32 	%r332, [%rd357+172];
	ld.param.v2.u32 	{%r1311, %r1312}, [%rd357+176];
	setp.le.s32 	%p284, %r1311, %r319;
	setp.le.s32 	%p285, %r1312, %r235;
	add.s32 	%r1313, %r208, 14;
	setp.le.s32 	%p286, %r332, %r1313;
	or.pred  	%p287, %p284, %p285;
	or.b32  	%r1314, %r319, %r1313;
	or.b32  	%r336, %r1314, %r235;
	setp.lt.s32 	%p288, %r336, 0;
	or.pred  	%p289, %p288, %p287;
	or.pred  	%p290, %p286, %p289;
	@%p290 bra 	$L__BB13_203;
	bra.uni 	$L__BB13_202;

$L__BB13_203:
	add.s32 	%r2139, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2139, %r235};
	add.s32 	%r2140, %r208, 14;
	st.local.v2.u32 	[%rd30+8], {%r2140, %r1311};
	st.local.v2.u32 	[%rd30+16], {%r1312, %r332};
	mov.u64 	%rd1382, $str$2;
	cvta.global.u64 	%rd1383, %rd1382;
	{ // callseq 383, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1383;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1317, [retval0+0];
	} // callseq 383
	bra.uni 	$L__BB13_204;

$L__BB13_202:
	ld.param.u32 	%r2763, [%rd357+88];
	ld.param.u64 	%rd3991, [%rd357];
	ld.param.u32 	%r2762, [%rd357+32];
	ld.param.u64 	%rd3990, [%rd357+112];
	ld.param.u32 	%r2761, [%rd357+144];
	ld.param.u64 	%rd3989, [%rd357+56];
	add.s32 	%r2648, %r208, 14;
	cvta.to.global.u64 	%rd1375, %rd3989;
	mul.wide.s32 	%rd1376, %r2761, %r2648;
	add.s64 	%rd1366, %rd3990, %rd1376;
	// begin inline asm
	{ atom.add.f64 %fd3421,[%rd1366],%fd66; }

	// end inline asm
	add.s64 	%rd1367, %rd1366, 8;
	// begin inline asm
	{ atom.add.f64 %fd3423,[%rd1367],%fd65; }

	// end inline asm
	add.s64 	%rd1368, %rd1366, 16;
	// begin inline asm
	{ atom.add.f64 %fd3425,[%rd1368],%fd64; }

	// end inline asm
	add.s64 	%rd1369, %rd1366, 24;
	// begin inline asm
	{ atom.add.f64 %fd3427,[%rd1369],%fd42; }

	// end inline asm
	add.s64 	%rd1370, %rd1366, 32;
	// begin inline asm
	{ atom.add.f64 %fd3429,[%rd1370],%fd41; }

	// end inline asm
	add.s64 	%rd1371, %rd1366, 40;
	// begin inline asm
	{ atom.add.f64 %fd3431,[%rd1371],%fd40; }

	// end inline asm
	add.s64 	%rd1372, %rd1366, 48;
	// begin inline asm
	{ atom.add.f64 %fd3433,[%rd1372],%fd18; }

	// end inline asm
	add.s64 	%rd1373, %rd1366, 56;
	// begin inline asm
	{ atom.add.f64 %fd3435,[%rd1373],%fd17; }

	// end inline asm
	add.s64 	%rd1374, %rd1366, 64;
	// begin inline asm
	{ atom.add.f64 %fd3437,[%rd1374],%fd16; }

	// end inline asm
	mul.wide.s32 	%rd1377, %r2762, %r2648;
	cvta.to.global.u64 	%rd1378, %rd3991;
	add.s64 	%rd1379, %rd1378, %rd1377;
	mul.wide.s32 	%rd1380, %r2763, %r2648;
	add.s64 	%rd1381, %rd1375, %rd1380;
	add.s32 	%r2137, %r209, 3;
	st.global.u32 	[%rd1379], %r2137;
	add.s32 	%r2138, %r210, 2;
	st.global.u32 	[%rd1381], %r2138;

$L__BB13_204:
	ld.param.u32 	%r340, [%rd357+172];
	ld.param.v2.u32 	{%r1318, %r1319}, [%rd357+176];
	setp.le.s32 	%p291, %r1318, %r319;
	setp.le.s32 	%p292, %r1319, %r244;
	add.s32 	%r1320, %r208, 15;
	setp.le.s32 	%p293, %r340, %r1320;
	or.pred  	%p294, %p291, %p292;
	or.b32  	%r1321, %r319, %r1320;
	or.b32  	%r344, %r1321, %r244;
	setp.lt.s32 	%p295, %r344, 0;
	or.pred  	%p296, %p295, %p294;
	or.pred  	%p297, %p293, %p296;
	@%p297 bra 	$L__BB13_206;
	bra.uni 	$L__BB13_205;

$L__BB13_206:
	add.s32 	%r2143, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2143, %r244};
	add.s32 	%r2144, %r208, 15;
	st.local.v2.u32 	[%rd30+8], {%r2144, %r1318};
	st.local.v2.u32 	[%rd30+16], {%r1319, %r340};
	mov.u64 	%rd1401, $str$2;
	cvta.global.u64 	%rd1402, %rd1401;
	{ // callseq 384, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1402;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1324, [retval0+0];
	} // callseq 384
	bra.uni 	$L__BB13_207;

$L__BB13_205:
	add.s32 	%r2376, %r208, 15;
	ld.param.u32 	%r2375, [%rd357+88];
	ld.param.u64 	%rd3968, [%rd357];
	ld.param.u32 	%r2374, [%rd357+32];
	ld.param.u64 	%rd3967, [%rd357+112];
	ld.param.u32 	%r2373, [%rd357+144];
	ld.param.u64 	%rd3966, [%rd357+56];
	cvta.to.global.u64 	%rd1394, %rd3966;
	mul.wide.s32 	%rd1395, %r2373, %r2376;
	add.s64 	%rd1385, %rd3967, %rd1395;
	// begin inline asm
	{ atom.add.f64 %fd3439,[%rd1385],%fd63; }

	// end inline asm
	add.s64 	%rd1386, %rd1385, 8;
	// begin inline asm
	{ atom.add.f64 %fd3441,[%rd1386],%fd62; }

	// end inline asm
	add.s64 	%rd1387, %rd1385, 16;
	// begin inline asm
	{ atom.add.f64 %fd3443,[%rd1387],%fd61; }

	// end inline asm
	add.s64 	%rd1388, %rd1385, 24;
	// begin inline asm
	{ atom.add.f64 %fd3445,[%rd1388],%fd39; }

	// end inline asm
	add.s64 	%rd1389, %rd1385, 32;
	// begin inline asm
	{ atom.add.f64 %fd3447,[%rd1389],%fd38; }

	// end inline asm
	add.s64 	%rd1390, %rd1385, 40;
	// begin inline asm
	{ atom.add.f64 %fd3449,[%rd1390],%fd37; }

	// end inline asm
	add.s64 	%rd1391, %rd1385, 48;
	// begin inline asm
	{ atom.add.f64 %fd3451,[%rd1391],%fd15; }

	// end inline asm
	add.s64 	%rd1392, %rd1385, 56;
	// begin inline asm
	{ atom.add.f64 %fd3453,[%rd1392],%fd14; }

	// end inline asm
	add.s64 	%rd1393, %rd1385, 64;
	// begin inline asm
	{ atom.add.f64 %fd3455,[%rd1393],%fd13; }

	// end inline asm
	mul.wide.s32 	%rd1396, %r2374, %r2376;
	cvta.to.global.u64 	%rd1397, %rd3968;
	add.s64 	%rd1398, %rd1397, %rd1396;
	mul.wide.s32 	%rd1399, %r2375, %r2376;
	add.s64 	%rd1400, %rd1394, %rd1399;
	add.s32 	%r2141, %r209, 3;
	st.global.u32 	[%rd1398], %r2141;
	add.s32 	%r2142, %r210, 3;
	st.global.u32 	[%rd1400], %r2142;

$L__BB13_207:
	ld.param.u32 	%r348, [%rd357+172];
	ld.param.v2.u32 	{%r1325, %r1326}, [%rd357+176];
	setp.le.s32 	%p299, %r1325, %r209;
	setp.le.s32 	%p300, %r1326, %r210;
	setp.le.s32 	%p301, %r348, %r208;
	or.pred  	%p302, %p299, %p300;
	or.pred  	%p303, %p190, %p302;
	or.pred  	%p304, %p301, %p303;
	@%p304 bra 	$L__BB13_209;
	bra.uni 	$L__BB13_208;

$L__BB13_209:
	st.local.v2.u32 	[%rd30], {%r209, %r210};
	st.local.v2.u32 	[%rd30+8], {%r208, %r1325};
	st.local.v2.u32 	[%rd30+16], {%r1326, %r348};
	mov.u64 	%rd1420, $str$2;
	cvta.global.u64 	%rd1421, %rd1420;
	{ // callseq 385, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1421;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1327, [retval0+0];
	} // callseq 385
	bra.uni 	$L__BB13_210;

$L__BB13_208:
	ld.param.u32 	%r2741, [%rd357+88];
	ld.param.u64 	%rd3973, [%rd357];
	ld.param.u32 	%r2740, [%rd357+32];
	ld.param.u64 	%rd3972, [%rd357+112];
	ld.param.u32 	%r2739, [%rd357+144];
	ld.param.u64 	%rd3971, [%rd357+56];
	cvta.to.global.u64 	%rd1413, %rd3971;
	mul.wide.s32 	%rd1414, %r2739, %r208;
	add.s64 	%rd1404, %rd3972, %rd1414;
	// begin inline asm
	{ atom.add.f64 %fd3457,[%rd1404],%fd288; }

	// end inline asm
	add.s64 	%rd1405, %rd1404, 8;
	// begin inline asm
	{ atom.add.f64 %fd3459,[%rd1405],%fd287; }

	// end inline asm
	add.s64 	%rd1406, %rd1404, 16;
	// begin inline asm
	{ atom.add.f64 %fd3461,[%rd1406],%fd286; }

	// end inline asm
	add.s64 	%rd1407, %rd1404, 24;
	// begin inline asm
	{ atom.add.f64 %fd3463,[%rd1407],%fd264; }

	// end inline asm
	add.s64 	%rd1408, %rd1404, 32;
	// begin inline asm
	{ atom.add.f64 %fd3465,[%rd1408],%fd263; }

	// end inline asm
	add.s64 	%rd1409, %rd1404, 40;
	// begin inline asm
	{ atom.add.f64 %fd3467,[%rd1409],%fd262; }

	// end inline asm
	add.s64 	%rd1410, %rd1404, 48;
	// begin inline asm
	{ atom.add.f64 %fd3469,[%rd1410],%fd240; }

	// end inline asm
	add.s64 	%rd1411, %rd1404, 56;
	// begin inline asm
	{ atom.add.f64 %fd3471,[%rd1411],%fd239; }

	// end inline asm
	add.s64 	%rd1412, %rd1404, 64;
	// begin inline asm
	{ atom.add.f64 %fd3473,[%rd1412],%fd238; }

	// end inline asm
	mul.wide.s32 	%rd1415, %r2740, %r208;
	cvta.to.global.u64 	%rd1416, %rd3973;
	add.s64 	%rd1417, %rd1416, %rd1415;
	mul.wide.s32 	%rd1418, %r2741, %r208;
	add.s64 	%rd1419, %rd1413, %rd1418;
	st.global.u32 	[%rd1417], %r209;
	st.global.u32 	[%rd1419], %r210;

$L__BB13_210:
	add.s32 	%r2377, %r208, 1;
	ld.param.u32 	%r355, [%rd357+172];
	ld.param.v2.u32 	{%r1328, %r1329}, [%rd357+176];
	setp.le.s32 	%p306, %r1328, %r209;
	setp.le.s32 	%p307, %r1329, %r226;
	setp.le.s32 	%p308, %r355, %r2377;
	or.pred  	%p309, %p306, %p307;
	or.pred  	%p310, %p197, %p309;
	or.pred  	%p311, %p308, %p310;
	@%p311 bra 	$L__BB13_212;
	bra.uni 	$L__BB13_211;

$L__BB13_212:
	add.s32 	%r2146, %r210, 1;
	st.local.v2.u32 	[%rd30], {%r209, %r2146};
	add.s32 	%r2147, %r208, 1;
	st.local.v2.u32 	[%rd30+8], {%r2147, %r1328};
	st.local.v2.u32 	[%rd30+16], {%r1329, %r355};
	mov.u64 	%rd1439, $str$2;
	cvta.global.u64 	%rd1440, %rd1439;
	{ // callseq 386, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1440;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1333, [retval0+0];
	} // callseq 386
	bra.uni 	$L__BB13_213;

$L__BB13_211:
	ld.param.u32 	%r2802, [%rd357+88];
	ld.param.u64 	%rd4030, [%rd357];
	ld.param.u32 	%r2801, [%rd357+32];
	ld.param.u64 	%rd4029, [%rd357+112];
	ld.param.u32 	%r2800, [%rd357+144];
	ld.param.u64 	%rd4028, [%rd357+56];
	add.s32 	%r2378, %r208, 1;
	cvta.to.global.u64 	%rd1432, %rd4028;
	mul.wide.s32 	%rd1433, %r2800, %r2378;
	add.s64 	%rd1423, %rd4029, %rd1433;
	// begin inline asm
	{ atom.add.f64 %fd3475,[%rd1423],%fd285; }

	// end inline asm
	add.s64 	%rd1424, %rd1423, 8;
	// begin inline asm
	{ atom.add.f64 %fd3477,[%rd1424],%fd284; }

	// end inline asm
	add.s64 	%rd1425, %rd1423, 16;
	// begin inline asm
	{ atom.add.f64 %fd3479,[%rd1425],%fd283; }

	// end inline asm
	add.s64 	%rd1426, %rd1423, 24;
	// begin inline asm
	{ atom.add.f64 %fd3481,[%rd1426],%fd261; }

	// end inline asm
	add.s64 	%rd1427, %rd1423, 32;
	// begin inline asm
	{ atom.add.f64 %fd3483,[%rd1427],%fd260; }

	// end inline asm
	add.s64 	%rd1428, %rd1423, 40;
	// begin inline asm
	{ atom.add.f64 %fd3485,[%rd1428],%fd259; }

	// end inline asm
	add.s64 	%rd1429, %rd1423, 48;
	// begin inline asm
	{ atom.add.f64 %fd3487,[%rd1429],%fd237; }

	// end inline asm
	add.s64 	%rd1430, %rd1423, 56;
	// begin inline asm
	{ atom.add.f64 %fd3489,[%rd1430],%fd236; }

	// end inline asm
	add.s64 	%rd1431, %rd1423, 64;
	// begin inline asm
	{ atom.add.f64 %fd3491,[%rd1431],%fd235; }

	// end inline asm
	mul.wide.s32 	%rd1434, %r2801, %r2378;
	cvta.to.global.u64 	%rd1435, %rd4030;
	add.s64 	%rd1436, %rd1435, %rd1434;
	mul.wide.s32 	%rd1437, %r2802, %r2378;
	add.s64 	%rd1438, %rd1432, %rd1437;
	st.global.u32 	[%rd1436], %r209;
	add.s32 	%r2145, %r210, 1;
	st.global.u32 	[%rd1438], %r2145;

$L__BB13_213:
	add.s32 	%r2379, %r208, 2;
	ld.param.u64 	%rd167, [%rd357];
	ld.param.u32 	%r359, [%rd357+32];
	ld.param.u64 	%rd168, [%rd357+56];
	ld.param.u32 	%r360, [%rd357+88];
	ld.param.u64 	%rd169, [%rd357+112];
	ld.param.u32 	%r361, [%rd357+144];
	ld.param.u32 	%r362, [%rd357+172];
	ld.param.v2.u32 	{%r1334, %r1335}, [%rd357+176];
	setp.le.s32 	%p313, %r1334, %r209;
	setp.le.s32 	%p314, %r1335, %r235;
	setp.le.s32 	%p315, %r362, %r2379;
	or.pred  	%p316, %p313, %p314;
	or.pred  	%p317, %p204, %p316;
	or.pred  	%p318, %p315, %p317;
	@%p318 bra 	$L__BB13_215;
	bra.uni 	$L__BB13_214;

$L__BB13_215:
	add.s32 	%r2149, %r210, 2;
	st.local.v2.u32 	[%rd30], {%r209, %r2149};
	add.s32 	%r2150, %r208, 2;
	st.local.v2.u32 	[%rd30+8], {%r2150, %r1334};
	st.local.v2.u32 	[%rd30+16], {%r1335, %r362};
	mov.u64 	%rd1458, $str$2;
	cvta.global.u64 	%rd1459, %rd1458;
	{ // callseq 387, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1459;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1339, [retval0+0];
	} // callseq 387
	bra.uni 	$L__BB13_216;

$L__BB13_214:
	add.s32 	%r2380, %r208, 2;
	cvta.to.global.u64 	%rd1451, %rd168;
	mul.wide.s32 	%rd1452, %r361, %r2380;
	add.s64 	%rd1442, %rd169, %rd1452;
	// begin inline asm
	{ atom.add.f64 %fd3493,[%rd1442],%fd282; }

	// end inline asm
	add.s64 	%rd1443, %rd1442, 8;
	// begin inline asm
	{ atom.add.f64 %fd3495,[%rd1443],%fd281; }

	// end inline asm
	add.s64 	%rd1444, %rd1442, 16;
	// begin inline asm
	{ atom.add.f64 %fd3497,[%rd1444],%fd280; }

	// end inline asm
	add.s64 	%rd1445, %rd1442, 24;
	// begin inline asm
	{ atom.add.f64 %fd3499,[%rd1445],%fd258; }

	// end inline asm
	add.s64 	%rd1446, %rd1442, 32;
	// begin inline asm
	{ atom.add.f64 %fd3501,[%rd1446],%fd257; }

	// end inline asm
	add.s64 	%rd1447, %rd1442, 40;
	// begin inline asm
	{ atom.add.f64 %fd3503,[%rd1447],%fd256; }

	// end inline asm
	add.s64 	%rd1448, %rd1442, 48;
	// begin inline asm
	{ atom.add.f64 %fd3505,[%rd1448],%fd234; }

	// end inline asm
	add.s64 	%rd1449, %rd1442, 56;
	// begin inline asm
	{ atom.add.f64 %fd3507,[%rd1449],%fd233; }

	// end inline asm
	add.s64 	%rd1450, %rd1442, 64;
	// begin inline asm
	{ atom.add.f64 %fd3509,[%rd1450],%fd232; }

	// end inline asm
	mul.wide.s32 	%rd1453, %r359, %r2380;
	cvta.to.global.u64 	%rd1454, %rd167;
	add.s64 	%rd1455, %rd1454, %rd1453;
	mul.wide.s32 	%rd1456, %r360, %r2380;
	add.s64 	%rd1457, %rd1451, %rd1456;
	st.global.u32 	[%rd1455], %r209;
	add.s32 	%r2148, %r210, 2;
	st.global.u32 	[%rd1457], %r2148;

$L__BB13_216:
	add.s32 	%r2381, %r208, 3;
	ld.param.u64 	%rd170, [%rd357];
	ld.param.u32 	%r366, [%rd357+32];
	ld.param.u64 	%rd171, [%rd357+56];
	ld.param.u32 	%r367, [%rd357+88];
	ld.param.u64 	%rd172, [%rd357+112];
	ld.param.u32 	%r368, [%rd357+144];
	ld.param.u32 	%r369, [%rd357+172];
	ld.param.v2.u32 	{%r1340, %r1341}, [%rd357+176];
	setp.le.s32 	%p320, %r1340, %r209;
	setp.le.s32 	%p321, %r1341, %r244;
	setp.le.s32 	%p322, %r369, %r2381;
	or.pred  	%p323, %p320, %p321;
	or.pred  	%p324, %p211, %p323;
	or.pred  	%p325, %p322, %p324;
	@%p325 bra 	$L__BB13_218;
	bra.uni 	$L__BB13_217;

$L__BB13_218:
	add.s32 	%r2152, %r210, 3;
	st.local.v2.u32 	[%rd30], {%r209, %r2152};
	add.s32 	%r2153, %r208, 3;
	st.local.v2.u32 	[%rd30+8], {%r2153, %r1340};
	st.local.v2.u32 	[%rd30+16], {%r1341, %r369};
	mov.u64 	%rd1477, $str$2;
	cvta.global.u64 	%rd1478, %rd1477;
	{ // callseq 388, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1478;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1345, [retval0+0];
	} // callseq 388
	bra.uni 	$L__BB13_219;

$L__BB13_217:
	add.s32 	%r2382, %r208, 3;
	cvta.to.global.u64 	%rd1470, %rd171;
	mul.wide.s32 	%rd1471, %r368, %r2382;
	add.s64 	%rd1461, %rd172, %rd1471;
	// begin inline asm
	{ atom.add.f64 %fd3511,[%rd1461],%fd279; }

	// end inline asm
	add.s64 	%rd1462, %rd1461, 8;
	// begin inline asm
	{ atom.add.f64 %fd3513,[%rd1462],%fd278; }

	// end inline asm
	add.s64 	%rd1463, %rd1461, 16;
	// begin inline asm
	{ atom.add.f64 %fd3515,[%rd1463],%fd277; }

	// end inline asm
	add.s64 	%rd1464, %rd1461, 24;
	// begin inline asm
	{ atom.add.f64 %fd3517,[%rd1464],%fd255; }

	// end inline asm
	add.s64 	%rd1465, %rd1461, 32;
	// begin inline asm
	{ atom.add.f64 %fd3519,[%rd1465],%fd254; }

	// end inline asm
	add.s64 	%rd1466, %rd1461, 40;
	// begin inline asm
	{ atom.add.f64 %fd3521,[%rd1466],%fd253; }

	// end inline asm
	add.s64 	%rd1467, %rd1461, 48;
	// begin inline asm
	{ atom.add.f64 %fd3523,[%rd1467],%fd231; }

	// end inline asm
	add.s64 	%rd1468, %rd1461, 56;
	// begin inline asm
	{ atom.add.f64 %fd3525,[%rd1468],%fd230; }

	// end inline asm
	add.s64 	%rd1469, %rd1461, 64;
	// begin inline asm
	{ atom.add.f64 %fd3527,[%rd1469],%fd229; }

	// end inline asm
	mul.wide.s32 	%rd1472, %r366, %r2382;
	cvta.to.global.u64 	%rd1473, %rd170;
	add.s64 	%rd1474, %rd1473, %rd1472;
	mul.wide.s32 	%rd1475, %r367, %r2382;
	add.s64 	%rd1476, %rd1470, %rd1475;
	st.global.u32 	[%rd1474], %r209;
	add.s32 	%r2151, %r210, 3;
	st.global.u32 	[%rd1476], %r2151;

$L__BB13_219:
	add.s32 	%r2383, %r208, 4;
	ld.param.u64 	%rd173, [%rd357];
	ld.param.u32 	%r373, [%rd357+32];
	ld.param.u64 	%rd174, [%rd357+56];
	ld.param.u32 	%r374, [%rd357+88];
	ld.param.u64 	%rd175, [%rd357+112];
	ld.param.u32 	%r375, [%rd357+144];
	ld.param.u32 	%r376, [%rd357+172];
	ld.param.v2.u32 	{%r1346, %r1347}, [%rd357+176];
	setp.le.s32 	%p327, %r1346, %r253;
	setp.le.s32 	%p328, %r1347, %r210;
	setp.le.s32 	%p329, %r376, %r2383;
	or.pred  	%p330, %p327, %p328;
	or.pred  	%p331, %p218, %p330;
	or.pred  	%p332, %p329, %p331;
	@%p332 bra 	$L__BB13_221;
	bra.uni 	$L__BB13_220;

$L__BB13_221:
	add.s32 	%r2155, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2155, %r210};
	add.s32 	%r2156, %r208, 4;
	st.local.v2.u32 	[%rd30+8], {%r2156, %r1346};
	st.local.v2.u32 	[%rd30+16], {%r1347, %r376};
	mov.u64 	%rd1496, $str$2;
	cvta.global.u64 	%rd1497, %rd1496;
	{ // callseq 389, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1497;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1351, [retval0+0];
	} // callseq 389
	bra.uni 	$L__BB13_222;

$L__BB13_220:
	add.s32 	%r2384, %r208, 4;
	cvta.to.global.u64 	%rd1489, %rd174;
	mul.wide.s32 	%rd1490, %r375, %r2384;
	add.s64 	%rd1480, %rd175, %rd1490;
	// begin inline asm
	{ atom.add.f64 %fd3529,[%rd1480],%fd216; }

	// end inline asm
	add.s64 	%rd1481, %rd1480, 8;
	// begin inline asm
	{ atom.add.f64 %fd3531,[%rd1481],%fd215; }

	// end inline asm
	add.s64 	%rd1482, %rd1480, 16;
	// begin inline asm
	{ atom.add.f64 %fd3533,[%rd1482],%fd214; }

	// end inline asm
	add.s64 	%rd1483, %rd1480, 24;
	// begin inline asm
	{ atom.add.f64 %fd3535,[%rd1483],%fd192; }

	// end inline asm
	add.s64 	%rd1484, %rd1480, 32;
	// begin inline asm
	{ atom.add.f64 %fd3537,[%rd1484],%fd191; }

	// end inline asm
	add.s64 	%rd1485, %rd1480, 40;
	// begin inline asm
	{ atom.add.f64 %fd3539,[%rd1485],%fd190; }

	// end inline asm
	add.s64 	%rd1486, %rd1480, 48;
	// begin inline asm
	{ atom.add.f64 %fd3541,[%rd1486],%fd168; }

	// end inline asm
	add.s64 	%rd1487, %rd1480, 56;
	// begin inline asm
	{ atom.add.f64 %fd3543,[%rd1487],%fd167; }

	// end inline asm
	add.s64 	%rd1488, %rd1480, 64;
	// begin inline asm
	{ atom.add.f64 %fd3545,[%rd1488],%fd166; }

	// end inline asm
	mul.wide.s32 	%rd1491, %r373, %r2384;
	cvta.to.global.u64 	%rd1492, %rd173;
	add.s64 	%rd1493, %rd1492, %rd1491;
	mul.wide.s32 	%rd1494, %r374, %r2384;
	add.s64 	%rd1495, %rd1489, %rd1494;
	add.s32 	%r2154, %r209, 1;
	st.global.u32 	[%rd1493], %r2154;
	st.global.u32 	[%rd1495], %r210;

$L__BB13_222:
	add.s32 	%r2385, %r208, 5;
	ld.param.u64 	%rd176, [%rd357];
	ld.param.u32 	%r380, [%rd357+32];
	ld.param.u64 	%rd177, [%rd357+56];
	ld.param.u32 	%r381, [%rd357+88];
	ld.param.u64 	%rd178, [%rd357+112];
	ld.param.u32 	%r382, [%rd357+144];
	ld.param.u32 	%r383, [%rd357+172];
	ld.param.v2.u32 	{%r1352, %r1353}, [%rd357+176];
	setp.le.s32 	%p334, %r1352, %r253;
	setp.le.s32 	%p335, %r1353, %r226;
	setp.le.s32 	%p336, %r383, %r2385;
	or.pred  	%p337, %p334, %p335;
	or.pred  	%p338, %p225, %p337;
	or.pred  	%p339, %p336, %p338;
	@%p339 bra 	$L__BB13_224;
	bra.uni 	$L__BB13_223;

$L__BB13_224:
	add.s32 	%r2159, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2159, %r226};
	add.s32 	%r2160, %r208, 5;
	st.local.v2.u32 	[%rd30+8], {%r2160, %r1352};
	st.local.v2.u32 	[%rd30+16], {%r1353, %r383};
	mov.u64 	%rd1515, $str$2;
	cvta.global.u64 	%rd1516, %rd1515;
	{ // callseq 390, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1516;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1357, [retval0+0];
	} // callseq 390
	bra.uni 	$L__BB13_225;

$L__BB13_223:
	add.s32 	%r2386, %r208, 5;
	cvta.to.global.u64 	%rd1508, %rd177;
	mul.wide.s32 	%rd1509, %r382, %r2386;
	add.s64 	%rd1499, %rd178, %rd1509;
	// begin inline asm
	{ atom.add.f64 %fd3547,[%rd1499],%fd213; }

	// end inline asm
	add.s64 	%rd1500, %rd1499, 8;
	// begin inline asm
	{ atom.add.f64 %fd3549,[%rd1500],%fd212; }

	// end inline asm
	add.s64 	%rd1501, %rd1499, 16;
	// begin inline asm
	{ atom.add.f64 %fd3551,[%rd1501],%fd211; }

	// end inline asm
	add.s64 	%rd1502, %rd1499, 24;
	// begin inline asm
	{ atom.add.f64 %fd3553,[%rd1502],%fd189; }

	// end inline asm
	add.s64 	%rd1503, %rd1499, 32;
	// begin inline asm
	{ atom.add.f64 %fd3555,[%rd1503],%fd188; }

	// end inline asm
	add.s64 	%rd1504, %rd1499, 40;
	// begin inline asm
	{ atom.add.f64 %fd3557,[%rd1504],%fd187; }

	// end inline asm
	add.s64 	%rd1505, %rd1499, 48;
	// begin inline asm
	{ atom.add.f64 %fd3559,[%rd1505],%fd165; }

	// end inline asm
	add.s64 	%rd1506, %rd1499, 56;
	// begin inline asm
	{ atom.add.f64 %fd3561,[%rd1506],%fd164; }

	// end inline asm
	add.s64 	%rd1507, %rd1499, 64;
	// begin inline asm
	{ atom.add.f64 %fd3563,[%rd1507],%fd163; }

	// end inline asm
	mul.wide.s32 	%rd1510, %r380, %r2386;
	cvta.to.global.u64 	%rd1511, %rd176;
	add.s64 	%rd1512, %rd1511, %rd1510;
	mul.wide.s32 	%rd1513, %r381, %r2386;
	add.s64 	%rd1514, %rd1508, %rd1513;
	add.s32 	%r2157, %r209, 1;
	st.global.u32 	[%rd1512], %r2157;
	add.s32 	%r2158, %r210, 1;
	st.global.u32 	[%rd1514], %r2158;

$L__BB13_225:
	add.s32 	%r2387, %r208, 6;
	ld.param.u64 	%rd179, [%rd357];
	ld.param.u32 	%r387, [%rd357+32];
	ld.param.u64 	%rd180, [%rd357+56];
	ld.param.u32 	%r388, [%rd357+88];
	ld.param.u64 	%rd181, [%rd357+112];
	ld.param.u32 	%r389, [%rd357+144];
	ld.param.u32 	%r390, [%rd357+172];
	ld.param.v2.u32 	{%r1358, %r1359}, [%rd357+176];
	setp.le.s32 	%p341, %r1358, %r253;
	setp.le.s32 	%p342, %r1359, %r235;
	setp.le.s32 	%p343, %r390, %r2387;
	or.pred  	%p344, %p341, %p342;
	or.pred  	%p345, %p232, %p344;
	or.pred  	%p346, %p343, %p345;
	@%p346 bra 	$L__BB13_227;
	bra.uni 	$L__BB13_226;

$L__BB13_227:
	add.s32 	%r2163, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2163, %r235};
	add.s32 	%r2164, %r208, 6;
	st.local.v2.u32 	[%rd30+8], {%r2164, %r1358};
	st.local.v2.u32 	[%rd30+16], {%r1359, %r390};
	mov.u64 	%rd1534, $str$2;
	cvta.global.u64 	%rd1535, %rd1534;
	{ // callseq 391, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1535;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1363, [retval0+0];
	} // callseq 391
	bra.uni 	$L__BB13_228;

$L__BB13_226:
	add.s32 	%r2388, %r208, 6;
	cvta.to.global.u64 	%rd1527, %rd180;
	mul.wide.s32 	%rd1528, %r389, %r2388;
	add.s64 	%rd1518, %rd181, %rd1528;
	// begin inline asm
	{ atom.add.f64 %fd3565,[%rd1518],%fd210; }

	// end inline asm
	add.s64 	%rd1519, %rd1518, 8;
	// begin inline asm
	{ atom.add.f64 %fd3567,[%rd1519],%fd209; }

	// end inline asm
	add.s64 	%rd1520, %rd1518, 16;
	// begin inline asm
	{ atom.add.f64 %fd3569,[%rd1520],%fd208; }

	// end inline asm
	add.s64 	%rd1521, %rd1518, 24;
	// begin inline asm
	{ atom.add.f64 %fd3571,[%rd1521],%fd186; }

	// end inline asm
	add.s64 	%rd1522, %rd1518, 32;
	// begin inline asm
	{ atom.add.f64 %fd3573,[%rd1522],%fd185; }

	// end inline asm
	add.s64 	%rd1523, %rd1518, 40;
	// begin inline asm
	{ atom.add.f64 %fd3575,[%rd1523],%fd184; }

	// end inline asm
	add.s64 	%rd1524, %rd1518, 48;
	// begin inline asm
	{ atom.add.f64 %fd3577,[%rd1524],%fd162; }

	// end inline asm
	add.s64 	%rd1525, %rd1518, 56;
	// begin inline asm
	{ atom.add.f64 %fd3579,[%rd1525],%fd161; }

	// end inline asm
	add.s64 	%rd1526, %rd1518, 64;
	// begin inline asm
	{ atom.add.f64 %fd3581,[%rd1526],%fd160; }

	// end inline asm
	mul.wide.s32 	%rd1529, %r387, %r2388;
	cvta.to.global.u64 	%rd1530, %rd179;
	add.s64 	%rd1531, %rd1530, %rd1529;
	mul.wide.s32 	%rd1532, %r388, %r2388;
	add.s64 	%rd1533, %rd1527, %rd1532;
	add.s32 	%r2161, %r209, 1;
	st.global.u32 	[%rd1531], %r2161;
	add.s32 	%r2162, %r210, 2;
	st.global.u32 	[%rd1533], %r2162;

$L__BB13_228:
	add.s32 	%r2389, %r208, 7;
	ld.param.u64 	%rd182, [%rd357];
	ld.param.u32 	%r394, [%rd357+32];
	ld.param.u64 	%rd183, [%rd357+56];
	ld.param.u32 	%r395, [%rd357+88];
	ld.param.u64 	%rd184, [%rd357+112];
	ld.param.u32 	%r396, [%rd357+144];
	ld.param.u32 	%r397, [%rd357+172];
	ld.param.v2.u32 	{%r1364, %r1365}, [%rd357+176];
	setp.le.s32 	%p348, %r1364, %r253;
	setp.le.s32 	%p349, %r1365, %r244;
	setp.le.s32 	%p350, %r397, %r2389;
	or.pred  	%p351, %p348, %p349;
	or.pred  	%p352, %p239, %p351;
	or.pred  	%p353, %p350, %p352;
	@%p353 bra 	$L__BB13_230;
	bra.uni 	$L__BB13_229;

$L__BB13_230:
	add.s32 	%r2167, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2167, %r244};
	add.s32 	%r2168, %r208, 7;
	st.local.v2.u32 	[%rd30+8], {%r2168, %r1364};
	st.local.v2.u32 	[%rd30+16], {%r1365, %r397};
	mov.u64 	%rd1553, $str$2;
	cvta.global.u64 	%rd1554, %rd1553;
	{ // callseq 392, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1554;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1369, [retval0+0];
	} // callseq 392
	bra.uni 	$L__BB13_231;

$L__BB13_229:
	add.s32 	%r2390, %r208, 7;
	cvta.to.global.u64 	%rd1546, %rd183;
	mul.wide.s32 	%rd1547, %r396, %r2390;
	add.s64 	%rd1537, %rd184, %rd1547;
	// begin inline asm
	{ atom.add.f64 %fd3583,[%rd1537],%fd207; }

	// end inline asm
	add.s64 	%rd1538, %rd1537, 8;
	// begin inline asm
	{ atom.add.f64 %fd3585,[%rd1538],%fd206; }

	// end inline asm
	add.s64 	%rd1539, %rd1537, 16;
	// begin inline asm
	{ atom.add.f64 %fd3587,[%rd1539],%fd205; }

	// end inline asm
	add.s64 	%rd1540, %rd1537, 24;
	// begin inline asm
	{ atom.add.f64 %fd3589,[%rd1540],%fd183; }

	// end inline asm
	add.s64 	%rd1541, %rd1537, 32;
	// begin inline asm
	{ atom.add.f64 %fd3591,[%rd1541],%fd182; }

	// end inline asm
	add.s64 	%rd1542, %rd1537, 40;
	// begin inline asm
	{ atom.add.f64 %fd3593,[%rd1542],%fd181; }

	// end inline asm
	add.s64 	%rd1543, %rd1537, 48;
	// begin inline asm
	{ atom.add.f64 %fd3595,[%rd1543],%fd159; }

	// end inline asm
	add.s64 	%rd1544, %rd1537, 56;
	// begin inline asm
	{ atom.add.f64 %fd3597,[%rd1544],%fd158; }

	// end inline asm
	add.s64 	%rd1545, %rd1537, 64;
	// begin inline asm
	{ atom.add.f64 %fd3599,[%rd1545],%fd157; }

	// end inline asm
	mul.wide.s32 	%rd1548, %r394, %r2390;
	cvta.to.global.u64 	%rd1549, %rd182;
	add.s64 	%rd1550, %rd1549, %rd1548;
	mul.wide.s32 	%rd1551, %r395, %r2390;
	add.s64 	%rd1552, %rd1546, %rd1551;
	add.s32 	%r2165, %r209, 1;
	st.global.u32 	[%rd1550], %r2165;
	add.s32 	%r2166, %r210, 3;
	st.global.u32 	[%rd1552], %r2166;

$L__BB13_231:
	add.s32 	%r2391, %r208, 8;
	ld.param.u64 	%rd185, [%rd357];
	ld.param.u32 	%r401, [%rd357+32];
	ld.param.u64 	%rd186, [%rd357+56];
	ld.param.u32 	%r402, [%rd357+88];
	ld.param.u64 	%rd187, [%rd357+112];
	ld.param.u32 	%r403, [%rd357+144];
	ld.param.u32 	%r404, [%rd357+172];
	ld.param.v2.u32 	{%r1370, %r1371}, [%rd357+176];
	setp.le.s32 	%p355, %r1370, %r286;
	setp.le.s32 	%p356, %r1371, %r210;
	setp.le.s32 	%p357, %r404, %r2391;
	or.pred  	%p358, %p355, %p356;
	or.pred  	%p359, %p246, %p358;
	or.pred  	%p360, %p357, %p359;
	@%p360 bra 	$L__BB13_233;
	bra.uni 	$L__BB13_232;

$L__BB13_233:
	add.s32 	%r2170, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2170, %r210};
	add.s32 	%r2171, %r208, 8;
	st.local.v2.u32 	[%rd30+8], {%r2171, %r1370};
	st.local.v2.u32 	[%rd30+16], {%r1371, %r404};
	mov.u64 	%rd1572, $str$2;
	cvta.global.u64 	%rd1573, %rd1572;
	{ // callseq 393, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1573;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1375, [retval0+0];
	} // callseq 393
	bra.uni 	$L__BB13_234;

$L__BB13_232:
	add.s32 	%r2392, %r208, 8;
	cvta.to.global.u64 	%rd1565, %rd186;
	mul.wide.s32 	%rd1566, %r403, %r2392;
	add.s64 	%rd1556, %rd187, %rd1566;
	// begin inline asm
	{ atom.add.f64 %fd3601,[%rd1556],%fd144; }

	// end inline asm
	add.s64 	%rd1557, %rd1556, 8;
	// begin inline asm
	{ atom.add.f64 %fd3603,[%rd1557],%fd143; }

	// end inline asm
	add.s64 	%rd1558, %rd1556, 16;
	// begin inline asm
	{ atom.add.f64 %fd3605,[%rd1558],%fd142; }

	// end inline asm
	add.s64 	%rd1559, %rd1556, 24;
	// begin inline asm
	{ atom.add.f64 %fd3607,[%rd1559],%fd120; }

	// end inline asm
	add.s64 	%rd1560, %rd1556, 32;
	// begin inline asm
	{ atom.add.f64 %fd3609,[%rd1560],%fd119; }

	// end inline asm
	add.s64 	%rd1561, %rd1556, 40;
	// begin inline asm
	{ atom.add.f64 %fd3611,[%rd1561],%fd118; }

	// end inline asm
	add.s64 	%rd1562, %rd1556, 48;
	// begin inline asm
	{ atom.add.f64 %fd3613,[%rd1562],%fd96; }

	// end inline asm
	add.s64 	%rd1563, %rd1556, 56;
	// begin inline asm
	{ atom.add.f64 %fd3615,[%rd1563],%fd95; }

	// end inline asm
	add.s64 	%rd1564, %rd1556, 64;
	// begin inline asm
	{ atom.add.f64 %fd3617,[%rd1564],%fd94; }

	// end inline asm
	mul.wide.s32 	%rd1567, %r401, %r2392;
	cvta.to.global.u64 	%rd1568, %rd185;
	add.s64 	%rd1569, %rd1568, %rd1567;
	mul.wide.s32 	%rd1570, %r402, %r2392;
	add.s64 	%rd1571, %rd1565, %rd1570;
	add.s32 	%r2169, %r209, 2;
	st.global.u32 	[%rd1569], %r2169;
	st.global.u32 	[%rd1571], %r210;

$L__BB13_234:
	add.s32 	%r2393, %r208, 9;
	ld.param.u64 	%rd188, [%rd357];
	ld.param.u32 	%r408, [%rd357+32];
	ld.param.u64 	%rd189, [%rd357+56];
	ld.param.u32 	%r409, [%rd357+88];
	ld.param.u64 	%rd190, [%rd357+112];
	ld.param.u32 	%r410, [%rd357+144];
	ld.param.u32 	%r411, [%rd357+172];
	ld.param.v2.u32 	{%r1376, %r1377}, [%rd357+176];
	setp.le.s32 	%p362, %r1376, %r286;
	setp.le.s32 	%p363, %r1377, %r226;
	setp.le.s32 	%p364, %r411, %r2393;
	or.pred  	%p365, %p362, %p363;
	or.pred  	%p366, %p253, %p365;
	or.pred  	%p367, %p364, %p366;
	@%p367 bra 	$L__BB13_236;
	bra.uni 	$L__BB13_235;

$L__BB13_236:
	add.s32 	%r2174, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2174, %r226};
	add.s32 	%r2175, %r208, 9;
	st.local.v2.u32 	[%rd30+8], {%r2175, %r1376};
	st.local.v2.u32 	[%rd30+16], {%r1377, %r411};
	mov.u64 	%rd1591, $str$2;
	cvta.global.u64 	%rd1592, %rd1591;
	{ // callseq 394, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1592;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1381, [retval0+0];
	} // callseq 394
	bra.uni 	$L__BB13_237;

$L__BB13_235:
	add.s32 	%r2394, %r208, 9;
	cvta.to.global.u64 	%rd1584, %rd189;
	mul.wide.s32 	%rd1585, %r410, %r2394;
	add.s64 	%rd1575, %rd190, %rd1585;
	// begin inline asm
	{ atom.add.f64 %fd3619,[%rd1575],%fd141; }

	// end inline asm
	add.s64 	%rd1576, %rd1575, 8;
	// begin inline asm
	{ atom.add.f64 %fd3621,[%rd1576],%fd140; }

	// end inline asm
	add.s64 	%rd1577, %rd1575, 16;
	// begin inline asm
	{ atom.add.f64 %fd3623,[%rd1577],%fd139; }

	// end inline asm
	add.s64 	%rd1578, %rd1575, 24;
	// begin inline asm
	{ atom.add.f64 %fd3625,[%rd1578],%fd117; }

	// end inline asm
	add.s64 	%rd1579, %rd1575, 32;
	// begin inline asm
	{ atom.add.f64 %fd3627,[%rd1579],%fd116; }

	// end inline asm
	add.s64 	%rd1580, %rd1575, 40;
	// begin inline asm
	{ atom.add.f64 %fd3629,[%rd1580],%fd115; }

	// end inline asm
	add.s64 	%rd1581, %rd1575, 48;
	// begin inline asm
	{ atom.add.f64 %fd3631,[%rd1581],%fd93; }

	// end inline asm
	add.s64 	%rd1582, %rd1575, 56;
	// begin inline asm
	{ atom.add.f64 %fd3633,[%rd1582],%fd92; }

	// end inline asm
	add.s64 	%rd1583, %rd1575, 64;
	// begin inline asm
	{ atom.add.f64 %fd3635,[%rd1583],%fd91; }

	// end inline asm
	mul.wide.s32 	%rd1586, %r408, %r2394;
	cvta.to.global.u64 	%rd1587, %rd188;
	add.s64 	%rd1588, %rd1587, %rd1586;
	mul.wide.s32 	%rd1589, %r409, %r2394;
	add.s64 	%rd1590, %rd1584, %rd1589;
	add.s32 	%r2172, %r209, 2;
	st.global.u32 	[%rd1588], %r2172;
	add.s32 	%r2173, %r210, 1;
	st.global.u32 	[%rd1590], %r2173;

$L__BB13_237:
	add.s32 	%r2395, %r208, 10;
	ld.param.u64 	%rd191, [%rd357];
	ld.param.u32 	%r415, [%rd357+32];
	ld.param.u64 	%rd192, [%rd357+56];
	ld.param.u32 	%r416, [%rd357+88];
	ld.param.u64 	%rd193, [%rd357+112];
	ld.param.u32 	%r417, [%rd357+144];
	ld.param.u32 	%r418, [%rd357+172];
	ld.param.v2.u32 	{%r1382, %r1383}, [%rd357+176];
	setp.le.s32 	%p369, %r1382, %r286;
	setp.le.s32 	%p370, %r1383, %r235;
	setp.le.s32 	%p371, %r418, %r2395;
	or.pred  	%p372, %p369, %p370;
	or.pred  	%p373, %p260, %p372;
	or.pred  	%p374, %p371, %p373;
	@%p374 bra 	$L__BB13_239;
	bra.uni 	$L__BB13_238;

$L__BB13_239:
	add.s32 	%r2178, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2178, %r235};
	add.s32 	%r2179, %r208, 10;
	st.local.v2.u32 	[%rd30+8], {%r2179, %r1382};
	st.local.v2.u32 	[%rd30+16], {%r1383, %r418};
	mov.u64 	%rd1610, $str$2;
	cvta.global.u64 	%rd1611, %rd1610;
	{ // callseq 395, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1611;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1387, [retval0+0];
	} // callseq 395
	bra.uni 	$L__BB13_240;

$L__BB13_238:
	add.s32 	%r2396, %r208, 10;
	cvta.to.global.u64 	%rd1603, %rd192;
	mul.wide.s32 	%rd1604, %r417, %r2396;
	add.s64 	%rd1594, %rd193, %rd1604;
	// begin inline asm
	{ atom.add.f64 %fd3637,[%rd1594],%fd138; }

	// end inline asm
	add.s64 	%rd1595, %rd1594, 8;
	// begin inline asm
	{ atom.add.f64 %fd3639,[%rd1595],%fd137; }

	// end inline asm
	add.s64 	%rd1596, %rd1594, 16;
	// begin inline asm
	{ atom.add.f64 %fd3641,[%rd1596],%fd136; }

	// end inline asm
	add.s64 	%rd1597, %rd1594, 24;
	// begin inline asm
	{ atom.add.f64 %fd3643,[%rd1597],%fd114; }

	// end inline asm
	add.s64 	%rd1598, %rd1594, 32;
	// begin inline asm
	{ atom.add.f64 %fd3645,[%rd1598],%fd113; }

	// end inline asm
	add.s64 	%rd1599, %rd1594, 40;
	// begin inline asm
	{ atom.add.f64 %fd3647,[%rd1599],%fd112; }

	// end inline asm
	add.s64 	%rd1600, %rd1594, 48;
	// begin inline asm
	{ atom.add.f64 %fd3649,[%rd1600],%fd90; }

	// end inline asm
	add.s64 	%rd1601, %rd1594, 56;
	// begin inline asm
	{ atom.add.f64 %fd3651,[%rd1601],%fd89; }

	// end inline asm
	add.s64 	%rd1602, %rd1594, 64;
	// begin inline asm
	{ atom.add.f64 %fd3653,[%rd1602],%fd88; }

	// end inline asm
	mul.wide.s32 	%rd1605, %r415, %r2396;
	cvta.to.global.u64 	%rd1606, %rd191;
	add.s64 	%rd1607, %rd1606, %rd1605;
	mul.wide.s32 	%rd1608, %r416, %r2396;
	add.s64 	%rd1609, %rd1603, %rd1608;
	add.s32 	%r2176, %r209, 2;
	st.global.u32 	[%rd1607], %r2176;
	add.s32 	%r2177, %r210, 2;
	st.global.u32 	[%rd1609], %r2177;

$L__BB13_240:
	add.s32 	%r2397, %r208, 11;
	ld.param.u64 	%rd194, [%rd357];
	ld.param.u32 	%r422, [%rd357+32];
	ld.param.u64 	%rd195, [%rd357+56];
	ld.param.u32 	%r423, [%rd357+88];
	ld.param.u64 	%rd196, [%rd357+112];
	ld.param.u32 	%r424, [%rd357+144];
	ld.param.u32 	%r425, [%rd357+172];
	ld.param.v2.u32 	{%r1388, %r1389}, [%rd357+176];
	setp.le.s32 	%p376, %r1388, %r286;
	setp.le.s32 	%p377, %r1389, %r244;
	setp.le.s32 	%p378, %r425, %r2397;
	or.pred  	%p379, %p376, %p377;
	or.pred  	%p380, %p267, %p379;
	or.pred  	%p381, %p378, %p380;
	@%p381 bra 	$L__BB13_242;
	bra.uni 	$L__BB13_241;

$L__BB13_242:
	add.s32 	%r2182, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2182, %r244};
	add.s32 	%r2183, %r208, 11;
	st.local.v2.u32 	[%rd30+8], {%r2183, %r1388};
	st.local.v2.u32 	[%rd30+16], {%r1389, %r425};
	mov.u64 	%rd1629, $str$2;
	cvta.global.u64 	%rd1630, %rd1629;
	{ // callseq 396, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1630;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1393, [retval0+0];
	} // callseq 396
	bra.uni 	$L__BB13_243;

$L__BB13_241:
	add.s32 	%r2398, %r208, 11;
	cvta.to.global.u64 	%rd1622, %rd195;
	mul.wide.s32 	%rd1623, %r424, %r2398;
	add.s64 	%rd1613, %rd196, %rd1623;
	// begin inline asm
	{ atom.add.f64 %fd3655,[%rd1613],%fd135; }

	// end inline asm
	add.s64 	%rd1614, %rd1613, 8;
	// begin inline asm
	{ atom.add.f64 %fd3657,[%rd1614],%fd134; }

	// end inline asm
	add.s64 	%rd1615, %rd1613, 16;
	// begin inline asm
	{ atom.add.f64 %fd3659,[%rd1615],%fd133; }

	// end inline asm
	add.s64 	%rd1616, %rd1613, 24;
	// begin inline asm
	{ atom.add.f64 %fd3661,[%rd1616],%fd111; }

	// end inline asm
	add.s64 	%rd1617, %rd1613, 32;
	// begin inline asm
	{ atom.add.f64 %fd3663,[%rd1617],%fd110; }

	// end inline asm
	add.s64 	%rd1618, %rd1613, 40;
	// begin inline asm
	{ atom.add.f64 %fd3665,[%rd1618],%fd109; }

	// end inline asm
	add.s64 	%rd1619, %rd1613, 48;
	// begin inline asm
	{ atom.add.f64 %fd3667,[%rd1619],%fd87; }

	// end inline asm
	add.s64 	%rd1620, %rd1613, 56;
	// begin inline asm
	{ atom.add.f64 %fd3669,[%rd1620],%fd86; }

	// end inline asm
	add.s64 	%rd1621, %rd1613, 64;
	// begin inline asm
	{ atom.add.f64 %fd3671,[%rd1621],%fd85; }

	// end inline asm
	mul.wide.s32 	%rd1624, %r422, %r2398;
	cvta.to.global.u64 	%rd1625, %rd194;
	add.s64 	%rd1626, %rd1625, %rd1624;
	mul.wide.s32 	%rd1627, %r423, %r2398;
	add.s64 	%rd1628, %rd1622, %rd1627;
	add.s32 	%r2180, %r209, 2;
	st.global.u32 	[%rd1626], %r2180;
	add.s32 	%r2181, %r210, 3;
	st.global.u32 	[%rd1628], %r2181;

$L__BB13_243:
	add.s32 	%r2399, %r208, 12;
	ld.param.u64 	%rd197, [%rd357];
	ld.param.u32 	%r429, [%rd357+32];
	ld.param.u64 	%rd198, [%rd357+56];
	ld.param.u32 	%r430, [%rd357+88];
	ld.param.u64 	%rd199, [%rd357+112];
	ld.param.u32 	%r431, [%rd357+144];
	ld.param.u32 	%r432, [%rd357+172];
	ld.param.v2.u32 	{%r1394, %r1395}, [%rd357+176];
	setp.le.s32 	%p383, %r1394, %r319;
	setp.le.s32 	%p384, %r1395, %r210;
	setp.le.s32 	%p385, %r432, %r2399;
	or.pred  	%p386, %p383, %p384;
	or.pred  	%p387, %p274, %p386;
	or.pred  	%p388, %p385, %p387;
	@%p388 bra 	$L__BB13_245;
	bra.uni 	$L__BB13_244;

$L__BB13_245:
	add.s32 	%r2185, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2185, %r210};
	add.s32 	%r2186, %r208, 12;
	st.local.v2.u32 	[%rd30+8], {%r2186, %r1394};
	st.local.v2.u32 	[%rd30+16], {%r1395, %r432};
	mov.u64 	%rd1648, $str$2;
	cvta.global.u64 	%rd1649, %rd1648;
	{ // callseq 397, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1649;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1399, [retval0+0];
	} // callseq 397
	bra.uni 	$L__BB13_246;

$L__BB13_244:
	add.s32 	%r2400, %r208, 12;
	cvta.to.global.u64 	%rd1641, %rd198;
	mul.wide.s32 	%rd1642, %r431, %r2400;
	add.s64 	%rd1632, %rd199, %rd1642;
	// begin inline asm
	{ atom.add.f64 %fd3673,[%rd1632],%fd72; }

	// end inline asm
	add.s64 	%rd1633, %rd1632, 8;
	// begin inline asm
	{ atom.add.f64 %fd3675,[%rd1633],%fd71; }

	// end inline asm
	add.s64 	%rd1634, %rd1632, 16;
	// begin inline asm
	{ atom.add.f64 %fd3677,[%rd1634],%fd70; }

	// end inline asm
	add.s64 	%rd1635, %rd1632, 24;
	// begin inline asm
	{ atom.add.f64 %fd3679,[%rd1635],%fd48; }

	// end inline asm
	add.s64 	%rd1636, %rd1632, 32;
	// begin inline asm
	{ atom.add.f64 %fd3681,[%rd1636],%fd47; }

	// end inline asm
	add.s64 	%rd1637, %rd1632, 40;
	// begin inline asm
	{ atom.add.f64 %fd3683,[%rd1637],%fd46; }

	// end inline asm
	add.s64 	%rd1638, %rd1632, 48;
	// begin inline asm
	{ atom.add.f64 %fd3685,[%rd1638],%fd24; }

	// end inline asm
	add.s64 	%rd1639, %rd1632, 56;
	// begin inline asm
	{ atom.add.f64 %fd3687,[%rd1639],%fd23; }

	// end inline asm
	add.s64 	%rd1640, %rd1632, 64;
	// begin inline asm
	{ atom.add.f64 %fd3689,[%rd1640],%fd22; }

	// end inline asm
	mul.wide.s32 	%rd1643, %r429, %r2400;
	cvta.to.global.u64 	%rd1644, %rd197;
	add.s64 	%rd1645, %rd1644, %rd1643;
	mul.wide.s32 	%rd1646, %r430, %r2400;
	add.s64 	%rd1647, %rd1641, %rd1646;
	add.s32 	%r2184, %r209, 3;
	st.global.u32 	[%rd1645], %r2184;
	st.global.u32 	[%rd1647], %r210;

$L__BB13_246:
	add.s32 	%r2401, %r208, 13;
	ld.param.u64 	%rd200, [%rd357];
	ld.param.u32 	%r436, [%rd357+32];
	ld.param.u64 	%rd201, [%rd357+56];
	ld.param.u32 	%r437, [%rd357+88];
	ld.param.u64 	%rd202, [%rd357+112];
	ld.param.u32 	%r438, [%rd357+144];
	ld.param.u32 	%r439, [%rd357+172];
	ld.param.v2.u32 	{%r1400, %r1401}, [%rd357+176];
	setp.le.s32 	%p390, %r1400, %r319;
	setp.le.s32 	%p391, %r1401, %r226;
	setp.le.s32 	%p392, %r439, %r2401;
	or.pred  	%p393, %p390, %p391;
	or.pred  	%p394, %p281, %p393;
	or.pred  	%p395, %p392, %p394;
	@%p395 bra 	$L__BB13_248;
	bra.uni 	$L__BB13_247;

$L__BB13_248:
	add.s32 	%r2189, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2189, %r226};
	add.s32 	%r2190, %r208, 13;
	st.local.v2.u32 	[%rd30+8], {%r2190, %r1400};
	st.local.v2.u32 	[%rd30+16], {%r1401, %r439};
	mov.u64 	%rd1667, $str$2;
	cvta.global.u64 	%rd1668, %rd1667;
	{ // callseq 398, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1668;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1405, [retval0+0];
	} // callseq 398
	bra.uni 	$L__BB13_249;

$L__BB13_247:
	add.s32 	%r2402, %r208, 13;
	cvta.to.global.u64 	%rd1660, %rd201;
	mul.wide.s32 	%rd1661, %r438, %r2402;
	add.s64 	%rd1651, %rd202, %rd1661;
	// begin inline asm
	{ atom.add.f64 %fd3691,[%rd1651],%fd69; }

	// end inline asm
	add.s64 	%rd1652, %rd1651, 8;
	// begin inline asm
	{ atom.add.f64 %fd3693,[%rd1652],%fd68; }

	// end inline asm
	add.s64 	%rd1653, %rd1651, 16;
	// begin inline asm
	{ atom.add.f64 %fd3695,[%rd1653],%fd67; }

	// end inline asm
	add.s64 	%rd1654, %rd1651, 24;
	// begin inline asm
	{ atom.add.f64 %fd3697,[%rd1654],%fd45; }

	// end inline asm
	add.s64 	%rd1655, %rd1651, 32;
	// begin inline asm
	{ atom.add.f64 %fd3699,[%rd1655],%fd44; }

	// end inline asm
	add.s64 	%rd1656, %rd1651, 40;
	// begin inline asm
	{ atom.add.f64 %fd3701,[%rd1656],%fd43; }

	// end inline asm
	add.s64 	%rd1657, %rd1651, 48;
	// begin inline asm
	{ atom.add.f64 %fd3703,[%rd1657],%fd21; }

	// end inline asm
	add.s64 	%rd1658, %rd1651, 56;
	// begin inline asm
	{ atom.add.f64 %fd3705,[%rd1658],%fd20; }

	// end inline asm
	add.s64 	%rd1659, %rd1651, 64;
	// begin inline asm
	{ atom.add.f64 %fd3707,[%rd1659],%fd19; }

	// end inline asm
	mul.wide.s32 	%rd1662, %r436, %r2402;
	cvta.to.global.u64 	%rd1663, %rd200;
	add.s64 	%rd1664, %rd1663, %rd1662;
	mul.wide.s32 	%rd1665, %r437, %r2402;
	add.s64 	%rd1666, %rd1660, %rd1665;
	add.s32 	%r2187, %r209, 3;
	st.global.u32 	[%rd1664], %r2187;
	add.s32 	%r2188, %r210, 1;
	st.global.u32 	[%rd1666], %r2188;

$L__BB13_249:
	add.s32 	%r2403, %r208, 14;
	ld.param.u64 	%rd203, [%rd357];
	ld.param.u32 	%r443, [%rd357+32];
	ld.param.u64 	%rd204, [%rd357+56];
	ld.param.u32 	%r444, [%rd357+88];
	ld.param.u64 	%rd205, [%rd357+112];
	ld.param.u32 	%r445, [%rd357+144];
	ld.param.u32 	%r446, [%rd357+172];
	ld.param.v2.u32 	{%r1406, %r1407}, [%rd357+176];
	setp.le.s32 	%p397, %r1406, %r319;
	setp.le.s32 	%p398, %r1407, %r235;
	setp.le.s32 	%p399, %r446, %r2403;
	or.pred  	%p400, %p397, %p398;
	or.pred  	%p401, %p288, %p400;
	or.pred  	%p402, %p399, %p401;
	@%p402 bra 	$L__BB13_251;
	bra.uni 	$L__BB13_250;

$L__BB13_251:
	add.s32 	%r2193, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2193, %r235};
	add.s32 	%r2194, %r208, 14;
	st.local.v2.u32 	[%rd30+8], {%r2194, %r1406};
	st.local.v2.u32 	[%rd30+16], {%r1407, %r446};
	mov.u64 	%rd1686, $str$2;
	cvta.global.u64 	%rd1687, %rd1686;
	{ // callseq 399, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1687;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1411, [retval0+0];
	} // callseq 399
	bra.uni 	$L__BB13_252;

$L__BB13_250:
	add.s32 	%r2404, %r208, 14;
	cvta.to.global.u64 	%rd1679, %rd204;
	mul.wide.s32 	%rd1680, %r445, %r2404;
	add.s64 	%rd1670, %rd205, %rd1680;
	// begin inline asm
	{ atom.add.f64 %fd3709,[%rd1670],%fd66; }

	// end inline asm
	add.s64 	%rd1671, %rd1670, 8;
	// begin inline asm
	{ atom.add.f64 %fd3711,[%rd1671],%fd65; }

	// end inline asm
	add.s64 	%rd1672, %rd1670, 16;
	// begin inline asm
	{ atom.add.f64 %fd3713,[%rd1672],%fd64; }

	// end inline asm
	add.s64 	%rd1673, %rd1670, 24;
	// begin inline asm
	{ atom.add.f64 %fd3715,[%rd1673],%fd42; }

	// end inline asm
	add.s64 	%rd1674, %rd1670, 32;
	// begin inline asm
	{ atom.add.f64 %fd3717,[%rd1674],%fd41; }

	// end inline asm
	add.s64 	%rd1675, %rd1670, 40;
	// begin inline asm
	{ atom.add.f64 %fd3719,[%rd1675],%fd40; }

	// end inline asm
	add.s64 	%rd1676, %rd1670, 48;
	// begin inline asm
	{ atom.add.f64 %fd3721,[%rd1676],%fd18; }

	// end inline asm
	add.s64 	%rd1677, %rd1670, 56;
	// begin inline asm
	{ atom.add.f64 %fd3723,[%rd1677],%fd17; }

	// end inline asm
	add.s64 	%rd1678, %rd1670, 64;
	// begin inline asm
	{ atom.add.f64 %fd3725,[%rd1678],%fd16; }

	// end inline asm
	mul.wide.s32 	%rd1681, %r443, %r2404;
	cvta.to.global.u64 	%rd1682, %rd203;
	add.s64 	%rd1683, %rd1682, %rd1681;
	mul.wide.s32 	%rd1684, %r444, %r2404;
	add.s64 	%rd1685, %rd1679, %rd1684;
	add.s32 	%r2191, %r209, 3;
	st.global.u32 	[%rd1683], %r2191;
	add.s32 	%r2192, %r210, 2;
	st.global.u32 	[%rd1685], %r2192;

$L__BB13_252:
	add.s32 	%r2405, %r208, 15;
	ld.param.u64 	%rd206, [%rd357];
	ld.param.u32 	%r450, [%rd357+32];
	ld.param.u64 	%rd207, [%rd357+56];
	ld.param.u32 	%r451, [%rd357+88];
	ld.param.u64 	%rd208, [%rd357+112];
	ld.param.u32 	%r452, [%rd357+144];
	ld.param.u32 	%r453, [%rd357+172];
	ld.param.v2.u32 	{%r1412, %r1413}, [%rd357+176];
	setp.le.s32 	%p404, %r1412, %r319;
	setp.le.s32 	%p405, %r1413, %r244;
	setp.le.s32 	%p406, %r453, %r2405;
	or.pred  	%p407, %p404, %p405;
	or.pred  	%p408, %p295, %p407;
	or.pred  	%p409, %p406, %p408;
	@%p409 bra 	$L__BB13_254;
	bra.uni 	$L__BB13_253;

$L__BB13_254:
	add.s32 	%r2197, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2197, %r244};
	add.s32 	%r2198, %r208, 15;
	st.local.v2.u32 	[%rd30+8], {%r2198, %r1412};
	st.local.v2.u32 	[%rd30+16], {%r1413, %r453};
	mov.u64 	%rd1705, $str$2;
	cvta.global.u64 	%rd1706, %rd1705;
	{ // callseq 400, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1706;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1417, [retval0+0];
	} // callseq 400
	bra.uni 	$L__BB13_255;

$L__BB13_253:
	add.s32 	%r2406, %r208, 15;
	cvta.to.global.u64 	%rd1698, %rd207;
	mul.wide.s32 	%rd1699, %r452, %r2406;
	add.s64 	%rd1689, %rd208, %rd1699;
	// begin inline asm
	{ atom.add.f64 %fd3727,[%rd1689],%fd63; }

	// end inline asm
	add.s64 	%rd1690, %rd1689, 8;
	// begin inline asm
	{ atom.add.f64 %fd3729,[%rd1690],%fd62; }

	// end inline asm
	add.s64 	%rd1691, %rd1689, 16;
	// begin inline asm
	{ atom.add.f64 %fd3731,[%rd1691],%fd61; }

	// end inline asm
	add.s64 	%rd1692, %rd1689, 24;
	// begin inline asm
	{ atom.add.f64 %fd3733,[%rd1692],%fd39; }

	// end inline asm
	add.s64 	%rd1693, %rd1689, 32;
	// begin inline asm
	{ atom.add.f64 %fd3735,[%rd1693],%fd38; }

	// end inline asm
	add.s64 	%rd1694, %rd1689, 40;
	// begin inline asm
	{ atom.add.f64 %fd3737,[%rd1694],%fd37; }

	// end inline asm
	add.s64 	%rd1695, %rd1689, 48;
	// begin inline asm
	{ atom.add.f64 %fd3739,[%rd1695],%fd15; }

	// end inline asm
	add.s64 	%rd1696, %rd1689, 56;
	// begin inline asm
	{ atom.add.f64 %fd3741,[%rd1696],%fd14; }

	// end inline asm
	add.s64 	%rd1697, %rd1689, 64;
	// begin inline asm
	{ atom.add.f64 %fd3743,[%rd1697],%fd13; }

	// end inline asm
	mul.wide.s32 	%rd1700, %r450, %r2406;
	cvta.to.global.u64 	%rd1701, %rd206;
	add.s64 	%rd1702, %rd1701, %rd1700;
	mul.wide.s32 	%rd1703, %r451, %r2406;
	add.s64 	%rd1704, %rd1698, %rd1703;
	add.s32 	%r2195, %r209, 3;
	st.global.u32 	[%rd1702], %r2195;
	add.s32 	%r2196, %r210, 3;
	st.global.u32 	[%rd1704], %r2196;

$L__BB13_255:
	add.s32 	%r2407, %r208, 15;
	ld.param.u64 	%rd210, [%rd357+120];
	ld.param.u32 	%r457, [%rd357+144];
	ld.param.u32 	%r458, [%rd357+172];
	ld.param.v2.u32 	{%r1418, %r1419}, [%rd357+176];
	setp.le.s32 	%p411, %r1418, %r319;
	setp.le.s32 	%p412, %r1419, %r244;
	setp.le.s32 	%p413, %r458, %r2407;
	or.pred  	%p414, %p411, %p412;
	or.pred  	%p415, %p295, %p414;
	or.pred  	%p416, %p413, %p415;
	mov.f64 	%fd9217, 0d0000000000000000;
	mov.f64 	%fd9218, 0d0000000000000000;
	mov.f64 	%fd9219, 0d0000000000000000;
	mov.f64 	%fd9220, 0d0000000000000000;
	mov.f64 	%fd9221, 0d0000000000000000;
	mov.f64 	%fd9222, 0d0000000000000000;
	mov.f64 	%fd9223, 0d0000000000000000;
	mov.f64 	%fd9224, 0d0000000000000000;
	mov.f64 	%fd9225, 0d0000000000000000;
	@%p416 bra 	$L__BB13_258;
	bra.uni 	$L__BB13_256;

$L__BB13_258:
	add.s32 	%r2199, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2199, %r244};
	add.s32 	%r2200, %r208, 15;
	st.local.v2.u32 	[%rd30+8], {%r2200, %r1418};
	st.local.v2.u32 	[%rd30+16], {%r1419, %r458};
	mov.u64 	%rd1711, $str$2;
	cvta.global.u64 	%rd1712, %rd1711;
	{ // callseq 401, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1712;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1423, [retval0+0];
	} // callseq 401
	bra.uni 	$L__BB13_259;

$L__BB13_256:
	setp.eq.s64 	%p417, %rd210, 0;
	@%p417 bra 	$L__BB13_259;

	add.s32 	%r2408, %r208, 15;
	cvta.to.global.u64 	%rd1708, %rd210;
	mul.wide.s32 	%rd1709, %r457, %r2408;
	add.s64 	%rd1710, %rd1708, %rd1709;
	ld.global.f64 	%fd3754, [%rd1710];
	add.f64 	%fd9217, %fd3754, 0d0000000000000000;
	ld.global.f64 	%fd3755, [%rd1710+8];
	add.f64 	%fd9218, %fd3755, 0d0000000000000000;
	ld.global.f64 	%fd3756, [%rd1710+16];
	add.f64 	%fd9219, %fd3756, 0d0000000000000000;
	ld.global.f64 	%fd3757, [%rd1710+24];
	add.f64 	%fd9220, %fd3757, 0d0000000000000000;
	ld.global.f64 	%fd3758, [%rd1710+32];
	add.f64 	%fd9221, %fd3758, 0d0000000000000000;
	ld.global.f64 	%fd3759, [%rd1710+40];
	add.f64 	%fd9222, %fd3759, 0d0000000000000000;
	ld.global.f64 	%fd3760, [%rd1710+48];
	add.f64 	%fd9223, %fd3760, 0d0000000000000000;
	ld.global.f64 	%fd3761, [%rd1710+56];
	add.f64 	%fd9224, %fd3761, 0d0000000000000000;
	ld.global.f64 	%fd3762, [%rd1710+64];
	add.f64 	%fd9225, %fd3762, 0d0000000000000000;

$L__BB13_259:
	add.s32 	%r2409, %r208, 14;
	add.f64 	%fd595, %fd9225, 0d0000000000000000;
	add.f64 	%fd596, %fd9224, 0d0000000000000000;
	add.f64 	%fd597, %fd9223, 0d0000000000000000;
	add.f64 	%fd598, %fd9222, 0d0000000000000000;
	add.f64 	%fd599, %fd9221, 0d0000000000000000;
	add.f64 	%fd600, %fd9220, 0d0000000000000000;
	add.f64 	%fd601, %fd9219, 0d0000000000000000;
	add.f64 	%fd602, %fd9218, 0d0000000000000000;
	add.f64 	%fd603, %fd9217, 0d0000000000000000;
	ld.param.u64 	%rd211, [%rd357+120];
	ld.param.u32 	%r462, [%rd357+144];
	ld.param.u32 	%r463, [%rd357+172];
	ld.param.v2.u32 	{%r1424, %r1425}, [%rd357+176];
	setp.le.s32 	%p418, %r1424, %r319;
	setp.le.s32 	%p419, %r1425, %r235;
	setp.le.s32 	%p420, %r463, %r2409;
	or.pred  	%p421, %p418, %p419;
	or.pred  	%p423, %p288, %p421;
	or.pred  	%p424, %p420, %p423;
	mov.f64 	%fd9226, 0d0000000000000000;
	mov.f64 	%fd9227, 0d0000000000000000;
	mov.f64 	%fd9228, 0d0000000000000000;
	mov.f64 	%fd9229, 0d0000000000000000;
	mov.f64 	%fd9230, 0d0000000000000000;
	mov.f64 	%fd9231, 0d0000000000000000;
	mov.f64 	%fd9232, 0d0000000000000000;
	mov.f64 	%fd9233, 0d0000000000000000;
	mov.f64 	%fd9234, 0d0000000000000000;
	@%p424 bra 	$L__BB13_262;
	bra.uni 	$L__BB13_260;

$L__BB13_262:
	add.s32 	%r2201, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2201, %r235};
	add.s32 	%r2202, %r208, 14;
	st.local.v2.u32 	[%rd30+8], {%r2202, %r1424};
	st.local.v2.u32 	[%rd30+16], {%r1425, %r463};
	mov.u64 	%rd1717, $str$2;
	cvta.global.u64 	%rd1718, %rd1717;
	{ // callseq 402, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1718;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1429, [retval0+0];
	} // callseq 402
	bra.uni 	$L__BB13_263;

$L__BB13_260:
	setp.eq.s64 	%p425, %rd211, 0;
	@%p425 bra 	$L__BB13_263;

	add.s32 	%r2410, %r208, 14;
	cvta.to.global.u64 	%rd1714, %rd211;
	mul.wide.s32 	%rd1715, %r462, %r2410;
	add.s64 	%rd1716, %rd1714, %rd1715;
	ld.global.f64 	%fd3781, [%rd1716];
	add.f64 	%fd9226, %fd3781, 0d0000000000000000;
	ld.global.f64 	%fd3782, [%rd1716+8];
	add.f64 	%fd9227, %fd3782, 0d0000000000000000;
	ld.global.f64 	%fd3783, [%rd1716+16];
	add.f64 	%fd9228, %fd3783, 0d0000000000000000;
	ld.global.f64 	%fd3784, [%rd1716+24];
	add.f64 	%fd9229, %fd3784, 0d0000000000000000;
	ld.global.f64 	%fd3785, [%rd1716+32];
	add.f64 	%fd9230, %fd3785, 0d0000000000000000;
	ld.global.f64 	%fd3786, [%rd1716+40];
	add.f64 	%fd9231, %fd3786, 0d0000000000000000;
	ld.global.f64 	%fd3787, [%rd1716+48];
	add.f64 	%fd9232, %fd3787, 0d0000000000000000;
	ld.global.f64 	%fd3788, [%rd1716+56];
	add.f64 	%fd9233, %fd3788, 0d0000000000000000;
	ld.global.f64 	%fd3789, [%rd1716+64];
	add.f64 	%fd9234, %fd3789, 0d0000000000000000;

$L__BB13_263:
	add.s32 	%r2411, %r208, 13;
	add.f64 	%fd622, %fd9234, 0d0000000000000000;
	add.f64 	%fd623, %fd9233, 0d0000000000000000;
	add.f64 	%fd624, %fd9232, 0d0000000000000000;
	add.f64 	%fd625, %fd9231, 0d0000000000000000;
	add.f64 	%fd626, %fd9230, 0d0000000000000000;
	add.f64 	%fd627, %fd9229, 0d0000000000000000;
	add.f64 	%fd628, %fd9228, 0d0000000000000000;
	add.f64 	%fd629, %fd9227, 0d0000000000000000;
	add.f64 	%fd630, %fd9226, 0d0000000000000000;
	ld.param.u64 	%rd212, [%rd357+120];
	ld.param.u32 	%r467, [%rd357+144];
	ld.param.u32 	%r468, [%rd357+172];
	ld.param.v2.u32 	{%r1430, %r1431}, [%rd357+176];
	setp.le.s32 	%p426, %r1430, %r319;
	setp.le.s32 	%p427, %r1431, %r226;
	setp.le.s32 	%p428, %r468, %r2411;
	or.pred  	%p429, %p426, %p427;
	or.pred  	%p431, %p281, %p429;
	or.pred  	%p432, %p428, %p431;
	mov.f64 	%fd9235, 0d0000000000000000;
	mov.f64 	%fd9236, 0d0000000000000000;
	mov.f64 	%fd9237, 0d0000000000000000;
	mov.f64 	%fd9238, 0d0000000000000000;
	mov.f64 	%fd9239, 0d0000000000000000;
	mov.f64 	%fd9240, 0d0000000000000000;
	mov.f64 	%fd9241, 0d0000000000000000;
	mov.f64 	%fd9242, 0d0000000000000000;
	mov.f64 	%fd9243, 0d0000000000000000;
	@%p432 bra 	$L__BB13_266;
	bra.uni 	$L__BB13_264;

$L__BB13_266:
	add.s32 	%r2203, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2203, %r226};
	add.s32 	%r2204, %r208, 13;
	st.local.v2.u32 	[%rd30+8], {%r2204, %r1430};
	st.local.v2.u32 	[%rd30+16], {%r1431, %r468};
	mov.u64 	%rd1723, $str$2;
	cvta.global.u64 	%rd1724, %rd1723;
	{ // callseq 403, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1724;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1435, [retval0+0];
	} // callseq 403
	bra.uni 	$L__BB13_267;

$L__BB13_264:
	setp.eq.s64 	%p433, %rd212, 0;
	@%p433 bra 	$L__BB13_267;

	add.s32 	%r2412, %r208, 13;
	cvta.to.global.u64 	%rd1720, %rd212;
	mul.wide.s32 	%rd1721, %r467, %r2412;
	add.s64 	%rd1722, %rd1720, %rd1721;
	ld.global.f64 	%fd3808, [%rd1722];
	add.f64 	%fd9235, %fd3808, 0d0000000000000000;
	ld.global.f64 	%fd3809, [%rd1722+8];
	add.f64 	%fd9236, %fd3809, 0d0000000000000000;
	ld.global.f64 	%fd3810, [%rd1722+16];
	add.f64 	%fd9237, %fd3810, 0d0000000000000000;
	ld.global.f64 	%fd3811, [%rd1722+24];
	add.f64 	%fd9238, %fd3811, 0d0000000000000000;
	ld.global.f64 	%fd3812, [%rd1722+32];
	add.f64 	%fd9239, %fd3812, 0d0000000000000000;
	ld.global.f64 	%fd3813, [%rd1722+40];
	add.f64 	%fd9240, %fd3813, 0d0000000000000000;
	ld.global.f64 	%fd3814, [%rd1722+48];
	add.f64 	%fd9241, %fd3814, 0d0000000000000000;
	ld.global.f64 	%fd3815, [%rd1722+56];
	add.f64 	%fd9242, %fd3815, 0d0000000000000000;
	ld.global.f64 	%fd3816, [%rd1722+64];
	add.f64 	%fd9243, %fd3816, 0d0000000000000000;

$L__BB13_267:
	add.s32 	%r2413, %r208, 12;
	add.f64 	%fd649, %fd9243, 0d0000000000000000;
	add.f64 	%fd650, %fd9242, 0d0000000000000000;
	add.f64 	%fd651, %fd9241, 0d0000000000000000;
	add.f64 	%fd652, %fd9240, 0d0000000000000000;
	add.f64 	%fd653, %fd9239, 0d0000000000000000;
	add.f64 	%fd654, %fd9238, 0d0000000000000000;
	add.f64 	%fd655, %fd9237, 0d0000000000000000;
	add.f64 	%fd656, %fd9236, 0d0000000000000000;
	add.f64 	%fd657, %fd9235, 0d0000000000000000;
	ld.param.u64 	%rd213, [%rd357+120];
	ld.param.u32 	%r472, [%rd357+144];
	ld.param.u32 	%r473, [%rd357+172];
	ld.param.v2.u32 	{%r1436, %r1437}, [%rd357+176];
	setp.le.s32 	%p434, %r1436, %r319;
	setp.le.s32 	%p435, %r1437, %r210;
	setp.le.s32 	%p436, %r473, %r2413;
	or.pred  	%p437, %p434, %p435;
	or.pred  	%p439, %p274, %p437;
	or.pred  	%p440, %p436, %p439;
	mov.f64 	%fd9244, 0d0000000000000000;
	mov.f64 	%fd9245, 0d0000000000000000;
	mov.f64 	%fd9246, 0d0000000000000000;
	mov.f64 	%fd9247, 0d0000000000000000;
	mov.f64 	%fd9248, 0d0000000000000000;
	mov.f64 	%fd9249, 0d0000000000000000;
	mov.f64 	%fd9250, 0d0000000000000000;
	mov.f64 	%fd9251, 0d0000000000000000;
	mov.f64 	%fd9252, 0d0000000000000000;
	@%p440 bra 	$L__BB13_270;
	bra.uni 	$L__BB13_268;

$L__BB13_270:
	add.s32 	%r2205, %r209, 3;
	st.local.v2.u32 	[%rd30], {%r2205, %r210};
	add.s32 	%r2206, %r208, 12;
	st.local.v2.u32 	[%rd30+8], {%r2206, %r1436};
	st.local.v2.u32 	[%rd30+16], {%r1437, %r473};
	mov.u64 	%rd1729, $str$2;
	cvta.global.u64 	%rd1730, %rd1729;
	{ // callseq 404, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1730;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1441, [retval0+0];
	} // callseq 404
	bra.uni 	$L__BB13_271;

$L__BB13_268:
	setp.eq.s64 	%p441, %rd213, 0;
	@%p441 bra 	$L__BB13_271;

	add.s32 	%r2414, %r208, 12;
	cvta.to.global.u64 	%rd1726, %rd213;
	mul.wide.s32 	%rd1727, %r472, %r2414;
	add.s64 	%rd1728, %rd1726, %rd1727;
	ld.global.f64 	%fd3835, [%rd1728];
	add.f64 	%fd9244, %fd3835, 0d0000000000000000;
	ld.global.f64 	%fd3836, [%rd1728+8];
	add.f64 	%fd9245, %fd3836, 0d0000000000000000;
	ld.global.f64 	%fd3837, [%rd1728+16];
	add.f64 	%fd9246, %fd3837, 0d0000000000000000;
	ld.global.f64 	%fd3838, [%rd1728+24];
	add.f64 	%fd9247, %fd3838, 0d0000000000000000;
	ld.global.f64 	%fd3839, [%rd1728+32];
	add.f64 	%fd9248, %fd3839, 0d0000000000000000;
	ld.global.f64 	%fd3840, [%rd1728+40];
	add.f64 	%fd9249, %fd3840, 0d0000000000000000;
	ld.global.f64 	%fd3841, [%rd1728+48];
	add.f64 	%fd9250, %fd3841, 0d0000000000000000;
	ld.global.f64 	%fd3842, [%rd1728+56];
	add.f64 	%fd9251, %fd3842, 0d0000000000000000;
	ld.global.f64 	%fd3843, [%rd1728+64];
	add.f64 	%fd9252, %fd3843, 0d0000000000000000;

$L__BB13_271:
	add.s32 	%r2415, %r208, 11;
	add.f64 	%fd676, %fd9252, 0d0000000000000000;
	add.f64 	%fd677, %fd9251, 0d0000000000000000;
	add.f64 	%fd678, %fd9250, 0d0000000000000000;
	add.f64 	%fd679, %fd9249, 0d0000000000000000;
	add.f64 	%fd680, %fd9248, 0d0000000000000000;
	add.f64 	%fd681, %fd9247, 0d0000000000000000;
	add.f64 	%fd682, %fd9246, 0d0000000000000000;
	add.f64 	%fd683, %fd9245, 0d0000000000000000;
	add.f64 	%fd684, %fd9244, 0d0000000000000000;
	ld.param.u64 	%rd214, [%rd357+120];
	ld.param.u32 	%r477, [%rd357+144];
	ld.param.u32 	%r478, [%rd357+172];
	ld.param.v2.u32 	{%r1442, %r1443}, [%rd357+176];
	setp.le.s32 	%p442, %r1442, %r286;
	setp.le.s32 	%p443, %r1443, %r244;
	setp.le.s32 	%p444, %r478, %r2415;
	or.pred  	%p445, %p442, %p443;
	or.pred  	%p447, %p267, %p445;
	or.pred  	%p448, %p444, %p447;
	mov.f64 	%fd9253, 0d0000000000000000;
	mov.f64 	%fd9254, 0d0000000000000000;
	mov.f64 	%fd9255, 0d0000000000000000;
	mov.f64 	%fd9256, 0d0000000000000000;
	mov.f64 	%fd9257, 0d0000000000000000;
	mov.f64 	%fd9258, 0d0000000000000000;
	mov.f64 	%fd9259, 0d0000000000000000;
	mov.f64 	%fd9260, 0d0000000000000000;
	mov.f64 	%fd9261, 0d0000000000000000;
	@%p448 bra 	$L__BB13_274;
	bra.uni 	$L__BB13_272;

$L__BB13_274:
	add.s32 	%r2207, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2207, %r244};
	add.s32 	%r2208, %r208, 11;
	st.local.v2.u32 	[%rd30+8], {%r2208, %r1442};
	st.local.v2.u32 	[%rd30+16], {%r1443, %r478};
	mov.u64 	%rd1735, $str$2;
	cvta.global.u64 	%rd1736, %rd1735;
	{ // callseq 405, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1736;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1447, [retval0+0];
	} // callseq 405
	bra.uni 	$L__BB13_275;

$L__BB13_272:
	setp.eq.s64 	%p449, %rd214, 0;
	@%p449 bra 	$L__BB13_275;

	add.s32 	%r2416, %r208, 11;
	cvta.to.global.u64 	%rd1732, %rd214;
	mul.wide.s32 	%rd1733, %r477, %r2416;
	add.s64 	%rd1734, %rd1732, %rd1733;
	ld.global.f64 	%fd3862, [%rd1734];
	add.f64 	%fd9253, %fd3862, 0d0000000000000000;
	ld.global.f64 	%fd3863, [%rd1734+8];
	add.f64 	%fd9254, %fd3863, 0d0000000000000000;
	ld.global.f64 	%fd3864, [%rd1734+16];
	add.f64 	%fd9255, %fd3864, 0d0000000000000000;
	ld.global.f64 	%fd3865, [%rd1734+24];
	add.f64 	%fd9256, %fd3865, 0d0000000000000000;
	ld.global.f64 	%fd3866, [%rd1734+32];
	add.f64 	%fd9257, %fd3866, 0d0000000000000000;
	ld.global.f64 	%fd3867, [%rd1734+40];
	add.f64 	%fd9258, %fd3867, 0d0000000000000000;
	ld.global.f64 	%fd3868, [%rd1734+48];
	add.f64 	%fd9259, %fd3868, 0d0000000000000000;
	ld.global.f64 	%fd3869, [%rd1734+56];
	add.f64 	%fd9260, %fd3869, 0d0000000000000000;
	ld.global.f64 	%fd3870, [%rd1734+64];
	add.f64 	%fd9261, %fd3870, 0d0000000000000000;

$L__BB13_275:
	add.s32 	%r2417, %r208, 10;
	add.f64 	%fd703, %fd9261, 0d0000000000000000;
	add.f64 	%fd704, %fd9260, 0d0000000000000000;
	add.f64 	%fd705, %fd9259, 0d0000000000000000;
	add.f64 	%fd706, %fd9258, 0d0000000000000000;
	add.f64 	%fd707, %fd9257, 0d0000000000000000;
	add.f64 	%fd708, %fd9256, 0d0000000000000000;
	add.f64 	%fd709, %fd9255, 0d0000000000000000;
	add.f64 	%fd710, %fd9254, 0d0000000000000000;
	add.f64 	%fd711, %fd9253, 0d0000000000000000;
	ld.param.u64 	%rd215, [%rd357+120];
	ld.param.u32 	%r482, [%rd357+144];
	ld.param.u32 	%r483, [%rd357+172];
	ld.param.v2.u32 	{%r1448, %r1449}, [%rd357+176];
	setp.le.s32 	%p450, %r1448, %r286;
	setp.le.s32 	%p451, %r1449, %r235;
	setp.le.s32 	%p452, %r483, %r2417;
	or.pred  	%p453, %p450, %p451;
	or.pred  	%p455, %p260, %p453;
	or.pred  	%p456, %p452, %p455;
	mov.f64 	%fd9262, 0d0000000000000000;
	mov.f64 	%fd9263, 0d0000000000000000;
	mov.f64 	%fd9264, 0d0000000000000000;
	mov.f64 	%fd9265, 0d0000000000000000;
	mov.f64 	%fd9266, 0d0000000000000000;
	mov.f64 	%fd9267, 0d0000000000000000;
	mov.f64 	%fd9268, 0d0000000000000000;
	mov.f64 	%fd9269, 0d0000000000000000;
	mov.f64 	%fd9270, 0d0000000000000000;
	@%p456 bra 	$L__BB13_278;
	bra.uni 	$L__BB13_276;

$L__BB13_278:
	add.s32 	%r2209, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2209, %r235};
	add.s32 	%r2210, %r208, 10;
	st.local.v2.u32 	[%rd30+8], {%r2210, %r1448};
	st.local.v2.u32 	[%rd30+16], {%r1449, %r483};
	mov.u64 	%rd1741, $str$2;
	cvta.global.u64 	%rd1742, %rd1741;
	{ // callseq 406, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1742;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1453, [retval0+0];
	} // callseq 406
	bra.uni 	$L__BB13_279;

$L__BB13_276:
	setp.eq.s64 	%p457, %rd215, 0;
	@%p457 bra 	$L__BB13_279;

	add.s32 	%r2418, %r208, 10;
	cvta.to.global.u64 	%rd1738, %rd215;
	mul.wide.s32 	%rd1739, %r482, %r2418;
	add.s64 	%rd1740, %rd1738, %rd1739;
	ld.global.f64 	%fd3889, [%rd1740];
	add.f64 	%fd9262, %fd3889, 0d0000000000000000;
	ld.global.f64 	%fd3890, [%rd1740+8];
	add.f64 	%fd9263, %fd3890, 0d0000000000000000;
	ld.global.f64 	%fd3891, [%rd1740+16];
	add.f64 	%fd9264, %fd3891, 0d0000000000000000;
	ld.global.f64 	%fd3892, [%rd1740+24];
	add.f64 	%fd9265, %fd3892, 0d0000000000000000;
	ld.global.f64 	%fd3893, [%rd1740+32];
	add.f64 	%fd9266, %fd3893, 0d0000000000000000;
	ld.global.f64 	%fd3894, [%rd1740+40];
	add.f64 	%fd9267, %fd3894, 0d0000000000000000;
	ld.global.f64 	%fd3895, [%rd1740+48];
	add.f64 	%fd9268, %fd3895, 0d0000000000000000;
	ld.global.f64 	%fd3896, [%rd1740+56];
	add.f64 	%fd9269, %fd3896, 0d0000000000000000;
	ld.global.f64 	%fd3897, [%rd1740+64];
	add.f64 	%fd9270, %fd3897, 0d0000000000000000;

$L__BB13_279:
	add.s32 	%r2419, %r208, 9;
	add.f64 	%fd730, %fd9270, 0d0000000000000000;
	add.f64 	%fd731, %fd9269, 0d0000000000000000;
	add.f64 	%fd732, %fd9268, 0d0000000000000000;
	add.f64 	%fd733, %fd9267, 0d0000000000000000;
	add.f64 	%fd734, %fd9266, 0d0000000000000000;
	add.f64 	%fd735, %fd9265, 0d0000000000000000;
	add.f64 	%fd736, %fd9264, 0d0000000000000000;
	add.f64 	%fd737, %fd9263, 0d0000000000000000;
	add.f64 	%fd738, %fd9262, 0d0000000000000000;
	ld.param.u64 	%rd216, [%rd357+120];
	ld.param.u32 	%r487, [%rd357+144];
	ld.param.u32 	%r488, [%rd357+172];
	ld.param.v2.u32 	{%r1454, %r1455}, [%rd357+176];
	setp.le.s32 	%p458, %r1454, %r286;
	setp.le.s32 	%p459, %r1455, %r226;
	setp.le.s32 	%p460, %r488, %r2419;
	or.pred  	%p461, %p458, %p459;
	or.pred  	%p463, %p253, %p461;
	or.pred  	%p464, %p460, %p463;
	mov.f64 	%fd9271, 0d0000000000000000;
	mov.f64 	%fd9272, 0d0000000000000000;
	mov.f64 	%fd9273, 0d0000000000000000;
	mov.f64 	%fd9274, 0d0000000000000000;
	mov.f64 	%fd9275, 0d0000000000000000;
	mov.f64 	%fd9276, 0d0000000000000000;
	mov.f64 	%fd9277, 0d0000000000000000;
	mov.f64 	%fd9278, 0d0000000000000000;
	mov.f64 	%fd9279, 0d0000000000000000;
	@%p464 bra 	$L__BB13_282;
	bra.uni 	$L__BB13_280;

$L__BB13_282:
	add.s32 	%r2211, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2211, %r226};
	add.s32 	%r2212, %r208, 9;
	st.local.v2.u32 	[%rd30+8], {%r2212, %r1454};
	st.local.v2.u32 	[%rd30+16], {%r1455, %r488};
	mov.u64 	%rd1747, $str$2;
	cvta.global.u64 	%rd1748, %rd1747;
	{ // callseq 407, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1748;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1459, [retval0+0];
	} // callseq 407
	bra.uni 	$L__BB13_283;

$L__BB13_280:
	setp.eq.s64 	%p465, %rd216, 0;
	@%p465 bra 	$L__BB13_283;

	add.s32 	%r2420, %r208, 9;
	cvta.to.global.u64 	%rd1744, %rd216;
	mul.wide.s32 	%rd1745, %r487, %r2420;
	add.s64 	%rd1746, %rd1744, %rd1745;
	ld.global.f64 	%fd3916, [%rd1746];
	add.f64 	%fd9271, %fd3916, 0d0000000000000000;
	ld.global.f64 	%fd3917, [%rd1746+8];
	add.f64 	%fd9272, %fd3917, 0d0000000000000000;
	ld.global.f64 	%fd3918, [%rd1746+16];
	add.f64 	%fd9273, %fd3918, 0d0000000000000000;
	ld.global.f64 	%fd3919, [%rd1746+24];
	add.f64 	%fd9274, %fd3919, 0d0000000000000000;
	ld.global.f64 	%fd3920, [%rd1746+32];
	add.f64 	%fd9275, %fd3920, 0d0000000000000000;
	ld.global.f64 	%fd3921, [%rd1746+40];
	add.f64 	%fd9276, %fd3921, 0d0000000000000000;
	ld.global.f64 	%fd3922, [%rd1746+48];
	add.f64 	%fd9277, %fd3922, 0d0000000000000000;
	ld.global.f64 	%fd3923, [%rd1746+56];
	add.f64 	%fd9278, %fd3923, 0d0000000000000000;
	ld.global.f64 	%fd3924, [%rd1746+64];
	add.f64 	%fd9279, %fd3924, 0d0000000000000000;

$L__BB13_283:
	add.s32 	%r2421, %r208, 8;
	add.f64 	%fd757, %fd9279, 0d0000000000000000;
	add.f64 	%fd758, %fd9278, 0d0000000000000000;
	add.f64 	%fd759, %fd9277, 0d0000000000000000;
	add.f64 	%fd760, %fd9276, 0d0000000000000000;
	add.f64 	%fd761, %fd9275, 0d0000000000000000;
	add.f64 	%fd762, %fd9274, 0d0000000000000000;
	add.f64 	%fd763, %fd9273, 0d0000000000000000;
	add.f64 	%fd764, %fd9272, 0d0000000000000000;
	add.f64 	%fd765, %fd9271, 0d0000000000000000;
	ld.param.u64 	%rd217, [%rd357+120];
	ld.param.u32 	%r492, [%rd357+144];
	ld.param.u32 	%r493, [%rd357+172];
	ld.param.v2.u32 	{%r1460, %r1461}, [%rd357+176];
	setp.le.s32 	%p466, %r1460, %r286;
	setp.le.s32 	%p467, %r1461, %r210;
	setp.le.s32 	%p468, %r493, %r2421;
	or.pred  	%p469, %p466, %p467;
	or.pred  	%p471, %p246, %p469;
	or.pred  	%p472, %p468, %p471;
	mov.f64 	%fd9280, 0d0000000000000000;
	mov.f64 	%fd9281, 0d0000000000000000;
	mov.f64 	%fd9282, 0d0000000000000000;
	mov.f64 	%fd9283, 0d0000000000000000;
	mov.f64 	%fd9284, 0d0000000000000000;
	mov.f64 	%fd9285, 0d0000000000000000;
	mov.f64 	%fd9286, 0d0000000000000000;
	mov.f64 	%fd9287, 0d0000000000000000;
	mov.f64 	%fd9288, 0d0000000000000000;
	@%p472 bra 	$L__BB13_286;
	bra.uni 	$L__BB13_284;

$L__BB13_286:
	add.s32 	%r2213, %r209, 2;
	st.local.v2.u32 	[%rd30], {%r2213, %r210};
	add.s32 	%r2214, %r208, 8;
	st.local.v2.u32 	[%rd30+8], {%r2214, %r1460};
	st.local.v2.u32 	[%rd30+16], {%r1461, %r493};
	mov.u64 	%rd1753, $str$2;
	cvta.global.u64 	%rd1754, %rd1753;
	{ // callseq 408, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1754;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1465, [retval0+0];
	} // callseq 408
	bra.uni 	$L__BB13_287;

$L__BB13_284:
	setp.eq.s64 	%p473, %rd217, 0;
	@%p473 bra 	$L__BB13_287;

	add.s32 	%r2422, %r208, 8;
	cvta.to.global.u64 	%rd1750, %rd217;
	mul.wide.s32 	%rd1751, %r492, %r2422;
	add.s64 	%rd1752, %rd1750, %rd1751;
	ld.global.f64 	%fd3943, [%rd1752];
	add.f64 	%fd9280, %fd3943, 0d0000000000000000;
	ld.global.f64 	%fd3944, [%rd1752+8];
	add.f64 	%fd9281, %fd3944, 0d0000000000000000;
	ld.global.f64 	%fd3945, [%rd1752+16];
	add.f64 	%fd9282, %fd3945, 0d0000000000000000;
	ld.global.f64 	%fd3946, [%rd1752+24];
	add.f64 	%fd9283, %fd3946, 0d0000000000000000;
	ld.global.f64 	%fd3947, [%rd1752+32];
	add.f64 	%fd9284, %fd3947, 0d0000000000000000;
	ld.global.f64 	%fd3948, [%rd1752+40];
	add.f64 	%fd9285, %fd3948, 0d0000000000000000;
	ld.global.f64 	%fd3949, [%rd1752+48];
	add.f64 	%fd9286, %fd3949, 0d0000000000000000;
	ld.global.f64 	%fd3950, [%rd1752+56];
	add.f64 	%fd9287, %fd3950, 0d0000000000000000;
	ld.global.f64 	%fd3951, [%rd1752+64];
	add.f64 	%fd9288, %fd3951, 0d0000000000000000;

$L__BB13_287:
	add.s32 	%r2423, %r208, 7;
	add.f64 	%fd784, %fd9288, 0d0000000000000000;
	add.f64 	%fd785, %fd9287, 0d0000000000000000;
	add.f64 	%fd786, %fd9286, 0d0000000000000000;
	add.f64 	%fd787, %fd9285, 0d0000000000000000;
	add.f64 	%fd788, %fd9284, 0d0000000000000000;
	add.f64 	%fd789, %fd9283, 0d0000000000000000;
	add.f64 	%fd790, %fd9282, 0d0000000000000000;
	add.f64 	%fd791, %fd9281, 0d0000000000000000;
	add.f64 	%fd792, %fd9280, 0d0000000000000000;
	ld.param.u64 	%rd218, [%rd357+120];
	ld.param.u32 	%r497, [%rd357+144];
	ld.param.u32 	%r498, [%rd357+172];
	ld.param.v2.u32 	{%r1466, %r1467}, [%rd357+176];
	setp.le.s32 	%p474, %r1466, %r253;
	setp.le.s32 	%p475, %r1467, %r244;
	setp.le.s32 	%p476, %r498, %r2423;
	or.pred  	%p477, %p474, %p475;
	or.pred  	%p479, %p239, %p477;
	or.pred  	%p480, %p476, %p479;
	mov.f64 	%fd9289, 0d0000000000000000;
	mov.f64 	%fd9290, 0d0000000000000000;
	mov.f64 	%fd9291, 0d0000000000000000;
	mov.f64 	%fd9292, 0d0000000000000000;
	mov.f64 	%fd9293, 0d0000000000000000;
	mov.f64 	%fd9294, 0d0000000000000000;
	mov.f64 	%fd9295, 0d0000000000000000;
	mov.f64 	%fd9296, 0d0000000000000000;
	mov.f64 	%fd9297, 0d0000000000000000;
	@%p480 bra 	$L__BB13_290;
	bra.uni 	$L__BB13_288;

$L__BB13_290:
	add.s32 	%r2215, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2215, %r244};
	add.s32 	%r2216, %r208, 7;
	st.local.v2.u32 	[%rd30+8], {%r2216, %r1466};
	st.local.v2.u32 	[%rd30+16], {%r1467, %r498};
	mov.u64 	%rd1759, $str$2;
	cvta.global.u64 	%rd1760, %rd1759;
	{ // callseq 409, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1760;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1471, [retval0+0];
	} // callseq 409
	bra.uni 	$L__BB13_291;

$L__BB13_288:
	setp.eq.s64 	%p481, %rd218, 0;
	@%p481 bra 	$L__BB13_291;

	add.s32 	%r2424, %r208, 7;
	cvta.to.global.u64 	%rd1756, %rd218;
	mul.wide.s32 	%rd1757, %r497, %r2424;
	add.s64 	%rd1758, %rd1756, %rd1757;
	ld.global.f64 	%fd3970, [%rd1758];
	add.f64 	%fd9289, %fd3970, 0d0000000000000000;
	ld.global.f64 	%fd3971, [%rd1758+8];
	add.f64 	%fd9290, %fd3971, 0d0000000000000000;
	ld.global.f64 	%fd3972, [%rd1758+16];
	add.f64 	%fd9291, %fd3972, 0d0000000000000000;
	ld.global.f64 	%fd3973, [%rd1758+24];
	add.f64 	%fd9292, %fd3973, 0d0000000000000000;
	ld.global.f64 	%fd3974, [%rd1758+32];
	add.f64 	%fd9293, %fd3974, 0d0000000000000000;
	ld.global.f64 	%fd3975, [%rd1758+40];
	add.f64 	%fd9294, %fd3975, 0d0000000000000000;
	ld.global.f64 	%fd3976, [%rd1758+48];
	add.f64 	%fd9295, %fd3976, 0d0000000000000000;
	ld.global.f64 	%fd3977, [%rd1758+56];
	add.f64 	%fd9296, %fd3977, 0d0000000000000000;
	ld.global.f64 	%fd3978, [%rd1758+64];
	add.f64 	%fd9297, %fd3978, 0d0000000000000000;

$L__BB13_291:
	add.s32 	%r2425, %r208, 6;
	add.f64 	%fd811, %fd9297, 0d0000000000000000;
	add.f64 	%fd812, %fd9296, 0d0000000000000000;
	add.f64 	%fd813, %fd9295, 0d0000000000000000;
	add.f64 	%fd814, %fd9294, 0d0000000000000000;
	add.f64 	%fd815, %fd9293, 0d0000000000000000;
	add.f64 	%fd816, %fd9292, 0d0000000000000000;
	add.f64 	%fd817, %fd9291, 0d0000000000000000;
	add.f64 	%fd818, %fd9290, 0d0000000000000000;
	add.f64 	%fd819, %fd9289, 0d0000000000000000;
	ld.param.u64 	%rd219, [%rd357+120];
	ld.param.u32 	%r502, [%rd357+144];
	ld.param.u32 	%r503, [%rd357+172];
	ld.param.v2.u32 	{%r1472, %r1473}, [%rd357+176];
	setp.le.s32 	%p482, %r1472, %r253;
	setp.le.s32 	%p483, %r1473, %r235;
	setp.le.s32 	%p484, %r503, %r2425;
	or.pred  	%p485, %p482, %p483;
	or.pred  	%p487, %p232, %p485;
	or.pred  	%p488, %p484, %p487;
	mov.f64 	%fd9298, 0d0000000000000000;
	mov.f64 	%fd9299, 0d0000000000000000;
	mov.f64 	%fd9300, 0d0000000000000000;
	mov.f64 	%fd9301, 0d0000000000000000;
	mov.f64 	%fd9302, 0d0000000000000000;
	mov.f64 	%fd9303, 0d0000000000000000;
	mov.f64 	%fd9304, 0d0000000000000000;
	mov.f64 	%fd9305, 0d0000000000000000;
	mov.f64 	%fd9306, 0d0000000000000000;
	@%p488 bra 	$L__BB13_294;
	bra.uni 	$L__BB13_292;

$L__BB13_294:
	add.s32 	%r2217, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2217, %r235};
	add.s32 	%r2218, %r208, 6;
	st.local.v2.u32 	[%rd30+8], {%r2218, %r1472};
	st.local.v2.u32 	[%rd30+16], {%r1473, %r503};
	mov.u64 	%rd1765, $str$2;
	cvta.global.u64 	%rd1766, %rd1765;
	{ // callseq 410, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1766;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1477, [retval0+0];
	} // callseq 410
	bra.uni 	$L__BB13_295;

$L__BB13_292:
	setp.eq.s64 	%p489, %rd219, 0;
	@%p489 bra 	$L__BB13_295;

	add.s32 	%r2426, %r208, 6;
	cvta.to.global.u64 	%rd1762, %rd219;
	mul.wide.s32 	%rd1763, %r502, %r2426;
	add.s64 	%rd1764, %rd1762, %rd1763;
	ld.global.f64 	%fd3997, [%rd1764];
	add.f64 	%fd9298, %fd3997, 0d0000000000000000;
	ld.global.f64 	%fd3998, [%rd1764+8];
	add.f64 	%fd9299, %fd3998, 0d0000000000000000;
	ld.global.f64 	%fd3999, [%rd1764+16];
	add.f64 	%fd9300, %fd3999, 0d0000000000000000;
	ld.global.f64 	%fd4000, [%rd1764+24];
	add.f64 	%fd9301, %fd4000, 0d0000000000000000;
	ld.global.f64 	%fd4001, [%rd1764+32];
	add.f64 	%fd9302, %fd4001, 0d0000000000000000;
	ld.global.f64 	%fd4002, [%rd1764+40];
	add.f64 	%fd9303, %fd4002, 0d0000000000000000;
	ld.global.f64 	%fd4003, [%rd1764+48];
	add.f64 	%fd9304, %fd4003, 0d0000000000000000;
	ld.global.f64 	%fd4004, [%rd1764+56];
	add.f64 	%fd9305, %fd4004, 0d0000000000000000;
	ld.global.f64 	%fd4005, [%rd1764+64];
	add.f64 	%fd9306, %fd4005, 0d0000000000000000;

$L__BB13_295:
	add.s32 	%r2427, %r208, 5;
	add.f64 	%fd838, %fd9306, 0d0000000000000000;
	add.f64 	%fd839, %fd9305, 0d0000000000000000;
	add.f64 	%fd840, %fd9304, 0d0000000000000000;
	add.f64 	%fd841, %fd9303, 0d0000000000000000;
	add.f64 	%fd842, %fd9302, 0d0000000000000000;
	add.f64 	%fd843, %fd9301, 0d0000000000000000;
	add.f64 	%fd844, %fd9300, 0d0000000000000000;
	add.f64 	%fd845, %fd9299, 0d0000000000000000;
	add.f64 	%fd846, %fd9298, 0d0000000000000000;
	ld.param.u64 	%rd220, [%rd357+120];
	ld.param.u32 	%r507, [%rd357+144];
	ld.param.u32 	%r508, [%rd357+172];
	ld.param.v2.u32 	{%r1478, %r1479}, [%rd357+176];
	setp.le.s32 	%p490, %r1478, %r253;
	setp.le.s32 	%p491, %r1479, %r226;
	setp.le.s32 	%p492, %r508, %r2427;
	or.pred  	%p493, %p490, %p491;
	or.pred  	%p495, %p225, %p493;
	or.pred  	%p496, %p492, %p495;
	mov.f64 	%fd9307, 0d0000000000000000;
	mov.f64 	%fd9308, 0d0000000000000000;
	mov.f64 	%fd9309, 0d0000000000000000;
	mov.f64 	%fd9310, 0d0000000000000000;
	mov.f64 	%fd9311, 0d0000000000000000;
	mov.f64 	%fd9312, 0d0000000000000000;
	mov.f64 	%fd9313, 0d0000000000000000;
	mov.f64 	%fd9314, 0d0000000000000000;
	mov.f64 	%fd9315, 0d0000000000000000;
	@%p496 bra 	$L__BB13_298;
	bra.uni 	$L__BB13_296;

$L__BB13_298:
	add.s32 	%r2219, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2219, %r226};
	add.s32 	%r2220, %r208, 5;
	st.local.v2.u32 	[%rd30+8], {%r2220, %r1478};
	st.local.v2.u32 	[%rd30+16], {%r1479, %r508};
	mov.u64 	%rd1771, $str$2;
	cvta.global.u64 	%rd1772, %rd1771;
	{ // callseq 411, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1772;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1483, [retval0+0];
	} // callseq 411
	bra.uni 	$L__BB13_299;

$L__BB13_296:
	setp.eq.s64 	%p497, %rd220, 0;
	@%p497 bra 	$L__BB13_299;

	add.s32 	%r2428, %r208, 5;
	cvta.to.global.u64 	%rd1768, %rd220;
	mul.wide.s32 	%rd1769, %r507, %r2428;
	add.s64 	%rd1770, %rd1768, %rd1769;
	ld.global.f64 	%fd4024, [%rd1770];
	add.f64 	%fd9307, %fd4024, 0d0000000000000000;
	ld.global.f64 	%fd4025, [%rd1770+8];
	add.f64 	%fd9308, %fd4025, 0d0000000000000000;
	ld.global.f64 	%fd4026, [%rd1770+16];
	add.f64 	%fd9309, %fd4026, 0d0000000000000000;
	ld.global.f64 	%fd4027, [%rd1770+24];
	add.f64 	%fd9310, %fd4027, 0d0000000000000000;
	ld.global.f64 	%fd4028, [%rd1770+32];
	add.f64 	%fd9311, %fd4028, 0d0000000000000000;
	ld.global.f64 	%fd4029, [%rd1770+40];
	add.f64 	%fd9312, %fd4029, 0d0000000000000000;
	ld.global.f64 	%fd4030, [%rd1770+48];
	add.f64 	%fd9313, %fd4030, 0d0000000000000000;
	ld.global.f64 	%fd4031, [%rd1770+56];
	add.f64 	%fd9314, %fd4031, 0d0000000000000000;
	ld.global.f64 	%fd4032, [%rd1770+64];
	add.f64 	%fd9315, %fd4032, 0d0000000000000000;

$L__BB13_299:
	add.s32 	%r2429, %r208, 4;
	add.f64 	%fd865, %fd9315, 0d0000000000000000;
	add.f64 	%fd866, %fd9314, 0d0000000000000000;
	add.f64 	%fd867, %fd9313, 0d0000000000000000;
	add.f64 	%fd868, %fd9312, 0d0000000000000000;
	add.f64 	%fd869, %fd9311, 0d0000000000000000;
	add.f64 	%fd870, %fd9310, 0d0000000000000000;
	add.f64 	%fd871, %fd9309, 0d0000000000000000;
	add.f64 	%fd872, %fd9308, 0d0000000000000000;
	add.f64 	%fd873, %fd9307, 0d0000000000000000;
	ld.param.u64 	%rd221, [%rd357+120];
	ld.param.u32 	%r512, [%rd357+144];
	ld.param.u32 	%r513, [%rd357+172];
	ld.param.v2.u32 	{%r1484, %r1485}, [%rd357+176];
	setp.le.s32 	%p498, %r1484, %r253;
	setp.le.s32 	%p499, %r1485, %r210;
	setp.le.s32 	%p500, %r513, %r2429;
	or.pred  	%p501, %p498, %p499;
	or.pred  	%p503, %p218, %p501;
	or.pred  	%p504, %p500, %p503;
	mov.f64 	%fd9316, 0d0000000000000000;
	mov.f64 	%fd9317, 0d0000000000000000;
	mov.f64 	%fd9318, 0d0000000000000000;
	mov.f64 	%fd9319, 0d0000000000000000;
	mov.f64 	%fd9320, 0d0000000000000000;
	mov.f64 	%fd9321, 0d0000000000000000;
	mov.f64 	%fd9322, 0d0000000000000000;
	mov.f64 	%fd9323, 0d0000000000000000;
	mov.f64 	%fd9324, 0d0000000000000000;
	@%p504 bra 	$L__BB13_302;
	bra.uni 	$L__BB13_300;

$L__BB13_302:
	add.s32 	%r2221, %r209, 1;
	st.local.v2.u32 	[%rd30], {%r2221, %r210};
	add.s32 	%r2222, %r208, 4;
	st.local.v2.u32 	[%rd30+8], {%r2222, %r1484};
	st.local.v2.u32 	[%rd30+16], {%r1485, %r513};
	mov.u64 	%rd1777, $str$2;
	cvta.global.u64 	%rd1778, %rd1777;
	{ // callseq 412, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1778;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1489, [retval0+0];
	} // callseq 412
	bra.uni 	$L__BB13_303;

$L__BB13_300:
	setp.eq.s64 	%p505, %rd221, 0;
	@%p505 bra 	$L__BB13_303;

	add.s32 	%r2430, %r208, 4;
	cvta.to.global.u64 	%rd1774, %rd221;
	mul.wide.s32 	%rd1775, %r512, %r2430;
	add.s64 	%rd1776, %rd1774, %rd1775;
	ld.global.f64 	%fd4051, [%rd1776];
	add.f64 	%fd9316, %fd4051, 0d0000000000000000;
	ld.global.f64 	%fd4052, [%rd1776+8];
	add.f64 	%fd9317, %fd4052, 0d0000000000000000;
	ld.global.f64 	%fd4053, [%rd1776+16];
	add.f64 	%fd9318, %fd4053, 0d0000000000000000;
	ld.global.f64 	%fd4054, [%rd1776+24];
	add.f64 	%fd9319, %fd4054, 0d0000000000000000;
	ld.global.f64 	%fd4055, [%rd1776+32];
	add.f64 	%fd9320, %fd4055, 0d0000000000000000;
	ld.global.f64 	%fd4056, [%rd1776+40];
	add.f64 	%fd9321, %fd4056, 0d0000000000000000;
	ld.global.f64 	%fd4057, [%rd1776+48];
	add.f64 	%fd9322, %fd4057, 0d0000000000000000;
	ld.global.f64 	%fd4058, [%rd1776+56];
	add.f64 	%fd9323, %fd4058, 0d0000000000000000;
	ld.global.f64 	%fd4059, [%rd1776+64];
	add.f64 	%fd9324, %fd4059, 0d0000000000000000;

$L__BB13_303:
	add.s32 	%r2431, %r208, 3;
	add.f64 	%fd892, %fd9324, 0d0000000000000000;
	add.f64 	%fd893, %fd9323, 0d0000000000000000;
	add.f64 	%fd894, %fd9322, 0d0000000000000000;
	add.f64 	%fd895, %fd9321, 0d0000000000000000;
	add.f64 	%fd896, %fd9320, 0d0000000000000000;
	add.f64 	%fd897, %fd9319, 0d0000000000000000;
	add.f64 	%fd898, %fd9318, 0d0000000000000000;
	add.f64 	%fd899, %fd9317, 0d0000000000000000;
	add.f64 	%fd900, %fd9316, 0d0000000000000000;
	ld.param.u64 	%rd222, [%rd357+120];
	ld.param.u32 	%r517, [%rd357+144];
	ld.param.u32 	%r518, [%rd357+172];
	ld.param.v2.u32 	{%r1490, %r1491}, [%rd357+176];
	setp.le.s32 	%p506, %r1490, %r209;
	setp.le.s32 	%p507, %r1491, %r244;
	setp.le.s32 	%p508, %r518, %r2431;
	or.pred  	%p509, %p506, %p507;
	or.pred  	%p511, %p211, %p509;
	or.pred  	%p512, %p508, %p511;
	mov.f64 	%fd9325, 0d0000000000000000;
	mov.f64 	%fd9326, 0d0000000000000000;
	mov.f64 	%fd9327, 0d0000000000000000;
	mov.f64 	%fd9328, 0d0000000000000000;
	mov.f64 	%fd9329, 0d0000000000000000;
	mov.f64 	%fd9330, 0d0000000000000000;
	mov.f64 	%fd9331, 0d0000000000000000;
	mov.f64 	%fd9332, 0d0000000000000000;
	mov.f64 	%fd9333, 0d0000000000000000;
	@%p512 bra 	$L__BB13_306;
	bra.uni 	$L__BB13_304;

$L__BB13_306:
	add.s32 	%r2223, %r210, 3;
	st.local.v2.u32 	[%rd30], {%r209, %r2223};
	add.s32 	%r2224, %r208, 3;
	st.local.v2.u32 	[%rd30+8], {%r2224, %r1490};
	st.local.v2.u32 	[%rd30+16], {%r1491, %r518};
	mov.u64 	%rd1783, $str$2;
	cvta.global.u64 	%rd1784, %rd1783;
	{ // callseq 413, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1784;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1495, [retval0+0];
	} // callseq 413
	bra.uni 	$L__BB13_307;

$L__BB13_304:
	setp.eq.s64 	%p513, %rd222, 0;
	@%p513 bra 	$L__BB13_307;

	add.s32 	%r2432, %r208, 3;
	cvta.to.global.u64 	%rd1780, %rd222;
	mul.wide.s32 	%rd1781, %r517, %r2432;
	add.s64 	%rd1782, %rd1780, %rd1781;
	ld.global.f64 	%fd4078, [%rd1782];
	add.f64 	%fd9325, %fd4078, 0d0000000000000000;
	ld.global.f64 	%fd4079, [%rd1782+8];
	add.f64 	%fd9326, %fd4079, 0d0000000000000000;
	ld.global.f64 	%fd4080, [%rd1782+16];
	add.f64 	%fd9327, %fd4080, 0d0000000000000000;
	ld.global.f64 	%fd4081, [%rd1782+24];
	add.f64 	%fd9328, %fd4081, 0d0000000000000000;
	ld.global.f64 	%fd4082, [%rd1782+32];
	add.f64 	%fd9329, %fd4082, 0d0000000000000000;
	ld.global.f64 	%fd4083, [%rd1782+40];
	add.f64 	%fd9330, %fd4083, 0d0000000000000000;
	ld.global.f64 	%fd4084, [%rd1782+48];
	add.f64 	%fd9331, %fd4084, 0d0000000000000000;
	ld.global.f64 	%fd4085, [%rd1782+56];
	add.f64 	%fd9332, %fd4085, 0d0000000000000000;
	ld.global.f64 	%fd4086, [%rd1782+64];
	add.f64 	%fd9333, %fd4086, 0d0000000000000000;

$L__BB13_307:
	add.s32 	%r2433, %r208, 2;
	add.f64 	%fd919, %fd9333, 0d0000000000000000;
	add.f64 	%fd920, %fd9332, 0d0000000000000000;
	add.f64 	%fd921, %fd9331, 0d0000000000000000;
	add.f64 	%fd922, %fd9330, 0d0000000000000000;
	add.f64 	%fd923, %fd9329, 0d0000000000000000;
	add.f64 	%fd924, %fd9328, 0d0000000000000000;
	add.f64 	%fd925, %fd9327, 0d0000000000000000;
	add.f64 	%fd926, %fd9326, 0d0000000000000000;
	add.f64 	%fd927, %fd9325, 0d0000000000000000;
	ld.param.u64 	%rd223, [%rd357+120];
	ld.param.u32 	%r522, [%rd357+144];
	ld.param.u32 	%r523, [%rd357+172];
	ld.param.v2.u32 	{%r1496, %r1497}, [%rd357+176];
	setp.le.s32 	%p514, %r1496, %r209;
	setp.le.s32 	%p515, %r1497, %r235;
	setp.le.s32 	%p516, %r523, %r2433;
	or.pred  	%p517, %p514, %p515;
	or.pred  	%p519, %p204, %p517;
	or.pred  	%p520, %p516, %p519;
	mov.f64 	%fd9334, 0d0000000000000000;
	mov.f64 	%fd9335, 0d0000000000000000;
	mov.f64 	%fd9336, 0d0000000000000000;
	mov.f64 	%fd9337, 0d0000000000000000;
	mov.f64 	%fd9338, 0d0000000000000000;
	mov.f64 	%fd9339, 0d0000000000000000;
	mov.f64 	%fd9340, 0d0000000000000000;
	mov.f64 	%fd9341, 0d0000000000000000;
	mov.f64 	%fd9342, 0d0000000000000000;
	@%p520 bra 	$L__BB13_310;
	bra.uni 	$L__BB13_308;

$L__BB13_310:
	add.s32 	%r2225, %r210, 2;
	st.local.v2.u32 	[%rd30], {%r209, %r2225};
	add.s32 	%r2226, %r208, 2;
	st.local.v2.u32 	[%rd30+8], {%r2226, %r1496};
	st.local.v2.u32 	[%rd30+16], {%r1497, %r523};
	mov.u64 	%rd1789, $str$2;
	cvta.global.u64 	%rd1790, %rd1789;
	{ // callseq 414, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1790;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1501, [retval0+0];
	} // callseq 414
	bra.uni 	$L__BB13_311;

$L__BB13_308:
	setp.eq.s64 	%p521, %rd223, 0;
	@%p521 bra 	$L__BB13_311;

	add.s32 	%r2434, %r208, 2;
	cvta.to.global.u64 	%rd1786, %rd223;
	mul.wide.s32 	%rd1787, %r522, %r2434;
	add.s64 	%rd1788, %rd1786, %rd1787;
	ld.global.f64 	%fd4105, [%rd1788];
	add.f64 	%fd9334, %fd4105, 0d0000000000000000;
	ld.global.f64 	%fd4106, [%rd1788+8];
	add.f64 	%fd9335, %fd4106, 0d0000000000000000;
	ld.global.f64 	%fd4107, [%rd1788+16];
	add.f64 	%fd9336, %fd4107, 0d0000000000000000;
	ld.global.f64 	%fd4108, [%rd1788+24];
	add.f64 	%fd9337, %fd4108, 0d0000000000000000;
	ld.global.f64 	%fd4109, [%rd1788+32];
	add.f64 	%fd9338, %fd4109, 0d0000000000000000;
	ld.global.f64 	%fd4110, [%rd1788+40];
	add.f64 	%fd9339, %fd4110, 0d0000000000000000;
	ld.global.f64 	%fd4111, [%rd1788+48];
	add.f64 	%fd9340, %fd4111, 0d0000000000000000;
	ld.global.f64 	%fd4112, [%rd1788+56];
	add.f64 	%fd9341, %fd4112, 0d0000000000000000;
	ld.global.f64 	%fd4113, [%rd1788+64];
	add.f64 	%fd9342, %fd4113, 0d0000000000000000;

$L__BB13_311:
	add.s32 	%r2435, %r208, 1;
	add.f64 	%fd946, %fd9342, 0d0000000000000000;
	add.f64 	%fd947, %fd9341, 0d0000000000000000;
	add.f64 	%fd948, %fd9340, 0d0000000000000000;
	add.f64 	%fd949, %fd9339, 0d0000000000000000;
	add.f64 	%fd950, %fd9338, 0d0000000000000000;
	add.f64 	%fd951, %fd9337, 0d0000000000000000;
	add.f64 	%fd952, %fd9336, 0d0000000000000000;
	add.f64 	%fd953, %fd9335, 0d0000000000000000;
	add.f64 	%fd954, %fd9334, 0d0000000000000000;
	ld.param.u64 	%rd224, [%rd357+120];
	ld.param.u32 	%r527, [%rd357+144];
	ld.param.u32 	%r528, [%rd357+172];
	ld.param.v2.u32 	{%r1502, %r1503}, [%rd357+176];
	setp.le.s32 	%p522, %r1502, %r209;
	setp.le.s32 	%p523, %r1503, %r226;
	setp.le.s32 	%p524, %r528, %r2435;
	or.pred  	%p525, %p522, %p523;
	or.pred  	%p527, %p197, %p525;
	or.pred  	%p528, %p524, %p527;
	mov.f64 	%fd9343, 0d0000000000000000;
	mov.f64 	%fd9344, 0d0000000000000000;
	mov.f64 	%fd9345, 0d0000000000000000;
	mov.f64 	%fd9346, 0d0000000000000000;
	mov.f64 	%fd9347, 0d0000000000000000;
	mov.f64 	%fd9348, 0d0000000000000000;
	mov.f64 	%fd9349, 0d0000000000000000;
	mov.f64 	%fd9350, 0d0000000000000000;
	mov.f64 	%fd9351, 0d0000000000000000;
	@%p528 bra 	$L__BB13_314;
	bra.uni 	$L__BB13_312;

$L__BB13_314:
	add.s32 	%r2227, %r210, 1;
	st.local.v2.u32 	[%rd30], {%r209, %r2227};
	add.s32 	%r2228, %r208, 1;
	st.local.v2.u32 	[%rd30+8], {%r2228, %r1502};
	st.local.v2.u32 	[%rd30+16], {%r1503, %r528};
	mov.u64 	%rd1795, $str$2;
	cvta.global.u64 	%rd1796, %rd1795;
	{ // callseq 415, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1796;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1507, [retval0+0];
	} // callseq 415
	bra.uni 	$L__BB13_315;

$L__BB13_312:
	setp.eq.s64 	%p529, %rd224, 0;
	@%p529 bra 	$L__BB13_315;

	add.s32 	%r2436, %r208, 1;
	cvta.to.global.u64 	%rd1792, %rd224;
	mul.wide.s32 	%rd1793, %r527, %r2436;
	add.s64 	%rd1794, %rd1792, %rd1793;
	ld.global.f64 	%fd4132, [%rd1794];
	add.f64 	%fd9343, %fd4132, 0d0000000000000000;
	ld.global.f64 	%fd4133, [%rd1794+8];
	add.f64 	%fd9344, %fd4133, 0d0000000000000000;
	ld.global.f64 	%fd4134, [%rd1794+16];
	add.f64 	%fd9345, %fd4134, 0d0000000000000000;
	ld.global.f64 	%fd4135, [%rd1794+24];
	add.f64 	%fd9346, %fd4135, 0d0000000000000000;
	ld.global.f64 	%fd4136, [%rd1794+32];
	add.f64 	%fd9347, %fd4136, 0d0000000000000000;
	ld.global.f64 	%fd4137, [%rd1794+40];
	add.f64 	%fd9348, %fd4137, 0d0000000000000000;
	ld.global.f64 	%fd4138, [%rd1794+48];
	add.f64 	%fd9349, %fd4138, 0d0000000000000000;
	ld.global.f64 	%fd4139, [%rd1794+56];
	add.f64 	%fd9350, %fd4139, 0d0000000000000000;
	ld.global.f64 	%fd4140, [%rd1794+64];
	add.f64 	%fd9351, %fd4140, 0d0000000000000000;

$L__BB13_315:
	add.f64 	%fd973, %fd9351, 0d0000000000000000;
	add.f64 	%fd974, %fd9350, 0d0000000000000000;
	add.f64 	%fd975, %fd9349, 0d0000000000000000;
	add.f64 	%fd976, %fd9348, 0d0000000000000000;
	add.f64 	%fd977, %fd9347, 0d0000000000000000;
	add.f64 	%fd978, %fd9346, 0d0000000000000000;
	add.f64 	%fd979, %fd9345, 0d0000000000000000;
	add.f64 	%fd980, %fd9344, 0d0000000000000000;
	add.f64 	%fd981, %fd9343, 0d0000000000000000;
	ld.param.u64 	%rd225, [%rd357+120];
	ld.param.u32 	%r532, [%rd357+144];
	ld.param.u32 	%r533, [%rd357+172];
	ld.param.v2.u32 	{%r1508, %r1509}, [%rd357+176];
	setp.le.s32 	%p530, %r1508, %r209;
	setp.le.s32 	%p531, %r1509, %r210;
	setp.le.s32 	%p532, %r533, %r208;
	or.pred  	%p533, %p530, %p531;
	or.pred  	%p535, %p190, %p533;
	or.pred  	%p536, %p532, %p535;
	mov.f64 	%fd9352, 0d0000000000000000;
	mov.f64 	%fd9353, 0d0000000000000000;
	mov.f64 	%fd9354, 0d0000000000000000;
	mov.f64 	%fd9355, 0d0000000000000000;
	mov.f64 	%fd9356, 0d0000000000000000;
	mov.f64 	%fd9357, 0d0000000000000000;
	mov.f64 	%fd9358, 0d0000000000000000;
	mov.f64 	%fd9359, 0d0000000000000000;
	mov.f64 	%fd9360, 0d0000000000000000;
	@%p536 bra 	$L__BB13_318;
	bra.uni 	$L__BB13_316;

$L__BB13_318:
	st.local.v2.u32 	[%rd30], {%r209, %r210};
	st.local.v2.u32 	[%rd30+8], {%r208, %r1508};
	st.local.v2.u32 	[%rd30+16], {%r1509, %r533};
	mov.u64 	%rd1801, $str$2;
	cvta.global.u64 	%rd1802, %rd1801;
	{ // callseq 416, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1802;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1510, [retval0+0];
	} // callseq 416
	bra.uni 	$L__BB13_319;

$L__BB13_316:
	setp.eq.s64 	%p537, %rd225, 0;
	@%p537 bra 	$L__BB13_319;

	cvta.to.global.u64 	%rd1798, %rd225;
	mul.wide.s32 	%rd1799, %r532, %r208;
	add.s64 	%rd1800, %rd1798, %rd1799;
	ld.global.f64 	%fd4159, [%rd1800];
	add.f64 	%fd9352, %fd4159, 0d0000000000000000;
	ld.global.f64 	%fd4160, [%rd1800+8];
	add.f64 	%fd9353, %fd4160, 0d0000000000000000;
	ld.global.f64 	%fd4161, [%rd1800+16];
	add.f64 	%fd9354, %fd4161, 0d0000000000000000;
	ld.global.f64 	%fd4162, [%rd1800+24];
	add.f64 	%fd9355, %fd4162, 0d0000000000000000;
	ld.global.f64 	%fd4163, [%rd1800+32];
	add.f64 	%fd9356, %fd4163, 0d0000000000000000;
	ld.global.f64 	%fd4164, [%rd1800+40];
	add.f64 	%fd9357, %fd4164, 0d0000000000000000;
	ld.global.f64 	%fd4165, [%rd1800+48];
	add.f64 	%fd9358, %fd4165, 0d0000000000000000;
	ld.global.f64 	%fd4166, [%rd1800+56];
	add.f64 	%fd9359, %fd4166, 0d0000000000000000;
	ld.global.f64 	%fd4167, [%rd1800+64];
	add.f64 	%fd9360, %fd4167, 0d0000000000000000;

$L__BB13_319:
	add.f64 	%fd1000, %fd9360, 0d0000000000000000;
	add.f64 	%fd1001, %fd9359, 0d0000000000000000;
	add.f64 	%fd1002, %fd9358, 0d0000000000000000;
	add.f64 	%fd1003, %fd9357, 0d0000000000000000;
	add.f64 	%fd1004, %fd9356, 0d0000000000000000;
	add.f64 	%fd1005, %fd9355, 0d0000000000000000;
	add.f64 	%fd1006, %fd9354, 0d0000000000000000;
	add.f64 	%fd1007, %fd9353, 0d0000000000000000;
	add.f64 	%fd1008, %fd9352, 0d0000000000000000;
	ld.param.u64 	%rd226, [%rd357];
	ld.param.u32 	%r537, [%rd357+32];
	ld.param.u64 	%rd227, [%rd357+56];
	ld.param.u32 	%r538, [%rd357+88];
	ld.param.u64 	%rd228, [%rd357+112];
	ld.param.u32 	%r539, [%rd357+144];
	ld.param.u32 	%r540, [%rd357+172];
	ld.param.v2.u32 	{%r1511, %r1512}, [%rd357+176];
	setp.le.s32 	%p538, %r1511, %r71;
	setp.le.s32 	%p539, %r1512, %r72;
	setp.le.s32 	%p540, %r540, %r80;
	or.pred  	%p541, %p538, %p539;
	or.pred  	%p543, %p78, %p541;
	or.pred  	%p544, %p540, %p543;
	@%p544 bra 	$L__BB13_321;
	bra.uni 	$L__BB13_320;

$L__BB13_321:
	st.local.v2.u32 	[%rd30], {%r71, %r72};
	st.local.v2.u32 	[%rd30+8], {%r80, %r1511};
	st.local.v2.u32 	[%rd30+16], {%r1512, %r540};
	mov.u64 	%rd1820, $str$2;
	cvta.global.u64 	%rd1821, %rd1820;
	{ // callseq 417, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1821;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1513, [retval0+0];
	} // callseq 417
	bra.uni 	$L__BB13_322;

$L__BB13_320:
	cvta.to.global.u64 	%rd1813, %rd227;
	mul.wide.s32 	%rd1814, %r539, %r80;
	add.s64 	%rd1804, %rd228, %rd1814;
	// begin inline asm
	{ atom.add.f64 %fd4177,[%rd1804],%fd564; }

	// end inline asm
	add.s64 	%rd1805, %rd1804, 8;
	// begin inline asm
	{ atom.add.f64 %fd4179,[%rd1805],%fd563; }

	// end inline asm
	add.s64 	%rd1806, %rd1804, 16;
	// begin inline asm
	{ atom.add.f64 %fd4181,[%rd1806],%fd562; }

	// end inline asm
	add.s64 	%rd1807, %rd1804, 24;
	// begin inline asm
	{ atom.add.f64 %fd4183,[%rd1807],%fd540; }

	// end inline asm
	add.s64 	%rd1808, %rd1804, 32;
	// begin inline asm
	{ atom.add.f64 %fd4185,[%rd1808],%fd539; }

	// end inline asm
	add.s64 	%rd1809, %rd1804, 40;
	// begin inline asm
	{ atom.add.f64 %fd4187,[%rd1809],%fd538; }

	// end inline asm
	add.s64 	%rd1810, %rd1804, 48;
	// begin inline asm
	{ atom.add.f64 %fd4189,[%rd1810],%fd516; }

	// end inline asm
	add.s64 	%rd1811, %rd1804, 56;
	// begin inline asm
	{ atom.add.f64 %fd4191,[%rd1811],%fd515; }

	// end inline asm
	add.s64 	%rd1812, %rd1804, 64;
	// begin inline asm
	{ atom.add.f64 %fd4193,[%rd1812],%fd514; }

	// end inline asm
	mul.wide.s32 	%rd1815, %r537, %r80;
	cvta.to.global.u64 	%rd1816, %rd226;
	add.s64 	%rd1817, %rd1816, %rd1815;
	mul.wide.s32 	%rd1818, %r538, %r80;
	add.s64 	%rd1819, %rd1813, %rd1818;
	st.global.u32 	[%rd1817], %r71;
	st.global.u32 	[%rd1819], %r72;

$L__BB13_322:
	add.s32 	%r2437, %r80, 1;
	ld.param.u64 	%rd229, [%rd357];
	ld.param.u32 	%r544, [%rd357+32];
	ld.param.u64 	%rd230, [%rd357+56];
	ld.param.u32 	%r545, [%rd357+88];
	ld.param.u64 	%rd231, [%rd357+112];
	ld.param.u32 	%r546, [%rd357+144];
	ld.param.u32 	%r547, [%rd357+172];
	ld.param.v2.u32 	{%r1514, %r1515}, [%rd357+176];
	setp.le.s32 	%p546, %r1514, %r71;
	setp.le.s32 	%p547, %r1515, %r89;
	setp.le.s32 	%p548, %r547, %r2437;
	or.pred  	%p549, %p546, %p547;
	or.pred  	%p550, %p85, %p549;
	or.pred  	%p551, %p548, %p550;
	@%p551 bra 	$L__BB13_324;
	bra.uni 	$L__BB13_323;

$L__BB13_324:
	add.s32 	%r2230, %r72, 1;
	st.local.v2.u32 	[%rd30], {%r71, %r2230};
	add.s32 	%r2231, %r80, 1;
	st.local.v2.u32 	[%rd30+8], {%r2231, %r1514};
	st.local.v2.u32 	[%rd30+16], {%r1515, %r547};
	mov.u64 	%rd1839, $str$2;
	cvta.global.u64 	%rd1840, %rd1839;
	{ // callseq 418, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1840;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1519, [retval0+0];
	} // callseq 418
	bra.uni 	$L__BB13_325;

$L__BB13_323:
	add.s32 	%r2438, %r80, 1;
	cvta.to.global.u64 	%rd1832, %rd230;
	mul.wide.s32 	%rd1833, %r546, %r2438;
	add.s64 	%rd1823, %rd231, %rd1833;
	// begin inline asm
	{ atom.add.f64 %fd4195,[%rd1823],%fd561; }

	// end inline asm
	add.s64 	%rd1824, %rd1823, 8;
	// begin inline asm
	{ atom.add.f64 %fd4197,[%rd1824],%fd560; }

	// end inline asm
	add.s64 	%rd1825, %rd1823, 16;
	// begin inline asm
	{ atom.add.f64 %fd4199,[%rd1825],%fd559; }

	// end inline asm
	add.s64 	%rd1826, %rd1823, 24;
	// begin inline asm
	{ atom.add.f64 %fd4201,[%rd1826],%fd537; }

	// end inline asm
	add.s64 	%rd1827, %rd1823, 32;
	// begin inline asm
	{ atom.add.f64 %fd4203,[%rd1827],%fd536; }

	// end inline asm
	add.s64 	%rd1828, %rd1823, 40;
	// begin inline asm
	{ atom.add.f64 %fd4205,[%rd1828],%fd535; }

	// end inline asm
	add.s64 	%rd1829, %rd1823, 48;
	// begin inline asm
	{ atom.add.f64 %fd4207,[%rd1829],%fd513; }

	// end inline asm
	add.s64 	%rd1830, %rd1823, 56;
	// begin inline asm
	{ atom.add.f64 %fd4209,[%rd1830],%fd512; }

	// end inline asm
	add.s64 	%rd1831, %rd1823, 64;
	// begin inline asm
	{ atom.add.f64 %fd4211,[%rd1831],%fd511; }

	// end inline asm
	mul.wide.s32 	%rd1834, %r544, %r2438;
	cvta.to.global.u64 	%rd1835, %rd229;
	add.s64 	%rd1836, %rd1835, %rd1834;
	mul.wide.s32 	%rd1837, %r545, %r2438;
	add.s64 	%rd1838, %rd1832, %rd1837;
	st.global.u32 	[%rd1836], %r71;
	add.s32 	%r2229, %r72, 1;
	st.global.u32 	[%rd1838], %r2229;

$L__BB13_325:
	add.s32 	%r2439, %r80, 2;
	ld.param.u64 	%rd232, [%rd357];
	ld.param.u32 	%r551, [%rd357+32];
	ld.param.u64 	%rd233, [%rd357+56];
	ld.param.u32 	%r552, [%rd357+88];
	ld.param.u64 	%rd234, [%rd357+112];
	ld.param.u32 	%r553, [%rd357+144];
	ld.param.u32 	%r554, [%rd357+172];
	ld.param.v2.u32 	{%r1520, %r1521}, [%rd357+176];
	setp.le.s32 	%p553, %r1520, %r71;
	setp.le.s32 	%p554, %r1521, %r98;
	setp.le.s32 	%p555, %r554, %r2439;
	or.pred  	%p556, %p553, %p554;
	or.pred  	%p557, %p92, %p556;
	or.pred  	%p558, %p555, %p557;
	@%p558 bra 	$L__BB13_327;
	bra.uni 	$L__BB13_326;

$L__BB13_327:
	add.s32 	%r2233, %r72, 2;
	st.local.v2.u32 	[%rd30], {%r71, %r2233};
	add.s32 	%r2234, %r80, 2;
	st.local.v2.u32 	[%rd30+8], {%r2234, %r1520};
	st.local.v2.u32 	[%rd30+16], {%r1521, %r554};
	mov.u64 	%rd1858, $str$2;
	cvta.global.u64 	%rd1859, %rd1858;
	{ // callseq 419, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1859;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1525, [retval0+0];
	} // callseq 419
	bra.uni 	$L__BB13_328;

$L__BB13_326:
	add.s32 	%r2440, %r80, 2;
	cvta.to.global.u64 	%rd1851, %rd233;
	mul.wide.s32 	%rd1852, %r553, %r2440;
	add.s64 	%rd1842, %rd234, %rd1852;
	// begin inline asm
	{ atom.add.f64 %fd4213,[%rd1842],%fd558; }

	// end inline asm
	add.s64 	%rd1843, %rd1842, 8;
	// begin inline asm
	{ atom.add.f64 %fd4215,[%rd1843],%fd557; }

	// end inline asm
	add.s64 	%rd1844, %rd1842, 16;
	// begin inline asm
	{ atom.add.f64 %fd4217,[%rd1844],%fd556; }

	// end inline asm
	add.s64 	%rd1845, %rd1842, 24;
	// begin inline asm
	{ atom.add.f64 %fd4219,[%rd1845],%fd534; }

	// end inline asm
	add.s64 	%rd1846, %rd1842, 32;
	// begin inline asm
	{ atom.add.f64 %fd4221,[%rd1846],%fd533; }

	// end inline asm
	add.s64 	%rd1847, %rd1842, 40;
	// begin inline asm
	{ atom.add.f64 %fd4223,[%rd1847],%fd532; }

	// end inline asm
	add.s64 	%rd1848, %rd1842, 48;
	// begin inline asm
	{ atom.add.f64 %fd4225,[%rd1848],%fd510; }

	// end inline asm
	add.s64 	%rd1849, %rd1842, 56;
	// begin inline asm
	{ atom.add.f64 %fd4227,[%rd1849],%fd509; }

	// end inline asm
	add.s64 	%rd1850, %rd1842, 64;
	// begin inline asm
	{ atom.add.f64 %fd4229,[%rd1850],%fd508; }

	// end inline asm
	mul.wide.s32 	%rd1853, %r551, %r2440;
	cvta.to.global.u64 	%rd1854, %rd232;
	add.s64 	%rd1855, %rd1854, %rd1853;
	mul.wide.s32 	%rd1856, %r552, %r2440;
	add.s64 	%rd1857, %rd1851, %rd1856;
	st.global.u32 	[%rd1855], %r71;
	add.s32 	%r2232, %r72, 2;
	st.global.u32 	[%rd1857], %r2232;

$L__BB13_328:
	add.s32 	%r2441, %r80, 3;
	ld.param.u64 	%rd235, [%rd357];
	ld.param.u32 	%r558, [%rd357+32];
	ld.param.u64 	%rd236, [%rd357+56];
	ld.param.u32 	%r559, [%rd357+88];
	ld.param.u64 	%rd237, [%rd357+112];
	ld.param.u32 	%r560, [%rd357+144];
	ld.param.u32 	%r561, [%rd357+172];
	ld.param.v2.u32 	{%r1526, %r1527}, [%rd357+176];
	setp.le.s32 	%p560, %r1526, %r71;
	setp.le.s32 	%p561, %r1527, %r107;
	setp.le.s32 	%p562, %r561, %r2441;
	or.pred  	%p563, %p560, %p561;
	or.pred  	%p564, %p99, %p563;
	or.pred  	%p565, %p562, %p564;
	@%p565 bra 	$L__BB13_330;
	bra.uni 	$L__BB13_329;

$L__BB13_330:
	add.s32 	%r2236, %r72, 3;
	st.local.v2.u32 	[%rd30], {%r71, %r2236};
	add.s32 	%r2237, %r80, 3;
	st.local.v2.u32 	[%rd30+8], {%r2237, %r1526};
	st.local.v2.u32 	[%rd30+16], {%r1527, %r561};
	mov.u64 	%rd1877, $str$2;
	cvta.global.u64 	%rd1878, %rd1877;
	{ // callseq 420, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1878;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1531, [retval0+0];
	} // callseq 420
	bra.uni 	$L__BB13_331;

$L__BB13_329:
	add.s32 	%r2442, %r80, 3;
	cvta.to.global.u64 	%rd1870, %rd236;
	mul.wide.s32 	%rd1871, %r560, %r2442;
	add.s64 	%rd1861, %rd237, %rd1871;
	// begin inline asm
	{ atom.add.f64 %fd4231,[%rd1861],%fd555; }

	// end inline asm
	add.s64 	%rd1862, %rd1861, 8;
	// begin inline asm
	{ atom.add.f64 %fd4233,[%rd1862],%fd554; }

	// end inline asm
	add.s64 	%rd1863, %rd1861, 16;
	// begin inline asm
	{ atom.add.f64 %fd4235,[%rd1863],%fd553; }

	// end inline asm
	add.s64 	%rd1864, %rd1861, 24;
	// begin inline asm
	{ atom.add.f64 %fd4237,[%rd1864],%fd531; }

	// end inline asm
	add.s64 	%rd1865, %rd1861, 32;
	// begin inline asm
	{ atom.add.f64 %fd4239,[%rd1865],%fd530; }

	// end inline asm
	add.s64 	%rd1866, %rd1861, 40;
	// begin inline asm
	{ atom.add.f64 %fd4241,[%rd1866],%fd529; }

	// end inline asm
	add.s64 	%rd1867, %rd1861, 48;
	// begin inline asm
	{ atom.add.f64 %fd4243,[%rd1867],%fd507; }

	// end inline asm
	add.s64 	%rd1868, %rd1861, 56;
	// begin inline asm
	{ atom.add.f64 %fd4245,[%rd1868],%fd506; }

	// end inline asm
	add.s64 	%rd1869, %rd1861, 64;
	// begin inline asm
	{ atom.add.f64 %fd4247,[%rd1869],%fd505; }

	// end inline asm
	mul.wide.s32 	%rd1872, %r558, %r2442;
	cvta.to.global.u64 	%rd1873, %rd235;
	add.s64 	%rd1874, %rd1873, %rd1872;
	mul.wide.s32 	%rd1875, %r559, %r2442;
	add.s64 	%rd1876, %rd1870, %rd1875;
	st.global.u32 	[%rd1874], %r71;
	add.s32 	%r2235, %r72, 3;
	st.global.u32 	[%rd1876], %r2235;

$L__BB13_331:
	add.s32 	%r2443, %r80, 4;
	ld.param.u64 	%rd238, [%rd357];
	ld.param.u32 	%r565, [%rd357+32];
	ld.param.u64 	%rd239, [%rd357+56];
	ld.param.u32 	%r566, [%rd357+88];
	ld.param.u64 	%rd240, [%rd357+112];
	ld.param.u32 	%r567, [%rd357+144];
	ld.param.u32 	%r568, [%rd357+172];
	ld.param.v2.u32 	{%r1532, %r1533}, [%rd357+176];
	setp.le.s32 	%p567, %r1532, %r116;
	setp.le.s32 	%p568, %r1533, %r72;
	setp.le.s32 	%p569, %r568, %r2443;
	or.pred  	%p570, %p567, %p568;
	or.pred  	%p571, %p106, %p570;
	or.pred  	%p572, %p569, %p571;
	@%p572 bra 	$L__BB13_333;
	bra.uni 	$L__BB13_332;

$L__BB13_333:
	add.s32 	%r2239, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2239, %r72};
	add.s32 	%r2240, %r80, 4;
	st.local.v2.u32 	[%rd30+8], {%r2240, %r1532};
	st.local.v2.u32 	[%rd30+16], {%r1533, %r568};
	mov.u64 	%rd1896, $str$2;
	cvta.global.u64 	%rd1897, %rd1896;
	{ // callseq 421, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1897;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1537, [retval0+0];
	} // callseq 421
	bra.uni 	$L__BB13_334;

$L__BB13_332:
	add.s32 	%r2444, %r80, 4;
	cvta.to.global.u64 	%rd1889, %rd239;
	mul.wide.s32 	%rd1890, %r567, %r2444;
	add.s64 	%rd1880, %rd240, %rd1890;
	// begin inline asm
	{ atom.add.f64 %fd4249,[%rd1880],%fd492; }

	// end inline asm
	add.s64 	%rd1881, %rd1880, 8;
	// begin inline asm
	{ atom.add.f64 %fd4251,[%rd1881],%fd491; }

	// end inline asm
	add.s64 	%rd1882, %rd1880, 16;
	// begin inline asm
	{ atom.add.f64 %fd4253,[%rd1882],%fd490; }

	// end inline asm
	add.s64 	%rd1883, %rd1880, 24;
	// begin inline asm
	{ atom.add.f64 %fd4255,[%rd1883],%fd468; }

	// end inline asm
	add.s64 	%rd1884, %rd1880, 32;
	// begin inline asm
	{ atom.add.f64 %fd4257,[%rd1884],%fd467; }

	// end inline asm
	add.s64 	%rd1885, %rd1880, 40;
	// begin inline asm
	{ atom.add.f64 %fd4259,[%rd1885],%fd466; }

	// end inline asm
	add.s64 	%rd1886, %rd1880, 48;
	// begin inline asm
	{ atom.add.f64 %fd4261,[%rd1886],%fd444; }

	// end inline asm
	add.s64 	%rd1887, %rd1880, 56;
	// begin inline asm
	{ atom.add.f64 %fd4263,[%rd1887],%fd443; }

	// end inline asm
	add.s64 	%rd1888, %rd1880, 64;
	// begin inline asm
	{ atom.add.f64 %fd4265,[%rd1888],%fd442; }

	// end inline asm
	mul.wide.s32 	%rd1891, %r565, %r2444;
	cvta.to.global.u64 	%rd1892, %rd238;
	add.s64 	%rd1893, %rd1892, %rd1891;
	mul.wide.s32 	%rd1894, %r566, %r2444;
	add.s64 	%rd1895, %rd1889, %rd1894;
	add.s32 	%r2238, %r71, 1;
	st.global.u32 	[%rd1893], %r2238;
	st.global.u32 	[%rd1895], %r72;

$L__BB13_334:
	add.s32 	%r2445, %r80, 5;
	ld.param.u64 	%rd241, [%rd357];
	ld.param.u32 	%r572, [%rd357+32];
	ld.param.u64 	%rd242, [%rd357+56];
	ld.param.u32 	%r573, [%rd357+88];
	ld.param.u64 	%rd243, [%rd357+112];
	ld.param.u32 	%r574, [%rd357+144];
	ld.param.u32 	%r575, [%rd357+172];
	ld.param.v2.u32 	{%r1538, %r1539}, [%rd357+176];
	setp.le.s32 	%p574, %r1538, %r116;
	setp.le.s32 	%p575, %r1539, %r89;
	setp.le.s32 	%p576, %r575, %r2445;
	or.pred  	%p577, %p574, %p575;
	or.pred  	%p578, %p113, %p577;
	or.pred  	%p579, %p576, %p578;
	@%p579 bra 	$L__BB13_336;
	bra.uni 	$L__BB13_335;

$L__BB13_336:
	add.s32 	%r2243, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2243, %r89};
	add.s32 	%r2244, %r80, 5;
	st.local.v2.u32 	[%rd30+8], {%r2244, %r1538};
	st.local.v2.u32 	[%rd30+16], {%r1539, %r575};
	mov.u64 	%rd1915, $str$2;
	cvta.global.u64 	%rd1916, %rd1915;
	{ // callseq 422, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1916;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1543, [retval0+0];
	} // callseq 422
	bra.uni 	$L__BB13_337;

$L__BB13_335:
	add.s32 	%r2446, %r80, 5;
	cvta.to.global.u64 	%rd1908, %rd242;
	mul.wide.s32 	%rd1909, %r574, %r2446;
	add.s64 	%rd1899, %rd243, %rd1909;
	// begin inline asm
	{ atom.add.f64 %fd4267,[%rd1899],%fd489; }

	// end inline asm
	add.s64 	%rd1900, %rd1899, 8;
	// begin inline asm
	{ atom.add.f64 %fd4269,[%rd1900],%fd488; }

	// end inline asm
	add.s64 	%rd1901, %rd1899, 16;
	// begin inline asm
	{ atom.add.f64 %fd4271,[%rd1901],%fd487; }

	// end inline asm
	add.s64 	%rd1902, %rd1899, 24;
	// begin inline asm
	{ atom.add.f64 %fd4273,[%rd1902],%fd465; }

	// end inline asm
	add.s64 	%rd1903, %rd1899, 32;
	// begin inline asm
	{ atom.add.f64 %fd4275,[%rd1903],%fd464; }

	// end inline asm
	add.s64 	%rd1904, %rd1899, 40;
	// begin inline asm
	{ atom.add.f64 %fd4277,[%rd1904],%fd463; }

	// end inline asm
	add.s64 	%rd1905, %rd1899, 48;
	// begin inline asm
	{ atom.add.f64 %fd4279,[%rd1905],%fd441; }

	// end inline asm
	add.s64 	%rd1906, %rd1899, 56;
	// begin inline asm
	{ atom.add.f64 %fd4281,[%rd1906],%fd440; }

	// end inline asm
	add.s64 	%rd1907, %rd1899, 64;
	// begin inline asm
	{ atom.add.f64 %fd4283,[%rd1907],%fd439; }

	// end inline asm
	mul.wide.s32 	%rd1910, %r572, %r2446;
	cvta.to.global.u64 	%rd1911, %rd241;
	add.s64 	%rd1912, %rd1911, %rd1910;
	mul.wide.s32 	%rd1913, %r573, %r2446;
	add.s64 	%rd1914, %rd1908, %rd1913;
	add.s32 	%r2241, %r71, 1;
	st.global.u32 	[%rd1912], %r2241;
	add.s32 	%r2242, %r72, 1;
	st.global.u32 	[%rd1914], %r2242;

$L__BB13_337:
	add.s32 	%r2447, %r80, 6;
	ld.param.u64 	%rd244, [%rd357];
	ld.param.u32 	%r579, [%rd357+32];
	ld.param.u64 	%rd245, [%rd357+56];
	ld.param.u32 	%r580, [%rd357+88];
	ld.param.u64 	%rd246, [%rd357+112];
	ld.param.u32 	%r581, [%rd357+144];
	ld.param.u32 	%r582, [%rd357+172];
	ld.param.v2.u32 	{%r1544, %r1545}, [%rd357+176];
	setp.le.s32 	%p581, %r1544, %r116;
	setp.le.s32 	%p582, %r1545, %r98;
	setp.le.s32 	%p583, %r582, %r2447;
	or.pred  	%p584, %p581, %p582;
	or.pred  	%p585, %p120, %p584;
	or.pred  	%p586, %p583, %p585;
	@%p586 bra 	$L__BB13_339;
	bra.uni 	$L__BB13_338;

$L__BB13_339:
	add.s32 	%r2247, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2247, %r98};
	add.s32 	%r2248, %r80, 6;
	st.local.v2.u32 	[%rd30+8], {%r2248, %r1544};
	st.local.v2.u32 	[%rd30+16], {%r1545, %r582};
	mov.u64 	%rd1934, $str$2;
	cvta.global.u64 	%rd1935, %rd1934;
	{ // callseq 423, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1935;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1549, [retval0+0];
	} // callseq 423
	bra.uni 	$L__BB13_340;

$L__BB13_338:
	add.s32 	%r2448, %r80, 6;
	cvta.to.global.u64 	%rd1927, %rd245;
	mul.wide.s32 	%rd1928, %r581, %r2448;
	add.s64 	%rd1918, %rd246, %rd1928;
	// begin inline asm
	{ atom.add.f64 %fd4285,[%rd1918],%fd486; }

	// end inline asm
	add.s64 	%rd1919, %rd1918, 8;
	// begin inline asm
	{ atom.add.f64 %fd4287,[%rd1919],%fd485; }

	// end inline asm
	add.s64 	%rd1920, %rd1918, 16;
	// begin inline asm
	{ atom.add.f64 %fd4289,[%rd1920],%fd484; }

	// end inline asm
	add.s64 	%rd1921, %rd1918, 24;
	// begin inline asm
	{ atom.add.f64 %fd4291,[%rd1921],%fd462; }

	// end inline asm
	add.s64 	%rd1922, %rd1918, 32;
	// begin inline asm
	{ atom.add.f64 %fd4293,[%rd1922],%fd461; }

	// end inline asm
	add.s64 	%rd1923, %rd1918, 40;
	// begin inline asm
	{ atom.add.f64 %fd4295,[%rd1923],%fd460; }

	// end inline asm
	add.s64 	%rd1924, %rd1918, 48;
	// begin inline asm
	{ atom.add.f64 %fd4297,[%rd1924],%fd438; }

	// end inline asm
	add.s64 	%rd1925, %rd1918, 56;
	// begin inline asm
	{ atom.add.f64 %fd4299,[%rd1925],%fd437; }

	// end inline asm
	add.s64 	%rd1926, %rd1918, 64;
	// begin inline asm
	{ atom.add.f64 %fd4301,[%rd1926],%fd436; }

	// end inline asm
	mul.wide.s32 	%rd1929, %r579, %r2448;
	cvta.to.global.u64 	%rd1930, %rd244;
	add.s64 	%rd1931, %rd1930, %rd1929;
	mul.wide.s32 	%rd1932, %r580, %r2448;
	add.s64 	%rd1933, %rd1927, %rd1932;
	add.s32 	%r2245, %r71, 1;
	st.global.u32 	[%rd1931], %r2245;
	add.s32 	%r2246, %r72, 2;
	st.global.u32 	[%rd1933], %r2246;

$L__BB13_340:
	add.s32 	%r2449, %r80, 7;
	ld.param.u64 	%rd247, [%rd357];
	ld.param.u32 	%r586, [%rd357+32];
	ld.param.u64 	%rd248, [%rd357+56];
	ld.param.u32 	%r587, [%rd357+88];
	ld.param.u64 	%rd249, [%rd357+112];
	ld.param.u32 	%r588, [%rd357+144];
	ld.param.u32 	%r589, [%rd357+172];
	ld.param.v2.u32 	{%r1550, %r1551}, [%rd357+176];
	setp.le.s32 	%p588, %r1550, %r116;
	setp.le.s32 	%p589, %r1551, %r107;
	setp.le.s32 	%p590, %r589, %r2449;
	or.pred  	%p591, %p588, %p589;
	or.pred  	%p592, %p127, %p591;
	or.pred  	%p593, %p590, %p592;
	@%p593 bra 	$L__BB13_342;
	bra.uni 	$L__BB13_341;

$L__BB13_342:
	add.s32 	%r2251, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2251, %r107};
	add.s32 	%r2252, %r80, 7;
	st.local.v2.u32 	[%rd30+8], {%r2252, %r1550};
	st.local.v2.u32 	[%rd30+16], {%r1551, %r589};
	mov.u64 	%rd1953, $str$2;
	cvta.global.u64 	%rd1954, %rd1953;
	{ // callseq 424, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1954;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1555, [retval0+0];
	} // callseq 424
	bra.uni 	$L__BB13_343;

$L__BB13_341:
	add.s32 	%r2450, %r80, 7;
	cvta.to.global.u64 	%rd1946, %rd248;
	mul.wide.s32 	%rd1947, %r588, %r2450;
	add.s64 	%rd1937, %rd249, %rd1947;
	// begin inline asm
	{ atom.add.f64 %fd4303,[%rd1937],%fd483; }

	// end inline asm
	add.s64 	%rd1938, %rd1937, 8;
	// begin inline asm
	{ atom.add.f64 %fd4305,[%rd1938],%fd482; }

	// end inline asm
	add.s64 	%rd1939, %rd1937, 16;
	// begin inline asm
	{ atom.add.f64 %fd4307,[%rd1939],%fd481; }

	// end inline asm
	add.s64 	%rd1940, %rd1937, 24;
	// begin inline asm
	{ atom.add.f64 %fd4309,[%rd1940],%fd459; }

	// end inline asm
	add.s64 	%rd1941, %rd1937, 32;
	// begin inline asm
	{ atom.add.f64 %fd4311,[%rd1941],%fd458; }

	// end inline asm
	add.s64 	%rd1942, %rd1937, 40;
	// begin inline asm
	{ atom.add.f64 %fd4313,[%rd1942],%fd457; }

	// end inline asm
	add.s64 	%rd1943, %rd1937, 48;
	// begin inline asm
	{ atom.add.f64 %fd4315,[%rd1943],%fd435; }

	// end inline asm
	add.s64 	%rd1944, %rd1937, 56;
	// begin inline asm
	{ atom.add.f64 %fd4317,[%rd1944],%fd434; }

	// end inline asm
	add.s64 	%rd1945, %rd1937, 64;
	// begin inline asm
	{ atom.add.f64 %fd4319,[%rd1945],%fd433; }

	// end inline asm
	mul.wide.s32 	%rd1948, %r586, %r2450;
	cvta.to.global.u64 	%rd1949, %rd247;
	add.s64 	%rd1950, %rd1949, %rd1948;
	mul.wide.s32 	%rd1951, %r587, %r2450;
	add.s64 	%rd1952, %rd1946, %rd1951;
	add.s32 	%r2249, %r71, 1;
	st.global.u32 	[%rd1950], %r2249;
	add.s32 	%r2250, %r72, 3;
	st.global.u32 	[%rd1952], %r2250;

$L__BB13_343:
	add.s32 	%r2451, %r80, 8;
	ld.param.u64 	%rd250, [%rd357];
	ld.param.u32 	%r593, [%rd357+32];
	ld.param.u64 	%rd251, [%rd357+56];
	ld.param.u32 	%r594, [%rd357+88];
	ld.param.u64 	%rd252, [%rd357+112];
	ld.param.u32 	%r595, [%rd357+144];
	ld.param.u32 	%r596, [%rd357+172];
	ld.param.v2.u32 	{%r1556, %r1557}, [%rd357+176];
	setp.le.s32 	%p595, %r1556, %r149;
	setp.le.s32 	%p596, %r1557, %r72;
	setp.le.s32 	%p597, %r596, %r2451;
	or.pred  	%p598, %p595, %p596;
	or.pred  	%p599, %p134, %p598;
	or.pred  	%p600, %p597, %p599;
	@%p600 bra 	$L__BB13_345;
	bra.uni 	$L__BB13_344;

$L__BB13_345:
	add.s32 	%r2254, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2254, %r72};
	add.s32 	%r2255, %r80, 8;
	st.local.v2.u32 	[%rd30+8], {%r2255, %r1556};
	st.local.v2.u32 	[%rd30+16], {%r1557, %r596};
	mov.u64 	%rd1972, $str$2;
	cvta.global.u64 	%rd1973, %rd1972;
	{ // callseq 425, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1973;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1561, [retval0+0];
	} // callseq 425
	bra.uni 	$L__BB13_346;

$L__BB13_344:
	add.s32 	%r2452, %r80, 8;
	cvta.to.global.u64 	%rd1965, %rd251;
	mul.wide.s32 	%rd1966, %r595, %r2452;
	add.s64 	%rd1956, %rd252, %rd1966;
	// begin inline asm
	{ atom.add.f64 %fd4321,[%rd1956],%fd420; }

	// end inline asm
	add.s64 	%rd1957, %rd1956, 8;
	// begin inline asm
	{ atom.add.f64 %fd4323,[%rd1957],%fd419; }

	// end inline asm
	add.s64 	%rd1958, %rd1956, 16;
	// begin inline asm
	{ atom.add.f64 %fd4325,[%rd1958],%fd418; }

	// end inline asm
	add.s64 	%rd1959, %rd1956, 24;
	// begin inline asm
	{ atom.add.f64 %fd4327,[%rd1959],%fd396; }

	// end inline asm
	add.s64 	%rd1960, %rd1956, 32;
	// begin inline asm
	{ atom.add.f64 %fd4329,[%rd1960],%fd395; }

	// end inline asm
	add.s64 	%rd1961, %rd1956, 40;
	// begin inline asm
	{ atom.add.f64 %fd4331,[%rd1961],%fd394; }

	// end inline asm
	add.s64 	%rd1962, %rd1956, 48;
	// begin inline asm
	{ atom.add.f64 %fd4333,[%rd1962],%fd372; }

	// end inline asm
	add.s64 	%rd1963, %rd1956, 56;
	// begin inline asm
	{ atom.add.f64 %fd4335,[%rd1963],%fd371; }

	// end inline asm
	add.s64 	%rd1964, %rd1956, 64;
	// begin inline asm
	{ atom.add.f64 %fd4337,[%rd1964],%fd370; }

	// end inline asm
	mul.wide.s32 	%rd1967, %r593, %r2452;
	cvta.to.global.u64 	%rd1968, %rd250;
	add.s64 	%rd1969, %rd1968, %rd1967;
	mul.wide.s32 	%rd1970, %r594, %r2452;
	add.s64 	%rd1971, %rd1965, %rd1970;
	add.s32 	%r2253, %r71, 2;
	st.global.u32 	[%rd1969], %r2253;
	st.global.u32 	[%rd1971], %r72;

$L__BB13_346:
	add.s32 	%r2453, %r80, 9;
	ld.param.u64 	%rd253, [%rd357];
	ld.param.u32 	%r600, [%rd357+32];
	ld.param.u64 	%rd254, [%rd357+56];
	ld.param.u32 	%r601, [%rd357+88];
	ld.param.u64 	%rd255, [%rd357+112];
	ld.param.u32 	%r602, [%rd357+144];
	ld.param.u32 	%r603, [%rd357+172];
	ld.param.v2.u32 	{%r1562, %r1563}, [%rd357+176];
	setp.le.s32 	%p602, %r1562, %r149;
	setp.le.s32 	%p603, %r1563, %r89;
	setp.le.s32 	%p604, %r603, %r2453;
	or.pred  	%p605, %p602, %p603;
	or.pred  	%p606, %p141, %p605;
	or.pred  	%p607, %p604, %p606;
	@%p607 bra 	$L__BB13_348;
	bra.uni 	$L__BB13_347;

$L__BB13_348:
	add.s32 	%r2258, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2258, %r89};
	add.s32 	%r2259, %r80, 9;
	st.local.v2.u32 	[%rd30+8], {%r2259, %r1562};
	st.local.v2.u32 	[%rd30+16], {%r1563, %r603};
	mov.u64 	%rd1991, $str$2;
	cvta.global.u64 	%rd1992, %rd1991;
	{ // callseq 426, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1992;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1567, [retval0+0];
	} // callseq 426
	bra.uni 	$L__BB13_349;

$L__BB13_347:
	add.s32 	%r2454, %r80, 9;
	cvta.to.global.u64 	%rd1984, %rd254;
	mul.wide.s32 	%rd1985, %r602, %r2454;
	add.s64 	%rd1975, %rd255, %rd1985;
	// begin inline asm
	{ atom.add.f64 %fd4339,[%rd1975],%fd417; }

	// end inline asm
	add.s64 	%rd1976, %rd1975, 8;
	// begin inline asm
	{ atom.add.f64 %fd4341,[%rd1976],%fd416; }

	// end inline asm
	add.s64 	%rd1977, %rd1975, 16;
	// begin inline asm
	{ atom.add.f64 %fd4343,[%rd1977],%fd415; }

	// end inline asm
	add.s64 	%rd1978, %rd1975, 24;
	// begin inline asm
	{ atom.add.f64 %fd4345,[%rd1978],%fd393; }

	// end inline asm
	add.s64 	%rd1979, %rd1975, 32;
	// begin inline asm
	{ atom.add.f64 %fd4347,[%rd1979],%fd392; }

	// end inline asm
	add.s64 	%rd1980, %rd1975, 40;
	// begin inline asm
	{ atom.add.f64 %fd4349,[%rd1980],%fd391; }

	// end inline asm
	add.s64 	%rd1981, %rd1975, 48;
	// begin inline asm
	{ atom.add.f64 %fd4351,[%rd1981],%fd369; }

	// end inline asm
	add.s64 	%rd1982, %rd1975, 56;
	// begin inline asm
	{ atom.add.f64 %fd4353,[%rd1982],%fd368; }

	// end inline asm
	add.s64 	%rd1983, %rd1975, 64;
	// begin inline asm
	{ atom.add.f64 %fd4355,[%rd1983],%fd367; }

	// end inline asm
	mul.wide.s32 	%rd1986, %r600, %r2454;
	cvta.to.global.u64 	%rd1987, %rd253;
	add.s64 	%rd1988, %rd1987, %rd1986;
	mul.wide.s32 	%rd1989, %r601, %r2454;
	add.s64 	%rd1990, %rd1984, %rd1989;
	add.s32 	%r2256, %r71, 2;
	st.global.u32 	[%rd1988], %r2256;
	add.s32 	%r2257, %r72, 1;
	st.global.u32 	[%rd1990], %r2257;

$L__BB13_349:
	add.s32 	%r2455, %r80, 10;
	ld.param.u64 	%rd256, [%rd357];
	ld.param.u32 	%r607, [%rd357+32];
	ld.param.u64 	%rd257, [%rd357+56];
	ld.param.u32 	%r608, [%rd357+88];
	ld.param.u64 	%rd258, [%rd357+112];
	ld.param.u32 	%r609, [%rd357+144];
	ld.param.u32 	%r610, [%rd357+172];
	ld.param.v2.u32 	{%r1568, %r1569}, [%rd357+176];
	setp.le.s32 	%p609, %r1568, %r149;
	setp.le.s32 	%p610, %r1569, %r98;
	setp.le.s32 	%p611, %r610, %r2455;
	or.pred  	%p612, %p609, %p610;
	or.pred  	%p613, %p148, %p612;
	or.pred  	%p614, %p611, %p613;
	@%p614 bra 	$L__BB13_351;
	bra.uni 	$L__BB13_350;

$L__BB13_351:
	add.s32 	%r2262, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2262, %r98};
	add.s32 	%r2263, %r80, 10;
	st.local.v2.u32 	[%rd30+8], {%r2263, %r1568};
	st.local.v2.u32 	[%rd30+16], {%r1569, %r610};
	mov.u64 	%rd2010, $str$2;
	cvta.global.u64 	%rd2011, %rd2010;
	{ // callseq 427, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2011;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1573, [retval0+0];
	} // callseq 427
	bra.uni 	$L__BB13_352;

$L__BB13_350:
	add.s32 	%r2456, %r80, 10;
	cvta.to.global.u64 	%rd2003, %rd257;
	mul.wide.s32 	%rd2004, %r609, %r2456;
	add.s64 	%rd1994, %rd258, %rd2004;
	// begin inline asm
	{ atom.add.f64 %fd4357,[%rd1994],%fd414; }

	// end inline asm
	add.s64 	%rd1995, %rd1994, 8;
	// begin inline asm
	{ atom.add.f64 %fd4359,[%rd1995],%fd413; }

	// end inline asm
	add.s64 	%rd1996, %rd1994, 16;
	// begin inline asm
	{ atom.add.f64 %fd4361,[%rd1996],%fd412; }

	// end inline asm
	add.s64 	%rd1997, %rd1994, 24;
	// begin inline asm
	{ atom.add.f64 %fd4363,[%rd1997],%fd390; }

	// end inline asm
	add.s64 	%rd1998, %rd1994, 32;
	// begin inline asm
	{ atom.add.f64 %fd4365,[%rd1998],%fd389; }

	// end inline asm
	add.s64 	%rd1999, %rd1994, 40;
	// begin inline asm
	{ atom.add.f64 %fd4367,[%rd1999],%fd388; }

	// end inline asm
	add.s64 	%rd2000, %rd1994, 48;
	// begin inline asm
	{ atom.add.f64 %fd4369,[%rd2000],%fd366; }

	// end inline asm
	add.s64 	%rd2001, %rd1994, 56;
	// begin inline asm
	{ atom.add.f64 %fd4371,[%rd2001],%fd365; }

	// end inline asm
	add.s64 	%rd2002, %rd1994, 64;
	// begin inline asm
	{ atom.add.f64 %fd4373,[%rd2002],%fd364; }

	// end inline asm
	mul.wide.s32 	%rd2005, %r607, %r2456;
	cvta.to.global.u64 	%rd2006, %rd256;
	add.s64 	%rd2007, %rd2006, %rd2005;
	mul.wide.s32 	%rd2008, %r608, %r2456;
	add.s64 	%rd2009, %rd2003, %rd2008;
	add.s32 	%r2260, %r71, 2;
	st.global.u32 	[%rd2007], %r2260;
	add.s32 	%r2261, %r72, 2;
	st.global.u32 	[%rd2009], %r2261;

$L__BB13_352:
	add.s32 	%r2457, %r80, 11;
	ld.param.u64 	%rd259, [%rd357];
	ld.param.u32 	%r614, [%rd357+32];
	ld.param.u64 	%rd260, [%rd357+56];
	ld.param.u32 	%r615, [%rd357+88];
	ld.param.u64 	%rd261, [%rd357+112];
	ld.param.u32 	%r616, [%rd357+144];
	ld.param.u32 	%r617, [%rd357+172];
	ld.param.v2.u32 	{%r1574, %r1575}, [%rd357+176];
	setp.le.s32 	%p616, %r1574, %r149;
	setp.le.s32 	%p617, %r1575, %r107;
	setp.le.s32 	%p618, %r617, %r2457;
	or.pred  	%p619, %p616, %p617;
	or.pred  	%p620, %p155, %p619;
	or.pred  	%p621, %p618, %p620;
	@%p621 bra 	$L__BB13_354;
	bra.uni 	$L__BB13_353;

$L__BB13_354:
	add.s32 	%r2266, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2266, %r107};
	add.s32 	%r2267, %r80, 11;
	st.local.v2.u32 	[%rd30+8], {%r2267, %r1574};
	st.local.v2.u32 	[%rd30+16], {%r1575, %r617};
	mov.u64 	%rd2029, $str$2;
	cvta.global.u64 	%rd2030, %rd2029;
	{ // callseq 428, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2030;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1579, [retval0+0];
	} // callseq 428
	bra.uni 	$L__BB13_355;

$L__BB13_353:
	add.s32 	%r2458, %r80, 11;
	cvta.to.global.u64 	%rd2022, %rd260;
	mul.wide.s32 	%rd2023, %r616, %r2458;
	add.s64 	%rd2013, %rd261, %rd2023;
	// begin inline asm
	{ atom.add.f64 %fd4375,[%rd2013],%fd411; }

	// end inline asm
	add.s64 	%rd2014, %rd2013, 8;
	// begin inline asm
	{ atom.add.f64 %fd4377,[%rd2014],%fd410; }

	// end inline asm
	add.s64 	%rd2015, %rd2013, 16;
	// begin inline asm
	{ atom.add.f64 %fd4379,[%rd2015],%fd409; }

	// end inline asm
	add.s64 	%rd2016, %rd2013, 24;
	// begin inline asm
	{ atom.add.f64 %fd4381,[%rd2016],%fd387; }

	// end inline asm
	add.s64 	%rd2017, %rd2013, 32;
	// begin inline asm
	{ atom.add.f64 %fd4383,[%rd2017],%fd386; }

	// end inline asm
	add.s64 	%rd2018, %rd2013, 40;
	// begin inline asm
	{ atom.add.f64 %fd4385,[%rd2018],%fd385; }

	// end inline asm
	add.s64 	%rd2019, %rd2013, 48;
	// begin inline asm
	{ atom.add.f64 %fd4387,[%rd2019],%fd363; }

	// end inline asm
	add.s64 	%rd2020, %rd2013, 56;
	// begin inline asm
	{ atom.add.f64 %fd4389,[%rd2020],%fd362; }

	// end inline asm
	add.s64 	%rd2021, %rd2013, 64;
	// begin inline asm
	{ atom.add.f64 %fd4391,[%rd2021],%fd361; }

	// end inline asm
	mul.wide.s32 	%rd2024, %r614, %r2458;
	cvta.to.global.u64 	%rd2025, %rd259;
	add.s64 	%rd2026, %rd2025, %rd2024;
	mul.wide.s32 	%rd2027, %r615, %r2458;
	add.s64 	%rd2028, %rd2022, %rd2027;
	add.s32 	%r2264, %r71, 2;
	st.global.u32 	[%rd2026], %r2264;
	add.s32 	%r2265, %r72, 3;
	st.global.u32 	[%rd2028], %r2265;

$L__BB13_355:
	add.s32 	%r2459, %r80, 12;
	ld.param.u64 	%rd262, [%rd357];
	ld.param.u32 	%r621, [%rd357+32];
	ld.param.u64 	%rd263, [%rd357+56];
	ld.param.u32 	%r622, [%rd357+88];
	ld.param.u64 	%rd264, [%rd357+112];
	ld.param.u32 	%r623, [%rd357+144];
	ld.param.u32 	%r624, [%rd357+172];
	ld.param.v2.u32 	{%r1580, %r1581}, [%rd357+176];
	setp.le.s32 	%p623, %r1580, %r182;
	setp.le.s32 	%p624, %r1581, %r72;
	setp.le.s32 	%p625, %r624, %r2459;
	or.pred  	%p626, %p623, %p624;
	or.pred  	%p627, %p162, %p626;
	or.pred  	%p628, %p625, %p627;
	@%p628 bra 	$L__BB13_357;
	bra.uni 	$L__BB13_356;

$L__BB13_357:
	add.s32 	%r2269, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2269, %r72};
	add.s32 	%r2270, %r80, 12;
	st.local.v2.u32 	[%rd30+8], {%r2270, %r1580};
	st.local.v2.u32 	[%rd30+16], {%r1581, %r624};
	mov.u64 	%rd2048, $str$2;
	cvta.global.u64 	%rd2049, %rd2048;
	{ // callseq 429, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2049;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1585, [retval0+0];
	} // callseq 429
	bra.uni 	$L__BB13_358;

$L__BB13_356:
	add.s32 	%r2460, %r80, 12;
	cvta.to.global.u64 	%rd2041, %rd263;
	mul.wide.s32 	%rd2042, %r623, %r2460;
	add.s64 	%rd2032, %rd264, %rd2042;
	// begin inline asm
	{ atom.add.f64 %fd4393,[%rd2032],%fd348; }

	// end inline asm
	add.s64 	%rd2033, %rd2032, 8;
	// begin inline asm
	{ atom.add.f64 %fd4395,[%rd2033],%fd347; }

	// end inline asm
	add.s64 	%rd2034, %rd2032, 16;
	// begin inline asm
	{ atom.add.f64 %fd4397,[%rd2034],%fd346; }

	// end inline asm
	add.s64 	%rd2035, %rd2032, 24;
	// begin inline asm
	{ atom.add.f64 %fd4399,[%rd2035],%fd324; }

	// end inline asm
	add.s64 	%rd2036, %rd2032, 32;
	// begin inline asm
	{ atom.add.f64 %fd4401,[%rd2036],%fd323; }

	// end inline asm
	add.s64 	%rd2037, %rd2032, 40;
	// begin inline asm
	{ atom.add.f64 %fd4403,[%rd2037],%fd322; }

	// end inline asm
	add.s64 	%rd2038, %rd2032, 48;
	// begin inline asm
	{ atom.add.f64 %fd4405,[%rd2038],%fd300; }

	// end inline asm
	add.s64 	%rd2039, %rd2032, 56;
	// begin inline asm
	{ atom.add.f64 %fd4407,[%rd2039],%fd299; }

	// end inline asm
	add.s64 	%rd2040, %rd2032, 64;
	// begin inline asm
	{ atom.add.f64 %fd4409,[%rd2040],%fd298; }

	// end inline asm
	mul.wide.s32 	%rd2043, %r621, %r2460;
	cvta.to.global.u64 	%rd2044, %rd262;
	add.s64 	%rd2045, %rd2044, %rd2043;
	mul.wide.s32 	%rd2046, %r622, %r2460;
	add.s64 	%rd2047, %rd2041, %rd2046;
	add.s32 	%r2268, %r71, 3;
	st.global.u32 	[%rd2045], %r2268;
	st.global.u32 	[%rd2047], %r72;

$L__BB13_358:
	add.s32 	%r2461, %r80, 13;
	ld.param.u64 	%rd265, [%rd357];
	ld.param.u32 	%r628, [%rd357+32];
	ld.param.u64 	%rd266, [%rd357+56];
	ld.param.u32 	%r629, [%rd357+88];
	ld.param.u64 	%rd267, [%rd357+112];
	ld.param.u32 	%r630, [%rd357+144];
	ld.param.u32 	%r631, [%rd357+172];
	ld.param.v2.u32 	{%r1586, %r1587}, [%rd357+176];
	setp.le.s32 	%p630, %r1586, %r182;
	setp.le.s32 	%p631, %r1587, %r89;
	setp.le.s32 	%p632, %r631, %r2461;
	or.pred  	%p633, %p630, %p631;
	or.pred  	%p634, %p169, %p633;
	or.pred  	%p635, %p632, %p634;
	@%p635 bra 	$L__BB13_360;
	bra.uni 	$L__BB13_359;

$L__BB13_360:
	add.s32 	%r2273, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2273, %r89};
	add.s32 	%r2274, %r80, 13;
	st.local.v2.u32 	[%rd30+8], {%r2274, %r1586};
	st.local.v2.u32 	[%rd30+16], {%r1587, %r631};
	mov.u64 	%rd2067, $str$2;
	cvta.global.u64 	%rd2068, %rd2067;
	{ // callseq 430, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2068;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1591, [retval0+0];
	} // callseq 430
	bra.uni 	$L__BB13_361;

$L__BB13_359:
	add.s32 	%r2462, %r80, 13;
	cvta.to.global.u64 	%rd2060, %rd266;
	mul.wide.s32 	%rd2061, %r630, %r2462;
	add.s64 	%rd2051, %rd267, %rd2061;
	// begin inline asm
	{ atom.add.f64 %fd4411,[%rd2051],%fd345; }

	// end inline asm
	add.s64 	%rd2052, %rd2051, 8;
	// begin inline asm
	{ atom.add.f64 %fd4413,[%rd2052],%fd344; }

	// end inline asm
	add.s64 	%rd2053, %rd2051, 16;
	// begin inline asm
	{ atom.add.f64 %fd4415,[%rd2053],%fd343; }

	// end inline asm
	add.s64 	%rd2054, %rd2051, 24;
	// begin inline asm
	{ atom.add.f64 %fd4417,[%rd2054],%fd321; }

	// end inline asm
	add.s64 	%rd2055, %rd2051, 32;
	// begin inline asm
	{ atom.add.f64 %fd4419,[%rd2055],%fd320; }

	// end inline asm
	add.s64 	%rd2056, %rd2051, 40;
	// begin inline asm
	{ atom.add.f64 %fd4421,[%rd2056],%fd319; }

	// end inline asm
	add.s64 	%rd2057, %rd2051, 48;
	// begin inline asm
	{ atom.add.f64 %fd4423,[%rd2057],%fd297; }

	// end inline asm
	add.s64 	%rd2058, %rd2051, 56;
	// begin inline asm
	{ atom.add.f64 %fd4425,[%rd2058],%fd296; }

	// end inline asm
	add.s64 	%rd2059, %rd2051, 64;
	// begin inline asm
	{ atom.add.f64 %fd4427,[%rd2059],%fd295; }

	// end inline asm
	mul.wide.s32 	%rd2062, %r628, %r2462;
	cvta.to.global.u64 	%rd2063, %rd265;
	add.s64 	%rd2064, %rd2063, %rd2062;
	mul.wide.s32 	%rd2065, %r629, %r2462;
	add.s64 	%rd2066, %rd2060, %rd2065;
	add.s32 	%r2271, %r71, 3;
	st.global.u32 	[%rd2064], %r2271;
	add.s32 	%r2272, %r72, 1;
	st.global.u32 	[%rd2066], %r2272;

$L__BB13_361:
	add.s32 	%r2463, %r80, 14;
	ld.param.u64 	%rd268, [%rd357];
	ld.param.u32 	%r635, [%rd357+32];
	ld.param.u64 	%rd269, [%rd357+56];
	ld.param.u32 	%r636, [%rd357+88];
	ld.param.u64 	%rd270, [%rd357+112];
	ld.param.u32 	%r637, [%rd357+144];
	ld.param.u32 	%r638, [%rd357+172];
	ld.param.v2.u32 	{%r1592, %r1593}, [%rd357+176];
	setp.le.s32 	%p637, %r1592, %r182;
	setp.le.s32 	%p638, %r1593, %r98;
	setp.le.s32 	%p639, %r638, %r2463;
	or.pred  	%p640, %p637, %p638;
	or.pred  	%p641, %p176, %p640;
	or.pred  	%p642, %p639, %p641;
	@%p642 bra 	$L__BB13_363;
	bra.uni 	$L__BB13_362;

$L__BB13_363:
	add.s32 	%r2277, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2277, %r98};
	add.s32 	%r2278, %r80, 14;
	st.local.v2.u32 	[%rd30+8], {%r2278, %r1592};
	st.local.v2.u32 	[%rd30+16], {%r1593, %r638};
	mov.u64 	%rd2086, $str$2;
	cvta.global.u64 	%rd2087, %rd2086;
	{ // callseq 431, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2087;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1597, [retval0+0];
	} // callseq 431
	bra.uni 	$L__BB13_364;

$L__BB13_362:
	add.s32 	%r2464, %r80, 14;
	cvta.to.global.u64 	%rd2079, %rd269;
	mul.wide.s32 	%rd2080, %r637, %r2464;
	add.s64 	%rd2070, %rd270, %rd2080;
	// begin inline asm
	{ atom.add.f64 %fd4429,[%rd2070],%fd342; }

	// end inline asm
	add.s64 	%rd2071, %rd2070, 8;
	// begin inline asm
	{ atom.add.f64 %fd4431,[%rd2071],%fd341; }

	// end inline asm
	add.s64 	%rd2072, %rd2070, 16;
	// begin inline asm
	{ atom.add.f64 %fd4433,[%rd2072],%fd340; }

	// end inline asm
	add.s64 	%rd2073, %rd2070, 24;
	// begin inline asm
	{ atom.add.f64 %fd4435,[%rd2073],%fd318; }

	// end inline asm
	add.s64 	%rd2074, %rd2070, 32;
	// begin inline asm
	{ atom.add.f64 %fd4437,[%rd2074],%fd317; }

	// end inline asm
	add.s64 	%rd2075, %rd2070, 40;
	// begin inline asm
	{ atom.add.f64 %fd4439,[%rd2075],%fd316; }

	// end inline asm
	add.s64 	%rd2076, %rd2070, 48;
	// begin inline asm
	{ atom.add.f64 %fd4441,[%rd2076],%fd294; }

	// end inline asm
	add.s64 	%rd2077, %rd2070, 56;
	// begin inline asm
	{ atom.add.f64 %fd4443,[%rd2077],%fd293; }

	// end inline asm
	add.s64 	%rd2078, %rd2070, 64;
	// begin inline asm
	{ atom.add.f64 %fd4445,[%rd2078],%fd292; }

	// end inline asm
	mul.wide.s32 	%rd2081, %r635, %r2464;
	cvta.to.global.u64 	%rd2082, %rd268;
	add.s64 	%rd2083, %rd2082, %rd2081;
	mul.wide.s32 	%rd2084, %r636, %r2464;
	add.s64 	%rd2085, %rd2079, %rd2084;
	add.s32 	%r2275, %r71, 3;
	st.global.u32 	[%rd2083], %r2275;
	add.s32 	%r2276, %r72, 2;
	st.global.u32 	[%rd2085], %r2276;

$L__BB13_364:
	add.s32 	%r2465, %r80, 15;
	ld.param.u64 	%rd271, [%rd357];
	ld.param.u32 	%r642, [%rd357+32];
	ld.param.u64 	%rd272, [%rd357+56];
	ld.param.u32 	%r643, [%rd357+88];
	ld.param.u64 	%rd273, [%rd357+112];
	ld.param.u32 	%r644, [%rd357+144];
	ld.param.u32 	%r645, [%rd357+172];
	ld.param.v2.u32 	{%r1598, %r1599}, [%rd357+176];
	setp.le.s32 	%p644, %r1598, %r182;
	setp.le.s32 	%p645, %r1599, %r107;
	setp.le.s32 	%p646, %r645, %r2465;
	or.pred  	%p647, %p644, %p645;
	or.pred  	%p648, %p183, %p647;
	or.pred  	%p649, %p646, %p648;
	@%p649 bra 	$L__BB13_366;
	bra.uni 	$L__BB13_365;

$L__BB13_366:
	add.s32 	%r2281, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2281, %r107};
	add.s32 	%r2282, %r80, 15;
	st.local.v2.u32 	[%rd30+8], {%r2282, %r1598};
	st.local.v2.u32 	[%rd30+16], {%r1599, %r645};
	mov.u64 	%rd2105, $str$2;
	cvta.global.u64 	%rd2106, %rd2105;
	{ // callseq 432, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2106;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1603, [retval0+0];
	} // callseq 432
	bra.uni 	$L__BB13_367;

$L__BB13_365:
	add.s32 	%r2466, %r80, 15;
	cvta.to.global.u64 	%rd2098, %rd272;
	mul.wide.s32 	%rd2099, %r644, %r2466;
	add.s64 	%rd2089, %rd273, %rd2099;
	// begin inline asm
	{ atom.add.f64 %fd4447,[%rd2089],%fd339; }

	// end inline asm
	add.s64 	%rd2090, %rd2089, 8;
	// begin inline asm
	{ atom.add.f64 %fd4449,[%rd2090],%fd338; }

	// end inline asm
	add.s64 	%rd2091, %rd2089, 16;
	// begin inline asm
	{ atom.add.f64 %fd4451,[%rd2091],%fd337; }

	// end inline asm
	add.s64 	%rd2092, %rd2089, 24;
	// begin inline asm
	{ atom.add.f64 %fd4453,[%rd2092],%fd315; }

	// end inline asm
	add.s64 	%rd2093, %rd2089, 32;
	// begin inline asm
	{ atom.add.f64 %fd4455,[%rd2093],%fd314; }

	// end inline asm
	add.s64 	%rd2094, %rd2089, 40;
	// begin inline asm
	{ atom.add.f64 %fd4457,[%rd2094],%fd313; }

	// end inline asm
	add.s64 	%rd2095, %rd2089, 48;
	// begin inline asm
	{ atom.add.f64 %fd4459,[%rd2095],%fd291; }

	// end inline asm
	add.s64 	%rd2096, %rd2089, 56;
	// begin inline asm
	{ atom.add.f64 %fd4461,[%rd2096],%fd290; }

	// end inline asm
	add.s64 	%rd2097, %rd2089, 64;
	// begin inline asm
	{ atom.add.f64 %fd4463,[%rd2097],%fd289; }

	// end inline asm
	mul.wide.s32 	%rd2100, %r642, %r2466;
	cvta.to.global.u64 	%rd2101, %rd271;
	add.s64 	%rd2102, %rd2101, %rd2100;
	mul.wide.s32 	%rd2103, %r643, %r2466;
	add.s64 	%rd2104, %rd2098, %rd2103;
	add.s32 	%r2279, %r71, 3;
	st.global.u32 	[%rd2102], %r2279;
	add.s32 	%r2280, %r72, 3;
	st.global.u32 	[%rd2104], %r2280;

$L__BB13_367:
	add.s32 	%r2467, %r80, 15;
	ld.param.u64 	%rd274, [%rd357+120];
	ld.param.u32 	%r649, [%rd357+144];
	ld.param.u32 	%r650, [%rd357+172];
	ld.param.v2.u32 	{%r1604, %r1605}, [%rd357+176];
	setp.le.s32 	%p651, %r1604, %r182;
	setp.le.s32 	%p652, %r1605, %r107;
	setp.le.s32 	%p653, %r650, %r2467;
	or.pred  	%p654, %p651, %p652;
	or.pred  	%p655, %p183, %p654;
	or.pred  	%p656, %p653, %p655;
	mov.f64 	%fd9361, 0d0000000000000000;
	mov.f64 	%fd9362, 0d0000000000000000;
	mov.f64 	%fd9363, 0d0000000000000000;
	mov.f64 	%fd9364, 0d0000000000000000;
	mov.f64 	%fd9365, 0d0000000000000000;
	mov.f64 	%fd9366, 0d0000000000000000;
	mov.f64 	%fd9367, 0d0000000000000000;
	mov.f64 	%fd9368, 0d0000000000000000;
	mov.f64 	%fd9369, 0d0000000000000000;
	@%p656 bra 	$L__BB13_370;
	bra.uni 	$L__BB13_368;

$L__BB13_370:
	add.s32 	%r2283, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2283, %r107};
	add.s32 	%r2284, %r80, 15;
	st.local.v2.u32 	[%rd30+8], {%r2284, %r1604};
	st.local.v2.u32 	[%rd30+16], {%r1605, %r650};
	mov.u64 	%rd2111, $str$2;
	cvta.global.u64 	%rd2112, %rd2111;
	{ // callseq 433, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2112;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1609, [retval0+0];
	} // callseq 433
	bra.uni 	$L__BB13_371;

$L__BB13_368:
	setp.eq.s64 	%p657, %rd274, 0;
	@%p657 bra 	$L__BB13_371;

	add.s32 	%r2468, %r80, 15;
	cvta.to.global.u64 	%rd2108, %rd274;
	mul.wide.s32 	%rd2109, %r649, %r2468;
	add.s64 	%rd2110, %rd2108, %rd2109;
	ld.global.f64 	%fd4474, [%rd2110];
	add.f64 	%fd9361, %fd4474, 0d0000000000000000;
	ld.global.f64 	%fd4475, [%rd2110+8];
	add.f64 	%fd9362, %fd4475, 0d0000000000000000;
	ld.global.f64 	%fd4476, [%rd2110+16];
	add.f64 	%fd9363, %fd4476, 0d0000000000000000;
	ld.global.f64 	%fd4477, [%rd2110+24];
	add.f64 	%fd9364, %fd4477, 0d0000000000000000;
	ld.global.f64 	%fd4478, [%rd2110+32];
	add.f64 	%fd9365, %fd4478, 0d0000000000000000;
	ld.global.f64 	%fd4479, [%rd2110+40];
	add.f64 	%fd9366, %fd4479, 0d0000000000000000;
	ld.global.f64 	%fd4480, [%rd2110+48];
	add.f64 	%fd9367, %fd4480, 0d0000000000000000;
	ld.global.f64 	%fd4481, [%rd2110+56];
	add.f64 	%fd9368, %fd4481, 0d0000000000000000;
	ld.global.f64 	%fd4482, [%rd2110+64];
	add.f64 	%fd9369, %fd4482, 0d0000000000000000;

$L__BB13_371:
	add.s32 	%r2469, %r80, 14;
	add.f64 	%fd1027, %fd9369, 0d0000000000000000;
	add.f64 	%fd1028, %fd9368, 0d0000000000000000;
	add.f64 	%fd1029, %fd9367, 0d0000000000000000;
	add.f64 	%fd1030, %fd9366, 0d0000000000000000;
	add.f64 	%fd1031, %fd9365, 0d0000000000000000;
	add.f64 	%fd1032, %fd9364, 0d0000000000000000;
	add.f64 	%fd1033, %fd9363, 0d0000000000000000;
	add.f64 	%fd1034, %fd9362, 0d0000000000000000;
	add.f64 	%fd1035, %fd9361, 0d0000000000000000;
	ld.param.u64 	%rd275, [%rd357+120];
	ld.param.u32 	%r654, [%rd357+144];
	ld.param.u32 	%r655, [%rd357+172];
	ld.param.v2.u32 	{%r1610, %r1611}, [%rd357+176];
	setp.le.s32 	%p658, %r1610, %r182;
	setp.le.s32 	%p659, %r1611, %r98;
	setp.le.s32 	%p660, %r655, %r2469;
	or.pred  	%p661, %p658, %p659;
	or.pred  	%p663, %p176, %p661;
	or.pred  	%p664, %p660, %p663;
	mov.f64 	%fd9370, 0d0000000000000000;
	mov.f64 	%fd9371, 0d0000000000000000;
	mov.f64 	%fd9372, 0d0000000000000000;
	mov.f64 	%fd9373, 0d0000000000000000;
	mov.f64 	%fd9374, 0d0000000000000000;
	mov.f64 	%fd9375, 0d0000000000000000;
	mov.f64 	%fd9376, 0d0000000000000000;
	mov.f64 	%fd9377, 0d0000000000000000;
	mov.f64 	%fd9378, 0d0000000000000000;
	@%p664 bra 	$L__BB13_374;
	bra.uni 	$L__BB13_372;

$L__BB13_374:
	add.s32 	%r2285, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2285, %r98};
	add.s32 	%r2286, %r80, 14;
	st.local.v2.u32 	[%rd30+8], {%r2286, %r1610};
	st.local.v2.u32 	[%rd30+16], {%r1611, %r655};
	mov.u64 	%rd2117, $str$2;
	cvta.global.u64 	%rd2118, %rd2117;
	{ // callseq 434, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2118;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1615, [retval0+0];
	} // callseq 434
	bra.uni 	$L__BB13_375;

$L__BB13_372:
	setp.eq.s64 	%p665, %rd275, 0;
	@%p665 bra 	$L__BB13_375;

	add.s32 	%r2470, %r80, 14;
	cvta.to.global.u64 	%rd2114, %rd275;
	mul.wide.s32 	%rd2115, %r654, %r2470;
	add.s64 	%rd2116, %rd2114, %rd2115;
	ld.global.f64 	%fd4501, [%rd2116];
	add.f64 	%fd9370, %fd4501, 0d0000000000000000;
	ld.global.f64 	%fd4502, [%rd2116+8];
	add.f64 	%fd9371, %fd4502, 0d0000000000000000;
	ld.global.f64 	%fd4503, [%rd2116+16];
	add.f64 	%fd9372, %fd4503, 0d0000000000000000;
	ld.global.f64 	%fd4504, [%rd2116+24];
	add.f64 	%fd9373, %fd4504, 0d0000000000000000;
	ld.global.f64 	%fd4505, [%rd2116+32];
	add.f64 	%fd9374, %fd4505, 0d0000000000000000;
	ld.global.f64 	%fd4506, [%rd2116+40];
	add.f64 	%fd9375, %fd4506, 0d0000000000000000;
	ld.global.f64 	%fd4507, [%rd2116+48];
	add.f64 	%fd9376, %fd4507, 0d0000000000000000;
	ld.global.f64 	%fd4508, [%rd2116+56];
	add.f64 	%fd9377, %fd4508, 0d0000000000000000;
	ld.global.f64 	%fd4509, [%rd2116+64];
	add.f64 	%fd9378, %fd4509, 0d0000000000000000;

$L__BB13_375:
	add.s32 	%r2471, %r80, 13;
	add.f64 	%fd1054, %fd9378, 0d0000000000000000;
	add.f64 	%fd1055, %fd9377, 0d0000000000000000;
	add.f64 	%fd1056, %fd9376, 0d0000000000000000;
	add.f64 	%fd1057, %fd9375, 0d0000000000000000;
	add.f64 	%fd1058, %fd9374, 0d0000000000000000;
	add.f64 	%fd1059, %fd9373, 0d0000000000000000;
	add.f64 	%fd1060, %fd9372, 0d0000000000000000;
	add.f64 	%fd1061, %fd9371, 0d0000000000000000;
	add.f64 	%fd1062, %fd9370, 0d0000000000000000;
	ld.param.u64 	%rd276, [%rd357+120];
	ld.param.u32 	%r659, [%rd357+144];
	ld.param.u32 	%r660, [%rd357+172];
	ld.param.v2.u32 	{%r1616, %r1617}, [%rd357+176];
	setp.le.s32 	%p666, %r1616, %r182;
	setp.le.s32 	%p667, %r1617, %r89;
	setp.le.s32 	%p668, %r660, %r2471;
	or.pred  	%p669, %p666, %p667;
	or.pred  	%p671, %p169, %p669;
	or.pred  	%p672, %p668, %p671;
	mov.f64 	%fd9379, 0d0000000000000000;
	mov.f64 	%fd9380, 0d0000000000000000;
	mov.f64 	%fd9381, 0d0000000000000000;
	mov.f64 	%fd9382, 0d0000000000000000;
	mov.f64 	%fd9383, 0d0000000000000000;
	mov.f64 	%fd9384, 0d0000000000000000;
	mov.f64 	%fd9385, 0d0000000000000000;
	mov.f64 	%fd9386, 0d0000000000000000;
	mov.f64 	%fd9387, 0d0000000000000000;
	@%p672 bra 	$L__BB13_378;
	bra.uni 	$L__BB13_376;

$L__BB13_378:
	add.s32 	%r2287, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2287, %r89};
	add.s32 	%r2288, %r80, 13;
	st.local.v2.u32 	[%rd30+8], {%r2288, %r1616};
	st.local.v2.u32 	[%rd30+16], {%r1617, %r660};
	mov.u64 	%rd2123, $str$2;
	cvta.global.u64 	%rd2124, %rd2123;
	{ // callseq 435, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2124;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1621, [retval0+0];
	} // callseq 435
	bra.uni 	$L__BB13_379;

$L__BB13_376:
	setp.eq.s64 	%p673, %rd276, 0;
	@%p673 bra 	$L__BB13_379;

	add.s32 	%r2472, %r80, 13;
	cvta.to.global.u64 	%rd2120, %rd276;
	mul.wide.s32 	%rd2121, %r659, %r2472;
	add.s64 	%rd2122, %rd2120, %rd2121;
	ld.global.f64 	%fd4528, [%rd2122];
	add.f64 	%fd9379, %fd4528, 0d0000000000000000;
	ld.global.f64 	%fd4529, [%rd2122+8];
	add.f64 	%fd9380, %fd4529, 0d0000000000000000;
	ld.global.f64 	%fd4530, [%rd2122+16];
	add.f64 	%fd9381, %fd4530, 0d0000000000000000;
	ld.global.f64 	%fd4531, [%rd2122+24];
	add.f64 	%fd9382, %fd4531, 0d0000000000000000;
	ld.global.f64 	%fd4532, [%rd2122+32];
	add.f64 	%fd9383, %fd4532, 0d0000000000000000;
	ld.global.f64 	%fd4533, [%rd2122+40];
	add.f64 	%fd9384, %fd4533, 0d0000000000000000;
	ld.global.f64 	%fd4534, [%rd2122+48];
	add.f64 	%fd9385, %fd4534, 0d0000000000000000;
	ld.global.f64 	%fd4535, [%rd2122+56];
	add.f64 	%fd9386, %fd4535, 0d0000000000000000;
	ld.global.f64 	%fd4536, [%rd2122+64];
	add.f64 	%fd9387, %fd4536, 0d0000000000000000;

$L__BB13_379:
	add.s32 	%r2473, %r80, 12;
	add.f64 	%fd1081, %fd9387, 0d0000000000000000;
	add.f64 	%fd1082, %fd9386, 0d0000000000000000;
	add.f64 	%fd1083, %fd9385, 0d0000000000000000;
	add.f64 	%fd1084, %fd9384, 0d0000000000000000;
	add.f64 	%fd1085, %fd9383, 0d0000000000000000;
	add.f64 	%fd1086, %fd9382, 0d0000000000000000;
	add.f64 	%fd1087, %fd9381, 0d0000000000000000;
	add.f64 	%fd1088, %fd9380, 0d0000000000000000;
	add.f64 	%fd1089, %fd9379, 0d0000000000000000;
	ld.param.u64 	%rd277, [%rd357+120];
	ld.param.u32 	%r664, [%rd357+144];
	ld.param.u32 	%r665, [%rd357+172];
	ld.param.v2.u32 	{%r1622, %r1623}, [%rd357+176];
	setp.le.s32 	%p674, %r1622, %r182;
	setp.le.s32 	%p675, %r1623, %r72;
	setp.le.s32 	%p676, %r665, %r2473;
	or.pred  	%p677, %p674, %p675;
	or.pred  	%p679, %p162, %p677;
	or.pred  	%p680, %p676, %p679;
	mov.f64 	%fd9388, 0d0000000000000000;
	mov.f64 	%fd9389, 0d0000000000000000;
	mov.f64 	%fd9390, 0d0000000000000000;
	mov.f64 	%fd9391, 0d0000000000000000;
	mov.f64 	%fd9392, 0d0000000000000000;
	mov.f64 	%fd9393, 0d0000000000000000;
	mov.f64 	%fd9394, 0d0000000000000000;
	mov.f64 	%fd9395, 0d0000000000000000;
	mov.f64 	%fd9396, 0d0000000000000000;
	@%p680 bra 	$L__BB13_382;
	bra.uni 	$L__BB13_380;

$L__BB13_382:
	add.s32 	%r2289, %r71, 3;
	st.local.v2.u32 	[%rd30], {%r2289, %r72};
	add.s32 	%r2290, %r80, 12;
	st.local.v2.u32 	[%rd30+8], {%r2290, %r1622};
	st.local.v2.u32 	[%rd30+16], {%r1623, %r665};
	mov.u64 	%rd2129, $str$2;
	cvta.global.u64 	%rd2130, %rd2129;
	{ // callseq 436, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2130;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1627, [retval0+0];
	} // callseq 436
	bra.uni 	$L__BB13_383;

$L__BB13_380:
	setp.eq.s64 	%p681, %rd277, 0;
	@%p681 bra 	$L__BB13_383;

	add.s32 	%r2474, %r80, 12;
	cvta.to.global.u64 	%rd2126, %rd277;
	mul.wide.s32 	%rd2127, %r664, %r2474;
	add.s64 	%rd2128, %rd2126, %rd2127;
	ld.global.f64 	%fd4555, [%rd2128];
	add.f64 	%fd9388, %fd4555, 0d0000000000000000;
	ld.global.f64 	%fd4556, [%rd2128+8];
	add.f64 	%fd9389, %fd4556, 0d0000000000000000;
	ld.global.f64 	%fd4557, [%rd2128+16];
	add.f64 	%fd9390, %fd4557, 0d0000000000000000;
	ld.global.f64 	%fd4558, [%rd2128+24];
	add.f64 	%fd9391, %fd4558, 0d0000000000000000;
	ld.global.f64 	%fd4559, [%rd2128+32];
	add.f64 	%fd9392, %fd4559, 0d0000000000000000;
	ld.global.f64 	%fd4560, [%rd2128+40];
	add.f64 	%fd9393, %fd4560, 0d0000000000000000;
	ld.global.f64 	%fd4561, [%rd2128+48];
	add.f64 	%fd9394, %fd4561, 0d0000000000000000;
	ld.global.f64 	%fd4562, [%rd2128+56];
	add.f64 	%fd9395, %fd4562, 0d0000000000000000;
	ld.global.f64 	%fd4563, [%rd2128+64];
	add.f64 	%fd9396, %fd4563, 0d0000000000000000;

$L__BB13_383:
	add.s32 	%r2475, %r80, 11;
	add.f64 	%fd1108, %fd9396, 0d0000000000000000;
	add.f64 	%fd1109, %fd9395, 0d0000000000000000;
	add.f64 	%fd1110, %fd9394, 0d0000000000000000;
	add.f64 	%fd1111, %fd9393, 0d0000000000000000;
	add.f64 	%fd1112, %fd9392, 0d0000000000000000;
	add.f64 	%fd1113, %fd9391, 0d0000000000000000;
	add.f64 	%fd1114, %fd9390, 0d0000000000000000;
	add.f64 	%fd1115, %fd9389, 0d0000000000000000;
	add.f64 	%fd1116, %fd9388, 0d0000000000000000;
	ld.param.u64 	%rd278, [%rd357+120];
	ld.param.u32 	%r669, [%rd357+144];
	ld.param.u32 	%r670, [%rd357+172];
	ld.param.v2.u32 	{%r1628, %r1629}, [%rd357+176];
	setp.le.s32 	%p682, %r1628, %r149;
	setp.le.s32 	%p683, %r1629, %r107;
	setp.le.s32 	%p684, %r670, %r2475;
	or.pred  	%p685, %p682, %p683;
	or.pred  	%p687, %p155, %p685;
	or.pred  	%p688, %p684, %p687;
	mov.f64 	%fd9397, 0d0000000000000000;
	mov.f64 	%fd9398, 0d0000000000000000;
	mov.f64 	%fd9399, 0d0000000000000000;
	mov.f64 	%fd9400, 0d0000000000000000;
	mov.f64 	%fd9401, 0d0000000000000000;
	mov.f64 	%fd9402, 0d0000000000000000;
	mov.f64 	%fd9403, 0d0000000000000000;
	mov.f64 	%fd9404, 0d0000000000000000;
	mov.f64 	%fd9405, 0d0000000000000000;
	@%p688 bra 	$L__BB13_386;
	bra.uni 	$L__BB13_384;

$L__BB13_386:
	add.s32 	%r2291, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2291, %r107};
	add.s32 	%r2292, %r80, 11;
	st.local.v2.u32 	[%rd30+8], {%r2292, %r1628};
	st.local.v2.u32 	[%rd30+16], {%r1629, %r670};
	mov.u64 	%rd2135, $str$2;
	cvta.global.u64 	%rd2136, %rd2135;
	{ // callseq 437, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2136;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1633, [retval0+0];
	} // callseq 437
	bra.uni 	$L__BB13_387;

$L__BB13_384:
	setp.eq.s64 	%p689, %rd278, 0;
	@%p689 bra 	$L__BB13_387;

	add.s32 	%r2476, %r80, 11;
	cvta.to.global.u64 	%rd2132, %rd278;
	mul.wide.s32 	%rd2133, %r669, %r2476;
	add.s64 	%rd2134, %rd2132, %rd2133;
	ld.global.f64 	%fd4582, [%rd2134];
	add.f64 	%fd9397, %fd4582, 0d0000000000000000;
	ld.global.f64 	%fd4583, [%rd2134+8];
	add.f64 	%fd9398, %fd4583, 0d0000000000000000;
	ld.global.f64 	%fd4584, [%rd2134+16];
	add.f64 	%fd9399, %fd4584, 0d0000000000000000;
	ld.global.f64 	%fd4585, [%rd2134+24];
	add.f64 	%fd9400, %fd4585, 0d0000000000000000;
	ld.global.f64 	%fd4586, [%rd2134+32];
	add.f64 	%fd9401, %fd4586, 0d0000000000000000;
	ld.global.f64 	%fd4587, [%rd2134+40];
	add.f64 	%fd9402, %fd4587, 0d0000000000000000;
	ld.global.f64 	%fd4588, [%rd2134+48];
	add.f64 	%fd9403, %fd4588, 0d0000000000000000;
	ld.global.f64 	%fd4589, [%rd2134+56];
	add.f64 	%fd9404, %fd4589, 0d0000000000000000;
	ld.global.f64 	%fd4590, [%rd2134+64];
	add.f64 	%fd9405, %fd4590, 0d0000000000000000;

$L__BB13_387:
	add.s32 	%r2477, %r80, 10;
	add.f64 	%fd1135, %fd9405, 0d0000000000000000;
	add.f64 	%fd1136, %fd9404, 0d0000000000000000;
	add.f64 	%fd1137, %fd9403, 0d0000000000000000;
	add.f64 	%fd1138, %fd9402, 0d0000000000000000;
	add.f64 	%fd1139, %fd9401, 0d0000000000000000;
	add.f64 	%fd1140, %fd9400, 0d0000000000000000;
	add.f64 	%fd1141, %fd9399, 0d0000000000000000;
	add.f64 	%fd1142, %fd9398, 0d0000000000000000;
	add.f64 	%fd1143, %fd9397, 0d0000000000000000;
	ld.param.u64 	%rd279, [%rd357+120];
	ld.param.u32 	%r674, [%rd357+144];
	ld.param.u32 	%r675, [%rd357+172];
	ld.param.v2.u32 	{%r1634, %r1635}, [%rd357+176];
	setp.le.s32 	%p690, %r1634, %r149;
	setp.le.s32 	%p691, %r1635, %r98;
	setp.le.s32 	%p692, %r675, %r2477;
	or.pred  	%p693, %p690, %p691;
	or.pred  	%p695, %p148, %p693;
	or.pred  	%p696, %p692, %p695;
	mov.f64 	%fd9406, 0d0000000000000000;
	mov.f64 	%fd9407, 0d0000000000000000;
	mov.f64 	%fd9408, 0d0000000000000000;
	mov.f64 	%fd9409, 0d0000000000000000;
	mov.f64 	%fd9410, 0d0000000000000000;
	mov.f64 	%fd9411, 0d0000000000000000;
	mov.f64 	%fd9412, 0d0000000000000000;
	mov.f64 	%fd9413, 0d0000000000000000;
	mov.f64 	%fd9414, 0d0000000000000000;
	@%p696 bra 	$L__BB13_390;
	bra.uni 	$L__BB13_388;

$L__BB13_390:
	add.s32 	%r2293, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2293, %r98};
	add.s32 	%r2294, %r80, 10;
	st.local.v2.u32 	[%rd30+8], {%r2294, %r1634};
	st.local.v2.u32 	[%rd30+16], {%r1635, %r675};
	mov.u64 	%rd2141, $str$2;
	cvta.global.u64 	%rd2142, %rd2141;
	{ // callseq 438, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2142;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1639, [retval0+0];
	} // callseq 438
	bra.uni 	$L__BB13_391;

$L__BB13_388:
	setp.eq.s64 	%p697, %rd279, 0;
	@%p697 bra 	$L__BB13_391;

	add.s32 	%r2478, %r80, 10;
	cvta.to.global.u64 	%rd2138, %rd279;
	mul.wide.s32 	%rd2139, %r674, %r2478;
	add.s64 	%rd2140, %rd2138, %rd2139;
	ld.global.f64 	%fd4609, [%rd2140];
	add.f64 	%fd9406, %fd4609, 0d0000000000000000;
	ld.global.f64 	%fd4610, [%rd2140+8];
	add.f64 	%fd9407, %fd4610, 0d0000000000000000;
	ld.global.f64 	%fd4611, [%rd2140+16];
	add.f64 	%fd9408, %fd4611, 0d0000000000000000;
	ld.global.f64 	%fd4612, [%rd2140+24];
	add.f64 	%fd9409, %fd4612, 0d0000000000000000;
	ld.global.f64 	%fd4613, [%rd2140+32];
	add.f64 	%fd9410, %fd4613, 0d0000000000000000;
	ld.global.f64 	%fd4614, [%rd2140+40];
	add.f64 	%fd9411, %fd4614, 0d0000000000000000;
	ld.global.f64 	%fd4615, [%rd2140+48];
	add.f64 	%fd9412, %fd4615, 0d0000000000000000;
	ld.global.f64 	%fd4616, [%rd2140+56];
	add.f64 	%fd9413, %fd4616, 0d0000000000000000;
	ld.global.f64 	%fd4617, [%rd2140+64];
	add.f64 	%fd9414, %fd4617, 0d0000000000000000;

$L__BB13_391:
	add.s32 	%r2479, %r80, 9;
	add.f64 	%fd1162, %fd9414, 0d0000000000000000;
	add.f64 	%fd1163, %fd9413, 0d0000000000000000;
	add.f64 	%fd1164, %fd9412, 0d0000000000000000;
	add.f64 	%fd1165, %fd9411, 0d0000000000000000;
	add.f64 	%fd1166, %fd9410, 0d0000000000000000;
	add.f64 	%fd1167, %fd9409, 0d0000000000000000;
	add.f64 	%fd1168, %fd9408, 0d0000000000000000;
	add.f64 	%fd1169, %fd9407, 0d0000000000000000;
	add.f64 	%fd1170, %fd9406, 0d0000000000000000;
	ld.param.u64 	%rd280, [%rd357+120];
	ld.param.u32 	%r679, [%rd357+144];
	ld.param.u32 	%r680, [%rd357+172];
	ld.param.v2.u32 	{%r1640, %r1641}, [%rd357+176];
	setp.le.s32 	%p698, %r1640, %r149;
	setp.le.s32 	%p699, %r1641, %r89;
	setp.le.s32 	%p700, %r680, %r2479;
	or.pred  	%p701, %p698, %p699;
	or.pred  	%p703, %p141, %p701;
	or.pred  	%p704, %p700, %p703;
	mov.f64 	%fd9415, 0d0000000000000000;
	mov.f64 	%fd9416, 0d0000000000000000;
	mov.f64 	%fd9417, 0d0000000000000000;
	mov.f64 	%fd9418, 0d0000000000000000;
	mov.f64 	%fd9419, 0d0000000000000000;
	mov.f64 	%fd9420, 0d0000000000000000;
	mov.f64 	%fd9421, 0d0000000000000000;
	mov.f64 	%fd9422, 0d0000000000000000;
	mov.f64 	%fd9423, 0d0000000000000000;
	@%p704 bra 	$L__BB13_394;
	bra.uni 	$L__BB13_392;

$L__BB13_394:
	add.s32 	%r2295, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2295, %r89};
	add.s32 	%r2296, %r80, 9;
	st.local.v2.u32 	[%rd30+8], {%r2296, %r1640};
	st.local.v2.u32 	[%rd30+16], {%r1641, %r680};
	mov.u64 	%rd2147, $str$2;
	cvta.global.u64 	%rd2148, %rd2147;
	{ // callseq 439, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2148;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1645, [retval0+0];
	} // callseq 439
	bra.uni 	$L__BB13_395;

$L__BB13_392:
	setp.eq.s64 	%p705, %rd280, 0;
	@%p705 bra 	$L__BB13_395;

	add.s32 	%r2480, %r80, 9;
	cvta.to.global.u64 	%rd2144, %rd280;
	mul.wide.s32 	%rd2145, %r679, %r2480;
	add.s64 	%rd2146, %rd2144, %rd2145;
	ld.global.f64 	%fd4636, [%rd2146];
	add.f64 	%fd9415, %fd4636, 0d0000000000000000;
	ld.global.f64 	%fd4637, [%rd2146+8];
	add.f64 	%fd9416, %fd4637, 0d0000000000000000;
	ld.global.f64 	%fd4638, [%rd2146+16];
	add.f64 	%fd9417, %fd4638, 0d0000000000000000;
	ld.global.f64 	%fd4639, [%rd2146+24];
	add.f64 	%fd9418, %fd4639, 0d0000000000000000;
	ld.global.f64 	%fd4640, [%rd2146+32];
	add.f64 	%fd9419, %fd4640, 0d0000000000000000;
	ld.global.f64 	%fd4641, [%rd2146+40];
	add.f64 	%fd9420, %fd4641, 0d0000000000000000;
	ld.global.f64 	%fd4642, [%rd2146+48];
	add.f64 	%fd9421, %fd4642, 0d0000000000000000;
	ld.global.f64 	%fd4643, [%rd2146+56];
	add.f64 	%fd9422, %fd4643, 0d0000000000000000;
	ld.global.f64 	%fd4644, [%rd2146+64];
	add.f64 	%fd9423, %fd4644, 0d0000000000000000;

$L__BB13_395:
	add.s32 	%r2481, %r80, 8;
	add.f64 	%fd1189, %fd9423, 0d0000000000000000;
	add.f64 	%fd1190, %fd9422, 0d0000000000000000;
	add.f64 	%fd1191, %fd9421, 0d0000000000000000;
	add.f64 	%fd1192, %fd9420, 0d0000000000000000;
	add.f64 	%fd1193, %fd9419, 0d0000000000000000;
	add.f64 	%fd1194, %fd9418, 0d0000000000000000;
	add.f64 	%fd1195, %fd9417, 0d0000000000000000;
	add.f64 	%fd1196, %fd9416, 0d0000000000000000;
	add.f64 	%fd1197, %fd9415, 0d0000000000000000;
	ld.param.u64 	%rd281, [%rd357+120];
	ld.param.u32 	%r684, [%rd357+144];
	ld.param.u32 	%r685, [%rd357+172];
	ld.param.v2.u32 	{%r1646, %r1647}, [%rd357+176];
	setp.le.s32 	%p706, %r1646, %r149;
	setp.le.s32 	%p707, %r1647, %r72;
	setp.le.s32 	%p708, %r685, %r2481;
	or.pred  	%p709, %p706, %p707;
	or.pred  	%p711, %p134, %p709;
	or.pred  	%p712, %p708, %p711;
	mov.f64 	%fd9424, 0d0000000000000000;
	mov.f64 	%fd9425, 0d0000000000000000;
	mov.f64 	%fd9426, 0d0000000000000000;
	mov.f64 	%fd9427, 0d0000000000000000;
	mov.f64 	%fd9428, 0d0000000000000000;
	mov.f64 	%fd9429, 0d0000000000000000;
	mov.f64 	%fd9430, 0d0000000000000000;
	mov.f64 	%fd9431, 0d0000000000000000;
	mov.f64 	%fd9432, 0d0000000000000000;
	@%p712 bra 	$L__BB13_398;
	bra.uni 	$L__BB13_396;

$L__BB13_398:
	add.s32 	%r2297, %r71, 2;
	st.local.v2.u32 	[%rd30], {%r2297, %r72};
	add.s32 	%r2298, %r80, 8;
	st.local.v2.u32 	[%rd30+8], {%r2298, %r1646};
	st.local.v2.u32 	[%rd30+16], {%r1647, %r685};
	mov.u64 	%rd2153, $str$2;
	cvta.global.u64 	%rd2154, %rd2153;
	{ // callseq 440, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2154;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1651, [retval0+0];
	} // callseq 440
	bra.uni 	$L__BB13_399;

$L__BB13_396:
	setp.eq.s64 	%p713, %rd281, 0;
	@%p713 bra 	$L__BB13_399;

	add.s32 	%r2482, %r80, 8;
	cvta.to.global.u64 	%rd2150, %rd281;
	mul.wide.s32 	%rd2151, %r684, %r2482;
	add.s64 	%rd2152, %rd2150, %rd2151;
	ld.global.f64 	%fd4663, [%rd2152];
	add.f64 	%fd9424, %fd4663, 0d0000000000000000;
	ld.global.f64 	%fd4664, [%rd2152+8];
	add.f64 	%fd9425, %fd4664, 0d0000000000000000;
	ld.global.f64 	%fd4665, [%rd2152+16];
	add.f64 	%fd9426, %fd4665, 0d0000000000000000;
	ld.global.f64 	%fd4666, [%rd2152+24];
	add.f64 	%fd9427, %fd4666, 0d0000000000000000;
	ld.global.f64 	%fd4667, [%rd2152+32];
	add.f64 	%fd9428, %fd4667, 0d0000000000000000;
	ld.global.f64 	%fd4668, [%rd2152+40];
	add.f64 	%fd9429, %fd4668, 0d0000000000000000;
	ld.global.f64 	%fd4669, [%rd2152+48];
	add.f64 	%fd9430, %fd4669, 0d0000000000000000;
	ld.global.f64 	%fd4670, [%rd2152+56];
	add.f64 	%fd9431, %fd4670, 0d0000000000000000;
	ld.global.f64 	%fd4671, [%rd2152+64];
	add.f64 	%fd9432, %fd4671, 0d0000000000000000;

$L__BB13_399:
	add.s32 	%r2483, %r80, 7;
	add.f64 	%fd1216, %fd9432, 0d0000000000000000;
	add.f64 	%fd1217, %fd9431, 0d0000000000000000;
	add.f64 	%fd1218, %fd9430, 0d0000000000000000;
	add.f64 	%fd1219, %fd9429, 0d0000000000000000;
	add.f64 	%fd1220, %fd9428, 0d0000000000000000;
	add.f64 	%fd1221, %fd9427, 0d0000000000000000;
	add.f64 	%fd1222, %fd9426, 0d0000000000000000;
	add.f64 	%fd1223, %fd9425, 0d0000000000000000;
	add.f64 	%fd1224, %fd9424, 0d0000000000000000;
	ld.param.u64 	%rd282, [%rd357+120];
	ld.param.u32 	%r689, [%rd357+144];
	ld.param.u32 	%r690, [%rd357+172];
	ld.param.v2.u32 	{%r1652, %r1653}, [%rd357+176];
	setp.le.s32 	%p714, %r1652, %r116;
	setp.le.s32 	%p715, %r1653, %r107;
	setp.le.s32 	%p716, %r690, %r2483;
	or.pred  	%p717, %p714, %p715;
	or.pred  	%p719, %p127, %p717;
	or.pred  	%p720, %p716, %p719;
	mov.f64 	%fd9433, 0d0000000000000000;
	mov.f64 	%fd9434, 0d0000000000000000;
	mov.f64 	%fd9435, 0d0000000000000000;
	mov.f64 	%fd9436, 0d0000000000000000;
	mov.f64 	%fd9437, 0d0000000000000000;
	mov.f64 	%fd9438, 0d0000000000000000;
	mov.f64 	%fd9439, 0d0000000000000000;
	mov.f64 	%fd9440, 0d0000000000000000;
	mov.f64 	%fd9441, 0d0000000000000000;
	@%p720 bra 	$L__BB13_402;
	bra.uni 	$L__BB13_400;

$L__BB13_402:
	add.s32 	%r2299, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2299, %r107};
	add.s32 	%r2300, %r80, 7;
	st.local.v2.u32 	[%rd30+8], {%r2300, %r1652};
	st.local.v2.u32 	[%rd30+16], {%r1653, %r690};
	mov.u64 	%rd2159, $str$2;
	cvta.global.u64 	%rd2160, %rd2159;
	{ // callseq 441, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2160;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1657, [retval0+0];
	} // callseq 441
	bra.uni 	$L__BB13_403;

$L__BB13_400:
	setp.eq.s64 	%p721, %rd282, 0;
	@%p721 bra 	$L__BB13_403;

	add.s32 	%r2484, %r80, 7;
	cvta.to.global.u64 	%rd2156, %rd282;
	mul.wide.s32 	%rd2157, %r689, %r2484;
	add.s64 	%rd2158, %rd2156, %rd2157;
	ld.global.f64 	%fd4690, [%rd2158];
	add.f64 	%fd9433, %fd4690, 0d0000000000000000;
	ld.global.f64 	%fd4691, [%rd2158+8];
	add.f64 	%fd9434, %fd4691, 0d0000000000000000;
	ld.global.f64 	%fd4692, [%rd2158+16];
	add.f64 	%fd9435, %fd4692, 0d0000000000000000;
	ld.global.f64 	%fd4693, [%rd2158+24];
	add.f64 	%fd9436, %fd4693, 0d0000000000000000;
	ld.global.f64 	%fd4694, [%rd2158+32];
	add.f64 	%fd9437, %fd4694, 0d0000000000000000;
	ld.global.f64 	%fd4695, [%rd2158+40];
	add.f64 	%fd9438, %fd4695, 0d0000000000000000;
	ld.global.f64 	%fd4696, [%rd2158+48];
	add.f64 	%fd9439, %fd4696, 0d0000000000000000;
	ld.global.f64 	%fd4697, [%rd2158+56];
	add.f64 	%fd9440, %fd4697, 0d0000000000000000;
	ld.global.f64 	%fd4698, [%rd2158+64];
	add.f64 	%fd9441, %fd4698, 0d0000000000000000;

$L__BB13_403:
	add.s32 	%r2485, %r80, 6;
	add.f64 	%fd1243, %fd9441, 0d0000000000000000;
	add.f64 	%fd1244, %fd9440, 0d0000000000000000;
	add.f64 	%fd1245, %fd9439, 0d0000000000000000;
	add.f64 	%fd1246, %fd9438, 0d0000000000000000;
	add.f64 	%fd1247, %fd9437, 0d0000000000000000;
	add.f64 	%fd1248, %fd9436, 0d0000000000000000;
	add.f64 	%fd1249, %fd9435, 0d0000000000000000;
	add.f64 	%fd1250, %fd9434, 0d0000000000000000;
	add.f64 	%fd1251, %fd9433, 0d0000000000000000;
	ld.param.u64 	%rd283, [%rd357+120];
	ld.param.u32 	%r694, [%rd357+144];
	ld.param.u32 	%r695, [%rd357+172];
	ld.param.v2.u32 	{%r1658, %r1659}, [%rd357+176];
	setp.le.s32 	%p722, %r1658, %r116;
	setp.le.s32 	%p723, %r1659, %r98;
	setp.le.s32 	%p724, %r695, %r2485;
	or.pred  	%p725, %p722, %p723;
	or.pred  	%p727, %p120, %p725;
	or.pred  	%p728, %p724, %p727;
	mov.f64 	%fd9442, 0d0000000000000000;
	mov.f64 	%fd9443, 0d0000000000000000;
	mov.f64 	%fd9444, 0d0000000000000000;
	mov.f64 	%fd9445, 0d0000000000000000;
	mov.f64 	%fd9446, 0d0000000000000000;
	mov.f64 	%fd9447, 0d0000000000000000;
	mov.f64 	%fd9448, 0d0000000000000000;
	mov.f64 	%fd9449, 0d0000000000000000;
	mov.f64 	%fd9450, 0d0000000000000000;
	@%p728 bra 	$L__BB13_406;
	bra.uni 	$L__BB13_404;

$L__BB13_406:
	add.s32 	%r2301, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2301, %r98};
	add.s32 	%r2302, %r80, 6;
	st.local.v2.u32 	[%rd30+8], {%r2302, %r1658};
	st.local.v2.u32 	[%rd30+16], {%r1659, %r695};
	mov.u64 	%rd2165, $str$2;
	cvta.global.u64 	%rd2166, %rd2165;
	{ // callseq 442, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2166;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1663, [retval0+0];
	} // callseq 442
	bra.uni 	$L__BB13_407;

$L__BB13_404:
	setp.eq.s64 	%p729, %rd283, 0;
	@%p729 bra 	$L__BB13_407;

	add.s32 	%r2486, %r80, 6;
	cvta.to.global.u64 	%rd2162, %rd283;
	mul.wide.s32 	%rd2163, %r694, %r2486;
	add.s64 	%rd2164, %rd2162, %rd2163;
	ld.global.f64 	%fd4717, [%rd2164];
	add.f64 	%fd9442, %fd4717, 0d0000000000000000;
	ld.global.f64 	%fd4718, [%rd2164+8];
	add.f64 	%fd9443, %fd4718, 0d0000000000000000;
	ld.global.f64 	%fd4719, [%rd2164+16];
	add.f64 	%fd9444, %fd4719, 0d0000000000000000;
	ld.global.f64 	%fd4720, [%rd2164+24];
	add.f64 	%fd9445, %fd4720, 0d0000000000000000;
	ld.global.f64 	%fd4721, [%rd2164+32];
	add.f64 	%fd9446, %fd4721, 0d0000000000000000;
	ld.global.f64 	%fd4722, [%rd2164+40];
	add.f64 	%fd9447, %fd4722, 0d0000000000000000;
	ld.global.f64 	%fd4723, [%rd2164+48];
	add.f64 	%fd9448, %fd4723, 0d0000000000000000;
	ld.global.f64 	%fd4724, [%rd2164+56];
	add.f64 	%fd9449, %fd4724, 0d0000000000000000;
	ld.global.f64 	%fd4725, [%rd2164+64];
	add.f64 	%fd9450, %fd4725, 0d0000000000000000;

$L__BB13_407:
	add.s32 	%r2487, %r80, 5;
	add.f64 	%fd1270, %fd9450, 0d0000000000000000;
	add.f64 	%fd1271, %fd9449, 0d0000000000000000;
	add.f64 	%fd1272, %fd9448, 0d0000000000000000;
	add.f64 	%fd1273, %fd9447, 0d0000000000000000;
	add.f64 	%fd1274, %fd9446, 0d0000000000000000;
	add.f64 	%fd1275, %fd9445, 0d0000000000000000;
	add.f64 	%fd1276, %fd9444, 0d0000000000000000;
	add.f64 	%fd1277, %fd9443, 0d0000000000000000;
	add.f64 	%fd1278, %fd9442, 0d0000000000000000;
	ld.param.u64 	%rd284, [%rd357+120];
	ld.param.u32 	%r699, [%rd357+144];
	ld.param.u32 	%r700, [%rd357+172];
	ld.param.v2.u32 	{%r1664, %r1665}, [%rd357+176];
	setp.le.s32 	%p730, %r1664, %r116;
	setp.le.s32 	%p731, %r1665, %r89;
	setp.le.s32 	%p732, %r700, %r2487;
	or.pred  	%p733, %p730, %p731;
	or.pred  	%p735, %p113, %p733;
	or.pred  	%p736, %p732, %p735;
	mov.f64 	%fd9451, 0d0000000000000000;
	mov.f64 	%fd9452, 0d0000000000000000;
	mov.f64 	%fd9453, 0d0000000000000000;
	mov.f64 	%fd9454, 0d0000000000000000;
	mov.f64 	%fd9455, 0d0000000000000000;
	mov.f64 	%fd9456, 0d0000000000000000;
	mov.f64 	%fd9457, 0d0000000000000000;
	mov.f64 	%fd9458, 0d0000000000000000;
	mov.f64 	%fd9459, 0d0000000000000000;
	@%p736 bra 	$L__BB13_410;
	bra.uni 	$L__BB13_408;

$L__BB13_410:
	add.s32 	%r2303, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2303, %r89};
	add.s32 	%r2304, %r80, 5;
	st.local.v2.u32 	[%rd30+8], {%r2304, %r1664};
	st.local.v2.u32 	[%rd30+16], {%r1665, %r700};
	mov.u64 	%rd2171, $str$2;
	cvta.global.u64 	%rd2172, %rd2171;
	{ // callseq 443, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2172;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1669, [retval0+0];
	} // callseq 443
	bra.uni 	$L__BB13_411;

$L__BB13_408:
	setp.eq.s64 	%p737, %rd284, 0;
	@%p737 bra 	$L__BB13_411;

	add.s32 	%r2488, %r80, 5;
	cvta.to.global.u64 	%rd2168, %rd284;
	mul.wide.s32 	%rd2169, %r699, %r2488;
	add.s64 	%rd2170, %rd2168, %rd2169;
	ld.global.f64 	%fd4744, [%rd2170];
	add.f64 	%fd9451, %fd4744, 0d0000000000000000;
	ld.global.f64 	%fd4745, [%rd2170+8];
	add.f64 	%fd9452, %fd4745, 0d0000000000000000;
	ld.global.f64 	%fd4746, [%rd2170+16];
	add.f64 	%fd9453, %fd4746, 0d0000000000000000;
	ld.global.f64 	%fd4747, [%rd2170+24];
	add.f64 	%fd9454, %fd4747, 0d0000000000000000;
	ld.global.f64 	%fd4748, [%rd2170+32];
	add.f64 	%fd9455, %fd4748, 0d0000000000000000;
	ld.global.f64 	%fd4749, [%rd2170+40];
	add.f64 	%fd9456, %fd4749, 0d0000000000000000;
	ld.global.f64 	%fd4750, [%rd2170+48];
	add.f64 	%fd9457, %fd4750, 0d0000000000000000;
	ld.global.f64 	%fd4751, [%rd2170+56];
	add.f64 	%fd9458, %fd4751, 0d0000000000000000;
	ld.global.f64 	%fd4752, [%rd2170+64];
	add.f64 	%fd9459, %fd4752, 0d0000000000000000;

$L__BB13_411:
	add.s32 	%r2489, %r80, 4;
	add.f64 	%fd1297, %fd9459, 0d0000000000000000;
	add.f64 	%fd1298, %fd9458, 0d0000000000000000;
	add.f64 	%fd1299, %fd9457, 0d0000000000000000;
	add.f64 	%fd1300, %fd9456, 0d0000000000000000;
	add.f64 	%fd1301, %fd9455, 0d0000000000000000;
	add.f64 	%fd1302, %fd9454, 0d0000000000000000;
	add.f64 	%fd1303, %fd9453, 0d0000000000000000;
	add.f64 	%fd1304, %fd9452, 0d0000000000000000;
	add.f64 	%fd1305, %fd9451, 0d0000000000000000;
	ld.param.u64 	%rd285, [%rd357+120];
	ld.param.u32 	%r704, [%rd357+144];
	ld.param.u32 	%r705, [%rd357+172];
	ld.param.v2.u32 	{%r1670, %r1671}, [%rd357+176];
	setp.le.s32 	%p738, %r1670, %r116;
	setp.le.s32 	%p739, %r1671, %r72;
	setp.le.s32 	%p740, %r705, %r2489;
	or.pred  	%p741, %p738, %p739;
	or.pred  	%p743, %p106, %p741;
	or.pred  	%p744, %p740, %p743;
	mov.f64 	%fd9460, 0d0000000000000000;
	mov.f64 	%fd9461, 0d0000000000000000;
	mov.f64 	%fd9462, 0d0000000000000000;
	mov.f64 	%fd9463, 0d0000000000000000;
	mov.f64 	%fd9464, 0d0000000000000000;
	mov.f64 	%fd9465, 0d0000000000000000;
	mov.f64 	%fd9466, 0d0000000000000000;
	mov.f64 	%fd9467, 0d0000000000000000;
	mov.f64 	%fd9468, 0d0000000000000000;
	@%p744 bra 	$L__BB13_414;
	bra.uni 	$L__BB13_412;

$L__BB13_414:
	add.s32 	%r2305, %r71, 1;
	st.local.v2.u32 	[%rd30], {%r2305, %r72};
	add.s32 	%r2306, %r80, 4;
	st.local.v2.u32 	[%rd30+8], {%r2306, %r1670};
	st.local.v2.u32 	[%rd30+16], {%r1671, %r705};
	mov.u64 	%rd2177, $str$2;
	cvta.global.u64 	%rd2178, %rd2177;
	{ // callseq 444, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2178;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1675, [retval0+0];
	} // callseq 444
	bra.uni 	$L__BB13_415;

$L__BB13_412:
	setp.eq.s64 	%p745, %rd285, 0;
	@%p745 bra 	$L__BB13_415;

	add.s32 	%r2490, %r80, 4;
	cvta.to.global.u64 	%rd2174, %rd285;
	mul.wide.s32 	%rd2175, %r704, %r2490;
	add.s64 	%rd2176, %rd2174, %rd2175;
	ld.global.f64 	%fd4771, [%rd2176];
	add.f64 	%fd9460, %fd4771, 0d0000000000000000;
	ld.global.f64 	%fd4772, [%rd2176+8];
	add.f64 	%fd9461, %fd4772, 0d0000000000000000;
	ld.global.f64 	%fd4773, [%rd2176+16];
	add.f64 	%fd9462, %fd4773, 0d0000000000000000;
	ld.global.f64 	%fd4774, [%rd2176+24];
	add.f64 	%fd9463, %fd4774, 0d0000000000000000;
	ld.global.f64 	%fd4775, [%rd2176+32];
	add.f64 	%fd9464, %fd4775, 0d0000000000000000;
	ld.global.f64 	%fd4776, [%rd2176+40];
	add.f64 	%fd9465, %fd4776, 0d0000000000000000;
	ld.global.f64 	%fd4777, [%rd2176+48];
	add.f64 	%fd9466, %fd4777, 0d0000000000000000;
	ld.global.f64 	%fd4778, [%rd2176+56];
	add.f64 	%fd9467, %fd4778, 0d0000000000000000;
	ld.global.f64 	%fd4779, [%rd2176+64];
	add.f64 	%fd9468, %fd4779, 0d0000000000000000;

$L__BB13_415:
	add.s32 	%r2491, %r80, 3;
	add.f64 	%fd1324, %fd9468, 0d0000000000000000;
	add.f64 	%fd1325, %fd9467, 0d0000000000000000;
	add.f64 	%fd1326, %fd9466, 0d0000000000000000;
	add.f64 	%fd1327, %fd9465, 0d0000000000000000;
	add.f64 	%fd1328, %fd9464, 0d0000000000000000;
	add.f64 	%fd1329, %fd9463, 0d0000000000000000;
	add.f64 	%fd1330, %fd9462, 0d0000000000000000;
	add.f64 	%fd1331, %fd9461, 0d0000000000000000;
	add.f64 	%fd1332, %fd9460, 0d0000000000000000;
	ld.param.u64 	%rd286, [%rd357+120];
	ld.param.u32 	%r709, [%rd357+144];
	ld.param.u32 	%r710, [%rd357+172];
	ld.param.v2.u32 	{%r1676, %r1677}, [%rd357+176];
	setp.le.s32 	%p746, %r1676, %r71;
	setp.le.s32 	%p747, %r1677, %r107;
	setp.le.s32 	%p748, %r710, %r2491;
	or.pred  	%p749, %p746, %p747;
	or.pred  	%p751, %p99, %p749;
	or.pred  	%p752, %p748, %p751;
	mov.f64 	%fd9469, 0d0000000000000000;
	mov.f64 	%fd9470, 0d0000000000000000;
	mov.f64 	%fd9471, 0d0000000000000000;
	mov.f64 	%fd9472, 0d0000000000000000;
	mov.f64 	%fd9473, 0d0000000000000000;
	mov.f64 	%fd9474, 0d0000000000000000;
	mov.f64 	%fd9475, 0d0000000000000000;
	mov.f64 	%fd9476, 0d0000000000000000;
	mov.f64 	%fd9477, 0d0000000000000000;
	@%p752 bra 	$L__BB13_418;
	bra.uni 	$L__BB13_416;

$L__BB13_418:
	add.s32 	%r2307, %r72, 3;
	st.local.v2.u32 	[%rd30], {%r71, %r2307};
	add.s32 	%r2308, %r80, 3;
	st.local.v2.u32 	[%rd30+8], {%r2308, %r1676};
	st.local.v2.u32 	[%rd30+16], {%r1677, %r710};
	mov.u64 	%rd2183, $str$2;
	cvta.global.u64 	%rd2184, %rd2183;
	{ // callseq 445, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2184;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1681, [retval0+0];
	} // callseq 445
	bra.uni 	$L__BB13_419;

$L__BB13_416:
	setp.eq.s64 	%p753, %rd286, 0;
	@%p753 bra 	$L__BB13_419;

	add.s32 	%r2492, %r80, 3;
	cvta.to.global.u64 	%rd2180, %rd286;
	mul.wide.s32 	%rd2181, %r709, %r2492;
	add.s64 	%rd2182, %rd2180, %rd2181;
	ld.global.f64 	%fd4798, [%rd2182];
	add.f64 	%fd9469, %fd4798, 0d0000000000000000;
	ld.global.f64 	%fd4799, [%rd2182+8];
	add.f64 	%fd9470, %fd4799, 0d0000000000000000;
	ld.global.f64 	%fd4800, [%rd2182+16];
	add.f64 	%fd9471, %fd4800, 0d0000000000000000;
	ld.global.f64 	%fd4801, [%rd2182+24];
	add.f64 	%fd9472, %fd4801, 0d0000000000000000;
	ld.global.f64 	%fd4802, [%rd2182+32];
	add.f64 	%fd9473, %fd4802, 0d0000000000000000;
	ld.global.f64 	%fd4803, [%rd2182+40];
	add.f64 	%fd9474, %fd4803, 0d0000000000000000;
	ld.global.f64 	%fd4804, [%rd2182+48];
	add.f64 	%fd9475, %fd4804, 0d0000000000000000;
	ld.global.f64 	%fd4805, [%rd2182+56];
	add.f64 	%fd9476, %fd4805, 0d0000000000000000;
	ld.global.f64 	%fd4806, [%rd2182+64];
	add.f64 	%fd9477, %fd4806, 0d0000000000000000;

$L__BB13_419:
	add.s32 	%r2493, %r80, 2;
	add.f64 	%fd1351, %fd9477, 0d0000000000000000;
	add.f64 	%fd1352, %fd9476, 0d0000000000000000;
	add.f64 	%fd1353, %fd9475, 0d0000000000000000;
	add.f64 	%fd1354, %fd9474, 0d0000000000000000;
	add.f64 	%fd1355, %fd9473, 0d0000000000000000;
	add.f64 	%fd1356, %fd9472, 0d0000000000000000;
	add.f64 	%fd1357, %fd9471, 0d0000000000000000;
	add.f64 	%fd1358, %fd9470, 0d0000000000000000;
	add.f64 	%fd1359, %fd9469, 0d0000000000000000;
	ld.param.u64 	%rd287, [%rd357+120];
	ld.param.u32 	%r714, [%rd357+144];
	ld.param.u32 	%r715, [%rd357+172];
	ld.param.v2.u32 	{%r1682, %r1683}, [%rd357+176];
	setp.le.s32 	%p754, %r1682, %r71;
	setp.le.s32 	%p755, %r1683, %r98;
	setp.le.s32 	%p756, %r715, %r2493;
	or.pred  	%p757, %p754, %p755;
	or.pred  	%p759, %p92, %p757;
	or.pred  	%p760, %p756, %p759;
	mov.f64 	%fd9478, 0d0000000000000000;
	mov.f64 	%fd9479, 0d0000000000000000;
	mov.f64 	%fd9480, 0d0000000000000000;
	mov.f64 	%fd9481, 0d0000000000000000;
	mov.f64 	%fd9482, 0d0000000000000000;
	mov.f64 	%fd9483, 0d0000000000000000;
	mov.f64 	%fd9484, 0d0000000000000000;
	mov.f64 	%fd9485, 0d0000000000000000;
	mov.f64 	%fd9486, 0d0000000000000000;
	@%p760 bra 	$L__BB13_422;
	bra.uni 	$L__BB13_420;

$L__BB13_422:
	add.s32 	%r2309, %r72, 2;
	st.local.v2.u32 	[%rd30], {%r71, %r2309};
	add.s32 	%r2310, %r80, 2;
	st.local.v2.u32 	[%rd30+8], {%r2310, %r1682};
	st.local.v2.u32 	[%rd30+16], {%r1683, %r715};
	mov.u64 	%rd2189, $str$2;
	cvta.global.u64 	%rd2190, %rd2189;
	{ // callseq 446, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2190;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1687, [retval0+0];
	} // callseq 446
	bra.uni 	$L__BB13_423;

$L__BB13_420:
	setp.eq.s64 	%p761, %rd287, 0;
	@%p761 bra 	$L__BB13_423;

	add.s32 	%r2494, %r80, 2;
	cvta.to.global.u64 	%rd2186, %rd287;
	mul.wide.s32 	%rd2187, %r714, %r2494;
	add.s64 	%rd2188, %rd2186, %rd2187;
	ld.global.f64 	%fd4825, [%rd2188];
	add.f64 	%fd9478, %fd4825, 0d0000000000000000;
	ld.global.f64 	%fd4826, [%rd2188+8];
	add.f64 	%fd9479, %fd4826, 0d0000000000000000;
	ld.global.f64 	%fd4827, [%rd2188+16];
	add.f64 	%fd9480, %fd4827, 0d0000000000000000;
	ld.global.f64 	%fd4828, [%rd2188+24];
	add.f64 	%fd9481, %fd4828, 0d0000000000000000;
	ld.global.f64 	%fd4829, [%rd2188+32];
	add.f64 	%fd9482, %fd4829, 0d0000000000000000;
	ld.global.f64 	%fd4830, [%rd2188+40];
	add.f64 	%fd9483, %fd4830, 0d0000000000000000;
	ld.global.f64 	%fd4831, [%rd2188+48];
	add.f64 	%fd9484, %fd4831, 0d0000000000000000;
	ld.global.f64 	%fd4832, [%rd2188+56];
	add.f64 	%fd9485, %fd4832, 0d0000000000000000;
	ld.global.f64 	%fd4833, [%rd2188+64];
	add.f64 	%fd9486, %fd4833, 0d0000000000000000;

$L__BB13_423:
	add.s32 	%r2495, %r80, 1;
	add.f64 	%fd1378, %fd9486, 0d0000000000000000;
	add.f64 	%fd1379, %fd9485, 0d0000000000000000;
	add.f64 	%fd1380, %fd9484, 0d0000000000000000;
	add.f64 	%fd1381, %fd9483, 0d0000000000000000;
	add.f64 	%fd1382, %fd9482, 0d0000000000000000;
	add.f64 	%fd1383, %fd9481, 0d0000000000000000;
	add.f64 	%fd1384, %fd9480, 0d0000000000000000;
	add.f64 	%fd1385, %fd9479, 0d0000000000000000;
	add.f64 	%fd1386, %fd9478, 0d0000000000000000;
	ld.param.u64 	%rd288, [%rd357+120];
	ld.param.u32 	%r719, [%rd357+144];
	ld.param.u32 	%r720, [%rd357+172];
	ld.param.v2.u32 	{%r1688, %r1689}, [%rd357+176];
	setp.le.s32 	%p762, %r1688, %r71;
	setp.le.s32 	%p763, %r1689, %r89;
	setp.le.s32 	%p764, %r720, %r2495;
	or.pred  	%p765, %p762, %p763;
	or.pred  	%p767, %p85, %p765;
	or.pred  	%p768, %p764, %p767;
	mov.f64 	%fd9487, 0d0000000000000000;
	mov.f64 	%fd9488, 0d0000000000000000;
	mov.f64 	%fd9489, 0d0000000000000000;
	mov.f64 	%fd9490, 0d0000000000000000;
	mov.f64 	%fd9491, 0d0000000000000000;
	mov.f64 	%fd9492, 0d0000000000000000;
	mov.f64 	%fd9493, 0d0000000000000000;
	mov.f64 	%fd9494, 0d0000000000000000;
	mov.f64 	%fd9495, 0d0000000000000000;
	@%p768 bra 	$L__BB13_426;
	bra.uni 	$L__BB13_424;

$L__BB13_426:
	add.s32 	%r2311, %r72, 1;
	st.local.v2.u32 	[%rd30], {%r71, %r2311};
	add.s32 	%r2312, %r80, 1;
	st.local.v2.u32 	[%rd30+8], {%r2312, %r1688};
	st.local.v2.u32 	[%rd30+16], {%r1689, %r720};
	mov.u64 	%rd2195, $str$2;
	cvta.global.u64 	%rd2196, %rd2195;
	{ // callseq 447, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2196;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1693, [retval0+0];
	} // callseq 447
	bra.uni 	$L__BB13_427;

$L__BB13_424:
	setp.eq.s64 	%p769, %rd288, 0;
	@%p769 bra 	$L__BB13_427;

	add.s32 	%r2496, %r80, 1;
	cvta.to.global.u64 	%rd2192, %rd288;
	mul.wide.s32 	%rd2193, %r719, %r2496;
	add.s64 	%rd2194, %rd2192, %rd2193;
	ld.global.f64 	%fd4852, [%rd2194];
	add.f64 	%fd9487, %fd4852, 0d0000000000000000;
	ld.global.f64 	%fd4853, [%rd2194+8];
	add.f64 	%fd9488, %fd4853, 0d0000000000000000;
	ld.global.f64 	%fd4854, [%rd2194+16];
	add.f64 	%fd9489, %fd4854, 0d0000000000000000;
	ld.global.f64 	%fd4855, [%rd2194+24];
	add.f64 	%fd9490, %fd4855, 0d0000000000000000;
	ld.global.f64 	%fd4856, [%rd2194+32];
	add.f64 	%fd9491, %fd4856, 0d0000000000000000;
	ld.global.f64 	%fd4857, [%rd2194+40];
	add.f64 	%fd9492, %fd4857, 0d0000000000000000;
	ld.global.f64 	%fd4858, [%rd2194+48];
	add.f64 	%fd9493, %fd4858, 0d0000000000000000;
	ld.global.f64 	%fd4859, [%rd2194+56];
	add.f64 	%fd9494, %fd4859, 0d0000000000000000;
	ld.global.f64 	%fd4860, [%rd2194+64];
	add.f64 	%fd9495, %fd4860, 0d0000000000000000;

$L__BB13_427:
	add.f64 	%fd1405, %fd9495, 0d0000000000000000;
	add.f64 	%fd1406, %fd9494, 0d0000000000000000;
	add.f64 	%fd1407, %fd9493, 0d0000000000000000;
	add.f64 	%fd1408, %fd9492, 0d0000000000000000;
	add.f64 	%fd1409, %fd9491, 0d0000000000000000;
	add.f64 	%fd1410, %fd9490, 0d0000000000000000;
	add.f64 	%fd1411, %fd9489, 0d0000000000000000;
	add.f64 	%fd1412, %fd9488, 0d0000000000000000;
	add.f64 	%fd1413, %fd9487, 0d0000000000000000;
	ld.param.u64 	%rd289, [%rd357+120];
	ld.param.u32 	%r724, [%rd357+144];
	ld.param.u32 	%r725, [%rd357+172];
	ld.param.v2.u32 	{%r1694, %r1695}, [%rd357+176];
	setp.le.s32 	%p770, %r1694, %r71;
	setp.le.s32 	%p771, %r1695, %r72;
	setp.le.s32 	%p772, %r725, %r80;
	or.pred  	%p773, %p770, %p771;
	or.pred  	%p775, %p78, %p773;
	or.pred  	%p776, %p772, %p775;
	mov.f64 	%fd9496, 0d0000000000000000;
	mov.f64 	%fd9497, 0d0000000000000000;
	mov.f64 	%fd9498, 0d0000000000000000;
	mov.f64 	%fd9499, 0d0000000000000000;
	mov.f64 	%fd9500, 0d0000000000000000;
	mov.f64 	%fd9501, 0d0000000000000000;
	mov.f64 	%fd9502, 0d0000000000000000;
	mov.f64 	%fd9503, 0d0000000000000000;
	mov.f64 	%fd9504, 0d0000000000000000;
	@%p776 bra 	$L__BB13_430;
	bra.uni 	$L__BB13_428;

$L__BB13_430:
	st.local.v2.u32 	[%rd30], {%r71, %r72};
	st.local.v2.u32 	[%rd30+8], {%r80, %r1694};
	st.local.v2.u32 	[%rd30+16], {%r1695, %r725};
	mov.u64 	%rd2201, $str$2;
	cvta.global.u64 	%rd2202, %rd2201;
	{ // callseq 448, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2202;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1696, [retval0+0];
	} // callseq 448
	bra.uni 	$L__BB13_431;

$L__BB13_428:
	setp.eq.s64 	%p777, %rd289, 0;
	@%p777 bra 	$L__BB13_431;

	cvta.to.global.u64 	%rd2198, %rd289;
	mul.wide.s32 	%rd2199, %r724, %r80;
	add.s64 	%rd2200, %rd2198, %rd2199;
	ld.global.f64 	%fd4879, [%rd2200];
	add.f64 	%fd9496, %fd4879, 0d0000000000000000;
	ld.global.f64 	%fd4880, [%rd2200+8];
	add.f64 	%fd9497, %fd4880, 0d0000000000000000;
	ld.global.f64 	%fd4881, [%rd2200+16];
	add.f64 	%fd9498, %fd4881, 0d0000000000000000;
	ld.global.f64 	%fd4882, [%rd2200+24];
	add.f64 	%fd9499, %fd4882, 0d0000000000000000;
	ld.global.f64 	%fd4883, [%rd2200+32];
	add.f64 	%fd9500, %fd4883, 0d0000000000000000;
	ld.global.f64 	%fd4884, [%rd2200+40];
	add.f64 	%fd9501, %fd4884, 0d0000000000000000;
	ld.global.f64 	%fd4885, [%rd2200+48];
	add.f64 	%fd9502, %fd4885, 0d0000000000000000;
	ld.global.f64 	%fd4886, [%rd2200+56];
	add.f64 	%fd9503, %fd4886, 0d0000000000000000;
	ld.global.f64 	%fd4887, [%rd2200+64];
	add.f64 	%fd9504, %fd4887, 0d0000000000000000;

$L__BB13_431:
	shr.u32 	%r2497, %r37, 27;
	cvt.u16.u32 	%rs328, %r2497;
	and.b16  	%rs327, %rs328, 1;
	add.f64 	%fd1432, %fd9504, 0d0000000000000000;
	add.f64 	%fd1433, %fd9503, 0d0000000000000000;
	add.f64 	%fd1434, %fd9502, 0d0000000000000000;
	add.f64 	%fd1435, %fd9501, 0d0000000000000000;
	add.f64 	%fd1436, %fd9500, 0d0000000000000000;
	add.f64 	%fd1437, %fd9499, 0d0000000000000000;
	add.f64 	%fd1438, %fd9498, 0d0000000000000000;
	add.f64 	%fd1439, %fd9497, 0d0000000000000000;
	add.f64 	%fd1440, %fd9496, 0d0000000000000000;
	ld.param.u64 	%rd290, [%rd28];
	ld.param.u32 	%r729, [%rd28+32];
	ld.param.u32 	%r730, [%rd28+60];
	setp.le.s32 	%p778, %r730, %r38;
	selp.u16 	%rs131, 1, 0, %p778;
	or.b16  	%rs134, %rs327, %rs131;
	setp.eq.s16 	%p779, %rs134, 0;
	@%p779 bra 	$L__BB13_433;

	st.local.v2.u32 	[%rd30], {%r38, %r730};
	mov.u64 	%rd2204, $str$1;
	cvta.global.u64 	%rd2205, %rd2204;
	{ // callseq 449, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2205;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1698, [retval0+0];
	} // callseq 449
	bra.uni 	$L__BB13_434;

$L__BB13_433:
	mul.wide.s32 	%rd2216, %r729, %r38;
	add.s64 	%rd2207, %rd290, %rd2216;
	// begin inline asm
	{ atom.add.f64 %fd4897,[%rd2207],%fd276; }

	// end inline asm
	add.s64 	%rd2208, %rd2207, 8;
	// begin inline asm
	{ atom.add.f64 %fd4899,[%rd2208],%fd275; }

	// end inline asm
	add.s64 	%rd2209, %rd2207, 16;
	// begin inline asm
	{ atom.add.f64 %fd4901,[%rd2209],%fd274; }

	// end inline asm
	add.s64 	%rd2210, %rd2207, 24;
	// begin inline asm
	{ atom.add.f64 %fd4903,[%rd2210],%fd252; }

	// end inline asm
	add.s64 	%rd2211, %rd2207, 32;
	// begin inline asm
	{ atom.add.f64 %fd4905,[%rd2211],%fd251; }

	// end inline asm
	add.s64 	%rd2212, %rd2207, 40;
	// begin inline asm
	{ atom.add.f64 %fd4907,[%rd2212],%fd250; }

	// end inline asm
	add.s64 	%rd2213, %rd2207, 48;
	// begin inline asm
	{ atom.add.f64 %fd4909,[%rd2213],%fd228; }

	// end inline asm
	add.s64 	%rd2214, %rd2207, 56;
	// begin inline asm
	{ atom.add.f64 %fd4911,[%rd2214],%fd227; }

	// end inline asm
	add.s64 	%rd2215, %rd2207, 64;
	// begin inline asm
	{ atom.add.f64 %fd4913,[%rd2215],%fd226; }

	// end inline asm

$L__BB13_434:
	add.s32 	%r2499, %r38, 1;
	shr.u32 	%r2498, %r2499, 31;
	cvt.u16.u32 	%rs329, %r2498;
	ld.param.u64 	%rd291, [%rd28];
	ld.param.u32 	%r731, [%rd28+32];
	ld.param.u32 	%r732, [%rd28+60];
	setp.le.s32 	%p780, %r732, %r2499;
	selp.u16 	%rs135, 1, 0, %p780;
	or.b16  	%rs137, %rs135, %rs329;
	setp.eq.s16 	%p781, %rs137, 0;
	@%p781 bra 	$L__BB13_436;

	add.s32 	%r2313, %r38, 1;
	st.local.v2.u32 	[%rd30], {%r2313, %r732};
	mov.u64 	%rd2217, $str$1;
	cvta.global.u64 	%rd2218, %rd2217;
	{ // callseq 450, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2218;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1702, [retval0+0];
	} // callseq 450
	bra.uni 	$L__BB13_437;

$L__BB13_436:
	add.s32 	%r2738, %r38, 1;
	mul.wide.s32 	%rd2229, %r731, %r2738;
	add.s64 	%rd2220, %rd291, %rd2229;
	// begin inline asm
	{ atom.add.f64 %fd4915,[%rd2220],%fd273; }

	// end inline asm
	add.s64 	%rd2221, %rd2220, 8;
	// begin inline asm
	{ atom.add.f64 %fd4917,[%rd2221],%fd272; }

	// end inline asm
	add.s64 	%rd2222, %rd2220, 16;
	// begin inline asm
	{ atom.add.f64 %fd4919,[%rd2222],%fd271; }

	// end inline asm
	add.s64 	%rd2223, %rd2220, 24;
	// begin inline asm
	{ atom.add.f64 %fd4921,[%rd2223],%fd249; }

	// end inline asm
	add.s64 	%rd2224, %rd2220, 32;
	// begin inline asm
	{ atom.add.f64 %fd4923,[%rd2224],%fd248; }

	// end inline asm
	add.s64 	%rd2225, %rd2220, 40;
	// begin inline asm
	{ atom.add.f64 %fd4925,[%rd2225],%fd247; }

	// end inline asm
	add.s64 	%rd2226, %rd2220, 48;
	// begin inline asm
	{ atom.add.f64 %fd4927,[%rd2226],%fd225; }

	// end inline asm
	add.s64 	%rd2227, %rd2220, 56;
	// begin inline asm
	{ atom.add.f64 %fd4929,[%rd2227],%fd224; }

	// end inline asm
	add.s64 	%rd2228, %rd2220, 64;
	// begin inline asm
	{ atom.add.f64 %fd4931,[%rd2228],%fd223; }

	// end inline asm

$L__BB13_437:
	add.s32 	%r2501, %r38, 2;
	shr.u32 	%r2500, %r2501, 31;
	cvt.u16.u32 	%rs330, %r2500;
	ld.param.u64 	%rd292, [%rd28];
	ld.param.u32 	%r733, [%rd28+32];
	ld.param.u32 	%r734, [%rd28+60];
	setp.le.s32 	%p782, %r734, %r2501;
	selp.u16 	%rs138, 1, 0, %p782;
	or.b16  	%rs140, %rs138, %rs330;
	setp.eq.s16 	%p783, %rs140, 0;
	@%p783 bra 	$L__BB13_439;

	add.s32 	%r2314, %r38, 2;
	st.local.v2.u32 	[%rd30], {%r2314, %r734};
	mov.u64 	%rd2230, $str$1;
	cvta.global.u64 	%rd2231, %rd2230;
	{ // callseq 451, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2231;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1707, [retval0+0];
	} // callseq 451
	bra.uni 	$L__BB13_440;

$L__BB13_439:
	add.s32 	%r2737, %r38, 2;
	mul.wide.s32 	%rd2242, %r733, %r2737;
	add.s64 	%rd2233, %rd292, %rd2242;
	// begin inline asm
	{ atom.add.f64 %fd4933,[%rd2233],%fd270; }

	// end inline asm
	add.s64 	%rd2234, %rd2233, 8;
	// begin inline asm
	{ atom.add.f64 %fd4935,[%rd2234],%fd269; }

	// end inline asm
	add.s64 	%rd2235, %rd2233, 16;
	// begin inline asm
	{ atom.add.f64 %fd4937,[%rd2235],%fd268; }

	// end inline asm
	add.s64 	%rd2236, %rd2233, 24;
	// begin inline asm
	{ atom.add.f64 %fd4939,[%rd2236],%fd246; }

	// end inline asm
	add.s64 	%rd2237, %rd2233, 32;
	// begin inline asm
	{ atom.add.f64 %fd4941,[%rd2237],%fd245; }

	// end inline asm
	add.s64 	%rd2238, %rd2233, 40;
	// begin inline asm
	{ atom.add.f64 %fd4943,[%rd2238],%fd244; }

	// end inline asm
	add.s64 	%rd2239, %rd2233, 48;
	// begin inline asm
	{ atom.add.f64 %fd4945,[%rd2239],%fd222; }

	// end inline asm
	add.s64 	%rd2240, %rd2233, 56;
	// begin inline asm
	{ atom.add.f64 %fd4947,[%rd2240],%fd221; }

	// end inline asm
	add.s64 	%rd2241, %rd2233, 64;
	// begin inline asm
	{ atom.add.f64 %fd4949,[%rd2241],%fd220; }

	// end inline asm

$L__BB13_440:
	add.s32 	%r2503, %r38, 3;
	shr.u32 	%r2502, %r2503, 31;
	cvt.u16.u32 	%rs331, %r2502;
	ld.param.u64 	%rd293, [%rd28];
	ld.param.u32 	%r735, [%rd28+32];
	ld.param.u32 	%r736, [%rd28+60];
	setp.le.s32 	%p784, %r736, %r2503;
	selp.u16 	%rs141, 1, 0, %p784;
	or.b16  	%rs143, %rs141, %rs331;
	setp.eq.s16 	%p785, %rs143, 0;
	@%p785 bra 	$L__BB13_442;

	add.s32 	%r2315, %r38, 3;
	st.local.v2.u32 	[%rd30], {%r2315, %r736};
	mov.u64 	%rd2243, $str$1;
	cvta.global.u64 	%rd2244, %rd2243;
	{ // callseq 452, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2244;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1712, [retval0+0];
	} // callseq 452
	bra.uni 	$L__BB13_443;

$L__BB13_442:
	add.s32 	%r2736, %r38, 3;
	mul.wide.s32 	%rd2255, %r735, %r2736;
	add.s64 	%rd2246, %rd293, %rd2255;
	// begin inline asm
	{ atom.add.f64 %fd4951,[%rd2246],%fd267; }

	// end inline asm
	add.s64 	%rd2247, %rd2246, 8;
	// begin inline asm
	{ atom.add.f64 %fd4953,[%rd2247],%fd266; }

	// end inline asm
	add.s64 	%rd2248, %rd2246, 16;
	// begin inline asm
	{ atom.add.f64 %fd4955,[%rd2248],%fd265; }

	// end inline asm
	add.s64 	%rd2249, %rd2246, 24;
	// begin inline asm
	{ atom.add.f64 %fd4957,[%rd2249],%fd243; }

	// end inline asm
	add.s64 	%rd2250, %rd2246, 32;
	// begin inline asm
	{ atom.add.f64 %fd4959,[%rd2250],%fd242; }

	// end inline asm
	add.s64 	%rd2251, %rd2246, 40;
	// begin inline asm
	{ atom.add.f64 %fd4961,[%rd2251],%fd241; }

	// end inline asm
	add.s64 	%rd2252, %rd2246, 48;
	// begin inline asm
	{ atom.add.f64 %fd4963,[%rd2252],%fd219; }

	// end inline asm
	add.s64 	%rd2253, %rd2246, 56;
	// begin inline asm
	{ atom.add.f64 %fd4965,[%rd2253],%fd218; }

	// end inline asm
	add.s64 	%rd2254, %rd2246, 64;
	// begin inline asm
	{ atom.add.f64 %fd4967,[%rd2254],%fd217; }

	// end inline asm

$L__BB13_443:
	add.s32 	%r2505, %r38, 4;
	shr.u32 	%r2504, %r2505, 31;
	cvt.u16.u32 	%rs332, %r2504;
	ld.param.u64 	%rd294, [%rd28];
	ld.param.u32 	%r737, [%rd28+32];
	ld.param.u32 	%r738, [%rd28+60];
	setp.le.s32 	%p786, %r738, %r2505;
	selp.u16 	%rs144, 1, 0, %p786;
	or.b16  	%rs146, %rs144, %rs332;
	setp.eq.s16 	%p787, %rs146, 0;
	@%p787 bra 	$L__BB13_445;

	add.s32 	%r2316, %r38, 4;
	st.local.v2.u32 	[%rd30], {%r2316, %r738};
	mov.u64 	%rd2256, $str$1;
	cvta.global.u64 	%rd2257, %rd2256;
	{ // callseq 453, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2257;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1717, [retval0+0];
	} // callseq 453
	bra.uni 	$L__BB13_446;

$L__BB13_445:
	add.s32 	%r2735, %r38, 4;
	mul.wide.s32 	%rd2268, %r737, %r2735;
	add.s64 	%rd2259, %rd294, %rd2268;
	// begin inline asm
	{ atom.add.f64 %fd4969,[%rd2259],%fd204; }

	// end inline asm
	add.s64 	%rd2260, %rd2259, 8;
	// begin inline asm
	{ atom.add.f64 %fd4971,[%rd2260],%fd203; }

	// end inline asm
	add.s64 	%rd2261, %rd2259, 16;
	// begin inline asm
	{ atom.add.f64 %fd4973,[%rd2261],%fd202; }

	// end inline asm
	add.s64 	%rd2262, %rd2259, 24;
	// begin inline asm
	{ atom.add.f64 %fd4975,[%rd2262],%fd180; }

	// end inline asm
	add.s64 	%rd2263, %rd2259, 32;
	// begin inline asm
	{ atom.add.f64 %fd4977,[%rd2263],%fd179; }

	// end inline asm
	add.s64 	%rd2264, %rd2259, 40;
	// begin inline asm
	{ atom.add.f64 %fd4979,[%rd2264],%fd178; }

	// end inline asm
	add.s64 	%rd2265, %rd2259, 48;
	// begin inline asm
	{ atom.add.f64 %fd4981,[%rd2265],%fd156; }

	// end inline asm
	add.s64 	%rd2266, %rd2259, 56;
	// begin inline asm
	{ atom.add.f64 %fd4983,[%rd2266],%fd155; }

	// end inline asm
	add.s64 	%rd2267, %rd2259, 64;
	// begin inline asm
	{ atom.add.f64 %fd4985,[%rd2267],%fd154; }

	// end inline asm

$L__BB13_446:
	add.s32 	%r2507, %r38, 5;
	shr.u32 	%r2506, %r2507, 31;
	cvt.u16.u32 	%rs333, %r2506;
	ld.param.u64 	%rd295, [%rd28];
	ld.param.u32 	%r739, [%rd28+32];
	ld.param.u32 	%r740, [%rd28+60];
	setp.le.s32 	%p788, %r740, %r2507;
	selp.u16 	%rs147, 1, 0, %p788;
	or.b16  	%rs149, %rs147, %rs333;
	setp.eq.s16 	%p789, %rs149, 0;
	@%p789 bra 	$L__BB13_448;

	add.s32 	%r2317, %r38, 5;
	st.local.v2.u32 	[%rd30], {%r2317, %r740};
	mov.u64 	%rd2269, $str$1;
	cvta.global.u64 	%rd2270, %rd2269;
	{ // callseq 454, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2270;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1722, [retval0+0];
	} // callseq 454
	bra.uni 	$L__BB13_449;

$L__BB13_448:
	add.s32 	%r2734, %r38, 5;
	mul.wide.s32 	%rd2281, %r739, %r2734;
	add.s64 	%rd2272, %rd295, %rd2281;
	// begin inline asm
	{ atom.add.f64 %fd4987,[%rd2272],%fd201; }

	// end inline asm
	add.s64 	%rd2273, %rd2272, 8;
	// begin inline asm
	{ atom.add.f64 %fd4989,[%rd2273],%fd200; }

	// end inline asm
	add.s64 	%rd2274, %rd2272, 16;
	// begin inline asm
	{ atom.add.f64 %fd4991,[%rd2274],%fd199; }

	// end inline asm
	add.s64 	%rd2275, %rd2272, 24;
	// begin inline asm
	{ atom.add.f64 %fd4993,[%rd2275],%fd177; }

	// end inline asm
	add.s64 	%rd2276, %rd2272, 32;
	// begin inline asm
	{ atom.add.f64 %fd4995,[%rd2276],%fd176; }

	// end inline asm
	add.s64 	%rd2277, %rd2272, 40;
	// begin inline asm
	{ atom.add.f64 %fd4997,[%rd2277],%fd175; }

	// end inline asm
	add.s64 	%rd2278, %rd2272, 48;
	// begin inline asm
	{ atom.add.f64 %fd4999,[%rd2278],%fd153; }

	// end inline asm
	add.s64 	%rd2279, %rd2272, 56;
	// begin inline asm
	{ atom.add.f64 %fd5001,[%rd2279],%fd152; }

	// end inline asm
	add.s64 	%rd2280, %rd2272, 64;
	// begin inline asm
	{ atom.add.f64 %fd5003,[%rd2280],%fd151; }

	// end inline asm

$L__BB13_449:
	add.s32 	%r2509, %r38, 6;
	shr.u32 	%r2508, %r2509, 31;
	cvt.u16.u32 	%rs334, %r2508;
	ld.param.u64 	%rd296, [%rd28];
	ld.param.u32 	%r741, [%rd28+32];
	ld.param.u32 	%r742, [%rd28+60];
	setp.le.s32 	%p790, %r742, %r2509;
	selp.u16 	%rs150, 1, 0, %p790;
	or.b16  	%rs152, %rs150, %rs334;
	setp.eq.s16 	%p791, %rs152, 0;
	@%p791 bra 	$L__BB13_451;

	add.s32 	%r2318, %r38, 6;
	st.local.v2.u32 	[%rd30], {%r2318, %r742};
	mov.u64 	%rd2282, $str$1;
	cvta.global.u64 	%rd2283, %rd2282;
	{ // callseq 455, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2283;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1727, [retval0+0];
	} // callseq 455
	bra.uni 	$L__BB13_452;

$L__BB13_451:
	add.s32 	%r2733, %r38, 6;
	mul.wide.s32 	%rd2294, %r741, %r2733;
	add.s64 	%rd2285, %rd296, %rd2294;
	// begin inline asm
	{ atom.add.f64 %fd5005,[%rd2285],%fd198; }

	// end inline asm
	add.s64 	%rd2286, %rd2285, 8;
	// begin inline asm
	{ atom.add.f64 %fd5007,[%rd2286],%fd197; }

	// end inline asm
	add.s64 	%rd2287, %rd2285, 16;
	// begin inline asm
	{ atom.add.f64 %fd5009,[%rd2287],%fd196; }

	// end inline asm
	add.s64 	%rd2288, %rd2285, 24;
	// begin inline asm
	{ atom.add.f64 %fd5011,[%rd2288],%fd174; }

	// end inline asm
	add.s64 	%rd2289, %rd2285, 32;
	// begin inline asm
	{ atom.add.f64 %fd5013,[%rd2289],%fd173; }

	// end inline asm
	add.s64 	%rd2290, %rd2285, 40;
	// begin inline asm
	{ atom.add.f64 %fd5015,[%rd2290],%fd172; }

	// end inline asm
	add.s64 	%rd2291, %rd2285, 48;
	// begin inline asm
	{ atom.add.f64 %fd5017,[%rd2291],%fd150; }

	// end inline asm
	add.s64 	%rd2292, %rd2285, 56;
	// begin inline asm
	{ atom.add.f64 %fd5019,[%rd2292],%fd149; }

	// end inline asm
	add.s64 	%rd2293, %rd2285, 64;
	// begin inline asm
	{ atom.add.f64 %fd5021,[%rd2293],%fd148; }

	// end inline asm

$L__BB13_452:
	add.s32 	%r2511, %r38, 7;
	shr.u32 	%r2510, %r2511, 31;
	cvt.u16.u32 	%rs335, %r2510;
	ld.param.u64 	%rd297, [%rd28];
	ld.param.u32 	%r743, [%rd28+32];
	ld.param.u32 	%r744, [%rd28+60];
	setp.le.s32 	%p792, %r744, %r2511;
	selp.u16 	%rs153, 1, 0, %p792;
	or.b16  	%rs155, %rs153, %rs335;
	setp.eq.s16 	%p793, %rs155, 0;
	@%p793 bra 	$L__BB13_454;

	add.s32 	%r2319, %r38, 7;
	st.local.v2.u32 	[%rd30], {%r2319, %r744};
	mov.u64 	%rd2295, $str$1;
	cvta.global.u64 	%rd2296, %rd2295;
	{ // callseq 456, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2296;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1732, [retval0+0];
	} // callseq 456
	bra.uni 	$L__BB13_455;

$L__BB13_454:
	add.s32 	%r2732, %r38, 7;
	mul.wide.s32 	%rd2307, %r743, %r2732;
	add.s64 	%rd2298, %rd297, %rd2307;
	// begin inline asm
	{ atom.add.f64 %fd5023,[%rd2298],%fd195; }

	// end inline asm
	add.s64 	%rd2299, %rd2298, 8;
	// begin inline asm
	{ atom.add.f64 %fd5025,[%rd2299],%fd194; }

	// end inline asm
	add.s64 	%rd2300, %rd2298, 16;
	// begin inline asm
	{ atom.add.f64 %fd5027,[%rd2300],%fd193; }

	// end inline asm
	add.s64 	%rd2301, %rd2298, 24;
	// begin inline asm
	{ atom.add.f64 %fd5029,[%rd2301],%fd171; }

	// end inline asm
	add.s64 	%rd2302, %rd2298, 32;
	// begin inline asm
	{ atom.add.f64 %fd5031,[%rd2302],%fd170; }

	// end inline asm
	add.s64 	%rd2303, %rd2298, 40;
	// begin inline asm
	{ atom.add.f64 %fd5033,[%rd2303],%fd169; }

	// end inline asm
	add.s64 	%rd2304, %rd2298, 48;
	// begin inline asm
	{ atom.add.f64 %fd5035,[%rd2304],%fd147; }

	// end inline asm
	add.s64 	%rd2305, %rd2298, 56;
	// begin inline asm
	{ atom.add.f64 %fd5037,[%rd2305],%fd146; }

	// end inline asm
	add.s64 	%rd2306, %rd2298, 64;
	// begin inline asm
	{ atom.add.f64 %fd5039,[%rd2306],%fd145; }

	// end inline asm

$L__BB13_455:
	add.s32 	%r2513, %r38, 8;
	shr.u32 	%r2512, %r2513, 31;
	cvt.u16.u32 	%rs336, %r2512;
	ld.param.u64 	%rd298, [%rd28];
	ld.param.u32 	%r745, [%rd28+32];
	ld.param.u32 	%r746, [%rd28+60];
	setp.le.s32 	%p794, %r746, %r2513;
	selp.u16 	%rs156, 1, 0, %p794;
	or.b16  	%rs158, %rs156, %rs336;
	setp.eq.s16 	%p795, %rs158, 0;
	@%p795 bra 	$L__BB13_457;

	add.s32 	%r2320, %r38, 8;
	st.local.v2.u32 	[%rd30], {%r2320, %r746};
	mov.u64 	%rd2308, $str$1;
	cvta.global.u64 	%rd2309, %rd2308;
	{ // callseq 457, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2309;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1737, [retval0+0];
	} // callseq 457
	bra.uni 	$L__BB13_458;

$L__BB13_457:
	add.s32 	%r2731, %r38, 8;
	mul.wide.s32 	%rd2320, %r745, %r2731;
	add.s64 	%rd2311, %rd298, %rd2320;
	// begin inline asm
	{ atom.add.f64 %fd5041,[%rd2311],%fd132; }

	// end inline asm
	add.s64 	%rd2312, %rd2311, 8;
	// begin inline asm
	{ atom.add.f64 %fd5043,[%rd2312],%fd131; }

	// end inline asm
	add.s64 	%rd2313, %rd2311, 16;
	// begin inline asm
	{ atom.add.f64 %fd5045,[%rd2313],%fd130; }

	// end inline asm
	add.s64 	%rd2314, %rd2311, 24;
	// begin inline asm
	{ atom.add.f64 %fd5047,[%rd2314],%fd108; }

	// end inline asm
	add.s64 	%rd2315, %rd2311, 32;
	// begin inline asm
	{ atom.add.f64 %fd5049,[%rd2315],%fd107; }

	// end inline asm
	add.s64 	%rd2316, %rd2311, 40;
	// begin inline asm
	{ atom.add.f64 %fd5051,[%rd2316],%fd106; }

	// end inline asm
	add.s64 	%rd2317, %rd2311, 48;
	// begin inline asm
	{ atom.add.f64 %fd5053,[%rd2317],%fd84; }

	// end inline asm
	add.s64 	%rd2318, %rd2311, 56;
	// begin inline asm
	{ atom.add.f64 %fd5055,[%rd2318],%fd83; }

	// end inline asm
	add.s64 	%rd2319, %rd2311, 64;
	// begin inline asm
	{ atom.add.f64 %fd5057,[%rd2319],%fd82; }

	// end inline asm

$L__BB13_458:
	add.s32 	%r2515, %r38, 9;
	shr.u32 	%r2514, %r2515, 31;
	cvt.u16.u32 	%rs337, %r2514;
	ld.param.u64 	%rd299, [%rd28];
	ld.param.u32 	%r747, [%rd28+32];
	ld.param.u32 	%r748, [%rd28+60];
	setp.le.s32 	%p796, %r748, %r2515;
	selp.u16 	%rs159, 1, 0, %p796;
	or.b16  	%rs161, %rs159, %rs337;
	setp.eq.s16 	%p797, %rs161, 0;
	@%p797 bra 	$L__BB13_460;

	add.s32 	%r2321, %r38, 9;
	st.local.v2.u32 	[%rd30], {%r2321, %r748};
	mov.u64 	%rd2321, $str$1;
	cvta.global.u64 	%rd2322, %rd2321;
	{ // callseq 458, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2322;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1742, [retval0+0];
	} // callseq 458
	bra.uni 	$L__BB13_461;

$L__BB13_460:
	add.s32 	%r2730, %r38, 9;
	mul.wide.s32 	%rd2333, %r747, %r2730;
	add.s64 	%rd2324, %rd299, %rd2333;
	// begin inline asm
	{ atom.add.f64 %fd5059,[%rd2324],%fd129; }

	// end inline asm
	add.s64 	%rd2325, %rd2324, 8;
	// begin inline asm
	{ atom.add.f64 %fd5061,[%rd2325],%fd128; }

	// end inline asm
	add.s64 	%rd2326, %rd2324, 16;
	// begin inline asm
	{ atom.add.f64 %fd5063,[%rd2326],%fd127; }

	// end inline asm
	add.s64 	%rd2327, %rd2324, 24;
	// begin inline asm
	{ atom.add.f64 %fd5065,[%rd2327],%fd105; }

	// end inline asm
	add.s64 	%rd2328, %rd2324, 32;
	// begin inline asm
	{ atom.add.f64 %fd5067,[%rd2328],%fd104; }

	// end inline asm
	add.s64 	%rd2329, %rd2324, 40;
	// begin inline asm
	{ atom.add.f64 %fd5069,[%rd2329],%fd103; }

	// end inline asm
	add.s64 	%rd2330, %rd2324, 48;
	// begin inline asm
	{ atom.add.f64 %fd5071,[%rd2330],%fd81; }

	// end inline asm
	add.s64 	%rd2331, %rd2324, 56;
	// begin inline asm
	{ atom.add.f64 %fd5073,[%rd2331],%fd80; }

	// end inline asm
	add.s64 	%rd2332, %rd2324, 64;
	// begin inline asm
	{ atom.add.f64 %fd5075,[%rd2332],%fd79; }

	// end inline asm

$L__BB13_461:
	add.s32 	%r2517, %r38, 10;
	shr.u32 	%r2516, %r2517, 31;
	cvt.u16.u32 	%rs338, %r2516;
	ld.param.u64 	%rd300, [%rd28];
	ld.param.u32 	%r749, [%rd28+32];
	ld.param.u32 	%r750, [%rd28+60];
	setp.le.s32 	%p798, %r750, %r2517;
	selp.u16 	%rs162, 1, 0, %p798;
	or.b16  	%rs164, %rs162, %rs338;
	setp.eq.s16 	%p799, %rs164, 0;
	@%p799 bra 	$L__BB13_463;

	add.s32 	%r2322, %r38, 10;
	st.local.v2.u32 	[%rd30], {%r2322, %r750};
	mov.u64 	%rd2334, $str$1;
	cvta.global.u64 	%rd2335, %rd2334;
	{ // callseq 459, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2335;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1747, [retval0+0];
	} // callseq 459
	bra.uni 	$L__BB13_464;

$L__BB13_463:
	add.s32 	%r2729, %r38, 10;
	mul.wide.s32 	%rd2346, %r749, %r2729;
	add.s64 	%rd2337, %rd300, %rd2346;
	// begin inline asm
	{ atom.add.f64 %fd5077,[%rd2337],%fd126; }

	// end inline asm
	add.s64 	%rd2338, %rd2337, 8;
	// begin inline asm
	{ atom.add.f64 %fd5079,[%rd2338],%fd125; }

	// end inline asm
	add.s64 	%rd2339, %rd2337, 16;
	// begin inline asm
	{ atom.add.f64 %fd5081,[%rd2339],%fd124; }

	// end inline asm
	add.s64 	%rd2340, %rd2337, 24;
	// begin inline asm
	{ atom.add.f64 %fd5083,[%rd2340],%fd102; }

	// end inline asm
	add.s64 	%rd2341, %rd2337, 32;
	// begin inline asm
	{ atom.add.f64 %fd5085,[%rd2341],%fd101; }

	// end inline asm
	add.s64 	%rd2342, %rd2337, 40;
	// begin inline asm
	{ atom.add.f64 %fd5087,[%rd2342],%fd100; }

	// end inline asm
	add.s64 	%rd2343, %rd2337, 48;
	// begin inline asm
	{ atom.add.f64 %fd5089,[%rd2343],%fd78; }

	// end inline asm
	add.s64 	%rd2344, %rd2337, 56;
	// begin inline asm
	{ atom.add.f64 %fd5091,[%rd2344],%fd77; }

	// end inline asm
	add.s64 	%rd2345, %rd2337, 64;
	// begin inline asm
	{ atom.add.f64 %fd5093,[%rd2345],%fd76; }

	// end inline asm

$L__BB13_464:
	add.s32 	%r2519, %r38, 11;
	shr.u32 	%r2518, %r2519, 31;
	cvt.u16.u32 	%rs339, %r2518;
	ld.param.u64 	%rd301, [%rd28];
	ld.param.u32 	%r751, [%rd28+32];
	ld.param.u32 	%r752, [%rd28+60];
	setp.le.s32 	%p800, %r752, %r2519;
	selp.u16 	%rs165, 1, 0, %p800;
	or.b16  	%rs167, %rs165, %rs339;
	setp.eq.s16 	%p801, %rs167, 0;
	@%p801 bra 	$L__BB13_466;

	add.s32 	%r2323, %r38, 11;
	st.local.v2.u32 	[%rd30], {%r2323, %r752};
	mov.u64 	%rd2347, $str$1;
	cvta.global.u64 	%rd2348, %rd2347;
	{ // callseq 460, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2348;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1752, [retval0+0];
	} // callseq 460
	bra.uni 	$L__BB13_467;

$L__BB13_466:
	add.s32 	%r2728, %r38, 11;
	mul.wide.s32 	%rd2359, %r751, %r2728;
	add.s64 	%rd2350, %rd301, %rd2359;
	// begin inline asm
	{ atom.add.f64 %fd5095,[%rd2350],%fd123; }

	// end inline asm
	add.s64 	%rd2351, %rd2350, 8;
	// begin inline asm
	{ atom.add.f64 %fd5097,[%rd2351],%fd122; }

	// end inline asm
	add.s64 	%rd2352, %rd2350, 16;
	// begin inline asm
	{ atom.add.f64 %fd5099,[%rd2352],%fd121; }

	// end inline asm
	add.s64 	%rd2353, %rd2350, 24;
	// begin inline asm
	{ atom.add.f64 %fd5101,[%rd2353],%fd99; }

	// end inline asm
	add.s64 	%rd2354, %rd2350, 32;
	// begin inline asm
	{ atom.add.f64 %fd5103,[%rd2354],%fd98; }

	// end inline asm
	add.s64 	%rd2355, %rd2350, 40;
	// begin inline asm
	{ atom.add.f64 %fd5105,[%rd2355],%fd97; }

	// end inline asm
	add.s64 	%rd2356, %rd2350, 48;
	// begin inline asm
	{ atom.add.f64 %fd5107,[%rd2356],%fd75; }

	// end inline asm
	add.s64 	%rd2357, %rd2350, 56;
	// begin inline asm
	{ atom.add.f64 %fd5109,[%rd2357],%fd74; }

	// end inline asm
	add.s64 	%rd2358, %rd2350, 64;
	// begin inline asm
	{ atom.add.f64 %fd5111,[%rd2358],%fd73; }

	// end inline asm

$L__BB13_467:
	add.s32 	%r2521, %r38, 12;
	shr.u32 	%r2520, %r2521, 31;
	cvt.u16.u32 	%rs340, %r2520;
	ld.param.u64 	%rd302, [%rd28];
	ld.param.u32 	%r753, [%rd28+32];
	ld.param.u32 	%r754, [%rd28+60];
	setp.le.s32 	%p802, %r754, %r2521;
	selp.u16 	%rs168, 1, 0, %p802;
	or.b16  	%rs170, %rs168, %rs340;
	setp.eq.s16 	%p803, %rs170, 0;
	@%p803 bra 	$L__BB13_469;

	add.s32 	%r2324, %r38, 12;
	st.local.v2.u32 	[%rd30], {%r2324, %r754};
	mov.u64 	%rd2360, $str$1;
	cvta.global.u64 	%rd2361, %rd2360;
	{ // callseq 461, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2361;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1757, [retval0+0];
	} // callseq 461
	bra.uni 	$L__BB13_470;

$L__BB13_469:
	add.s32 	%r2727, %r38, 12;
	mul.wide.s32 	%rd2372, %r753, %r2727;
	add.s64 	%rd2363, %rd302, %rd2372;
	// begin inline asm
	{ atom.add.f64 %fd5113,[%rd2363],%fd60; }

	// end inline asm
	add.s64 	%rd2364, %rd2363, 8;
	// begin inline asm
	{ atom.add.f64 %fd5115,[%rd2364],%fd59; }

	// end inline asm
	add.s64 	%rd2365, %rd2363, 16;
	// begin inline asm
	{ atom.add.f64 %fd5117,[%rd2365],%fd58; }

	// end inline asm
	add.s64 	%rd2366, %rd2363, 24;
	// begin inline asm
	{ atom.add.f64 %fd5119,[%rd2366],%fd36; }

	// end inline asm
	add.s64 	%rd2367, %rd2363, 32;
	// begin inline asm
	{ atom.add.f64 %fd5121,[%rd2367],%fd35; }

	// end inline asm
	add.s64 	%rd2368, %rd2363, 40;
	// begin inline asm
	{ atom.add.f64 %fd5123,[%rd2368],%fd34; }

	// end inline asm
	add.s64 	%rd2369, %rd2363, 48;
	// begin inline asm
	{ atom.add.f64 %fd5125,[%rd2369],%fd12; }

	// end inline asm
	add.s64 	%rd2370, %rd2363, 56;
	// begin inline asm
	{ atom.add.f64 %fd5127,[%rd2370],%fd11; }

	// end inline asm
	add.s64 	%rd2371, %rd2363, 64;
	// begin inline asm
	{ atom.add.f64 %fd5129,[%rd2371],%fd10; }

	// end inline asm

$L__BB13_470:
	add.s32 	%r2523, %r38, 13;
	shr.u32 	%r2522, %r2523, 31;
	cvt.u16.u32 	%rs341, %r2522;
	ld.param.u64 	%rd303, [%rd28];
	ld.param.u32 	%r755, [%rd28+32];
	ld.param.u32 	%r756, [%rd28+60];
	setp.le.s32 	%p804, %r756, %r2523;
	selp.u16 	%rs171, 1, 0, %p804;
	or.b16  	%rs173, %rs171, %rs341;
	setp.eq.s16 	%p805, %rs173, 0;
	@%p805 bra 	$L__BB13_472;

	add.s32 	%r2325, %r38, 13;
	st.local.v2.u32 	[%rd30], {%r2325, %r756};
	mov.u64 	%rd2373, $str$1;
	cvta.global.u64 	%rd2374, %rd2373;
	{ // callseq 462, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2374;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1762, [retval0+0];
	} // callseq 462
	bra.uni 	$L__BB13_473;

$L__BB13_472:
	add.s32 	%r2726, %r38, 13;
	mul.wide.s32 	%rd2385, %r755, %r2726;
	add.s64 	%rd2376, %rd303, %rd2385;
	// begin inline asm
	{ atom.add.f64 %fd5131,[%rd2376],%fd57; }

	// end inline asm
	add.s64 	%rd2377, %rd2376, 8;
	// begin inline asm
	{ atom.add.f64 %fd5133,[%rd2377],%fd56; }

	// end inline asm
	add.s64 	%rd2378, %rd2376, 16;
	// begin inline asm
	{ atom.add.f64 %fd5135,[%rd2378],%fd55; }

	// end inline asm
	add.s64 	%rd2379, %rd2376, 24;
	// begin inline asm
	{ atom.add.f64 %fd5137,[%rd2379],%fd33; }

	// end inline asm
	add.s64 	%rd2380, %rd2376, 32;
	// begin inline asm
	{ atom.add.f64 %fd5139,[%rd2380],%fd32; }

	// end inline asm
	add.s64 	%rd2381, %rd2376, 40;
	// begin inline asm
	{ atom.add.f64 %fd5141,[%rd2381],%fd31; }

	// end inline asm
	add.s64 	%rd2382, %rd2376, 48;
	// begin inline asm
	{ atom.add.f64 %fd5143,[%rd2382],%fd9; }

	// end inline asm
	add.s64 	%rd2383, %rd2376, 56;
	// begin inline asm
	{ atom.add.f64 %fd5145,[%rd2383],%fd8; }

	// end inline asm
	add.s64 	%rd2384, %rd2376, 64;
	// begin inline asm
	{ atom.add.f64 %fd5147,[%rd2384],%fd7; }

	// end inline asm

$L__BB13_473:
	add.s32 	%r2525, %r38, 14;
	shr.u32 	%r2524, %r2525, 31;
	cvt.u16.u32 	%rs342, %r2524;
	ld.param.u64 	%rd304, [%rd28];
	ld.param.u32 	%r757, [%rd28+32];
	ld.param.u32 	%r758, [%rd28+60];
	setp.le.s32 	%p806, %r758, %r2525;
	selp.u16 	%rs174, 1, 0, %p806;
	or.b16  	%rs176, %rs174, %rs342;
	setp.eq.s16 	%p807, %rs176, 0;
	@%p807 bra 	$L__BB13_475;

	add.s32 	%r2326, %r38, 14;
	st.local.v2.u32 	[%rd30], {%r2326, %r758};
	mov.u64 	%rd2386, $str$1;
	cvta.global.u64 	%rd2387, %rd2386;
	{ // callseq 463, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2387;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1767, [retval0+0];
	} // callseq 463
	bra.uni 	$L__BB13_476;

$L__BB13_475:
	add.s32 	%r2725, %r38, 14;
	mul.wide.s32 	%rd2398, %r757, %r2725;
	add.s64 	%rd2389, %rd304, %rd2398;
	// begin inline asm
	{ atom.add.f64 %fd5149,[%rd2389],%fd54; }

	// end inline asm
	add.s64 	%rd2390, %rd2389, 8;
	// begin inline asm
	{ atom.add.f64 %fd5151,[%rd2390],%fd53; }

	// end inline asm
	add.s64 	%rd2391, %rd2389, 16;
	// begin inline asm
	{ atom.add.f64 %fd5153,[%rd2391],%fd52; }

	// end inline asm
	add.s64 	%rd2392, %rd2389, 24;
	// begin inline asm
	{ atom.add.f64 %fd5155,[%rd2392],%fd30; }

	// end inline asm
	add.s64 	%rd2393, %rd2389, 32;
	// begin inline asm
	{ atom.add.f64 %fd5157,[%rd2393],%fd29; }

	// end inline asm
	add.s64 	%rd2394, %rd2389, 40;
	// begin inline asm
	{ atom.add.f64 %fd5159,[%rd2394],%fd28; }

	// end inline asm
	add.s64 	%rd2395, %rd2389, 48;
	// begin inline asm
	{ atom.add.f64 %fd5161,[%rd2395],%fd6; }

	// end inline asm
	add.s64 	%rd2396, %rd2389, 56;
	// begin inline asm
	{ atom.add.f64 %fd5163,[%rd2396],%fd5; }

	// end inline asm
	add.s64 	%rd2397, %rd2389, 64;
	// begin inline asm
	{ atom.add.f64 %fd5165,[%rd2397],%fd4; }

	// end inline asm

$L__BB13_476:
	add.s32 	%r2527, %r38, 15;
	shr.u32 	%r2526, %r2527, 31;
	cvt.u16.u32 	%rs343, %r2526;
	ld.param.u64 	%rd305, [%rd28];
	ld.param.u32 	%r759, [%rd28+32];
	ld.param.u32 	%r760, [%rd28+60];
	setp.le.s32 	%p808, %r760, %r2527;
	selp.u16 	%rs177, 1, 0, %p808;
	or.b16  	%rs179, %rs177, %rs343;
	setp.eq.s16 	%p809, %rs179, 0;
	@%p809 bra 	$L__BB13_478;

	add.s32 	%r2327, %r38, 15;
	st.local.v2.u32 	[%rd30], {%r2327, %r760};
	mov.u64 	%rd2399, $str$1;
	cvta.global.u64 	%rd2400, %rd2399;
	{ // callseq 464, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2400;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1772, [retval0+0];
	} // callseq 464
	bra.uni 	$L__BB13_479;

$L__BB13_478:
	add.s32 	%r2724, %r38, 15;
	mul.wide.s32 	%rd2411, %r759, %r2724;
	add.s64 	%rd2402, %rd305, %rd2411;
	// begin inline asm
	{ atom.add.f64 %fd5167,[%rd2402],%fd51; }

	// end inline asm
	add.s64 	%rd2403, %rd2402, 8;
	// begin inline asm
	{ atom.add.f64 %fd5169,[%rd2403],%fd50; }

	// end inline asm
	add.s64 	%rd2404, %rd2402, 16;
	// begin inline asm
	{ atom.add.f64 %fd5171,[%rd2404],%fd49; }

	// end inline asm
	add.s64 	%rd2405, %rd2402, 24;
	// begin inline asm
	{ atom.add.f64 %fd5173,[%rd2405],%fd27; }

	// end inline asm
	add.s64 	%rd2406, %rd2402, 32;
	// begin inline asm
	{ atom.add.f64 %fd5175,[%rd2406],%fd26; }

	// end inline asm
	add.s64 	%rd2407, %rd2402, 40;
	// begin inline asm
	{ atom.add.f64 %fd5177,[%rd2407],%fd25; }

	// end inline asm
	add.s64 	%rd2408, %rd2402, 48;
	// begin inline asm
	{ atom.add.f64 %fd5179,[%rd2408],%fd3; }

	// end inline asm
	add.s64 	%rd2409, %rd2402, 56;
	// begin inline asm
	{ atom.add.f64 %fd5181,[%rd2409],%fd2; }

	// end inline asm
	add.s64 	%rd2410, %rd2402, 64;
	// begin inline asm
	{ atom.add.f64 %fd5183,[%rd2410],%fd1; }

	// end inline asm

$L__BB13_479:
	add.s32 	%r2529, %r38, 15;
	shr.u32 	%r2528, %r2529, 31;
	cvt.u16.u32 	%rs344, %r2528;
	ld.param.u64 	%rd306, [%rd28+8];
	ld.param.u32 	%r761, [%rd28+32];
	ld.param.u32 	%r762, [%rd28+60];
	setp.le.s32 	%p810, %r762, %r2529;
	selp.u16 	%rs180, 1, 0, %p810;
	or.b16  	%rs182, %rs180, %rs344;
	setp.eq.s16 	%p811, %rs182, 0;
	mov.f64 	%fd9505, 0d0000000000000000;
	mov.f64 	%fd9506, 0d0000000000000000;
	mov.f64 	%fd9507, 0d0000000000000000;
	mov.f64 	%fd9508, 0d0000000000000000;
	mov.f64 	%fd9509, 0d0000000000000000;
	mov.f64 	%fd9510, 0d0000000000000000;
	mov.f64 	%fd9511, 0d0000000000000000;
	mov.f64 	%fd9512, 0d0000000000000000;
	mov.f64 	%fd9513, 0d0000000000000000;
	@%p811 bra 	$L__BB13_481;

	add.s32 	%r2328, %r38, 15;
	st.local.v2.u32 	[%rd30], {%r2328, %r762};
	mov.u64 	%rd2412, $str$1;
	cvta.global.u64 	%rd2413, %rd2412;
	{ // callseq 465, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2413;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1777, [retval0+0];
	} // callseq 465
	bra.uni 	$L__BB13_483;

$L__BB13_481:
	setp.eq.s64 	%p812, %rd306, 0;
	@%p812 bra 	$L__BB13_483;

	add.s32 	%r2723, %r38, 15;
	cvta.to.global.u64 	%rd2415, %rd306;
	mul.wide.s32 	%rd2416, %r761, %r2723;
	add.s64 	%rd2417, %rd2415, %rd2416;
	ld.global.f64 	%fd5203, [%rd2417];
	add.f64 	%fd9505, %fd5203, 0d0000000000000000;
	ld.global.f64 	%fd5204, [%rd2417+8];
	add.f64 	%fd9506, %fd5204, 0d0000000000000000;
	ld.global.f64 	%fd5205, [%rd2417+16];
	add.f64 	%fd9507, %fd5205, 0d0000000000000000;
	ld.global.f64 	%fd5206, [%rd2417+24];
	add.f64 	%fd9508, %fd5206, 0d0000000000000000;
	ld.global.f64 	%fd5207, [%rd2417+32];
	add.f64 	%fd9509, %fd5207, 0d0000000000000000;
	ld.global.f64 	%fd5208, [%rd2417+40];
	add.f64 	%fd9510, %fd5208, 0d0000000000000000;
	ld.global.f64 	%fd5209, [%rd2417+48];
	add.f64 	%fd9511, %fd5209, 0d0000000000000000;
	ld.global.f64 	%fd5210, [%rd2417+56];
	add.f64 	%fd9512, %fd5210, 0d0000000000000000;
	ld.global.f64 	%fd5211, [%rd2417+64];
	add.f64 	%fd9513, %fd5211, 0d0000000000000000;

$L__BB13_483:
	add.s32 	%r2531, %r38, 14;
	shr.u32 	%r2530, %r2531, 31;
	cvt.u16.u32 	%rs345, %r2530;
	add.f64 	%fd1459, %fd9513, 0d0000000000000000;
	add.f64 	%fd1460, %fd9512, 0d0000000000000000;
	add.f64 	%fd1461, %fd9511, 0d0000000000000000;
	add.f64 	%fd1462, %fd9510, 0d0000000000000000;
	add.f64 	%fd1463, %fd9509, 0d0000000000000000;
	add.f64 	%fd1464, %fd9508, 0d0000000000000000;
	add.f64 	%fd1465, %fd9507, 0d0000000000000000;
	add.f64 	%fd1466, %fd9506, 0d0000000000000000;
	add.f64 	%fd1467, %fd9505, 0d0000000000000000;
	ld.param.u64 	%rd307, [%rd28+8];
	ld.param.u32 	%r763, [%rd28+32];
	ld.param.u32 	%r764, [%rd28+60];
	setp.le.s32 	%p813, %r764, %r2531;
	selp.u16 	%rs183, 1, 0, %p813;
	or.b16  	%rs185, %rs183, %rs345;
	setp.eq.s16 	%p814, %rs185, 0;
	mov.f64 	%fd9514, 0d0000000000000000;
	mov.f64 	%fd9515, 0d0000000000000000;
	mov.f64 	%fd9516, 0d0000000000000000;
	mov.f64 	%fd9517, 0d0000000000000000;
	mov.f64 	%fd9518, 0d0000000000000000;
	mov.f64 	%fd9519, 0d0000000000000000;
	mov.f64 	%fd9520, 0d0000000000000000;
	mov.f64 	%fd9521, 0d0000000000000000;
	mov.f64 	%fd9522, 0d0000000000000000;
	@%p814 bra 	$L__BB13_485;

	add.s32 	%r2329, %r38, 14;
	st.local.v2.u32 	[%rd30], {%r2329, %r764};
	mov.u64 	%rd2418, $str$1;
	cvta.global.u64 	%rd2419, %rd2418;
	{ // callseq 466, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2419;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1782, [retval0+0];
	} // callseq 466
	bra.uni 	$L__BB13_487;

$L__BB13_485:
	setp.eq.s64 	%p815, %rd307, 0;
	@%p815 bra 	$L__BB13_487;

	add.s32 	%r2722, %r38, 14;
	cvta.to.global.u64 	%rd2421, %rd307;
	mul.wide.s32 	%rd2422, %r763, %r2722;
	add.s64 	%rd2423, %rd2421, %rd2422;
	ld.global.f64 	%fd5230, [%rd2423];
	add.f64 	%fd9514, %fd5230, 0d0000000000000000;
	ld.global.f64 	%fd5231, [%rd2423+8];
	add.f64 	%fd9515, %fd5231, 0d0000000000000000;
	ld.global.f64 	%fd5232, [%rd2423+16];
	add.f64 	%fd9516, %fd5232, 0d0000000000000000;
	ld.global.f64 	%fd5233, [%rd2423+24];
	add.f64 	%fd9517, %fd5233, 0d0000000000000000;
	ld.global.f64 	%fd5234, [%rd2423+32];
	add.f64 	%fd9518, %fd5234, 0d0000000000000000;
	ld.global.f64 	%fd5235, [%rd2423+40];
	add.f64 	%fd9519, %fd5235, 0d0000000000000000;
	ld.global.f64 	%fd5236, [%rd2423+48];
	add.f64 	%fd9520, %fd5236, 0d0000000000000000;
	ld.global.f64 	%fd5237, [%rd2423+56];
	add.f64 	%fd9521, %fd5237, 0d0000000000000000;
	ld.global.f64 	%fd5238, [%rd2423+64];
	add.f64 	%fd9522, %fd5238, 0d0000000000000000;

$L__BB13_487:
	add.s32 	%r2533, %r38, 13;
	shr.u32 	%r2532, %r2533, 31;
	cvt.u16.u32 	%rs346, %r2532;
	add.f64 	%fd1486, %fd9522, 0d0000000000000000;
	add.f64 	%fd1487, %fd9521, 0d0000000000000000;
	add.f64 	%fd1488, %fd9520, 0d0000000000000000;
	add.f64 	%fd1489, %fd9519, 0d0000000000000000;
	add.f64 	%fd1490, %fd9518, 0d0000000000000000;
	add.f64 	%fd1491, %fd9517, 0d0000000000000000;
	add.f64 	%fd1492, %fd9516, 0d0000000000000000;
	add.f64 	%fd1493, %fd9515, 0d0000000000000000;
	add.f64 	%fd1494, %fd9514, 0d0000000000000000;
	ld.param.u64 	%rd308, [%rd28+8];
	ld.param.u32 	%r765, [%rd28+32];
	ld.param.u32 	%r766, [%rd28+60];
	setp.le.s32 	%p816, %r766, %r2533;
	selp.u16 	%rs186, 1, 0, %p816;
	or.b16  	%rs188, %rs186, %rs346;
	setp.eq.s16 	%p817, %rs188, 0;
	mov.f64 	%fd9523, 0d0000000000000000;
	mov.f64 	%fd9524, 0d0000000000000000;
	mov.f64 	%fd9525, 0d0000000000000000;
	mov.f64 	%fd9526, 0d0000000000000000;
	mov.f64 	%fd9527, 0d0000000000000000;
	mov.f64 	%fd9528, 0d0000000000000000;
	mov.f64 	%fd9529, 0d0000000000000000;
	mov.f64 	%fd9530, 0d0000000000000000;
	mov.f64 	%fd9531, 0d0000000000000000;
	@%p817 bra 	$L__BB13_489;

	add.s32 	%r2330, %r38, 13;
	st.local.v2.u32 	[%rd30], {%r2330, %r766};
	mov.u64 	%rd2424, $str$1;
	cvta.global.u64 	%rd2425, %rd2424;
	{ // callseq 467, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2425;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1787, [retval0+0];
	} // callseq 467
	bra.uni 	$L__BB13_491;

$L__BB13_489:
	setp.eq.s64 	%p818, %rd308, 0;
	@%p818 bra 	$L__BB13_491;

	add.s32 	%r2721, %r38, 13;
	cvta.to.global.u64 	%rd2427, %rd308;
	mul.wide.s32 	%rd2428, %r765, %r2721;
	add.s64 	%rd2429, %rd2427, %rd2428;
	ld.global.f64 	%fd5257, [%rd2429];
	add.f64 	%fd9523, %fd5257, 0d0000000000000000;
	ld.global.f64 	%fd5258, [%rd2429+8];
	add.f64 	%fd9524, %fd5258, 0d0000000000000000;
	ld.global.f64 	%fd5259, [%rd2429+16];
	add.f64 	%fd9525, %fd5259, 0d0000000000000000;
	ld.global.f64 	%fd5260, [%rd2429+24];
	add.f64 	%fd9526, %fd5260, 0d0000000000000000;
	ld.global.f64 	%fd5261, [%rd2429+32];
	add.f64 	%fd9527, %fd5261, 0d0000000000000000;
	ld.global.f64 	%fd5262, [%rd2429+40];
	add.f64 	%fd9528, %fd5262, 0d0000000000000000;
	ld.global.f64 	%fd5263, [%rd2429+48];
	add.f64 	%fd9529, %fd5263, 0d0000000000000000;
	ld.global.f64 	%fd5264, [%rd2429+56];
	add.f64 	%fd9530, %fd5264, 0d0000000000000000;
	ld.global.f64 	%fd5265, [%rd2429+64];
	add.f64 	%fd9531, %fd5265, 0d0000000000000000;

$L__BB13_491:
	add.s32 	%r2535, %r38, 12;
	shr.u32 	%r2534, %r2535, 31;
	cvt.u16.u32 	%rs347, %r2534;
	add.f64 	%fd1513, %fd9531, 0d0000000000000000;
	add.f64 	%fd1514, %fd9530, 0d0000000000000000;
	add.f64 	%fd1515, %fd9529, 0d0000000000000000;
	add.f64 	%fd1516, %fd9528, 0d0000000000000000;
	add.f64 	%fd1517, %fd9527, 0d0000000000000000;
	add.f64 	%fd1518, %fd9526, 0d0000000000000000;
	add.f64 	%fd1519, %fd9525, 0d0000000000000000;
	add.f64 	%fd1520, %fd9524, 0d0000000000000000;
	add.f64 	%fd1521, %fd9523, 0d0000000000000000;
	ld.param.u64 	%rd309, [%rd28+8];
	ld.param.u32 	%r767, [%rd28+32];
	ld.param.u32 	%r768, [%rd28+60];
	setp.le.s32 	%p819, %r768, %r2535;
	selp.u16 	%rs189, 1, 0, %p819;
	or.b16  	%rs191, %rs189, %rs347;
	setp.eq.s16 	%p820, %rs191, 0;
	mov.f64 	%fd9532, 0d0000000000000000;
	mov.f64 	%fd9533, 0d0000000000000000;
	mov.f64 	%fd9534, 0d0000000000000000;
	mov.f64 	%fd9535, 0d0000000000000000;
	mov.f64 	%fd9536, 0d0000000000000000;
	mov.f64 	%fd9537, 0d0000000000000000;
	mov.f64 	%fd9538, 0d0000000000000000;
	mov.f64 	%fd9539, 0d0000000000000000;
	mov.f64 	%fd9540, 0d0000000000000000;
	@%p820 bra 	$L__BB13_493;

	add.s32 	%r2331, %r38, 12;
	st.local.v2.u32 	[%rd30], {%r2331, %r768};
	mov.u64 	%rd2430, $str$1;
	cvta.global.u64 	%rd2431, %rd2430;
	{ // callseq 468, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2431;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1792, [retval0+0];
	} // callseq 468
	bra.uni 	$L__BB13_495;

$L__BB13_493:
	setp.eq.s64 	%p821, %rd309, 0;
	@%p821 bra 	$L__BB13_495;

	add.s32 	%r2720, %r38, 12;
	cvta.to.global.u64 	%rd2433, %rd309;
	mul.wide.s32 	%rd2434, %r767, %r2720;
	add.s64 	%rd2435, %rd2433, %rd2434;
	ld.global.f64 	%fd5284, [%rd2435];
	add.f64 	%fd9532, %fd5284, 0d0000000000000000;
	ld.global.f64 	%fd5285, [%rd2435+8];
	add.f64 	%fd9533, %fd5285, 0d0000000000000000;
	ld.global.f64 	%fd5286, [%rd2435+16];
	add.f64 	%fd9534, %fd5286, 0d0000000000000000;
	ld.global.f64 	%fd5287, [%rd2435+24];
	add.f64 	%fd9535, %fd5287, 0d0000000000000000;
	ld.global.f64 	%fd5288, [%rd2435+32];
	add.f64 	%fd9536, %fd5288, 0d0000000000000000;
	ld.global.f64 	%fd5289, [%rd2435+40];
	add.f64 	%fd9537, %fd5289, 0d0000000000000000;
	ld.global.f64 	%fd5290, [%rd2435+48];
	add.f64 	%fd9538, %fd5290, 0d0000000000000000;
	ld.global.f64 	%fd5291, [%rd2435+56];
	add.f64 	%fd9539, %fd5291, 0d0000000000000000;
	ld.global.f64 	%fd5292, [%rd2435+64];
	add.f64 	%fd9540, %fd5292, 0d0000000000000000;

$L__BB13_495:
	add.s32 	%r2537, %r38, 11;
	shr.u32 	%r2536, %r2537, 31;
	cvt.u16.u32 	%rs348, %r2536;
	add.f64 	%fd1540, %fd9540, 0d0000000000000000;
	add.f64 	%fd1541, %fd9539, 0d0000000000000000;
	add.f64 	%fd1542, %fd9538, 0d0000000000000000;
	add.f64 	%fd1543, %fd9537, 0d0000000000000000;
	add.f64 	%fd1544, %fd9536, 0d0000000000000000;
	add.f64 	%fd1545, %fd9535, 0d0000000000000000;
	add.f64 	%fd1546, %fd9534, 0d0000000000000000;
	add.f64 	%fd1547, %fd9533, 0d0000000000000000;
	add.f64 	%fd1548, %fd9532, 0d0000000000000000;
	ld.param.u64 	%rd310, [%rd28+8];
	ld.param.u32 	%r769, [%rd28+32];
	ld.param.u32 	%r770, [%rd28+60];
	setp.le.s32 	%p822, %r770, %r2537;
	selp.u16 	%rs192, 1, 0, %p822;
	or.b16  	%rs194, %rs192, %rs348;
	setp.eq.s16 	%p823, %rs194, 0;
	mov.f64 	%fd9541, 0d0000000000000000;
	mov.f64 	%fd9542, 0d0000000000000000;
	mov.f64 	%fd9543, 0d0000000000000000;
	mov.f64 	%fd9544, 0d0000000000000000;
	mov.f64 	%fd9545, 0d0000000000000000;
	mov.f64 	%fd9546, 0d0000000000000000;
	mov.f64 	%fd9547, 0d0000000000000000;
	mov.f64 	%fd9548, 0d0000000000000000;
	mov.f64 	%fd9549, 0d0000000000000000;
	@%p823 bra 	$L__BB13_497;

	add.s32 	%r2332, %r38, 11;
	st.local.v2.u32 	[%rd30], {%r2332, %r770};
	mov.u64 	%rd2436, $str$1;
	cvta.global.u64 	%rd2437, %rd2436;
	{ // callseq 469, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2437;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1797, [retval0+0];
	} // callseq 469
	bra.uni 	$L__BB13_499;

$L__BB13_497:
	setp.eq.s64 	%p824, %rd310, 0;
	@%p824 bra 	$L__BB13_499;

	add.s32 	%r2719, %r38, 11;
	cvta.to.global.u64 	%rd2439, %rd310;
	mul.wide.s32 	%rd2440, %r769, %r2719;
	add.s64 	%rd2441, %rd2439, %rd2440;
	ld.global.f64 	%fd5311, [%rd2441];
	add.f64 	%fd9541, %fd5311, 0d0000000000000000;
	ld.global.f64 	%fd5312, [%rd2441+8];
	add.f64 	%fd9542, %fd5312, 0d0000000000000000;
	ld.global.f64 	%fd5313, [%rd2441+16];
	add.f64 	%fd9543, %fd5313, 0d0000000000000000;
	ld.global.f64 	%fd5314, [%rd2441+24];
	add.f64 	%fd9544, %fd5314, 0d0000000000000000;
	ld.global.f64 	%fd5315, [%rd2441+32];
	add.f64 	%fd9545, %fd5315, 0d0000000000000000;
	ld.global.f64 	%fd5316, [%rd2441+40];
	add.f64 	%fd9546, %fd5316, 0d0000000000000000;
	ld.global.f64 	%fd5317, [%rd2441+48];
	add.f64 	%fd9547, %fd5317, 0d0000000000000000;
	ld.global.f64 	%fd5318, [%rd2441+56];
	add.f64 	%fd9548, %fd5318, 0d0000000000000000;
	ld.global.f64 	%fd5319, [%rd2441+64];
	add.f64 	%fd9549, %fd5319, 0d0000000000000000;

$L__BB13_499:
	add.s32 	%r2539, %r38, 10;
	shr.u32 	%r2538, %r2539, 31;
	cvt.u16.u32 	%rs349, %r2538;
	add.f64 	%fd1567, %fd9549, 0d0000000000000000;
	add.f64 	%fd1568, %fd9548, 0d0000000000000000;
	add.f64 	%fd1569, %fd9547, 0d0000000000000000;
	add.f64 	%fd1570, %fd9546, 0d0000000000000000;
	add.f64 	%fd1571, %fd9545, 0d0000000000000000;
	add.f64 	%fd1572, %fd9544, 0d0000000000000000;
	add.f64 	%fd1573, %fd9543, 0d0000000000000000;
	add.f64 	%fd1574, %fd9542, 0d0000000000000000;
	add.f64 	%fd1575, %fd9541, 0d0000000000000000;
	ld.param.u64 	%rd311, [%rd28+8];
	ld.param.u32 	%r771, [%rd28+32];
	ld.param.u32 	%r772, [%rd28+60];
	setp.le.s32 	%p825, %r772, %r2539;
	selp.u16 	%rs195, 1, 0, %p825;
	or.b16  	%rs197, %rs195, %rs349;
	setp.eq.s16 	%p826, %rs197, 0;
	mov.f64 	%fd9550, 0d0000000000000000;
	mov.f64 	%fd9551, 0d0000000000000000;
	mov.f64 	%fd9552, 0d0000000000000000;
	mov.f64 	%fd9553, 0d0000000000000000;
	mov.f64 	%fd9554, 0d0000000000000000;
	mov.f64 	%fd9555, 0d0000000000000000;
	mov.f64 	%fd9556, 0d0000000000000000;
	mov.f64 	%fd9557, 0d0000000000000000;
	mov.f64 	%fd9558, 0d0000000000000000;
	@%p826 bra 	$L__BB13_501;

	add.s32 	%r2333, %r38, 10;
	st.local.v2.u32 	[%rd30], {%r2333, %r772};
	mov.u64 	%rd2442, $str$1;
	cvta.global.u64 	%rd2443, %rd2442;
	{ // callseq 470, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2443;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1802, [retval0+0];
	} // callseq 470
	bra.uni 	$L__BB13_503;

$L__BB13_501:
	setp.eq.s64 	%p827, %rd311, 0;
	@%p827 bra 	$L__BB13_503;

	add.s32 	%r2718, %r38, 10;
	cvta.to.global.u64 	%rd2445, %rd311;
	mul.wide.s32 	%rd2446, %r771, %r2718;
	add.s64 	%rd2447, %rd2445, %rd2446;
	ld.global.f64 	%fd5338, [%rd2447];
	add.f64 	%fd9550, %fd5338, 0d0000000000000000;
	ld.global.f64 	%fd5339, [%rd2447+8];
	add.f64 	%fd9551, %fd5339, 0d0000000000000000;
	ld.global.f64 	%fd5340, [%rd2447+16];
	add.f64 	%fd9552, %fd5340, 0d0000000000000000;
	ld.global.f64 	%fd5341, [%rd2447+24];
	add.f64 	%fd9553, %fd5341, 0d0000000000000000;
	ld.global.f64 	%fd5342, [%rd2447+32];
	add.f64 	%fd9554, %fd5342, 0d0000000000000000;
	ld.global.f64 	%fd5343, [%rd2447+40];
	add.f64 	%fd9555, %fd5343, 0d0000000000000000;
	ld.global.f64 	%fd5344, [%rd2447+48];
	add.f64 	%fd9556, %fd5344, 0d0000000000000000;
	ld.global.f64 	%fd5345, [%rd2447+56];
	add.f64 	%fd9557, %fd5345, 0d0000000000000000;
	ld.global.f64 	%fd5346, [%rd2447+64];
	add.f64 	%fd9558, %fd5346, 0d0000000000000000;

$L__BB13_503:
	add.s32 	%r2541, %r38, 9;
	shr.u32 	%r2540, %r2541, 31;
	cvt.u16.u32 	%rs350, %r2540;
	add.f64 	%fd1594, %fd9558, 0d0000000000000000;
	add.f64 	%fd1595, %fd9557, 0d0000000000000000;
	add.f64 	%fd1596, %fd9556, 0d0000000000000000;
	add.f64 	%fd1597, %fd9555, 0d0000000000000000;
	add.f64 	%fd1598, %fd9554, 0d0000000000000000;
	add.f64 	%fd1599, %fd9553, 0d0000000000000000;
	add.f64 	%fd1600, %fd9552, 0d0000000000000000;
	add.f64 	%fd1601, %fd9551, 0d0000000000000000;
	add.f64 	%fd1602, %fd9550, 0d0000000000000000;
	ld.param.u64 	%rd312, [%rd28+8];
	ld.param.u32 	%r773, [%rd28+32];
	ld.param.u32 	%r774, [%rd28+60];
	setp.le.s32 	%p828, %r774, %r2541;
	selp.u16 	%rs198, 1, 0, %p828;
	or.b16  	%rs200, %rs198, %rs350;
	setp.eq.s16 	%p829, %rs200, 0;
	mov.f64 	%fd9559, 0d0000000000000000;
	mov.f64 	%fd9560, 0d0000000000000000;
	mov.f64 	%fd9561, 0d0000000000000000;
	mov.f64 	%fd9562, 0d0000000000000000;
	mov.f64 	%fd9563, 0d0000000000000000;
	mov.f64 	%fd9564, 0d0000000000000000;
	mov.f64 	%fd9565, 0d0000000000000000;
	mov.f64 	%fd9566, 0d0000000000000000;
	mov.f64 	%fd9567, 0d0000000000000000;
	@%p829 bra 	$L__BB13_505;

	add.s32 	%r2334, %r38, 9;
	st.local.v2.u32 	[%rd30], {%r2334, %r774};
	mov.u64 	%rd2448, $str$1;
	cvta.global.u64 	%rd2449, %rd2448;
	{ // callseq 471, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2449;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1807, [retval0+0];
	} // callseq 471
	bra.uni 	$L__BB13_507;

$L__BB13_505:
	setp.eq.s64 	%p830, %rd312, 0;
	@%p830 bra 	$L__BB13_507;

	add.s32 	%r2717, %r38, 9;
	cvta.to.global.u64 	%rd2451, %rd312;
	mul.wide.s32 	%rd2452, %r773, %r2717;
	add.s64 	%rd2453, %rd2451, %rd2452;
	ld.global.f64 	%fd5365, [%rd2453];
	add.f64 	%fd9559, %fd5365, 0d0000000000000000;
	ld.global.f64 	%fd5366, [%rd2453+8];
	add.f64 	%fd9560, %fd5366, 0d0000000000000000;
	ld.global.f64 	%fd5367, [%rd2453+16];
	add.f64 	%fd9561, %fd5367, 0d0000000000000000;
	ld.global.f64 	%fd5368, [%rd2453+24];
	add.f64 	%fd9562, %fd5368, 0d0000000000000000;
	ld.global.f64 	%fd5369, [%rd2453+32];
	add.f64 	%fd9563, %fd5369, 0d0000000000000000;
	ld.global.f64 	%fd5370, [%rd2453+40];
	add.f64 	%fd9564, %fd5370, 0d0000000000000000;
	ld.global.f64 	%fd5371, [%rd2453+48];
	add.f64 	%fd9565, %fd5371, 0d0000000000000000;
	ld.global.f64 	%fd5372, [%rd2453+56];
	add.f64 	%fd9566, %fd5372, 0d0000000000000000;
	ld.global.f64 	%fd5373, [%rd2453+64];
	add.f64 	%fd9567, %fd5373, 0d0000000000000000;

$L__BB13_507:
	add.s32 	%r2543, %r38, 8;
	shr.u32 	%r2542, %r2543, 31;
	cvt.u16.u32 	%rs351, %r2542;
	add.f64 	%fd1621, %fd9567, 0d0000000000000000;
	add.f64 	%fd1622, %fd9566, 0d0000000000000000;
	add.f64 	%fd1623, %fd9565, 0d0000000000000000;
	add.f64 	%fd1624, %fd9564, 0d0000000000000000;
	add.f64 	%fd1625, %fd9563, 0d0000000000000000;
	add.f64 	%fd1626, %fd9562, 0d0000000000000000;
	add.f64 	%fd1627, %fd9561, 0d0000000000000000;
	add.f64 	%fd1628, %fd9560, 0d0000000000000000;
	add.f64 	%fd1629, %fd9559, 0d0000000000000000;
	ld.param.u64 	%rd313, [%rd28+8];
	ld.param.u32 	%r775, [%rd28+32];
	ld.param.u32 	%r776, [%rd28+60];
	setp.le.s32 	%p831, %r776, %r2543;
	selp.u16 	%rs201, 1, 0, %p831;
	or.b16  	%rs203, %rs201, %rs351;
	setp.eq.s16 	%p832, %rs203, 0;
	mov.f64 	%fd9568, 0d0000000000000000;
	mov.f64 	%fd9569, 0d0000000000000000;
	mov.f64 	%fd9570, 0d0000000000000000;
	mov.f64 	%fd9571, 0d0000000000000000;
	mov.f64 	%fd9572, 0d0000000000000000;
	mov.f64 	%fd9573, 0d0000000000000000;
	mov.f64 	%fd9574, 0d0000000000000000;
	mov.f64 	%fd9575, 0d0000000000000000;
	mov.f64 	%fd9576, 0d0000000000000000;
	@%p832 bra 	$L__BB13_509;

	add.s32 	%r2335, %r38, 8;
	st.local.v2.u32 	[%rd30], {%r2335, %r776};
	mov.u64 	%rd2454, $str$1;
	cvta.global.u64 	%rd2455, %rd2454;
	{ // callseq 472, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2455;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1812, [retval0+0];
	} // callseq 472
	bra.uni 	$L__BB13_511;

$L__BB13_509:
	setp.eq.s64 	%p833, %rd313, 0;
	@%p833 bra 	$L__BB13_511;

	add.s32 	%r2716, %r38, 8;
	cvta.to.global.u64 	%rd2457, %rd313;
	mul.wide.s32 	%rd2458, %r775, %r2716;
	add.s64 	%rd2459, %rd2457, %rd2458;
	ld.global.f64 	%fd5392, [%rd2459];
	add.f64 	%fd9568, %fd5392, 0d0000000000000000;
	ld.global.f64 	%fd5393, [%rd2459+8];
	add.f64 	%fd9569, %fd5393, 0d0000000000000000;
	ld.global.f64 	%fd5394, [%rd2459+16];
	add.f64 	%fd9570, %fd5394, 0d0000000000000000;
	ld.global.f64 	%fd5395, [%rd2459+24];
	add.f64 	%fd9571, %fd5395, 0d0000000000000000;
	ld.global.f64 	%fd5396, [%rd2459+32];
	add.f64 	%fd9572, %fd5396, 0d0000000000000000;
	ld.global.f64 	%fd5397, [%rd2459+40];
	add.f64 	%fd9573, %fd5397, 0d0000000000000000;
	ld.global.f64 	%fd5398, [%rd2459+48];
	add.f64 	%fd9574, %fd5398, 0d0000000000000000;
	ld.global.f64 	%fd5399, [%rd2459+56];
	add.f64 	%fd9575, %fd5399, 0d0000000000000000;
	ld.global.f64 	%fd5400, [%rd2459+64];
	add.f64 	%fd9576, %fd5400, 0d0000000000000000;

$L__BB13_511:
	add.s32 	%r2545, %r38, 7;
	shr.u32 	%r2544, %r2545, 31;
	cvt.u16.u32 	%rs352, %r2544;
	add.f64 	%fd1648, %fd9576, 0d0000000000000000;
	add.f64 	%fd1649, %fd9575, 0d0000000000000000;
	add.f64 	%fd1650, %fd9574, 0d0000000000000000;
	add.f64 	%fd1651, %fd9573, 0d0000000000000000;
	add.f64 	%fd1652, %fd9572, 0d0000000000000000;
	add.f64 	%fd1653, %fd9571, 0d0000000000000000;
	add.f64 	%fd1654, %fd9570, 0d0000000000000000;
	add.f64 	%fd1655, %fd9569, 0d0000000000000000;
	add.f64 	%fd1656, %fd9568, 0d0000000000000000;
	ld.param.u64 	%rd314, [%rd28+8];
	ld.param.u32 	%r777, [%rd28+32];
	ld.param.u32 	%r778, [%rd28+60];
	setp.le.s32 	%p834, %r778, %r2545;
	selp.u16 	%rs204, 1, 0, %p834;
	or.b16  	%rs206, %rs204, %rs352;
	setp.eq.s16 	%p835, %rs206, 0;
	mov.f64 	%fd9577, 0d0000000000000000;
	mov.f64 	%fd9578, 0d0000000000000000;
	mov.f64 	%fd9579, 0d0000000000000000;
	mov.f64 	%fd9580, 0d0000000000000000;
	mov.f64 	%fd9581, 0d0000000000000000;
	mov.f64 	%fd9582, 0d0000000000000000;
	mov.f64 	%fd9583, 0d0000000000000000;
	mov.f64 	%fd9584, 0d0000000000000000;
	mov.f64 	%fd9585, 0d0000000000000000;
	@%p835 bra 	$L__BB13_513;

	add.s32 	%r2336, %r38, 7;
	st.local.v2.u32 	[%rd30], {%r2336, %r778};
	mov.u64 	%rd2460, $str$1;
	cvta.global.u64 	%rd2461, %rd2460;
	{ // callseq 473, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2461;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1817, [retval0+0];
	} // callseq 473
	bra.uni 	$L__BB13_515;

$L__BB13_513:
	setp.eq.s64 	%p836, %rd314, 0;
	@%p836 bra 	$L__BB13_515;

	add.s32 	%r2715, %r38, 7;
	cvta.to.global.u64 	%rd2463, %rd314;
	mul.wide.s32 	%rd2464, %r777, %r2715;
	add.s64 	%rd2465, %rd2463, %rd2464;
	ld.global.f64 	%fd5419, [%rd2465];
	add.f64 	%fd9577, %fd5419, 0d0000000000000000;
	ld.global.f64 	%fd5420, [%rd2465+8];
	add.f64 	%fd9578, %fd5420, 0d0000000000000000;
	ld.global.f64 	%fd5421, [%rd2465+16];
	add.f64 	%fd9579, %fd5421, 0d0000000000000000;
	ld.global.f64 	%fd5422, [%rd2465+24];
	add.f64 	%fd9580, %fd5422, 0d0000000000000000;
	ld.global.f64 	%fd5423, [%rd2465+32];
	add.f64 	%fd9581, %fd5423, 0d0000000000000000;
	ld.global.f64 	%fd5424, [%rd2465+40];
	add.f64 	%fd9582, %fd5424, 0d0000000000000000;
	ld.global.f64 	%fd5425, [%rd2465+48];
	add.f64 	%fd9583, %fd5425, 0d0000000000000000;
	ld.global.f64 	%fd5426, [%rd2465+56];
	add.f64 	%fd9584, %fd5426, 0d0000000000000000;
	ld.global.f64 	%fd5427, [%rd2465+64];
	add.f64 	%fd9585, %fd5427, 0d0000000000000000;

$L__BB13_515:
	add.s32 	%r2547, %r38, 6;
	shr.u32 	%r2546, %r2547, 31;
	cvt.u16.u32 	%rs353, %r2546;
	add.f64 	%fd1675, %fd9585, 0d0000000000000000;
	add.f64 	%fd1676, %fd9584, 0d0000000000000000;
	add.f64 	%fd1677, %fd9583, 0d0000000000000000;
	add.f64 	%fd1678, %fd9582, 0d0000000000000000;
	add.f64 	%fd1679, %fd9581, 0d0000000000000000;
	add.f64 	%fd1680, %fd9580, 0d0000000000000000;
	add.f64 	%fd1681, %fd9579, 0d0000000000000000;
	add.f64 	%fd1682, %fd9578, 0d0000000000000000;
	add.f64 	%fd1683, %fd9577, 0d0000000000000000;
	ld.param.u64 	%rd315, [%rd28+8];
	ld.param.u32 	%r779, [%rd28+32];
	ld.param.u32 	%r780, [%rd28+60];
	setp.le.s32 	%p837, %r780, %r2547;
	selp.u16 	%rs207, 1, 0, %p837;
	or.b16  	%rs209, %rs207, %rs353;
	setp.eq.s16 	%p838, %rs209, 0;
	mov.f64 	%fd9586, 0d0000000000000000;
	mov.f64 	%fd9587, 0d0000000000000000;
	mov.f64 	%fd9588, 0d0000000000000000;
	mov.f64 	%fd9589, 0d0000000000000000;
	mov.f64 	%fd9590, 0d0000000000000000;
	mov.f64 	%fd9591, 0d0000000000000000;
	mov.f64 	%fd9592, 0d0000000000000000;
	mov.f64 	%fd9593, 0d0000000000000000;
	mov.f64 	%fd9594, 0d0000000000000000;
	@%p838 bra 	$L__BB13_517;

	add.s32 	%r2337, %r38, 6;
	st.local.v2.u32 	[%rd30], {%r2337, %r780};
	mov.u64 	%rd2466, $str$1;
	cvta.global.u64 	%rd2467, %rd2466;
	{ // callseq 474, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2467;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1822, [retval0+0];
	} // callseq 474
	bra.uni 	$L__BB13_519;

$L__BB13_517:
	setp.eq.s64 	%p839, %rd315, 0;
	@%p839 bra 	$L__BB13_519;

	add.s32 	%r2714, %r38, 6;
	cvta.to.global.u64 	%rd2469, %rd315;
	mul.wide.s32 	%rd2470, %r779, %r2714;
	add.s64 	%rd2471, %rd2469, %rd2470;
	ld.global.f64 	%fd5446, [%rd2471];
	add.f64 	%fd9586, %fd5446, 0d0000000000000000;
	ld.global.f64 	%fd5447, [%rd2471+8];
	add.f64 	%fd9587, %fd5447, 0d0000000000000000;
	ld.global.f64 	%fd5448, [%rd2471+16];
	add.f64 	%fd9588, %fd5448, 0d0000000000000000;
	ld.global.f64 	%fd5449, [%rd2471+24];
	add.f64 	%fd9589, %fd5449, 0d0000000000000000;
	ld.global.f64 	%fd5450, [%rd2471+32];
	add.f64 	%fd9590, %fd5450, 0d0000000000000000;
	ld.global.f64 	%fd5451, [%rd2471+40];
	add.f64 	%fd9591, %fd5451, 0d0000000000000000;
	ld.global.f64 	%fd5452, [%rd2471+48];
	add.f64 	%fd9592, %fd5452, 0d0000000000000000;
	ld.global.f64 	%fd5453, [%rd2471+56];
	add.f64 	%fd9593, %fd5453, 0d0000000000000000;
	ld.global.f64 	%fd5454, [%rd2471+64];
	add.f64 	%fd9594, %fd5454, 0d0000000000000000;

$L__BB13_519:
	add.s32 	%r2549, %r38, 5;
	shr.u32 	%r2548, %r2549, 31;
	cvt.u16.u32 	%rs354, %r2548;
	add.f64 	%fd1702, %fd9594, 0d0000000000000000;
	add.f64 	%fd1703, %fd9593, 0d0000000000000000;
	add.f64 	%fd1704, %fd9592, 0d0000000000000000;
	add.f64 	%fd1705, %fd9591, 0d0000000000000000;
	add.f64 	%fd1706, %fd9590, 0d0000000000000000;
	add.f64 	%fd1707, %fd9589, 0d0000000000000000;
	add.f64 	%fd1708, %fd9588, 0d0000000000000000;
	add.f64 	%fd1709, %fd9587, 0d0000000000000000;
	add.f64 	%fd1710, %fd9586, 0d0000000000000000;
	ld.param.u64 	%rd316, [%rd28+8];
	ld.param.u32 	%r781, [%rd28+32];
	ld.param.u32 	%r782, [%rd28+60];
	setp.le.s32 	%p840, %r782, %r2549;
	selp.u16 	%rs210, 1, 0, %p840;
	or.b16  	%rs212, %rs210, %rs354;
	setp.eq.s16 	%p841, %rs212, 0;
	mov.f64 	%fd9595, 0d0000000000000000;
	mov.f64 	%fd9596, 0d0000000000000000;
	mov.f64 	%fd9597, 0d0000000000000000;
	mov.f64 	%fd9598, 0d0000000000000000;
	mov.f64 	%fd9599, 0d0000000000000000;
	mov.f64 	%fd9600, 0d0000000000000000;
	mov.f64 	%fd9601, 0d0000000000000000;
	mov.f64 	%fd9602, 0d0000000000000000;
	mov.f64 	%fd9603, 0d0000000000000000;
	@%p841 bra 	$L__BB13_521;

	add.s32 	%r2338, %r38, 5;
	st.local.v2.u32 	[%rd30], {%r2338, %r782};
	mov.u64 	%rd2472, $str$1;
	cvta.global.u64 	%rd2473, %rd2472;
	{ // callseq 475, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2473;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1827, [retval0+0];
	} // callseq 475
	bra.uni 	$L__BB13_523;

$L__BB13_521:
	setp.eq.s64 	%p842, %rd316, 0;
	@%p842 bra 	$L__BB13_523;

	add.s32 	%r2713, %r38, 5;
	cvta.to.global.u64 	%rd2475, %rd316;
	mul.wide.s32 	%rd2476, %r781, %r2713;
	add.s64 	%rd2477, %rd2475, %rd2476;
	ld.global.f64 	%fd5473, [%rd2477];
	add.f64 	%fd9595, %fd5473, 0d0000000000000000;
	ld.global.f64 	%fd5474, [%rd2477+8];
	add.f64 	%fd9596, %fd5474, 0d0000000000000000;
	ld.global.f64 	%fd5475, [%rd2477+16];
	add.f64 	%fd9597, %fd5475, 0d0000000000000000;
	ld.global.f64 	%fd5476, [%rd2477+24];
	add.f64 	%fd9598, %fd5476, 0d0000000000000000;
	ld.global.f64 	%fd5477, [%rd2477+32];
	add.f64 	%fd9599, %fd5477, 0d0000000000000000;
	ld.global.f64 	%fd5478, [%rd2477+40];
	add.f64 	%fd9600, %fd5478, 0d0000000000000000;
	ld.global.f64 	%fd5479, [%rd2477+48];
	add.f64 	%fd9601, %fd5479, 0d0000000000000000;
	ld.global.f64 	%fd5480, [%rd2477+56];
	add.f64 	%fd9602, %fd5480, 0d0000000000000000;
	ld.global.f64 	%fd5481, [%rd2477+64];
	add.f64 	%fd9603, %fd5481, 0d0000000000000000;

$L__BB13_523:
	add.s32 	%r2551, %r38, 4;
	shr.u32 	%r2550, %r2551, 31;
	cvt.u16.u32 	%rs355, %r2550;
	add.f64 	%fd1729, %fd9603, 0d0000000000000000;
	add.f64 	%fd1730, %fd9602, 0d0000000000000000;
	add.f64 	%fd1731, %fd9601, 0d0000000000000000;
	add.f64 	%fd1732, %fd9600, 0d0000000000000000;
	add.f64 	%fd1733, %fd9599, 0d0000000000000000;
	add.f64 	%fd1734, %fd9598, 0d0000000000000000;
	add.f64 	%fd1735, %fd9597, 0d0000000000000000;
	add.f64 	%fd1736, %fd9596, 0d0000000000000000;
	add.f64 	%fd1737, %fd9595, 0d0000000000000000;
	ld.param.u64 	%rd317, [%rd28+8];
	ld.param.u32 	%r783, [%rd28+32];
	ld.param.u32 	%r784, [%rd28+60];
	setp.le.s32 	%p843, %r784, %r2551;
	selp.u16 	%rs213, 1, 0, %p843;
	or.b16  	%rs215, %rs213, %rs355;
	setp.eq.s16 	%p844, %rs215, 0;
	mov.f64 	%fd9604, 0d0000000000000000;
	mov.f64 	%fd9605, 0d0000000000000000;
	mov.f64 	%fd9606, 0d0000000000000000;
	mov.f64 	%fd9607, 0d0000000000000000;
	mov.f64 	%fd9608, 0d0000000000000000;
	mov.f64 	%fd9609, 0d0000000000000000;
	mov.f64 	%fd9610, 0d0000000000000000;
	mov.f64 	%fd9611, 0d0000000000000000;
	mov.f64 	%fd9612, 0d0000000000000000;
	@%p844 bra 	$L__BB13_525;

	add.s32 	%r2339, %r38, 4;
	st.local.v2.u32 	[%rd30], {%r2339, %r784};
	mov.u64 	%rd2478, $str$1;
	cvta.global.u64 	%rd2479, %rd2478;
	{ // callseq 476, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2479;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1832, [retval0+0];
	} // callseq 476
	bra.uni 	$L__BB13_527;

$L__BB13_525:
	setp.eq.s64 	%p845, %rd317, 0;
	@%p845 bra 	$L__BB13_527;

	add.s32 	%r2712, %r38, 4;
	cvta.to.global.u64 	%rd2481, %rd317;
	mul.wide.s32 	%rd2482, %r783, %r2712;
	add.s64 	%rd2483, %rd2481, %rd2482;
	ld.global.f64 	%fd5500, [%rd2483];
	add.f64 	%fd9604, %fd5500, 0d0000000000000000;
	ld.global.f64 	%fd5501, [%rd2483+8];
	add.f64 	%fd9605, %fd5501, 0d0000000000000000;
	ld.global.f64 	%fd5502, [%rd2483+16];
	add.f64 	%fd9606, %fd5502, 0d0000000000000000;
	ld.global.f64 	%fd5503, [%rd2483+24];
	add.f64 	%fd9607, %fd5503, 0d0000000000000000;
	ld.global.f64 	%fd5504, [%rd2483+32];
	add.f64 	%fd9608, %fd5504, 0d0000000000000000;
	ld.global.f64 	%fd5505, [%rd2483+40];
	add.f64 	%fd9609, %fd5505, 0d0000000000000000;
	ld.global.f64 	%fd5506, [%rd2483+48];
	add.f64 	%fd9610, %fd5506, 0d0000000000000000;
	ld.global.f64 	%fd5507, [%rd2483+56];
	add.f64 	%fd9611, %fd5507, 0d0000000000000000;
	ld.global.f64 	%fd5508, [%rd2483+64];
	add.f64 	%fd9612, %fd5508, 0d0000000000000000;

$L__BB13_527:
	add.s32 	%r2553, %r38, 3;
	shr.u32 	%r2552, %r2553, 31;
	cvt.u16.u32 	%rs356, %r2552;
	add.f64 	%fd1756, %fd9612, 0d0000000000000000;
	add.f64 	%fd1757, %fd9611, 0d0000000000000000;
	add.f64 	%fd1758, %fd9610, 0d0000000000000000;
	add.f64 	%fd1759, %fd9609, 0d0000000000000000;
	add.f64 	%fd1760, %fd9608, 0d0000000000000000;
	add.f64 	%fd1761, %fd9607, 0d0000000000000000;
	add.f64 	%fd1762, %fd9606, 0d0000000000000000;
	add.f64 	%fd1763, %fd9605, 0d0000000000000000;
	add.f64 	%fd1764, %fd9604, 0d0000000000000000;
	ld.param.u64 	%rd318, [%rd28+8];
	ld.param.u32 	%r785, [%rd28+32];
	ld.param.u32 	%r786, [%rd28+60];
	setp.le.s32 	%p846, %r786, %r2553;
	selp.u16 	%rs216, 1, 0, %p846;
	or.b16  	%rs218, %rs216, %rs356;
	setp.eq.s16 	%p847, %rs218, 0;
	mov.f64 	%fd9613, 0d0000000000000000;
	mov.f64 	%fd9614, 0d0000000000000000;
	mov.f64 	%fd9615, 0d0000000000000000;
	mov.f64 	%fd9616, 0d0000000000000000;
	mov.f64 	%fd9617, 0d0000000000000000;
	mov.f64 	%fd9618, 0d0000000000000000;
	mov.f64 	%fd9619, 0d0000000000000000;
	mov.f64 	%fd9620, 0d0000000000000000;
	mov.f64 	%fd9621, 0d0000000000000000;
	@%p847 bra 	$L__BB13_529;

	add.s32 	%r2340, %r38, 3;
	st.local.v2.u32 	[%rd30], {%r2340, %r786};
	mov.u64 	%rd2484, $str$1;
	cvta.global.u64 	%rd2485, %rd2484;
	{ // callseq 477, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2485;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1837, [retval0+0];
	} // callseq 477
	bra.uni 	$L__BB13_531;

$L__BB13_529:
	setp.eq.s64 	%p848, %rd318, 0;
	@%p848 bra 	$L__BB13_531;

	add.s32 	%r2711, %r38, 3;
	cvta.to.global.u64 	%rd2487, %rd318;
	mul.wide.s32 	%rd2488, %r785, %r2711;
	add.s64 	%rd2489, %rd2487, %rd2488;
	ld.global.f64 	%fd5527, [%rd2489];
	add.f64 	%fd9613, %fd5527, 0d0000000000000000;
	ld.global.f64 	%fd5528, [%rd2489+8];
	add.f64 	%fd9614, %fd5528, 0d0000000000000000;
	ld.global.f64 	%fd5529, [%rd2489+16];
	add.f64 	%fd9615, %fd5529, 0d0000000000000000;
	ld.global.f64 	%fd5530, [%rd2489+24];
	add.f64 	%fd9616, %fd5530, 0d0000000000000000;
	ld.global.f64 	%fd5531, [%rd2489+32];
	add.f64 	%fd9617, %fd5531, 0d0000000000000000;
	ld.global.f64 	%fd5532, [%rd2489+40];
	add.f64 	%fd9618, %fd5532, 0d0000000000000000;
	ld.global.f64 	%fd5533, [%rd2489+48];
	add.f64 	%fd9619, %fd5533, 0d0000000000000000;
	ld.global.f64 	%fd5534, [%rd2489+56];
	add.f64 	%fd9620, %fd5534, 0d0000000000000000;
	ld.global.f64 	%fd5535, [%rd2489+64];
	add.f64 	%fd9621, %fd5535, 0d0000000000000000;

$L__BB13_531:
	add.s32 	%r2555, %r38, 2;
	shr.u32 	%r2554, %r2555, 31;
	cvt.u16.u32 	%rs357, %r2554;
	add.f64 	%fd1783, %fd9621, 0d0000000000000000;
	add.f64 	%fd1784, %fd9620, 0d0000000000000000;
	add.f64 	%fd1785, %fd9619, 0d0000000000000000;
	add.f64 	%fd1786, %fd9618, 0d0000000000000000;
	add.f64 	%fd1787, %fd9617, 0d0000000000000000;
	add.f64 	%fd1788, %fd9616, 0d0000000000000000;
	add.f64 	%fd1789, %fd9615, 0d0000000000000000;
	add.f64 	%fd1790, %fd9614, 0d0000000000000000;
	add.f64 	%fd1791, %fd9613, 0d0000000000000000;
	ld.param.u64 	%rd319, [%rd28+8];
	ld.param.u32 	%r787, [%rd28+32];
	ld.param.u32 	%r788, [%rd28+60];
	setp.le.s32 	%p849, %r788, %r2555;
	selp.u16 	%rs219, 1, 0, %p849;
	or.b16  	%rs221, %rs219, %rs357;
	setp.eq.s16 	%p850, %rs221, 0;
	mov.f64 	%fd9622, 0d0000000000000000;
	mov.f64 	%fd9623, 0d0000000000000000;
	mov.f64 	%fd9624, 0d0000000000000000;
	mov.f64 	%fd9625, 0d0000000000000000;
	mov.f64 	%fd9626, 0d0000000000000000;
	mov.f64 	%fd9627, 0d0000000000000000;
	mov.f64 	%fd9628, 0d0000000000000000;
	mov.f64 	%fd9629, 0d0000000000000000;
	mov.f64 	%fd9630, 0d0000000000000000;
	@%p850 bra 	$L__BB13_533;

	add.s32 	%r2341, %r38, 2;
	st.local.v2.u32 	[%rd30], {%r2341, %r788};
	mov.u64 	%rd2490, $str$1;
	cvta.global.u64 	%rd2491, %rd2490;
	{ // callseq 478, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2491;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1842, [retval0+0];
	} // callseq 478
	bra.uni 	$L__BB13_535;

$L__BB13_533:
	setp.eq.s64 	%p851, %rd319, 0;
	@%p851 bra 	$L__BB13_535;

	add.s32 	%r2710, %r38, 2;
	cvta.to.global.u64 	%rd2493, %rd319;
	mul.wide.s32 	%rd2494, %r787, %r2710;
	add.s64 	%rd2495, %rd2493, %rd2494;
	ld.global.f64 	%fd5554, [%rd2495];
	add.f64 	%fd9622, %fd5554, 0d0000000000000000;
	ld.global.f64 	%fd5555, [%rd2495+8];
	add.f64 	%fd9623, %fd5555, 0d0000000000000000;
	ld.global.f64 	%fd5556, [%rd2495+16];
	add.f64 	%fd9624, %fd5556, 0d0000000000000000;
	ld.global.f64 	%fd5557, [%rd2495+24];
	add.f64 	%fd9625, %fd5557, 0d0000000000000000;
	ld.global.f64 	%fd5558, [%rd2495+32];
	add.f64 	%fd9626, %fd5558, 0d0000000000000000;
	ld.global.f64 	%fd5559, [%rd2495+40];
	add.f64 	%fd9627, %fd5559, 0d0000000000000000;
	ld.global.f64 	%fd5560, [%rd2495+48];
	add.f64 	%fd9628, %fd5560, 0d0000000000000000;
	ld.global.f64 	%fd5561, [%rd2495+56];
	add.f64 	%fd9629, %fd5561, 0d0000000000000000;
	ld.global.f64 	%fd5562, [%rd2495+64];
	add.f64 	%fd9630, %fd5562, 0d0000000000000000;

$L__BB13_535:
	add.s32 	%r2557, %r38, 1;
	shr.u32 	%r2556, %r2557, 31;
	cvt.u16.u32 	%rs358, %r2556;
	add.f64 	%fd1810, %fd9630, 0d0000000000000000;
	add.f64 	%fd1811, %fd9629, 0d0000000000000000;
	add.f64 	%fd1812, %fd9628, 0d0000000000000000;
	add.f64 	%fd1813, %fd9627, 0d0000000000000000;
	add.f64 	%fd1814, %fd9626, 0d0000000000000000;
	add.f64 	%fd1815, %fd9625, 0d0000000000000000;
	add.f64 	%fd1816, %fd9624, 0d0000000000000000;
	add.f64 	%fd1817, %fd9623, 0d0000000000000000;
	add.f64 	%fd1818, %fd9622, 0d0000000000000000;
	ld.param.u64 	%rd320, [%rd28+8];
	ld.param.u32 	%r789, [%rd28+32];
	ld.param.u32 	%r790, [%rd28+60];
	setp.le.s32 	%p852, %r790, %r2557;
	selp.u16 	%rs222, 1, 0, %p852;
	or.b16  	%rs224, %rs222, %rs358;
	setp.eq.s16 	%p853, %rs224, 0;
	mov.f64 	%fd9631, 0d0000000000000000;
	mov.f64 	%fd9632, 0d0000000000000000;
	mov.f64 	%fd9633, 0d0000000000000000;
	mov.f64 	%fd9634, 0d0000000000000000;
	mov.f64 	%fd9635, 0d0000000000000000;
	mov.f64 	%fd9636, 0d0000000000000000;
	mov.f64 	%fd9637, 0d0000000000000000;
	mov.f64 	%fd9638, 0d0000000000000000;
	mov.f64 	%fd9639, 0d0000000000000000;
	@%p853 bra 	$L__BB13_537;

	add.s32 	%r2342, %r38, 1;
	st.local.v2.u32 	[%rd30], {%r2342, %r790};
	mov.u64 	%rd2496, $str$1;
	cvta.global.u64 	%rd2497, %rd2496;
	{ // callseq 479, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2497;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1847, [retval0+0];
	} // callseq 479
	bra.uni 	$L__BB13_539;

$L__BB13_537:
	setp.eq.s64 	%p854, %rd320, 0;
	@%p854 bra 	$L__BB13_539;

	add.s32 	%r2709, %r38, 1;
	cvta.to.global.u64 	%rd2499, %rd320;
	mul.wide.s32 	%rd2500, %r789, %r2709;
	add.s64 	%rd2501, %rd2499, %rd2500;
	ld.global.f64 	%fd5581, [%rd2501];
	add.f64 	%fd9631, %fd5581, 0d0000000000000000;
	ld.global.f64 	%fd5582, [%rd2501+8];
	add.f64 	%fd9632, %fd5582, 0d0000000000000000;
	ld.global.f64 	%fd5583, [%rd2501+16];
	add.f64 	%fd9633, %fd5583, 0d0000000000000000;
	ld.global.f64 	%fd5584, [%rd2501+24];
	add.f64 	%fd9634, %fd5584, 0d0000000000000000;
	ld.global.f64 	%fd5585, [%rd2501+32];
	add.f64 	%fd9635, %fd5585, 0d0000000000000000;
	ld.global.f64 	%fd5586, [%rd2501+40];
	add.f64 	%fd9636, %fd5586, 0d0000000000000000;
	ld.global.f64 	%fd5587, [%rd2501+48];
	add.f64 	%fd9637, %fd5587, 0d0000000000000000;
	ld.global.f64 	%fd5588, [%rd2501+56];
	add.f64 	%fd9638, %fd5588, 0d0000000000000000;
	ld.global.f64 	%fd5589, [%rd2501+64];
	add.f64 	%fd9639, %fd5589, 0d0000000000000000;

$L__BB13_539:
	shr.u32 	%r2558, %r37, 27;
	cvt.u16.u32 	%rs360, %r2558;
	and.b16  	%rs359, %rs360, 1;
	add.f64 	%fd1837, %fd9639, 0d0000000000000000;
	add.f64 	%fd1838, %fd9638, 0d0000000000000000;
	add.f64 	%fd1839, %fd9637, 0d0000000000000000;
	add.f64 	%fd1840, %fd9636, 0d0000000000000000;
	add.f64 	%fd1841, %fd9635, 0d0000000000000000;
	add.f64 	%fd1842, %fd9634, 0d0000000000000000;
	add.f64 	%fd1843, %fd9633, 0d0000000000000000;
	add.f64 	%fd1844, %fd9632, 0d0000000000000000;
	add.f64 	%fd1845, %fd9631, 0d0000000000000000;
	ld.param.u64 	%rd321, [%rd28+8];
	ld.param.u32 	%r791, [%rd28+32];
	ld.param.u32 	%r792, [%rd28+60];
	setp.le.s32 	%p855, %r792, %r38;
	selp.u16 	%rs225, 1, 0, %p855;
	or.b16  	%rs228, %rs359, %rs225;
	setp.eq.s16 	%p856, %rs228, 0;
	mov.f64 	%fd9640, 0d0000000000000000;
	mov.f64 	%fd9641, 0d0000000000000000;
	mov.f64 	%fd9642, 0d0000000000000000;
	mov.f64 	%fd9643, 0d0000000000000000;
	mov.f64 	%fd9644, 0d0000000000000000;
	mov.f64 	%fd9645, 0d0000000000000000;
	mov.f64 	%fd9646, 0d0000000000000000;
	mov.f64 	%fd9647, 0d0000000000000000;
	mov.f64 	%fd9648, 0d0000000000000000;
	@%p856 bra 	$L__BB13_541;

	st.local.v2.u32 	[%rd30], {%r38, %r792};
	mov.u64 	%rd2502, $str$1;
	cvta.global.u64 	%rd2503, %rd2502;
	{ // callseq 480, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2503;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1850, [retval0+0];
	} // callseq 480
	bra.uni 	$L__BB13_543;

$L__BB13_541:
	setp.eq.s64 	%p857, %rd321, 0;
	@%p857 bra 	$L__BB13_543;

	cvta.to.global.u64 	%rd2505, %rd321;
	mul.wide.s32 	%rd2506, %r791, %r38;
	add.s64 	%rd2507, %rd2505, %rd2506;
	ld.global.f64 	%fd5608, [%rd2507];
	add.f64 	%fd9640, %fd5608, 0d0000000000000000;
	ld.global.f64 	%fd5609, [%rd2507+8];
	add.f64 	%fd9641, %fd5609, 0d0000000000000000;
	ld.global.f64 	%fd5610, [%rd2507+16];
	add.f64 	%fd9642, %fd5610, 0d0000000000000000;
	ld.global.f64 	%fd5611, [%rd2507+24];
	add.f64 	%fd9643, %fd5611, 0d0000000000000000;
	ld.global.f64 	%fd5612, [%rd2507+32];
	add.f64 	%fd9644, %fd5612, 0d0000000000000000;
	ld.global.f64 	%fd5613, [%rd2507+40];
	add.f64 	%fd9645, %fd5613, 0d0000000000000000;
	ld.global.f64 	%fd5614, [%rd2507+48];
	add.f64 	%fd9646, %fd5614, 0d0000000000000000;
	ld.global.f64 	%fd5615, [%rd2507+56];
	add.f64 	%fd9647, %fd5615, 0d0000000000000000;
	ld.global.f64 	%fd5616, [%rd2507+64];
	add.f64 	%fd9648, %fd5616, 0d0000000000000000;

$L__BB13_543:
	shr.u32 	%r2559, %r3, 27;
	cvt.u16.u32 	%rs362, %r2559;
	and.b16  	%rs361, %rs362, 1;
	add.f64 	%fd1864, %fd9648, 0d0000000000000000;
	add.f64 	%fd1865, %fd9647, 0d0000000000000000;
	add.f64 	%fd1866, %fd9646, 0d0000000000000000;
	add.f64 	%fd1867, %fd9645, 0d0000000000000000;
	add.f64 	%fd1868, %fd9644, 0d0000000000000000;
	add.f64 	%fd1869, %fd9643, 0d0000000000000000;
	add.f64 	%fd1870, %fd9642, 0d0000000000000000;
	add.f64 	%fd1871, %fd9641, 0d0000000000000000;
	add.f64 	%fd1872, %fd9640, 0d0000000000000000;
	ld.param.u64 	%rd322, [%rd28];
	ld.param.u32 	%r793, [%rd28+32];
	ld.param.u32 	%r794, [%rd28+60];
	setp.le.s32 	%p858, %r794, %r4;
	selp.u16 	%rs229, 1, 0, %p858;
	or.b16  	%rs232, %rs361, %rs229;
	setp.eq.s16 	%p859, %rs232, 0;
	@%p859 bra 	$L__BB13_545;

	st.local.v2.u32 	[%rd30], {%r4, %r794};
	mov.u64 	%rd2508, $str$1;
	cvta.global.u64 	%rd2509, %rd2508;
	{ // callseq 481, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2509;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1852, [retval0+0];
	} // callseq 481
	bra.uni 	$L__BB13_546;

$L__BB13_545:
	mul.wide.s32 	%rd2520, %r793, %r4;
	add.s64 	%rd2511, %rd322, %rd2520;
	// begin inline asm
	{ atom.add.f64 %fd5617,[%rd2511],%fd576; }

	// end inline asm
	add.s64 	%rd2512, %rd2511, 8;
	// begin inline asm
	{ atom.add.f64 %fd5619,[%rd2512],%fd575; }

	// end inline asm
	add.s64 	%rd2513, %rd2511, 16;
	// begin inline asm
	{ atom.add.f64 %fd5621,[%rd2513],%fd574; }

	// end inline asm
	add.s64 	%rd2514, %rd2511, 24;
	// begin inline asm
	{ atom.add.f64 %fd5623,[%rd2514],%fd552; }

	// end inline asm
	add.s64 	%rd2515, %rd2511, 32;
	// begin inline asm
	{ atom.add.f64 %fd5625,[%rd2515],%fd551; }

	// end inline asm
	add.s64 	%rd2516, %rd2511, 40;
	// begin inline asm
	{ atom.add.f64 %fd5627,[%rd2516],%fd550; }

	// end inline asm
	add.s64 	%rd2517, %rd2511, 48;
	// begin inline asm
	{ atom.add.f64 %fd5629,[%rd2517],%fd528; }

	// end inline asm
	add.s64 	%rd2518, %rd2511, 56;
	// begin inline asm
	{ atom.add.f64 %fd5631,[%rd2518],%fd527; }

	// end inline asm
	add.s64 	%rd2519, %rd2511, 64;
	// begin inline asm
	{ atom.add.f64 %fd5633,[%rd2519],%fd526; }

	// end inline asm

$L__BB13_546:
	add.s32 	%r2561, %r4, 1;
	shr.u32 	%r2560, %r2561, 31;
	cvt.u16.u32 	%rs363, %r2560;
	ld.param.u64 	%rd323, [%rd28];
	ld.param.u32 	%r795, [%rd28+32];
	ld.param.u32 	%r796, [%rd28+60];
	setp.le.s32 	%p860, %r796, %r2561;
	selp.u16 	%rs233, 1, 0, %p860;
	or.b16  	%rs235, %rs233, %rs363;
	setp.eq.s16 	%p861, %rs235, 0;
	@%p861 bra 	$L__BB13_548;

	add.s32 	%r2343, %r4, 1;
	st.local.v2.u32 	[%rd30], {%r2343, %r796};
	mov.u64 	%rd2521, $str$1;
	cvta.global.u64 	%rd2522, %rd2521;
	{ // callseq 482, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2522;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1856, [retval0+0];
	} // callseq 482
	bra.uni 	$L__BB13_549;

$L__BB13_548:
	add.s32 	%r2708, %r4, 1;
	mul.wide.s32 	%rd2533, %r795, %r2708;
	add.s64 	%rd2524, %rd323, %rd2533;
	// begin inline asm
	{ atom.add.f64 %fd5635,[%rd2524],%fd573; }

	// end inline asm
	add.s64 	%rd2525, %rd2524, 8;
	// begin inline asm
	{ atom.add.f64 %fd5637,[%rd2525],%fd572; }

	// end inline asm
	add.s64 	%rd2526, %rd2524, 16;
	// begin inline asm
	{ atom.add.f64 %fd5639,[%rd2526],%fd571; }

	// end inline asm
	add.s64 	%rd2527, %rd2524, 24;
	// begin inline asm
	{ atom.add.f64 %fd5641,[%rd2527],%fd549; }

	// end inline asm
	add.s64 	%rd2528, %rd2524, 32;
	// begin inline asm
	{ atom.add.f64 %fd5643,[%rd2528],%fd548; }

	// end inline asm
	add.s64 	%rd2529, %rd2524, 40;
	// begin inline asm
	{ atom.add.f64 %fd5645,[%rd2529],%fd547; }

	// end inline asm
	add.s64 	%rd2530, %rd2524, 48;
	// begin inline asm
	{ atom.add.f64 %fd5647,[%rd2530],%fd525; }

	// end inline asm
	add.s64 	%rd2531, %rd2524, 56;
	// begin inline asm
	{ atom.add.f64 %fd5649,[%rd2531],%fd524; }

	// end inline asm
	add.s64 	%rd2532, %rd2524, 64;
	// begin inline asm
	{ atom.add.f64 %fd5651,[%rd2532],%fd523; }

	// end inline asm

$L__BB13_549:
	add.s32 	%r2563, %r4, 2;
	shr.u32 	%r2562, %r2563, 31;
	cvt.u16.u32 	%rs364, %r2562;
	ld.param.u64 	%rd324, [%rd28];
	ld.param.u32 	%r797, [%rd28+32];
	ld.param.u32 	%r798, [%rd28+60];
	setp.le.s32 	%p862, %r798, %r2563;
	selp.u16 	%rs236, 1, 0, %p862;
	or.b16  	%rs238, %rs236, %rs364;
	setp.eq.s16 	%p863, %rs238, 0;
	@%p863 bra 	$L__BB13_551;

	add.s32 	%r2344, %r4, 2;
	st.local.v2.u32 	[%rd30], {%r2344, %r798};
	mov.u64 	%rd2534, $str$1;
	cvta.global.u64 	%rd2535, %rd2534;
	{ // callseq 483, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2535;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1861, [retval0+0];
	} // callseq 483
	bra.uni 	$L__BB13_552;

$L__BB13_551:
	add.s32 	%r2707, %r4, 2;
	mul.wide.s32 	%rd2546, %r797, %r2707;
	add.s64 	%rd2537, %rd324, %rd2546;
	// begin inline asm
	{ atom.add.f64 %fd5653,[%rd2537],%fd570; }

	// end inline asm
	add.s64 	%rd2538, %rd2537, 8;
	// begin inline asm
	{ atom.add.f64 %fd5655,[%rd2538],%fd569; }

	// end inline asm
	add.s64 	%rd2539, %rd2537, 16;
	// begin inline asm
	{ atom.add.f64 %fd5657,[%rd2539],%fd568; }

	// end inline asm
	add.s64 	%rd2540, %rd2537, 24;
	// begin inline asm
	{ atom.add.f64 %fd5659,[%rd2540],%fd546; }

	// end inline asm
	add.s64 	%rd2541, %rd2537, 32;
	// begin inline asm
	{ atom.add.f64 %fd5661,[%rd2541],%fd545; }

	// end inline asm
	add.s64 	%rd2542, %rd2537, 40;
	// begin inline asm
	{ atom.add.f64 %fd5663,[%rd2542],%fd544; }

	// end inline asm
	add.s64 	%rd2543, %rd2537, 48;
	// begin inline asm
	{ atom.add.f64 %fd5665,[%rd2543],%fd522; }

	// end inline asm
	add.s64 	%rd2544, %rd2537, 56;
	// begin inline asm
	{ atom.add.f64 %fd5667,[%rd2544],%fd521; }

	// end inline asm
	add.s64 	%rd2545, %rd2537, 64;
	// begin inline asm
	{ atom.add.f64 %fd5669,[%rd2545],%fd520; }

	// end inline asm

$L__BB13_552:
	add.s32 	%r2565, %r4, 3;
	shr.u32 	%r2564, %r2565, 31;
	cvt.u16.u32 	%rs365, %r2564;
	ld.param.u64 	%rd325, [%rd28];
	ld.param.u32 	%r799, [%rd28+32];
	ld.param.u32 	%r800, [%rd28+60];
	setp.le.s32 	%p864, %r800, %r2565;
	selp.u16 	%rs239, 1, 0, %p864;
	or.b16  	%rs241, %rs239, %rs365;
	setp.eq.s16 	%p865, %rs241, 0;
	@%p865 bra 	$L__BB13_554;

	add.s32 	%r2345, %r4, 3;
	st.local.v2.u32 	[%rd30], {%r2345, %r800};
	mov.u64 	%rd2547, $str$1;
	cvta.global.u64 	%rd2548, %rd2547;
	{ // callseq 484, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2548;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1866, [retval0+0];
	} // callseq 484
	bra.uni 	$L__BB13_555;

$L__BB13_554:
	add.s32 	%r2706, %r4, 3;
	mul.wide.s32 	%rd2559, %r799, %r2706;
	add.s64 	%rd2550, %rd325, %rd2559;
	// begin inline asm
	{ atom.add.f64 %fd5671,[%rd2550],%fd567; }

	// end inline asm
	add.s64 	%rd2551, %rd2550, 8;
	// begin inline asm
	{ atom.add.f64 %fd5673,[%rd2551],%fd566; }

	// end inline asm
	add.s64 	%rd2552, %rd2550, 16;
	// begin inline asm
	{ atom.add.f64 %fd5675,[%rd2552],%fd565; }

	// end inline asm
	add.s64 	%rd2553, %rd2550, 24;
	// begin inline asm
	{ atom.add.f64 %fd5677,[%rd2553],%fd543; }

	// end inline asm
	add.s64 	%rd2554, %rd2550, 32;
	// begin inline asm
	{ atom.add.f64 %fd5679,[%rd2554],%fd542; }

	// end inline asm
	add.s64 	%rd2555, %rd2550, 40;
	// begin inline asm
	{ atom.add.f64 %fd5681,[%rd2555],%fd541; }

	// end inline asm
	add.s64 	%rd2556, %rd2550, 48;
	// begin inline asm
	{ atom.add.f64 %fd5683,[%rd2556],%fd519; }

	// end inline asm
	add.s64 	%rd2557, %rd2550, 56;
	// begin inline asm
	{ atom.add.f64 %fd5685,[%rd2557],%fd518; }

	// end inline asm
	add.s64 	%rd2558, %rd2550, 64;
	// begin inline asm
	{ atom.add.f64 %fd5687,[%rd2558],%fd517; }

	// end inline asm

$L__BB13_555:
	add.s32 	%r2567, %r4, 4;
	shr.u32 	%r2566, %r2567, 31;
	cvt.u16.u32 	%rs366, %r2566;
	ld.param.u64 	%rd326, [%rd28];
	ld.param.u32 	%r801, [%rd28+32];
	ld.param.u32 	%r802, [%rd28+60];
	setp.le.s32 	%p866, %r802, %r2567;
	selp.u16 	%rs242, 1, 0, %p866;
	or.b16  	%rs244, %rs242, %rs366;
	setp.eq.s16 	%p867, %rs244, 0;
	@%p867 bra 	$L__BB13_557;

	add.s32 	%r2346, %r4, 4;
	st.local.v2.u32 	[%rd30], {%r2346, %r802};
	mov.u64 	%rd2560, $str$1;
	cvta.global.u64 	%rd2561, %rd2560;
	{ // callseq 485, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2561;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1871, [retval0+0];
	} // callseq 485
	bra.uni 	$L__BB13_558;

$L__BB13_557:
	add.s32 	%r2705, %r4, 4;
	mul.wide.s32 	%rd2572, %r801, %r2705;
	add.s64 	%rd2563, %rd326, %rd2572;
	// begin inline asm
	{ atom.add.f64 %fd5689,[%rd2563],%fd504; }

	// end inline asm
	add.s64 	%rd2564, %rd2563, 8;
	// begin inline asm
	{ atom.add.f64 %fd5691,[%rd2564],%fd503; }

	// end inline asm
	add.s64 	%rd2565, %rd2563, 16;
	// begin inline asm
	{ atom.add.f64 %fd5693,[%rd2565],%fd502; }

	// end inline asm
	add.s64 	%rd2566, %rd2563, 24;
	// begin inline asm
	{ atom.add.f64 %fd5695,[%rd2566],%fd480; }

	// end inline asm
	add.s64 	%rd2567, %rd2563, 32;
	// begin inline asm
	{ atom.add.f64 %fd5697,[%rd2567],%fd479; }

	// end inline asm
	add.s64 	%rd2568, %rd2563, 40;
	// begin inline asm
	{ atom.add.f64 %fd5699,[%rd2568],%fd478; }

	// end inline asm
	add.s64 	%rd2569, %rd2563, 48;
	// begin inline asm
	{ atom.add.f64 %fd5701,[%rd2569],%fd456; }

	// end inline asm
	add.s64 	%rd2570, %rd2563, 56;
	// begin inline asm
	{ atom.add.f64 %fd5703,[%rd2570],%fd455; }

	// end inline asm
	add.s64 	%rd2571, %rd2563, 64;
	// begin inline asm
	{ atom.add.f64 %fd5705,[%rd2571],%fd454; }

	// end inline asm

$L__BB13_558:
	add.s32 	%r2569, %r4, 5;
	shr.u32 	%r2568, %r2569, 31;
	cvt.u16.u32 	%rs367, %r2568;
	ld.param.u64 	%rd327, [%rd28];
	ld.param.u32 	%r803, [%rd28+32];
	ld.param.u32 	%r804, [%rd28+60];
	setp.le.s32 	%p868, %r804, %r2569;
	selp.u16 	%rs245, 1, 0, %p868;
	or.b16  	%rs247, %rs245, %rs367;
	setp.eq.s16 	%p869, %rs247, 0;
	@%p869 bra 	$L__BB13_560;

	add.s32 	%r2347, %r4, 5;
	st.local.v2.u32 	[%rd30], {%r2347, %r804};
	mov.u64 	%rd2573, $str$1;
	cvta.global.u64 	%rd2574, %rd2573;
	{ // callseq 486, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2574;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1876, [retval0+0];
	} // callseq 486
	bra.uni 	$L__BB13_561;

$L__BB13_560:
	add.s32 	%r2704, %r4, 5;
	mul.wide.s32 	%rd2585, %r803, %r2704;
	add.s64 	%rd2576, %rd327, %rd2585;
	// begin inline asm
	{ atom.add.f64 %fd5707,[%rd2576],%fd501; }

	// end inline asm
	add.s64 	%rd2577, %rd2576, 8;
	// begin inline asm
	{ atom.add.f64 %fd5709,[%rd2577],%fd500; }

	// end inline asm
	add.s64 	%rd2578, %rd2576, 16;
	// begin inline asm
	{ atom.add.f64 %fd5711,[%rd2578],%fd499; }

	// end inline asm
	add.s64 	%rd2579, %rd2576, 24;
	// begin inline asm
	{ atom.add.f64 %fd5713,[%rd2579],%fd477; }

	// end inline asm
	add.s64 	%rd2580, %rd2576, 32;
	// begin inline asm
	{ atom.add.f64 %fd5715,[%rd2580],%fd476; }

	// end inline asm
	add.s64 	%rd2581, %rd2576, 40;
	// begin inline asm
	{ atom.add.f64 %fd5717,[%rd2581],%fd475; }

	// end inline asm
	add.s64 	%rd2582, %rd2576, 48;
	// begin inline asm
	{ atom.add.f64 %fd5719,[%rd2582],%fd453; }

	// end inline asm
	add.s64 	%rd2583, %rd2576, 56;
	// begin inline asm
	{ atom.add.f64 %fd5721,[%rd2583],%fd452; }

	// end inline asm
	add.s64 	%rd2584, %rd2576, 64;
	// begin inline asm
	{ atom.add.f64 %fd5723,[%rd2584],%fd451; }

	// end inline asm

$L__BB13_561:
	add.s32 	%r2571, %r4, 6;
	shr.u32 	%r2570, %r2571, 31;
	cvt.u16.u32 	%rs368, %r2570;
	ld.param.u64 	%rd328, [%rd28];
	ld.param.u32 	%r805, [%rd28+32];
	ld.param.u32 	%r806, [%rd28+60];
	setp.le.s32 	%p870, %r806, %r2571;
	selp.u16 	%rs248, 1, 0, %p870;
	or.b16  	%rs250, %rs248, %rs368;
	setp.eq.s16 	%p871, %rs250, 0;
	@%p871 bra 	$L__BB13_563;

	add.s32 	%r2348, %r4, 6;
	st.local.v2.u32 	[%rd30], {%r2348, %r806};
	mov.u64 	%rd2586, $str$1;
	cvta.global.u64 	%rd2587, %rd2586;
	{ // callseq 487, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2587;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1881, [retval0+0];
	} // callseq 487
	bra.uni 	$L__BB13_564;

$L__BB13_563:
	add.s32 	%r2703, %r4, 6;
	mul.wide.s32 	%rd2598, %r805, %r2703;
	add.s64 	%rd2589, %rd328, %rd2598;
	// begin inline asm
	{ atom.add.f64 %fd5725,[%rd2589],%fd498; }

	// end inline asm
	add.s64 	%rd2590, %rd2589, 8;
	// begin inline asm
	{ atom.add.f64 %fd5727,[%rd2590],%fd497; }

	// end inline asm
	add.s64 	%rd2591, %rd2589, 16;
	// begin inline asm
	{ atom.add.f64 %fd5729,[%rd2591],%fd496; }

	// end inline asm
	add.s64 	%rd2592, %rd2589, 24;
	// begin inline asm
	{ atom.add.f64 %fd5731,[%rd2592],%fd474; }

	// end inline asm
	add.s64 	%rd2593, %rd2589, 32;
	// begin inline asm
	{ atom.add.f64 %fd5733,[%rd2593],%fd473; }

	// end inline asm
	add.s64 	%rd2594, %rd2589, 40;
	// begin inline asm
	{ atom.add.f64 %fd5735,[%rd2594],%fd472; }

	// end inline asm
	add.s64 	%rd2595, %rd2589, 48;
	// begin inline asm
	{ atom.add.f64 %fd5737,[%rd2595],%fd450; }

	// end inline asm
	add.s64 	%rd2596, %rd2589, 56;
	// begin inline asm
	{ atom.add.f64 %fd5739,[%rd2596],%fd449; }

	// end inline asm
	add.s64 	%rd2597, %rd2589, 64;
	// begin inline asm
	{ atom.add.f64 %fd5741,[%rd2597],%fd448; }

	// end inline asm

$L__BB13_564:
	add.s32 	%r2573, %r4, 7;
	shr.u32 	%r2572, %r2573, 31;
	cvt.u16.u32 	%rs369, %r2572;
	ld.param.u64 	%rd329, [%rd28];
	ld.param.u32 	%r807, [%rd28+32];
	ld.param.u32 	%r808, [%rd28+60];
	setp.le.s32 	%p872, %r808, %r2573;
	selp.u16 	%rs251, 1, 0, %p872;
	or.b16  	%rs253, %rs251, %rs369;
	setp.eq.s16 	%p873, %rs253, 0;
	@%p873 bra 	$L__BB13_566;

	add.s32 	%r2349, %r4, 7;
	st.local.v2.u32 	[%rd30], {%r2349, %r808};
	mov.u64 	%rd2599, $str$1;
	cvta.global.u64 	%rd2600, %rd2599;
	{ // callseq 488, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2600;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1886, [retval0+0];
	} // callseq 488
	bra.uni 	$L__BB13_567;

$L__BB13_566:
	add.s32 	%r2702, %r4, 7;
	mul.wide.s32 	%rd2611, %r807, %r2702;
	add.s64 	%rd2602, %rd329, %rd2611;
	// begin inline asm
	{ atom.add.f64 %fd5743,[%rd2602],%fd495; }

	// end inline asm
	add.s64 	%rd2603, %rd2602, 8;
	// begin inline asm
	{ atom.add.f64 %fd5745,[%rd2603],%fd494; }

	// end inline asm
	add.s64 	%rd2604, %rd2602, 16;
	// begin inline asm
	{ atom.add.f64 %fd5747,[%rd2604],%fd493; }

	// end inline asm
	add.s64 	%rd2605, %rd2602, 24;
	// begin inline asm
	{ atom.add.f64 %fd5749,[%rd2605],%fd471; }

	// end inline asm
	add.s64 	%rd2606, %rd2602, 32;
	// begin inline asm
	{ atom.add.f64 %fd5751,[%rd2606],%fd470; }

	// end inline asm
	add.s64 	%rd2607, %rd2602, 40;
	// begin inline asm
	{ atom.add.f64 %fd5753,[%rd2607],%fd469; }

	// end inline asm
	add.s64 	%rd2608, %rd2602, 48;
	// begin inline asm
	{ atom.add.f64 %fd5755,[%rd2608],%fd447; }

	// end inline asm
	add.s64 	%rd2609, %rd2602, 56;
	// begin inline asm
	{ atom.add.f64 %fd5757,[%rd2609],%fd446; }

	// end inline asm
	add.s64 	%rd2610, %rd2602, 64;
	// begin inline asm
	{ atom.add.f64 %fd5759,[%rd2610],%fd445; }

	// end inline asm

$L__BB13_567:
	add.s32 	%r2575, %r4, 8;
	shr.u32 	%r2574, %r2575, 31;
	cvt.u16.u32 	%rs370, %r2574;
	ld.param.u64 	%rd330, [%rd28];
	ld.param.u32 	%r809, [%rd28+32];
	ld.param.u32 	%r810, [%rd28+60];
	setp.le.s32 	%p874, %r810, %r2575;
	selp.u16 	%rs254, 1, 0, %p874;
	or.b16  	%rs256, %rs254, %rs370;
	setp.eq.s16 	%p875, %rs256, 0;
	@%p875 bra 	$L__BB13_569;

	add.s32 	%r2350, %r4, 8;
	st.local.v2.u32 	[%rd30], {%r2350, %r810};
	mov.u64 	%rd2612, $str$1;
	cvta.global.u64 	%rd2613, %rd2612;
	{ // callseq 489, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2613;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1891, [retval0+0];
	} // callseq 489
	bra.uni 	$L__BB13_570;

$L__BB13_569:
	add.s32 	%r2701, %r4, 8;
	mul.wide.s32 	%rd2624, %r809, %r2701;
	add.s64 	%rd2615, %rd330, %rd2624;
	// begin inline asm
	{ atom.add.f64 %fd5761,[%rd2615],%fd432; }

	// end inline asm
	add.s64 	%rd2616, %rd2615, 8;
	// begin inline asm
	{ atom.add.f64 %fd5763,[%rd2616],%fd431; }

	// end inline asm
	add.s64 	%rd2617, %rd2615, 16;
	// begin inline asm
	{ atom.add.f64 %fd5765,[%rd2617],%fd430; }

	// end inline asm
	add.s64 	%rd2618, %rd2615, 24;
	// begin inline asm
	{ atom.add.f64 %fd5767,[%rd2618],%fd408; }

	// end inline asm
	add.s64 	%rd2619, %rd2615, 32;
	// begin inline asm
	{ atom.add.f64 %fd5769,[%rd2619],%fd407; }

	// end inline asm
	add.s64 	%rd2620, %rd2615, 40;
	// begin inline asm
	{ atom.add.f64 %fd5771,[%rd2620],%fd406; }

	// end inline asm
	add.s64 	%rd2621, %rd2615, 48;
	// begin inline asm
	{ atom.add.f64 %fd5773,[%rd2621],%fd384; }

	// end inline asm
	add.s64 	%rd2622, %rd2615, 56;
	// begin inline asm
	{ atom.add.f64 %fd5775,[%rd2622],%fd383; }

	// end inline asm
	add.s64 	%rd2623, %rd2615, 64;
	// begin inline asm
	{ atom.add.f64 %fd5777,[%rd2623],%fd382; }

	// end inline asm

$L__BB13_570:
	add.s32 	%r2577, %r4, 9;
	shr.u32 	%r2576, %r2577, 31;
	cvt.u16.u32 	%rs371, %r2576;
	ld.param.u64 	%rd331, [%rd28];
	ld.param.u32 	%r811, [%rd28+32];
	ld.param.u32 	%r812, [%rd28+60];
	setp.le.s32 	%p876, %r812, %r2577;
	selp.u16 	%rs257, 1, 0, %p876;
	or.b16  	%rs259, %rs257, %rs371;
	setp.eq.s16 	%p877, %rs259, 0;
	@%p877 bra 	$L__BB13_572;

	add.s32 	%r2351, %r4, 9;
	st.local.v2.u32 	[%rd30], {%r2351, %r812};
	mov.u64 	%rd2625, $str$1;
	cvta.global.u64 	%rd2626, %rd2625;
	{ // callseq 490, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2626;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1896, [retval0+0];
	} // callseq 490
	bra.uni 	$L__BB13_573;

$L__BB13_572:
	add.s32 	%r2700, %r4, 9;
	mul.wide.s32 	%rd2637, %r811, %r2700;
	add.s64 	%rd2628, %rd331, %rd2637;
	// begin inline asm
	{ atom.add.f64 %fd5779,[%rd2628],%fd429; }

	// end inline asm
	add.s64 	%rd2629, %rd2628, 8;
	// begin inline asm
	{ atom.add.f64 %fd5781,[%rd2629],%fd428; }

	// end inline asm
	add.s64 	%rd2630, %rd2628, 16;
	// begin inline asm
	{ atom.add.f64 %fd5783,[%rd2630],%fd427; }

	// end inline asm
	add.s64 	%rd2631, %rd2628, 24;
	// begin inline asm
	{ atom.add.f64 %fd5785,[%rd2631],%fd405; }

	// end inline asm
	add.s64 	%rd2632, %rd2628, 32;
	// begin inline asm
	{ atom.add.f64 %fd5787,[%rd2632],%fd404; }

	// end inline asm
	add.s64 	%rd2633, %rd2628, 40;
	// begin inline asm
	{ atom.add.f64 %fd5789,[%rd2633],%fd403; }

	// end inline asm
	add.s64 	%rd2634, %rd2628, 48;
	// begin inline asm
	{ atom.add.f64 %fd5791,[%rd2634],%fd381; }

	// end inline asm
	add.s64 	%rd2635, %rd2628, 56;
	// begin inline asm
	{ atom.add.f64 %fd5793,[%rd2635],%fd380; }

	// end inline asm
	add.s64 	%rd2636, %rd2628, 64;
	// begin inline asm
	{ atom.add.f64 %fd5795,[%rd2636],%fd379; }

	// end inline asm

$L__BB13_573:
	add.s32 	%r2579, %r4, 10;
	shr.u32 	%r2578, %r2579, 31;
	cvt.u16.u32 	%rs372, %r2578;
	ld.param.u64 	%rd332, [%rd28];
	ld.param.u32 	%r813, [%rd28+32];
	ld.param.u32 	%r814, [%rd28+60];
	setp.le.s32 	%p878, %r814, %r2579;
	selp.u16 	%rs260, 1, 0, %p878;
	or.b16  	%rs262, %rs260, %rs372;
	setp.eq.s16 	%p879, %rs262, 0;
	@%p879 bra 	$L__BB13_575;

	add.s32 	%r2352, %r4, 10;
	st.local.v2.u32 	[%rd30], {%r2352, %r814};
	mov.u64 	%rd2638, $str$1;
	cvta.global.u64 	%rd2639, %rd2638;
	{ // callseq 491, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2639;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1901, [retval0+0];
	} // callseq 491
	bra.uni 	$L__BB13_576;

$L__BB13_575:
	add.s32 	%r2699, %r4, 10;
	mul.wide.s32 	%rd2650, %r813, %r2699;
	add.s64 	%rd2641, %rd332, %rd2650;
	// begin inline asm
	{ atom.add.f64 %fd5797,[%rd2641],%fd426; }

	// end inline asm
	add.s64 	%rd2642, %rd2641, 8;
	// begin inline asm
	{ atom.add.f64 %fd5799,[%rd2642],%fd425; }

	// end inline asm
	add.s64 	%rd2643, %rd2641, 16;
	// begin inline asm
	{ atom.add.f64 %fd5801,[%rd2643],%fd424; }

	// end inline asm
	add.s64 	%rd2644, %rd2641, 24;
	// begin inline asm
	{ atom.add.f64 %fd5803,[%rd2644],%fd402; }

	// end inline asm
	add.s64 	%rd2645, %rd2641, 32;
	// begin inline asm
	{ atom.add.f64 %fd5805,[%rd2645],%fd401; }

	// end inline asm
	add.s64 	%rd2646, %rd2641, 40;
	// begin inline asm
	{ atom.add.f64 %fd5807,[%rd2646],%fd400; }

	// end inline asm
	add.s64 	%rd2647, %rd2641, 48;
	// begin inline asm
	{ atom.add.f64 %fd5809,[%rd2647],%fd378; }

	// end inline asm
	add.s64 	%rd2648, %rd2641, 56;
	// begin inline asm
	{ atom.add.f64 %fd5811,[%rd2648],%fd377; }

	// end inline asm
	add.s64 	%rd2649, %rd2641, 64;
	// begin inline asm
	{ atom.add.f64 %fd5813,[%rd2649],%fd376; }

	// end inline asm

$L__BB13_576:
	add.s32 	%r2581, %r4, 11;
	shr.u32 	%r2580, %r2581, 31;
	cvt.u16.u32 	%rs373, %r2580;
	ld.param.u64 	%rd333, [%rd28];
	ld.param.u32 	%r815, [%rd28+32];
	ld.param.u32 	%r816, [%rd28+60];
	setp.le.s32 	%p880, %r816, %r2581;
	selp.u16 	%rs263, 1, 0, %p880;
	or.b16  	%rs265, %rs263, %rs373;
	setp.eq.s16 	%p881, %rs265, 0;
	@%p881 bra 	$L__BB13_578;

	add.s32 	%r2353, %r4, 11;
	st.local.v2.u32 	[%rd30], {%r2353, %r816};
	mov.u64 	%rd2651, $str$1;
	cvta.global.u64 	%rd2652, %rd2651;
	{ // callseq 492, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2652;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1906, [retval0+0];
	} // callseq 492
	bra.uni 	$L__BB13_579;

$L__BB13_578:
	add.s32 	%r2698, %r4, 11;
	mul.wide.s32 	%rd2663, %r815, %r2698;
	add.s64 	%rd2654, %rd333, %rd2663;
	// begin inline asm
	{ atom.add.f64 %fd5815,[%rd2654],%fd423; }

	// end inline asm
	add.s64 	%rd2655, %rd2654, 8;
	// begin inline asm
	{ atom.add.f64 %fd5817,[%rd2655],%fd422; }

	// end inline asm
	add.s64 	%rd2656, %rd2654, 16;
	// begin inline asm
	{ atom.add.f64 %fd5819,[%rd2656],%fd421; }

	// end inline asm
	add.s64 	%rd2657, %rd2654, 24;
	// begin inline asm
	{ atom.add.f64 %fd5821,[%rd2657],%fd399; }

	// end inline asm
	add.s64 	%rd2658, %rd2654, 32;
	// begin inline asm
	{ atom.add.f64 %fd5823,[%rd2658],%fd398; }

	// end inline asm
	add.s64 	%rd2659, %rd2654, 40;
	// begin inline asm
	{ atom.add.f64 %fd5825,[%rd2659],%fd397; }

	// end inline asm
	add.s64 	%rd2660, %rd2654, 48;
	// begin inline asm
	{ atom.add.f64 %fd5827,[%rd2660],%fd375; }

	// end inline asm
	add.s64 	%rd2661, %rd2654, 56;
	// begin inline asm
	{ atom.add.f64 %fd5829,[%rd2661],%fd374; }

	// end inline asm
	add.s64 	%rd2662, %rd2654, 64;
	// begin inline asm
	{ atom.add.f64 %fd5831,[%rd2662],%fd373; }

	// end inline asm

$L__BB13_579:
	add.s32 	%r2583, %r4, 12;
	shr.u32 	%r2582, %r2583, 31;
	cvt.u16.u32 	%rs374, %r2582;
	ld.param.u64 	%rd334, [%rd28];
	ld.param.u32 	%r817, [%rd28+32];
	ld.param.u32 	%r818, [%rd28+60];
	setp.le.s32 	%p882, %r818, %r2583;
	selp.u16 	%rs266, 1, 0, %p882;
	or.b16  	%rs268, %rs266, %rs374;
	setp.eq.s16 	%p883, %rs268, 0;
	@%p883 bra 	$L__BB13_581;

	add.s32 	%r2354, %r4, 12;
	st.local.v2.u32 	[%rd30], {%r2354, %r818};
	mov.u64 	%rd2664, $str$1;
	cvta.global.u64 	%rd2665, %rd2664;
	{ // callseq 493, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2665;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1911, [retval0+0];
	} // callseq 493
	bra.uni 	$L__BB13_582;

$L__BB13_581:
	add.s32 	%r2697, %r4, 12;
	mul.wide.s32 	%rd2676, %r817, %r2697;
	add.s64 	%rd2667, %rd334, %rd2676;
	// begin inline asm
	{ atom.add.f64 %fd5833,[%rd2667],%fd360; }

	// end inline asm
	add.s64 	%rd2668, %rd2667, 8;
	// begin inline asm
	{ atom.add.f64 %fd5835,[%rd2668],%fd359; }

	// end inline asm
	add.s64 	%rd2669, %rd2667, 16;
	// begin inline asm
	{ atom.add.f64 %fd5837,[%rd2669],%fd358; }

	// end inline asm
	add.s64 	%rd2670, %rd2667, 24;
	// begin inline asm
	{ atom.add.f64 %fd5839,[%rd2670],%fd336; }

	// end inline asm
	add.s64 	%rd2671, %rd2667, 32;
	// begin inline asm
	{ atom.add.f64 %fd5841,[%rd2671],%fd335; }

	// end inline asm
	add.s64 	%rd2672, %rd2667, 40;
	// begin inline asm
	{ atom.add.f64 %fd5843,[%rd2672],%fd334; }

	// end inline asm
	add.s64 	%rd2673, %rd2667, 48;
	// begin inline asm
	{ atom.add.f64 %fd5845,[%rd2673],%fd312; }

	// end inline asm
	add.s64 	%rd2674, %rd2667, 56;
	// begin inline asm
	{ atom.add.f64 %fd5847,[%rd2674],%fd311; }

	// end inline asm
	add.s64 	%rd2675, %rd2667, 64;
	// begin inline asm
	{ atom.add.f64 %fd5849,[%rd2675],%fd310; }

	// end inline asm

$L__BB13_582:
	add.s32 	%r2585, %r4, 13;
	shr.u32 	%r2584, %r2585, 31;
	cvt.u16.u32 	%rs375, %r2584;
	ld.param.u64 	%rd335, [%rd28];
	ld.param.u32 	%r819, [%rd28+32];
	ld.param.u32 	%r820, [%rd28+60];
	setp.le.s32 	%p884, %r820, %r2585;
	selp.u16 	%rs269, 1, 0, %p884;
	or.b16  	%rs271, %rs269, %rs375;
	setp.eq.s16 	%p885, %rs271, 0;
	@%p885 bra 	$L__BB13_584;

	add.s32 	%r2355, %r4, 13;
	st.local.v2.u32 	[%rd30], {%r2355, %r820};
	mov.u64 	%rd2677, $str$1;
	cvta.global.u64 	%rd2678, %rd2677;
	{ // callseq 494, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2678;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1916, [retval0+0];
	} // callseq 494
	bra.uni 	$L__BB13_585;

$L__BB13_584:
	add.s32 	%r2696, %r4, 13;
	mul.wide.s32 	%rd2689, %r819, %r2696;
	add.s64 	%rd2680, %rd335, %rd2689;
	// begin inline asm
	{ atom.add.f64 %fd5851,[%rd2680],%fd357; }

	// end inline asm
	add.s64 	%rd2681, %rd2680, 8;
	// begin inline asm
	{ atom.add.f64 %fd5853,[%rd2681],%fd356; }

	// end inline asm
	add.s64 	%rd2682, %rd2680, 16;
	// begin inline asm
	{ atom.add.f64 %fd5855,[%rd2682],%fd355; }

	// end inline asm
	add.s64 	%rd2683, %rd2680, 24;
	// begin inline asm
	{ atom.add.f64 %fd5857,[%rd2683],%fd333; }

	// end inline asm
	add.s64 	%rd2684, %rd2680, 32;
	// begin inline asm
	{ atom.add.f64 %fd5859,[%rd2684],%fd332; }

	// end inline asm
	add.s64 	%rd2685, %rd2680, 40;
	// begin inline asm
	{ atom.add.f64 %fd5861,[%rd2685],%fd331; }

	// end inline asm
	add.s64 	%rd2686, %rd2680, 48;
	// begin inline asm
	{ atom.add.f64 %fd5863,[%rd2686],%fd309; }

	// end inline asm
	add.s64 	%rd2687, %rd2680, 56;
	// begin inline asm
	{ atom.add.f64 %fd5865,[%rd2687],%fd308; }

	// end inline asm
	add.s64 	%rd2688, %rd2680, 64;
	// begin inline asm
	{ atom.add.f64 %fd5867,[%rd2688],%fd307; }

	// end inline asm

$L__BB13_585:
	add.s32 	%r2587, %r4, 14;
	shr.u32 	%r2586, %r2587, 31;
	cvt.u16.u32 	%rs376, %r2586;
	ld.param.u64 	%rd336, [%rd28];
	ld.param.u32 	%r821, [%rd28+32];
	ld.param.u32 	%r822, [%rd28+60];
	setp.le.s32 	%p886, %r822, %r2587;
	selp.u16 	%rs272, 1, 0, %p886;
	or.b16  	%rs274, %rs272, %rs376;
	setp.eq.s16 	%p887, %rs274, 0;
	@%p887 bra 	$L__BB13_587;

	add.s32 	%r2356, %r4, 14;
	st.local.v2.u32 	[%rd30], {%r2356, %r822};
	mov.u64 	%rd2690, $str$1;
	cvta.global.u64 	%rd2691, %rd2690;
	{ // callseq 495, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2691;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1921, [retval0+0];
	} // callseq 495
	bra.uni 	$L__BB13_588;

$L__BB13_587:
	add.s32 	%r2695, %r4, 14;
	mul.wide.s32 	%rd2702, %r821, %r2695;
	add.s64 	%rd2693, %rd336, %rd2702;
	// begin inline asm
	{ atom.add.f64 %fd5869,[%rd2693],%fd354; }

	// end inline asm
	add.s64 	%rd2694, %rd2693, 8;
	// begin inline asm
	{ atom.add.f64 %fd5871,[%rd2694],%fd353; }

	// end inline asm
	add.s64 	%rd2695, %rd2693, 16;
	// begin inline asm
	{ atom.add.f64 %fd5873,[%rd2695],%fd352; }

	// end inline asm
	add.s64 	%rd2696, %rd2693, 24;
	// begin inline asm
	{ atom.add.f64 %fd5875,[%rd2696],%fd330; }

	// end inline asm
	add.s64 	%rd2697, %rd2693, 32;
	// begin inline asm
	{ atom.add.f64 %fd5877,[%rd2697],%fd329; }

	// end inline asm
	add.s64 	%rd2698, %rd2693, 40;
	// begin inline asm
	{ atom.add.f64 %fd5879,[%rd2698],%fd328; }

	// end inline asm
	add.s64 	%rd2699, %rd2693, 48;
	// begin inline asm
	{ atom.add.f64 %fd5881,[%rd2699],%fd306; }

	// end inline asm
	add.s64 	%rd2700, %rd2693, 56;
	// begin inline asm
	{ atom.add.f64 %fd5883,[%rd2700],%fd305; }

	// end inline asm
	add.s64 	%rd2701, %rd2693, 64;
	// begin inline asm
	{ atom.add.f64 %fd5885,[%rd2701],%fd304; }

	// end inline asm

$L__BB13_588:
	add.s32 	%r2743, %r4, 15;
	shr.u32 	%r2742, %r2743, 31;
	cvt.u16.u32 	%rs393, %r2742;
	add.s32 	%r2588, %r4, 15;
	ld.param.u64 	%rd337, [%rd28];
	ld.param.u32 	%r823, [%rd28+32];
	ld.param.u32 	%r824, [%rd28+60];
	setp.le.s32 	%p888, %r824, %r2588;
	selp.u16 	%rs275, 1, 0, %p888;
	or.b16  	%rs277, %rs275, %rs393;
	setp.eq.s16 	%p889, %rs277, 0;
	@%p889 bra 	$L__BB13_590;

	add.s32 	%r2357, %r4, 15;
	st.local.v2.u32 	[%rd30], {%r2357, %r824};
	mov.u64 	%rd2703, $str$1;
	cvta.global.u64 	%rd2704, %rd2703;
	{ // callseq 496, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2704;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1926, [retval0+0];
	} // callseq 496
	bra.uni 	$L__BB13_591;

$L__BB13_590:
	add.s32 	%r2694, %r4, 15;
	mul.wide.s32 	%rd2715, %r823, %r2694;
	add.s64 	%rd2706, %rd337, %rd2715;
	// begin inline asm
	{ atom.add.f64 %fd5887,[%rd2706],%fd351; }

	// end inline asm
	add.s64 	%rd2707, %rd2706, 8;
	// begin inline asm
	{ atom.add.f64 %fd5889,[%rd2707],%fd350; }

	// end inline asm
	add.s64 	%rd2708, %rd2706, 16;
	// begin inline asm
	{ atom.add.f64 %fd5891,[%rd2708],%fd349; }

	// end inline asm
	add.s64 	%rd2709, %rd2706, 24;
	// begin inline asm
	{ atom.add.f64 %fd5893,[%rd2709],%fd327; }

	// end inline asm
	add.s64 	%rd2710, %rd2706, 32;
	// begin inline asm
	{ atom.add.f64 %fd5895,[%rd2710],%fd326; }

	// end inline asm
	add.s64 	%rd2711, %rd2706, 40;
	// begin inline asm
	{ atom.add.f64 %fd5897,[%rd2711],%fd325; }

	// end inline asm
	add.s64 	%rd2712, %rd2706, 48;
	// begin inline asm
	{ atom.add.f64 %fd5899,[%rd2712],%fd303; }

	// end inline asm
	add.s64 	%rd2713, %rd2706, 56;
	// begin inline asm
	{ atom.add.f64 %fd5901,[%rd2713],%fd302; }

	// end inline asm
	add.s64 	%rd2714, %rd2706, 64;
	// begin inline asm
	{ atom.add.f64 %fd5903,[%rd2714],%fd301; }

	// end inline asm

$L__BB13_591:
	add.s32 	%r2745, %r4, 15;
	shr.u32 	%r2744, %r2745, 31;
	cvt.u16.u32 	%rs394, %r2744;
	add.s32 	%r2589, %r4, 15;
	ld.param.u64 	%rd338, [%rd28+8];
	ld.param.u32 	%r825, [%rd28+32];
	ld.param.u32 	%r826, [%rd28+60];
	setp.le.s32 	%p890, %r826, %r2589;
	selp.u16 	%rs278, 1, 0, %p890;
	or.b16  	%rs280, %rs278, %rs394;
	setp.eq.s16 	%p891, %rs280, 0;
	mov.f64 	%fd9649, 0d0000000000000000;
	mov.f64 	%fd9650, 0d0000000000000000;
	mov.f64 	%fd9651, 0d0000000000000000;
	mov.f64 	%fd9652, 0d0000000000000000;
	mov.f64 	%fd9653, 0d0000000000000000;
	mov.f64 	%fd9654, 0d0000000000000000;
	mov.f64 	%fd9655, 0d0000000000000000;
	mov.f64 	%fd9656, 0d0000000000000000;
	mov.f64 	%fd9657, 0d0000000000000000;
	@%p891 bra 	$L__BB13_593;

	add.s32 	%r2358, %r4, 15;
	st.local.v2.u32 	[%rd30], {%r2358, %r826};
	mov.u64 	%rd2716, $str$1;
	cvta.global.u64 	%rd2717, %rd2716;
	{ // callseq 497, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2717;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1931, [retval0+0];
	} // callseq 497
	bra.uni 	$L__BB13_595;

$L__BB13_593:
	setp.eq.s64 	%p892, %rd338, 0;
	@%p892 bra 	$L__BB13_595;

	add.s32 	%r2693, %r4, 15;
	cvta.to.global.u64 	%rd2719, %rd338;
	mul.wide.s32 	%rd2720, %r825, %r2693;
	add.s64 	%rd2721, %rd2719, %rd2720;
	ld.global.f64 	%fd5923, [%rd2721];
	add.f64 	%fd9649, %fd5923, 0d0000000000000000;
	ld.global.f64 	%fd5924, [%rd2721+8];
	add.f64 	%fd9650, %fd5924, 0d0000000000000000;
	ld.global.f64 	%fd5925, [%rd2721+16];
	add.f64 	%fd9651, %fd5925, 0d0000000000000000;
	ld.global.f64 	%fd5926, [%rd2721+24];
	add.f64 	%fd9652, %fd5926, 0d0000000000000000;
	ld.global.f64 	%fd5927, [%rd2721+32];
	add.f64 	%fd9653, %fd5927, 0d0000000000000000;
	ld.global.f64 	%fd5928, [%rd2721+40];
	add.f64 	%fd9654, %fd5928, 0d0000000000000000;
	ld.global.f64 	%fd5929, [%rd2721+48];
	add.f64 	%fd9655, %fd5929, 0d0000000000000000;
	ld.global.f64 	%fd5930, [%rd2721+56];
	add.f64 	%fd9656, %fd5930, 0d0000000000000000;
	ld.global.f64 	%fd5931, [%rd2721+64];
	add.f64 	%fd9657, %fd5931, 0d0000000000000000;

$L__BB13_595:
	add.s32 	%r2591, %r4, 14;
	shr.u32 	%r2590, %r2591, 31;
	cvt.u16.u32 	%rs377, %r2590;
	add.f64 	%fd1891, %fd9657, 0d0000000000000000;
	add.f64 	%fd1892, %fd9656, 0d0000000000000000;
	add.f64 	%fd1893, %fd9655, 0d0000000000000000;
	add.f64 	%fd1894, %fd9654, 0d0000000000000000;
	add.f64 	%fd1895, %fd9653, 0d0000000000000000;
	add.f64 	%fd1896, %fd9652, 0d0000000000000000;
	add.f64 	%fd1897, %fd9651, 0d0000000000000000;
	add.f64 	%fd1898, %fd9650, 0d0000000000000000;
	add.f64 	%fd1899, %fd9649, 0d0000000000000000;
	ld.param.u64 	%rd339, [%rd28+8];
	ld.param.u32 	%r827, [%rd28+32];
	ld.param.u32 	%r828, [%rd28+60];
	setp.le.s32 	%p893, %r828, %r2591;
	selp.u16 	%rs281, 1, 0, %p893;
	or.b16  	%rs283, %rs281, %rs377;
	setp.eq.s16 	%p894, %rs283, 0;
	mov.f64 	%fd9658, 0d0000000000000000;
	mov.f64 	%fd9659, 0d0000000000000000;
	mov.f64 	%fd9660, 0d0000000000000000;
	mov.f64 	%fd9661, 0d0000000000000000;
	mov.f64 	%fd9662, 0d0000000000000000;
	mov.f64 	%fd9663, 0d0000000000000000;
	mov.f64 	%fd9664, 0d0000000000000000;
	mov.f64 	%fd9665, 0d0000000000000000;
	mov.f64 	%fd9666, 0d0000000000000000;
	@%p894 bra 	$L__BB13_597;

	add.s32 	%r2359, %r4, 14;
	st.local.v2.u32 	[%rd30], {%r2359, %r828};
	mov.u64 	%rd2722, $str$1;
	cvta.global.u64 	%rd2723, %rd2722;
	{ // callseq 498, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2723;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1936, [retval0+0];
	} // callseq 498
	bra.uni 	$L__BB13_599;

$L__BB13_597:
	setp.eq.s64 	%p895, %rd339, 0;
	@%p895 bra 	$L__BB13_599;

	add.s32 	%r2692, %r4, 14;
	cvta.to.global.u64 	%rd2725, %rd339;
	mul.wide.s32 	%rd2726, %r827, %r2692;
	add.s64 	%rd2727, %rd2725, %rd2726;
	ld.global.f64 	%fd5950, [%rd2727];
	add.f64 	%fd9658, %fd5950, 0d0000000000000000;
	ld.global.f64 	%fd5951, [%rd2727+8];
	add.f64 	%fd9659, %fd5951, 0d0000000000000000;
	ld.global.f64 	%fd5952, [%rd2727+16];
	add.f64 	%fd9660, %fd5952, 0d0000000000000000;
	ld.global.f64 	%fd5953, [%rd2727+24];
	add.f64 	%fd9661, %fd5953, 0d0000000000000000;
	ld.global.f64 	%fd5954, [%rd2727+32];
	add.f64 	%fd9662, %fd5954, 0d0000000000000000;
	ld.global.f64 	%fd5955, [%rd2727+40];
	add.f64 	%fd9663, %fd5955, 0d0000000000000000;
	ld.global.f64 	%fd5956, [%rd2727+48];
	add.f64 	%fd9664, %fd5956, 0d0000000000000000;
	ld.global.f64 	%fd5957, [%rd2727+56];
	add.f64 	%fd9665, %fd5957, 0d0000000000000000;
	ld.global.f64 	%fd5958, [%rd2727+64];
	add.f64 	%fd9666, %fd5958, 0d0000000000000000;

$L__BB13_599:
	add.s32 	%r2593, %r4, 13;
	shr.u32 	%r2592, %r2593, 31;
	cvt.u16.u32 	%rs378, %r2592;
	add.f64 	%fd1918, %fd9666, 0d0000000000000000;
	add.f64 	%fd1919, %fd9665, 0d0000000000000000;
	add.f64 	%fd1920, %fd9664, 0d0000000000000000;
	add.f64 	%fd1921, %fd9663, 0d0000000000000000;
	add.f64 	%fd1922, %fd9662, 0d0000000000000000;
	add.f64 	%fd1923, %fd9661, 0d0000000000000000;
	add.f64 	%fd1924, %fd9660, 0d0000000000000000;
	add.f64 	%fd1925, %fd9659, 0d0000000000000000;
	add.f64 	%fd1926, %fd9658, 0d0000000000000000;
	ld.param.u64 	%rd340, [%rd28+8];
	ld.param.u32 	%r829, [%rd28+32];
	ld.param.u32 	%r830, [%rd28+60];
	setp.le.s32 	%p896, %r830, %r2593;
	selp.u16 	%rs284, 1, 0, %p896;
	or.b16  	%rs286, %rs284, %rs378;
	setp.eq.s16 	%p897, %rs286, 0;
	mov.f64 	%fd9667, 0d0000000000000000;
	mov.f64 	%fd9668, 0d0000000000000000;
	mov.f64 	%fd9669, 0d0000000000000000;
	mov.f64 	%fd9670, 0d0000000000000000;
	mov.f64 	%fd9671, 0d0000000000000000;
	mov.f64 	%fd9672, 0d0000000000000000;
	mov.f64 	%fd9673, 0d0000000000000000;
	mov.f64 	%fd9674, 0d0000000000000000;
	mov.f64 	%fd9675, 0d0000000000000000;
	@%p897 bra 	$L__BB13_601;

	add.s32 	%r2360, %r4, 13;
	st.local.v2.u32 	[%rd30], {%r2360, %r830};
	mov.u64 	%rd2728, $str$1;
	cvta.global.u64 	%rd2729, %rd2728;
	{ // callseq 499, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2729;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1941, [retval0+0];
	} // callseq 499
	bra.uni 	$L__BB13_603;

$L__BB13_601:
	setp.eq.s64 	%p898, %rd340, 0;
	@%p898 bra 	$L__BB13_603;

	add.s32 	%r2691, %r4, 13;
	cvta.to.global.u64 	%rd2731, %rd340;
	mul.wide.s32 	%rd2732, %r829, %r2691;
	add.s64 	%rd2733, %rd2731, %rd2732;
	ld.global.f64 	%fd5977, [%rd2733];
	add.f64 	%fd9667, %fd5977, 0d0000000000000000;
	ld.global.f64 	%fd5978, [%rd2733+8];
	add.f64 	%fd9668, %fd5978, 0d0000000000000000;
	ld.global.f64 	%fd5979, [%rd2733+16];
	add.f64 	%fd9669, %fd5979, 0d0000000000000000;
	ld.global.f64 	%fd5980, [%rd2733+24];
	add.f64 	%fd9670, %fd5980, 0d0000000000000000;
	ld.global.f64 	%fd5981, [%rd2733+32];
	add.f64 	%fd9671, %fd5981, 0d0000000000000000;
	ld.global.f64 	%fd5982, [%rd2733+40];
	add.f64 	%fd9672, %fd5982, 0d0000000000000000;
	ld.global.f64 	%fd5983, [%rd2733+48];
	add.f64 	%fd9673, %fd5983, 0d0000000000000000;
	ld.global.f64 	%fd5984, [%rd2733+56];
	add.f64 	%fd9674, %fd5984, 0d0000000000000000;
	ld.global.f64 	%fd5985, [%rd2733+64];
	add.f64 	%fd9675, %fd5985, 0d0000000000000000;

$L__BB13_603:
	add.s32 	%r2595, %r4, 12;
	shr.u32 	%r2594, %r2595, 31;
	cvt.u16.u32 	%rs379, %r2594;
	add.f64 	%fd1945, %fd9675, 0d0000000000000000;
	add.f64 	%fd1946, %fd9674, 0d0000000000000000;
	add.f64 	%fd1947, %fd9673, 0d0000000000000000;
	add.f64 	%fd1948, %fd9672, 0d0000000000000000;
	add.f64 	%fd1949, %fd9671, 0d0000000000000000;
	add.f64 	%fd1950, %fd9670, 0d0000000000000000;
	add.f64 	%fd1951, %fd9669, 0d0000000000000000;
	add.f64 	%fd1952, %fd9668, 0d0000000000000000;
	add.f64 	%fd1953, %fd9667, 0d0000000000000000;
	ld.param.u64 	%rd341, [%rd28+8];
	ld.param.u32 	%r831, [%rd28+32];
	ld.param.u32 	%r832, [%rd28+60];
	setp.le.s32 	%p899, %r832, %r2595;
	selp.u16 	%rs287, 1, 0, %p899;
	or.b16  	%rs289, %rs287, %rs379;
	setp.eq.s16 	%p900, %rs289, 0;
	mov.f64 	%fd9676, 0d0000000000000000;
	mov.f64 	%fd9677, 0d0000000000000000;
	mov.f64 	%fd9678, 0d0000000000000000;
	mov.f64 	%fd9679, 0d0000000000000000;
	mov.f64 	%fd9680, 0d0000000000000000;
	mov.f64 	%fd9681, 0d0000000000000000;
	mov.f64 	%fd9682, 0d0000000000000000;
	mov.f64 	%fd9683, 0d0000000000000000;
	mov.f64 	%fd9684, 0d0000000000000000;
	@%p900 bra 	$L__BB13_605;

	add.s32 	%r2361, %r4, 12;
	st.local.v2.u32 	[%rd30], {%r2361, %r832};
	mov.u64 	%rd2734, $str$1;
	cvta.global.u64 	%rd2735, %rd2734;
	{ // callseq 500, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2735;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1946, [retval0+0];
	} // callseq 500
	bra.uni 	$L__BB13_607;

$L__BB13_605:
	setp.eq.s64 	%p901, %rd341, 0;
	@%p901 bra 	$L__BB13_607;

	add.s32 	%r2690, %r4, 12;
	cvta.to.global.u64 	%rd2737, %rd341;
	mul.wide.s32 	%rd2738, %r831, %r2690;
	add.s64 	%rd2739, %rd2737, %rd2738;
	ld.global.f64 	%fd6004, [%rd2739];
	add.f64 	%fd9676, %fd6004, 0d0000000000000000;
	ld.global.f64 	%fd6005, [%rd2739+8];
	add.f64 	%fd9677, %fd6005, 0d0000000000000000;
	ld.global.f64 	%fd6006, [%rd2739+16];
	add.f64 	%fd9678, %fd6006, 0d0000000000000000;
	ld.global.f64 	%fd6007, [%rd2739+24];
	add.f64 	%fd9679, %fd6007, 0d0000000000000000;
	ld.global.f64 	%fd6008, [%rd2739+32];
	add.f64 	%fd9680, %fd6008, 0d0000000000000000;
	ld.global.f64 	%fd6009, [%rd2739+40];
	add.f64 	%fd9681, %fd6009, 0d0000000000000000;
	ld.global.f64 	%fd6010, [%rd2739+48];
	add.f64 	%fd9682, %fd6010, 0d0000000000000000;
	ld.global.f64 	%fd6011, [%rd2739+56];
	add.f64 	%fd9683, %fd6011, 0d0000000000000000;
	ld.global.f64 	%fd6012, [%rd2739+64];
	add.f64 	%fd9684, %fd6012, 0d0000000000000000;

$L__BB13_607:
	add.s32 	%r2597, %r4, 11;
	shr.u32 	%r2596, %r2597, 31;
	cvt.u16.u32 	%rs380, %r2596;
	add.f64 	%fd1972, %fd9684, 0d0000000000000000;
	add.f64 	%fd1973, %fd9683, 0d0000000000000000;
	add.f64 	%fd1974, %fd9682, 0d0000000000000000;
	add.f64 	%fd1975, %fd9681, 0d0000000000000000;
	add.f64 	%fd1976, %fd9680, 0d0000000000000000;
	add.f64 	%fd1977, %fd9679, 0d0000000000000000;
	add.f64 	%fd1978, %fd9678, 0d0000000000000000;
	add.f64 	%fd1979, %fd9677, 0d0000000000000000;
	add.f64 	%fd1980, %fd9676, 0d0000000000000000;
	ld.param.u64 	%rd342, [%rd28+8];
	ld.param.u32 	%r833, [%rd28+32];
	ld.param.u32 	%r834, [%rd28+60];
	setp.le.s32 	%p902, %r834, %r2597;
	selp.u16 	%rs290, 1, 0, %p902;
	or.b16  	%rs292, %rs290, %rs380;
	setp.eq.s16 	%p903, %rs292, 0;
	mov.f64 	%fd9685, 0d0000000000000000;
	mov.f64 	%fd9686, 0d0000000000000000;
	mov.f64 	%fd9687, 0d0000000000000000;
	mov.f64 	%fd9688, 0d0000000000000000;
	mov.f64 	%fd9689, 0d0000000000000000;
	mov.f64 	%fd9690, 0d0000000000000000;
	mov.f64 	%fd9691, 0d0000000000000000;
	mov.f64 	%fd9692, 0d0000000000000000;
	mov.f64 	%fd9693, 0d0000000000000000;
	@%p903 bra 	$L__BB13_609;

	add.s32 	%r2362, %r4, 11;
	st.local.v2.u32 	[%rd30], {%r2362, %r834};
	mov.u64 	%rd2740, $str$1;
	cvta.global.u64 	%rd2741, %rd2740;
	{ // callseq 501, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2741;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1951, [retval0+0];
	} // callseq 501
	bra.uni 	$L__BB13_611;

$L__BB13_609:
	setp.eq.s64 	%p904, %rd342, 0;
	@%p904 bra 	$L__BB13_611;

	add.s32 	%r2689, %r4, 11;
	cvta.to.global.u64 	%rd2743, %rd342;
	mul.wide.s32 	%rd2744, %r833, %r2689;
	add.s64 	%rd2745, %rd2743, %rd2744;
	ld.global.f64 	%fd6031, [%rd2745];
	add.f64 	%fd9685, %fd6031, 0d0000000000000000;
	ld.global.f64 	%fd6032, [%rd2745+8];
	add.f64 	%fd9686, %fd6032, 0d0000000000000000;
	ld.global.f64 	%fd6033, [%rd2745+16];
	add.f64 	%fd9687, %fd6033, 0d0000000000000000;
	ld.global.f64 	%fd6034, [%rd2745+24];
	add.f64 	%fd9688, %fd6034, 0d0000000000000000;
	ld.global.f64 	%fd6035, [%rd2745+32];
	add.f64 	%fd9689, %fd6035, 0d0000000000000000;
	ld.global.f64 	%fd6036, [%rd2745+40];
	add.f64 	%fd9690, %fd6036, 0d0000000000000000;
	ld.global.f64 	%fd6037, [%rd2745+48];
	add.f64 	%fd9691, %fd6037, 0d0000000000000000;
	ld.global.f64 	%fd6038, [%rd2745+56];
	add.f64 	%fd9692, %fd6038, 0d0000000000000000;
	ld.global.f64 	%fd6039, [%rd2745+64];
	add.f64 	%fd9693, %fd6039, 0d0000000000000000;

$L__BB13_611:
	add.s32 	%r2599, %r4, 10;
	shr.u32 	%r2598, %r2599, 31;
	cvt.u16.u32 	%rs381, %r2598;
	add.f64 	%fd1999, %fd9693, 0d0000000000000000;
	add.f64 	%fd2000, %fd9692, 0d0000000000000000;
	add.f64 	%fd2001, %fd9691, 0d0000000000000000;
	add.f64 	%fd2002, %fd9690, 0d0000000000000000;
	add.f64 	%fd2003, %fd9689, 0d0000000000000000;
	add.f64 	%fd2004, %fd9688, 0d0000000000000000;
	add.f64 	%fd2005, %fd9687, 0d0000000000000000;
	add.f64 	%fd2006, %fd9686, 0d0000000000000000;
	add.f64 	%fd2007, %fd9685, 0d0000000000000000;
	ld.param.u64 	%rd343, [%rd28+8];
	ld.param.u32 	%r835, [%rd28+32];
	ld.param.u32 	%r836, [%rd28+60];
	setp.le.s32 	%p905, %r836, %r2599;
	selp.u16 	%rs293, 1, 0, %p905;
	or.b16  	%rs295, %rs293, %rs381;
	setp.eq.s16 	%p906, %rs295, 0;
	mov.f64 	%fd9694, 0d0000000000000000;
	mov.f64 	%fd9695, 0d0000000000000000;
	mov.f64 	%fd9696, 0d0000000000000000;
	mov.f64 	%fd9697, 0d0000000000000000;
	mov.f64 	%fd9698, 0d0000000000000000;
	mov.f64 	%fd9699, 0d0000000000000000;
	mov.f64 	%fd9700, 0d0000000000000000;
	mov.f64 	%fd9701, 0d0000000000000000;
	mov.f64 	%fd9702, 0d0000000000000000;
	@%p906 bra 	$L__BB13_613;

	add.s32 	%r2363, %r4, 10;
	st.local.v2.u32 	[%rd30], {%r2363, %r836};
	mov.u64 	%rd2746, $str$1;
	cvta.global.u64 	%rd2747, %rd2746;
	{ // callseq 502, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2747;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1956, [retval0+0];
	} // callseq 502
	bra.uni 	$L__BB13_615;

$L__BB13_613:
	setp.eq.s64 	%p907, %rd343, 0;
	@%p907 bra 	$L__BB13_615;

	add.s32 	%r2688, %r4, 10;
	cvta.to.global.u64 	%rd2749, %rd343;
	mul.wide.s32 	%rd2750, %r835, %r2688;
	add.s64 	%rd2751, %rd2749, %rd2750;
	ld.global.f64 	%fd6058, [%rd2751];
	add.f64 	%fd9694, %fd6058, 0d0000000000000000;
	ld.global.f64 	%fd6059, [%rd2751+8];
	add.f64 	%fd9695, %fd6059, 0d0000000000000000;
	ld.global.f64 	%fd6060, [%rd2751+16];
	add.f64 	%fd9696, %fd6060, 0d0000000000000000;
	ld.global.f64 	%fd6061, [%rd2751+24];
	add.f64 	%fd9697, %fd6061, 0d0000000000000000;
	ld.global.f64 	%fd6062, [%rd2751+32];
	add.f64 	%fd9698, %fd6062, 0d0000000000000000;
	ld.global.f64 	%fd6063, [%rd2751+40];
	add.f64 	%fd9699, %fd6063, 0d0000000000000000;
	ld.global.f64 	%fd6064, [%rd2751+48];
	add.f64 	%fd9700, %fd6064, 0d0000000000000000;
	ld.global.f64 	%fd6065, [%rd2751+56];
	add.f64 	%fd9701, %fd6065, 0d0000000000000000;
	ld.global.f64 	%fd6066, [%rd2751+64];
	add.f64 	%fd9702, %fd6066, 0d0000000000000000;

$L__BB13_615:
	add.s32 	%r2601, %r4, 9;
	shr.u32 	%r2600, %r2601, 31;
	cvt.u16.u32 	%rs382, %r2600;
	add.f64 	%fd2026, %fd9702, 0d0000000000000000;
	add.f64 	%fd2027, %fd9701, 0d0000000000000000;
	add.f64 	%fd2028, %fd9700, 0d0000000000000000;
	add.f64 	%fd2029, %fd9699, 0d0000000000000000;
	add.f64 	%fd2030, %fd9698, 0d0000000000000000;
	add.f64 	%fd2031, %fd9697, 0d0000000000000000;
	add.f64 	%fd2032, %fd9696, 0d0000000000000000;
	add.f64 	%fd2033, %fd9695, 0d0000000000000000;
	add.f64 	%fd2034, %fd9694, 0d0000000000000000;
	ld.param.u64 	%rd344, [%rd28+8];
	ld.param.u32 	%r837, [%rd28+32];
	ld.param.u32 	%r838, [%rd28+60];
	setp.le.s32 	%p908, %r838, %r2601;
	selp.u16 	%rs296, 1, 0, %p908;
	or.b16  	%rs298, %rs296, %rs382;
	setp.eq.s16 	%p909, %rs298, 0;
	mov.f64 	%fd9703, 0d0000000000000000;
	mov.f64 	%fd9704, 0d0000000000000000;
	mov.f64 	%fd9705, 0d0000000000000000;
	mov.f64 	%fd9706, 0d0000000000000000;
	mov.f64 	%fd9707, 0d0000000000000000;
	mov.f64 	%fd9708, 0d0000000000000000;
	mov.f64 	%fd9709, 0d0000000000000000;
	mov.f64 	%fd9710, 0d0000000000000000;
	mov.f64 	%fd9711, 0d0000000000000000;
	@%p909 bra 	$L__BB13_617;

	add.s32 	%r2364, %r4, 9;
	st.local.v2.u32 	[%rd30], {%r2364, %r838};
	mov.u64 	%rd2752, $str$1;
	cvta.global.u64 	%rd2753, %rd2752;
	{ // callseq 503, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2753;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1961, [retval0+0];
	} // callseq 503
	bra.uni 	$L__BB13_619;

$L__BB13_617:
	setp.eq.s64 	%p910, %rd344, 0;
	@%p910 bra 	$L__BB13_619;

	add.s32 	%r2687, %r4, 9;
	cvta.to.global.u64 	%rd2755, %rd344;
	mul.wide.s32 	%rd2756, %r837, %r2687;
	add.s64 	%rd2757, %rd2755, %rd2756;
	ld.global.f64 	%fd6085, [%rd2757];
	add.f64 	%fd9703, %fd6085, 0d0000000000000000;
	ld.global.f64 	%fd6086, [%rd2757+8];
	add.f64 	%fd9704, %fd6086, 0d0000000000000000;
	ld.global.f64 	%fd6087, [%rd2757+16];
	add.f64 	%fd9705, %fd6087, 0d0000000000000000;
	ld.global.f64 	%fd6088, [%rd2757+24];
	add.f64 	%fd9706, %fd6088, 0d0000000000000000;
	ld.global.f64 	%fd6089, [%rd2757+32];
	add.f64 	%fd9707, %fd6089, 0d0000000000000000;
	ld.global.f64 	%fd6090, [%rd2757+40];
	add.f64 	%fd9708, %fd6090, 0d0000000000000000;
	ld.global.f64 	%fd6091, [%rd2757+48];
	add.f64 	%fd9709, %fd6091, 0d0000000000000000;
	ld.global.f64 	%fd6092, [%rd2757+56];
	add.f64 	%fd9710, %fd6092, 0d0000000000000000;
	ld.global.f64 	%fd6093, [%rd2757+64];
	add.f64 	%fd9711, %fd6093, 0d0000000000000000;

$L__BB13_619:
	add.s32 	%r2603, %r4, 8;
	shr.u32 	%r2602, %r2603, 31;
	cvt.u16.u32 	%rs383, %r2602;
	add.f64 	%fd2053, %fd9711, 0d0000000000000000;
	add.f64 	%fd2054, %fd9710, 0d0000000000000000;
	add.f64 	%fd2055, %fd9709, 0d0000000000000000;
	add.f64 	%fd2056, %fd9708, 0d0000000000000000;
	add.f64 	%fd2057, %fd9707, 0d0000000000000000;
	add.f64 	%fd2058, %fd9706, 0d0000000000000000;
	add.f64 	%fd2059, %fd9705, 0d0000000000000000;
	add.f64 	%fd2060, %fd9704, 0d0000000000000000;
	add.f64 	%fd2061, %fd9703, 0d0000000000000000;
	ld.param.u64 	%rd345, [%rd28+8];
	ld.param.u32 	%r839, [%rd28+32];
	ld.param.u32 	%r840, [%rd28+60];
	setp.le.s32 	%p911, %r840, %r2603;
	selp.u16 	%rs299, 1, 0, %p911;
	or.b16  	%rs301, %rs299, %rs383;
	setp.eq.s16 	%p912, %rs301, 0;
	mov.f64 	%fd9712, 0d0000000000000000;
	mov.f64 	%fd9713, 0d0000000000000000;
	mov.f64 	%fd9714, 0d0000000000000000;
	mov.f64 	%fd9715, 0d0000000000000000;
	mov.f64 	%fd9716, 0d0000000000000000;
	mov.f64 	%fd9717, 0d0000000000000000;
	mov.f64 	%fd9718, 0d0000000000000000;
	mov.f64 	%fd9719, 0d0000000000000000;
	mov.f64 	%fd9720, 0d0000000000000000;
	@%p912 bra 	$L__BB13_621;

	add.s32 	%r2365, %r4, 8;
	st.local.v2.u32 	[%rd30], {%r2365, %r840};
	mov.u64 	%rd2758, $str$1;
	cvta.global.u64 	%rd2759, %rd2758;
	{ // callseq 504, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2759;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1966, [retval0+0];
	} // callseq 504
	bra.uni 	$L__BB13_623;

$L__BB13_621:
	setp.eq.s64 	%p913, %rd345, 0;
	@%p913 bra 	$L__BB13_623;

	add.s32 	%r2686, %r4, 8;
	cvta.to.global.u64 	%rd2761, %rd345;
	mul.wide.s32 	%rd2762, %r839, %r2686;
	add.s64 	%rd2763, %rd2761, %rd2762;
	ld.global.f64 	%fd6112, [%rd2763];
	add.f64 	%fd9712, %fd6112, 0d0000000000000000;
	ld.global.f64 	%fd6113, [%rd2763+8];
	add.f64 	%fd9713, %fd6113, 0d0000000000000000;
	ld.global.f64 	%fd6114, [%rd2763+16];
	add.f64 	%fd9714, %fd6114, 0d0000000000000000;
	ld.global.f64 	%fd6115, [%rd2763+24];
	add.f64 	%fd9715, %fd6115, 0d0000000000000000;
	ld.global.f64 	%fd6116, [%rd2763+32];
	add.f64 	%fd9716, %fd6116, 0d0000000000000000;
	ld.global.f64 	%fd6117, [%rd2763+40];
	add.f64 	%fd9717, %fd6117, 0d0000000000000000;
	ld.global.f64 	%fd6118, [%rd2763+48];
	add.f64 	%fd9718, %fd6118, 0d0000000000000000;
	ld.global.f64 	%fd6119, [%rd2763+56];
	add.f64 	%fd9719, %fd6119, 0d0000000000000000;
	ld.global.f64 	%fd6120, [%rd2763+64];
	add.f64 	%fd9720, %fd6120, 0d0000000000000000;

$L__BB13_623:
	add.s32 	%r2605, %r4, 7;
	shr.u32 	%r2604, %r2605, 31;
	cvt.u16.u32 	%rs384, %r2604;
	add.f64 	%fd2080, %fd9720, 0d0000000000000000;
	add.f64 	%fd2081, %fd9719, 0d0000000000000000;
	add.f64 	%fd2082, %fd9718, 0d0000000000000000;
	add.f64 	%fd2083, %fd9717, 0d0000000000000000;
	add.f64 	%fd2084, %fd9716, 0d0000000000000000;
	add.f64 	%fd2085, %fd9715, 0d0000000000000000;
	add.f64 	%fd2086, %fd9714, 0d0000000000000000;
	add.f64 	%fd2087, %fd9713, 0d0000000000000000;
	add.f64 	%fd2088, %fd9712, 0d0000000000000000;
	ld.param.u64 	%rd346, [%rd28+8];
	ld.param.u32 	%r841, [%rd28+32];
	ld.param.u32 	%r842, [%rd28+60];
	setp.le.s32 	%p914, %r842, %r2605;
	selp.u16 	%rs302, 1, 0, %p914;
	or.b16  	%rs304, %rs302, %rs384;
	setp.eq.s16 	%p915, %rs304, 0;
	mov.f64 	%fd9721, 0d0000000000000000;
	mov.f64 	%fd9722, 0d0000000000000000;
	mov.f64 	%fd9723, 0d0000000000000000;
	mov.f64 	%fd9724, 0d0000000000000000;
	mov.f64 	%fd9725, 0d0000000000000000;
	mov.f64 	%fd9726, 0d0000000000000000;
	mov.f64 	%fd9727, 0d0000000000000000;
	mov.f64 	%fd9728, 0d0000000000000000;
	mov.f64 	%fd9729, 0d0000000000000000;
	@%p915 bra 	$L__BB13_625;

	add.s32 	%r2366, %r4, 7;
	st.local.v2.u32 	[%rd30], {%r2366, %r842};
	mov.u64 	%rd2764, $str$1;
	cvta.global.u64 	%rd2765, %rd2764;
	{ // callseq 505, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2765;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1971, [retval0+0];
	} // callseq 505
	bra.uni 	$L__BB13_627;

$L__BB13_625:
	setp.eq.s64 	%p916, %rd346, 0;
	@%p916 bra 	$L__BB13_627;

	add.s32 	%r2685, %r4, 7;
	cvta.to.global.u64 	%rd2767, %rd346;
	mul.wide.s32 	%rd2768, %r841, %r2685;
	add.s64 	%rd2769, %rd2767, %rd2768;
	ld.global.f64 	%fd6139, [%rd2769];
	add.f64 	%fd9721, %fd6139, 0d0000000000000000;
	ld.global.f64 	%fd6140, [%rd2769+8];
	add.f64 	%fd9722, %fd6140, 0d0000000000000000;
	ld.global.f64 	%fd6141, [%rd2769+16];
	add.f64 	%fd9723, %fd6141, 0d0000000000000000;
	ld.global.f64 	%fd6142, [%rd2769+24];
	add.f64 	%fd9724, %fd6142, 0d0000000000000000;
	ld.global.f64 	%fd6143, [%rd2769+32];
	add.f64 	%fd9725, %fd6143, 0d0000000000000000;
	ld.global.f64 	%fd6144, [%rd2769+40];
	add.f64 	%fd9726, %fd6144, 0d0000000000000000;
	ld.global.f64 	%fd6145, [%rd2769+48];
	add.f64 	%fd9727, %fd6145, 0d0000000000000000;
	ld.global.f64 	%fd6146, [%rd2769+56];
	add.f64 	%fd9728, %fd6146, 0d0000000000000000;
	ld.global.f64 	%fd6147, [%rd2769+64];
	add.f64 	%fd9729, %fd6147, 0d0000000000000000;

$L__BB13_627:
	add.s32 	%r2607, %r4, 6;
	shr.u32 	%r2606, %r2607, 31;
	cvt.u16.u32 	%rs385, %r2606;
	add.f64 	%fd2107, %fd9729, 0d0000000000000000;
	add.f64 	%fd2108, %fd9728, 0d0000000000000000;
	add.f64 	%fd2109, %fd9727, 0d0000000000000000;
	add.f64 	%fd2110, %fd9726, 0d0000000000000000;
	add.f64 	%fd2111, %fd9725, 0d0000000000000000;
	add.f64 	%fd2112, %fd9724, 0d0000000000000000;
	add.f64 	%fd2113, %fd9723, 0d0000000000000000;
	add.f64 	%fd2114, %fd9722, 0d0000000000000000;
	add.f64 	%fd2115, %fd9721, 0d0000000000000000;
	ld.param.u64 	%rd347, [%rd28+8];
	ld.param.u32 	%r843, [%rd28+32];
	ld.param.u32 	%r844, [%rd28+60];
	setp.le.s32 	%p917, %r844, %r2607;
	selp.u16 	%rs305, 1, 0, %p917;
	or.b16  	%rs307, %rs305, %rs385;
	setp.eq.s16 	%p918, %rs307, 0;
	mov.f64 	%fd9730, 0d0000000000000000;
	mov.f64 	%fd9731, 0d0000000000000000;
	mov.f64 	%fd9732, 0d0000000000000000;
	mov.f64 	%fd9733, 0d0000000000000000;
	mov.f64 	%fd9734, 0d0000000000000000;
	mov.f64 	%fd9735, 0d0000000000000000;
	mov.f64 	%fd9736, 0d0000000000000000;
	mov.f64 	%fd9737, 0d0000000000000000;
	mov.f64 	%fd9738, 0d0000000000000000;
	@%p918 bra 	$L__BB13_629;

	add.s32 	%r2367, %r4, 6;
	st.local.v2.u32 	[%rd30], {%r2367, %r844};
	mov.u64 	%rd2770, $str$1;
	cvta.global.u64 	%rd2771, %rd2770;
	{ // callseq 506, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2771;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1976, [retval0+0];
	} // callseq 506
	bra.uni 	$L__BB13_631;

$L__BB13_629:
	setp.eq.s64 	%p919, %rd347, 0;
	@%p919 bra 	$L__BB13_631;

	add.s32 	%r2684, %r4, 6;
	cvta.to.global.u64 	%rd2773, %rd347;
	mul.wide.s32 	%rd2774, %r843, %r2684;
	add.s64 	%rd2775, %rd2773, %rd2774;
	ld.global.f64 	%fd6166, [%rd2775];
	add.f64 	%fd9730, %fd6166, 0d0000000000000000;
	ld.global.f64 	%fd6167, [%rd2775+8];
	add.f64 	%fd9731, %fd6167, 0d0000000000000000;
	ld.global.f64 	%fd6168, [%rd2775+16];
	add.f64 	%fd9732, %fd6168, 0d0000000000000000;
	ld.global.f64 	%fd6169, [%rd2775+24];
	add.f64 	%fd9733, %fd6169, 0d0000000000000000;
	ld.global.f64 	%fd6170, [%rd2775+32];
	add.f64 	%fd9734, %fd6170, 0d0000000000000000;
	ld.global.f64 	%fd6171, [%rd2775+40];
	add.f64 	%fd9735, %fd6171, 0d0000000000000000;
	ld.global.f64 	%fd6172, [%rd2775+48];
	add.f64 	%fd9736, %fd6172, 0d0000000000000000;
	ld.global.f64 	%fd6173, [%rd2775+56];
	add.f64 	%fd9737, %fd6173, 0d0000000000000000;
	ld.global.f64 	%fd6174, [%rd2775+64];
	add.f64 	%fd9738, %fd6174, 0d0000000000000000;

$L__BB13_631:
	add.s32 	%r2609, %r4, 5;
	shr.u32 	%r2608, %r2609, 31;
	cvt.u16.u32 	%rs386, %r2608;
	add.f64 	%fd2134, %fd9738, 0d0000000000000000;
	add.f64 	%fd2135, %fd9737, 0d0000000000000000;
	add.f64 	%fd2136, %fd9736, 0d0000000000000000;
	add.f64 	%fd2137, %fd9735, 0d0000000000000000;
	add.f64 	%fd2138, %fd9734, 0d0000000000000000;
	add.f64 	%fd2139, %fd9733, 0d0000000000000000;
	add.f64 	%fd2140, %fd9732, 0d0000000000000000;
	add.f64 	%fd2141, %fd9731, 0d0000000000000000;
	add.f64 	%fd2142, %fd9730, 0d0000000000000000;
	ld.param.u64 	%rd348, [%rd28+8];
	ld.param.u32 	%r845, [%rd28+32];
	ld.param.u32 	%r846, [%rd28+60];
	setp.le.s32 	%p920, %r846, %r2609;
	selp.u16 	%rs308, 1, 0, %p920;
	or.b16  	%rs310, %rs308, %rs386;
	setp.eq.s16 	%p921, %rs310, 0;
	mov.f64 	%fd9739, 0d0000000000000000;
	mov.f64 	%fd9740, 0d0000000000000000;
	mov.f64 	%fd9741, 0d0000000000000000;
	mov.f64 	%fd9742, 0d0000000000000000;
	mov.f64 	%fd9743, 0d0000000000000000;
	mov.f64 	%fd9744, 0d0000000000000000;
	mov.f64 	%fd9745, 0d0000000000000000;
	mov.f64 	%fd9746, 0d0000000000000000;
	mov.f64 	%fd9747, 0d0000000000000000;
	@%p921 bra 	$L__BB13_633;

	add.s32 	%r2368, %r4, 5;
	st.local.v2.u32 	[%rd30], {%r2368, %r846};
	mov.u64 	%rd2776, $str$1;
	cvta.global.u64 	%rd2777, %rd2776;
	{ // callseq 507, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2777;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1981, [retval0+0];
	} // callseq 507
	bra.uni 	$L__BB13_635;

$L__BB13_633:
	setp.eq.s64 	%p922, %rd348, 0;
	@%p922 bra 	$L__BB13_635;

	add.s32 	%r2683, %r4, 5;
	cvta.to.global.u64 	%rd2779, %rd348;
	mul.wide.s32 	%rd2780, %r845, %r2683;
	add.s64 	%rd2781, %rd2779, %rd2780;
	ld.global.f64 	%fd6193, [%rd2781];
	add.f64 	%fd9739, %fd6193, 0d0000000000000000;
	ld.global.f64 	%fd6194, [%rd2781+8];
	add.f64 	%fd9740, %fd6194, 0d0000000000000000;
	ld.global.f64 	%fd6195, [%rd2781+16];
	add.f64 	%fd9741, %fd6195, 0d0000000000000000;
	ld.global.f64 	%fd6196, [%rd2781+24];
	add.f64 	%fd9742, %fd6196, 0d0000000000000000;
	ld.global.f64 	%fd6197, [%rd2781+32];
	add.f64 	%fd9743, %fd6197, 0d0000000000000000;
	ld.global.f64 	%fd6198, [%rd2781+40];
	add.f64 	%fd9744, %fd6198, 0d0000000000000000;
	ld.global.f64 	%fd6199, [%rd2781+48];
	add.f64 	%fd9745, %fd6199, 0d0000000000000000;
	ld.global.f64 	%fd6200, [%rd2781+56];
	add.f64 	%fd9746, %fd6200, 0d0000000000000000;
	ld.global.f64 	%fd6201, [%rd2781+64];
	add.f64 	%fd9747, %fd6201, 0d0000000000000000;

$L__BB13_635:
	add.s32 	%r2611, %r4, 4;
	shr.u32 	%r2610, %r2611, 31;
	cvt.u16.u32 	%rs387, %r2610;
	add.f64 	%fd2161, %fd9747, 0d0000000000000000;
	add.f64 	%fd2162, %fd9746, 0d0000000000000000;
	add.f64 	%fd2163, %fd9745, 0d0000000000000000;
	add.f64 	%fd2164, %fd9744, 0d0000000000000000;
	add.f64 	%fd2165, %fd9743, 0d0000000000000000;
	add.f64 	%fd2166, %fd9742, 0d0000000000000000;
	add.f64 	%fd2167, %fd9741, 0d0000000000000000;
	add.f64 	%fd2168, %fd9740, 0d0000000000000000;
	add.f64 	%fd2169, %fd9739, 0d0000000000000000;
	ld.param.u64 	%rd349, [%rd28+8];
	ld.param.u32 	%r847, [%rd28+32];
	ld.param.u32 	%r848, [%rd28+60];
	setp.le.s32 	%p923, %r848, %r2611;
	selp.u16 	%rs311, 1, 0, %p923;
	or.b16  	%rs313, %rs311, %rs387;
	setp.eq.s16 	%p924, %rs313, 0;
	mov.f64 	%fd9748, 0d0000000000000000;
	mov.f64 	%fd9749, 0d0000000000000000;
	mov.f64 	%fd9750, 0d0000000000000000;
	mov.f64 	%fd9751, 0d0000000000000000;
	mov.f64 	%fd9752, 0d0000000000000000;
	mov.f64 	%fd9753, 0d0000000000000000;
	mov.f64 	%fd9754, 0d0000000000000000;
	mov.f64 	%fd9755, 0d0000000000000000;
	mov.f64 	%fd9756, 0d0000000000000000;
	@%p924 bra 	$L__BB13_637;

	add.s32 	%r2369, %r4, 4;
	st.local.v2.u32 	[%rd30], {%r2369, %r848};
	mov.u64 	%rd2782, $str$1;
	cvta.global.u64 	%rd2783, %rd2782;
	{ // callseq 508, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2783;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1986, [retval0+0];
	} // callseq 508
	bra.uni 	$L__BB13_639;

$L__BB13_637:
	setp.eq.s64 	%p925, %rd349, 0;
	@%p925 bra 	$L__BB13_639;

	add.s32 	%r2682, %r4, 4;
	cvta.to.global.u64 	%rd2785, %rd349;
	mul.wide.s32 	%rd2786, %r847, %r2682;
	add.s64 	%rd2787, %rd2785, %rd2786;
	ld.global.f64 	%fd6220, [%rd2787];
	add.f64 	%fd9748, %fd6220, 0d0000000000000000;
	ld.global.f64 	%fd6221, [%rd2787+8];
	add.f64 	%fd9749, %fd6221, 0d0000000000000000;
	ld.global.f64 	%fd6222, [%rd2787+16];
	add.f64 	%fd9750, %fd6222, 0d0000000000000000;
	ld.global.f64 	%fd6223, [%rd2787+24];
	add.f64 	%fd9751, %fd6223, 0d0000000000000000;
	ld.global.f64 	%fd6224, [%rd2787+32];
	add.f64 	%fd9752, %fd6224, 0d0000000000000000;
	ld.global.f64 	%fd6225, [%rd2787+40];
	add.f64 	%fd9753, %fd6225, 0d0000000000000000;
	ld.global.f64 	%fd6226, [%rd2787+48];
	add.f64 	%fd9754, %fd6226, 0d0000000000000000;
	ld.global.f64 	%fd6227, [%rd2787+56];
	add.f64 	%fd9755, %fd6227, 0d0000000000000000;
	ld.global.f64 	%fd6228, [%rd2787+64];
	add.f64 	%fd9756, %fd6228, 0d0000000000000000;

$L__BB13_639:
	add.s32 	%r2613, %r4, 3;
	shr.u32 	%r2612, %r2613, 31;
	cvt.u16.u32 	%rs388, %r2612;
	add.f64 	%fd2188, %fd9756, 0d0000000000000000;
	add.f64 	%fd2189, %fd9755, 0d0000000000000000;
	add.f64 	%fd2190, %fd9754, 0d0000000000000000;
	add.f64 	%fd2191, %fd9753, 0d0000000000000000;
	add.f64 	%fd2192, %fd9752, 0d0000000000000000;
	add.f64 	%fd2193, %fd9751, 0d0000000000000000;
	add.f64 	%fd2194, %fd9750, 0d0000000000000000;
	add.f64 	%fd2195, %fd9749, 0d0000000000000000;
	add.f64 	%fd2196, %fd9748, 0d0000000000000000;
	ld.param.u64 	%rd350, [%rd28+8];
	ld.param.u32 	%r849, [%rd28+32];
	ld.param.u32 	%r850, [%rd28+60];
	setp.le.s32 	%p926, %r850, %r2613;
	selp.u16 	%rs314, 1, 0, %p926;
	or.b16  	%rs316, %rs314, %rs388;
	setp.eq.s16 	%p927, %rs316, 0;
	mov.f64 	%fd9757, 0d0000000000000000;
	mov.f64 	%fd9758, 0d0000000000000000;
	mov.f64 	%fd9759, 0d0000000000000000;
	mov.f64 	%fd9760, 0d0000000000000000;
	mov.f64 	%fd9761, 0d0000000000000000;
	mov.f64 	%fd9762, 0d0000000000000000;
	mov.f64 	%fd9763, 0d0000000000000000;
	mov.f64 	%fd9764, 0d0000000000000000;
	mov.f64 	%fd9765, 0d0000000000000000;
	@%p927 bra 	$L__BB13_641;

	add.s32 	%r2370, %r4, 3;
	st.local.v2.u32 	[%rd30], {%r2370, %r850};
	mov.u64 	%rd2788, $str$1;
	cvta.global.u64 	%rd2789, %rd2788;
	{ // callseq 509, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2789;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1991, [retval0+0];
	} // callseq 509
	bra.uni 	$L__BB13_643;

$L__BB13_641:
	setp.eq.s64 	%p928, %rd350, 0;
	@%p928 bra 	$L__BB13_643;

	add.s32 	%r2681, %r4, 3;
	cvta.to.global.u64 	%rd2791, %rd350;
	mul.wide.s32 	%rd2792, %r849, %r2681;
	add.s64 	%rd2793, %rd2791, %rd2792;
	ld.global.f64 	%fd6247, [%rd2793];
	add.f64 	%fd9757, %fd6247, 0d0000000000000000;
	ld.global.f64 	%fd6248, [%rd2793+8];
	add.f64 	%fd9758, %fd6248, 0d0000000000000000;
	ld.global.f64 	%fd6249, [%rd2793+16];
	add.f64 	%fd9759, %fd6249, 0d0000000000000000;
	ld.global.f64 	%fd6250, [%rd2793+24];
	add.f64 	%fd9760, %fd6250, 0d0000000000000000;
	ld.global.f64 	%fd6251, [%rd2793+32];
	add.f64 	%fd9761, %fd6251, 0d0000000000000000;
	ld.global.f64 	%fd6252, [%rd2793+40];
	add.f64 	%fd9762, %fd6252, 0d0000000000000000;
	ld.global.f64 	%fd6253, [%rd2793+48];
	add.f64 	%fd9763, %fd6253, 0d0000000000000000;
	ld.global.f64 	%fd6254, [%rd2793+56];
	add.f64 	%fd9764, %fd6254, 0d0000000000000000;
	ld.global.f64 	%fd6255, [%rd2793+64];
	add.f64 	%fd9765, %fd6255, 0d0000000000000000;

$L__BB13_643:
	add.s32 	%r2615, %r4, 2;
	shr.u32 	%r2614, %r2615, 31;
	cvt.u16.u32 	%rs389, %r2614;
	add.f64 	%fd2215, %fd9765, 0d0000000000000000;
	add.f64 	%fd2216, %fd9764, 0d0000000000000000;
	add.f64 	%fd2217, %fd9763, 0d0000000000000000;
	add.f64 	%fd2218, %fd9762, 0d0000000000000000;
	add.f64 	%fd2219, %fd9761, 0d0000000000000000;
	add.f64 	%fd2220, %fd9760, 0d0000000000000000;
	add.f64 	%fd2221, %fd9759, 0d0000000000000000;
	add.f64 	%fd2222, %fd9758, 0d0000000000000000;
	add.f64 	%fd2223, %fd9757, 0d0000000000000000;
	ld.param.u64 	%rd351, [%rd28+8];
	ld.param.u32 	%r851, [%rd28+32];
	ld.param.u32 	%r852, [%rd28+60];
	setp.le.s32 	%p929, %r852, %r2615;
	selp.u16 	%rs317, 1, 0, %p929;
	or.b16  	%rs319, %rs317, %rs389;
	setp.eq.s16 	%p930, %rs319, 0;
	mov.f64 	%fd9766, 0d0000000000000000;
	mov.f64 	%fd9767, 0d0000000000000000;
	mov.f64 	%fd9768, 0d0000000000000000;
	mov.f64 	%fd9769, 0d0000000000000000;
	mov.f64 	%fd9770, 0d0000000000000000;
	mov.f64 	%fd9771, 0d0000000000000000;
	mov.f64 	%fd9772, 0d0000000000000000;
	mov.f64 	%fd9773, 0d0000000000000000;
	mov.f64 	%fd9774, 0d0000000000000000;
	@%p930 bra 	$L__BB13_645;

	add.s32 	%r2371, %r4, 2;
	st.local.v2.u32 	[%rd30], {%r2371, %r852};
	mov.u64 	%rd2794, $str$1;
	cvta.global.u64 	%rd2795, %rd2794;
	{ // callseq 510, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2795;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r1996, [retval0+0];
	} // callseq 510
	bra.uni 	$L__BB13_647;

$L__BB13_645:
	setp.eq.s64 	%p931, %rd351, 0;
	@%p931 bra 	$L__BB13_647;

	add.s32 	%r2680, %r4, 2;
	cvta.to.global.u64 	%rd2797, %rd351;
	mul.wide.s32 	%rd2798, %r851, %r2680;
	add.s64 	%rd2799, %rd2797, %rd2798;
	ld.global.f64 	%fd6274, [%rd2799];
	add.f64 	%fd9766, %fd6274, 0d0000000000000000;
	ld.global.f64 	%fd6275, [%rd2799+8];
	add.f64 	%fd9767, %fd6275, 0d0000000000000000;
	ld.global.f64 	%fd6276, [%rd2799+16];
	add.f64 	%fd9768, %fd6276, 0d0000000000000000;
	ld.global.f64 	%fd6277, [%rd2799+24];
	add.f64 	%fd9769, %fd6277, 0d0000000000000000;
	ld.global.f64 	%fd6278, [%rd2799+32];
	add.f64 	%fd9770, %fd6278, 0d0000000000000000;
	ld.global.f64 	%fd6279, [%rd2799+40];
	add.f64 	%fd9771, %fd6279, 0d0000000000000000;
	ld.global.f64 	%fd6280, [%rd2799+48];
	add.f64 	%fd9772, %fd6280, 0d0000000000000000;
	ld.global.f64 	%fd6281, [%rd2799+56];
	add.f64 	%fd9773, %fd6281, 0d0000000000000000;
	ld.global.f64 	%fd6282, [%rd2799+64];
	add.f64 	%fd9774, %fd6282, 0d0000000000000000;

$L__BB13_647:
	add.s32 	%r2617, %r4, 1;
	shr.u32 	%r2616, %r2617, 31;
	cvt.u16.u32 	%rs390, %r2616;
	add.f64 	%fd2242, %fd9774, 0d0000000000000000;
	add.f64 	%fd2243, %fd9773, 0d0000000000000000;
	add.f64 	%fd2244, %fd9772, 0d0000000000000000;
	add.f64 	%fd2245, %fd9771, 0d0000000000000000;
	add.f64 	%fd2246, %fd9770, 0d0000000000000000;
	add.f64 	%fd2247, %fd9769, 0d0000000000000000;
	add.f64 	%fd2248, %fd9768, 0d0000000000000000;
	add.f64 	%fd2249, %fd9767, 0d0000000000000000;
	add.f64 	%fd2250, %fd9766, 0d0000000000000000;
	ld.param.u64 	%rd352, [%rd28+8];
	ld.param.u32 	%r853, [%rd28+32];
	ld.param.u32 	%r854, [%rd28+60];
	setp.le.s32 	%p932, %r854, %r2617;
	selp.u16 	%rs320, 1, 0, %p932;
	or.b16  	%rs322, %rs320, %rs390;
	setp.eq.s16 	%p933, %rs322, 0;
	mov.f64 	%fd9775, 0d0000000000000000;
	mov.f64 	%fd9776, 0d0000000000000000;
	mov.f64 	%fd9777, 0d0000000000000000;
	mov.f64 	%fd9778, 0d0000000000000000;
	mov.f64 	%fd9779, 0d0000000000000000;
	mov.f64 	%fd9780, 0d0000000000000000;
	mov.f64 	%fd9781, 0d0000000000000000;
	mov.f64 	%fd9782, 0d0000000000000000;
	mov.f64 	%fd9783, 0d0000000000000000;
	@%p933 bra 	$L__BB13_649;

	add.s32 	%r2372, %r4, 1;
	st.local.v2.u32 	[%rd30], {%r2372, %r854};
	mov.u64 	%rd2800, $str$1;
	cvta.global.u64 	%rd2801, %rd2800;
	{ // callseq 511, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2801;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r2001, [retval0+0];
	} // callseq 511
	bra.uni 	$L__BB13_651;

$L__BB13_649:
	setp.eq.s64 	%p934, %rd352, 0;
	@%p934 bra 	$L__BB13_651;

	add.s32 	%r2679, %r4, 1;
	cvta.to.global.u64 	%rd2803, %rd352;
	mul.wide.s32 	%rd2804, %r853, %r2679;
	add.s64 	%rd2805, %rd2803, %rd2804;
	ld.global.f64 	%fd6301, [%rd2805];
	add.f64 	%fd9775, %fd6301, 0d0000000000000000;
	ld.global.f64 	%fd6302, [%rd2805+8];
	add.f64 	%fd9776, %fd6302, 0d0000000000000000;
	ld.global.f64 	%fd6303, [%rd2805+16];
	add.f64 	%fd9777, %fd6303, 0d0000000000000000;
	ld.global.f64 	%fd6304, [%rd2805+24];
	add.f64 	%fd9778, %fd6304, 0d0000000000000000;
	ld.global.f64 	%fd6305, [%rd2805+32];
	add.f64 	%fd9779, %fd6305, 0d0000000000000000;
	ld.global.f64 	%fd6306, [%rd2805+40];
	add.f64 	%fd9780, %fd6306, 0d0000000000000000;
	ld.global.f64 	%fd6307, [%rd2805+48];
	add.f64 	%fd9781, %fd6307, 0d0000000000000000;
	ld.global.f64 	%fd6308, [%rd2805+56];
	add.f64 	%fd9782, %fd6308, 0d0000000000000000;
	ld.global.f64 	%fd6309, [%rd2805+64];
	add.f64 	%fd9783, %fd6309, 0d0000000000000000;

$L__BB13_651:
	shr.u32 	%r2618, %r3, 27;
	cvt.u16.u32 	%rs392, %r2618;
	and.b16  	%rs391, %rs392, 1;
	add.f64 	%fd2269, %fd9783, 0d0000000000000000;
	add.f64 	%fd2270, %fd9782, 0d0000000000000000;
	add.f64 	%fd2271, %fd9781, 0d0000000000000000;
	add.f64 	%fd2272, %fd9780, 0d0000000000000000;
	add.f64 	%fd2273, %fd9779, 0d0000000000000000;
	add.f64 	%fd2274, %fd9778, 0d0000000000000000;
	add.f64 	%fd2275, %fd9777, 0d0000000000000000;
	add.f64 	%fd2276, %fd9776, 0d0000000000000000;
	add.f64 	%fd2277, %fd9775, 0d0000000000000000;
	ld.param.u64 	%rd353, [%rd28+8];
	ld.param.u32 	%r855, [%rd28+32];
	ld.param.u32 	%r856, [%rd28+60];
	setp.le.s32 	%p935, %r856, %r4;
	selp.u16 	%rs323, 1, 0, %p935;
	or.b16  	%rs326, %rs391, %rs323;
	setp.eq.s16 	%p936, %rs326, 0;
	mov.f64 	%fd9784, 0d0000000000000000;
	mov.f64 	%fd9785, 0d0000000000000000;
	mov.f64 	%fd9786, 0d0000000000000000;
	mov.f64 	%fd9787, 0d0000000000000000;
	mov.f64 	%fd9788, 0d0000000000000000;
	mov.f64 	%fd9789, 0d0000000000000000;
	mov.f64 	%fd9790, 0d0000000000000000;
	mov.f64 	%fd9791, 0d0000000000000000;
	mov.f64 	%fd9792, 0d0000000000000000;
	@%p936 bra 	$L__BB13_653;

	st.local.v2.u32 	[%rd30], {%r4, %r856};
	mov.u64 	%rd2806, $str$1;
	cvta.global.u64 	%rd2807, %rd2806;
	{ // callseq 512, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2807;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd377;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r2004, [retval0+0];
	} // callseq 512
	bra.uni 	$L__BB13_655;

$L__BB13_653:
	setp.eq.s64 	%p937, %rd353, 0;
	@%p937 bra 	$L__BB13_655;

	cvta.to.global.u64 	%rd2809, %rd353;
	mul.wide.s32 	%rd2810, %r855, %r4;
	add.s64 	%rd2811, %rd2809, %rd2810;
	ld.global.f64 	%fd6328, [%rd2811];
	add.f64 	%fd9784, %fd6328, 0d0000000000000000;
	ld.global.f64 	%fd6329, [%rd2811+8];
	add.f64 	%fd9785, %fd6329, 0d0000000000000000;
	ld.global.f64 	%fd6330, [%rd2811+16];
	add.f64 	%fd9786, %fd6330, 0d0000000000000000;
	ld.global.f64 	%fd6331, [%rd2811+24];
	add.f64 	%fd9787, %fd6331, 0d0000000000000000;
	ld.global.f64 	%fd6332, [%rd2811+32];
	add.f64 	%fd9788, %fd6332, 0d0000000000000000;
	ld.global.f64 	%fd6333, [%rd2811+40];
	add.f64 	%fd9789, %fd6333, 0d0000000000000000;
	ld.global.f64 	%fd6334, [%rd2811+48];
	add.f64 	%fd9790, %fd6334, 0d0000000000000000;
	ld.global.f64 	%fd6335, [%rd2811+56];
	add.f64 	%fd9791, %fd6335, 0d0000000000000000;
	ld.global.f64 	%fd6336, [%rd2811+64];
	add.f64 	%fd9792, %fd6336, 0d0000000000000000;

$L__BB13_655:
	add.f64 	%fd2296, %fd9792, 0d0000000000000000;
	add.f64 	%fd2297, %fd9791, 0d0000000000000000;
	add.f64 	%fd2298, %fd9790, 0d0000000000000000;
	add.f64 	%fd2299, %fd9789, 0d0000000000000000;
	add.f64 	%fd2300, %fd9788, 0d0000000000000000;
	add.f64 	%fd2301, %fd9787, 0d0000000000000000;
	add.f64 	%fd2302, %fd9786, 0d0000000000000000;
	add.f64 	%fd2303, %fd9785, 0d0000000000000000;
	add.f64 	%fd2304, %fd9784, 0d0000000000000000;
	setp.eq.s64 	%p938, %rd364, 0;
	@%p938 bra 	$L__BB13_657;

	mul.lo.s64 	%rd3388, %rd25, %rd14;
	add.s64 	%rd2812, %rd364, %rd3388;
	// begin inline asm
	{ atom.add.f64 %fd6337,[%rd2812],%fd2304; }

	// end inline asm
	add.s64 	%rd2813, %rd2812, 8;
	// begin inline asm
	{ atom.add.f64 %fd6339,[%rd2813],%fd2303; }

	// end inline asm
	add.s64 	%rd2814, %rd2812, 16;
	// begin inline asm
	{ atom.add.f64 %fd6341,[%rd2814],%fd2302; }

	// end inline asm
	add.s64 	%rd2815, %rd2812, 24;
	// begin inline asm
	{ atom.add.f64 %fd6343,[%rd2815],%fd2277; }

	// end inline asm
	add.s64 	%rd2816, %rd2812, 32;
	// begin inline asm
	{ atom.add.f64 %fd6345,[%rd2816],%fd2276; }

	// end inline asm
	add.s64 	%rd2817, %rd2812, 40;
	// begin inline asm
	{ atom.add.f64 %fd6347,[%rd2817],%fd2275; }

	// end inline asm
	add.s64 	%rd2818, %rd2812, 48;
	// begin inline asm
	{ atom.add.f64 %fd6349,[%rd2818],%fd2250; }

	// end inline asm
	add.s64 	%rd2819, %rd2812, 56;
	// begin inline asm
	{ atom.add.f64 %fd6351,[%rd2819],%fd2249; }

	// end inline asm
	add.s64 	%rd2820, %rd2812, 64;
	// begin inline asm
	{ atom.add.f64 %fd6353,[%rd2820],%fd2248; }

	// end inline asm
	add.s64 	%rd2821, %rd2812, 72;
	// begin inline asm
	{ atom.add.f64 %fd6355,[%rd2821],%fd2223; }

	// end inline asm
	add.s64 	%rd2822, %rd2812, 80;
	// begin inline asm
	{ atom.add.f64 %fd6357,[%rd2822],%fd2222; }

	// end inline asm
	add.s64 	%rd2823, %rd2812, 88;
	// begin inline asm
	{ atom.add.f64 %fd6359,[%rd2823],%fd2221; }

	// end inline asm
	add.s64 	%rd2824, %rd2812, 96;
	// begin inline asm
	{ atom.add.f64 %fd6361,[%rd2824],%fd1440; }

	// end inline asm
	add.s64 	%rd2825, %rd2812, 104;
	// begin inline asm
	{ atom.add.f64 %fd6363,[%rd2825],%fd1439; }

	// end inline asm
	add.s64 	%rd2826, %rd2812, 112;
	// begin inline asm
	{ atom.add.f64 %fd6365,[%rd2826],%fd1438; }

	// end inline asm
	add.s64 	%rd2827, %rd2812, 120;
	// begin inline asm
	{ atom.add.f64 %fd6367,[%rd2827],%fd1413; }

	// end inline asm
	add.s64 	%rd2828, %rd2812, 128;
	// begin inline asm
	{ atom.add.f64 %fd6369,[%rd2828],%fd1412; }

	// end inline asm
	add.s64 	%rd2829, %rd2812, 136;
	// begin inline asm
	{ atom.add.f64 %fd6371,[%rd2829],%fd1411; }

	// end inline asm
	add.s64 	%rd2830, %rd2812, 144;
	// begin inline asm
	{ atom.add.f64 %fd6373,[%rd2830],%fd1386; }

	// end inline asm
	add.s64 	%rd2831, %rd2812, 152;
	// begin inline asm
	{ atom.add.f64 %fd6375,[%rd2831],%fd1385; }

	// end inline asm
	add.s64 	%rd2832, %rd2812, 160;
	// begin inline asm
	{ atom.add.f64 %fd6377,[%rd2832],%fd1384; }

	// end inline asm
	add.s64 	%rd2833, %rd2812, 168;
	// begin inline asm
	{ atom.add.f64 %fd6379,[%rd2833],%fd1359; }

	// end inline asm
	add.s64 	%rd2834, %rd2812, 176;
	// begin inline asm
	{ atom.add.f64 %fd6381,[%rd2834],%fd1358; }

	// end inline asm
	add.s64 	%rd2835, %rd2812, 184;
	// begin inline asm
	{ atom.add.f64 %fd6383,[%rd2835],%fd1357; }

	// end inline asm
	add.s64 	%rd2836, %rd2812, 192;
	// begin inline asm
	{ atom.add.f64 %fd6385,[%rd2836],%fd2301; }

	// end inline asm
	add.s64 	%rd2837, %rd2812, 200;
	// begin inline asm
	{ atom.add.f64 %fd6387,[%rd2837],%fd2300; }

	// end inline asm
	add.s64 	%rd2838, %rd2812, 208;
	// begin inline asm
	{ atom.add.f64 %fd6389,[%rd2838],%fd2299; }

	// end inline asm
	add.s64 	%rd2839, %rd2812, 216;
	// begin inline asm
	{ atom.add.f64 %fd6391,[%rd2839],%fd2274; }

	// end inline asm
	add.s64 	%rd2840, %rd2812, 224;
	// begin inline asm
	{ atom.add.f64 %fd6393,[%rd2840],%fd2273; }

	// end inline asm
	add.s64 	%rd2841, %rd2812, 232;
	// begin inline asm
	{ atom.add.f64 %fd6395,[%rd2841],%fd2272; }

	// end inline asm
	add.s64 	%rd2842, %rd2812, 240;
	// begin inline asm
	{ atom.add.f64 %fd6397,[%rd2842],%fd2247; }

	// end inline asm
	add.s64 	%rd2843, %rd2812, 248;
	// begin inline asm
	{ atom.add.f64 %fd6399,[%rd2843],%fd2246; }

	// end inline asm
	add.s64 	%rd2844, %rd2812, 256;
	// begin inline asm
	{ atom.add.f64 %fd6401,[%rd2844],%fd2245; }

	// end inline asm
	add.s64 	%rd2845, %rd2812, 264;
	// begin inline asm
	{ atom.add.f64 %fd6403,[%rd2845],%fd2220; }

	// end inline asm
	add.s64 	%rd2846, %rd2812, 272;
	// begin inline asm
	{ atom.add.f64 %fd6405,[%rd2846],%fd2219; }

	// end inline asm
	add.s64 	%rd2847, %rd2812, 280;
	// begin inline asm
	{ atom.add.f64 %fd6407,[%rd2847],%fd2218; }

	// end inline asm
	add.s64 	%rd2848, %rd2812, 288;
	// begin inline asm
	{ atom.add.f64 %fd6409,[%rd2848],%fd1437; }

	// end inline asm
	add.s64 	%rd2849, %rd2812, 296;
	// begin inline asm
	{ atom.add.f64 %fd6411,[%rd2849],%fd1436; }

	// end inline asm
	add.s64 	%rd2850, %rd2812, 304;
	// begin inline asm
	{ atom.add.f64 %fd6413,[%rd2850],%fd1435; }

	// end inline asm
	add.s64 	%rd2851, %rd2812, 312;
	// begin inline asm
	{ atom.add.f64 %fd6415,[%rd2851],%fd1410; }

	// end inline asm
	add.s64 	%rd2852, %rd2812, 320;
	// begin inline asm
	{ atom.add.f64 %fd6417,[%rd2852],%fd1409; }

	// end inline asm
	add.s64 	%rd2853, %rd2812, 328;
	// begin inline asm
	{ atom.add.f64 %fd6419,[%rd2853],%fd1408; }

	// end inline asm
	add.s64 	%rd2854, %rd2812, 336;
	// begin inline asm
	{ atom.add.f64 %fd6421,[%rd2854],%fd1383; }

	// end inline asm
	add.s64 	%rd2855, %rd2812, 344;
	// begin inline asm
	{ atom.add.f64 %fd6423,[%rd2855],%fd1382; }

	// end inline asm
	add.s64 	%rd2856, %rd2812, 352;
	// begin inline asm
	{ atom.add.f64 %fd6425,[%rd2856],%fd1381; }

	// end inline asm
	add.s64 	%rd2857, %rd2812, 360;
	// begin inline asm
	{ atom.add.f64 %fd6427,[%rd2857],%fd1356; }

	// end inline asm
	add.s64 	%rd2858, %rd2812, 368;
	// begin inline asm
	{ atom.add.f64 %fd6429,[%rd2858],%fd1355; }

	// end inline asm
	add.s64 	%rd2859, %rd2812, 376;
	// begin inline asm
	{ atom.add.f64 %fd6431,[%rd2859],%fd1354; }

	// end inline asm
	add.s64 	%rd2860, %rd2812, 384;
	// begin inline asm
	{ atom.add.f64 %fd6433,[%rd2860],%fd2298; }

	// end inline asm
	add.s64 	%rd2861, %rd2812, 392;
	// begin inline asm
	{ atom.add.f64 %fd6435,[%rd2861],%fd2297; }

	// end inline asm
	add.s64 	%rd2862, %rd2812, 400;
	// begin inline asm
	{ atom.add.f64 %fd6437,[%rd2862],%fd2296; }

	// end inline asm
	add.s64 	%rd2863, %rd2812, 408;
	// begin inline asm
	{ atom.add.f64 %fd6439,[%rd2863],%fd2271; }

	// end inline asm
	add.s64 	%rd2864, %rd2812, 416;
	// begin inline asm
	{ atom.add.f64 %fd6441,[%rd2864],%fd2270; }

	// end inline asm
	add.s64 	%rd2865, %rd2812, 424;
	// begin inline asm
	{ atom.add.f64 %fd6443,[%rd2865],%fd2269; }

	// end inline asm
	add.s64 	%rd2866, %rd2812, 432;
	// begin inline asm
	{ atom.add.f64 %fd6445,[%rd2866],%fd2244; }

	// end inline asm
	add.s64 	%rd2867, %rd2812, 440;
	// begin inline asm
	{ atom.add.f64 %fd6447,[%rd2867],%fd2243; }

	// end inline asm
	add.s64 	%rd2868, %rd2812, 448;
	// begin inline asm
	{ atom.add.f64 %fd6449,[%rd2868],%fd2242; }

	// end inline asm
	add.s64 	%rd2869, %rd2812, 456;
	// begin inline asm
	{ atom.add.f64 %fd6451,[%rd2869],%fd2217; }

	// end inline asm
	add.s64 	%rd2870, %rd2812, 464;
	// begin inline asm
	{ atom.add.f64 %fd6453,[%rd2870],%fd2216; }

	// end inline asm
	add.s64 	%rd2871, %rd2812, 472;
	// begin inline asm
	{ atom.add.f64 %fd6455,[%rd2871],%fd2215; }

	// end inline asm
	add.s64 	%rd2872, %rd2812, 480;
	// begin inline asm
	{ atom.add.f64 %fd6457,[%rd2872],%fd1434; }

	// end inline asm
	add.s64 	%rd2873, %rd2812, 488;
	// begin inline asm
	{ atom.add.f64 %fd6459,[%rd2873],%fd1433; }

	// end inline asm
	add.s64 	%rd2874, %rd2812, 496;
	// begin inline asm
	{ atom.add.f64 %fd6461,[%rd2874],%fd1432; }

	// end inline asm
	add.s64 	%rd2875, %rd2812, 504;
	// begin inline asm
	{ atom.add.f64 %fd6463,[%rd2875],%fd1407; }

	// end inline asm
	add.s64 	%rd2876, %rd2812, 512;
	// begin inline asm
	{ atom.add.f64 %fd6465,[%rd2876],%fd1406; }

	// end inline asm
	add.s64 	%rd2877, %rd2812, 520;
	// begin inline asm
	{ atom.add.f64 %fd6467,[%rd2877],%fd1405; }

	// end inline asm
	add.s64 	%rd2878, %rd2812, 528;
	// begin inline asm
	{ atom.add.f64 %fd6469,[%rd2878],%fd1380; }

	// end inline asm
	add.s64 	%rd2879, %rd2812, 536;
	// begin inline asm
	{ atom.add.f64 %fd6471,[%rd2879],%fd1379; }

	// end inline asm
	add.s64 	%rd2880, %rd2812, 544;
	// begin inline asm
	{ atom.add.f64 %fd6473,[%rd2880],%fd1378; }

	// end inline asm
	add.s64 	%rd2881, %rd2812, 552;
	// begin inline asm
	{ atom.add.f64 %fd6475,[%rd2881],%fd1353; }

	// end inline asm
	add.s64 	%rd2882, %rd2812, 560;
	// begin inline asm
	{ atom.add.f64 %fd6477,[%rd2882],%fd1352; }

	// end inline asm
	add.s64 	%rd2883, %rd2812, 568;
	// begin inline asm
	{ atom.add.f64 %fd6479,[%rd2883],%fd1351; }

	// end inline asm
	add.s64 	%rd2884, %rd2812, 576;
	// begin inline asm
	{ atom.add.f64 %fd6481,[%rd2884],%fd2196; }

	// end inline asm
	add.s64 	%rd2885, %rd2812, 584;
	// begin inline asm
	{ atom.add.f64 %fd6483,[%rd2885],%fd2195; }

	// end inline asm
	add.s64 	%rd2886, %rd2812, 592;
	// begin inline asm
	{ atom.add.f64 %fd6485,[%rd2886],%fd2194; }

	// end inline asm
	add.s64 	%rd2887, %rd2812, 600;
	// begin inline asm
	{ atom.add.f64 %fd6487,[%rd2887],%fd2169; }

	// end inline asm
	add.s64 	%rd2888, %rd2812, 608;
	// begin inline asm
	{ atom.add.f64 %fd6489,[%rd2888],%fd2168; }

	// end inline asm
	add.s64 	%rd2889, %rd2812, 616;
	// begin inline asm
	{ atom.add.f64 %fd6491,[%rd2889],%fd2167; }

	// end inline asm
	add.s64 	%rd2890, %rd2812, 624;
	// begin inline asm
	{ atom.add.f64 %fd6493,[%rd2890],%fd2142; }

	// end inline asm
	add.s64 	%rd2891, %rd2812, 632;
	// begin inline asm
	{ atom.add.f64 %fd6495,[%rd2891],%fd2141; }

	// end inline asm
	add.s64 	%rd2892, %rd2812, 640;
	// begin inline asm
	{ atom.add.f64 %fd6497,[%rd2892],%fd2140; }

	// end inline asm
	add.s64 	%rd2893, %rd2812, 648;
	// begin inline asm
	{ atom.add.f64 %fd6499,[%rd2893],%fd2115; }

	// end inline asm
	add.s64 	%rd2894, %rd2812, 656;
	// begin inline asm
	{ atom.add.f64 %fd6501,[%rd2894],%fd2114; }

	// end inline asm
	add.s64 	%rd2895, %rd2812, 664;
	// begin inline asm
	{ atom.add.f64 %fd6503,[%rd2895],%fd2113; }

	// end inline asm
	add.s64 	%rd2896, %rd2812, 672;
	// begin inline asm
	{ atom.add.f64 %fd6505,[%rd2896],%fd1332; }

	// end inline asm
	add.s64 	%rd2897, %rd2812, 680;
	// begin inline asm
	{ atom.add.f64 %fd6507,[%rd2897],%fd1331; }

	// end inline asm
	add.s64 	%rd2898, %rd2812, 688;
	// begin inline asm
	{ atom.add.f64 %fd6509,[%rd2898],%fd1330; }

	// end inline asm
	add.s64 	%rd2899, %rd2812, 696;
	// begin inline asm
	{ atom.add.f64 %fd6511,[%rd2899],%fd1305; }

	// end inline asm
	add.s64 	%rd2900, %rd2812, 704;
	// begin inline asm
	{ atom.add.f64 %fd6513,[%rd2900],%fd1304; }

	// end inline asm
	add.s64 	%rd2901, %rd2812, 712;
	// begin inline asm
	{ atom.add.f64 %fd6515,[%rd2901],%fd1303; }

	// end inline asm
	add.s64 	%rd2902, %rd2812, 720;
	// begin inline asm
	{ atom.add.f64 %fd6517,[%rd2902],%fd1278; }

	// end inline asm
	add.s64 	%rd2903, %rd2812, 728;
	// begin inline asm
	{ atom.add.f64 %fd6519,[%rd2903],%fd1277; }

	// end inline asm
	add.s64 	%rd2904, %rd2812, 736;
	// begin inline asm
	{ atom.add.f64 %fd6521,[%rd2904],%fd1276; }

	// end inline asm
	add.s64 	%rd2905, %rd2812, 744;
	// begin inline asm
	{ atom.add.f64 %fd6523,[%rd2905],%fd1251; }

	// end inline asm
	add.s64 	%rd2906, %rd2812, 752;
	// begin inline asm
	{ atom.add.f64 %fd6525,[%rd2906],%fd1250; }

	// end inline asm
	add.s64 	%rd2907, %rd2812, 760;
	// begin inline asm
	{ atom.add.f64 %fd6527,[%rd2907],%fd1249; }

	// end inline asm
	add.s64 	%rd2908, %rd2812, 768;
	// begin inline asm
	{ atom.add.f64 %fd6529,[%rd2908],%fd2193; }

	// end inline asm
	add.s64 	%rd2909, %rd2812, 776;
	// begin inline asm
	{ atom.add.f64 %fd6531,[%rd2909],%fd2192; }

	// end inline asm
	add.s64 	%rd2910, %rd2812, 784;
	// begin inline asm
	{ atom.add.f64 %fd6533,[%rd2910],%fd2191; }

	// end inline asm
	add.s64 	%rd2911, %rd2812, 792;
	// begin inline asm
	{ atom.add.f64 %fd6535,[%rd2911],%fd2166; }

	// end inline asm
	add.s64 	%rd2912, %rd2812, 800;
	// begin inline asm
	{ atom.add.f64 %fd6537,[%rd2912],%fd2165; }

	// end inline asm
	add.s64 	%rd2913, %rd2812, 808;
	// begin inline asm
	{ atom.add.f64 %fd6539,[%rd2913],%fd2164; }

	// end inline asm
	add.s64 	%rd2914, %rd2812, 816;
	// begin inline asm
	{ atom.add.f64 %fd6541,[%rd2914],%fd2139; }

	// end inline asm
	add.s64 	%rd2915, %rd2812, 824;
	// begin inline asm
	{ atom.add.f64 %fd6543,[%rd2915],%fd2138; }

	// end inline asm
	add.s64 	%rd2916, %rd2812, 832;
	// begin inline asm
	{ atom.add.f64 %fd6545,[%rd2916],%fd2137; }

	// end inline asm
	add.s64 	%rd2917, %rd2812, 840;
	// begin inline asm
	{ atom.add.f64 %fd6547,[%rd2917],%fd2112; }

	// end inline asm
	add.s64 	%rd2918, %rd2812, 848;
	// begin inline asm
	{ atom.add.f64 %fd6549,[%rd2918],%fd2111; }

	// end inline asm
	add.s64 	%rd2919, %rd2812, 856;
	// begin inline asm
	{ atom.add.f64 %fd6551,[%rd2919],%fd2110; }

	// end inline asm
	add.s64 	%rd2920, %rd2812, 864;
	// begin inline asm
	{ atom.add.f64 %fd6553,[%rd2920],%fd1329; }

	// end inline asm
	add.s64 	%rd2921, %rd2812, 872;
	// begin inline asm
	{ atom.add.f64 %fd6555,[%rd2921],%fd1328; }

	// end inline asm
	add.s64 	%rd2922, %rd2812, 880;
	// begin inline asm
	{ atom.add.f64 %fd6557,[%rd2922],%fd1327; }

	// end inline asm
	add.s64 	%rd2923, %rd2812, 888;
	// begin inline asm
	{ atom.add.f64 %fd6559,[%rd2923],%fd1302; }

	// end inline asm
	add.s64 	%rd2924, %rd2812, 896;
	// begin inline asm
	{ atom.add.f64 %fd6561,[%rd2924],%fd1301; }

	// end inline asm
	add.s64 	%rd2925, %rd2812, 904;
	// begin inline asm
	{ atom.add.f64 %fd6563,[%rd2925],%fd1300; }

	// end inline asm
	add.s64 	%rd2926, %rd2812, 912;
	// begin inline asm
	{ atom.add.f64 %fd6565,[%rd2926],%fd1275; }

	// end inline asm
	add.s64 	%rd2927, %rd2812, 920;
	// begin inline asm
	{ atom.add.f64 %fd6567,[%rd2927],%fd1274; }

	// end inline asm
	add.s64 	%rd2928, %rd2812, 928;
	// begin inline asm
	{ atom.add.f64 %fd6569,[%rd2928],%fd1273; }

	// end inline asm
	add.s64 	%rd2929, %rd2812, 936;
	// begin inline asm
	{ atom.add.f64 %fd6571,[%rd2929],%fd1248; }

	// end inline asm
	add.s64 	%rd2930, %rd2812, 944;
	// begin inline asm
	{ atom.add.f64 %fd6573,[%rd2930],%fd1247; }

	// end inline asm
	add.s64 	%rd2931, %rd2812, 952;
	// begin inline asm
	{ atom.add.f64 %fd6575,[%rd2931],%fd1246; }

	// end inline asm
	add.s64 	%rd2932, %rd2812, 960;
	// begin inline asm
	{ atom.add.f64 %fd6577,[%rd2932],%fd2190; }

	// end inline asm
	add.s64 	%rd2933, %rd2812, 968;
	// begin inline asm
	{ atom.add.f64 %fd6579,[%rd2933],%fd2189; }

	// end inline asm
	add.s64 	%rd2934, %rd2812, 976;
	// begin inline asm
	{ atom.add.f64 %fd6581,[%rd2934],%fd2188; }

	// end inline asm
	add.s64 	%rd2935, %rd2812, 984;
	// begin inline asm
	{ atom.add.f64 %fd6583,[%rd2935],%fd2163; }

	// end inline asm
	add.s64 	%rd2936, %rd2812, 992;
	// begin inline asm
	{ atom.add.f64 %fd6585,[%rd2936],%fd2162; }

	// end inline asm
	add.s64 	%rd2937, %rd2812, 1000;
	// begin inline asm
	{ atom.add.f64 %fd6587,[%rd2937],%fd2161; }

	// end inline asm
	add.s64 	%rd2938, %rd2812, 1008;
	// begin inline asm
	{ atom.add.f64 %fd6589,[%rd2938],%fd2136; }

	// end inline asm
	add.s64 	%rd2939, %rd2812, 1016;
	// begin inline asm
	{ atom.add.f64 %fd6591,[%rd2939],%fd2135; }

	// end inline asm
	add.s64 	%rd2940, %rd2812, 1024;
	// begin inline asm
	{ atom.add.f64 %fd6593,[%rd2940],%fd2134; }

	// end inline asm
	add.s64 	%rd2941, %rd2812, 1032;
	// begin inline asm
	{ atom.add.f64 %fd6595,[%rd2941],%fd2109; }

	// end inline asm
	add.s64 	%rd2942, %rd2812, 1040;
	// begin inline asm
	{ atom.add.f64 %fd6597,[%rd2942],%fd2108; }

	// end inline asm
	add.s64 	%rd2943, %rd2812, 1048;
	// begin inline asm
	{ atom.add.f64 %fd6599,[%rd2943],%fd2107; }

	// end inline asm
	add.s64 	%rd2944, %rd2812, 1056;
	// begin inline asm
	{ atom.add.f64 %fd6601,[%rd2944],%fd1326; }

	// end inline asm
	add.s64 	%rd2945, %rd2812, 1064;
	// begin inline asm
	{ atom.add.f64 %fd6603,[%rd2945],%fd1325; }

	// end inline asm
	add.s64 	%rd2946, %rd2812, 1072;
	// begin inline asm
	{ atom.add.f64 %fd6605,[%rd2946],%fd1324; }

	// end inline asm
	add.s64 	%rd2947, %rd2812, 1080;
	// begin inline asm
	{ atom.add.f64 %fd6607,[%rd2947],%fd1299; }

	// end inline asm
	add.s64 	%rd2948, %rd2812, 1088;
	// begin inline asm
	{ atom.add.f64 %fd6609,[%rd2948],%fd1298; }

	// end inline asm
	add.s64 	%rd2949, %rd2812, 1096;
	// begin inline asm
	{ atom.add.f64 %fd6611,[%rd2949],%fd1297; }

	// end inline asm
	add.s64 	%rd2950, %rd2812, 1104;
	// begin inline asm
	{ atom.add.f64 %fd6613,[%rd2950],%fd1272; }

	// end inline asm
	add.s64 	%rd2951, %rd2812, 1112;
	// begin inline asm
	{ atom.add.f64 %fd6615,[%rd2951],%fd1271; }

	// end inline asm
	add.s64 	%rd2952, %rd2812, 1120;
	// begin inline asm
	{ atom.add.f64 %fd6617,[%rd2952],%fd1270; }

	// end inline asm
	add.s64 	%rd2953, %rd2812, 1128;
	// begin inline asm
	{ atom.add.f64 %fd6619,[%rd2953],%fd1245; }

	// end inline asm
	add.s64 	%rd2954, %rd2812, 1136;
	// begin inline asm
	{ atom.add.f64 %fd6621,[%rd2954],%fd1244; }

	// end inline asm
	add.s64 	%rd2955, %rd2812, 1144;
	// begin inline asm
	{ atom.add.f64 %fd6623,[%rd2955],%fd1243; }

	// end inline asm
	add.s64 	%rd2956, %rd2812, 1152;
	// begin inline asm
	{ atom.add.f64 %fd6625,[%rd2956],%fd2088; }

	// end inline asm
	add.s64 	%rd2957, %rd2812, 1160;
	// begin inline asm
	{ atom.add.f64 %fd6627,[%rd2957],%fd2087; }

	// end inline asm
	add.s64 	%rd2958, %rd2812, 1168;
	// begin inline asm
	{ atom.add.f64 %fd6629,[%rd2958],%fd2086; }

	// end inline asm
	add.s64 	%rd2959, %rd2812, 1176;
	// begin inline asm
	{ atom.add.f64 %fd6631,[%rd2959],%fd2061; }

	// end inline asm
	add.s64 	%rd2960, %rd2812, 1184;
	// begin inline asm
	{ atom.add.f64 %fd6633,[%rd2960],%fd2060; }

	// end inline asm
	add.s64 	%rd2961, %rd2812, 1192;
	// begin inline asm
	{ atom.add.f64 %fd6635,[%rd2961],%fd2059; }

	// end inline asm
	add.s64 	%rd2962, %rd2812, 1200;
	// begin inline asm
	{ atom.add.f64 %fd6637,[%rd2962],%fd2034; }

	// end inline asm
	add.s64 	%rd2963, %rd2812, 1208;
	// begin inline asm
	{ atom.add.f64 %fd6639,[%rd2963],%fd2033; }

	// end inline asm
	add.s64 	%rd2964, %rd2812, 1216;
	// begin inline asm
	{ atom.add.f64 %fd6641,[%rd2964],%fd2032; }

	// end inline asm
	add.s64 	%rd2965, %rd2812, 1224;
	// begin inline asm
	{ atom.add.f64 %fd6643,[%rd2965],%fd2007; }

	// end inline asm
	add.s64 	%rd2966, %rd2812, 1232;
	// begin inline asm
	{ atom.add.f64 %fd6645,[%rd2966],%fd2006; }

	// end inline asm
	add.s64 	%rd2967, %rd2812, 1240;
	// begin inline asm
	{ atom.add.f64 %fd6647,[%rd2967],%fd2005; }

	// end inline asm
	add.s64 	%rd2968, %rd2812, 1248;
	// begin inline asm
	{ atom.add.f64 %fd6649,[%rd2968],%fd1224; }

	// end inline asm
	add.s64 	%rd2969, %rd2812, 1256;
	// begin inline asm
	{ atom.add.f64 %fd6651,[%rd2969],%fd1223; }

	// end inline asm
	add.s64 	%rd2970, %rd2812, 1264;
	// begin inline asm
	{ atom.add.f64 %fd6653,[%rd2970],%fd1222; }

	// end inline asm
	add.s64 	%rd2971, %rd2812, 1272;
	// begin inline asm
	{ atom.add.f64 %fd6655,[%rd2971],%fd1197; }

	// end inline asm
	add.s64 	%rd2972, %rd2812, 1280;
	// begin inline asm
	{ atom.add.f64 %fd6657,[%rd2972],%fd1196; }

	// end inline asm
	add.s64 	%rd2973, %rd2812, 1288;
	// begin inline asm
	{ atom.add.f64 %fd6659,[%rd2973],%fd1195; }

	// end inline asm
	add.s64 	%rd2974, %rd2812, 1296;
	// begin inline asm
	{ atom.add.f64 %fd6661,[%rd2974],%fd1170; }

	// end inline asm
	add.s64 	%rd2975, %rd2812, 1304;
	// begin inline asm
	{ atom.add.f64 %fd6663,[%rd2975],%fd1169; }

	// end inline asm
	add.s64 	%rd2976, %rd2812, 1312;
	// begin inline asm
	{ atom.add.f64 %fd6665,[%rd2976],%fd1168; }

	// end inline asm
	add.s64 	%rd2977, %rd2812, 1320;
	// begin inline asm
	{ atom.add.f64 %fd6667,[%rd2977],%fd1143; }

	// end inline asm
	add.s64 	%rd2978, %rd2812, 1328;
	// begin inline asm
	{ atom.add.f64 %fd6669,[%rd2978],%fd1142; }

	// end inline asm
	add.s64 	%rd2979, %rd2812, 1336;
	// begin inline asm
	{ atom.add.f64 %fd6671,[%rd2979],%fd1141; }

	// end inline asm
	add.s64 	%rd2980, %rd2812, 1344;
	// begin inline asm
	{ atom.add.f64 %fd6673,[%rd2980],%fd2085; }

	// end inline asm
	add.s64 	%rd2981, %rd2812, 1352;
	// begin inline asm
	{ atom.add.f64 %fd6675,[%rd2981],%fd2084; }

	// end inline asm
	add.s64 	%rd2982, %rd2812, 1360;
	// begin inline asm
	{ atom.add.f64 %fd6677,[%rd2982],%fd2083; }

	// end inline asm
	add.s64 	%rd2983, %rd2812, 1368;
	// begin inline asm
	{ atom.add.f64 %fd6679,[%rd2983],%fd2058; }

	// end inline asm
	add.s64 	%rd2984, %rd2812, 1376;
	// begin inline asm
	{ atom.add.f64 %fd6681,[%rd2984],%fd2057; }

	// end inline asm
	add.s64 	%rd2985, %rd2812, 1384;
	// begin inline asm
	{ atom.add.f64 %fd6683,[%rd2985],%fd2056; }

	// end inline asm
	add.s64 	%rd2986, %rd2812, 1392;
	// begin inline asm
	{ atom.add.f64 %fd6685,[%rd2986],%fd2031; }

	// end inline asm
	add.s64 	%rd2987, %rd2812, 1400;
	// begin inline asm
	{ atom.add.f64 %fd6687,[%rd2987],%fd2030; }

	// end inline asm
	add.s64 	%rd2988, %rd2812, 1408;
	// begin inline asm
	{ atom.add.f64 %fd6689,[%rd2988],%fd2029; }

	// end inline asm
	add.s64 	%rd2989, %rd2812, 1416;
	// begin inline asm
	{ atom.add.f64 %fd6691,[%rd2989],%fd2004; }

	// end inline asm
	add.s64 	%rd2990, %rd2812, 1424;
	// begin inline asm
	{ atom.add.f64 %fd6693,[%rd2990],%fd2003; }

	// end inline asm
	add.s64 	%rd2991, %rd2812, 1432;
	// begin inline asm
	{ atom.add.f64 %fd6695,[%rd2991],%fd2002; }

	// end inline asm
	add.s64 	%rd2992, %rd2812, 1440;
	// begin inline asm
	{ atom.add.f64 %fd6697,[%rd2992],%fd1221; }

	// end inline asm
	add.s64 	%rd2993, %rd2812, 1448;
	// begin inline asm
	{ atom.add.f64 %fd6699,[%rd2993],%fd1220; }

	// end inline asm
	add.s64 	%rd2994, %rd2812, 1456;
	// begin inline asm
	{ atom.add.f64 %fd6701,[%rd2994],%fd1219; }

	// end inline asm
	add.s64 	%rd2995, %rd2812, 1464;
	// begin inline asm
	{ atom.add.f64 %fd6703,[%rd2995],%fd1194; }

	// end inline asm
	add.s64 	%rd2996, %rd2812, 1472;
	// begin inline asm
	{ atom.add.f64 %fd6705,[%rd2996],%fd1193; }

	// end inline asm
	add.s64 	%rd2997, %rd2812, 1480;
	// begin inline asm
	{ atom.add.f64 %fd6707,[%rd2997],%fd1192; }

	// end inline asm
	add.s64 	%rd2998, %rd2812, 1488;
	// begin inline asm
	{ atom.add.f64 %fd6709,[%rd2998],%fd1167; }

	// end inline asm
	add.s64 	%rd2999, %rd2812, 1496;
	// begin inline asm
	{ atom.add.f64 %fd6711,[%rd2999],%fd1166; }

	// end inline asm
	add.s64 	%rd3000, %rd2812, 1504;
	// begin inline asm
	{ atom.add.f64 %fd6713,[%rd3000],%fd1165; }

	// end inline asm
	add.s64 	%rd3001, %rd2812, 1512;
	// begin inline asm
	{ atom.add.f64 %fd6715,[%rd3001],%fd1140; }

	// end inline asm
	add.s64 	%rd3002, %rd2812, 1520;
	// begin inline asm
	{ atom.add.f64 %fd6717,[%rd3002],%fd1139; }

	// end inline asm
	add.s64 	%rd3003, %rd2812, 1528;
	// begin inline asm
	{ atom.add.f64 %fd6719,[%rd3003],%fd1138; }

	// end inline asm
	add.s64 	%rd3004, %rd2812, 1536;
	// begin inline asm
	{ atom.add.f64 %fd6721,[%rd3004],%fd2082; }

	// end inline asm
	add.s64 	%rd3005, %rd2812, 1544;
	// begin inline asm
	{ atom.add.f64 %fd6723,[%rd3005],%fd2081; }

	// end inline asm
	add.s64 	%rd3006, %rd2812, 1552;
	// begin inline asm
	{ atom.add.f64 %fd6725,[%rd3006],%fd2080; }

	// end inline asm
	add.s64 	%rd3007, %rd2812, 1560;
	// begin inline asm
	{ atom.add.f64 %fd6727,[%rd3007],%fd2055; }

	// end inline asm
	add.s64 	%rd3008, %rd2812, 1568;
	// begin inline asm
	{ atom.add.f64 %fd6729,[%rd3008],%fd2054; }

	// end inline asm
	add.s64 	%rd3009, %rd2812, 1576;
	// begin inline asm
	{ atom.add.f64 %fd6731,[%rd3009],%fd2053; }

	// end inline asm
	add.s64 	%rd3010, %rd2812, 1584;
	// begin inline asm
	{ atom.add.f64 %fd6733,[%rd3010],%fd2028; }

	// end inline asm
	add.s64 	%rd3011, %rd2812, 1592;
	// begin inline asm
	{ atom.add.f64 %fd6735,[%rd3011],%fd2027; }

	// end inline asm
	add.s64 	%rd3012, %rd2812, 1600;
	// begin inline asm
	{ atom.add.f64 %fd6737,[%rd3012],%fd2026; }

	// end inline asm
	add.s64 	%rd3013, %rd2812, 1608;
	// begin inline asm
	{ atom.add.f64 %fd6739,[%rd3013],%fd2001; }

	// end inline asm
	add.s64 	%rd3014, %rd2812, 1616;
	// begin inline asm
	{ atom.add.f64 %fd6741,[%rd3014],%fd2000; }

	// end inline asm
	add.s64 	%rd3015, %rd2812, 1624;
	// begin inline asm
	{ atom.add.f64 %fd6743,[%rd3015],%fd1999; }

	// end inline asm
	add.s64 	%rd3016, %rd2812, 1632;
	// begin inline asm
	{ atom.add.f64 %fd6745,[%rd3016],%fd1218; }

	// end inline asm
	add.s64 	%rd3017, %rd2812, 1640;
	// begin inline asm
	{ atom.add.f64 %fd6747,[%rd3017],%fd1217; }

	// end inline asm
	add.s64 	%rd3018, %rd2812, 1648;
	// begin inline asm
	{ atom.add.f64 %fd6749,[%rd3018],%fd1216; }

	// end inline asm
	add.s64 	%rd3019, %rd2812, 1656;
	// begin inline asm
	{ atom.add.f64 %fd6751,[%rd3019],%fd1191; }

	// end inline asm
	add.s64 	%rd3020, %rd2812, 1664;
	// begin inline asm
	{ atom.add.f64 %fd6753,[%rd3020],%fd1190; }

	// end inline asm
	add.s64 	%rd3021, %rd2812, 1672;
	// begin inline asm
	{ atom.add.f64 %fd6755,[%rd3021],%fd1189; }

	// end inline asm
	add.s64 	%rd3022, %rd2812, 1680;
	// begin inline asm
	{ atom.add.f64 %fd6757,[%rd3022],%fd1164; }

	// end inline asm
	add.s64 	%rd3023, %rd2812, 1688;
	// begin inline asm
	{ atom.add.f64 %fd6759,[%rd3023],%fd1163; }

	// end inline asm
	add.s64 	%rd3024, %rd2812, 1696;
	// begin inline asm
	{ atom.add.f64 %fd6761,[%rd3024],%fd1162; }

	// end inline asm
	add.s64 	%rd3025, %rd2812, 1704;
	// begin inline asm
	{ atom.add.f64 %fd6763,[%rd3025],%fd1137; }

	// end inline asm
	add.s64 	%rd3026, %rd2812, 1712;
	// begin inline asm
	{ atom.add.f64 %fd6765,[%rd3026],%fd1136; }

	// end inline asm
	add.s64 	%rd3027, %rd2812, 1720;
	// begin inline asm
	{ atom.add.f64 %fd6767,[%rd3027],%fd1135; }

	// end inline asm
	add.s64 	%rd3028, %rd2812, 1728;
	// begin inline asm
	{ atom.add.f64 %fd6769,[%rd3028],%fd1980; }

	// end inline asm
	add.s64 	%rd3029, %rd2812, 1736;
	// begin inline asm
	{ atom.add.f64 %fd6771,[%rd3029],%fd1979; }

	// end inline asm
	add.s64 	%rd3030, %rd2812, 1744;
	// begin inline asm
	{ atom.add.f64 %fd6773,[%rd3030],%fd1978; }

	// end inline asm
	add.s64 	%rd3031, %rd2812, 1752;
	// begin inline asm
	{ atom.add.f64 %fd6775,[%rd3031],%fd1953; }

	// end inline asm
	add.s64 	%rd3032, %rd2812, 1760;
	// begin inline asm
	{ atom.add.f64 %fd6777,[%rd3032],%fd1952; }

	// end inline asm
	add.s64 	%rd3033, %rd2812, 1768;
	// begin inline asm
	{ atom.add.f64 %fd6779,[%rd3033],%fd1951; }

	// end inline asm
	add.s64 	%rd3034, %rd2812, 1776;
	// begin inline asm
	{ atom.add.f64 %fd6781,[%rd3034],%fd1926; }

	// end inline asm
	add.s64 	%rd3035, %rd2812, 1784;
	// begin inline asm
	{ atom.add.f64 %fd6783,[%rd3035],%fd1925; }

	// end inline asm
	add.s64 	%rd3036, %rd2812, 1792;
	// begin inline asm
	{ atom.add.f64 %fd6785,[%rd3036],%fd1924; }

	// end inline asm
	add.s64 	%rd3037, %rd2812, 1800;
	// begin inline asm
	{ atom.add.f64 %fd6787,[%rd3037],%fd1899; }

	// end inline asm
	add.s64 	%rd3038, %rd2812, 1808;
	// begin inline asm
	{ atom.add.f64 %fd6789,[%rd3038],%fd1898; }

	// end inline asm
	add.s64 	%rd3039, %rd2812, 1816;
	// begin inline asm
	{ atom.add.f64 %fd6791,[%rd3039],%fd1897; }

	// end inline asm
	add.s64 	%rd3040, %rd2812, 1824;
	// begin inline asm
	{ atom.add.f64 %fd6793,[%rd3040],%fd1116; }

	// end inline asm
	add.s64 	%rd3041, %rd2812, 1832;
	// begin inline asm
	{ atom.add.f64 %fd6795,[%rd3041],%fd1115; }

	// end inline asm
	add.s64 	%rd3042, %rd2812, 1840;
	// begin inline asm
	{ atom.add.f64 %fd6797,[%rd3042],%fd1114; }

	// end inline asm
	add.s64 	%rd3043, %rd2812, 1848;
	// begin inline asm
	{ atom.add.f64 %fd6799,[%rd3043],%fd1089; }

	// end inline asm
	add.s64 	%rd3044, %rd2812, 1856;
	// begin inline asm
	{ atom.add.f64 %fd6801,[%rd3044],%fd1088; }

	// end inline asm
	add.s64 	%rd3045, %rd2812, 1864;
	// begin inline asm
	{ atom.add.f64 %fd6803,[%rd3045],%fd1087; }

	// end inline asm
	add.s64 	%rd3046, %rd2812, 1872;
	// begin inline asm
	{ atom.add.f64 %fd6805,[%rd3046],%fd1062; }

	// end inline asm
	add.s64 	%rd3047, %rd2812, 1880;
	// begin inline asm
	{ atom.add.f64 %fd6807,[%rd3047],%fd1061; }

	// end inline asm
	add.s64 	%rd3048, %rd2812, 1888;
	// begin inline asm
	{ atom.add.f64 %fd6809,[%rd3048],%fd1060; }

	// end inline asm
	add.s64 	%rd3049, %rd2812, 1896;
	// begin inline asm
	{ atom.add.f64 %fd6811,[%rd3049],%fd1035; }

	// end inline asm
	add.s64 	%rd3050, %rd2812, 1904;
	// begin inline asm
	{ atom.add.f64 %fd6813,[%rd3050],%fd1034; }

	// end inline asm
	add.s64 	%rd3051, %rd2812, 1912;
	// begin inline asm
	{ atom.add.f64 %fd6815,[%rd3051],%fd1033; }

	// end inline asm
	add.s64 	%rd3052, %rd2812, 1920;
	// begin inline asm
	{ atom.add.f64 %fd6817,[%rd3052],%fd1977; }

	// end inline asm
	add.s64 	%rd3053, %rd2812, 1928;
	// begin inline asm
	{ atom.add.f64 %fd6819,[%rd3053],%fd1976; }

	// end inline asm
	add.s64 	%rd3054, %rd2812, 1936;
	// begin inline asm
	{ atom.add.f64 %fd6821,[%rd3054],%fd1975; }

	// end inline asm
	add.s64 	%rd3055, %rd2812, 1944;
	// begin inline asm
	{ atom.add.f64 %fd6823,[%rd3055],%fd1950; }

	// end inline asm
	add.s64 	%rd3056, %rd2812, 1952;
	// begin inline asm
	{ atom.add.f64 %fd6825,[%rd3056],%fd1949; }

	// end inline asm
	add.s64 	%rd3057, %rd2812, 1960;
	// begin inline asm
	{ atom.add.f64 %fd6827,[%rd3057],%fd1948; }

	// end inline asm
	add.s64 	%rd3058, %rd2812, 1968;
	// begin inline asm
	{ atom.add.f64 %fd6829,[%rd3058],%fd1923; }

	// end inline asm
	add.s64 	%rd3059, %rd2812, 1976;
	// begin inline asm
	{ atom.add.f64 %fd6831,[%rd3059],%fd1922; }

	// end inline asm
	add.s64 	%rd3060, %rd2812, 1984;
	// begin inline asm
	{ atom.add.f64 %fd6833,[%rd3060],%fd1921; }

	// end inline asm
	add.s64 	%rd3061, %rd2812, 1992;
	// begin inline asm
	{ atom.add.f64 %fd6835,[%rd3061],%fd1896; }

	// end inline asm
	add.s64 	%rd3062, %rd2812, 2000;
	// begin inline asm
	{ atom.add.f64 %fd6837,[%rd3062],%fd1895; }

	// end inline asm
	add.s64 	%rd3063, %rd2812, 2008;
	// begin inline asm
	{ atom.add.f64 %fd6839,[%rd3063],%fd1894; }

	// end inline asm
	add.s64 	%rd3064, %rd2812, 2016;
	// begin inline asm
	{ atom.add.f64 %fd6841,[%rd3064],%fd1113; }

	// end inline asm
	add.s64 	%rd3065, %rd2812, 2024;
	// begin inline asm
	{ atom.add.f64 %fd6843,[%rd3065],%fd1112; }

	// end inline asm
	add.s64 	%rd3066, %rd2812, 2032;
	// begin inline asm
	{ atom.add.f64 %fd6845,[%rd3066],%fd1111; }

	// end inline asm
	add.s64 	%rd3067, %rd2812, 2040;
	// begin inline asm
	{ atom.add.f64 %fd6847,[%rd3067],%fd1086; }

	// end inline asm
	add.s64 	%rd3068, %rd2812, 2048;
	// begin inline asm
	{ atom.add.f64 %fd6849,[%rd3068],%fd1085; }

	// end inline asm
	add.s64 	%rd3069, %rd2812, 2056;
	// begin inline asm
	{ atom.add.f64 %fd6851,[%rd3069],%fd1084; }

	// end inline asm
	add.s64 	%rd3070, %rd2812, 2064;
	// begin inline asm
	{ atom.add.f64 %fd6853,[%rd3070],%fd1059; }

	// end inline asm
	add.s64 	%rd3071, %rd2812, 2072;
	// begin inline asm
	{ atom.add.f64 %fd6855,[%rd3071],%fd1058; }

	// end inline asm
	add.s64 	%rd3072, %rd2812, 2080;
	// begin inline asm
	{ atom.add.f64 %fd6857,[%rd3072],%fd1057; }

	// end inline asm
	add.s64 	%rd3073, %rd2812, 2088;
	// begin inline asm
	{ atom.add.f64 %fd6859,[%rd3073],%fd1032; }

	// end inline asm
	add.s64 	%rd3074, %rd2812, 2096;
	// begin inline asm
	{ atom.add.f64 %fd6861,[%rd3074],%fd1031; }

	// end inline asm
	add.s64 	%rd3075, %rd2812, 2104;
	// begin inline asm
	{ atom.add.f64 %fd6863,[%rd3075],%fd1030; }

	// end inline asm
	add.s64 	%rd3076, %rd2812, 2112;
	// begin inline asm
	{ atom.add.f64 %fd6865,[%rd3076],%fd1974; }

	// end inline asm
	add.s64 	%rd3077, %rd2812, 2120;
	// begin inline asm
	{ atom.add.f64 %fd6867,[%rd3077],%fd1973; }

	// end inline asm
	add.s64 	%rd3078, %rd2812, 2128;
	// begin inline asm
	{ atom.add.f64 %fd6869,[%rd3078],%fd1972; }

	// end inline asm
	add.s64 	%rd3079, %rd2812, 2136;
	// begin inline asm
	{ atom.add.f64 %fd6871,[%rd3079],%fd1947; }

	// end inline asm
	add.s64 	%rd3080, %rd2812, 2144;
	// begin inline asm
	{ atom.add.f64 %fd6873,[%rd3080],%fd1946; }

	// end inline asm
	add.s64 	%rd3081, %rd2812, 2152;
	// begin inline asm
	{ atom.add.f64 %fd6875,[%rd3081],%fd1945; }

	// end inline asm
	add.s64 	%rd3082, %rd2812, 2160;
	// begin inline asm
	{ atom.add.f64 %fd6877,[%rd3082],%fd1920; }

	// end inline asm
	add.s64 	%rd3083, %rd2812, 2168;
	// begin inline asm
	{ atom.add.f64 %fd6879,[%rd3083],%fd1919; }

	// end inline asm
	add.s64 	%rd3084, %rd2812, 2176;
	// begin inline asm
	{ atom.add.f64 %fd6881,[%rd3084],%fd1918; }

	// end inline asm
	add.s64 	%rd3085, %rd2812, 2184;
	// begin inline asm
	{ atom.add.f64 %fd6883,[%rd3085],%fd1893; }

	// end inline asm
	add.s64 	%rd3086, %rd2812, 2192;
	// begin inline asm
	{ atom.add.f64 %fd6885,[%rd3086],%fd1892; }

	// end inline asm
	add.s64 	%rd3087, %rd2812, 2200;
	// begin inline asm
	{ atom.add.f64 %fd6887,[%rd3087],%fd1891; }

	// end inline asm
	add.s64 	%rd3088, %rd2812, 2208;
	// begin inline asm
	{ atom.add.f64 %fd6889,[%rd3088],%fd1110; }

	// end inline asm
	add.s64 	%rd3089, %rd2812, 2216;
	// begin inline asm
	{ atom.add.f64 %fd6891,[%rd3089],%fd1109; }

	// end inline asm
	add.s64 	%rd3090, %rd2812, 2224;
	// begin inline asm
	{ atom.add.f64 %fd6893,[%rd3090],%fd1108; }

	// end inline asm
	add.s64 	%rd3091, %rd2812, 2232;
	// begin inline asm
	{ atom.add.f64 %fd6895,[%rd3091],%fd1083; }

	// end inline asm
	add.s64 	%rd3092, %rd2812, 2240;
	// begin inline asm
	{ atom.add.f64 %fd6897,[%rd3092],%fd1082; }

	// end inline asm
	add.s64 	%rd3093, %rd2812, 2248;
	// begin inline asm
	{ atom.add.f64 %fd6899,[%rd3093],%fd1081; }

	// end inline asm
	add.s64 	%rd3094, %rd2812, 2256;
	// begin inline asm
	{ atom.add.f64 %fd6901,[%rd3094],%fd1056; }

	// end inline asm
	add.s64 	%rd3095, %rd2812, 2264;
	// begin inline asm
	{ atom.add.f64 %fd6903,[%rd3095],%fd1055; }

	// end inline asm
	add.s64 	%rd3096, %rd2812, 2272;
	// begin inline asm
	{ atom.add.f64 %fd6905,[%rd3096],%fd1054; }

	// end inline asm
	add.s64 	%rd3097, %rd2812, 2280;
	// begin inline asm
	{ atom.add.f64 %fd6907,[%rd3097],%fd1029; }

	// end inline asm
	add.s64 	%rd3098, %rd2812, 2288;
	// begin inline asm
	{ atom.add.f64 %fd6909,[%rd3098],%fd1028; }

	// end inline asm
	add.s64 	%rd3099, %rd2812, 2296;
	// begin inline asm
	{ atom.add.f64 %fd6911,[%rd3099],%fd1027; }

	// end inline asm
	add.s64 	%rd3100, %rd2812, 2304;
	// begin inline asm
	{ atom.add.f64 %fd6913,[%rd3100],%fd1008; }

	// end inline asm
	add.s64 	%rd3101, %rd2812, 2312;
	// begin inline asm
	{ atom.add.f64 %fd6915,[%rd3101],%fd1007; }

	// end inline asm
	add.s64 	%rd3102, %rd2812, 2320;
	// begin inline asm
	{ atom.add.f64 %fd6917,[%rd3102],%fd1006; }

	// end inline asm
	add.s64 	%rd3103, %rd2812, 2328;
	// begin inline asm
	{ atom.add.f64 %fd6919,[%rd3103],%fd981; }

	// end inline asm
	add.s64 	%rd3104, %rd2812, 2336;
	// begin inline asm
	{ atom.add.f64 %fd6921,[%rd3104],%fd980; }

	// end inline asm
	add.s64 	%rd3105, %rd2812, 2344;
	// begin inline asm
	{ atom.add.f64 %fd6923,[%rd3105],%fd979; }

	// end inline asm
	add.s64 	%rd3106, %rd2812, 2352;
	// begin inline asm
	{ atom.add.f64 %fd6925,[%rd3106],%fd954; }

	// end inline asm
	add.s64 	%rd3107, %rd2812, 2360;
	// begin inline asm
	{ atom.add.f64 %fd6927,[%rd3107],%fd953; }

	// end inline asm
	add.s64 	%rd3108, %rd2812, 2368;
	// begin inline asm
	{ atom.add.f64 %fd6929,[%rd3108],%fd952; }

	// end inline asm
	add.s64 	%rd3109, %rd2812, 2376;
	// begin inline asm
	{ atom.add.f64 %fd6931,[%rd3109],%fd927; }

	// end inline asm
	add.s64 	%rd3110, %rd2812, 2384;
	// begin inline asm
	{ atom.add.f64 %fd6933,[%rd3110],%fd926; }

	// end inline asm
	add.s64 	%rd3111, %rd2812, 2392;
	// begin inline asm
	{ atom.add.f64 %fd6935,[%rd3111],%fd925; }

	// end inline asm
	add.s64 	%rd3112, %rd2812, 2400;
	// begin inline asm
	{ atom.add.f64 %fd6937,[%rd3112],%fd1872; }

	// end inline asm
	add.s64 	%rd3113, %rd2812, 2408;
	// begin inline asm
	{ atom.add.f64 %fd6939,[%rd3113],%fd1871; }

	// end inline asm
	add.s64 	%rd3114, %rd2812, 2416;
	// begin inline asm
	{ atom.add.f64 %fd6941,[%rd3114],%fd1870; }

	// end inline asm
	add.s64 	%rd3115, %rd2812, 2424;
	// begin inline asm
	{ atom.add.f64 %fd6943,[%rd3115],%fd1845; }

	// end inline asm
	add.s64 	%rd3116, %rd2812, 2432;
	// begin inline asm
	{ atom.add.f64 %fd6945,[%rd3116],%fd1844; }

	// end inline asm
	add.s64 	%rd3117, %rd2812, 2440;
	// begin inline asm
	{ atom.add.f64 %fd6947,[%rd3117],%fd1843; }

	// end inline asm
	add.s64 	%rd3118, %rd2812, 2448;
	// begin inline asm
	{ atom.add.f64 %fd6949,[%rd3118],%fd1818; }

	// end inline asm
	add.s64 	%rd3119, %rd2812, 2456;
	// begin inline asm
	{ atom.add.f64 %fd6951,[%rd3119],%fd1817; }

	// end inline asm
	add.s64 	%rd3120, %rd2812, 2464;
	// begin inline asm
	{ atom.add.f64 %fd6953,[%rd3120],%fd1816; }

	// end inline asm
	add.s64 	%rd3121, %rd2812, 2472;
	// begin inline asm
	{ atom.add.f64 %fd6955,[%rd3121],%fd1791; }

	// end inline asm
	add.s64 	%rd3122, %rd2812, 2480;
	// begin inline asm
	{ atom.add.f64 %fd6957,[%rd3122],%fd1790; }

	// end inline asm
	add.s64 	%rd3123, %rd2812, 2488;
	// begin inline asm
	{ atom.add.f64 %fd6959,[%rd3123],%fd1789; }

	// end inline asm
	add.s64 	%rd3124, %rd2812, 2496;
	// begin inline asm
	{ atom.add.f64 %fd6961,[%rd3124],%fd1005; }

	// end inline asm
	add.s64 	%rd3125, %rd2812, 2504;
	// begin inline asm
	{ atom.add.f64 %fd6963,[%rd3125],%fd1004; }

	// end inline asm
	add.s64 	%rd3126, %rd2812, 2512;
	// begin inline asm
	{ atom.add.f64 %fd6965,[%rd3126],%fd1003; }

	// end inline asm
	add.s64 	%rd3127, %rd2812, 2520;
	// begin inline asm
	{ atom.add.f64 %fd6967,[%rd3127],%fd978; }

	// end inline asm
	add.s64 	%rd3128, %rd2812, 2528;
	// begin inline asm
	{ atom.add.f64 %fd6969,[%rd3128],%fd977; }

	// end inline asm
	add.s64 	%rd3129, %rd2812, 2536;
	// begin inline asm
	{ atom.add.f64 %fd6971,[%rd3129],%fd976; }

	// end inline asm
	add.s64 	%rd3130, %rd2812, 2544;
	// begin inline asm
	{ atom.add.f64 %fd6973,[%rd3130],%fd951; }

	// end inline asm
	add.s64 	%rd3131, %rd2812, 2552;
	// begin inline asm
	{ atom.add.f64 %fd6975,[%rd3131],%fd950; }

	// end inline asm
	add.s64 	%rd3132, %rd2812, 2560;
	// begin inline asm
	{ atom.add.f64 %fd6977,[%rd3132],%fd949; }

	// end inline asm
	add.s64 	%rd3133, %rd2812, 2568;
	// begin inline asm
	{ atom.add.f64 %fd6979,[%rd3133],%fd924; }

	// end inline asm
	add.s64 	%rd3134, %rd2812, 2576;
	// begin inline asm
	{ atom.add.f64 %fd6981,[%rd3134],%fd923; }

	// end inline asm
	add.s64 	%rd3135, %rd2812, 2584;
	// begin inline asm
	{ atom.add.f64 %fd6983,[%rd3135],%fd922; }

	// end inline asm
	add.s64 	%rd3136, %rd2812, 2592;
	// begin inline asm
	{ atom.add.f64 %fd6985,[%rd3136],%fd1869; }

	// end inline asm
	add.s64 	%rd3137, %rd2812, 2600;
	// begin inline asm
	{ atom.add.f64 %fd6987,[%rd3137],%fd1868; }

	// end inline asm
	add.s64 	%rd3138, %rd2812, 2608;
	// begin inline asm
	{ atom.add.f64 %fd6989,[%rd3138],%fd1867; }

	// end inline asm
	add.s64 	%rd3139, %rd2812, 2616;
	// begin inline asm
	{ atom.add.f64 %fd6991,[%rd3139],%fd1842; }

	// end inline asm
	add.s64 	%rd3140, %rd2812, 2624;
	// begin inline asm
	{ atom.add.f64 %fd6993,[%rd3140],%fd1841; }

	// end inline asm
	add.s64 	%rd3141, %rd2812, 2632;
	// begin inline asm
	{ atom.add.f64 %fd6995,[%rd3141],%fd1840; }

	// end inline asm
	add.s64 	%rd3142, %rd2812, 2640;
	// begin inline asm
	{ atom.add.f64 %fd6997,[%rd3142],%fd1815; }

	// end inline asm
	add.s64 	%rd3143, %rd2812, 2648;
	// begin inline asm
	{ atom.add.f64 %fd6999,[%rd3143],%fd1814; }

	// end inline asm
	add.s64 	%rd3144, %rd2812, 2656;
	// begin inline asm
	{ atom.add.f64 %fd7001,[%rd3144],%fd1813; }

	// end inline asm
	add.s64 	%rd3145, %rd2812, 2664;
	// begin inline asm
	{ atom.add.f64 %fd7003,[%rd3145],%fd1788; }

	// end inline asm
	add.s64 	%rd3146, %rd2812, 2672;
	// begin inline asm
	{ atom.add.f64 %fd7005,[%rd3146],%fd1787; }

	// end inline asm
	add.s64 	%rd3147, %rd2812, 2680;
	// begin inline asm
	{ atom.add.f64 %fd7007,[%rd3147],%fd1786; }

	// end inline asm
	add.s64 	%rd3148, %rd2812, 2688;
	// begin inline asm
	{ atom.add.f64 %fd7009,[%rd3148],%fd1002; }

	// end inline asm
	add.s64 	%rd3149, %rd2812, 2696;
	// begin inline asm
	{ atom.add.f64 %fd7011,[%rd3149],%fd1001; }

	// end inline asm
	add.s64 	%rd3150, %rd2812, 2704;
	// begin inline asm
	{ atom.add.f64 %fd7013,[%rd3150],%fd1000; }

	// end inline asm
	add.s64 	%rd3151, %rd2812, 2712;
	// begin inline asm
	{ atom.add.f64 %fd7015,[%rd3151],%fd975; }

	// end inline asm
	add.s64 	%rd3152, %rd2812, 2720;
	// begin inline asm
	{ atom.add.f64 %fd7017,[%rd3152],%fd974; }

	// end inline asm
	add.s64 	%rd3153, %rd2812, 2728;
	// begin inline asm
	{ atom.add.f64 %fd7019,[%rd3153],%fd973; }

	// end inline asm
	add.s64 	%rd3154, %rd2812, 2736;
	// begin inline asm
	{ atom.add.f64 %fd7021,[%rd3154],%fd948; }

	// end inline asm
	add.s64 	%rd3155, %rd2812, 2744;
	// begin inline asm
	{ atom.add.f64 %fd7023,[%rd3155],%fd947; }

	// end inline asm
	add.s64 	%rd3156, %rd2812, 2752;
	// begin inline asm
	{ atom.add.f64 %fd7025,[%rd3156],%fd946; }

	// end inline asm
	add.s64 	%rd3157, %rd2812, 2760;
	// begin inline asm
	{ atom.add.f64 %fd7027,[%rd3157],%fd921; }

	// end inline asm
	add.s64 	%rd3158, %rd2812, 2768;
	// begin inline asm
	{ atom.add.f64 %fd7029,[%rd3158],%fd920; }

	// end inline asm
	add.s64 	%rd3159, %rd2812, 2776;
	// begin inline asm
	{ atom.add.f64 %fd7031,[%rd3159],%fd919; }

	// end inline asm
	add.s64 	%rd3160, %rd2812, 2784;
	// begin inline asm
	{ atom.add.f64 %fd7033,[%rd3160],%fd1866; }

	// end inline asm
	add.s64 	%rd3161, %rd2812, 2792;
	// begin inline asm
	{ atom.add.f64 %fd7035,[%rd3161],%fd1865; }

	// end inline asm
	add.s64 	%rd3162, %rd2812, 2800;
	// begin inline asm
	{ atom.add.f64 %fd7037,[%rd3162],%fd1864; }

	// end inline asm
	add.s64 	%rd3163, %rd2812, 2808;
	// begin inline asm
	{ atom.add.f64 %fd7039,[%rd3163],%fd1839; }

	// end inline asm
	add.s64 	%rd3164, %rd2812, 2816;
	// begin inline asm
	{ atom.add.f64 %fd7041,[%rd3164],%fd1838; }

	// end inline asm
	add.s64 	%rd3165, %rd2812, 2824;
	// begin inline asm
	{ atom.add.f64 %fd7043,[%rd3165],%fd1837; }

	// end inline asm
	add.s64 	%rd3166, %rd2812, 2832;
	// begin inline asm
	{ atom.add.f64 %fd7045,[%rd3166],%fd1812; }

	// end inline asm
	add.s64 	%rd3167, %rd2812, 2840;
	// begin inline asm
	{ atom.add.f64 %fd7047,[%rd3167],%fd1811; }

	// end inline asm
	add.s64 	%rd3168, %rd2812, 2848;
	// begin inline asm
	{ atom.add.f64 %fd7049,[%rd3168],%fd1810; }

	// end inline asm
	add.s64 	%rd3169, %rd2812, 2856;
	// begin inline asm
	{ atom.add.f64 %fd7051,[%rd3169],%fd1785; }

	// end inline asm
	add.s64 	%rd3170, %rd2812, 2864;
	// begin inline asm
	{ atom.add.f64 %fd7053,[%rd3170],%fd1784; }

	// end inline asm
	add.s64 	%rd3171, %rd2812, 2872;
	// begin inline asm
	{ atom.add.f64 %fd7055,[%rd3171],%fd1783; }

	// end inline asm
	add.s64 	%rd3172, %rd2812, 2880;
	// begin inline asm
	{ atom.add.f64 %fd7057,[%rd3172],%fd900; }

	// end inline asm
	add.s64 	%rd3173, %rd2812, 2888;
	// begin inline asm
	{ atom.add.f64 %fd7059,[%rd3173],%fd899; }

	// end inline asm
	add.s64 	%rd3174, %rd2812, 2896;
	// begin inline asm
	{ atom.add.f64 %fd7061,[%rd3174],%fd898; }

	// end inline asm
	add.s64 	%rd3175, %rd2812, 2904;
	// begin inline asm
	{ atom.add.f64 %fd7063,[%rd3175],%fd873; }

	// end inline asm
	add.s64 	%rd3176, %rd2812, 2912;
	// begin inline asm
	{ atom.add.f64 %fd7065,[%rd3176],%fd872; }

	// end inline asm
	add.s64 	%rd3177, %rd2812, 2920;
	// begin inline asm
	{ atom.add.f64 %fd7067,[%rd3177],%fd871; }

	// end inline asm
	add.s64 	%rd3178, %rd2812, 2928;
	// begin inline asm
	{ atom.add.f64 %fd7069,[%rd3178],%fd846; }

	// end inline asm
	add.s64 	%rd3179, %rd2812, 2936;
	// begin inline asm
	{ atom.add.f64 %fd7071,[%rd3179],%fd845; }

	// end inline asm
	add.s64 	%rd3180, %rd2812, 2944;
	// begin inline asm
	{ atom.add.f64 %fd7073,[%rd3180],%fd844; }

	// end inline asm
	add.s64 	%rd3181, %rd2812, 2952;
	// begin inline asm
	{ atom.add.f64 %fd7075,[%rd3181],%fd819; }

	// end inline asm
	add.s64 	%rd3182, %rd2812, 2960;
	// begin inline asm
	{ atom.add.f64 %fd7077,[%rd3182],%fd818; }

	// end inline asm
	add.s64 	%rd3183, %rd2812, 2968;
	// begin inline asm
	{ atom.add.f64 %fd7079,[%rd3183],%fd817; }

	// end inline asm
	add.s64 	%rd3184, %rd2812, 2976;
	// begin inline asm
	{ atom.add.f64 %fd7081,[%rd3184],%fd1764; }

	// end inline asm
	add.s64 	%rd3185, %rd2812, 2984;
	// begin inline asm
	{ atom.add.f64 %fd7083,[%rd3185],%fd1763; }

	// end inline asm
	add.s64 	%rd3186, %rd2812, 2992;
	// begin inline asm
	{ atom.add.f64 %fd7085,[%rd3186],%fd1762; }

	// end inline asm
	add.s64 	%rd3187, %rd2812, 3000;
	// begin inline asm
	{ atom.add.f64 %fd7087,[%rd3187],%fd1737; }

	// end inline asm
	add.s64 	%rd3188, %rd2812, 3008;
	// begin inline asm
	{ atom.add.f64 %fd7089,[%rd3188],%fd1736; }

	// end inline asm
	add.s64 	%rd3189, %rd2812, 3016;
	// begin inline asm
	{ atom.add.f64 %fd7091,[%rd3189],%fd1735; }

	// end inline asm
	add.s64 	%rd3190, %rd2812, 3024;
	// begin inline asm
	{ atom.add.f64 %fd7093,[%rd3190],%fd1710; }

	// end inline asm
	add.s64 	%rd3191, %rd2812, 3032;
	// begin inline asm
	{ atom.add.f64 %fd7095,[%rd3191],%fd1709; }

	// end inline asm
	add.s64 	%rd3192, %rd2812, 3040;
	// begin inline asm
	{ atom.add.f64 %fd7097,[%rd3192],%fd1708; }

	// end inline asm
	add.s64 	%rd3193, %rd2812, 3048;
	// begin inline asm
	{ atom.add.f64 %fd7099,[%rd3193],%fd1683; }

	// end inline asm
	add.s64 	%rd3194, %rd2812, 3056;
	// begin inline asm
	{ atom.add.f64 %fd7101,[%rd3194],%fd1682; }

	// end inline asm
	add.s64 	%rd3195, %rd2812, 3064;
	// begin inline asm
	{ atom.add.f64 %fd7103,[%rd3195],%fd1681; }

	// end inline asm
	add.s64 	%rd3196, %rd2812, 3072;
	// begin inline asm
	{ atom.add.f64 %fd7105,[%rd3196],%fd897; }

	// end inline asm
	add.s64 	%rd3197, %rd2812, 3080;
	// begin inline asm
	{ atom.add.f64 %fd7107,[%rd3197],%fd896; }

	// end inline asm
	add.s64 	%rd3198, %rd2812, 3088;
	// begin inline asm
	{ atom.add.f64 %fd7109,[%rd3198],%fd895; }

	// end inline asm
	add.s64 	%rd3199, %rd2812, 3096;
	// begin inline asm
	{ atom.add.f64 %fd7111,[%rd3199],%fd870; }

	// end inline asm
	add.s64 	%rd3200, %rd2812, 3104;
	// begin inline asm
	{ atom.add.f64 %fd7113,[%rd3200],%fd869; }

	// end inline asm
	add.s64 	%rd3201, %rd2812, 3112;
	// begin inline asm
	{ atom.add.f64 %fd7115,[%rd3201],%fd868; }

	// end inline asm
	add.s64 	%rd3202, %rd2812, 3120;
	// begin inline asm
	{ atom.add.f64 %fd7117,[%rd3202],%fd843; }

	// end inline asm
	add.s64 	%rd3203, %rd2812, 3128;
	// begin inline asm
	{ atom.add.f64 %fd7119,[%rd3203],%fd842; }

	// end inline asm
	add.s64 	%rd3204, %rd2812, 3136;
	// begin inline asm
	{ atom.add.f64 %fd7121,[%rd3204],%fd841; }

	// end inline asm
	add.s64 	%rd3205, %rd2812, 3144;
	// begin inline asm
	{ atom.add.f64 %fd7123,[%rd3205],%fd816; }

	// end inline asm
	add.s64 	%rd3206, %rd2812, 3152;
	// begin inline asm
	{ atom.add.f64 %fd7125,[%rd3206],%fd815; }

	// end inline asm
	add.s64 	%rd3207, %rd2812, 3160;
	// begin inline asm
	{ atom.add.f64 %fd7127,[%rd3207],%fd814; }

	// end inline asm
	add.s64 	%rd3208, %rd2812, 3168;
	// begin inline asm
	{ atom.add.f64 %fd7129,[%rd3208],%fd1761; }

	// end inline asm
	add.s64 	%rd3209, %rd2812, 3176;
	// begin inline asm
	{ atom.add.f64 %fd7131,[%rd3209],%fd1760; }

	// end inline asm
	add.s64 	%rd3210, %rd2812, 3184;
	// begin inline asm
	{ atom.add.f64 %fd7133,[%rd3210],%fd1759; }

	// end inline asm
	add.s64 	%rd3211, %rd2812, 3192;
	// begin inline asm
	{ atom.add.f64 %fd7135,[%rd3211],%fd1734; }

	// end inline asm
	add.s64 	%rd3212, %rd2812, 3200;
	// begin inline asm
	{ atom.add.f64 %fd7137,[%rd3212],%fd1733; }

	// end inline asm
	add.s64 	%rd3213, %rd2812, 3208;
	// begin inline asm
	{ atom.add.f64 %fd7139,[%rd3213],%fd1732; }

	// end inline asm
	add.s64 	%rd3214, %rd2812, 3216;
	// begin inline asm
	{ atom.add.f64 %fd7141,[%rd3214],%fd1707; }

	// end inline asm
	add.s64 	%rd3215, %rd2812, 3224;
	// begin inline asm
	{ atom.add.f64 %fd7143,[%rd3215],%fd1706; }

	// end inline asm
	add.s64 	%rd3216, %rd2812, 3232;
	// begin inline asm
	{ atom.add.f64 %fd7145,[%rd3216],%fd1705; }

	// end inline asm
	add.s64 	%rd3217, %rd2812, 3240;
	// begin inline asm
	{ atom.add.f64 %fd7147,[%rd3217],%fd1680; }

	// end inline asm
	add.s64 	%rd3218, %rd2812, 3248;
	// begin inline asm
	{ atom.add.f64 %fd7149,[%rd3218],%fd1679; }

	// end inline asm
	add.s64 	%rd3219, %rd2812, 3256;
	// begin inline asm
	{ atom.add.f64 %fd7151,[%rd3219],%fd1678; }

	// end inline asm
	add.s64 	%rd3220, %rd2812, 3264;
	// begin inline asm
	{ atom.add.f64 %fd7153,[%rd3220],%fd894; }

	// end inline asm
	add.s64 	%rd3221, %rd2812, 3272;
	// begin inline asm
	{ atom.add.f64 %fd7155,[%rd3221],%fd893; }

	// end inline asm
	add.s64 	%rd3222, %rd2812, 3280;
	// begin inline asm
	{ atom.add.f64 %fd7157,[%rd3222],%fd892; }

	// end inline asm
	add.s64 	%rd3223, %rd2812, 3288;
	// begin inline asm
	{ atom.add.f64 %fd7159,[%rd3223],%fd867; }

	// end inline asm
	add.s64 	%rd3224, %rd2812, 3296;
	// begin inline asm
	{ atom.add.f64 %fd7161,[%rd3224],%fd866; }

	// end inline asm
	add.s64 	%rd3225, %rd2812, 3304;
	// begin inline asm
	{ atom.add.f64 %fd7163,[%rd3225],%fd865; }

	// end inline asm
	add.s64 	%rd3226, %rd2812, 3312;
	// begin inline asm
	{ atom.add.f64 %fd7165,[%rd3226],%fd840; }

	// end inline asm
	add.s64 	%rd3227, %rd2812, 3320;
	// begin inline asm
	{ atom.add.f64 %fd7167,[%rd3227],%fd839; }

	// end inline asm
	add.s64 	%rd3228, %rd2812, 3328;
	// begin inline asm
	{ atom.add.f64 %fd7169,[%rd3228],%fd838; }

	// end inline asm
	add.s64 	%rd3229, %rd2812, 3336;
	// begin inline asm
	{ atom.add.f64 %fd7171,[%rd3229],%fd813; }

	// end inline asm
	add.s64 	%rd3230, %rd2812, 3344;
	// begin inline asm
	{ atom.add.f64 %fd7173,[%rd3230],%fd812; }

	// end inline asm
	add.s64 	%rd3231, %rd2812, 3352;
	// begin inline asm
	{ atom.add.f64 %fd7175,[%rd3231],%fd811; }

	// end inline asm
	add.s64 	%rd3232, %rd2812, 3360;
	// begin inline asm
	{ atom.add.f64 %fd7177,[%rd3232],%fd1758; }

	// end inline asm
	add.s64 	%rd3233, %rd2812, 3368;
	// begin inline asm
	{ atom.add.f64 %fd7179,[%rd3233],%fd1757; }

	// end inline asm
	add.s64 	%rd3234, %rd2812, 3376;
	// begin inline asm
	{ atom.add.f64 %fd7181,[%rd3234],%fd1756; }

	// end inline asm
	add.s64 	%rd3235, %rd2812, 3384;
	// begin inline asm
	{ atom.add.f64 %fd7183,[%rd3235],%fd1731; }

	// end inline asm
	add.s64 	%rd3236, %rd2812, 3392;
	// begin inline asm
	{ atom.add.f64 %fd7185,[%rd3236],%fd1730; }

	// end inline asm
	add.s64 	%rd3237, %rd2812, 3400;
	// begin inline asm
	{ atom.add.f64 %fd7187,[%rd3237],%fd1729; }

	// end inline asm
	add.s64 	%rd3238, %rd2812, 3408;
	// begin inline asm
	{ atom.add.f64 %fd7189,[%rd3238],%fd1704; }

	// end inline asm
	add.s64 	%rd3239, %rd2812, 3416;
	// begin inline asm
	{ atom.add.f64 %fd7191,[%rd3239],%fd1703; }

	// end inline asm
	add.s64 	%rd3240, %rd2812, 3424;
	// begin inline asm
	{ atom.add.f64 %fd7193,[%rd3240],%fd1702; }

	// end inline asm
	add.s64 	%rd3241, %rd2812, 3432;
	// begin inline asm
	{ atom.add.f64 %fd7195,[%rd3241],%fd1677; }

	// end inline asm
	add.s64 	%rd3242, %rd2812, 3440;
	// begin inline asm
	{ atom.add.f64 %fd7197,[%rd3242],%fd1676; }

	// end inline asm
	add.s64 	%rd3243, %rd2812, 3448;
	// begin inline asm
	{ atom.add.f64 %fd7199,[%rd3243],%fd1675; }

	// end inline asm
	add.s64 	%rd3244, %rd2812, 3456;
	// begin inline asm
	{ atom.add.f64 %fd7201,[%rd3244],%fd792; }

	// end inline asm
	add.s64 	%rd3245, %rd2812, 3464;
	// begin inline asm
	{ atom.add.f64 %fd7203,[%rd3245],%fd791; }

	// end inline asm
	add.s64 	%rd3246, %rd2812, 3472;
	// begin inline asm
	{ atom.add.f64 %fd7205,[%rd3246],%fd790; }

	// end inline asm
	add.s64 	%rd3247, %rd2812, 3480;
	// begin inline asm
	{ atom.add.f64 %fd7207,[%rd3247],%fd765; }

	// end inline asm
	add.s64 	%rd3248, %rd2812, 3488;
	// begin inline asm
	{ atom.add.f64 %fd7209,[%rd3248],%fd764; }

	// end inline asm
	add.s64 	%rd3249, %rd2812, 3496;
	// begin inline asm
	{ atom.add.f64 %fd7211,[%rd3249],%fd763; }

	// end inline asm
	add.s64 	%rd3250, %rd2812, 3504;
	// begin inline asm
	{ atom.add.f64 %fd7213,[%rd3250],%fd738; }

	// end inline asm
	add.s64 	%rd3251, %rd2812, 3512;
	// begin inline asm
	{ atom.add.f64 %fd7215,[%rd3251],%fd737; }

	// end inline asm
	add.s64 	%rd3252, %rd2812, 3520;
	// begin inline asm
	{ atom.add.f64 %fd7217,[%rd3252],%fd736; }

	// end inline asm
	add.s64 	%rd3253, %rd2812, 3528;
	// begin inline asm
	{ atom.add.f64 %fd7219,[%rd3253],%fd711; }

	// end inline asm
	add.s64 	%rd3254, %rd2812, 3536;
	// begin inline asm
	{ atom.add.f64 %fd7221,[%rd3254],%fd710; }

	// end inline asm
	add.s64 	%rd3255, %rd2812, 3544;
	// begin inline asm
	{ atom.add.f64 %fd7223,[%rd3255],%fd709; }

	// end inline asm
	add.s64 	%rd3256, %rd2812, 3552;
	// begin inline asm
	{ atom.add.f64 %fd7225,[%rd3256],%fd1656; }

	// end inline asm
	add.s64 	%rd3257, %rd2812, 3560;
	// begin inline asm
	{ atom.add.f64 %fd7227,[%rd3257],%fd1655; }

	// end inline asm
	add.s64 	%rd3258, %rd2812, 3568;
	// begin inline asm
	{ atom.add.f64 %fd7229,[%rd3258],%fd1654; }

	// end inline asm
	add.s64 	%rd3259, %rd2812, 3576;
	// begin inline asm
	{ atom.add.f64 %fd7231,[%rd3259],%fd1629; }

	// end inline asm
	add.s64 	%rd3260, %rd2812, 3584;
	// begin inline asm
	{ atom.add.f64 %fd7233,[%rd3260],%fd1628; }

	// end inline asm
	add.s64 	%rd3261, %rd2812, 3592;
	// begin inline asm
	{ atom.add.f64 %fd7235,[%rd3261],%fd1627; }

	// end inline asm
	add.s64 	%rd3262, %rd2812, 3600;
	// begin inline asm
	{ atom.add.f64 %fd7237,[%rd3262],%fd1602; }

	// end inline asm
	add.s64 	%rd3263, %rd2812, 3608;
	// begin inline asm
	{ atom.add.f64 %fd7239,[%rd3263],%fd1601; }

	// end inline asm
	add.s64 	%rd3264, %rd2812, 3616;
	// begin inline asm
	{ atom.add.f64 %fd7241,[%rd3264],%fd1600; }

	// end inline asm
	add.s64 	%rd3265, %rd2812, 3624;
	// begin inline asm
	{ atom.add.f64 %fd7243,[%rd3265],%fd1575; }

	// end inline asm
	add.s64 	%rd3266, %rd2812, 3632;
	// begin inline asm
	{ atom.add.f64 %fd7245,[%rd3266],%fd1574; }

	// end inline asm
	add.s64 	%rd3267, %rd2812, 3640;
	// begin inline asm
	{ atom.add.f64 %fd7247,[%rd3267],%fd1573; }

	// end inline asm
	add.s64 	%rd3268, %rd2812, 3648;
	// begin inline asm
	{ atom.add.f64 %fd7249,[%rd3268],%fd789; }

	// end inline asm
	add.s64 	%rd3269, %rd2812, 3656;
	// begin inline asm
	{ atom.add.f64 %fd7251,[%rd3269],%fd788; }

	// end inline asm
	add.s64 	%rd3270, %rd2812, 3664;
	// begin inline asm
	{ atom.add.f64 %fd7253,[%rd3270],%fd787; }

	// end inline asm
	add.s64 	%rd3271, %rd2812, 3672;
	// begin inline asm
	{ atom.add.f64 %fd7255,[%rd3271],%fd762; }

	// end inline asm
	add.s64 	%rd3272, %rd2812, 3680;
	// begin inline asm
	{ atom.add.f64 %fd7257,[%rd3272],%fd761; }

	// end inline asm
	add.s64 	%rd3273, %rd2812, 3688;
	// begin inline asm
	{ atom.add.f64 %fd7259,[%rd3273],%fd760; }

	// end inline asm
	add.s64 	%rd3274, %rd2812, 3696;
	// begin inline asm
	{ atom.add.f64 %fd7261,[%rd3274],%fd735; }

	// end inline asm
	add.s64 	%rd3275, %rd2812, 3704;
	// begin inline asm
	{ atom.add.f64 %fd7263,[%rd3275],%fd734; }

	// end inline asm
	add.s64 	%rd3276, %rd2812, 3712;
	// begin inline asm
	{ atom.add.f64 %fd7265,[%rd3276],%fd733; }

	// end inline asm
	add.s64 	%rd3277, %rd2812, 3720;
	// begin inline asm
	{ atom.add.f64 %fd7267,[%rd3277],%fd708; }

	// end inline asm
	add.s64 	%rd3278, %rd2812, 3728;
	// begin inline asm
	{ atom.add.f64 %fd7269,[%rd3278],%fd707; }

	// end inline asm
	add.s64 	%rd3279, %rd2812, 3736;
	// begin inline asm
	{ atom.add.f64 %fd7271,[%rd3279],%fd706; }

	// end inline asm
	add.s64 	%rd3280, %rd2812, 3744;
	// begin inline asm
	{ atom.add.f64 %fd7273,[%rd3280],%fd1653; }

	// end inline asm
	add.s64 	%rd3281, %rd2812, 3752;
	// begin inline asm
	{ atom.add.f64 %fd7275,[%rd3281],%fd1652; }

	// end inline asm
	add.s64 	%rd3282, %rd2812, 3760;
	// begin inline asm
	{ atom.add.f64 %fd7277,[%rd3282],%fd1651; }

	// end inline asm
	add.s64 	%rd3283, %rd2812, 3768;
	// begin inline asm
	{ atom.add.f64 %fd7279,[%rd3283],%fd1626; }

	// end inline asm
	add.s64 	%rd3284, %rd2812, 3776;
	// begin inline asm
	{ atom.add.f64 %fd7281,[%rd3284],%fd1625; }

	// end inline asm
	add.s64 	%rd3285, %rd2812, 3784;
	// begin inline asm
	{ atom.add.f64 %fd7283,[%rd3285],%fd1624; }

	// end inline asm
	add.s64 	%rd3286, %rd2812, 3792;
	// begin inline asm
	{ atom.add.f64 %fd7285,[%rd3286],%fd1599; }

	// end inline asm
	add.s64 	%rd3287, %rd2812, 3800;
	// begin inline asm
	{ atom.add.f64 %fd7287,[%rd3287],%fd1598; }

	// end inline asm
	add.s64 	%rd3288, %rd2812, 3808;
	// begin inline asm
	{ atom.add.f64 %fd7289,[%rd3288],%fd1597; }

	// end inline asm
	add.s64 	%rd3289, %rd2812, 3816;
	// begin inline asm
	{ atom.add.f64 %fd7291,[%rd3289],%fd1572; }

	// end inline asm
	add.s64 	%rd3290, %rd2812, 3824;
	// begin inline asm
	{ atom.add.f64 %fd7293,[%rd3290],%fd1571; }

	// end inline asm
	add.s64 	%rd3291, %rd2812, 3832;
	// begin inline asm
	{ atom.add.f64 %fd7295,[%rd3291],%fd1570; }

	// end inline asm
	add.s64 	%rd3292, %rd2812, 3840;
	// begin inline asm
	{ atom.add.f64 %fd7297,[%rd3292],%fd786; }

	// end inline asm
	add.s64 	%rd3293, %rd2812, 3848;
	// begin inline asm
	{ atom.add.f64 %fd7299,[%rd3293],%fd785; }

	// end inline asm
	add.s64 	%rd3294, %rd2812, 3856;
	// begin inline asm
	{ atom.add.f64 %fd7301,[%rd3294],%fd784; }

	// end inline asm
	add.s64 	%rd3295, %rd2812, 3864;
	// begin inline asm
	{ atom.add.f64 %fd7303,[%rd3295],%fd759; }

	// end inline asm
	add.s64 	%rd3296, %rd2812, 3872;
	// begin inline asm
	{ atom.add.f64 %fd7305,[%rd3296],%fd758; }

	// end inline asm
	add.s64 	%rd3297, %rd2812, 3880;
	// begin inline asm
	{ atom.add.f64 %fd7307,[%rd3297],%fd757; }

	// end inline asm
	add.s64 	%rd3298, %rd2812, 3888;
	// begin inline asm
	{ atom.add.f64 %fd7309,[%rd3298],%fd732; }

	// end inline asm
	add.s64 	%rd3299, %rd2812, 3896;
	// begin inline asm
	{ atom.add.f64 %fd7311,[%rd3299],%fd731; }

	// end inline asm
	add.s64 	%rd3300, %rd2812, 3904;
	// begin inline asm
	{ atom.add.f64 %fd7313,[%rd3300],%fd730; }

	// end inline asm
	add.s64 	%rd3301, %rd2812, 3912;
	// begin inline asm
	{ atom.add.f64 %fd7315,[%rd3301],%fd705; }

	// end inline asm
	add.s64 	%rd3302, %rd2812, 3920;
	// begin inline asm
	{ atom.add.f64 %fd7317,[%rd3302],%fd704; }

	// end inline asm
	add.s64 	%rd3303, %rd2812, 3928;
	// begin inline asm
	{ atom.add.f64 %fd7319,[%rd3303],%fd703; }

	// end inline asm
	add.s64 	%rd3304, %rd2812, 3936;
	// begin inline asm
	{ atom.add.f64 %fd7321,[%rd3304],%fd1650; }

	// end inline asm
	add.s64 	%rd3305, %rd2812, 3944;
	// begin inline asm
	{ atom.add.f64 %fd7323,[%rd3305],%fd1649; }

	// end inline asm
	add.s64 	%rd3306, %rd2812, 3952;
	// begin inline asm
	{ atom.add.f64 %fd7325,[%rd3306],%fd1648; }

	// end inline asm
	add.s64 	%rd3307, %rd2812, 3960;
	// begin inline asm
	{ atom.add.f64 %fd7327,[%rd3307],%fd1623; }

	// end inline asm
	add.s64 	%rd3308, %rd2812, 3968;
	// begin inline asm
	{ atom.add.f64 %fd7329,[%rd3308],%fd1622; }

	// end inline asm
	add.s64 	%rd3309, %rd2812, 3976;
	// begin inline asm
	{ atom.add.f64 %fd7331,[%rd3309],%fd1621; }

	// end inline asm
	add.s64 	%rd3310, %rd2812, 3984;
	// begin inline asm
	{ atom.add.f64 %fd7333,[%rd3310],%fd1596; }

	// end inline asm
	add.s64 	%rd3311, %rd2812, 3992;
	// begin inline asm
	{ atom.add.f64 %fd7335,[%rd3311],%fd1595; }

	// end inline asm
	add.s64 	%rd3312, %rd2812, 4000;
	// begin inline asm
	{ atom.add.f64 %fd7337,[%rd3312],%fd1594; }

	// end inline asm
	add.s64 	%rd3313, %rd2812, 4008;
	// begin inline asm
	{ atom.add.f64 %fd7339,[%rd3313],%fd1569; }

	// end inline asm
	add.s64 	%rd3314, %rd2812, 4016;
	// begin inline asm
	{ atom.add.f64 %fd7341,[%rd3314],%fd1568; }

	// end inline asm
	add.s64 	%rd3315, %rd2812, 4024;
	// begin inline asm
	{ atom.add.f64 %fd7343,[%rd3315],%fd1567; }

	// end inline asm
	add.s64 	%rd3316, %rd2812, 4032;
	// begin inline asm
	{ atom.add.f64 %fd7345,[%rd3316],%fd684; }

	// end inline asm
	add.s64 	%rd3317, %rd2812, 4040;
	// begin inline asm
	{ atom.add.f64 %fd7347,[%rd3317],%fd683; }

	// end inline asm
	add.s64 	%rd3318, %rd2812, 4048;
	// begin inline asm
	{ atom.add.f64 %fd7349,[%rd3318],%fd682; }

	// end inline asm
	add.s64 	%rd3319, %rd2812, 4056;
	// begin inline asm
	{ atom.add.f64 %fd7351,[%rd3319],%fd657; }

	// end inline asm
	add.s64 	%rd3320, %rd2812, 4064;
	// begin inline asm
	{ atom.add.f64 %fd7353,[%rd3320],%fd656; }

	// end inline asm
	add.s64 	%rd3321, %rd2812, 4072;
	// begin inline asm
	{ atom.add.f64 %fd7355,[%rd3321],%fd655; }

	// end inline asm
	add.s64 	%rd3322, %rd2812, 4080;
	// begin inline asm
	{ atom.add.f64 %fd7357,[%rd3322],%fd630; }

	// end inline asm
	add.s64 	%rd3323, %rd2812, 4088;
	// begin inline asm
	{ atom.add.f64 %fd7359,[%rd3323],%fd629; }

	// end inline asm
	add.s64 	%rd3324, %rd2812, 4096;
	// begin inline asm
	{ atom.add.f64 %fd7361,[%rd3324],%fd628; }

	// end inline asm
	add.s64 	%rd3325, %rd2812, 4104;
	// begin inline asm
	{ atom.add.f64 %fd7363,[%rd3325],%fd603; }

	// end inline asm
	add.s64 	%rd3326, %rd2812, 4112;
	// begin inline asm
	{ atom.add.f64 %fd7365,[%rd3326],%fd602; }

	// end inline asm
	add.s64 	%rd3327, %rd2812, 4120;
	// begin inline asm
	{ atom.add.f64 %fd7367,[%rd3327],%fd601; }

	// end inline asm
	add.s64 	%rd3328, %rd2812, 4128;
	// begin inline asm
	{ atom.add.f64 %fd7369,[%rd3328],%fd1548; }

	// end inline asm
	add.s64 	%rd3329, %rd2812, 4136;
	// begin inline asm
	{ atom.add.f64 %fd7371,[%rd3329],%fd1547; }

	// end inline asm
	add.s64 	%rd3330, %rd2812, 4144;
	// begin inline asm
	{ atom.add.f64 %fd7373,[%rd3330],%fd1546; }

	// end inline asm
	add.s64 	%rd3331, %rd2812, 4152;
	// begin inline asm
	{ atom.add.f64 %fd7375,[%rd3331],%fd1521; }

	// end inline asm
	add.s64 	%rd3332, %rd2812, 4160;
	// begin inline asm
	{ atom.add.f64 %fd7377,[%rd3332],%fd1520; }

	// end inline asm
	add.s64 	%rd3333, %rd2812, 4168;
	// begin inline asm
	{ atom.add.f64 %fd7379,[%rd3333],%fd1519; }

	// end inline asm
	add.s64 	%rd3334, %rd2812, 4176;
	// begin inline asm
	{ atom.add.f64 %fd7381,[%rd3334],%fd1494; }

	// end inline asm
	add.s64 	%rd3335, %rd2812, 4184;
	// begin inline asm
	{ atom.add.f64 %fd7383,[%rd3335],%fd1493; }

	// end inline asm
	add.s64 	%rd3336, %rd2812, 4192;
	// begin inline asm
	{ atom.add.f64 %fd7385,[%rd3336],%fd1492; }

	// end inline asm
	add.s64 	%rd3337, %rd2812, 4200;
	// begin inline asm
	{ atom.add.f64 %fd7387,[%rd3337],%fd1467; }

	// end inline asm
	add.s64 	%rd3338, %rd2812, 4208;
	// begin inline asm
	{ atom.add.f64 %fd7389,[%rd3338],%fd1466; }

	// end inline asm
	add.s64 	%rd3339, %rd2812, 4216;
	// begin inline asm
	{ atom.add.f64 %fd7391,[%rd3339],%fd1465; }

	// end inline asm
	add.s64 	%rd3340, %rd2812, 4224;
	// begin inline asm
	{ atom.add.f64 %fd7393,[%rd3340],%fd681; }

	// end inline asm
	add.s64 	%rd3341, %rd2812, 4232;
	// begin inline asm
	{ atom.add.f64 %fd7395,[%rd3341],%fd680; }

	// end inline asm
	add.s64 	%rd3342, %rd2812, 4240;
	// begin inline asm
	{ atom.add.f64 %fd7397,[%rd3342],%fd679; }

	// end inline asm
	add.s64 	%rd3343, %rd2812, 4248;
	// begin inline asm
	{ atom.add.f64 %fd7399,[%rd3343],%fd654; }

	// end inline asm
	add.s64 	%rd3344, %rd2812, 4256;
	// begin inline asm
	{ atom.add.f64 %fd7401,[%rd3344],%fd653; }

	// end inline asm
	add.s64 	%rd3345, %rd2812, 4264;
	// begin inline asm
	{ atom.add.f64 %fd7403,[%rd3345],%fd652; }

	// end inline asm
	add.s64 	%rd3346, %rd2812, 4272;
	// begin inline asm
	{ atom.add.f64 %fd7405,[%rd3346],%fd627; }

	// end inline asm
	add.s64 	%rd3347, %rd2812, 4280;
	// begin inline asm
	{ atom.add.f64 %fd7407,[%rd3347],%fd626; }

	// end inline asm
	add.s64 	%rd3348, %rd2812, 4288;
	// begin inline asm
	{ atom.add.f64 %fd7409,[%rd3348],%fd625; }

	// end inline asm
	add.s64 	%rd3349, %rd2812, 4296;
	// begin inline asm
	{ atom.add.f64 %fd7411,[%rd3349],%fd600; }

	// end inline asm
	add.s64 	%rd3350, %rd2812, 4304;
	// begin inline asm
	{ atom.add.f64 %fd7413,[%rd3350],%fd599; }

	// end inline asm
	add.s64 	%rd3351, %rd2812, 4312;
	// begin inline asm
	{ atom.add.f64 %fd7415,[%rd3351],%fd598; }

	// end inline asm
	add.s64 	%rd3352, %rd2812, 4320;
	// begin inline asm
	{ atom.add.f64 %fd7417,[%rd3352],%fd1545; }

	// end inline asm
	add.s64 	%rd3353, %rd2812, 4328;
	// begin inline asm
	{ atom.add.f64 %fd7419,[%rd3353],%fd1544; }

	// end inline asm
	add.s64 	%rd3354, %rd2812, 4336;
	// begin inline asm
	{ atom.add.f64 %fd7421,[%rd3354],%fd1543; }

	// end inline asm
	add.s64 	%rd3355, %rd2812, 4344;
	// begin inline asm
	{ atom.add.f64 %fd7423,[%rd3355],%fd1518; }

	// end inline asm
	add.s64 	%rd3356, %rd2812, 4352;
	// begin inline asm
	{ atom.add.f64 %fd7425,[%rd3356],%fd1517; }

	// end inline asm
	add.s64 	%rd3357, %rd2812, 4360;
	// begin inline asm
	{ atom.add.f64 %fd7427,[%rd3357],%fd1516; }

	// end inline asm
	add.s64 	%rd3358, %rd2812, 4368;
	// begin inline asm
	{ atom.add.f64 %fd7429,[%rd3358],%fd1491; }

	// end inline asm
	add.s64 	%rd3359, %rd2812, 4376;
	// begin inline asm
	{ atom.add.f64 %fd7431,[%rd3359],%fd1490; }

	// end inline asm
	add.s64 	%rd3360, %rd2812, 4384;
	// begin inline asm
	{ atom.add.f64 %fd7433,[%rd3360],%fd1489; }

	// end inline asm
	add.s64 	%rd3361, %rd2812, 4392;
	// begin inline asm
	{ atom.add.f64 %fd7435,[%rd3361],%fd1464; }

	// end inline asm
	add.s64 	%rd3362, %rd2812, 4400;
	// begin inline asm
	{ atom.add.f64 %fd7437,[%rd3362],%fd1463; }

	// end inline asm
	add.s64 	%rd3363, %rd2812, 4408;
	// begin inline asm
	{ atom.add.f64 %fd7439,[%rd3363],%fd1462; }

	// end inline asm
	add.s64 	%rd3364, %rd2812, 4416;
	// begin inline asm
	{ atom.add.f64 %fd7441,[%rd3364],%fd678; }

	// end inline asm
	add.s64 	%rd3365, %rd2812, 4424;
	// begin inline asm
	{ atom.add.f64 %fd7443,[%rd3365],%fd677; }

	// end inline asm
	add.s64 	%rd3366, %rd2812, 4432;
	// begin inline asm
	{ atom.add.f64 %fd7445,[%rd3366],%fd676; }

	// end inline asm
	add.s64 	%rd3367, %rd2812, 4440;
	// begin inline asm
	{ atom.add.f64 %fd7447,[%rd3367],%fd651; }

	// end inline asm
	add.s64 	%rd3368, %rd2812, 4448;
	// begin inline asm
	{ atom.add.f64 %fd7449,[%rd3368],%fd650; }

	// end inline asm
	add.s64 	%rd3369, %rd2812, 4456;
	// begin inline asm
	{ atom.add.f64 %fd7451,[%rd3369],%fd649; }

	// end inline asm
	add.s64 	%rd3370, %rd2812, 4464;
	// begin inline asm
	{ atom.add.f64 %fd7453,[%rd3370],%fd624; }

	// end inline asm
	add.s64 	%rd3371, %rd2812, 4472;
	// begin inline asm
	{ atom.add.f64 %fd7455,[%rd3371],%fd623; }

	// end inline asm
	add.s64 	%rd3372, %rd2812, 4480;
	// begin inline asm
	{ atom.add.f64 %fd7457,[%rd3372],%fd622; }

	// end inline asm
	add.s64 	%rd3373, %rd2812, 4488;
	// begin inline asm
	{ atom.add.f64 %fd7459,[%rd3373],%fd597; }

	// end inline asm
	add.s64 	%rd3374, %rd2812, 4496;
	// begin inline asm
	{ atom.add.f64 %fd7461,[%rd3374],%fd596; }

	// end inline asm
	add.s64 	%rd3375, %rd2812, 4504;
	// begin inline asm
	{ atom.add.f64 %fd7463,[%rd3375],%fd595; }

	// end inline asm
	add.s64 	%rd3376, %rd2812, 4512;
	// begin inline asm
	{ atom.add.f64 %fd7465,[%rd3376],%fd1542; }

	// end inline asm
	add.s64 	%rd3377, %rd2812, 4520;
	// begin inline asm
	{ atom.add.f64 %fd7467,[%rd3377],%fd1541; }

	// end inline asm
	add.s64 	%rd3378, %rd2812, 4528;
	// begin inline asm
	{ atom.add.f64 %fd7469,[%rd3378],%fd1540; }

	// end inline asm
	add.s64 	%rd3379, %rd2812, 4536;
	// begin inline asm
	{ atom.add.f64 %fd7471,[%rd3379],%fd1515; }

	// end inline asm
	add.s64 	%rd3380, %rd2812, 4544;
	// begin inline asm
	{ atom.add.f64 %fd7473,[%rd3380],%fd1514; }

	// end inline asm
	add.s64 	%rd3381, %rd2812, 4552;
	// begin inline asm
	{ atom.add.f64 %fd7475,[%rd3381],%fd1513; }

	// end inline asm
	add.s64 	%rd3382, %rd2812, 4560;
	// begin inline asm
	{ atom.add.f64 %fd7477,[%rd3382],%fd1488; }

	// end inline asm
	add.s64 	%rd3383, %rd2812, 4568;
	// begin inline asm
	{ atom.add.f64 %fd7479,[%rd3383],%fd1487; }

	// end inline asm
	add.s64 	%rd3384, %rd2812, 4576;
	// begin inline asm
	{ atom.add.f64 %fd7481,[%rd3384],%fd1486; }

	// end inline asm
	add.s64 	%rd3385, %rd2812, 4584;
	// begin inline asm
	{ atom.add.f64 %fd7483,[%rd3385],%fd1461; }

	// end inline asm
	add.s64 	%rd3386, %rd2812, 4592;
	// begin inline asm
	{ atom.add.f64 %fd7485,[%rd3386],%fd1460; }

	// end inline asm
	add.s64 	%rd3387, %rd2812, 4600;
	// begin inline asm
	{ atom.add.f64 %fd7487,[%rd3387],%fd1459; }

	// end inline asm
	bra.uni 	$L__BB13_659;

$L__BB13_657:
	setp.eq.s64 	%p939, %rd363, 0;
	@%p939 bra 	$L__BB13_659;

	add.s64 	%rd3389, %rd363, %rd26;
	// begin inline asm
	{ atom.add.f64 %fd7489,[%rd3389],%fd2304; }

	// end inline asm
	add.s64 	%rd3390, %rd3389, 8;
	// begin inline asm
	{ atom.add.f64 %fd7491,[%rd3390],%fd2303; }

	// end inline asm
	add.s64 	%rd3391, %rd3389, 16;
	// begin inline asm
	{ atom.add.f64 %fd7493,[%rd3391],%fd2302; }

	// end inline asm
	add.s64 	%rd3392, %rd3389, 24;
	// begin inline asm
	{ atom.add.f64 %fd7495,[%rd3392],%fd2277; }

	// end inline asm
	add.s64 	%rd3393, %rd3389, 32;
	// begin inline asm
	{ atom.add.f64 %fd7497,[%rd3393],%fd2276; }

	// end inline asm
	add.s64 	%rd3394, %rd3389, 40;
	// begin inline asm
	{ atom.add.f64 %fd7499,[%rd3394],%fd2275; }

	// end inline asm
	add.s64 	%rd3395, %rd3389, 48;
	// begin inline asm
	{ atom.add.f64 %fd7501,[%rd3395],%fd2250; }

	// end inline asm
	add.s64 	%rd3396, %rd3389, 56;
	// begin inline asm
	{ atom.add.f64 %fd7503,[%rd3396],%fd2249; }

	// end inline asm
	add.s64 	%rd3397, %rd3389, 64;
	// begin inline asm
	{ atom.add.f64 %fd7505,[%rd3397],%fd2248; }

	// end inline asm
	add.s64 	%rd3398, %rd3389, 72;
	// begin inline asm
	{ atom.add.f64 %fd7507,[%rd3398],%fd2223; }

	// end inline asm
	add.s64 	%rd3399, %rd3389, 80;
	// begin inline asm
	{ atom.add.f64 %fd7509,[%rd3399],%fd2222; }

	// end inline asm
	add.s64 	%rd3400, %rd3389, 88;
	// begin inline asm
	{ atom.add.f64 %fd7511,[%rd3400],%fd2221; }

	// end inline asm
	add.s64 	%rd3401, %rd3389, 96;
	// begin inline asm
	{ atom.add.f64 %fd7513,[%rd3401],%fd1440; }

	// end inline asm
	add.s64 	%rd3402, %rd3389, 104;
	// begin inline asm
	{ atom.add.f64 %fd7515,[%rd3402],%fd1439; }

	// end inline asm
	add.s64 	%rd3403, %rd3389, 112;
	// begin inline asm
	{ atom.add.f64 %fd7517,[%rd3403],%fd1438; }

	// end inline asm
	add.s64 	%rd3404, %rd3389, 120;
	// begin inline asm
	{ atom.add.f64 %fd7519,[%rd3404],%fd1413; }

	// end inline asm
	add.s64 	%rd3405, %rd3389, 128;
	// begin inline asm
	{ atom.add.f64 %fd7521,[%rd3405],%fd1412; }

	// end inline asm
	add.s64 	%rd3406, %rd3389, 136;
	// begin inline asm
	{ atom.add.f64 %fd7523,[%rd3406],%fd1411; }

	// end inline asm
	add.s64 	%rd3407, %rd3389, 144;
	// begin inline asm
	{ atom.add.f64 %fd7525,[%rd3407],%fd1386; }

	// end inline asm
	add.s64 	%rd3408, %rd3389, 152;
	// begin inline asm
	{ atom.add.f64 %fd7527,[%rd3408],%fd1385; }

	// end inline asm
	add.s64 	%rd3409, %rd3389, 160;
	// begin inline asm
	{ atom.add.f64 %fd7529,[%rd3409],%fd1384; }

	// end inline asm
	add.s64 	%rd3410, %rd3389, 168;
	// begin inline asm
	{ atom.add.f64 %fd7531,[%rd3410],%fd1359; }

	// end inline asm
	add.s64 	%rd3411, %rd3389, 176;
	// begin inline asm
	{ atom.add.f64 %fd7533,[%rd3411],%fd1358; }

	// end inline asm
	add.s64 	%rd3412, %rd3389, 184;
	// begin inline asm
	{ atom.add.f64 %fd7535,[%rd3412],%fd1357; }

	// end inline asm
	add.s64 	%rd3413, %rd3389, 192;
	// begin inline asm
	{ atom.add.f64 %fd7537,[%rd3413],%fd2301; }

	// end inline asm
	add.s64 	%rd3414, %rd3389, 200;
	// begin inline asm
	{ atom.add.f64 %fd7539,[%rd3414],%fd2300; }

	// end inline asm
	add.s64 	%rd3415, %rd3389, 208;
	// begin inline asm
	{ atom.add.f64 %fd7541,[%rd3415],%fd2299; }

	// end inline asm
	add.s64 	%rd3416, %rd3389, 216;
	// begin inline asm
	{ atom.add.f64 %fd7543,[%rd3416],%fd2274; }

	// end inline asm
	add.s64 	%rd3417, %rd3389, 224;
	// begin inline asm
	{ atom.add.f64 %fd7545,[%rd3417],%fd2273; }

	// end inline asm
	add.s64 	%rd3418, %rd3389, 232;
	// begin inline asm
	{ atom.add.f64 %fd7547,[%rd3418],%fd2272; }

	// end inline asm
	add.s64 	%rd3419, %rd3389, 240;
	// begin inline asm
	{ atom.add.f64 %fd7549,[%rd3419],%fd2247; }

	// end inline asm
	add.s64 	%rd3420, %rd3389, 248;
	// begin inline asm
	{ atom.add.f64 %fd7551,[%rd3420],%fd2246; }

	// end inline asm
	add.s64 	%rd3421, %rd3389, 256;
	// begin inline asm
	{ atom.add.f64 %fd7553,[%rd3421],%fd2245; }

	// end inline asm
	add.s64 	%rd3422, %rd3389, 264;
	// begin inline asm
	{ atom.add.f64 %fd7555,[%rd3422],%fd2220; }

	// end inline asm
	add.s64 	%rd3423, %rd3389, 272;
	// begin inline asm
	{ atom.add.f64 %fd7557,[%rd3423],%fd2219; }

	// end inline asm
	add.s64 	%rd3424, %rd3389, 280;
	// begin inline asm
	{ atom.add.f64 %fd7559,[%rd3424],%fd2218; }

	// end inline asm
	add.s64 	%rd3425, %rd3389, 288;
	// begin inline asm
	{ atom.add.f64 %fd7561,[%rd3425],%fd1437; }

	// end inline asm
	add.s64 	%rd3426, %rd3389, 296;
	// begin inline asm
	{ atom.add.f64 %fd7563,[%rd3426],%fd1436; }

	// end inline asm
	add.s64 	%rd3427, %rd3389, 304;
	// begin inline asm
	{ atom.add.f64 %fd7565,[%rd3427],%fd1435; }

	// end inline asm
	add.s64 	%rd3428, %rd3389, 312;
	// begin inline asm
	{ atom.add.f64 %fd7567,[%rd3428],%fd1410; }

	// end inline asm
	add.s64 	%rd3429, %rd3389, 320;
	// begin inline asm
	{ atom.add.f64 %fd7569,[%rd3429],%fd1409; }

	// end inline asm
	add.s64 	%rd3430, %rd3389, 328;
	// begin inline asm
	{ atom.add.f64 %fd7571,[%rd3430],%fd1408; }

	// end inline asm
	add.s64 	%rd3431, %rd3389, 336;
	// begin inline asm
	{ atom.add.f64 %fd7573,[%rd3431],%fd1383; }

	// end inline asm
	add.s64 	%rd3432, %rd3389, 344;
	// begin inline asm
	{ atom.add.f64 %fd7575,[%rd3432],%fd1382; }

	// end inline asm
	add.s64 	%rd3433, %rd3389, 352;
	// begin inline asm
	{ atom.add.f64 %fd7577,[%rd3433],%fd1381; }

	// end inline asm
	add.s64 	%rd3434, %rd3389, 360;
	// begin inline asm
	{ atom.add.f64 %fd7579,[%rd3434],%fd1356; }

	// end inline asm
	add.s64 	%rd3435, %rd3389, 368;
	// begin inline asm
	{ atom.add.f64 %fd7581,[%rd3435],%fd1355; }

	// end inline asm
	add.s64 	%rd3436, %rd3389, 376;
	// begin inline asm
	{ atom.add.f64 %fd7583,[%rd3436],%fd1354; }

	// end inline asm
	add.s64 	%rd3437, %rd3389, 384;
	// begin inline asm
	{ atom.add.f64 %fd7585,[%rd3437],%fd2298; }

	// end inline asm
	add.s64 	%rd3438, %rd3389, 392;
	// begin inline asm
	{ atom.add.f64 %fd7587,[%rd3438],%fd2297; }

	// end inline asm
	add.s64 	%rd3439, %rd3389, 400;
	// begin inline asm
	{ atom.add.f64 %fd7589,[%rd3439],%fd2296; }

	// end inline asm
	add.s64 	%rd3440, %rd3389, 408;
	// begin inline asm
	{ atom.add.f64 %fd7591,[%rd3440],%fd2271; }

	// end inline asm
	add.s64 	%rd3441, %rd3389, 416;
	// begin inline asm
	{ atom.add.f64 %fd7593,[%rd3441],%fd2270; }

	// end inline asm
	add.s64 	%rd3442, %rd3389, 424;
	// begin inline asm
	{ atom.add.f64 %fd7595,[%rd3442],%fd2269; }

	// end inline asm
	add.s64 	%rd3443, %rd3389, 432;
	// begin inline asm
	{ atom.add.f64 %fd7597,[%rd3443],%fd2244; }

	// end inline asm
	add.s64 	%rd3444, %rd3389, 440;
	// begin inline asm
	{ atom.add.f64 %fd7599,[%rd3444],%fd2243; }

	// end inline asm
	add.s64 	%rd3445, %rd3389, 448;
	// begin inline asm
	{ atom.add.f64 %fd7601,[%rd3445],%fd2242; }

	// end inline asm
	add.s64 	%rd3446, %rd3389, 456;
	// begin inline asm
	{ atom.add.f64 %fd7603,[%rd3446],%fd2217; }

	// end inline asm
	add.s64 	%rd3447, %rd3389, 464;
	// begin inline asm
	{ atom.add.f64 %fd7605,[%rd3447],%fd2216; }

	// end inline asm
	add.s64 	%rd3448, %rd3389, 472;
	// begin inline asm
	{ atom.add.f64 %fd7607,[%rd3448],%fd2215; }

	// end inline asm
	add.s64 	%rd3449, %rd3389, 480;
	// begin inline asm
	{ atom.add.f64 %fd7609,[%rd3449],%fd1434; }

	// end inline asm
	add.s64 	%rd3450, %rd3389, 488;
	// begin inline asm
	{ atom.add.f64 %fd7611,[%rd3450],%fd1433; }

	// end inline asm
	add.s64 	%rd3451, %rd3389, 496;
	// begin inline asm
	{ atom.add.f64 %fd7613,[%rd3451],%fd1432; }

	// end inline asm
	add.s64 	%rd3452, %rd3389, 504;
	// begin inline asm
	{ atom.add.f64 %fd7615,[%rd3452],%fd1407; }

	// end inline asm
	add.s64 	%rd3453, %rd3389, 512;
	// begin inline asm
	{ atom.add.f64 %fd7617,[%rd3453],%fd1406; }

	// end inline asm
	add.s64 	%rd3454, %rd3389, 520;
	// begin inline asm
	{ atom.add.f64 %fd7619,[%rd3454],%fd1405; }

	// end inline asm
	add.s64 	%rd3455, %rd3389, 528;
	// begin inline asm
	{ atom.add.f64 %fd7621,[%rd3455],%fd1380; }

	// end inline asm
	add.s64 	%rd3456, %rd3389, 536;
	// begin inline asm
	{ atom.add.f64 %fd7623,[%rd3456],%fd1379; }

	// end inline asm
	add.s64 	%rd3457, %rd3389, 544;
	// begin inline asm
	{ atom.add.f64 %fd7625,[%rd3457],%fd1378; }

	// end inline asm
	add.s64 	%rd3458, %rd3389, 552;
	// begin inline asm
	{ atom.add.f64 %fd7627,[%rd3458],%fd1353; }

	// end inline asm
	add.s64 	%rd3459, %rd3389, 560;
	// begin inline asm
	{ atom.add.f64 %fd7629,[%rd3459],%fd1352; }

	// end inline asm
	add.s64 	%rd3460, %rd3389, 568;
	// begin inline asm
	{ atom.add.f64 %fd7631,[%rd3460],%fd1351; }

	// end inline asm
	add.s64 	%rd3461, %rd3389, 576;
	// begin inline asm
	{ atom.add.f64 %fd7633,[%rd3461],%fd2196; }

	// end inline asm
	add.s64 	%rd3462, %rd3389, 584;
	// begin inline asm
	{ atom.add.f64 %fd7635,[%rd3462],%fd2195; }

	// end inline asm
	add.s64 	%rd3463, %rd3389, 592;
	// begin inline asm
	{ atom.add.f64 %fd7637,[%rd3463],%fd2194; }

	// end inline asm
	add.s64 	%rd3464, %rd3389, 600;
	// begin inline asm
	{ atom.add.f64 %fd7639,[%rd3464],%fd2169; }

	// end inline asm
	add.s64 	%rd3465, %rd3389, 608;
	// begin inline asm
	{ atom.add.f64 %fd7641,[%rd3465],%fd2168; }

	// end inline asm
	add.s64 	%rd3466, %rd3389, 616;
	// begin inline asm
	{ atom.add.f64 %fd7643,[%rd3466],%fd2167; }

	// end inline asm
	add.s64 	%rd3467, %rd3389, 624;
	// begin inline asm
	{ atom.add.f64 %fd7645,[%rd3467],%fd2142; }

	// end inline asm
	add.s64 	%rd3468, %rd3389, 632;
	// begin inline asm
	{ atom.add.f64 %fd7647,[%rd3468],%fd2141; }

	// end inline asm
	add.s64 	%rd3469, %rd3389, 640;
	// begin inline asm
	{ atom.add.f64 %fd7649,[%rd3469],%fd2140; }

	// end inline asm
	add.s64 	%rd3470, %rd3389, 648;
	// begin inline asm
	{ atom.add.f64 %fd7651,[%rd3470],%fd2115; }

	// end inline asm
	add.s64 	%rd3471, %rd3389, 656;
	// begin inline asm
	{ atom.add.f64 %fd7653,[%rd3471],%fd2114; }

	// end inline asm
	add.s64 	%rd3472, %rd3389, 664;
	// begin inline asm
	{ atom.add.f64 %fd7655,[%rd3472],%fd2113; }

	// end inline asm
	add.s64 	%rd3473, %rd3389, 672;
	// begin inline asm
	{ atom.add.f64 %fd7657,[%rd3473],%fd1332; }

	// end inline asm
	add.s64 	%rd3474, %rd3389, 680;
	// begin inline asm
	{ atom.add.f64 %fd7659,[%rd3474],%fd1331; }

	// end inline asm
	add.s64 	%rd3475, %rd3389, 688;
	// begin inline asm
	{ atom.add.f64 %fd7661,[%rd3475],%fd1330; }

	// end inline asm
	add.s64 	%rd3476, %rd3389, 696;
	// begin inline asm
	{ atom.add.f64 %fd7663,[%rd3476],%fd1305; }

	// end inline asm
	add.s64 	%rd3477, %rd3389, 704;
	// begin inline asm
	{ atom.add.f64 %fd7665,[%rd3477],%fd1304; }

	// end inline asm
	add.s64 	%rd3478, %rd3389, 712;
	// begin inline asm
	{ atom.add.f64 %fd7667,[%rd3478],%fd1303; }

	// end inline asm
	add.s64 	%rd3479, %rd3389, 720;
	// begin inline asm
	{ atom.add.f64 %fd7669,[%rd3479],%fd1278; }

	// end inline asm
	add.s64 	%rd3480, %rd3389, 728;
	// begin inline asm
	{ atom.add.f64 %fd7671,[%rd3480],%fd1277; }

	// end inline asm
	add.s64 	%rd3481, %rd3389, 736;
	// begin inline asm
	{ atom.add.f64 %fd7673,[%rd3481],%fd1276; }

	// end inline asm
	add.s64 	%rd3482, %rd3389, 744;
	// begin inline asm
	{ atom.add.f64 %fd7675,[%rd3482],%fd1251; }

	// end inline asm
	add.s64 	%rd3483, %rd3389, 752;
	// begin inline asm
	{ atom.add.f64 %fd7677,[%rd3483],%fd1250; }

	// end inline asm
	add.s64 	%rd3484, %rd3389, 760;
	// begin inline asm
	{ atom.add.f64 %fd7679,[%rd3484],%fd1249; }

	// end inline asm
	add.s64 	%rd3485, %rd3389, 768;
	// begin inline asm
	{ atom.add.f64 %fd7681,[%rd3485],%fd2193; }

	// end inline asm
	add.s64 	%rd3486, %rd3389, 776;
	// begin inline asm
	{ atom.add.f64 %fd7683,[%rd3486],%fd2192; }

	// end inline asm
	add.s64 	%rd3487, %rd3389, 784;
	// begin inline asm
	{ atom.add.f64 %fd7685,[%rd3487],%fd2191; }

	// end inline asm
	add.s64 	%rd3488, %rd3389, 792;
	// begin inline asm
	{ atom.add.f64 %fd7687,[%rd3488],%fd2166; }

	// end inline asm
	add.s64 	%rd3489, %rd3389, 800;
	// begin inline asm
	{ atom.add.f64 %fd7689,[%rd3489],%fd2165; }

	// end inline asm
	add.s64 	%rd3490, %rd3389, 808;
	// begin inline asm
	{ atom.add.f64 %fd7691,[%rd3490],%fd2164; }

	// end inline asm
	add.s64 	%rd3491, %rd3389, 816;
	// begin inline asm
	{ atom.add.f64 %fd7693,[%rd3491],%fd2139; }

	// end inline asm
	add.s64 	%rd3492, %rd3389, 824;
	// begin inline asm
	{ atom.add.f64 %fd7695,[%rd3492],%fd2138; }

	// end inline asm
	add.s64 	%rd3493, %rd3389, 832;
	// begin inline asm
	{ atom.add.f64 %fd7697,[%rd3493],%fd2137; }

	// end inline asm
	add.s64 	%rd3494, %rd3389, 840;
	// begin inline asm
	{ atom.add.f64 %fd7699,[%rd3494],%fd2112; }

	// end inline asm
	add.s64 	%rd3495, %rd3389, 848;
	// begin inline asm
	{ atom.add.f64 %fd7701,[%rd3495],%fd2111; }

	// end inline asm
	add.s64 	%rd3496, %rd3389, 856;
	// begin inline asm
	{ atom.add.f64 %fd7703,[%rd3496],%fd2110; }

	// end inline asm
	add.s64 	%rd3497, %rd3389, 864;
	// begin inline asm
	{ atom.add.f64 %fd7705,[%rd3497],%fd1329; }

	// end inline asm
	add.s64 	%rd3498, %rd3389, 872;
	// begin inline asm
	{ atom.add.f64 %fd7707,[%rd3498],%fd1328; }

	// end inline asm
	add.s64 	%rd3499, %rd3389, 880;
	// begin inline asm
	{ atom.add.f64 %fd7709,[%rd3499],%fd1327; }

	// end inline asm
	add.s64 	%rd3500, %rd3389, 888;
	// begin inline asm
	{ atom.add.f64 %fd7711,[%rd3500],%fd1302; }

	// end inline asm
	add.s64 	%rd3501, %rd3389, 896;
	// begin inline asm
	{ atom.add.f64 %fd7713,[%rd3501],%fd1301; }

	// end inline asm
	add.s64 	%rd3502, %rd3389, 904;
	// begin inline asm
	{ atom.add.f64 %fd7715,[%rd3502],%fd1300; }

	// end inline asm
	add.s64 	%rd3503, %rd3389, 912;
	// begin inline asm
	{ atom.add.f64 %fd7717,[%rd3503],%fd1275; }

	// end inline asm
	add.s64 	%rd3504, %rd3389, 920;
	// begin inline asm
	{ atom.add.f64 %fd7719,[%rd3504],%fd1274; }

	// end inline asm
	add.s64 	%rd3505, %rd3389, 928;
	// begin inline asm
	{ atom.add.f64 %fd7721,[%rd3505],%fd1273; }

	// end inline asm
	add.s64 	%rd3506, %rd3389, 936;
	// begin inline asm
	{ atom.add.f64 %fd7723,[%rd3506],%fd1248; }

	// end inline asm
	add.s64 	%rd3507, %rd3389, 944;
	// begin inline asm
	{ atom.add.f64 %fd7725,[%rd3507],%fd1247; }

	// end inline asm
	add.s64 	%rd3508, %rd3389, 952;
	// begin inline asm
	{ atom.add.f64 %fd7727,[%rd3508],%fd1246; }

	// end inline asm
	add.s64 	%rd3509, %rd3389, 960;
	// begin inline asm
	{ atom.add.f64 %fd7729,[%rd3509],%fd2190; }

	// end inline asm
	add.s64 	%rd3510, %rd3389, 968;
	// begin inline asm
	{ atom.add.f64 %fd7731,[%rd3510],%fd2189; }

	// end inline asm
	add.s64 	%rd3511, %rd3389, 976;
	// begin inline asm
	{ atom.add.f64 %fd7733,[%rd3511],%fd2188; }

	// end inline asm
	add.s64 	%rd3512, %rd3389, 984;
	// begin inline asm
	{ atom.add.f64 %fd7735,[%rd3512],%fd2163; }

	// end inline asm
	add.s64 	%rd3513, %rd3389, 992;
	// begin inline asm
	{ atom.add.f64 %fd7737,[%rd3513],%fd2162; }

	// end inline asm
	add.s64 	%rd3514, %rd3389, 1000;
	// begin inline asm
	{ atom.add.f64 %fd7739,[%rd3514],%fd2161; }

	// end inline asm
	add.s64 	%rd3515, %rd3389, 1008;
	// begin inline asm
	{ atom.add.f64 %fd7741,[%rd3515],%fd2136; }

	// end inline asm
	add.s64 	%rd3516, %rd3389, 1016;
	// begin inline asm
	{ atom.add.f64 %fd7743,[%rd3516],%fd2135; }

	// end inline asm
	add.s64 	%rd3517, %rd3389, 1024;
	// begin inline asm
	{ atom.add.f64 %fd7745,[%rd3517],%fd2134; }

	// end inline asm
	add.s64 	%rd3518, %rd3389, 1032;
	// begin inline asm
	{ atom.add.f64 %fd7747,[%rd3518],%fd2109; }

	// end inline asm
	add.s64 	%rd3519, %rd3389, 1040;
	// begin inline asm
	{ atom.add.f64 %fd7749,[%rd3519],%fd2108; }

	// end inline asm
	add.s64 	%rd3520, %rd3389, 1048;
	// begin inline asm
	{ atom.add.f64 %fd7751,[%rd3520],%fd2107; }

	// end inline asm
	add.s64 	%rd3521, %rd3389, 1056;
	// begin inline asm
	{ atom.add.f64 %fd7753,[%rd3521],%fd1326; }

	// end inline asm
	add.s64 	%rd3522, %rd3389, 1064;
	// begin inline asm
	{ atom.add.f64 %fd7755,[%rd3522],%fd1325; }

	// end inline asm
	add.s64 	%rd3523, %rd3389, 1072;
	// begin inline asm
	{ atom.add.f64 %fd7757,[%rd3523],%fd1324; }

	// end inline asm
	add.s64 	%rd3524, %rd3389, 1080;
	// begin inline asm
	{ atom.add.f64 %fd7759,[%rd3524],%fd1299; }

	// end inline asm
	add.s64 	%rd3525, %rd3389, 1088;
	// begin inline asm
	{ atom.add.f64 %fd7761,[%rd3525],%fd1298; }

	// end inline asm
	add.s64 	%rd3526, %rd3389, 1096;
	// begin inline asm
	{ atom.add.f64 %fd7763,[%rd3526],%fd1297; }

	// end inline asm
	add.s64 	%rd3527, %rd3389, 1104;
	// begin inline asm
	{ atom.add.f64 %fd7765,[%rd3527],%fd1272; }

	// end inline asm
	add.s64 	%rd3528, %rd3389, 1112;
	// begin inline asm
	{ atom.add.f64 %fd7767,[%rd3528],%fd1271; }

	// end inline asm
	add.s64 	%rd3529, %rd3389, 1120;
	// begin inline asm
	{ atom.add.f64 %fd7769,[%rd3529],%fd1270; }

	// end inline asm
	add.s64 	%rd3530, %rd3389, 1128;
	// begin inline asm
	{ atom.add.f64 %fd7771,[%rd3530],%fd1245; }

	// end inline asm
	add.s64 	%rd3531, %rd3389, 1136;
	// begin inline asm
	{ atom.add.f64 %fd7773,[%rd3531],%fd1244; }

	// end inline asm
	add.s64 	%rd3532, %rd3389, 1144;
	// begin inline asm
	{ atom.add.f64 %fd7775,[%rd3532],%fd1243; }

	// end inline asm
	add.s64 	%rd3533, %rd3389, 1152;
	// begin inline asm
	{ atom.add.f64 %fd7777,[%rd3533],%fd2088; }

	// end inline asm
	add.s64 	%rd3534, %rd3389, 1160;
	// begin inline asm
	{ atom.add.f64 %fd7779,[%rd3534],%fd2087; }

	// end inline asm
	add.s64 	%rd3535, %rd3389, 1168;
	// begin inline asm
	{ atom.add.f64 %fd7781,[%rd3535],%fd2086; }

	// end inline asm
	add.s64 	%rd3536, %rd3389, 1176;
	// begin inline asm
	{ atom.add.f64 %fd7783,[%rd3536],%fd2061; }

	// end inline asm
	add.s64 	%rd3537, %rd3389, 1184;
	// begin inline asm
	{ atom.add.f64 %fd7785,[%rd3537],%fd2060; }

	// end inline asm
	add.s64 	%rd3538, %rd3389, 1192;
	// begin inline asm
	{ atom.add.f64 %fd7787,[%rd3538],%fd2059; }

	// end inline asm
	add.s64 	%rd3539, %rd3389, 1200;
	// begin inline asm
	{ atom.add.f64 %fd7789,[%rd3539],%fd2034; }

	// end inline asm
	add.s64 	%rd3540, %rd3389, 1208;
	// begin inline asm
	{ atom.add.f64 %fd7791,[%rd3540],%fd2033; }

	// end inline asm
	add.s64 	%rd3541, %rd3389, 1216;
	// begin inline asm
	{ atom.add.f64 %fd7793,[%rd3541],%fd2032; }

	// end inline asm
	add.s64 	%rd3542, %rd3389, 1224;
	// begin inline asm
	{ atom.add.f64 %fd7795,[%rd3542],%fd2007; }

	// end inline asm
	add.s64 	%rd3543, %rd3389, 1232;
	// begin inline asm
	{ atom.add.f64 %fd7797,[%rd3543],%fd2006; }

	// end inline asm
	add.s64 	%rd3544, %rd3389, 1240;
	// begin inline asm
	{ atom.add.f64 %fd7799,[%rd3544],%fd2005; }

	// end inline asm
	add.s64 	%rd3545, %rd3389, 1248;
	// begin inline asm
	{ atom.add.f64 %fd7801,[%rd3545],%fd1224; }

	// end inline asm
	add.s64 	%rd3546, %rd3389, 1256;
	// begin inline asm
	{ atom.add.f64 %fd7803,[%rd3546],%fd1223; }

	// end inline asm
	add.s64 	%rd3547, %rd3389, 1264;
	// begin inline asm
	{ atom.add.f64 %fd7805,[%rd3547],%fd1222; }

	// end inline asm
	add.s64 	%rd3548, %rd3389, 1272;
	// begin inline asm
	{ atom.add.f64 %fd7807,[%rd3548],%fd1197; }

	// end inline asm
	add.s64 	%rd3549, %rd3389, 1280;
	// begin inline asm
	{ atom.add.f64 %fd7809,[%rd3549],%fd1196; }

	// end inline asm
	add.s64 	%rd3550, %rd3389, 1288;
	// begin inline asm
	{ atom.add.f64 %fd7811,[%rd3550],%fd1195; }

	// end inline asm
	add.s64 	%rd3551, %rd3389, 1296;
	// begin inline asm
	{ atom.add.f64 %fd7813,[%rd3551],%fd1170; }

	// end inline asm
	add.s64 	%rd3552, %rd3389, 1304;
	// begin inline asm
	{ atom.add.f64 %fd7815,[%rd3552],%fd1169; }

	// end inline asm
	add.s64 	%rd3553, %rd3389, 1312;
	// begin inline asm
	{ atom.add.f64 %fd7817,[%rd3553],%fd1168; }

	// end inline asm
	add.s64 	%rd3554, %rd3389, 1320;
	// begin inline asm
	{ atom.add.f64 %fd7819,[%rd3554],%fd1143; }

	// end inline asm
	add.s64 	%rd3555, %rd3389, 1328;
	// begin inline asm
	{ atom.add.f64 %fd7821,[%rd3555],%fd1142; }

	// end inline asm
	add.s64 	%rd3556, %rd3389, 1336;
	// begin inline asm
	{ atom.add.f64 %fd7823,[%rd3556],%fd1141; }

	// end inline asm
	add.s64 	%rd3557, %rd3389, 1344;
	// begin inline asm
	{ atom.add.f64 %fd7825,[%rd3557],%fd2085; }

	// end inline asm
	add.s64 	%rd3558, %rd3389, 1352;
	// begin inline asm
	{ atom.add.f64 %fd7827,[%rd3558],%fd2084; }

	// end inline asm
	add.s64 	%rd3559, %rd3389, 1360;
	// begin inline asm
	{ atom.add.f64 %fd7829,[%rd3559],%fd2083; }

	// end inline asm
	add.s64 	%rd3560, %rd3389, 1368;
	// begin inline asm
	{ atom.add.f64 %fd7831,[%rd3560],%fd2058; }

	// end inline asm
	add.s64 	%rd3561, %rd3389, 1376;
	// begin inline asm
	{ atom.add.f64 %fd7833,[%rd3561],%fd2057; }

	// end inline asm
	add.s64 	%rd3562, %rd3389, 1384;
	// begin inline asm
	{ atom.add.f64 %fd7835,[%rd3562],%fd2056; }

	// end inline asm
	add.s64 	%rd3563, %rd3389, 1392;
	// begin inline asm
	{ atom.add.f64 %fd7837,[%rd3563],%fd2031; }

	// end inline asm
	add.s64 	%rd3564, %rd3389, 1400;
	// begin inline asm
	{ atom.add.f64 %fd7839,[%rd3564],%fd2030; }

	// end inline asm
	add.s64 	%rd3565, %rd3389, 1408;
	// begin inline asm
	{ atom.add.f64 %fd7841,[%rd3565],%fd2029; }

	// end inline asm
	add.s64 	%rd3566, %rd3389, 1416;
	// begin inline asm
	{ atom.add.f64 %fd7843,[%rd3566],%fd2004; }

	// end inline asm
	add.s64 	%rd3567, %rd3389, 1424;
	// begin inline asm
	{ atom.add.f64 %fd7845,[%rd3567],%fd2003; }

	// end inline asm
	add.s64 	%rd3568, %rd3389, 1432;
	// begin inline asm
	{ atom.add.f64 %fd7847,[%rd3568],%fd2002; }

	// end inline asm
	add.s64 	%rd3569, %rd3389, 1440;
	// begin inline asm
	{ atom.add.f64 %fd7849,[%rd3569],%fd1221; }

	// end inline asm
	add.s64 	%rd3570, %rd3389, 1448;
	// begin inline asm
	{ atom.add.f64 %fd7851,[%rd3570],%fd1220; }

	// end inline asm
	add.s64 	%rd3571, %rd3389, 1456;
	// begin inline asm
	{ atom.add.f64 %fd7853,[%rd3571],%fd1219; }

	// end inline asm
	add.s64 	%rd3572, %rd3389, 1464;
	// begin inline asm
	{ atom.add.f64 %fd7855,[%rd3572],%fd1194; }

	// end inline asm
	add.s64 	%rd3573, %rd3389, 1472;
	// begin inline asm
	{ atom.add.f64 %fd7857,[%rd3573],%fd1193; }

	// end inline asm
	add.s64 	%rd3574, %rd3389, 1480;
	// begin inline asm
	{ atom.add.f64 %fd7859,[%rd3574],%fd1192; }

	// end inline asm
	add.s64 	%rd3575, %rd3389, 1488;
	// begin inline asm
	{ atom.add.f64 %fd7861,[%rd3575],%fd1167; }

	// end inline asm
	add.s64 	%rd3576, %rd3389, 1496;
	// begin inline asm
	{ atom.add.f64 %fd7863,[%rd3576],%fd1166; }

	// end inline asm
	add.s64 	%rd3577, %rd3389, 1504;
	// begin inline asm
	{ atom.add.f64 %fd7865,[%rd3577],%fd1165; }

	// end inline asm
	add.s64 	%rd3578, %rd3389, 1512;
	// begin inline asm
	{ atom.add.f64 %fd7867,[%rd3578],%fd1140; }

	// end inline asm
	add.s64 	%rd3579, %rd3389, 1520;
	// begin inline asm
	{ atom.add.f64 %fd7869,[%rd3579],%fd1139; }

	// end inline asm
	add.s64 	%rd3580, %rd3389, 1528;
	// begin inline asm
	{ atom.add.f64 %fd7871,[%rd3580],%fd1138; }

	// end inline asm
	add.s64 	%rd3581, %rd3389, 1536;
	// begin inline asm
	{ atom.add.f64 %fd7873,[%rd3581],%fd2082; }

	// end inline asm
	add.s64 	%rd3582, %rd3389, 1544;
	// begin inline asm
	{ atom.add.f64 %fd7875,[%rd3582],%fd2081; }

	// end inline asm
	add.s64 	%rd3583, %rd3389, 1552;
	// begin inline asm
	{ atom.add.f64 %fd7877,[%rd3583],%fd2080; }

	// end inline asm
	add.s64 	%rd3584, %rd3389, 1560;
	// begin inline asm
	{ atom.add.f64 %fd7879,[%rd3584],%fd2055; }

	// end inline asm
	add.s64 	%rd3585, %rd3389, 1568;
	// begin inline asm
	{ atom.add.f64 %fd7881,[%rd3585],%fd2054; }

	// end inline asm
	add.s64 	%rd3586, %rd3389, 1576;
	// begin inline asm
	{ atom.add.f64 %fd7883,[%rd3586],%fd2053; }

	// end inline asm
	add.s64 	%rd3587, %rd3389, 1584;
	// begin inline asm
	{ atom.add.f64 %fd7885,[%rd3587],%fd2028; }

	// end inline asm
	add.s64 	%rd3588, %rd3389, 1592;
	// begin inline asm
	{ atom.add.f64 %fd7887,[%rd3588],%fd2027; }

	// end inline asm
	add.s64 	%rd3589, %rd3389, 1600;
	// begin inline asm
	{ atom.add.f64 %fd7889,[%rd3589],%fd2026; }

	// end inline asm
	add.s64 	%rd3590, %rd3389, 1608;
	// begin inline asm
	{ atom.add.f64 %fd7891,[%rd3590],%fd2001; }

	// end inline asm
	add.s64 	%rd3591, %rd3389, 1616;
	// begin inline asm
	{ atom.add.f64 %fd7893,[%rd3591],%fd2000; }

	// end inline asm
	add.s64 	%rd3592, %rd3389, 1624;
	// begin inline asm
	{ atom.add.f64 %fd7895,[%rd3592],%fd1999; }

	// end inline asm
	add.s64 	%rd3593, %rd3389, 1632;
	// begin inline asm
	{ atom.add.f64 %fd7897,[%rd3593],%fd1218; }

	// end inline asm
	add.s64 	%rd3594, %rd3389, 1640;
	// begin inline asm
	{ atom.add.f64 %fd7899,[%rd3594],%fd1217; }

	// end inline asm
	add.s64 	%rd3595, %rd3389, 1648;
	// begin inline asm
	{ atom.add.f64 %fd7901,[%rd3595],%fd1216; }

	// end inline asm
	add.s64 	%rd3596, %rd3389, 1656;
	// begin inline asm
	{ atom.add.f64 %fd7903,[%rd3596],%fd1191; }

	// end inline asm
	add.s64 	%rd3597, %rd3389, 1664;
	// begin inline asm
	{ atom.add.f64 %fd7905,[%rd3597],%fd1190; }

	// end inline asm
	add.s64 	%rd3598, %rd3389, 1672;
	// begin inline asm
	{ atom.add.f64 %fd7907,[%rd3598],%fd1189; }

	// end inline asm
	add.s64 	%rd3599, %rd3389, 1680;
	// begin inline asm
	{ atom.add.f64 %fd7909,[%rd3599],%fd1164; }

	// end inline asm
	add.s64 	%rd3600, %rd3389, 1688;
	// begin inline asm
	{ atom.add.f64 %fd7911,[%rd3600],%fd1163; }

	// end inline asm
	add.s64 	%rd3601, %rd3389, 1696;
	// begin inline asm
	{ atom.add.f64 %fd7913,[%rd3601],%fd1162; }

	// end inline asm
	add.s64 	%rd3602, %rd3389, 1704;
	// begin inline asm
	{ atom.add.f64 %fd7915,[%rd3602],%fd1137; }

	// end inline asm
	add.s64 	%rd3603, %rd3389, 1712;
	// begin inline asm
	{ atom.add.f64 %fd7917,[%rd3603],%fd1136; }

	// end inline asm
	add.s64 	%rd3604, %rd3389, 1720;
	// begin inline asm
	{ atom.add.f64 %fd7919,[%rd3604],%fd1135; }

	// end inline asm
	add.s64 	%rd3605, %rd3389, 1728;
	// begin inline asm
	{ atom.add.f64 %fd7921,[%rd3605],%fd1980; }

	// end inline asm
	add.s64 	%rd3606, %rd3389, 1736;
	// begin inline asm
	{ atom.add.f64 %fd7923,[%rd3606],%fd1979; }

	// end inline asm
	add.s64 	%rd3607, %rd3389, 1744;
	// begin inline asm
	{ atom.add.f64 %fd7925,[%rd3607],%fd1978; }

	// end inline asm
	add.s64 	%rd3608, %rd3389, 1752;
	// begin inline asm
	{ atom.add.f64 %fd7927,[%rd3608],%fd1953; }

	// end inline asm
	add.s64 	%rd3609, %rd3389, 1760;
	// begin inline asm
	{ atom.add.f64 %fd7929,[%rd3609],%fd1952; }

	// end inline asm
	add.s64 	%rd3610, %rd3389, 1768;
	// begin inline asm
	{ atom.add.f64 %fd7931,[%rd3610],%fd1951; }

	// end inline asm
	add.s64 	%rd3611, %rd3389, 1776;
	// begin inline asm
	{ atom.add.f64 %fd7933,[%rd3611],%fd1926; }

	// end inline asm
	add.s64 	%rd3612, %rd3389, 1784;
	// begin inline asm
	{ atom.add.f64 %fd7935,[%rd3612],%fd1925; }

	// end inline asm
	add.s64 	%rd3613, %rd3389, 1792;
	// begin inline asm
	{ atom.add.f64 %fd7937,[%rd3613],%fd1924; }

	// end inline asm
	add.s64 	%rd3614, %rd3389, 1800;
	// begin inline asm
	{ atom.add.f64 %fd7939,[%rd3614],%fd1899; }

	// end inline asm
	add.s64 	%rd3615, %rd3389, 1808;
	// begin inline asm
	{ atom.add.f64 %fd7941,[%rd3615],%fd1898; }

	// end inline asm
	add.s64 	%rd3616, %rd3389, 1816;
	// begin inline asm
	{ atom.add.f64 %fd7943,[%rd3616],%fd1897; }

	// end inline asm
	add.s64 	%rd3617, %rd3389, 1824;
	// begin inline asm
	{ atom.add.f64 %fd7945,[%rd3617],%fd1116; }

	// end inline asm
	add.s64 	%rd3618, %rd3389, 1832;
	// begin inline asm
	{ atom.add.f64 %fd7947,[%rd3618],%fd1115; }

	// end inline asm
	add.s64 	%rd3619, %rd3389, 1840;
	// begin inline asm
	{ atom.add.f64 %fd7949,[%rd3619],%fd1114; }

	// end inline asm
	add.s64 	%rd3620, %rd3389, 1848;
	// begin inline asm
	{ atom.add.f64 %fd7951,[%rd3620],%fd1089; }

	// end inline asm
	add.s64 	%rd3621, %rd3389, 1856;
	// begin inline asm
	{ atom.add.f64 %fd7953,[%rd3621],%fd1088; }

	// end inline asm
	add.s64 	%rd3622, %rd3389, 1864;
	// begin inline asm
	{ atom.add.f64 %fd7955,[%rd3622],%fd1087; }

	// end inline asm
	add.s64 	%rd3623, %rd3389, 1872;
	// begin inline asm
	{ atom.add.f64 %fd7957,[%rd3623],%fd1062; }

	// end inline asm
	add.s64 	%rd3624, %rd3389, 1880;
	// begin inline asm
	{ atom.add.f64 %fd7959,[%rd3624],%fd1061; }

	// end inline asm
	add.s64 	%rd3625, %rd3389, 1888;
	// begin inline asm
	{ atom.add.f64 %fd7961,[%rd3625],%fd1060; }

	// end inline asm
	add.s64 	%rd3626, %rd3389, 1896;
	// begin inline asm
	{ atom.add.f64 %fd7963,[%rd3626],%fd1035; }

	// end inline asm
	add.s64 	%rd3627, %rd3389, 1904;
	// begin inline asm
	{ atom.add.f64 %fd7965,[%rd3627],%fd1034; }

	// end inline asm
	add.s64 	%rd3628, %rd3389, 1912;
	// begin inline asm
	{ atom.add.f64 %fd7967,[%rd3628],%fd1033; }

	// end inline asm
	add.s64 	%rd3629, %rd3389, 1920;
	// begin inline asm
	{ atom.add.f64 %fd7969,[%rd3629],%fd1977; }

	// end inline asm
	add.s64 	%rd3630, %rd3389, 1928;
	// begin inline asm
	{ atom.add.f64 %fd7971,[%rd3630],%fd1976; }

	// end inline asm
	add.s64 	%rd3631, %rd3389, 1936;
	// begin inline asm
	{ atom.add.f64 %fd7973,[%rd3631],%fd1975; }

	// end inline asm
	add.s64 	%rd3632, %rd3389, 1944;
	// begin inline asm
	{ atom.add.f64 %fd7975,[%rd3632],%fd1950; }

	// end inline asm
	add.s64 	%rd3633, %rd3389, 1952;
	// begin inline asm
	{ atom.add.f64 %fd7977,[%rd3633],%fd1949; }

	// end inline asm
	add.s64 	%rd3634, %rd3389, 1960;
	// begin inline asm
	{ atom.add.f64 %fd7979,[%rd3634],%fd1948; }

	// end inline asm
	add.s64 	%rd3635, %rd3389, 1968;
	// begin inline asm
	{ atom.add.f64 %fd7981,[%rd3635],%fd1923; }

	// end inline asm
	add.s64 	%rd3636, %rd3389, 1976;
	// begin inline asm
	{ atom.add.f64 %fd7983,[%rd3636],%fd1922; }

	// end inline asm
	add.s64 	%rd3637, %rd3389, 1984;
	// begin inline asm
	{ atom.add.f64 %fd7985,[%rd3637],%fd1921; }

	// end inline asm
	add.s64 	%rd3638, %rd3389, 1992;
	// begin inline asm
	{ atom.add.f64 %fd7987,[%rd3638],%fd1896; }

	// end inline asm
	add.s64 	%rd3639, %rd3389, 2000;
	// begin inline asm
	{ atom.add.f64 %fd7989,[%rd3639],%fd1895; }

	// end inline asm
	add.s64 	%rd3640, %rd3389, 2008;
	// begin inline asm
	{ atom.add.f64 %fd7991,[%rd3640],%fd1894; }

	// end inline asm
	add.s64 	%rd3641, %rd3389, 2016;
	// begin inline asm
	{ atom.add.f64 %fd7993,[%rd3641],%fd1113; }

	// end inline asm
	add.s64 	%rd3642, %rd3389, 2024;
	// begin inline asm
	{ atom.add.f64 %fd7995,[%rd3642],%fd1112; }

	// end inline asm
	add.s64 	%rd3643, %rd3389, 2032;
	// begin inline asm
	{ atom.add.f64 %fd7997,[%rd3643],%fd1111; }

	// end inline asm
	add.s64 	%rd3644, %rd3389, 2040;
	// begin inline asm
	{ atom.add.f64 %fd7999,[%rd3644],%fd1086; }

	// end inline asm
	add.s64 	%rd3645, %rd3389, 2048;
	// begin inline asm
	{ atom.add.f64 %fd8001,[%rd3645],%fd1085; }

	// end inline asm
	add.s64 	%rd3646, %rd3389, 2056;
	// begin inline asm
	{ atom.add.f64 %fd8003,[%rd3646],%fd1084; }

	// end inline asm
	add.s64 	%rd3647, %rd3389, 2064;
	// begin inline asm
	{ atom.add.f64 %fd8005,[%rd3647],%fd1059; }

	// end inline asm
	add.s64 	%rd3648, %rd3389, 2072;
	// begin inline asm
	{ atom.add.f64 %fd8007,[%rd3648],%fd1058; }

	// end inline asm
	add.s64 	%rd3649, %rd3389, 2080;
	// begin inline asm
	{ atom.add.f64 %fd8009,[%rd3649],%fd1057; }

	// end inline asm
	add.s64 	%rd3650, %rd3389, 2088;
	// begin inline asm
	{ atom.add.f64 %fd8011,[%rd3650],%fd1032; }

	// end inline asm
	add.s64 	%rd3651, %rd3389, 2096;
	// begin inline asm
	{ atom.add.f64 %fd8013,[%rd3651],%fd1031; }

	// end inline asm
	add.s64 	%rd3652, %rd3389, 2104;
	// begin inline asm
	{ atom.add.f64 %fd8015,[%rd3652],%fd1030; }

	// end inline asm
	add.s64 	%rd3653, %rd3389, 2112;
	// begin inline asm
	{ atom.add.f64 %fd8017,[%rd3653],%fd1974; }

	// end inline asm
	add.s64 	%rd3654, %rd3389, 2120;
	// begin inline asm
	{ atom.add.f64 %fd8019,[%rd3654],%fd1973; }

	// end inline asm
	add.s64 	%rd3655, %rd3389, 2128;
	// begin inline asm
	{ atom.add.f64 %fd8021,[%rd3655],%fd1972; }

	// end inline asm
	add.s64 	%rd3656, %rd3389, 2136;
	// begin inline asm
	{ atom.add.f64 %fd8023,[%rd3656],%fd1947; }

	// end inline asm
	add.s64 	%rd3657, %rd3389, 2144;
	// begin inline asm
	{ atom.add.f64 %fd8025,[%rd3657],%fd1946; }

	// end inline asm
	add.s64 	%rd3658, %rd3389, 2152;
	// begin inline asm
	{ atom.add.f64 %fd8027,[%rd3658],%fd1945; }

	// end inline asm
	add.s64 	%rd3659, %rd3389, 2160;
	// begin inline asm
	{ atom.add.f64 %fd8029,[%rd3659],%fd1920; }

	// end inline asm
	add.s64 	%rd3660, %rd3389, 2168;
	// begin inline asm
	{ atom.add.f64 %fd8031,[%rd3660],%fd1919; }

	// end inline asm
	add.s64 	%rd3661, %rd3389, 2176;
	// begin inline asm
	{ atom.add.f64 %fd8033,[%rd3661],%fd1918; }

	// end inline asm
	add.s64 	%rd3662, %rd3389, 2184;
	// begin inline asm
	{ atom.add.f64 %fd8035,[%rd3662],%fd1893; }

	// end inline asm
	add.s64 	%rd3663, %rd3389, 2192;
	// begin inline asm
	{ atom.add.f64 %fd8037,[%rd3663],%fd1892; }

	// end inline asm
	add.s64 	%rd3664, %rd3389, 2200;
	// begin inline asm
	{ atom.add.f64 %fd8039,[%rd3664],%fd1891; }

	// end inline asm
	add.s64 	%rd3665, %rd3389, 2208;
	// begin inline asm
	{ atom.add.f64 %fd8041,[%rd3665],%fd1110; }

	// end inline asm
	add.s64 	%rd3666, %rd3389, 2216;
	// begin inline asm
	{ atom.add.f64 %fd8043,[%rd3666],%fd1109; }

	// end inline asm
	add.s64 	%rd3667, %rd3389, 2224;
	// begin inline asm
	{ atom.add.f64 %fd8045,[%rd3667],%fd1108; }

	// end inline asm
	add.s64 	%rd3668, %rd3389, 2232;
	// begin inline asm
	{ atom.add.f64 %fd8047,[%rd3668],%fd1083; }

	// end inline asm
	add.s64 	%rd3669, %rd3389, 2240;
	// begin inline asm
	{ atom.add.f64 %fd8049,[%rd3669],%fd1082; }

	// end inline asm
	add.s64 	%rd3670, %rd3389, 2248;
	// begin inline asm
	{ atom.add.f64 %fd8051,[%rd3670],%fd1081; }

	// end inline asm
	add.s64 	%rd3671, %rd3389, 2256;
	// begin inline asm
	{ atom.add.f64 %fd8053,[%rd3671],%fd1056; }

	// end inline asm
	add.s64 	%rd3672, %rd3389, 2264;
	// begin inline asm
	{ atom.add.f64 %fd8055,[%rd3672],%fd1055; }

	// end inline asm
	add.s64 	%rd3673, %rd3389, 2272;
	// begin inline asm
	{ atom.add.f64 %fd8057,[%rd3673],%fd1054; }

	// end inline asm
	add.s64 	%rd3674, %rd3389, 2280;
	// begin inline asm
	{ atom.add.f64 %fd8059,[%rd3674],%fd1029; }

	// end inline asm
	add.s64 	%rd3675, %rd3389, 2288;
	// begin inline asm
	{ atom.add.f64 %fd8061,[%rd3675],%fd1028; }

	// end inline asm
	add.s64 	%rd3676, %rd3389, 2296;
	// begin inline asm
	{ atom.add.f64 %fd8063,[%rd3676],%fd1027; }

	// end inline asm
	add.s64 	%rd3677, %rd3389, 2304;
	// begin inline asm
	{ atom.add.f64 %fd8065,[%rd3677],%fd1008; }

	// end inline asm
	add.s64 	%rd3678, %rd3389, 2312;
	// begin inline asm
	{ atom.add.f64 %fd8067,[%rd3678],%fd1007; }

	// end inline asm
	add.s64 	%rd3679, %rd3389, 2320;
	// begin inline asm
	{ atom.add.f64 %fd8069,[%rd3679],%fd1006; }

	// end inline asm
	add.s64 	%rd3680, %rd3389, 2328;
	// begin inline asm
	{ atom.add.f64 %fd8071,[%rd3680],%fd981; }

	// end inline asm
	add.s64 	%rd3681, %rd3389, 2336;
	// begin inline asm
	{ atom.add.f64 %fd8073,[%rd3681],%fd980; }

	// end inline asm
	add.s64 	%rd3682, %rd3389, 2344;
	// begin inline asm
	{ atom.add.f64 %fd8075,[%rd3682],%fd979; }

	// end inline asm
	add.s64 	%rd3683, %rd3389, 2352;
	// begin inline asm
	{ atom.add.f64 %fd8077,[%rd3683],%fd954; }

	// end inline asm
	add.s64 	%rd3684, %rd3389, 2360;
	// begin inline asm
	{ atom.add.f64 %fd8079,[%rd3684],%fd953; }

	// end inline asm
	add.s64 	%rd3685, %rd3389, 2368;
	// begin inline asm
	{ atom.add.f64 %fd8081,[%rd3685],%fd952; }

	// end inline asm
	add.s64 	%rd3686, %rd3389, 2376;
	// begin inline asm
	{ atom.add.f64 %fd8083,[%rd3686],%fd927; }

	// end inline asm
	add.s64 	%rd3687, %rd3389, 2384;
	// begin inline asm
	{ atom.add.f64 %fd8085,[%rd3687],%fd926; }

	// end inline asm
	add.s64 	%rd3688, %rd3389, 2392;
	// begin inline asm
	{ atom.add.f64 %fd8087,[%rd3688],%fd925; }

	// end inline asm
	add.s64 	%rd3689, %rd3389, 2400;
	// begin inline asm
	{ atom.add.f64 %fd8089,[%rd3689],%fd1872; }

	// end inline asm
	add.s64 	%rd3690, %rd3389, 2408;
	// begin inline asm
	{ atom.add.f64 %fd8091,[%rd3690],%fd1871; }

	// end inline asm
	add.s64 	%rd3691, %rd3389, 2416;
	// begin inline asm
	{ atom.add.f64 %fd8093,[%rd3691],%fd1870; }

	// end inline asm
	add.s64 	%rd3692, %rd3389, 2424;
	// begin inline asm
	{ atom.add.f64 %fd8095,[%rd3692],%fd1845; }

	// end inline asm
	add.s64 	%rd3693, %rd3389, 2432;
	// begin inline asm
	{ atom.add.f64 %fd8097,[%rd3693],%fd1844; }

	// end inline asm
	add.s64 	%rd3694, %rd3389, 2440;
	// begin inline asm
	{ atom.add.f64 %fd8099,[%rd3694],%fd1843; }

	// end inline asm
	add.s64 	%rd3695, %rd3389, 2448;
	// begin inline asm
	{ atom.add.f64 %fd8101,[%rd3695],%fd1818; }

	// end inline asm
	add.s64 	%rd3696, %rd3389, 2456;
	// begin inline asm
	{ atom.add.f64 %fd8103,[%rd3696],%fd1817; }

	// end inline asm
	add.s64 	%rd3697, %rd3389, 2464;
	// begin inline asm
	{ atom.add.f64 %fd8105,[%rd3697],%fd1816; }

	// end inline asm
	add.s64 	%rd3698, %rd3389, 2472;
	// begin inline asm
	{ atom.add.f64 %fd8107,[%rd3698],%fd1791; }

	// end inline asm
	add.s64 	%rd3699, %rd3389, 2480;
	// begin inline asm
	{ atom.add.f64 %fd8109,[%rd3699],%fd1790; }

	// end inline asm
	add.s64 	%rd3700, %rd3389, 2488;
	// begin inline asm
	{ atom.add.f64 %fd8111,[%rd3700],%fd1789; }

	// end inline asm
	add.s64 	%rd3701, %rd3389, 2496;
	// begin inline asm
	{ atom.add.f64 %fd8113,[%rd3701],%fd1005; }

	// end inline asm
	add.s64 	%rd3702, %rd3389, 2504;
	// begin inline asm
	{ atom.add.f64 %fd8115,[%rd3702],%fd1004; }

	// end inline asm
	add.s64 	%rd3703, %rd3389, 2512;
	// begin inline asm
	{ atom.add.f64 %fd8117,[%rd3703],%fd1003; }

	// end inline asm
	add.s64 	%rd3704, %rd3389, 2520;
	// begin inline asm
	{ atom.add.f64 %fd8119,[%rd3704],%fd978; }

	// end inline asm
	add.s64 	%rd3705, %rd3389, 2528;
	// begin inline asm
	{ atom.add.f64 %fd8121,[%rd3705],%fd977; }

	// end inline asm
	add.s64 	%rd3706, %rd3389, 2536;
	// begin inline asm
	{ atom.add.f64 %fd8123,[%rd3706],%fd976; }

	// end inline asm
	add.s64 	%rd3707, %rd3389, 2544;
	// begin inline asm
	{ atom.add.f64 %fd8125,[%rd3707],%fd951; }

	// end inline asm
	add.s64 	%rd3708, %rd3389, 2552;
	// begin inline asm
	{ atom.add.f64 %fd8127,[%rd3708],%fd950; }

	// end inline asm
	add.s64 	%rd3709, %rd3389, 2560;
	// begin inline asm
	{ atom.add.f64 %fd8129,[%rd3709],%fd949; }

	// end inline asm
	add.s64 	%rd3710, %rd3389, 2568;
	// begin inline asm
	{ atom.add.f64 %fd8131,[%rd3710],%fd924; }

	// end inline asm
	add.s64 	%rd3711, %rd3389, 2576;
	// begin inline asm
	{ atom.add.f64 %fd8133,[%rd3711],%fd923; }

	// end inline asm
	add.s64 	%rd3712, %rd3389, 2584;
	// begin inline asm
	{ atom.add.f64 %fd8135,[%rd3712],%fd922; }

	// end inline asm
	add.s64 	%rd3713, %rd3389, 2592;
	// begin inline asm
	{ atom.add.f64 %fd8137,[%rd3713],%fd1869; }

	// end inline asm
	add.s64 	%rd3714, %rd3389, 2600;
	// begin inline asm
	{ atom.add.f64 %fd8139,[%rd3714],%fd1868; }

	// end inline asm
	add.s64 	%rd3715, %rd3389, 2608;
	// begin inline asm
	{ atom.add.f64 %fd8141,[%rd3715],%fd1867; }

	// end inline asm
	add.s64 	%rd3716, %rd3389, 2616;
	// begin inline asm
	{ atom.add.f64 %fd8143,[%rd3716],%fd1842; }

	// end inline asm
	add.s64 	%rd3717, %rd3389, 2624;
	// begin inline asm
	{ atom.add.f64 %fd8145,[%rd3717],%fd1841; }

	// end inline asm
	add.s64 	%rd3718, %rd3389, 2632;
	// begin inline asm
	{ atom.add.f64 %fd8147,[%rd3718],%fd1840; }

	// end inline asm
	add.s64 	%rd3719, %rd3389, 2640;
	// begin inline asm
	{ atom.add.f64 %fd8149,[%rd3719],%fd1815; }

	// end inline asm
	add.s64 	%rd3720, %rd3389, 2648;
	// begin inline asm
	{ atom.add.f64 %fd8151,[%rd3720],%fd1814; }

	// end inline asm
	add.s64 	%rd3721, %rd3389, 2656;
	// begin inline asm
	{ atom.add.f64 %fd8153,[%rd3721],%fd1813; }

	// end inline asm
	add.s64 	%rd3722, %rd3389, 2664;
	// begin inline asm
	{ atom.add.f64 %fd8155,[%rd3722],%fd1788; }

	// end inline asm
	add.s64 	%rd3723, %rd3389, 2672;
	// begin inline asm
	{ atom.add.f64 %fd8157,[%rd3723],%fd1787; }

	// end inline asm
	add.s64 	%rd3724, %rd3389, 2680;
	// begin inline asm
	{ atom.add.f64 %fd8159,[%rd3724],%fd1786; }

	// end inline asm
	add.s64 	%rd3725, %rd3389, 2688;
	// begin inline asm
	{ atom.add.f64 %fd8161,[%rd3725],%fd1002; }

	// end inline asm
	add.s64 	%rd3726, %rd3389, 2696;
	// begin inline asm
	{ atom.add.f64 %fd8163,[%rd3726],%fd1001; }

	// end inline asm
	add.s64 	%rd3727, %rd3389, 2704;
	// begin inline asm
	{ atom.add.f64 %fd8165,[%rd3727],%fd1000; }

	// end inline asm
	add.s64 	%rd3728, %rd3389, 2712;
	// begin inline asm
	{ atom.add.f64 %fd8167,[%rd3728],%fd975; }

	// end inline asm
	add.s64 	%rd3729, %rd3389, 2720;
	// begin inline asm
	{ atom.add.f64 %fd8169,[%rd3729],%fd974; }

	// end inline asm
	add.s64 	%rd3730, %rd3389, 2728;
	// begin inline asm
	{ atom.add.f64 %fd8171,[%rd3730],%fd973; }

	// end inline asm
	add.s64 	%rd3731, %rd3389, 2736;
	// begin inline asm
	{ atom.add.f64 %fd8173,[%rd3731],%fd948; }

	// end inline asm
	add.s64 	%rd3732, %rd3389, 2744;
	// begin inline asm
	{ atom.add.f64 %fd8175,[%rd3732],%fd947; }

	// end inline asm
	add.s64 	%rd3733, %rd3389, 2752;
	// begin inline asm
	{ atom.add.f64 %fd8177,[%rd3733],%fd946; }

	// end inline asm
	add.s64 	%rd3734, %rd3389, 2760;
	// begin inline asm
	{ atom.add.f64 %fd8179,[%rd3734],%fd921; }

	// end inline asm
	add.s64 	%rd3735, %rd3389, 2768;
	// begin inline asm
	{ atom.add.f64 %fd8181,[%rd3735],%fd920; }

	// end inline asm
	add.s64 	%rd3736, %rd3389, 2776;
	// begin inline asm
	{ atom.add.f64 %fd8183,[%rd3736],%fd919; }

	// end inline asm
	add.s64 	%rd3737, %rd3389, 2784;
	// begin inline asm
	{ atom.add.f64 %fd8185,[%rd3737],%fd1866; }

	// end inline asm
	add.s64 	%rd3738, %rd3389, 2792;
	// begin inline asm
	{ atom.add.f64 %fd8187,[%rd3738],%fd1865; }

	// end inline asm
	add.s64 	%rd3739, %rd3389, 2800;
	// begin inline asm
	{ atom.add.f64 %fd8189,[%rd3739],%fd1864; }

	// end inline asm
	add.s64 	%rd3740, %rd3389, 2808;
	// begin inline asm
	{ atom.add.f64 %fd8191,[%rd3740],%fd1839; }

	// end inline asm
	add.s64 	%rd3741, %rd3389, 2816;
	// begin inline asm
	{ atom.add.f64 %fd8193,[%rd3741],%fd1838; }

	// end inline asm
	add.s64 	%rd3742, %rd3389, 2824;
	// begin inline asm
	{ atom.add.f64 %fd8195,[%rd3742],%fd1837; }

	// end inline asm
	add.s64 	%rd3743, %rd3389, 2832;
	// begin inline asm
	{ atom.add.f64 %fd8197,[%rd3743],%fd1812; }

	// end inline asm
	add.s64 	%rd3744, %rd3389, 2840;
	// begin inline asm
	{ atom.add.f64 %fd8199,[%rd3744],%fd1811; }

	// end inline asm
	add.s64 	%rd3745, %rd3389, 2848;
	// begin inline asm
	{ atom.add.f64 %fd8201,[%rd3745],%fd1810; }

	// end inline asm
	add.s64 	%rd3746, %rd3389, 2856;
	// begin inline asm
	{ atom.add.f64 %fd8203,[%rd3746],%fd1785; }

	// end inline asm
	add.s64 	%rd3747, %rd3389, 2864;
	// begin inline asm
	{ atom.add.f64 %fd8205,[%rd3747],%fd1784; }

	// end inline asm
	add.s64 	%rd3748, %rd3389, 2872;
	// begin inline asm
	{ atom.add.f64 %fd8207,[%rd3748],%fd1783; }

	// end inline asm
	add.s64 	%rd3749, %rd3389, 2880;
	// begin inline asm
	{ atom.add.f64 %fd8209,[%rd3749],%fd900; }

	// end inline asm
	add.s64 	%rd3750, %rd3389, 2888;
	// begin inline asm
	{ atom.add.f64 %fd8211,[%rd3750],%fd899; }

	// end inline asm
	add.s64 	%rd3751, %rd3389, 2896;
	// begin inline asm
	{ atom.add.f64 %fd8213,[%rd3751],%fd898; }

	// end inline asm
	add.s64 	%rd3752, %rd3389, 2904;
	// begin inline asm
	{ atom.add.f64 %fd8215,[%rd3752],%fd873; }

	// end inline asm
	add.s64 	%rd3753, %rd3389, 2912;
	// begin inline asm
	{ atom.add.f64 %fd8217,[%rd3753],%fd872; }

	// end inline asm
	add.s64 	%rd3754, %rd3389, 2920;
	// begin inline asm
	{ atom.add.f64 %fd8219,[%rd3754],%fd871; }

	// end inline asm
	add.s64 	%rd3755, %rd3389, 2928;
	// begin inline asm
	{ atom.add.f64 %fd8221,[%rd3755],%fd846; }

	// end inline asm
	add.s64 	%rd3756, %rd3389, 2936;
	// begin inline asm
	{ atom.add.f64 %fd8223,[%rd3756],%fd845; }

	// end inline asm
	add.s64 	%rd3757, %rd3389, 2944;
	// begin inline asm
	{ atom.add.f64 %fd8225,[%rd3757],%fd844; }

	// end inline asm
	add.s64 	%rd3758, %rd3389, 2952;
	// begin inline asm
	{ atom.add.f64 %fd8227,[%rd3758],%fd819; }

	// end inline asm
	add.s64 	%rd3759, %rd3389, 2960;
	// begin inline asm
	{ atom.add.f64 %fd8229,[%rd3759],%fd818; }

	// end inline asm
	add.s64 	%rd3760, %rd3389, 2968;
	// begin inline asm
	{ atom.add.f64 %fd8231,[%rd3760],%fd817; }

	// end inline asm
	add.s64 	%rd3761, %rd3389, 2976;
	// begin inline asm
	{ atom.add.f64 %fd8233,[%rd3761],%fd1764; }

	// end inline asm
	add.s64 	%rd3762, %rd3389, 2984;
	// begin inline asm
	{ atom.add.f64 %fd8235,[%rd3762],%fd1763; }

	// end inline asm
	add.s64 	%rd3763, %rd3389, 2992;
	// begin inline asm
	{ atom.add.f64 %fd8237,[%rd3763],%fd1762; }

	// end inline asm
	add.s64 	%rd3764, %rd3389, 3000;
	// begin inline asm
	{ atom.add.f64 %fd8239,[%rd3764],%fd1737; }

	// end inline asm
	add.s64 	%rd3765, %rd3389, 3008;
	// begin inline asm
	{ atom.add.f64 %fd8241,[%rd3765],%fd1736; }

	// end inline asm
	add.s64 	%rd3766, %rd3389, 3016;
	// begin inline asm
	{ atom.add.f64 %fd8243,[%rd3766],%fd1735; }

	// end inline asm
	add.s64 	%rd3767, %rd3389, 3024;
	// begin inline asm
	{ atom.add.f64 %fd8245,[%rd3767],%fd1710; }

	// end inline asm
	add.s64 	%rd3768, %rd3389, 3032;
	// begin inline asm
	{ atom.add.f64 %fd8247,[%rd3768],%fd1709; }

	// end inline asm
	add.s64 	%rd3769, %rd3389, 3040;
	// begin inline asm
	{ atom.add.f64 %fd8249,[%rd3769],%fd1708; }

	// end inline asm
	add.s64 	%rd3770, %rd3389, 3048;
	// begin inline asm
	{ atom.add.f64 %fd8251,[%rd3770],%fd1683; }

	// end inline asm
	add.s64 	%rd3771, %rd3389, 3056;
	// begin inline asm
	{ atom.add.f64 %fd8253,[%rd3771],%fd1682; }

	// end inline asm
	add.s64 	%rd3772, %rd3389, 3064;
	// begin inline asm
	{ atom.add.f64 %fd8255,[%rd3772],%fd1681; }

	// end inline asm
	add.s64 	%rd3773, %rd3389, 3072;
	// begin inline asm
	{ atom.add.f64 %fd8257,[%rd3773],%fd897; }

	// end inline asm
	add.s64 	%rd3774, %rd3389, 3080;
	// begin inline asm
	{ atom.add.f64 %fd8259,[%rd3774],%fd896; }

	// end inline asm
	add.s64 	%rd3775, %rd3389, 3088;
	// begin inline asm
	{ atom.add.f64 %fd8261,[%rd3775],%fd895; }

	// end inline asm
	add.s64 	%rd3776, %rd3389, 3096;
	// begin inline asm
	{ atom.add.f64 %fd8263,[%rd3776],%fd870; }

	// end inline asm
	add.s64 	%rd3777, %rd3389, 3104;
	// begin inline asm
	{ atom.add.f64 %fd8265,[%rd3777],%fd869; }

	// end inline asm
	add.s64 	%rd3778, %rd3389, 3112;
	// begin inline asm
	{ atom.add.f64 %fd8267,[%rd3778],%fd868; }

	// end inline asm
	add.s64 	%rd3779, %rd3389, 3120;
	// begin inline asm
	{ atom.add.f64 %fd8269,[%rd3779],%fd843; }

	// end inline asm
	add.s64 	%rd3780, %rd3389, 3128;
	// begin inline asm
	{ atom.add.f64 %fd8271,[%rd3780],%fd842; }

	// end inline asm
	add.s64 	%rd3781, %rd3389, 3136;
	// begin inline asm
	{ atom.add.f64 %fd8273,[%rd3781],%fd841; }

	// end inline asm
	add.s64 	%rd3782, %rd3389, 3144;
	// begin inline asm
	{ atom.add.f64 %fd8275,[%rd3782],%fd816; }

	// end inline asm
	add.s64 	%rd3783, %rd3389, 3152;
	// begin inline asm
	{ atom.add.f64 %fd8277,[%rd3783],%fd815; }

	// end inline asm
	add.s64 	%rd3784, %rd3389, 3160;
	// begin inline asm
	{ atom.add.f64 %fd8279,[%rd3784],%fd814; }

	// end inline asm
	add.s64 	%rd3785, %rd3389, 3168;
	// begin inline asm
	{ atom.add.f64 %fd8281,[%rd3785],%fd1761; }

	// end inline asm
	add.s64 	%rd3786, %rd3389, 3176;
	// begin inline asm
	{ atom.add.f64 %fd8283,[%rd3786],%fd1760; }

	// end inline asm
	add.s64 	%rd3787, %rd3389, 3184;
	// begin inline asm
	{ atom.add.f64 %fd8285,[%rd3787],%fd1759; }

	// end inline asm
	add.s64 	%rd3788, %rd3389, 3192;
	// begin inline asm
	{ atom.add.f64 %fd8287,[%rd3788],%fd1734; }

	// end inline asm
	add.s64 	%rd3789, %rd3389, 3200;
	// begin inline asm
	{ atom.add.f64 %fd8289,[%rd3789],%fd1733; }

	// end inline asm
	add.s64 	%rd3790, %rd3389, 3208;
	// begin inline asm
	{ atom.add.f64 %fd8291,[%rd3790],%fd1732; }

	// end inline asm
	add.s64 	%rd3791, %rd3389, 3216;
	// begin inline asm
	{ atom.add.f64 %fd8293,[%rd3791],%fd1707; }

	// end inline asm
	add.s64 	%rd3792, %rd3389, 3224;
	// begin inline asm
	{ atom.add.f64 %fd8295,[%rd3792],%fd1706; }

	// end inline asm
	add.s64 	%rd3793, %rd3389, 3232;
	// begin inline asm
	{ atom.add.f64 %fd8297,[%rd3793],%fd1705; }

	// end inline asm
	add.s64 	%rd3794, %rd3389, 3240;
	// begin inline asm
	{ atom.add.f64 %fd8299,[%rd3794],%fd1680; }

	// end inline asm
	add.s64 	%rd3795, %rd3389, 3248;
	// begin inline asm
	{ atom.add.f64 %fd8301,[%rd3795],%fd1679; }

	// end inline asm
	add.s64 	%rd3796, %rd3389, 3256;
	// begin inline asm
	{ atom.add.f64 %fd8303,[%rd3796],%fd1678; }

	// end inline asm
	add.s64 	%rd3797, %rd3389, 3264;
	// begin inline asm
	{ atom.add.f64 %fd8305,[%rd3797],%fd894; }

	// end inline asm
	add.s64 	%rd3798, %rd3389, 3272;
	// begin inline asm
	{ atom.add.f64 %fd8307,[%rd3798],%fd893; }

	// end inline asm
	add.s64 	%rd3799, %rd3389, 3280;
	// begin inline asm
	{ atom.add.f64 %fd8309,[%rd3799],%fd892; }

	// end inline asm
	add.s64 	%rd3800, %rd3389, 3288;
	// begin inline asm
	{ atom.add.f64 %fd8311,[%rd3800],%fd867; }

	// end inline asm
	add.s64 	%rd3801, %rd3389, 3296;
	// begin inline asm
	{ atom.add.f64 %fd8313,[%rd3801],%fd866; }

	// end inline asm
	add.s64 	%rd3802, %rd3389, 3304;
	// begin inline asm
	{ atom.add.f64 %fd8315,[%rd3802],%fd865; }

	// end inline asm
	add.s64 	%rd3803, %rd3389, 3312;
	// begin inline asm
	{ atom.add.f64 %fd8317,[%rd3803],%fd840; }

	// end inline asm
	add.s64 	%rd3804, %rd3389, 3320;
	// begin inline asm
	{ atom.add.f64 %fd8319,[%rd3804],%fd839; }

	// end inline asm
	add.s64 	%rd3805, %rd3389, 3328;
	// begin inline asm
	{ atom.add.f64 %fd8321,[%rd3805],%fd838; }

	// end inline asm
	add.s64 	%rd3806, %rd3389, 3336;
	// begin inline asm
	{ atom.add.f64 %fd8323,[%rd3806],%fd813; }

	// end inline asm
	add.s64 	%rd3807, %rd3389, 3344;
	// begin inline asm
	{ atom.add.f64 %fd8325,[%rd3807],%fd812; }

	// end inline asm
	add.s64 	%rd3808, %rd3389, 3352;
	// begin inline asm
	{ atom.add.f64 %fd8327,[%rd3808],%fd811; }

	// end inline asm
	add.s64 	%rd3809, %rd3389, 3360;
	// begin inline asm
	{ atom.add.f64 %fd8329,[%rd3809],%fd1758; }

	// end inline asm
	add.s64 	%rd3810, %rd3389, 3368;
	// begin inline asm
	{ atom.add.f64 %fd8331,[%rd3810],%fd1757; }

	// end inline asm
	add.s64 	%rd3811, %rd3389, 3376;
	// begin inline asm
	{ atom.add.f64 %fd8333,[%rd3811],%fd1756; }

	// end inline asm
	add.s64 	%rd3812, %rd3389, 3384;
	// begin inline asm
	{ atom.add.f64 %fd8335,[%rd3812],%fd1731; }

	// end inline asm
	add.s64 	%rd3813, %rd3389, 3392;
	// begin inline asm
	{ atom.add.f64 %fd8337,[%rd3813],%fd1730; }

	// end inline asm
	add.s64 	%rd3814, %rd3389, 3400;
	// begin inline asm
	{ atom.add.f64 %fd8339,[%rd3814],%fd1729; }

	// end inline asm
	add.s64 	%rd3815, %rd3389, 3408;
	// begin inline asm
	{ atom.add.f64 %fd8341,[%rd3815],%fd1704; }

	// end inline asm
	add.s64 	%rd3816, %rd3389, 3416;
	// begin inline asm
	{ atom.add.f64 %fd8343,[%rd3816],%fd1703; }

	// end inline asm
	add.s64 	%rd3817, %rd3389, 3424;
	// begin inline asm
	{ atom.add.f64 %fd8345,[%rd3817],%fd1702; }

	// end inline asm
	add.s64 	%rd3818, %rd3389, 3432;
	// begin inline asm
	{ atom.add.f64 %fd8347,[%rd3818],%fd1677; }

	// end inline asm
	add.s64 	%rd3819, %rd3389, 3440;
	// begin inline asm
	{ atom.add.f64 %fd8349,[%rd3819],%fd1676; }

	// end inline asm
	add.s64 	%rd3820, %rd3389, 3448;
	// begin inline asm
	{ atom.add.f64 %fd8351,[%rd3820],%fd1675; }

	// end inline asm
	add.s64 	%rd3821, %rd3389, 3456;
	// begin inline asm
	{ atom.add.f64 %fd8353,[%rd3821],%fd792; }

	// end inline asm
	add.s64 	%rd3822, %rd3389, 3464;
	// begin inline asm
	{ atom.add.f64 %fd8355,[%rd3822],%fd791; }

	// end inline asm
	add.s64 	%rd3823, %rd3389, 3472;
	// begin inline asm
	{ atom.add.f64 %fd8357,[%rd3823],%fd790; }

	// end inline asm
	add.s64 	%rd3824, %rd3389, 3480;
	// begin inline asm
	{ atom.add.f64 %fd8359,[%rd3824],%fd765; }

	// end inline asm
	add.s64 	%rd3825, %rd3389, 3488;
	// begin inline asm
	{ atom.add.f64 %fd8361,[%rd3825],%fd764; }

	// end inline asm
	add.s64 	%rd3826, %rd3389, 3496;
	// begin inline asm
	{ atom.add.f64 %fd8363,[%rd3826],%fd763; }

	// end inline asm
	add.s64 	%rd3827, %rd3389, 3504;
	// begin inline asm
	{ atom.add.f64 %fd8365,[%rd3827],%fd738; }

	// end inline asm
	add.s64 	%rd3828, %rd3389, 3512;
	// begin inline asm
	{ atom.add.f64 %fd8367,[%rd3828],%fd737; }

	// end inline asm
	add.s64 	%rd3829, %rd3389, 3520;
	// begin inline asm
	{ atom.add.f64 %fd8369,[%rd3829],%fd736; }

	// end inline asm
	add.s64 	%rd3830, %rd3389, 3528;
	// begin inline asm
	{ atom.add.f64 %fd8371,[%rd3830],%fd711; }

	// end inline asm
	add.s64 	%rd3831, %rd3389, 3536;
	// begin inline asm
	{ atom.add.f64 %fd8373,[%rd3831],%fd710; }

	// end inline asm
	add.s64 	%rd3832, %rd3389, 3544;
	// begin inline asm
	{ atom.add.f64 %fd8375,[%rd3832],%fd709; }

	// end inline asm
	add.s64 	%rd3833, %rd3389, 3552;
	// begin inline asm
	{ atom.add.f64 %fd8377,[%rd3833],%fd1656; }

	// end inline asm
	add.s64 	%rd3834, %rd3389, 3560;
	// begin inline asm
	{ atom.add.f64 %fd8379,[%rd3834],%fd1655; }

	// end inline asm
	add.s64 	%rd3835, %rd3389, 3568;
	// begin inline asm
	{ atom.add.f64 %fd8381,[%rd3835],%fd1654; }

	// end inline asm
	add.s64 	%rd3836, %rd3389, 3576;
	// begin inline asm
	{ atom.add.f64 %fd8383,[%rd3836],%fd1629; }

	// end inline asm
	add.s64 	%rd3837, %rd3389, 3584;
	// begin inline asm
	{ atom.add.f64 %fd8385,[%rd3837],%fd1628; }

	// end inline asm
	add.s64 	%rd3838, %rd3389, 3592;
	// begin inline asm
	{ atom.add.f64 %fd8387,[%rd3838],%fd1627; }

	// end inline asm
	add.s64 	%rd3839, %rd3389, 3600;
	// begin inline asm
	{ atom.add.f64 %fd8389,[%rd3839],%fd1602; }

	// end inline asm
	add.s64 	%rd3840, %rd3389, 3608;
	// begin inline asm
	{ atom.add.f64 %fd8391,[%rd3840],%fd1601; }

	// end inline asm
	add.s64 	%rd3841, %rd3389, 3616;
	// begin inline asm
	{ atom.add.f64 %fd8393,[%rd3841],%fd1600; }

	// end inline asm
	add.s64 	%rd3842, %rd3389, 3624;
	// begin inline asm
	{ atom.add.f64 %fd8395,[%rd3842],%fd1575; }

	// end inline asm
	add.s64 	%rd3843, %rd3389, 3632;
	// begin inline asm
	{ atom.add.f64 %fd8397,[%rd3843],%fd1574; }

	// end inline asm
	add.s64 	%rd3844, %rd3389, 3640;
	// begin inline asm
	{ atom.add.f64 %fd8399,[%rd3844],%fd1573; }

	// end inline asm
	add.s64 	%rd3845, %rd3389, 3648;
	// begin inline asm
	{ atom.add.f64 %fd8401,[%rd3845],%fd789; }

	// end inline asm
	add.s64 	%rd3846, %rd3389, 3656;
	// begin inline asm
	{ atom.add.f64 %fd8403,[%rd3846],%fd788; }

	// end inline asm
	add.s64 	%rd3847, %rd3389, 3664;
	// begin inline asm
	{ atom.add.f64 %fd8405,[%rd3847],%fd787; }

	// end inline asm
	add.s64 	%rd3848, %rd3389, 3672;
	// begin inline asm
	{ atom.add.f64 %fd8407,[%rd3848],%fd762; }

	// end inline asm
	add.s64 	%rd3849, %rd3389, 3680;
	// begin inline asm
	{ atom.add.f64 %fd8409,[%rd3849],%fd761; }

	// end inline asm
	add.s64 	%rd3850, %rd3389, 3688;
	// begin inline asm
	{ atom.add.f64 %fd8411,[%rd3850],%fd760; }

	// end inline asm
	add.s64 	%rd3851, %rd3389, 3696;
	// begin inline asm
	{ atom.add.f64 %fd8413,[%rd3851],%fd735; }

	// end inline asm
	add.s64 	%rd3852, %rd3389, 3704;
	// begin inline asm
	{ atom.add.f64 %fd8415,[%rd3852],%fd734; }

	// end inline asm
	add.s64 	%rd3853, %rd3389, 3712;
	// begin inline asm
	{ atom.add.f64 %fd8417,[%rd3853],%fd733; }

	// end inline asm
	add.s64 	%rd3854, %rd3389, 3720;
	// begin inline asm
	{ atom.add.f64 %fd8419,[%rd3854],%fd708; }

	// end inline asm
	add.s64 	%rd3855, %rd3389, 3728;
	// begin inline asm
	{ atom.add.f64 %fd8421,[%rd3855],%fd707; }

	// end inline asm
	add.s64 	%rd3856, %rd3389, 3736;
	// begin inline asm
	{ atom.add.f64 %fd8423,[%rd3856],%fd706; }

	// end inline asm
	add.s64 	%rd3857, %rd3389, 3744;
	// begin inline asm
	{ atom.add.f64 %fd8425,[%rd3857],%fd1653; }

	// end inline asm
	add.s64 	%rd3858, %rd3389, 3752;
	// begin inline asm
	{ atom.add.f64 %fd8427,[%rd3858],%fd1652; }

	// end inline asm
	add.s64 	%rd3859, %rd3389, 3760;
	// begin inline asm
	{ atom.add.f64 %fd8429,[%rd3859],%fd1651; }

	// end inline asm
	add.s64 	%rd3860, %rd3389, 3768;
	// begin inline asm
	{ atom.add.f64 %fd8431,[%rd3860],%fd1626; }

	// end inline asm
	add.s64 	%rd3861, %rd3389, 3776;
	// begin inline asm
	{ atom.add.f64 %fd8433,[%rd3861],%fd1625; }

	// end inline asm
	add.s64 	%rd3862, %rd3389, 3784;
	// begin inline asm
	{ atom.add.f64 %fd8435,[%rd3862],%fd1624; }

	// end inline asm
	add.s64 	%rd3863, %rd3389, 3792;
	// begin inline asm
	{ atom.add.f64 %fd8437,[%rd3863],%fd1599; }

	// end inline asm
	add.s64 	%rd3864, %rd3389, 3800;
	// begin inline asm
	{ atom.add.f64 %fd8439,[%rd3864],%fd1598; }

	// end inline asm
	add.s64 	%rd3865, %rd3389, 3808;
	// begin inline asm
	{ atom.add.f64 %fd8441,[%rd3865],%fd1597; }

	// end inline asm
	add.s64 	%rd3866, %rd3389, 3816;
	// begin inline asm
	{ atom.add.f64 %fd8443,[%rd3866],%fd1572; }

	// end inline asm
	add.s64 	%rd3867, %rd3389, 3824;
	// begin inline asm
	{ atom.add.f64 %fd8445,[%rd3867],%fd1571; }

	// end inline asm
	add.s64 	%rd3868, %rd3389, 3832;
	// begin inline asm
	{ atom.add.f64 %fd8447,[%rd3868],%fd1570; }

	// end inline asm
	add.s64 	%rd3869, %rd3389, 3840;
	// begin inline asm
	{ atom.add.f64 %fd8449,[%rd3869],%fd786; }

	// end inline asm
	add.s64 	%rd3870, %rd3389, 3848;
	// begin inline asm
	{ atom.add.f64 %fd8451,[%rd3870],%fd785; }

	// end inline asm
	add.s64 	%rd3871, %rd3389, 3856;
	// begin inline asm
	{ atom.add.f64 %fd8453,[%rd3871],%fd784; }

	// end inline asm
	add.s64 	%rd3872, %rd3389, 3864;
	// begin inline asm
	{ atom.add.f64 %fd8455,[%rd3872],%fd759; }

	// end inline asm
	add.s64 	%rd3873, %rd3389, 3872;
	// begin inline asm
	{ atom.add.f64 %fd8457,[%rd3873],%fd758; }

	// end inline asm
	add.s64 	%rd3874, %rd3389, 3880;
	// begin inline asm
	{ atom.add.f64 %fd8459,[%rd3874],%fd757; }

	// end inline asm
	add.s64 	%rd3875, %rd3389, 3888;
	// begin inline asm
	{ atom.add.f64 %fd8461,[%rd3875],%fd732; }

	// end inline asm
	add.s64 	%rd3876, %rd3389, 3896;
	// begin inline asm
	{ atom.add.f64 %fd8463,[%rd3876],%fd731; }

	// end inline asm
	add.s64 	%rd3877, %rd3389, 3904;
	// begin inline asm
	{ atom.add.f64 %fd8465,[%rd3877],%fd730; }

	// end inline asm
	add.s64 	%rd3878, %rd3389, 3912;
	// begin inline asm
	{ atom.add.f64 %fd8467,[%rd3878],%fd705; }

	// end inline asm
	add.s64 	%rd3879, %rd3389, 3920;
	// begin inline asm
	{ atom.add.f64 %fd8469,[%rd3879],%fd704; }

	// end inline asm
	add.s64 	%rd3880, %rd3389, 3928;
	// begin inline asm
	{ atom.add.f64 %fd8471,[%rd3880],%fd703; }

	// end inline asm
	add.s64 	%rd3881, %rd3389, 3936;
	// begin inline asm
	{ atom.add.f64 %fd8473,[%rd3881],%fd1650; }

	// end inline asm
	add.s64 	%rd3882, %rd3389, 3944;
	// begin inline asm
	{ atom.add.f64 %fd8475,[%rd3882],%fd1649; }

	// end inline asm
	add.s64 	%rd3883, %rd3389, 3952;
	// begin inline asm
	{ atom.add.f64 %fd8477,[%rd3883],%fd1648; }

	// end inline asm
	add.s64 	%rd3884, %rd3389, 3960;
	// begin inline asm
	{ atom.add.f64 %fd8479,[%rd3884],%fd1623; }

	// end inline asm
	add.s64 	%rd3885, %rd3389, 3968;
	// begin inline asm
	{ atom.add.f64 %fd8481,[%rd3885],%fd1622; }

	// end inline asm
	add.s64 	%rd3886, %rd3389, 3976;
	// begin inline asm
	{ atom.add.f64 %fd8483,[%rd3886],%fd1621; }

	// end inline asm
	add.s64 	%rd3887, %rd3389, 3984;
	// begin inline asm
	{ atom.add.f64 %fd8485,[%rd3887],%fd1596; }

	// end inline asm
	add.s64 	%rd3888, %rd3389, 3992;
	// begin inline asm
	{ atom.add.f64 %fd8487,[%rd3888],%fd1595; }

	// end inline asm
	add.s64 	%rd3889, %rd3389, 4000;
	// begin inline asm
	{ atom.add.f64 %fd8489,[%rd3889],%fd1594; }

	// end inline asm
	add.s64 	%rd3890, %rd3389, 4008;
	// begin inline asm
	{ atom.add.f64 %fd8491,[%rd3890],%fd1569; }

	// end inline asm
	add.s64 	%rd3891, %rd3389, 4016;
	// begin inline asm
	{ atom.add.f64 %fd8493,[%rd3891],%fd1568; }

	// end inline asm
	add.s64 	%rd3892, %rd3389, 4024;
	// begin inline asm
	{ atom.add.f64 %fd8495,[%rd3892],%fd1567; }

	// end inline asm
	add.s64 	%rd3893, %rd3389, 4032;
	// begin inline asm
	{ atom.add.f64 %fd8497,[%rd3893],%fd684; }

	// end inline asm
	add.s64 	%rd3894, %rd3389, 4040;
	// begin inline asm
	{ atom.add.f64 %fd8499,[%rd3894],%fd683; }

	// end inline asm
	add.s64 	%rd3895, %rd3389, 4048;
	// begin inline asm
	{ atom.add.f64 %fd8501,[%rd3895],%fd682; }

	// end inline asm
	add.s64 	%rd3896, %rd3389, 4056;
	// begin inline asm
	{ atom.add.f64 %fd8503,[%rd3896],%fd657; }

	// end inline asm
	add.s64 	%rd3897, %rd3389, 4064;
	// begin inline asm
	{ atom.add.f64 %fd8505,[%rd3897],%fd656; }

	// end inline asm
	add.s64 	%rd3898, %rd3389, 4072;
	// begin inline asm
	{ atom.add.f64 %fd8507,[%rd3898],%fd655; }

	// end inline asm
	add.s64 	%rd3899, %rd3389, 4080;
	// begin inline asm
	{ atom.add.f64 %fd8509,[%rd3899],%fd630; }

	// end inline asm
	add.s64 	%rd3900, %rd3389, 4088;
	// begin inline asm
	{ atom.add.f64 %fd8511,[%rd3900],%fd629; }

	// end inline asm
	add.s64 	%rd3901, %rd3389, 4096;
	// begin inline asm
	{ atom.add.f64 %fd8513,[%rd3901],%fd628; }

	// end inline asm
	add.s64 	%rd3902, %rd3389, 4104;
	// begin inline asm
	{ atom.add.f64 %fd8515,[%rd3902],%fd603; }

	// end inline asm
	add.s64 	%rd3903, %rd3389, 4112;
	// begin inline asm
	{ atom.add.f64 %fd8517,[%rd3903],%fd602; }

	// end inline asm
	add.s64 	%rd3904, %rd3389, 4120;
	// begin inline asm
	{ atom.add.f64 %fd8519,[%rd3904],%fd601; }

	// end inline asm
	add.s64 	%rd3905, %rd3389, 4128;
	// begin inline asm
	{ atom.add.f64 %fd8521,[%rd3905],%fd1548; }

	// end inline asm
	add.s64 	%rd3906, %rd3389, 4136;
	// begin inline asm
	{ atom.add.f64 %fd8523,[%rd3906],%fd1547; }

	// end inline asm
	add.s64 	%rd3907, %rd3389, 4144;
	// begin inline asm
	{ atom.add.f64 %fd8525,[%rd3907],%fd1546; }

	// end inline asm
	add.s64 	%rd3908, %rd3389, 4152;
	// begin inline asm
	{ atom.add.f64 %fd8527,[%rd3908],%fd1521; }

	// end inline asm
	add.s64 	%rd3909, %rd3389, 4160;
	// begin inline asm
	{ atom.add.f64 %fd8529,[%rd3909],%fd1520; }

	// end inline asm
	add.s64 	%rd3910, %rd3389, 4168;
	// begin inline asm
	{ atom.add.f64 %fd8531,[%rd3910],%fd1519; }

	// end inline asm
	add.s64 	%rd3911, %rd3389, 4176;
	// begin inline asm
	{ atom.add.f64 %fd8533,[%rd3911],%fd1494; }

	// end inline asm
	add.s64 	%rd3912, %rd3389, 4184;
	// begin inline asm
	{ atom.add.f64 %fd8535,[%rd3912],%fd1493; }

	// end inline asm
	add.s64 	%rd3913, %rd3389, 4192;
	// begin inline asm
	{ atom.add.f64 %fd8537,[%rd3913],%fd1492; }

	// end inline asm
	add.s64 	%rd3914, %rd3389, 4200;
	// begin inline asm
	{ atom.add.f64 %fd8539,[%rd3914],%fd1467; }

	// end inline asm
	add.s64 	%rd3915, %rd3389, 4208;
	// begin inline asm
	{ atom.add.f64 %fd8541,[%rd3915],%fd1466; }

	// end inline asm
	add.s64 	%rd3916, %rd3389, 4216;
	// begin inline asm
	{ atom.add.f64 %fd8543,[%rd3916],%fd1465; }

	// end inline asm
	add.s64 	%rd3917, %rd3389, 4224;
	// begin inline asm
	{ atom.add.f64 %fd8545,[%rd3917],%fd681; }

	// end inline asm
	add.s64 	%rd3918, %rd3389, 4232;
	// begin inline asm
	{ atom.add.f64 %fd8547,[%rd3918],%fd680; }

	// end inline asm
	add.s64 	%rd3919, %rd3389, 4240;
	// begin inline asm
	{ atom.add.f64 %fd8549,[%rd3919],%fd679; }

	// end inline asm
	add.s64 	%rd3920, %rd3389, 4248;
	// begin inline asm
	{ atom.add.f64 %fd8551,[%rd3920],%fd654; }

	// end inline asm
	add.s64 	%rd3921, %rd3389, 4256;
	// begin inline asm
	{ atom.add.f64 %fd8553,[%rd3921],%fd653; }

	// end inline asm
	add.s64 	%rd3922, %rd3389, 4264;
	// begin inline asm
	{ atom.add.f64 %fd8555,[%rd3922],%fd652; }

	// end inline asm
	add.s64 	%rd3923, %rd3389, 4272;
	// begin inline asm
	{ atom.add.f64 %fd8557,[%rd3923],%fd627; }

	// end inline asm
	add.s64 	%rd3924, %rd3389, 4280;
	// begin inline asm
	{ atom.add.f64 %fd8559,[%rd3924],%fd626; }

	// end inline asm
	add.s64 	%rd3925, %rd3389, 4288;
	// begin inline asm
	{ atom.add.f64 %fd8561,[%rd3925],%fd625; }

	// end inline asm
	add.s64 	%rd3926, %rd3389, 4296;
	// begin inline asm
	{ atom.add.f64 %fd8563,[%rd3926],%fd600; }

	// end inline asm
	add.s64 	%rd3927, %rd3389, 4304;
	// begin inline asm
	{ atom.add.f64 %fd8565,[%rd3927],%fd599; }

	// end inline asm
	add.s64 	%rd3928, %rd3389, 4312;
	// begin inline asm
	{ atom.add.f64 %fd8567,[%rd3928],%fd598; }

	// end inline asm
	add.s64 	%rd3929, %rd3389, 4320;
	// begin inline asm
	{ atom.add.f64 %fd8569,[%rd3929],%fd1545; }

	// end inline asm
	add.s64 	%rd3930, %rd3389, 4328;
	// begin inline asm
	{ atom.add.f64 %fd8571,[%rd3930],%fd1544; }

	// end inline asm
	add.s64 	%rd3931, %rd3389, 4336;
	// begin inline asm
	{ atom.add.f64 %fd8573,[%rd3931],%fd1543; }

	// end inline asm
	add.s64 	%rd3932, %rd3389, 4344;
	// begin inline asm
	{ atom.add.f64 %fd8575,[%rd3932],%fd1518; }

	// end inline asm
	add.s64 	%rd3933, %rd3389, 4352;
	// begin inline asm
	{ atom.add.f64 %fd8577,[%rd3933],%fd1517; }

	// end inline asm
	add.s64 	%rd3934, %rd3389, 4360;
	// begin inline asm
	{ atom.add.f64 %fd8579,[%rd3934],%fd1516; }

	// end inline asm
	add.s64 	%rd3935, %rd3389, 4368;
	// begin inline asm
	{ atom.add.f64 %fd8581,[%rd3935],%fd1491; }

	// end inline asm
	add.s64 	%rd3936, %rd3389, 4376;
	// begin inline asm
	{ atom.add.f64 %fd8583,[%rd3936],%fd1490; }

	// end inline asm
	add.s64 	%rd3937, %rd3389, 4384;
	// begin inline asm
	{ atom.add.f64 %fd8585,[%rd3937],%fd1489; }

	// end inline asm
	add.s64 	%rd3938, %rd3389, 4392;
	// begin inline asm
	{ atom.add.f64 %fd8587,[%rd3938],%fd1464; }

	// end inline asm
	add.s64 	%rd3939, %rd3389, 4400;
	// begin inline asm
	{ atom.add.f64 %fd8589,[%rd3939],%fd1463; }

	// end inline asm
	add.s64 	%rd3940, %rd3389, 4408;
	// begin inline asm
	{ atom.add.f64 %fd8591,[%rd3940],%fd1462; }

	// end inline asm
	add.s64 	%rd3941, %rd3389, 4416;
	// begin inline asm
	{ atom.add.f64 %fd8593,[%rd3941],%fd678; }

	// end inline asm
	add.s64 	%rd3942, %rd3389, 4424;
	// begin inline asm
	{ atom.add.f64 %fd8595,[%rd3942],%fd677; }

	// end inline asm
	add.s64 	%rd3943, %rd3389, 4432;
	// begin inline asm
	{ atom.add.f64 %fd8597,[%rd3943],%fd676; }

	// end inline asm
	add.s64 	%rd3944, %rd3389, 4440;
	// begin inline asm
	{ atom.add.f64 %fd8599,[%rd3944],%fd651; }

	// end inline asm
	add.s64 	%rd3945, %rd3389, 4448;
	// begin inline asm
	{ atom.add.f64 %fd8601,[%rd3945],%fd650; }

	// end inline asm
	add.s64 	%rd3946, %rd3389, 4456;
	// begin inline asm
	{ atom.add.f64 %fd8603,[%rd3946],%fd649; }

	// end inline asm
	add.s64 	%rd3947, %rd3389, 4464;
	// begin inline asm
	{ atom.add.f64 %fd8605,[%rd3947],%fd624; }

	// end inline asm
	add.s64 	%rd3948, %rd3389, 4472;
	// begin inline asm
	{ atom.add.f64 %fd8607,[%rd3948],%fd623; }

	// end inline asm
	add.s64 	%rd3949, %rd3389, 4480;
	// begin inline asm
	{ atom.add.f64 %fd8609,[%rd3949],%fd622; }

	// end inline asm
	add.s64 	%rd3950, %rd3389, 4488;
	// begin inline asm
	{ atom.add.f64 %fd8611,[%rd3950],%fd597; }

	// end inline asm
	add.s64 	%rd3951, %rd3389, 4496;
	// begin inline asm
	{ atom.add.f64 %fd8613,[%rd3951],%fd596; }

	// end inline asm
	add.s64 	%rd3952, %rd3389, 4504;
	// begin inline asm
	{ atom.add.f64 %fd8615,[%rd3952],%fd595; }

	// end inline asm
	add.s64 	%rd3953, %rd3389, 4512;
	// begin inline asm
	{ atom.add.f64 %fd8617,[%rd3953],%fd1542; }

	// end inline asm
	add.s64 	%rd3954, %rd3389, 4520;
	// begin inline asm
	{ atom.add.f64 %fd8619,[%rd3954],%fd1541; }

	// end inline asm
	add.s64 	%rd3955, %rd3389, 4528;
	// begin inline asm
	{ atom.add.f64 %fd8621,[%rd3955],%fd1540; }

	// end inline asm
	add.s64 	%rd3956, %rd3389, 4536;
	// begin inline asm
	{ atom.add.f64 %fd8623,[%rd3956],%fd1515; }

	// end inline asm
	add.s64 	%rd3957, %rd3389, 4544;
	// begin inline asm
	{ atom.add.f64 %fd8625,[%rd3957],%fd1514; }

	// end inline asm
	add.s64 	%rd3958, %rd3389, 4552;
	// begin inline asm
	{ atom.add.f64 %fd8627,[%rd3958],%fd1513; }

	// end inline asm
	add.s64 	%rd3959, %rd3389, 4560;
	// begin inline asm
	{ atom.add.f64 %fd8629,[%rd3959],%fd1488; }

	// end inline asm
	add.s64 	%rd3960, %rd3389, 4568;
	// begin inline asm
	{ atom.add.f64 %fd8631,[%rd3960],%fd1487; }

	// end inline asm
	add.s64 	%rd3961, %rd3389, 4576;
	// begin inline asm
	{ atom.add.f64 %fd8633,[%rd3961],%fd1486; }

	// end inline asm
	add.s64 	%rd3962, %rd3389, 4584;
	// begin inline asm
	{ atom.add.f64 %fd8635,[%rd3962],%fd1461; }

	// end inline asm
	add.s64 	%rd3963, %rd3389, 4592;
	// begin inline asm
	{ atom.add.f64 %fd8637,[%rd3963],%fd1460; }

	// end inline asm
	add.s64 	%rd3964, %rd3389, 4600;
	// begin inline asm
	{ atom.add.f64 %fd8639,[%rd3964],%fd1459; }

	// end inline asm

$L__BB13_659:
	ld.param.u64 	%rd3969, [assemble_matrix_cuda_kernel_backward_param_0+24];
	mov.u32 	%r2619, %ntid.x;
	mov.u32 	%r2006, %nctaid.x;
	mul.wide.u32 	%rd3965, %r2619, %r2006;
	add.s64 	%rd4034, %rd4034, %rd3965;
	setp.lt.u64 	%p940, %rd4034, %rd3969;
	@%p940 bra 	$L__BB13_2;

$L__BB13_660:
	ret;

}

 
