//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32965470
// Cuda compilation tools, release 12.2, V12.2.91
// Based on NVVM 7.0.1
//

.version 8.2
.target sm_86
.address_size 64

	// .globl	array_matmul_cuda_kernel_forward
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9all_hostsE = 1;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_35_bitE = 2;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_37_bitE = 4;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_50_bitE = 8;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_52_bitE = 16;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_53_bitE = 32;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_60_bitE = 64;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_61_bitE = 128;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_62_bitE = 256;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_70_bitE = 512;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_72_bitE = 1024;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_75_bitE = 2048;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_80_bitE = 4096;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_86_bitE = 8192;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_87_bitE = 16384;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_89_bitE = 32768;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail9sm_90_bitE = 65536;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target6detail11all_devicesE = 131070;
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target7is_hostE[8] = {1};
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target9is_deviceE[8] = {254, 255, 1};
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target10any_targetE[8] = {255, 255, 1};
.global .align 8 .b8 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target9no_targetE[8];
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_35E = 35;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_37E = 37;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_50E = 50;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_52E = 52;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_53E = 53;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_60E = 60;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_61E = 61;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_62E = 62;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_70E = 70;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_72E = 72;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_75E = 75;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_80E = 80;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_86E = 86;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_87E = 87;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_89E = 89;
.global .align 8 .u64 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2nv6target5sm_90E = 90;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN70_INTERNAL_00000000_39_wp_warp_ipc_utils_conjugate_gradient_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;

.visible .entry array_matmul_cuda_kernel_forward(
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_forward_param_3[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<25>;
	.reg .b32 	%r<81>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<76>;


	ld.param.v2.u32 	{%r34, %r35}, [array_matmul_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r36, %r37}, [array_matmul_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r42, %r43}, [array_matmul_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r50, %r51}, [array_matmul_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r58, %r59}, [array_matmul_cuda_kernel_forward_param_3+32];
	ld.param.u64 	%rd38, [array_matmul_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd36, [array_matmul_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd34, [array_matmul_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd33, [array_matmul_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [array_matmul_cuda_kernel_forward_param_0+16];
	mov.u32 	%r62, %ntid.x;
	cvt.u64.u32 	%rd1, %r62;
	mov.u32 	%r63, %ctaid.x;
	mul.wide.u32 	%rd40, %r62, %r63;
	mov.u32 	%r64, %tid.x;
	cvt.u64.u32 	%rd41, %r64;
	add.s64 	%rd69, %rd40, %rd41;
	setp.ge.u64 	%p1, %rd69, %rd33;
	@%p1 bra 	$L__BB0_25;

	cvta.to.global.u64 	%rd4, %rd38;
	cvta.to.global.u64 	%rd5, %rd36;
	cvta.to.global.u64 	%rd6, %rd34;
	cvt.s64.s32 	%rd7, %r58;
	cvt.s64.s32 	%rd8, %r37;
	cvt.s64.s32 	%rd9, %r36;
	cvt.s64.s32 	%rd10, %r35;
	cvt.s64.s32 	%rd11, %r42;
	cvt.s64.s32 	%rd12, %r50;
	mov.u32 	%r65, %nctaid.x;
	cvt.u64.u32 	%rd42, %r65;
	mul.lo.s64 	%rd13, %rd1, %rd42;
	setp.gt.s32 	%p2, %r6, 3;
	@%p2 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_2;

$L__BB0_12:
	cvt.u32.u64 	%r72, %rd8;
	cvt.u32.u64 	%r75, %rd9;
	cvt.u32.u64 	%r78, %rd10;

$L__BB0_13:
	or.b64  	%rd55, %rd69, %rd8;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p9, %rd56, 0;
	@%p9 bra 	$L__BB0_15;

	div.u64 	%rd74, %rd69, %rd8;
	bra.uni 	$L__BB0_16;

$L__BB0_15:
	cvt.u32.u64 	%r73, %rd69;
	div.u32 	%r74, %r73, %r72;
	cvt.u64.u32 	%rd74, %r74;

$L__BB0_16:
	setp.lt.s32 	%p10, %r6, 3;
	@%p10 bra 	$L__BB0_20;

	or.b64  	%rd57, %rd74, %rd9;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p11, %rd58, 0;
	@%p11 bra 	$L__BB0_19;

	div.u64 	%rd74, %rd74, %rd9;
	bra.uni 	$L__BB0_20;

$L__BB0_19:
	cvt.u32.u64 	%r76, %rd74;
	div.u32 	%r77, %r76, %r75;
	cvt.u64.u32 	%rd74, %r77;

$L__BB0_20:
	setp.lt.s32 	%p12, %r6, 2;
	@%p12 bra 	$L__BB0_24;

	or.b64  	%rd59, %rd74, %rd10;
	and.b64  	%rd60, %rd59, -4294967296;
	setp.eq.s64 	%p13, %rd60, 0;
	@%p13 bra 	$L__BB0_23;

	div.u64 	%rd74, %rd74, %rd10;
	bra.uni 	$L__BB0_24;

$L__BB0_23:
	cvt.u32.u64 	%r79, %rd74;
	div.u32 	%r80, %r79, %r78;
	cvt.u64.u32 	%rd74, %r80;

$L__BB0_24:
	cvt.s64.s32 	%rd61, %rd74;
	setp.gt.s32 	%p14, %r6, 0;
	selp.b64 	%rd62, %rd61, 0, %p14;
	mul.lo.s64 	%rd63, %rd62, %rd11;
	add.s64 	%rd64, %rd6, %rd63;
	mul.lo.s64 	%rd65, %rd62, %rd12;
	add.s64 	%rd66, %rd5, %rd65;
	ld.global.f64 	%fd22, [%rd66];
	ld.global.f64 	%fd23, [%rd64];
	ld.global.f64 	%fd24, [%rd64+24];
	ld.global.f64 	%fd25, [%rd64+48];
	ld.global.f64 	%fd26, [%rd66+8];
	ld.global.f64 	%fd27, [%rd64+8];
	mul.f64 	%fd28, %fd27, %fd26;
	ld.global.f64 	%fd29, [%rd64+32];
	mul.f64 	%fd30, %fd29, %fd26;
	ld.global.f64 	%fd31, [%rd64+56];
	mul.f64 	%fd32, %fd31, %fd26;
	fma.rn.f64 	%fd33, %fd23, %fd22, %fd28;
	fma.rn.f64 	%fd34, %fd24, %fd22, %fd30;
	fma.rn.f64 	%fd35, %fd25, %fd22, %fd32;
	ld.global.f64 	%fd36, [%rd66+16];
	ld.global.f64 	%fd37, [%rd64+16];
	ld.global.f64 	%fd38, [%rd64+40];
	ld.global.f64 	%fd39, [%rd64+64];
	fma.rn.f64 	%fd40, %fd37, %fd36, %fd33;
	fma.rn.f64 	%fd41, %fd38, %fd36, %fd34;
	fma.rn.f64 	%fd42, %fd39, %fd36, %fd35;
	mul.lo.s64 	%rd67, %rd62, %rd7;
	add.s64 	%rd68, %rd4, %rd67;
	st.global.f64 	[%rd68], %fd40;
	st.global.f64 	[%rd68+8], %fd41;
	st.global.f64 	[%rd68+16], %fd42;
	add.s64 	%rd69, %rd69, %rd13;
	setp.lt.u64 	%p15, %rd69, %rd33;
	@%p15 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_25;

$L__BB0_2:
	cvt.u32.u64 	%r66, %rd9;
	cvt.u32.u64 	%r69, %rd10;

$L__BB0_3:
	setp.lt.s32 	%p3, %r6, 3;
	mov.u64 	%rd70, %rd69;
	@%p3 bra 	$L__BB0_7;

	or.b64  	%rd43, %rd69, %rd9;
	and.b64  	%rd44, %rd43, -4294967296;
	setp.eq.s64 	%p4, %rd44, 0;
	@%p4 bra 	$L__BB0_6;

	div.u64 	%rd70, %rd69, %rd9;
	bra.uni 	$L__BB0_7;

$L__BB0_6:
	cvt.u32.u64 	%r67, %rd69;
	div.u32 	%r68, %r67, %r66;
	cvt.u64.u32 	%rd70, %r68;

$L__BB0_7:
	setp.lt.s32 	%p5, %r6, 2;
	@%p5 bra 	$L__BB0_11;

	or.b64  	%rd45, %rd70, %rd10;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p6, %rd46, 0;
	@%p6 bra 	$L__BB0_10;

	div.u64 	%rd70, %rd70, %rd10;
	bra.uni 	$L__BB0_11;

$L__BB0_10:
	cvt.u32.u64 	%r70, %rd70;
	div.u32 	%r71, %r70, %r69;
	cvt.u64.u32 	%rd70, %r71;

$L__BB0_11:
	cvt.s64.s32 	%rd47, %rd70;
	setp.gt.s32 	%p7, %r6, 0;
	selp.b64 	%rd48, %rd47, 0, %p7;
	mul.lo.s64 	%rd49, %rd48, %rd11;
	add.s64 	%rd50, %rd6, %rd49;
	mul.lo.s64 	%rd51, %rd48, %rd12;
	add.s64 	%rd52, %rd5, %rd51;
	ld.global.f64 	%fd1, [%rd52];
	ld.global.f64 	%fd2, [%rd50];
	ld.global.f64 	%fd3, [%rd50+24];
	ld.global.f64 	%fd4, [%rd50+48];
	ld.global.f64 	%fd5, [%rd52+8];
	ld.global.f64 	%fd6, [%rd50+8];
	mul.f64 	%fd7, %fd6, %fd5;
	ld.global.f64 	%fd8, [%rd50+32];
	mul.f64 	%fd9, %fd8, %fd5;
	ld.global.f64 	%fd10, [%rd50+56];
	mul.f64 	%fd11, %fd10, %fd5;
	fma.rn.f64 	%fd12, %fd2, %fd1, %fd7;
	fma.rn.f64 	%fd13, %fd3, %fd1, %fd9;
	fma.rn.f64 	%fd14, %fd4, %fd1, %fd11;
	ld.global.f64 	%fd15, [%rd52+16];
	ld.global.f64 	%fd16, [%rd50+16];
	ld.global.f64 	%fd17, [%rd50+40];
	ld.global.f64 	%fd18, [%rd50+64];
	fma.rn.f64 	%fd19, %fd16, %fd15, %fd12;
	fma.rn.f64 	%fd20, %fd17, %fd15, %fd13;
	fma.rn.f64 	%fd21, %fd18, %fd15, %fd14;
	mul.lo.s64 	%rd53, %rd48, %rd7;
	add.s64 	%rd54, %rd4, %rd53;
	st.global.f64 	[%rd54], %fd19;
	st.global.f64 	[%rd54+8], %fd20;
	st.global.f64 	[%rd54+16], %fd21;
	add.s64 	%rd69, %rd69, %rd13;
	setp.lt.u64 	%p8, %rd69, %rd33;
	@%p8 bra 	$L__BB0_3;

$L__BB0_25:
	ret;

}
	// .globl	array_matmul_cuda_kernel_backward
.visible .entry array_matmul_cuda_kernel_backward(
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 array_matmul_cuda_kernel_backward_param_6[56]
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<126>;
	.reg .f64 	%fd<103>;
	.reg .b64 	%rd<100>;


	ld.param.v2.u32 	{%r61, %r62}, [array_matmul_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r63, %r64}, [array_matmul_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r69, %r70}, [array_matmul_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r77, %r78}, [array_matmul_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r85, %r86}, [array_matmul_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r93, %r94}, [array_matmul_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r101, %r102}, [array_matmul_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r109, %r110}, [array_matmul_cuda_kernel_backward_param_6+32];
	ld.param.u64 	%rd49, [array_matmul_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd47, [array_matmul_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd45, [array_matmul_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd44, [array_matmul_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd42, [array_matmul_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd41, [array_matmul_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd40, [array_matmul_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd39, [array_matmul_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd38, [array_matmul_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [array_matmul_cuda_kernel_backward_param_0+16];
	mov.u32 	%r113, %ntid.x;
	cvt.u64.u32 	%rd1, %r113;
	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd51, %r113, %r114;
	mov.u32 	%r115, %tid.x;
	cvt.u64.u32 	%rd52, %r115;
	add.s64 	%rd96, %rd51, %rd52;
	setp.ge.u64 	%p1, %rd96, %rd38;
	@%p1 bra 	$L__BB1_27;

	cvta.to.global.u64 	%rd10, %rd49;
	cvta.to.global.u64 	%rd11, %rd44;
	cvta.to.global.u64 	%rd12, %rd41;
	cvta.to.global.u64 	%rd13, %rd39;
	cvt.s64.s32 	%rd14, %r64;
	cvt.s64.s32 	%rd15, %r63;
	cvt.s64.s32 	%rd16, %r62;
	cvt.s64.s32 	%rd17, %r69;
	cvt.s64.s32 	%rd18, %r77;
	cvt.s64.s32 	%rd19, %r109;
	cvt.s64.s32 	%rd20, %r85;
	cvt.s64.s32 	%rd21, %r101;
	mov.u32 	%r116, %nctaid.x;
	cvt.u64.u32 	%rd53, %r116;
	mul.lo.s64 	%rd22, %rd1, %rd53;
	cvt.s64.s32 	%rd23, %r93;

$L__BB1_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd97, %rd96;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd54, %rd96, %rd14;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p3, %rd55, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd97, %rd96, %rd14;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r117, %rd14;
	cvt.u32.u64 	%r118, %rd96;
	div.u32 	%r119, %r118, %r117;
	cvt.u64.u32 	%rd97, %r119;

$L__BB1_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd56, %rd97, %rd15;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p5, %rd57, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd97, %rd97, %rd15;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r120, %rd15;
	cvt.u32.u64 	%r121, %rd97;
	div.u32 	%r122, %r121, %r120;
	cvt.u64.u32 	%rd97, %r122;

$L__BB1_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd58, %rd97, %rd16;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p7, %rd59, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd97, %rd97, %rd16;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r123, %rd16;
	cvt.u32.u64 	%r124, %rd97;
	div.u32 	%r125, %r124, %r123;
	cvt.u64.u32 	%rd97, %r125;

$L__BB1_14:
	ld.param.u64 	%rd94, [array_matmul_cuda_kernel_backward_param_6];
	cvt.s64.s32 	%rd60, %rd97;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd34, %rd60, 0, %p8;
	mul.lo.s64 	%rd35, %rd34, %rd17;
	add.s64 	%rd61, %rd13, %rd35;
	mul.lo.s64 	%rd36, %rd34, %rd18;
	add.s64 	%rd62, %rd12, %rd36;
	ld.global.f64 	%fd1, [%rd61];
	ld.global.f64 	%fd2, [%rd61+8];
	ld.global.f64 	%fd3, [%rd61+16];
	ld.global.f64 	%fd4, [%rd61+24];
	ld.global.f64 	%fd5, [%rd61+32];
	ld.global.f64 	%fd6, [%rd61+40];
	ld.global.f64 	%fd7, [%rd61+48];
	ld.global.f64 	%fd8, [%rd61+56];
	ld.global.f64 	%fd9, [%rd61+64];
	ld.global.f64 	%fd10, [%rd62];
	ld.global.f64 	%fd11, [%rd62+8];
	ld.global.f64 	%fd12, [%rd62+16];
	setp.eq.s64 	%p9, %rd94, 0;
	@%p9 bra 	$L__BB1_16;

	mul.lo.s64 	%rd63, %rd34, %rd19;
	add.s64 	%rd64, %rd10, %rd63;
	ld.global.f64 	%fd34, [%rd64];
	add.f64 	%fd102, %fd34, 0d0000000000000000;
	ld.global.f64 	%fd35, [%rd64+8];
	add.f64 	%fd101, %fd35, 0d0000000000000000;
	ld.global.f64 	%fd36, [%rd64+16];
	add.f64 	%fd100, %fd36, 0d0000000000000000;
	bra.uni 	$L__BB1_18;

$L__BB1_16:
	ld.param.u64 	%rd95, [array_matmul_cuda_kernel_backward_param_3+8];
	setp.eq.s64 	%p10, %rd95, 0;
	mov.f64 	%fd100, 0d0000000000000000;
	mov.f64 	%fd101, %fd100;
	mov.f64 	%fd102, %fd100;
	@%p10 bra 	$L__BB1_18;

	mul.lo.s64 	%rd65, %rd34, %rd20;
	add.s64 	%rd66, %rd11, %rd65;
	ld.global.f64 	%fd40, [%rd66];
	add.f64 	%fd102, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd66+8];
	add.f64 	%fd101, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd66+16];
	add.f64 	%fd100, %fd42, 0d0000000000000000;

$L__BB1_18:
	fma.rn.f64 	%fd22, %fd102, %fd10, 0d0000000000000000;
	fma.rn.f64 	%fd23, %fd102, %fd11, 0d0000000000000000;
	fma.rn.f64 	%fd24, %fd102, %fd12, 0d0000000000000000;
	fma.rn.f64 	%fd25, %fd101, %fd10, 0d0000000000000000;
	fma.rn.f64 	%fd26, %fd101, %fd11, 0d0000000000000000;
	fma.rn.f64 	%fd27, %fd101, %fd12, 0d0000000000000000;
	fma.rn.f64 	%fd28, %fd100, %fd10, 0d0000000000000000;
	fma.rn.f64 	%fd29, %fd100, %fd11, 0d0000000000000000;
	fma.rn.f64 	%fd30, %fd100, %fd12, 0d0000000000000000;
	mul.f64 	%fd43, %fd4, %fd101;
	fma.rn.f64 	%fd44, %fd1, %fd102, %fd43;
	mul.f64 	%fd45, %fd5, %fd101;
	fma.rn.f64 	%fd46, %fd2, %fd102, %fd45;
	mul.f64 	%fd47, %fd6, %fd101;
	fma.rn.f64 	%fd48, %fd3, %fd102, %fd47;
	fma.rn.f64 	%fd49, %fd7, %fd100, %fd44;
	fma.rn.f64 	%fd50, %fd8, %fd100, %fd46;
	fma.rn.f64 	%fd51, %fd9, %fd100, %fd48;
	add.f64 	%fd31, %fd49, 0d0000000000000000;
	add.f64 	%fd32, %fd50, 0d0000000000000000;
	add.f64 	%fd33, %fd51, 0d0000000000000000;
	setp.eq.s64 	%p11, %rd47, 0;
	@%p11 bra 	$L__BB1_20;

	mul.lo.s64 	%rd70, %rd34, %rd21;
	add.s64 	%rd67, %rd47, %rd70;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd67],%fd31; }

	// end inline asm
	add.s64 	%rd68, %rd67, 8;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd68],%fd32; }

	// end inline asm
	add.s64 	%rd69, %rd67, 16;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd69],%fd33; }

	// end inline asm
	bra.uni 	$L__BB1_22;

$L__BB1_20:
	setp.eq.s64 	%p12, %rd42, 0;
	@%p12 bra 	$L__BB1_22;

	add.s64 	%rd71, %rd42, %rd36;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd71],%fd31; }

	// end inline asm
	add.s64 	%rd72, %rd71, 8;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd72],%fd32; }

	// end inline asm
	add.s64 	%rd73, %rd71, 16;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd73],%fd33; }

	// end inline asm

$L__BB1_22:
	setp.eq.s64 	%p13, %rd45, 0;
	@%p13 bra 	$L__BB1_24;

	mul.lo.s64 	%rd83, %rd34, %rd23;
	add.s64 	%rd74, %rd45, %rd83;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd74],%fd22; }

	// end inline asm
	add.s64 	%rd75, %rd74, 8;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd75],%fd23; }

	// end inline asm
	add.s64 	%rd76, %rd74, 16;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd76],%fd24; }

	// end inline asm
	add.s64 	%rd77, %rd74, 24;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd77],%fd25; }

	// end inline asm
	add.s64 	%rd78, %rd74, 32;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd78],%fd26; }

	// end inline asm
	add.s64 	%rd79, %rd74, 40;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd79],%fd27; }

	// end inline asm
	add.s64 	%rd80, %rd74, 48;
	// begin inline asm
	{ atom.add.f64 %fd76,[%rd80],%fd28; }

	// end inline asm
	add.s64 	%rd81, %rd74, 56;
	// begin inline asm
	{ atom.add.f64 %fd78,[%rd81],%fd29; }

	// end inline asm
	add.s64 	%rd82, %rd74, 64;
	// begin inline asm
	{ atom.add.f64 %fd80,[%rd82],%fd30; }

	// end inline asm
	bra.uni 	$L__BB1_26;

$L__BB1_24:
	setp.eq.s64 	%p14, %rd40, 0;
	@%p14 bra 	$L__BB1_26;

	add.s64 	%rd84, %rd40, %rd35;
	// begin inline asm
	{ atom.add.f64 %fd82,[%rd84],%fd22; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd84,[%rd85],%fd23; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd86,[%rd86],%fd24; }

	// end inline asm
	add.s64 	%rd87, %rd84, 24;
	// begin inline asm
	{ atom.add.f64 %fd88,[%rd87],%fd25; }

	// end inline asm
	add.s64 	%rd88, %rd84, 32;
	// begin inline asm
	{ atom.add.f64 %fd90,[%rd88],%fd26; }

	// end inline asm
	add.s64 	%rd89, %rd84, 40;
	// begin inline asm
	{ atom.add.f64 %fd92,[%rd89],%fd27; }

	// end inline asm
	add.s64 	%rd90, %rd84, 48;
	// begin inline asm
	{ atom.add.f64 %fd94,[%rd90],%fd28; }

	// end inline asm
	add.s64 	%rd91, %rd84, 56;
	// begin inline asm
	{ atom.add.f64 %fd96,[%rd91],%fd29; }

	// end inline asm
	add.s64 	%rd92, %rd84, 64;
	// begin inline asm
	{ atom.add.f64 %fd98,[%rd92],%fd30; }

	// end inline asm

$L__BB1_26:
	ld.param.u64 	%rd93, [array_matmul_cuda_kernel_backward_param_0+24];
	add.s64 	%rd96, %rd96, %rd22;
	setp.lt.u64 	%p15, %rd96, %rd93;
	@%p15 bra 	$L__BB1_2;

$L__BB1_27:
	ret;

}
	// .globl	axpy_cuda_kernel_forward
.visible .entry axpy_cuda_kernel_forward(
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 axpy_cuda_kernel_forward_param_6[56],
	.param .u32 axpy_cuda_kernel_forward_param_7,
	.param .u32 axpy_cuda_kernel_forward_param_8
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<152>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<110>;


	ld.param.v2.u32 	{%r67, %r68}, [axpy_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r69, %r70}, [axpy_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r75, %r76}, [axpy_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r83, %r84}, [axpy_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r91, %r92}, [axpy_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r99, %r100}, [axpy_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r107, %r108}, [axpy_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r115, %r116}, [axpy_cuda_kernel_forward_param_6+32];
	ld.param.u32 	%r65, [axpy_cuda_kernel_forward_param_7];
	ld.param.u32 	%r66, [axpy_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd50, [axpy_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd48, [axpy_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd46, [axpy_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd44, [axpy_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd42, [axpy_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd40, [axpy_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd39, [axpy_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r10, [axpy_cuda_kernel_forward_param_0+16];
	mov.u32 	%r119, %ntid.x;
	cvt.u64.u32 	%rd1, %r119;
	mov.u32 	%r120, %ctaid.x;
	mul.wide.u32 	%rd52, %r119, %r120;
	mov.u32 	%r121, %tid.x;
	cvt.u64.u32 	%rd53, %r121;
	add.s64 	%rd103, %rd52, %rd53;
	setp.ge.u64 	%p1, %rd103, %rd39;
	@%p1 bra 	$L__BB2_33;

	cvta.to.global.u64 	%rd4, %rd44;
	cvta.to.global.u64 	%rd5, %rd50;
	cvta.to.global.u64 	%rd6, %rd48;
	cvta.to.global.u64 	%rd7, %rd46;
	cvta.to.global.u64 	%rd8, %rd42;
	cvta.to.global.u64 	%rd9, %rd40;
	cvt.s64.s32 	%rd10, %r70;
	cvt.s64.s32 	%rd11, %r69;
	cvt.s64.s32 	%rd12, %r68;
	cvt.s64.s32 	%rd13, %r107;
	cvt.s64.s32 	%rd14, %r91;
	cvt.s64.s32 	%rd15, %r75;
	cvt.s64.s32 	%rd16, %r83;
	cvt.s64.s32 	%rd17, %r99;
	mov.u32 	%r122, %nctaid.x;
	cvt.u64.u32 	%rd54, %r122;
	mul.lo.s64 	%rd18, %rd1, %rd54;
	mul.hi.s32 	%r123, %r65, 1431655766;
	shr.u32 	%r124, %r123, 31;
	add.s32 	%r2, %r123, %r124;
	sub.s32 	%r3, %r66, %r2;
	cvt.s64.s32 	%rd19, %r115;
	setp.gt.s32 	%p2, %r10, 3;
	@%p2 bra 	$L__BB2_16;
	bra.uni 	$L__BB2_2;

$L__BB2_16:

$L__BB2_17:
	or.b64  	%rd78, %rd103, %rd10;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p12, %rd79, 0;
	@%p12 bra 	$L__BB2_19;

	div.u64 	%rd108, %rd103, %rd10;
	bra.uni 	$L__BB2_20;

$L__BB2_19:
	cvt.u32.u64 	%r137, %rd10;
	cvt.u32.u64 	%r138, %rd103;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd108, %r139;

$L__BB2_20:
	setp.lt.s32 	%p13, %r10, 3;
	@%p13 bra 	$L__BB2_24;

	or.b64  	%rd80, %rd108, %rd11;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p14, %rd81, 0;
	@%p14 bra 	$L__BB2_23;

	div.u64 	%rd108, %rd108, %rd11;
	bra.uni 	$L__BB2_24;

$L__BB2_23:
	cvt.u32.u64 	%r140, %rd11;
	cvt.u32.u64 	%r141, %rd108;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd108, %r142;

$L__BB2_24:
	setp.lt.s32 	%p15, %r10, 2;
	@%p15 bra 	$L__BB2_28;

	or.b64  	%rd82, %rd108, %rd12;
	and.b64  	%rd83, %rd82, -4294967296;
	setp.eq.s64 	%p16, %rd83, 0;
	@%p16 bra 	$L__BB2_27;

	div.u64 	%rd108, %rd108, %rd12;
	bra.uni 	$L__BB2_28;

$L__BB2_27:
	cvt.u32.u64 	%r143, %rd12;
	cvt.u32.u64 	%r144, %rd108;
	div.u32 	%r145, %r144, %r143;
	cvt.u64.u32 	%rd108, %r145;

$L__BB2_28:
	cvt.u32.u64 	%r146, %rd108;
	setp.gt.s32 	%p17, %r10, 0;
	selp.b32 	%r5, %r146, 0, %p17;
	setp.ge.s32 	%p18, %r5, %r2;
	@%p18 bra 	$L__BB2_30;

	shr.s32 	%r147, %r5, 31;
	shr.u32 	%r148, %r147, 30;
	add.s32 	%r149, %r5, %r148;
	shr.s32 	%r150, %r149, 2;
	cvt.s64.s32 	%rd84, %r150;
	mul.lo.s64 	%rd85, %rd84, %rd13;
	add.s64 	%rd86, %rd6, %rd85;
	ld.global.s32 	%rd87, [%rd86];
	mul.lo.s64 	%rd88, %rd87, %rd14;
	add.s64 	%rd89, %rd4, %rd88;
	ld.global.f64 	%fd41, [%rd89];

$L__BB2_30:
	setp.lt.s32 	%p19, %r5, %r2;
	@%p19 bra 	$L__BB2_32;

	add.s32 	%r151, %r3, %r5;
	cvt.s64.s32 	%rd90, %r151;
	mul.lo.s64 	%rd91, %rd90, %rd19;
	add.s64 	%rd92, %rd5, %rd91;
	ld.global.s32 	%rd93, [%rd92];
	mul.lo.s64 	%rd94, %rd93, %rd14;
	add.s64 	%rd95, %rd4, %rd94;
	ld.global.f64 	%fd42, [%rd95];

$L__BB2_32:
	selp.f64 	%fd25, %fd41, %fd42, %p19;
	cvt.s64.s32 	%rd96, %r5;
	mul.lo.s64 	%rd97, %rd96, %rd15;
	add.s64 	%rd98, %rd9, %rd97;
	mul.lo.s64 	%rd99, %rd96, %rd16;
	add.s64 	%rd100, %rd8, %rd99;
	ld.global.f64 	%fd26, [%rd100];
	ld.global.f64 	%fd27, [%rd100+8];
	ld.global.f64 	%fd28, [%rd100+16];
	ld.global.f64 	%fd29, [%rd98];
	fma.rn.f64 	%fd30, %fd26, %fd25, %fd29;
	ld.global.f64 	%fd31, [%rd98+8];
	fma.rn.f64 	%fd32, %fd25, %fd27, %fd31;
	ld.global.f64 	%fd33, [%rd98+16];
	fma.rn.f64 	%fd34, %fd25, %fd28, %fd33;
	mul.lo.s64 	%rd101, %rd96, %rd17;
	add.s64 	%rd102, %rd7, %rd101;
	st.global.f64 	[%rd102], %fd30;
	st.global.f64 	[%rd102+8], %fd32;
	st.global.f64 	[%rd102+16], %fd34;
	add.s64 	%rd103, %rd103, %rd18;
	setp.lt.u64 	%p21, %rd103, %rd39;
	@%p21 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_33;

$L__BB2_2:

$L__BB2_3:
	setp.lt.s32 	%p3, %r10, 3;
	mov.u64 	%rd104, %rd103;
	@%p3 bra 	$L__BB2_7;

	or.b64  	%rd55, %rd103, %rd11;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p4, %rd56, 0;
	@%p4 bra 	$L__BB2_6;

	div.u64 	%rd104, %rd103, %rd11;
	bra.uni 	$L__BB2_7;

$L__BB2_6:
	cvt.u32.u64 	%r125, %rd11;
	cvt.u32.u64 	%r126, %rd103;
	div.u32 	%r127, %r126, %r125;
	cvt.u64.u32 	%rd104, %r127;

$L__BB2_7:
	setp.lt.s32 	%p5, %r10, 2;
	@%p5 bra 	$L__BB2_11;

	or.b64  	%rd57, %rd104, %rd12;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p6, %rd58, 0;
	@%p6 bra 	$L__BB2_10;

	div.u64 	%rd104, %rd104, %rd12;
	bra.uni 	$L__BB2_11;

$L__BB2_10:
	cvt.u32.u64 	%r128, %rd12;
	cvt.u32.u64 	%r129, %rd104;
	div.u32 	%r130, %r129, %r128;
	cvt.u64.u32 	%rd104, %r130;

$L__BB2_11:
	cvt.u32.u64 	%r131, %rd104;
	setp.gt.s32 	%p7, %r10, 0;
	selp.b32 	%r4, %r131, 0, %p7;
	setp.ge.s32 	%p8, %r4, %r2;
	@%p8 bra 	$L__BB2_13;

	shr.s32 	%r132, %r4, 31;
	shr.u32 	%r133, %r132, 30;
	add.s32 	%r134, %r4, %r133;
	shr.s32 	%r135, %r134, 2;
	cvt.s64.s32 	%rd59, %r135;
	mul.lo.s64 	%rd60, %rd59, %rd13;
	add.s64 	%rd61, %rd6, %rd60;
	ld.global.s32 	%rd62, [%rd61];
	mul.lo.s64 	%rd63, %rd62, %rd14;
	add.s64 	%rd64, %rd4, %rd63;
	ld.global.f64 	%fd37, [%rd64];

$L__BB2_13:
	setp.lt.s32 	%p9, %r4, %r2;
	@%p9 bra 	$L__BB2_15;

	add.s32 	%r136, %r3, %r4;
	cvt.s64.s32 	%rd65, %r136;
	mul.lo.s64 	%rd66, %rd65, %rd19;
	add.s64 	%rd67, %rd5, %rd66;
	ld.global.s32 	%rd68, [%rd67];
	mul.lo.s64 	%rd69, %rd68, %rd14;
	add.s64 	%rd70, %rd4, %rd69;
	ld.global.f64 	%fd38, [%rd70];

$L__BB2_15:
	selp.f64 	%fd14, %fd37, %fd38, %p9;
	cvt.s64.s32 	%rd71, %r4;
	mul.lo.s64 	%rd72, %rd71, %rd15;
	add.s64 	%rd73, %rd9, %rd72;
	mul.lo.s64 	%rd74, %rd71, %rd16;
	add.s64 	%rd75, %rd8, %rd74;
	ld.global.f64 	%fd15, [%rd75];
	ld.global.f64 	%fd16, [%rd75+8];
	ld.global.f64 	%fd17, [%rd75+16];
	ld.global.f64 	%fd18, [%rd73];
	fma.rn.f64 	%fd19, %fd15, %fd14, %fd18;
	ld.global.f64 	%fd20, [%rd73+8];
	fma.rn.f64 	%fd21, %fd14, %fd16, %fd20;
	ld.global.f64 	%fd22, [%rd73+16];
	fma.rn.f64 	%fd23, %fd14, %fd17, %fd22;
	mul.lo.s64 	%rd76, %rd71, %rd17;
	add.s64 	%rd77, %rd7, %rd76;
	st.global.f64 	[%rd77], %fd19;
	st.global.f64 	[%rd77+8], %fd21;
	st.global.f64 	[%rd77+16], %fd23;
	add.s64 	%rd103, %rd103, %rd18;
	setp.lt.u64 	%p11, %rd103, %rd39;
	@%p11 bra 	$L__BB2_3;

$L__BB2_33:
	ret;

}
	// .globl	axpy_cuda_kernel_backward
.visible .entry axpy_cuda_kernel_backward(
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_6[56],
	.param .u32 axpy_cuda_kernel_backward_param_7,
	.param .u32 axpy_cuda_kernel_backward_param_8,
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 axpy_cuda_kernel_backward_param_14[56],
	.param .u32 axpy_cuda_kernel_backward_param_15,
	.param .u32 axpy_cuda_kernel_backward_param_16
)
{
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<81>;
	.reg .b32 	%r<222>;
	.reg .f64 	%fd<79>;
	.reg .b64 	%rd<126>;


	ld.param.v2.u32 	{%r108, %r109}, [axpy_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r110, %r111}, [axpy_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r116, %r117}, [axpy_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r124, %r125}, [axpy_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r132, %r133}, [axpy_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r140, %r141}, [axpy_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r148, %r149}, [axpy_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r156, %r157}, [axpy_cuda_kernel_backward_param_6+32];
	ld.param.u32 	%r70, [axpy_cuda_kernel_backward_param_7];
	ld.param.v2.u32 	{%r164, %r165}, [axpy_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r172, %r173}, [axpy_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r180, %r181}, [axpy_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r188, %r189}, [axpy_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd64, [axpy_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd62, [axpy_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd60, [axpy_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd58, [axpy_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd56, [axpy_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd54, [axpy_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd53, [axpy_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd51, [axpy_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd50, [axpy_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd49, [axpy_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd48, [axpy_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd47, [axpy_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd45, [axpy_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r15, [axpy_cuda_kernel_backward_param_0+16];
	mov.u32 	%r192, %ntid.x;
	cvt.u64.u32 	%rd1, %r192;
	mov.u32 	%r193, %ctaid.x;
	mul.wide.u32 	%rd66, %r192, %r193;
	mov.u32 	%r194, %tid.x;
	cvt.u64.u32 	%rd67, %r194;
	add.s64 	%rd122, %rd66, %rd67;
	setp.ge.u64 	%p1, %rd122, %rd45;
	@%p1 bra 	$L__BB3_41;

	cvta.to.global.u64 	%rd12, %rd50;
	cvta.to.global.u64 	%rd13, %rd64;
	cvta.to.global.u64 	%rd14, %rd56;
	cvta.to.global.u64 	%rd15, %rd54;
	cvta.to.global.u64 	%rd16, %rd53;
	cvta.to.global.u64 	%rd17, %rd48;
	cvt.s64.s32 	%rd18, %r111;
	cvt.s64.s32 	%rd19, %r110;
	cvt.s64.s32 	%rd20, %r109;
	cvt.s64.s32 	%rd21, %r148;
	cvt.s64.s32 	%rd22, %r132;
	cvt.s64.s32 	%rd23, %r124;
	mul.hi.s32 	%r196, %r70, 1431655766;
	shr.u32 	%r197, %r196, 31;
	add.s32 	%r2, %r196, %r197;
	cvt.s64.s32 	%rd24, %r156;
	cvt.s64.s32 	%rd25, %r188;
	cvt.s64.s32 	%rd26, %r140;
	cvt.s64.s32 	%rd27, %r172;
	cvt.s64.s32 	%rd28, %r164;
	cvt.s64.s32 	%rd29, %r116;
	mov.u32 	%r198, %nctaid.x;
	cvt.u64.u32 	%rd68, %r198;
	mul.lo.s64 	%rd30, %rd1, %rd68;
	cvt.s64.s32 	%rd31, %r180;

$L__BB3_2:
	setp.lt.s32 	%p2, %r15, 4;
	mov.u64 	%rd123, %rd122;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd69, %rd122, %rd18;
	and.b64  	%rd70, %rd69, -4294967296;
	setp.eq.s64 	%p3, %rd70, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd123, %rd122, %rd18;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r199, %rd18;
	cvt.u32.u64 	%r200, %rd122;
	div.u32 	%r201, %r200, %r199;
	cvt.u64.u32 	%rd123, %r201;

$L__BB3_6:
	setp.lt.s32 	%p4, %r15, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd71, %rd123, %rd19;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd123, %rd123, %rd19;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r202, %rd19;
	cvt.u32.u64 	%r203, %rd123;
	div.u32 	%r204, %r203, %r202;
	cvt.u64.u32 	%rd123, %r204;

$L__BB3_10:
	setp.lt.s32 	%p6, %r15, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd73, %rd123, %rd20;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p7, %rd74, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd123, %rd123, %rd20;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r205, %rd20;
	cvt.u32.u64 	%r206, %rd123;
	div.u32 	%r207, %r206, %r205;
	cvt.u64.u32 	%rd123, %r207;

$L__BB3_14:
	cvt.u32.u64 	%r208, %rd123;
	setp.gt.s32 	%p8, %r15, 0;
	selp.b32 	%r6, %r208, 0, %p8;
	setp.ge.s32 	%p9, %r6, %r2;
	@%p9 bra 	$L__BB3_16;

	shr.s32 	%r209, %r6, 31;
	shr.u32 	%r210, %r209, 30;
	add.s32 	%r211, %r6, %r210;
	shr.s32 	%r212, %r211, 2;
	cvt.s64.s32 	%rd75, %r212;
	mul.lo.s64 	%rd76, %rd75, %rd21;
	add.s64 	%rd77, %rd15, %rd76;
	ld.global.u32 	%r220, [%rd77];
	cvt.s64.s32 	%rd78, %r220;
	mul.lo.s64 	%rd79, %rd78, %rd22;
	add.s64 	%rd80, %rd12, %rd79;
	ld.global.f64 	%fd74, [%rd80];

$L__BB3_16:
	setp.lt.s32 	%p10, %r6, %r2;
	@%p10 bra 	$L__BB3_18;

	ld.param.u32 	%r217, [axpy_cuda_kernel_backward_param_8];
	sub.s32 	%r216, %r217, %r2;
	add.s32 	%r213, %r216, %r6;
	cvt.s64.s32 	%rd81, %r213;
	mul.lo.s64 	%rd82, %rd81, %rd24;
	add.s64 	%rd83, %rd14, %rd82;
	ld.global.u32 	%r221, [%rd83];
	cvt.s64.s32 	%rd84, %r221;
	mul.lo.s64 	%rd85, %rd84, %rd22;
	add.s64 	%rd86, %rd12, %rd85;
	ld.global.f64 	%fd75, [%rd86];

$L__BB3_18:
	ld.param.u64 	%rd120, [axpy_cuda_kernel_backward_param_12];
	cvt.s64.s32 	%rd42, %r6;
	mul.lo.s64 	%rd43, %rd42, %rd23;
	add.s64 	%rd87, %rd17, %rd43;
	ld.global.f64 	%fd7, [%rd87];
	ld.global.f64 	%fd8, [%rd87+8];
	ld.global.f64 	%fd9, [%rd87+16];
	setp.eq.s64 	%p11, %rd120, 0;
	@%p11 bra 	$L__BB3_20;

	mul.lo.s64 	%rd88, %rd42, %rd25;
	add.s64 	%rd89, %rd13, %rd88;
	ld.global.f64 	%fd27, [%rd89];
	add.f64 	%fd78, %fd27, 0d0000000000000000;
	ld.global.f64 	%fd28, [%rd89+8];
	add.f64 	%fd77, %fd28, 0d0000000000000000;
	ld.global.f64 	%fd29, [%rd89+16];
	add.f64 	%fd76, %fd29, 0d0000000000000000;
	bra.uni 	$L__BB3_22;

$L__BB3_20:
	ld.param.u64 	%rd121, [axpy_cuda_kernel_backward_param_4+8];
	setp.eq.s64 	%p12, %rd121, 0;
	mov.f64 	%fd76, 0d0000000000000000;
	mov.f64 	%fd77, %fd76;
	mov.f64 	%fd78, %fd76;
	@%p12 bra 	$L__BB3_22;

	mul.lo.s64 	%rd90, %rd42, %rd26;
	add.s64 	%rd91, %rd16, %rd90;
	ld.global.f64 	%fd33, [%rd91];
	add.f64 	%fd78, %fd33, 0d0000000000000000;
	ld.global.f64 	%fd34, [%rd91+8];
	add.f64 	%fd77, %fd34, 0d0000000000000000;
	ld.global.f64 	%fd35, [%rd91+16];
	add.f64 	%fd76, %fd35, 0d0000000000000000;

$L__BB3_22:
	selp.f64 	%fd36, %fd74, %fd75, %p10;
	add.f64 	%fd19, %fd78, 0d0000000000000000;
	fma.rn.f64 	%fd20, %fd36, %fd19, 0d0000000000000000;
	add.f64 	%fd21, %fd77, 0d0000000000000000;
	fma.rn.f64 	%fd22, %fd36, %fd21, 0d0000000000000000;
	add.f64 	%fd23, %fd76, 0d0000000000000000;
	fma.rn.f64 	%fd24, %fd36, %fd23, 0d0000000000000000;
	mul.f64 	%fd37, %fd8, %fd21;
	fma.rn.f64 	%fd38, %fd7, %fd19, %fd37;
	fma.rn.f64 	%fd39, %fd9, %fd23, %fd38;
	add.f64 	%fd25, %fd39, 0d0000000000000000;
	setp.eq.s64 	%p14, %rd60, 0;
	@%p14 bra 	$L__BB3_24;

	mul.lo.s64 	%rd95, %rd42, %rd27;
	add.s64 	%rd92, %rd60, %rd95;
	// begin inline asm
	{ atom.add.f64 %fd40,[%rd92],%fd20; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	// begin inline asm
	{ atom.add.f64 %fd42,[%rd93],%fd22; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd44,[%rd94],%fd24; }

	// end inline asm
	bra.uni 	$L__BB3_26;

$L__BB3_24:
	setp.eq.s64 	%p15, %rd49, 0;
	@%p15 bra 	$L__BB3_26;

	add.s64 	%rd96, %rd49, %rd43;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd96],%fd20; }

	// end inline asm
	add.s64 	%rd97, %rd96, 8;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd97],%fd22; }

	// end inline asm
	add.s64 	%rd98, %rd96, 16;
	// begin inline asm
	{ atom.add.f64 %fd50,[%rd98],%fd24; }

	// end inline asm

$L__BB3_26:
	setp.eq.s64 	%p16, %rd58, 0;
	@%p16 bra 	$L__BB3_28;

	mul.lo.s64 	%rd102, %rd42, %rd28;
	add.s64 	%rd99, %rd58, %rd102;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd99],%fd19; }

	// end inline asm
	add.s64 	%rd100, %rd99, 8;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd100],%fd21; }

	// end inline asm
	add.s64 	%rd101, %rd99, 16;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd101],%fd23; }

	// end inline asm
	bra.uni 	$L__BB3_30;

$L__BB3_28:
	setp.eq.s64 	%p17, %rd47, 0;
	@%p17 bra 	$L__BB3_30;

	mul.lo.s64 	%rd106, %rd42, %rd29;
	add.s64 	%rd103, %rd47, %rd106;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd103],%fd19; }

	// end inline asm
	add.s64 	%rd104, %rd103, 8;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd104],%fd21; }

	// end inline asm
	add.s64 	%rd105, %rd103, 16;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd105],%fd23; }

	// end inline asm

$L__BB3_30:
	@%p10 bra 	$L__BB3_35;

	setp.eq.s64 	%p19, %rd62, 0;
	@%p19 bra 	$L__BB3_33;

	cvt.s64.s32 	%rd108, %r221;
	mul.lo.s64 	%rd109, %rd108, %rd31;
	add.s64 	%rd107, %rd62, %rd109;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd107],%fd25; }

	// end inline asm
	bra.uni 	$L__BB3_35;

$L__BB3_33:
	setp.eq.s64 	%p20, %rd51, 0;
	@%p20 bra 	$L__BB3_35;

	cvt.s64.s32 	%rd111, %r221;
	mul.lo.s64 	%rd112, %rd111, %rd22;
	add.s64 	%rd110, %rd51, %rd112;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd110],%fd25; }

	// end inline asm

$L__BB3_35:
	setp.gt.s32 	%p26, %r15, 0;
	cvt.u32.u64 	%r215, %rd123;
	selp.b32 	%r214, %r215, 0, %p26;
	setp.ge.s32 	%p25, %r214, %r2;
	@%p25 bra 	$L__BB3_40;

	setp.eq.s64 	%p22, %rd62, 0;
	@%p22 bra 	$L__BB3_38;

	cvt.s64.s32 	%rd114, %r220;
	mul.lo.s64 	%rd115, %rd114, %rd31;
	add.s64 	%rd113, %rd62, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd113],%fd25; }

	// end inline asm
	bra.uni 	$L__BB3_40;

$L__BB3_38:
	setp.eq.s64 	%p23, %rd51, 0;
	@%p23 bra 	$L__BB3_40;

	cvt.s64.s32 	%rd117, %r220;
	mul.lo.s64 	%rd118, %rd117, %rd22;
	add.s64 	%rd116, %rd51, %rd118;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd116],%fd25; }

	// end inline asm

$L__BB3_40:
	ld.param.u64 	%rd119, [axpy_cuda_kernel_backward_param_0+24];
	add.s64 	%rd122, %rd122, %rd30;
	setp.lt.u64 	%p24, %rd122, %rd119;
	@%p24 bra 	$L__BB3_2;

$L__BB3_41:
	ret;

}
	// .globl	cg_one_iter_cuda_kernel_forward
.visible .entry cg_one_iter_cuda_kernel_forward(
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_forward_param_9[56],
	.param .u32 cg_one_iter_cuda_kernel_forward_param_10,
	.param .u32 cg_one_iter_cuda_kernel_forward_param_11
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<192>;
	.reg .f64 	%fd<55>;
	.reg .b64 	%rd<103>;


	ld.param.v2.u32 	{%r93, %r94}, [cg_one_iter_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r95, %r96}, [cg_one_iter_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r101, %r102}, [cg_one_iter_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r109, %r110}, [cg_one_iter_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r117, %r118}, [cg_one_iter_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r125, %r126}, [cg_one_iter_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r133, %r134}, [cg_one_iter_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r141, %r142}, [cg_one_iter_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r149, %r150}, [cg_one_iter_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r157, %r158}, [cg_one_iter_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r165, %r166}, [cg_one_iter_cuda_kernel_forward_param_9+32];
	ld.param.u32 	%r91, [cg_one_iter_cuda_kernel_forward_param_10];
	ld.param.u64 	%rd55, [cg_one_iter_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd53, [cg_one_iter_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd51, [cg_one_iter_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd49, [cg_one_iter_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd47, [cg_one_iter_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd45, [cg_one_iter_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd43, [cg_one_iter_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd41, [cg_one_iter_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd38, [cg_one_iter_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r9, [cg_one_iter_cuda_kernel_forward_param_0+16];
	mov.u32 	%r169, %ntid.x;
	cvt.u64.u32 	%rd1, %r169;
	mov.u32 	%r170, %ctaid.x;
	mul.wide.u32 	%rd57, %r169, %r170;
	mov.u32 	%r171, %tid.x;
	cvt.u64.u32 	%rd58, %r171;
	add.s64 	%rd99, %rd57, %rd58;
	setp.ge.u64 	%p1, %rd99, %rd38;
	@%p1 bra 	$L__BB4_19;

	cvta.to.global.u64 	%rd5, %rd45;
	cvta.to.global.u64 	%rd6, %rd55;
	cvta.to.global.u64 	%rd7, %rd53;
	cvta.to.global.u64 	%rd8, %rd51;
	cvta.to.global.u64 	%rd9, %rd49;
	cvta.to.global.u64 	%rd10, %rd47;
	cvta.to.global.u64 	%rd11, %rd43;
	cvta.to.global.u64 	%rd12, %rd41;
	cvt.s64.s32 	%rd14, %r117;
	cvt.s64.s32 	%rd15, %r96;
	cvt.s64.s32 	%rd16, %r95;
	cvt.s64.s32 	%rd17, %r94;
	cvt.s64.s32 	%rd18, %r157;
	cvt.s64.s32 	%rd19, %r125;
	cvt.s64.s32 	%rd20, %r133;
	cvt.s64.s32 	%rd21, %r101;
	mul.hi.s32 	%r172, %r91, 1431655766;
	shr.u32 	%r173, %r172, 31;
	add.s32 	%r2, %r172, %r173;
	cvt.s64.s32 	%rd22, %r165;
	cvt.s64.s32 	%rd23, %r141;
	cvt.s64.s32 	%rd24, %r109;
	cvt.s64.s32 	%rd25, %r149;
	mov.u32 	%r174, %nctaid.x;
	cvt.u64.u32 	%rd59, %r174;
	mul.lo.s64 	%rd26, %rd1, %rd59;

$L__BB4_2:
	setp.lt.s32 	%p2, %r9, 4;
	mov.u64 	%rd100, %rd99;
	@%p2 bra 	$L__BB4_6;

	or.b64  	%rd60, %rd99, %rd15;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p3, %rd61, 0;
	@%p3 bra 	$L__BB4_5;

	div.u64 	%rd100, %rd99, %rd15;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r175, %rd15;
	cvt.u32.u64 	%r176, %rd99;
	div.u32 	%r177, %r176, %r175;
	cvt.u64.u32 	%rd100, %r177;

$L__BB4_6:
	setp.lt.s32 	%p4, %r9, 3;
	@%p4 bra 	$L__BB4_10;

	or.b64  	%rd62, %rd100, %rd16;
	and.b64  	%rd63, %rd62, -4294967296;
	setp.eq.s64 	%p5, %rd63, 0;
	@%p5 bra 	$L__BB4_9;

	div.u64 	%rd100, %rd100, %rd16;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r178, %rd16;
	cvt.u32.u64 	%r179, %rd100;
	div.u32 	%r180, %r179, %r178;
	cvt.u64.u32 	%rd100, %r180;

$L__BB4_10:
	setp.lt.s32 	%p6, %r9, 2;
	@%p6 bra 	$L__BB4_14;

	or.b64  	%rd64, %rd100, %rd17;
	and.b64  	%rd65, %rd64, -4294967296;
	setp.eq.s64 	%p7, %rd65, 0;
	@%p7 bra 	$L__BB4_13;

	div.u64 	%rd100, %rd100, %rd17;
	bra.uni 	$L__BB4_14;

$L__BB4_13:
	cvt.u32.u64 	%r181, %rd17;
	cvt.u32.u64 	%r182, %rd100;
	div.u32 	%r183, %r182, %r181;
	cvt.u64.u32 	%rd100, %r183;

$L__BB4_14:
	cvt.u32.u64 	%r184, %rd100;
	setp.gt.s32 	%p8, %r9, 0;
	selp.b32 	%r4, %r184, 0, %p8;
	setp.ge.s32 	%p9, %r4, %r2;
	@%p9 bra 	$L__BB4_16;

	shr.s32 	%r185, %r4, 31;
	shr.u32 	%r186, %r185, 30;
	add.s32 	%r187, %r4, %r186;
	shr.s32 	%r188, %r187, 2;
	cvt.s64.s32 	%rd66, %r188;
	mul.lo.s64 	%rd67, %rd66, %rd18;
	add.s64 	%rd68, %rd7, %rd67;
	ld.global.s32 	%rd69, [%rd68];
	mul.lo.s64 	%rd70, %rd69, %rd19;
	add.s64 	%rd71, %rd5, %rd70;
	ld.global.f64 	%fd53, [%rd71];

$L__BB4_16:
	setp.lt.s32 	%p10, %r4, %r2;
	@%p10 bra 	$L__BB4_18;

	ld.param.u32 	%r191, [cg_one_iter_cuda_kernel_forward_param_11];
	sub.s32 	%r190, %r191, %r2;
	add.s32 	%r189, %r190, %r4;
	cvt.s64.s32 	%rd72, %r189;
	mul.lo.s64 	%rd73, %rd72, %rd22;
	add.s64 	%rd74, %rd6, %rd73;
	ld.global.s32 	%rd75, [%rd74];
	mul.lo.s64 	%rd76, %rd75, %rd19;
	add.s64 	%rd77, %rd5, %rd76;
	ld.global.f64 	%fd54, [%rd77];

$L__BB4_18:
	ld.param.u64 	%rd98, [cg_one_iter_cuda_kernel_forward_param_0+24];
	ld.param.u64 	%rd97, [cg_one_iter_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd96, [cg_one_iter_cuda_kernel_forward_param_2];
	selp.f64 	%fd20, %fd53, %fd54, %p10;
	cvt.s64.s32 	%rd84, %r4;
	mul.lo.s64 	%rd85, %rd84, %rd20;
	add.s64 	%rd86, %rd10, %rd85;
	ld.global.f64 	%fd21, [%rd86];
	mul.f64 	%fd9, %fd21, %fd20;
	ld.global.f64 	%fd22, [%rd86+8];
	mul.f64 	%fd11, %fd20, %fd22;
	ld.global.f64 	%fd23, [%rd86+16];
	mul.f64 	%fd13, %fd20, %fd23;
	mul.lo.s64 	%rd87, %rd84, %rd21;
	add.s64 	%rd78, %rd97, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd8,[%rd78],%fd9; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	// begin inline asm
	{ atom.add.f64 %fd10,[%rd79],%fd11; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd12,[%rd80],%fd13; }

	// end inline asm
	mul.lo.s64 	%rd88, %rd84, %rd23;
	add.s64 	%rd89, %rd9, %rd88;
	ld.global.f64 	%fd24, [%rd89];
	mul.f64 	%fd25, %fd20, %fd24;
	ld.global.f64 	%fd26, [%rd89+8];
	mul.f64 	%fd27, %fd20, %fd26;
	ld.global.f64 	%fd28, [%rd89+16];
	mul.f64 	%fd29, %fd20, %fd28;
	mul.lo.s64 	%rd90, %rd84, %rd24;
	add.s64 	%rd81, %rd96, %rd90;
	neg.f64 	%fd15, %fd25;
	neg.f64 	%fd17, %fd27;
	neg.f64 	%fd19, %fd29;
	// begin inline asm
	{ atom.add.f64 %fd14,[%rd81],%fd15; }

	// end inline asm
	add.s64 	%rd82, %rd81, 8;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd82],%fd17; }

	// end inline asm
	add.s64 	%rd83, %rd81, 16;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd83],%fd19; }

	// end inline asm
	mul.lo.s64 	%rd91, %rd84, %rd25;
	add.s64 	%rd92, %rd8, %rd91;
	add.s64 	%rd93, %rd12, %rd90;
	ld.global.f64 	%fd30, [%rd93];
	ld.global.f64 	%fd31, [%rd92];
	ld.global.f64 	%fd32, [%rd92+24];
	ld.global.f64 	%fd33, [%rd92+48];
	ld.global.f64 	%fd34, [%rd93+8];
	ld.global.f64 	%fd35, [%rd92+8];
	mul.f64 	%fd36, %fd35, %fd34;
	ld.global.f64 	%fd37, [%rd92+32];
	mul.f64 	%fd38, %fd37, %fd34;
	ld.global.f64 	%fd39, [%rd92+56];
	mul.f64 	%fd40, %fd39, %fd34;
	fma.rn.f64 	%fd41, %fd31, %fd30, %fd36;
	fma.rn.f64 	%fd42, %fd32, %fd30, %fd38;
	fma.rn.f64 	%fd43, %fd33, %fd30, %fd40;
	ld.global.f64 	%fd44, [%rd93+16];
	ld.global.f64 	%fd45, [%rd92+16];
	ld.global.f64 	%fd46, [%rd92+40];
	ld.global.f64 	%fd47, [%rd92+64];
	fma.rn.f64 	%fd48, %fd45, %fd44, %fd41;
	fma.rn.f64 	%fd49, %fd46, %fd44, %fd42;
	fma.rn.f64 	%fd50, %fd47, %fd44, %fd43;
	mul.lo.s64 	%rd94, %rd84, %rd14;
	add.s64 	%rd95, %rd11, %rd94;
	st.global.f64 	[%rd95], %fd48;
	st.global.f64 	[%rd95+8], %fd49;
	st.global.f64 	[%rd95+16], %fd50;
	add.s64 	%rd99, %rd99, %rd26;
	setp.lt.u64 	%p12, %rd99, %rd98;
	@%p12 bra 	$L__BB4_2;

$L__BB4_19:
	ret;

}
	// .globl	cg_one_iter_cuda_kernel_backward
.visible .entry cg_one_iter_cuda_kernel_backward(
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_9[56],
	.param .u32 cg_one_iter_cuda_kernel_backward_param_10,
	.param .u32 cg_one_iter_cuda_kernel_backward_param_11,
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_18[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_19[56],
	.param .align 8 .b8 cg_one_iter_cuda_kernel_backward_param_20[56],
	.param .u32 cg_one_iter_cuda_kernel_backward_param_21,
	.param .u32 cg_one_iter_cuda_kernel_backward_param_22
)
{
	.reg .pred 	%p<36>;
	.reg .b16 	%rs<129>;
	.reg .b32 	%r<324>;
	.reg .f64 	%fd<212>;
	.reg .b64 	%rd<199>;


	ld.param.v2.u32 	{%r162, %r163}, [cg_one_iter_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r164, %r165}, [cg_one_iter_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r170, %r171}, [cg_one_iter_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r178, %r179}, [cg_one_iter_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r186, %r187}, [cg_one_iter_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r194, %r195}, [cg_one_iter_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r202, %r203}, [cg_one_iter_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r210, %r211}, [cg_one_iter_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r218, %r219}, [cg_one_iter_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r226, %r227}, [cg_one_iter_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r234, %r235}, [cg_one_iter_cuda_kernel_backward_param_9+32];
	ld.param.u32 	%r97, [cg_one_iter_cuda_kernel_backward_param_10];
	ld.param.v2.u32 	{%r242, %r243}, [cg_one_iter_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r250, %r251}, [cg_one_iter_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r258, %r259}, [cg_one_iter_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r266, %r267}, [cg_one_iter_cuda_kernel_backward_param_15+32];
	ld.param.v2.u32 	{%r274, %r275}, [cg_one_iter_cuda_kernel_backward_param_16+32];
	ld.param.v2.u32 	{%r282, %r283}, [cg_one_iter_cuda_kernel_backward_param_17+32];
	ld.param.v2.u32 	{%r290, %r291}, [cg_one_iter_cuda_kernel_backward_param_18+32];
	ld.param.u64 	%rd98, [cg_one_iter_cuda_kernel_backward_param_18];
	ld.param.u64 	%rd96, [cg_one_iter_cuda_kernel_backward_param_17];
	ld.param.u64 	%rd94, [cg_one_iter_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd92, [cg_one_iter_cuda_kernel_backward_param_15];
	ld.param.u64 	%rd90, [cg_one_iter_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd88, [cg_one_iter_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd86, [cg_one_iter_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd84, [cg_one_iter_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd82, [cg_one_iter_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd81, [cg_one_iter_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd80, [cg_one_iter_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd79, [cg_one_iter_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd78, [cg_one_iter_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd77, [cg_one_iter_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd76, [cg_one_iter_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd75, [cg_one_iter_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd74, [cg_one_iter_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd73, [cg_one_iter_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd71, [cg_one_iter_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd70, [cg_one_iter_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd69, [cg_one_iter_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd67, [cg_one_iter_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r15, [cg_one_iter_cuda_kernel_backward_param_0+16];
	mov.u32 	%r294, %ntid.x;
	cvt.u64.u32 	%rd1, %r294;
	mov.u32 	%r295, %ctaid.x;
	mul.wide.u32 	%rd100, %r294, %r295;
	mov.u32 	%r296, %tid.x;
	cvt.u64.u32 	%rd101, %r296;
	add.s64 	%rd195, %rd100, %rd101;
	setp.ge.u64 	%p1, %rd195, %rd67;
	@%p1 bra 	$L__BB5_57;

	cvta.to.global.u64 	%rd18, %rd74;
	cvta.to.global.u64 	%rd19, %rd90;
	cvta.to.global.u64 	%rd21, %rd86;
	cvta.to.global.u64 	%rd22, %rd84;
	cvta.to.global.u64 	%rd23, %rd82;
	cvta.to.global.u64 	%rd24, %rd80;
	cvta.to.global.u64 	%rd25, %rd78;
	cvta.to.global.u64 	%rd26, %rd76;
	cvta.to.global.u64 	%rd27, %rd73;
	cvta.to.global.u64 	%rd29, %rd70;
	cvta.to.global.u64 	%rd30, %rd69;
	cvt.s64.s32 	%rd31, %r165;
	cvt.s64.s32 	%rd32, %r164;
	cvt.s64.s32 	%rd33, %r163;
	cvt.s64.s32 	%rd34, %r226;
	cvt.s64.s32 	%rd35, %r194;
	cvt.s64.s32 	%rd36, %r202;
	cvt.s64.s32 	%rd37, %r210;
	cvt.s64.s32 	%rd38, %r218;
	cvt.s64.s32 	%rd39, %r178;
	mul.hi.s32 	%r298, %r97, 1431655766;
	shr.u32 	%r299, %r298, 31;
	add.s32 	%r2, %r298, %r299;
	cvt.s64.s32 	%rd40, %r234;
	cvt.s64.s32 	%rd41, %r258;
	cvt.s64.s32 	%rd42, %r186;
	cvt.s64.s32 	%rd43, %r250;
	cvt.s64.s32 	%rd44, %r290;
	cvt.s64.s32 	%rd45, %r282;
	cvt.s64.s32 	%rd46, %r242;
	cvt.s64.s32 	%rd47, %r170;
	cvt.s64.s32 	%rd48, %r274;
	mov.u32 	%r300, %nctaid.x;
	cvt.u64.u32 	%rd102, %r300;
	mul.lo.s64 	%rd49, %rd1, %rd102;
	cvt.s64.s32 	%rd50, %r266;

$L__BB5_2:
	setp.lt.s32 	%p2, %r15, 4;
	mov.u64 	%rd196, %rd195;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd103, %rd195, %rd31;
	and.b64  	%rd104, %rd103, -4294967296;
	setp.eq.s64 	%p3, %rd104, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd196, %rd195, %rd31;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r301, %rd31;
	cvt.u32.u64 	%r302, %rd195;
	div.u32 	%r303, %r302, %r301;
	cvt.u64.u32 	%rd196, %r303;

$L__BB5_6:
	setp.lt.s32 	%p4, %r15, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd105, %rd196, %rd32;
	and.b64  	%rd106, %rd105, -4294967296;
	setp.eq.s64 	%p5, %rd106, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd196, %rd196, %rd32;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r304, %rd32;
	cvt.u32.u64 	%r305, %rd196;
	div.u32 	%r306, %r305, %r304;
	cvt.u64.u32 	%rd196, %r306;

$L__BB5_10:
	setp.lt.s32 	%p6, %r15, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd107, %rd196, %rd33;
	and.b64  	%rd108, %rd107, -4294967296;
	setp.eq.s64 	%p7, %rd108, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd196, %rd196, %rd33;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r307, %rd33;
	cvt.u32.u64 	%r308, %rd196;
	div.u32 	%r309, %r308, %r307;
	cvt.u64.u32 	%rd196, %r309;

$L__BB5_14:
	cvt.u32.u64 	%r310, %rd196;
	setp.gt.s32 	%p8, %r15, 0;
	selp.b32 	%r6, %r310, 0, %p8;
	setp.ge.s32 	%p9, %r6, %r2;
	@%p9 bra 	$L__BB5_16;

	shr.s32 	%r311, %r6, 31;
	shr.u32 	%r312, %r311, 30;
	add.s32 	%r313, %r6, %r312;
	shr.s32 	%r314, %r313, 2;
	cvt.s64.s32 	%rd109, %r314;
	mul.lo.s64 	%rd110, %rd109, %rd34;
	add.s64 	%rd111, %rd23, %rd110;
	ld.global.u32 	%r322, [%rd111];
	cvt.s64.s32 	%rd112, %r322;
	mul.lo.s64 	%rd113, %rd112, %rd35;
	add.s64 	%rd114, %rd18, %rd113;
	ld.global.f64 	%fd201, [%rd114];

$L__BB5_16:
	setp.lt.s32 	%p10, %r6, %r2;
	@%p10 bra 	$L__BB5_18;

	ld.param.u32 	%r319, [cg_one_iter_cuda_kernel_backward_param_11];
	sub.s32 	%r318, %r319, %r2;
	add.s32 	%r315, %r318, %r6;
	cvt.s64.s32 	%rd115, %r315;
	mul.lo.s64 	%rd116, %rd115, %rd40;
	add.s64 	%rd117, %rd22, %rd116;
	ld.global.u32 	%r323, [%rd117];
	cvt.s64.s32 	%rd118, %r323;
	mul.lo.s64 	%rd119, %rd118, %rd35;
	add.s64 	%rd120, %rd18, %rd119;
	ld.global.f64 	%fd202, [%rd120];

$L__BB5_18:
	ld.param.u64 	%rd191, [cg_one_iter_cuda_kernel_backward_param_14];
	cvt.s64.s32 	%rd61, %r6;
	mul.lo.s64 	%rd62, %rd61, %rd36;
	add.s64 	%rd121, %rd26, %rd62;
	ld.global.f64 	%fd7, [%rd121];
	ld.global.f64 	%fd8, [%rd121+8];
	ld.global.f64 	%fd9, [%rd121+16];
	mul.lo.s64 	%rd63, %rd61, %rd37;
	add.s64 	%rd122, %rd25, %rd63;
	ld.global.f64 	%fd10, [%rd122];
	ld.global.f64 	%fd11, [%rd122+8];
	ld.global.f64 	%fd12, [%rd122+16];
	mul.lo.s64 	%rd64, %rd61, %rd38;
	add.s64 	%rd123, %rd24, %rd64;
	mul.lo.s64 	%rd65, %rd61, %rd39;
	add.s64 	%rd124, %rd29, %rd65;
	ld.global.f64 	%fd13, [%rd123];
	ld.global.f64 	%fd14, [%rd123+8];
	ld.global.f64 	%fd15, [%rd123+16];
	ld.global.f64 	%fd16, [%rd123+24];
	ld.global.f64 	%fd17, [%rd123+32];
	ld.global.f64 	%fd18, [%rd123+40];
	ld.global.f64 	%fd19, [%rd123+48];
	ld.global.f64 	%fd20, [%rd123+56];
	ld.global.f64 	%fd21, [%rd123+64];
	ld.global.f64 	%fd22, [%rd124];
	ld.global.f64 	%fd23, [%rd124+8];
	ld.global.f64 	%fd24, [%rd124+16];
	selp.f64 	%fd25, %fd201, %fd202, %p10;
	setp.eq.s64 	%p12, %rd191, 0;
	@%p12 bra 	$L__BB5_20;

	mul.lo.s64 	%rd125, %rd61, %rd41;
	add.s64 	%rd126, %rd19, %rd125;
	ld.global.f64 	%fd75, [%rd126];
	add.f64 	%fd205, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd126+8];
	add.f64 	%fd204, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd126+16];
	add.f64 	%fd203, %fd77, 0d0000000000000000;
	bra.uni 	$L__BB5_22;

$L__BB5_20:
	ld.param.u64 	%rd192, [cg_one_iter_cuda_kernel_backward_param_3+8];
	setp.eq.s64 	%p13, %rd192, 0;
	mov.f64 	%fd203, 0d0000000000000000;
	mov.f64 	%fd204, %fd203;
	mov.f64 	%fd205, %fd203;
	@%p13 bra 	$L__BB5_22;

	mul.lo.s64 	%rd127, %rd61, %rd42;
	add.s64 	%rd128, %rd27, %rd127;
	ld.global.f64 	%fd81, [%rd128];
	add.f64 	%fd205, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd128+8];
	add.f64 	%fd204, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd128+16];
	add.f64 	%fd203, %fd83, 0d0000000000000000;

$L__BB5_22:
	fma.rn.f64 	%fd35, %fd205, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd36, %fd205, %fd23, 0d0000000000000000;
	fma.rn.f64 	%fd37, %fd205, %fd24, 0d0000000000000000;
	fma.rn.f64 	%fd38, %fd204, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd39, %fd204, %fd23, 0d0000000000000000;
	fma.rn.f64 	%fd40, %fd204, %fd24, 0d0000000000000000;
	fma.rn.f64 	%fd41, %fd203, %fd22, 0d0000000000000000;
	fma.rn.f64 	%fd42, %fd203, %fd23, 0d0000000000000000;
	fma.rn.f64 	%fd43, %fd203, %fd24, 0d0000000000000000;
	mul.f64 	%fd84, %fd16, %fd204;
	fma.rn.f64 	%fd85, %fd13, %fd205, %fd84;
	mul.f64 	%fd86, %fd17, %fd204;
	fma.rn.f64 	%fd87, %fd14, %fd205, %fd86;
	mul.f64 	%fd88, %fd18, %fd204;
	fma.rn.f64 	%fd89, %fd15, %fd205, %fd88;
	fma.rn.f64 	%fd90, %fd19, %fd203, %fd85;
	fma.rn.f64 	%fd91, %fd20, %fd203, %fd87;
	fma.rn.f64 	%fd92, %fd21, %fd203, %fd89;
	add.f64 	%fd44, %fd90, 0d0000000000000000;
	add.f64 	%fd45, %fd91, 0d0000000000000000;
	add.f64 	%fd46, %fd92, 0d0000000000000000;
	setp.eq.s64 	%p14, %rd88, 0;
	@%p14 bra 	$L__BB5_24;

	mul.lo.s64 	%rd132, %rd61, %rd43;
	add.s64 	%rd129, %rd88, %rd132;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd129],%fd44; }

	// end inline asm
	add.s64 	%rd130, %rd129, 8;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd130],%fd45; }

	// end inline asm
	add.s64 	%rd131, %rd129, 16;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd131],%fd46; }

	// end inline asm
	bra.uni 	$L__BB5_26;

$L__BB5_24:
	setp.eq.s64 	%p15, %rd71, 0;
	@%p15 bra 	$L__BB5_26;

	add.s64 	%rd133, %rd71, %rd65;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd133],%fd44; }

	// end inline asm
	add.s64 	%rd134, %rd133, 8;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd134],%fd45; }

	// end inline asm
	add.s64 	%rd135, %rd133, 16;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd135],%fd46; }

	// end inline asm

$L__BB5_26:
	setp.eq.s64 	%p16, %rd98, 0;
	@%p16 bra 	$L__BB5_28;

	mul.lo.s64 	%rd145, %rd61, %rd44;
	add.s64 	%rd136, %rd98, %rd145;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd136],%fd35; }

	// end inline asm
	add.s64 	%rd137, %rd136, 8;
	// begin inline asm
	{ atom.add.f64 %fd107,[%rd137],%fd36; }

	// end inline asm
	add.s64 	%rd138, %rd136, 16;
	// begin inline asm
	{ atom.add.f64 %fd109,[%rd138],%fd37; }

	// end inline asm
	add.s64 	%rd139, %rd136, 24;
	// begin inline asm
	{ atom.add.f64 %fd111,[%rd139],%fd38; }

	// end inline asm
	add.s64 	%rd140, %rd136, 32;
	// begin inline asm
	{ atom.add.f64 %fd113,[%rd140],%fd39; }

	// end inline asm
	add.s64 	%rd141, %rd136, 40;
	// begin inline asm
	{ atom.add.f64 %fd115,[%rd141],%fd40; }

	// end inline asm
	add.s64 	%rd142, %rd136, 48;
	// begin inline asm
	{ atom.add.f64 %fd117,[%rd142],%fd41; }

	// end inline asm
	add.s64 	%rd143, %rd136, 56;
	// begin inline asm
	{ atom.add.f64 %fd119,[%rd143],%fd42; }

	// end inline asm
	add.s64 	%rd144, %rd136, 64;
	// begin inline asm
	{ atom.add.f64 %fd121,[%rd144],%fd43; }

	// end inline asm
	bra.uni 	$L__BB5_30;

$L__BB5_28:
	setp.eq.s64 	%p17, %rd81, 0;
	@%p17 bra 	$L__BB5_30;

	add.s64 	%rd146, %rd81, %rd64;
	// begin inline asm
	{ atom.add.f64 %fd123,[%rd146],%fd35; }

	// end inline asm
	add.s64 	%rd147, %rd146, 8;
	// begin inline asm
	{ atom.add.f64 %fd125,[%rd147],%fd36; }

	// end inline asm
	add.s64 	%rd148, %rd146, 16;
	// begin inline asm
	{ atom.add.f64 %fd127,[%rd148],%fd37; }

	// end inline asm
	add.s64 	%rd149, %rd146, 24;
	// begin inline asm
	{ atom.add.f64 %fd129,[%rd149],%fd38; }

	// end inline asm
	add.s64 	%rd150, %rd146, 32;
	// begin inline asm
	{ atom.add.f64 %fd131,[%rd150],%fd39; }

	// end inline asm
	add.s64 	%rd151, %rd146, 40;
	// begin inline asm
	{ atom.add.f64 %fd133,[%rd151],%fd40; }

	// end inline asm
	add.s64 	%rd152, %rd146, 48;
	// begin inline asm
	{ atom.add.f64 %fd135,[%rd152],%fd41; }

	// end inline asm
	add.s64 	%rd153, %rd146, 56;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd153],%fd42; }

	// end inline asm
	add.s64 	%rd154, %rd146, 64;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd154],%fd43; }

	// end inline asm

$L__BB5_30:
	setp.eq.s64 	%p33, %rd88, 0;
	@%p33 bra 	$L__BB5_32;

	cvta.to.global.u64 	%rd188, %rd88;
	mul.lo.s64 	%rd155, %rd61, %rd43;
	add.s64 	%rd156, %rd188, %rd155;
	ld.global.f64 	%fd141, [%rd156];
	mov.f64 	%fd142, 0d0000000000000000;
	sub.f64 	%fd208, %fd142, %fd141;
	ld.global.f64 	%fd143, [%rd156+8];
	sub.f64 	%fd207, %fd142, %fd143;
	ld.global.f64 	%fd144, [%rd156+16];
	sub.f64 	%fd206, %fd142, %fd144;
	bra.uni 	$L__BB5_34;

$L__BB5_32:
	setp.eq.s64 	%p19, %rd71, 0;
	mov.f64 	%fd206, 0d0000000000000000;
	mov.f64 	%fd207, %fd206;
	mov.f64 	%fd208, %fd206;
	@%p19 bra 	$L__BB5_34;

	cvta.to.global.u64 	%rd194, %rd71;
	add.s64 	%rd157, %rd194, %rd65;
	ld.global.f64 	%fd148, [%rd157];
	mov.f64 	%fd149, 0d0000000000000000;
	sub.f64 	%fd208, %fd149, %fd148;
	ld.global.f64 	%fd150, [%rd157+8];
	sub.f64 	%fd207, %fd149, %fd150;
	ld.global.f64 	%fd151, [%rd157+16];
	sub.f64 	%fd206, %fd149, %fd151;

$L__BB5_34:
	fma.rn.f64 	%fd56, %fd25, %fd208, 0d0000000000000000;
	fma.rn.f64 	%fd57, %fd25, %fd207, 0d0000000000000000;
	fma.rn.f64 	%fd58, %fd25, %fd206, 0d0000000000000000;
	mul.f64 	%fd152, %fd11, %fd207;
	fma.rn.f64 	%fd153, %fd10, %fd208, %fd152;
	fma.rn.f64 	%fd59, %fd12, %fd206, %fd153;
	setp.eq.s64 	%p20, %rd96, 0;
	@%p20 bra 	$L__BB5_36;

	mul.lo.s64 	%rd161, %rd61, %rd45;
	add.s64 	%rd158, %rd96, %rd161;
	// begin inline asm
	{ atom.add.f64 %fd154,[%rd158],%fd56; }

	// end inline asm
	add.s64 	%rd159, %rd158, 8;
	// begin inline asm
	{ atom.add.f64 %fd156,[%rd159],%fd57; }

	// end inline asm
	add.s64 	%rd160, %rd158, 16;
	// begin inline asm
	{ atom.add.f64 %fd158,[%rd160],%fd58; }

	// end inline asm
	bra.uni 	$L__BB5_38;

$L__BB5_36:
	setp.eq.s64 	%p21, %rd79, 0;
	@%p21 bra 	$L__BB5_38;

	add.s64 	%rd162, %rd79, %rd63;
	// begin inline asm
	{ atom.add.f64 %fd160,[%rd162],%fd56; }

	// end inline asm
	add.s64 	%rd163, %rd162, 8;
	// begin inline asm
	{ atom.add.f64 %fd162,[%rd163],%fd57; }

	// end inline asm
	add.s64 	%rd164, %rd162, 16;
	// begin inline asm
	{ atom.add.f64 %fd164,[%rd164],%fd58; }

	// end inline asm

$L__BB5_38:
	ld.param.u64 	%rd189, [cg_one_iter_cuda_kernel_backward_param_12];
	setp.eq.s64 	%p22, %rd189, 0;
	@%p22 bra 	$L__BB5_40;

	mul.lo.s64 	%rd165, %rd61, %rd46;
	add.s64 	%rd166, %rd21, %rd165;
	ld.global.f64 	%fd166, [%rd166];
	add.f64 	%fd211, %fd166, 0d0000000000000000;
	ld.global.f64 	%fd167, [%rd166+8];
	add.f64 	%fd210, %fd167, 0d0000000000000000;
	ld.global.f64 	%fd168, [%rd166+16];
	add.f64 	%fd209, %fd168, 0d0000000000000000;
	bra.uni 	$L__BB5_42;

$L__BB5_40:
	ld.param.u64 	%rd193, [cg_one_iter_cuda_kernel_backward_param_1+8];
	setp.eq.s64 	%p23, %rd193, 0;
	mov.f64 	%fd209, 0d0000000000000000;
	mov.f64 	%fd210, %fd209;
	mov.f64 	%fd211, %fd209;
	@%p23 bra 	$L__BB5_42;

	mul.lo.s64 	%rd167, %rd61, %rd47;
	add.s64 	%rd168, %rd30, %rd167;
	ld.global.f64 	%fd172, [%rd168];
	add.f64 	%fd211, %fd172, 0d0000000000000000;
	ld.global.f64 	%fd173, [%rd168+8];
	add.f64 	%fd210, %fd173, 0d0000000000000000;
	ld.global.f64 	%fd174, [%rd168+16];
	add.f64 	%fd209, %fd174, 0d0000000000000000;

$L__BB5_42:
	fma.rn.f64 	%fd69, %fd25, %fd211, 0d0000000000000000;
	fma.rn.f64 	%fd70, %fd25, %fd210, 0d0000000000000000;
	fma.rn.f64 	%fd71, %fd25, %fd209, 0d0000000000000000;
	mul.f64 	%fd175, %fd8, %fd210;
	fma.rn.f64 	%fd176, %fd7, %fd211, %fd175;
	fma.rn.f64 	%fd177, %fd9, %fd209, %fd176;
	add.f64 	%fd178, %fd59, 0d0000000000000000;
	add.f64 	%fd72, %fd178, %fd177;
	setp.eq.s64 	%p24, %rd94, 0;
	@%p24 bra 	$L__BB5_44;

	mul.lo.s64 	%rd172, %rd61, %rd48;
	add.s64 	%rd169, %rd94, %rd172;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd169],%fd69; }

	// end inline asm
	add.s64 	%rd170, %rd169, 8;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd170],%fd70; }

	// end inline asm
	add.s64 	%rd171, %rd169, 16;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd171],%fd71; }

	// end inline asm
	bra.uni 	$L__BB5_46;

$L__BB5_44:
	setp.eq.s64 	%p25, %rd77, 0;
	@%p25 bra 	$L__BB5_46;

	add.s64 	%rd173, %rd77, %rd62;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd173],%fd69; }

	// end inline asm
	add.s64 	%rd174, %rd173, 8;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd174],%fd70; }

	// end inline asm
	add.s64 	%rd175, %rd173, 16;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd175],%fd71; }

	// end inline asm

$L__BB5_46:
	add.f64 	%fd73, %fd72, 0d0000000000000000;
	@%p10 bra 	$L__BB5_51;

	setp.eq.s64 	%p27, %rd92, 0;
	@%p27 bra 	$L__BB5_49;

	cvt.s64.s32 	%rd177, %r323;
	mul.lo.s64 	%rd178, %rd177, %rd50;
	add.s64 	%rd176, %rd92, %rd178;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd176],%fd73; }

	// end inline asm
	bra.uni 	$L__BB5_51;

$L__BB5_49:
	setp.eq.s64 	%p28, %rd75, 0;
	@%p28 bra 	$L__BB5_51;

	cvt.s64.s32 	%rd180, %r323;
	mul.lo.s64 	%rd181, %rd180, %rd35;
	add.s64 	%rd179, %rd75, %rd181;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd179],%fd73; }

	// end inline asm

$L__BB5_51:
	setp.gt.s32 	%p35, %r15, 0;
	cvt.u32.u64 	%r317, %rd196;
	selp.b32 	%r316, %r317, 0, %p35;
	setp.ge.s32 	%p34, %r316, %r2;
	@%p34 bra 	$L__BB5_56;

	setp.eq.s64 	%p30, %rd92, 0;
	@%p30 bra 	$L__BB5_54;

	cvt.s64.s32 	%rd183, %r322;
	mul.lo.s64 	%rd184, %rd183, %rd50;
	add.s64 	%rd182, %rd92, %rd184;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd182],%fd73; }

	// end inline asm
	bra.uni 	$L__BB5_56;

$L__BB5_54:
	setp.eq.s64 	%p31, %rd75, 0;
	@%p31 bra 	$L__BB5_56;

	cvt.s64.s32 	%rd186, %r322;
	mul.lo.s64 	%rd187, %rd186, %rd35;
	add.s64 	%rd185, %rd75, %rd187;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd185],%fd73; }

	// end inline asm

$L__BB5_56:
	ld.param.u64 	%rd190, [cg_one_iter_cuda_kernel_backward_param_0+24];
	add.s64 	%rd195, %rd195, %rd49;
	setp.lt.u64 	%p32, %rd195, %rd190;
	@%p32 bra 	$L__BB5_2;

$L__BB5_57:
	ret;

}
	// .globl	array_inv_cuda_kernel_forward
.visible .entry array_inv_cuda_kernel_forward(
	.param .align 8 .b8 array_inv_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 array_inv_cuda_kernel_forward_param_1[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<41>;
	.reg .f64 	%fd<77>;
	.reg .b64 	%rd<42>;


	ld.param.v2.u32 	{%r16, %r17}, [array_inv_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r18, %r19}, [array_inv_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r24, %r25}, [array_inv_cuda_kernel_forward_param_1+32];
	ld.param.u64 	%rd23, [array_inv_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd22, [array_inv_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [array_inv_cuda_kernel_forward_param_0+16];
	mov.u32 	%r28, %ntid.x;
	cvt.u64.u32 	%rd1, %r28;
	mov.u32 	%r29, %ctaid.x;
	mul.wide.u32 	%rd25, %r28, %r29;
	mov.u32 	%r30, %tid.x;
	cvt.u64.u32 	%rd26, %r30;
	add.s64 	%rd38, %rd25, %rd26;
	setp.ge.u64 	%p1, %rd38, %rd22;
	@%p1 bra 	$L__BB6_17;

	cvta.to.global.u64 	%rd4, %rd23;
	cvt.s64.s32 	%rd5, %r19;
	cvt.s64.s32 	%rd6, %r18;
	cvt.s64.s32 	%rd7, %r17;
	cvt.s64.s32 	%rd8, %r24;
	mov.u32 	%r31, %nctaid.x;
	cvt.u64.u32 	%rd27, %r31;
	mul.lo.s64 	%rd9, %rd1, %rd27;

$L__BB6_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd39, %rd38;
	@%p2 bra 	$L__BB6_6;

	or.b64  	%rd28, %rd38, %rd5;
	and.b64  	%rd29, %rd28, -4294967296;
	setp.eq.s64 	%p3, %rd29, 0;
	@%p3 bra 	$L__BB6_5;

	div.u64 	%rd39, %rd38, %rd5;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r32, %rd5;
	cvt.u32.u64 	%r33, %rd38;
	div.u32 	%r34, %r33, %r32;
	cvt.u64.u32 	%rd39, %r34;

$L__BB6_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB6_10;

	or.b64  	%rd30, %rd39, %rd6;
	and.b64  	%rd31, %rd30, -4294967296;
	setp.eq.s64 	%p5, %rd31, 0;
	@%p5 bra 	$L__BB6_9;

	div.u64 	%rd39, %rd39, %rd6;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r35, %rd6;
	cvt.u32.u64 	%r36, %rd39;
	div.u32 	%r37, %r36, %r35;
	cvt.u64.u32 	%rd39, %r37;

$L__BB6_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB6_14;

	or.b64  	%rd32, %rd39, %rd7;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.eq.s64 	%p7, %rd33, 0;
	@%p7 bra 	$L__BB6_13;

	div.u64 	%rd39, %rd39, %rd7;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r38, %rd7;
	cvt.u32.u64 	%r39, %rd39;
	div.u32 	%r40, %r39, %r38;
	cvt.u64.u32 	%rd39, %r40;

$L__BB6_14:
	cvt.s64.s32 	%rd34, %rd39;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd35, %rd34, 0, %p8;
	mul.lo.s64 	%rd36, %rd35, %rd8;
	add.s64 	%rd20, %rd4, %rd36;
	ld.global.f64 	%fd1, [%rd20+64];
	ld.global.f64 	%fd2, [%rd20+32];
	mul.f64 	%fd41, %fd2, %fd1;
	ld.global.f64 	%fd3, [%rd20+56];
	ld.global.f64 	%fd4, [%rd20+40];
	mul.f64 	%fd42, %fd4, %fd3;
	sub.f64 	%fd5, %fd41, %fd42;
	ld.global.f64 	%fd6, [%rd20+48];
	mul.f64 	%fd43, %fd4, %fd6;
	ld.global.f64 	%fd7, [%rd20+24];
	mul.f64 	%fd44, %fd7, %fd1;
	sub.f64 	%fd8, %fd43, %fd44;
	mul.f64 	%fd45, %fd7, %fd3;
	mul.f64 	%fd46, %fd2, %fd6;
	sub.f64 	%fd9, %fd45, %fd46;
	ld.global.f64 	%fd10, [%rd20];
	ld.global.f64 	%fd11, [%rd20+8];
	mul.f64 	%fd47, %fd11, %fd8;
	fma.rn.f64 	%fd48, %fd10, %fd5, %fd47;
	ld.global.f64 	%fd12, [%rd20+16];
	fma.rn.f64 	%fd13, %fd12, %fd9, %fd48;
	setp.eq.f64 	%p9, %fd13, 0d0000000000000000;
	mov.f64 	%fd68, 0d0000000000000000;
	mov.f64 	%fd69, %fd68;
	mov.f64 	%fd70, %fd68;
	mov.f64 	%fd71, %fd68;
	mov.f64 	%fd72, %fd68;
	mov.f64 	%fd73, %fd68;
	mov.f64 	%fd74, %fd68;
	mov.f64 	%fd75, %fd68;
	mov.f64 	%fd76, %fd68;
	@%p9 bra 	$L__BB6_16;

	mul.f64 	%fd49, %fd12, %fd3;
	mul.f64 	%fd50, %fd11, %fd1;
	sub.f64 	%fd51, %fd49, %fd50;
	mul.f64 	%fd52, %fd12, %fd6;
	mul.f64 	%fd53, %fd10, %fd1;
	sub.f64 	%fd54, %fd53, %fd52;
	mul.f64 	%fd55, %fd10, %fd3;
	mul.f64 	%fd56, %fd11, %fd6;
	sub.f64 	%fd57, %fd56, %fd55;
	mul.f64 	%fd58, %fd12, %fd2;
	mul.f64 	%fd59, %fd11, %fd4;
	sub.f64 	%fd60, %fd59, %fd58;
	mul.f64 	%fd61, %fd10, %fd4;
	mul.f64 	%fd62, %fd12, %fd7;
	sub.f64 	%fd63, %fd62, %fd61;
	mul.f64 	%fd64, %fd11, %fd7;
	mul.f64 	%fd65, %fd10, %fd2;
	sub.f64 	%fd66, %fd65, %fd64;
	rcp.rn.f64 	%fd67, %fd13;
	mul.f64 	%fd76, %fd67, %fd66;
	mul.f64 	%fd75, %fd67, %fd57;
	mul.f64 	%fd73, %fd67, %fd63;
	mul.f64 	%fd72, %fd67, %fd54;
	mul.f64 	%fd70, %fd67, %fd60;
	mul.f64 	%fd69, %fd67, %fd51;
	mul.f64 	%fd74, %fd67, %fd9;
	mul.f64 	%fd71, %fd67, %fd8;
	mul.f64 	%fd68, %fd67, %fd5;

$L__BB6_16:
	ld.param.u64 	%rd37, [array_inv_cuda_kernel_forward_param_0+24];
	st.global.f64 	[%rd20], %fd68;
	st.global.f64 	[%rd20+8], %fd69;
	st.global.f64 	[%rd20+16], %fd70;
	st.global.f64 	[%rd20+24], %fd71;
	st.global.f64 	[%rd20+32], %fd72;
	st.global.f64 	[%rd20+40], %fd73;
	st.global.f64 	[%rd20+48], %fd74;
	st.global.f64 	[%rd20+56], %fd75;
	st.global.f64 	[%rd20+64], %fd76;
	add.s64 	%rd38, %rd38, %rd9;
	setp.lt.u64 	%p10, %rd38, %rd37;
	@%p10 bra 	$L__BB6_2;

$L__BB6_17:
	ret;

}
	// .globl	array_inv_cuda_kernel_backward
.visible .entry array_inv_cuda_kernel_backward(
	.param .align 8 .b8 array_inv_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 array_inv_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 array_inv_cuda_kernel_backward_param_2[56]
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<240>;
	.reg .b64 	%rd<73>;


	ld.param.v2.u32 	{%r25, %r26}, [array_inv_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r27, %r28}, [array_inv_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r33, %r34}, [array_inv_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r41, %r42}, [array_inv_cuda_kernel_backward_param_2+32];
	ld.param.u64 	%rd31, [array_inv_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd30, [array_inv_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd29, [array_inv_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd28, [array_inv_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [array_inv_cuda_kernel_backward_param_0+16];
	mov.u32 	%r45, %ntid.x;
	cvt.u64.u32 	%rd1, %r45;
	mov.u32 	%r46, %ctaid.x;
	mul.wide.u32 	%rd33, %r45, %r46;
	mov.u32 	%r47, %tid.x;
	cvt.u64.u32 	%rd34, %r47;
	add.s64 	%rd69, %rd33, %rd34;
	setp.ge.u64 	%p1, %rd69, %rd28;
	@%p1 bra 	$L__BB7_25;

	cvta.to.global.u64 	%rd8, %rd29;
	cvt.s64.s32 	%rd9, %r28;
	cvt.s64.s32 	%rd10, %r27;
	cvt.s64.s32 	%rd11, %r26;
	cvt.s64.s32 	%rd12, %r33;
	cvt.s64.s32 	%rd13, %r41;
	mov.u32 	%r48, %nctaid.x;
	cvt.u64.u32 	%rd35, %r48;
	mul.lo.s64 	%rd14, %rd1, %rd35;

$L__BB7_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd70, %rd69;
	@%p2 bra 	$L__BB7_6;

	or.b64  	%rd36, %rd69, %rd9;
	and.b64  	%rd37, %rd36, -4294967296;
	setp.eq.s64 	%p3, %rd37, 0;
	@%p3 bra 	$L__BB7_5;

	div.u64 	%rd70, %rd69, %rd9;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r49, %rd9;
	cvt.u32.u64 	%r50, %rd69;
	div.u32 	%r51, %r50, %r49;
	cvt.u64.u32 	%rd70, %r51;

$L__BB7_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB7_10;

	or.b64  	%rd38, %rd70, %rd10;
	and.b64  	%rd39, %rd38, -4294967296;
	setp.eq.s64 	%p5, %rd39, 0;
	@%p5 bra 	$L__BB7_9;

	div.u64 	%rd70, %rd70, %rd10;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r52, %rd10;
	cvt.u32.u64 	%r53, %rd70;
	div.u32 	%r54, %r53, %r52;
	cvt.u64.u32 	%rd70, %r54;

$L__BB7_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB7_14;

	or.b64  	%rd40, %rd70, %rd11;
	and.b64  	%rd41, %rd40, -4294967296;
	setp.eq.s64 	%p7, %rd41, 0;
	@%p7 bra 	$L__BB7_13;

	div.u64 	%rd70, %rd70, %rd11;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r55, %rd11;
	cvt.u32.u64 	%r56, %rd70;
	div.u32 	%r57, %r56, %r55;
	cvt.u64.u32 	%rd70, %r57;

$L__BB7_14:
	cvt.s64.s32 	%rd42, %rd70;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd25, %rd42, 0, %p8;
	mul.lo.s64 	%rd26, %rd25, %rd12;
	add.s64 	%rd43, %rd8, %rd26;
	ld.global.f64 	%fd1, [%rd43+64];
	ld.global.f64 	%fd2, [%rd43+32];
	mul.f64 	%fd77, %fd2, %fd1;
	ld.global.f64 	%fd3, [%rd43+56];
	ld.global.f64 	%fd4, [%rd43+40];
	mul.f64 	%fd78, %fd4, %fd3;
	sub.f64 	%fd5, %fd77, %fd78;
	ld.global.f64 	%fd6, [%rd43+48];
	mul.f64 	%fd79, %fd4, %fd6;
	ld.global.f64 	%fd7, [%rd43+24];
	mul.f64 	%fd80, %fd7, %fd1;
	sub.f64 	%fd8, %fd79, %fd80;
	mul.f64 	%fd81, %fd7, %fd3;
	mul.f64 	%fd82, %fd2, %fd6;
	sub.f64 	%fd9, %fd81, %fd82;
	ld.global.f64 	%fd10, [%rd43];
	ld.global.f64 	%fd11, [%rd43+8];
	mul.f64 	%fd83, %fd11, %fd8;
	fma.rn.f64 	%fd84, %fd10, %fd5, %fd83;
	ld.global.f64 	%fd12, [%rd43+16];
	fma.rn.f64 	%fd13, %fd12, %fd9, %fd84;
	setp.eq.f64 	%p9, %fd13, 0d0000000000000000;
	mov.f64 	%fd222, 0d0000000000000000;
	mov.f64 	%fd223, %fd222;
	mov.f64 	%fd224, %fd222;
	mov.f64 	%fd225, %fd222;
	mov.f64 	%fd226, %fd222;
	mov.f64 	%fd227, %fd222;
	mov.f64 	%fd228, %fd222;
	mov.f64 	%fd229, %fd222;
	mov.f64 	%fd230, %fd222;
	@%p9 bra 	$L__BB7_16;

	mul.f64 	%fd85, %fd12, %fd3;
	mul.f64 	%fd86, %fd11, %fd1;
	sub.f64 	%fd87, %fd85, %fd86;
	mul.f64 	%fd88, %fd12, %fd6;
	mul.f64 	%fd89, %fd10, %fd1;
	sub.f64 	%fd90, %fd89, %fd88;
	mul.f64 	%fd91, %fd10, %fd3;
	mul.f64 	%fd92, %fd11, %fd6;
	sub.f64 	%fd93, %fd92, %fd91;
	mul.f64 	%fd94, %fd12, %fd2;
	mul.f64 	%fd95, %fd11, %fd4;
	sub.f64 	%fd96, %fd95, %fd94;
	mul.f64 	%fd97, %fd10, %fd4;
	mul.f64 	%fd98, %fd12, %fd7;
	sub.f64 	%fd99, %fd98, %fd97;
	mul.f64 	%fd100, %fd11, %fd7;
	mul.f64 	%fd101, %fd10, %fd2;
	sub.f64 	%fd102, %fd101, %fd100;
	rcp.rn.f64 	%fd103, %fd13;
	mul.f64 	%fd230, %fd103, %fd102;
	mul.f64 	%fd229, %fd103, %fd93;
	mul.f64 	%fd227, %fd103, %fd99;
	mul.f64 	%fd226, %fd103, %fd90;
	mul.f64 	%fd224, %fd103, %fd96;
	mul.f64 	%fd223, %fd103, %fd87;
	mul.f64 	%fd228, %fd103, %fd9;
	mul.f64 	%fd225, %fd103, %fd8;
	mul.f64 	%fd222, %fd103, %fd5;

$L__BB7_16:
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB7_18;

	cvta.to.global.u64 	%rd66, %rd31;
	mul.lo.s64 	%rd44, %rd25, %rd13;
	add.s64 	%rd45, %rd66, %rd44;
	ld.global.f64 	%fd104, [%rd45];
	add.f64 	%fd239, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd45+8];
	add.f64 	%fd238, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd45+16];
	add.f64 	%fd237, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd45+24];
	add.f64 	%fd236, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd45+32];
	add.f64 	%fd235, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd45+40];
	add.f64 	%fd234, %fd109, 0d0000000000000000;
	ld.global.f64 	%fd110, [%rd45+48];
	add.f64 	%fd233, %fd110, 0d0000000000000000;
	ld.global.f64 	%fd111, [%rd45+56];
	add.f64 	%fd232, %fd111, 0d0000000000000000;
	ld.global.f64 	%fd112, [%rd45+64];
	add.f64 	%fd231, %fd112, 0d0000000000000000;
	bra.uni 	$L__BB7_20;

$L__BB7_18:
	setp.eq.s64 	%p11, %rd30, 0;
	mov.f64 	%fd231, 0d0000000000000000;
	mov.f64 	%fd232, %fd231;
	mov.f64 	%fd233, %fd231;
	mov.f64 	%fd234, %fd231;
	mov.f64 	%fd235, %fd231;
	mov.f64 	%fd236, %fd231;
	mov.f64 	%fd237, %fd231;
	mov.f64 	%fd238, %fd231;
	mov.f64 	%fd239, %fd231;
	@%p11 bra 	$L__BB7_20;

	cvta.to.global.u64 	%rd68, %rd30;
	add.s64 	%rd46, %rd68, %rd26;
	ld.global.f64 	%fd122, [%rd46];
	add.f64 	%fd239, %fd122, 0d0000000000000000;
	ld.global.f64 	%fd123, [%rd46+8];
	add.f64 	%fd238, %fd123, 0d0000000000000000;
	ld.global.f64 	%fd124, [%rd46+16];
	add.f64 	%fd237, %fd124, 0d0000000000000000;
	ld.global.f64 	%fd125, [%rd46+24];
	add.f64 	%fd236, %fd125, 0d0000000000000000;
	ld.global.f64 	%fd126, [%rd46+32];
	add.f64 	%fd235, %fd126, 0d0000000000000000;
	ld.global.f64 	%fd127, [%rd46+40];
	add.f64 	%fd234, %fd127, 0d0000000000000000;
	ld.global.f64 	%fd128, [%rd46+48];
	add.f64 	%fd233, %fd128, 0d0000000000000000;
	ld.global.f64 	%fd129, [%rd46+56];
	add.f64 	%fd232, %fd129, 0d0000000000000000;
	ld.global.f64 	%fd130, [%rd46+64];
	add.f64 	%fd231, %fd130, 0d0000000000000000;

$L__BB7_20:
	mov.f64 	%fd131, 0d0000000000000000;
	fma.rn.f64 	%fd132, %fd222, %fd239, %fd131;
	fma.rn.f64 	%fd133, %fd225, %fd236, %fd132;
	fma.rn.f64 	%fd134, %fd228, %fd233, %fd133;
	fma.rn.f64 	%fd135, %fd222, %fd238, %fd131;
	fma.rn.f64 	%fd136, %fd225, %fd235, %fd135;
	fma.rn.f64 	%fd137, %fd228, %fd232, %fd136;
	fma.rn.f64 	%fd138, %fd222, %fd237, %fd131;
	fma.rn.f64 	%fd139, %fd225, %fd234, %fd138;
	fma.rn.f64 	%fd140, %fd228, %fd231, %fd139;
	fma.rn.f64 	%fd141, %fd223, %fd239, %fd131;
	fma.rn.f64 	%fd142, %fd226, %fd236, %fd141;
	fma.rn.f64 	%fd143, %fd229, %fd233, %fd142;
	fma.rn.f64 	%fd144, %fd223, %fd238, %fd131;
	fma.rn.f64 	%fd145, %fd226, %fd235, %fd144;
	fma.rn.f64 	%fd146, %fd229, %fd232, %fd145;
	fma.rn.f64 	%fd147, %fd223, %fd237, %fd131;
	fma.rn.f64 	%fd148, %fd226, %fd234, %fd147;
	fma.rn.f64 	%fd149, %fd229, %fd231, %fd148;
	fma.rn.f64 	%fd150, %fd224, %fd239, %fd131;
	fma.rn.f64 	%fd151, %fd227, %fd236, %fd150;
	fma.rn.f64 	%fd152, %fd230, %fd233, %fd151;
	fma.rn.f64 	%fd153, %fd224, %fd238, %fd131;
	fma.rn.f64 	%fd154, %fd227, %fd235, %fd153;
	fma.rn.f64 	%fd155, %fd230, %fd232, %fd154;
	fma.rn.f64 	%fd156, %fd224, %fd237, %fd131;
	fma.rn.f64 	%fd157, %fd227, %fd234, %fd156;
	fma.rn.f64 	%fd158, %fd230, %fd231, %fd157;
	fma.rn.f64 	%fd159, %fd134, %fd222, %fd131;
	fma.rn.f64 	%fd160, %fd137, %fd223, %fd159;
	fma.rn.f64 	%fd161, %fd140, %fd224, %fd160;
	fma.rn.f64 	%fd162, %fd134, %fd225, %fd131;
	fma.rn.f64 	%fd163, %fd137, %fd226, %fd162;
	fma.rn.f64 	%fd164, %fd140, %fd227, %fd163;
	fma.rn.f64 	%fd165, %fd134, %fd228, %fd131;
	fma.rn.f64 	%fd166, %fd137, %fd229, %fd165;
	fma.rn.f64 	%fd167, %fd140, %fd230, %fd166;
	fma.rn.f64 	%fd168, %fd143, %fd222, %fd131;
	fma.rn.f64 	%fd169, %fd146, %fd223, %fd168;
	fma.rn.f64 	%fd170, %fd149, %fd224, %fd169;
	fma.rn.f64 	%fd171, %fd143, %fd225, %fd131;
	fma.rn.f64 	%fd172, %fd146, %fd226, %fd171;
	fma.rn.f64 	%fd173, %fd149, %fd227, %fd172;
	fma.rn.f64 	%fd174, %fd143, %fd228, %fd131;
	fma.rn.f64 	%fd175, %fd146, %fd229, %fd174;
	fma.rn.f64 	%fd176, %fd149, %fd230, %fd175;
	fma.rn.f64 	%fd177, %fd152, %fd222, %fd131;
	fma.rn.f64 	%fd178, %fd155, %fd223, %fd177;
	fma.rn.f64 	%fd179, %fd158, %fd224, %fd178;
	fma.rn.f64 	%fd180, %fd152, %fd225, %fd131;
	fma.rn.f64 	%fd181, %fd155, %fd226, %fd180;
	fma.rn.f64 	%fd182, %fd158, %fd227, %fd181;
	fma.rn.f64 	%fd183, %fd152, %fd228, %fd131;
	fma.rn.f64 	%fd184, %fd155, %fd229, %fd183;
	fma.rn.f64 	%fd185, %fd158, %fd230, %fd184;
	sub.f64 	%fd59, %fd131, %fd161;
	sub.f64 	%fd60, %fd131, %fd164;
	sub.f64 	%fd61, %fd131, %fd167;
	sub.f64 	%fd62, %fd131, %fd170;
	sub.f64 	%fd63, %fd131, %fd173;
	sub.f64 	%fd64, %fd131, %fd176;
	sub.f64 	%fd65, %fd131, %fd179;
	sub.f64 	%fd66, %fd131, %fd182;
	sub.f64 	%fd67, %fd131, %fd185;
	@%p10 bra 	$L__BB7_22;

	mul.lo.s64 	%rd56, %rd25, %rd13;
	add.s64 	%rd47, %rd31, %rd56;
	// begin inline asm
	{ atom.add.f64 %fd186,[%rd47],%fd59; }

	// end inline asm
	add.s64 	%rd48, %rd47, 8;
	// begin inline asm
	{ atom.add.f64 %fd188,[%rd48],%fd60; }

	// end inline asm
	add.s64 	%rd49, %rd47, 16;
	// begin inline asm
	{ atom.add.f64 %fd190,[%rd49],%fd61; }

	// end inline asm
	add.s64 	%rd50, %rd47, 24;
	// begin inline asm
	{ atom.add.f64 %fd192,[%rd50],%fd62; }

	// end inline asm
	add.s64 	%rd51, %rd47, 32;
	// begin inline asm
	{ atom.add.f64 %fd194,[%rd51],%fd63; }

	// end inline asm
	add.s64 	%rd52, %rd47, 40;
	// begin inline asm
	{ atom.add.f64 %fd196,[%rd52],%fd64; }

	// end inline asm
	add.s64 	%rd53, %rd47, 48;
	// begin inline asm
	{ atom.add.f64 %fd198,[%rd53],%fd65; }

	// end inline asm
	add.s64 	%rd54, %rd47, 56;
	// begin inline asm
	{ atom.add.f64 %fd200,[%rd54],%fd66; }

	// end inline asm
	add.s64 	%rd55, %rd47, 64;
	// begin inline asm
	{ atom.add.f64 %fd202,[%rd55],%fd67; }

	// end inline asm
	bra.uni 	$L__BB7_24;

$L__BB7_22:
	setp.eq.s64 	%p13, %rd30, 0;
	@%p13 bra 	$L__BB7_24;

	add.s64 	%rd57, %rd30, %rd26;
	// begin inline asm
	{ atom.add.f64 %fd204,[%rd57],%fd59; }

	// end inline asm
	add.s64 	%rd58, %rd57, 8;
	// begin inline asm
	{ atom.add.f64 %fd206,[%rd58],%fd60; }

	// end inline asm
	add.s64 	%rd59, %rd57, 16;
	// begin inline asm
	{ atom.add.f64 %fd208,[%rd59],%fd61; }

	// end inline asm
	add.s64 	%rd60, %rd57, 24;
	// begin inline asm
	{ atom.add.f64 %fd210,[%rd60],%fd62; }

	// end inline asm
	add.s64 	%rd61, %rd57, 32;
	// begin inline asm
	{ atom.add.f64 %fd212,[%rd61],%fd63; }

	// end inline asm
	add.s64 	%rd62, %rd57, 40;
	// begin inline asm
	{ atom.add.f64 %fd214,[%rd62],%fd64; }

	// end inline asm
	add.s64 	%rd63, %rd57, 48;
	// begin inline asm
	{ atom.add.f64 %fd216,[%rd63],%fd65; }

	// end inline asm
	add.s64 	%rd64, %rd57, 56;
	// begin inline asm
	{ atom.add.f64 %fd218,[%rd64],%fd66; }

	// end inline asm
	add.s64 	%rd65, %rd57, 64;
	// begin inline asm
	{ atom.add.f64 %fd220,[%rd65],%fd67; }

	// end inline asm

$L__BB7_24:
	ld.param.u64 	%rd67, [array_inv_cuda_kernel_backward_param_0+24];
	add.s64 	%rd69, %rd69, %rd14;
	setp.lt.u64 	%p14, %rd69, %rd67;
	@%p14 bra 	$L__BB7_2;

$L__BB7_25:
	ret;

}

 