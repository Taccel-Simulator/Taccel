//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34431801
// Cuda compilation tools, release 12.6, V12.6.20
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_86
.address_size 64

	// .globl	compute_affine_kinematic_grad_cuda_kernel_forward
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.const .align 4 .b8 pnanovdb_grid_type_value_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 1, 0, 0, 0, 32, 0, 0, 0, 4, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_table_strides_bits[108] = {64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64};
.const .align 4 .b8 pnanovdb_grid_type_minmax_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 192, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 128, 0, 0, 0, 0, 1, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 48, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_minmax_aligns_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 16, 0, 0, 0, 8};
.const .align 4 .b8 pnanovdb_grid_type_stat_strides_bits[108] = {0, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 32, 0, 0, 0, 32};
.const .align 4 .b8 pnanovdb_grid_type_leaf_type[108] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 5};
.const .align 4 .b8 pnanovdb_grid_type_constants[3024] = {28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 0, 0, 32, 32, 4, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 4, 0, 0, 32, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 128, 16, 0, 0, 28, 0, 0, 0, 40, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 68, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 44, 32, 0, 0, 56, 32, 0, 0, 60, 32, 0, 0, 64, 32, 0, 0, 64, 32, 8, 0, 32, 4, 0, 0, 44, 4, 0, 0, 56, 4, 0, 0, 60, 4, 0, 0, 64, 4, 0, 0, 64, 4, 1, 0, 80, 0, 0, 0, 92, 0, 0, 0, 104, 0, 0, 0, 108, 0, 0, 0, 128, 0, 0, 0, 128, 24, 0, 0, 32, 0, 0, 0, 56, 0, 0, 0, 80, 0, 0, 0, 104, 0, 0, 0, 112, 0, 0, 0, 128, 0, 0, 0, 192, 0, 0, 0, 24, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 56, 32, 0, 0, 80, 32, 0, 0, 88, 32, 0, 0, 96, 32, 0, 0, 96, 32, 12, 0, 32, 4, 0, 0, 56, 4, 0, 0, 80, 4, 0, 0, 88, 4, 0, 0, 96, 4, 0, 0, 96, 132, 1, 0, 80, 0, 0, 0, 104, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 160, 48, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 64, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 34, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 34, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 82, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 31, 0, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 34, 32, 0, 0, 35, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 34, 4, 0, 0, 35, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 160, 0, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 32, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 8, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 1, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 2, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 88, 0, 0, 0, 90, 0, 0, 0, 92, 0, 0, 0, 94, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 28, 0, 0, 0, 44, 0, 0, 0, 60, 0, 0, 0, 76, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 16, 0, 0, 0, 20, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 68, 32, 0, 0, 96, 32, 0, 0, 96, 32, 8, 0, 32, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 68, 4, 0, 0, 96, 4, 0, 0, 96, 4, 1, 0, 80, 0, 0, 0, 96, 0, 0, 0, 112, 0, 0, 0, 116, 0, 0, 0, 128, 0, 0, 0, 128, 32, 0, 0, 32, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 136, 0, 0, 0, 160, 0, 0, 0, 0, 1, 0, 0, 32, 0, 0, 0, 24, 0, 0, 0, 64, 0, 0, 0, 32, 32, 0, 0, 64, 32, 0, 0, 96, 32, 0, 0, 104, 32, 0, 0, 128, 32, 0, 0, 128, 32, 16, 0, 32, 4, 0, 0, 64, 4, 0, 0, 96, 4, 0, 0, 104, 4, 0, 0, 128, 4, 0, 0, 128, 4, 2, 0, 80, 0, 0, 0, 112, 0, 0, 0, 144, 0, 0, 0, 152, 0, 0, 0, 160, 0, 0, 0, 160, 64, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 96, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 80, 0, 0, 0, 160, 0, 0, 0, 32, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 56, 0, 0, 0, 64, 0, 0, 0, 96, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 24, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 40, 32, 0, 0, 48, 32, 0, 0, 56, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 40, 4, 0, 0, 48, 4, 0, 0, 56, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 0, 0, 0, 96, 4, 0, 0, 28, 0, 0, 0, 31, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 44, 0, 0, 0, 64, 0, 0, 0, 24, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 35, 32, 0, 0, 40, 32, 0, 0, 44, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 35, 4, 0, 0, 40, 4, 0, 0, 44, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 83, 0, 0, 0, 88, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 96, 6, 0, 0, 28, 0, 0, 0, 34, 0, 0, 0, 40, 0, 0, 0, 48, 0, 0, 0, 52, 0, 0, 0, 64, 0, 0, 0, 48, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 38, 32, 0, 0, 44, 32, 0, 0, 48, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 38, 4, 0, 0, 44, 4, 0, 0, 48, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 86, 0, 0, 0, 92, 0, 0, 0, 96, 0, 0, 0, 128, 0, 0, 0, 128, 12, 0, 0, 28, 0, 0, 0, 29, 0, 0, 0, 30, 0, 0, 0, 32, 0, 0, 0, 36, 0, 0, 0, 64, 0, 0, 0, 8, 0, 0, 0, 8, 0, 0, 0, 20, 0, 0, 0, 32, 0, 0, 0, 32, 32, 0, 0, 33, 32, 0, 0, 36, 32, 0, 0, 40, 32, 0, 0, 64, 32, 0, 0, 64, 32, 4, 0, 32, 4, 0, 0, 33, 4, 0, 0, 36, 4, 0, 0, 40, 4, 0, 0, 64, 4, 0, 0, 64, 132, 0, 0, 80, 0, 0, 0, 81, 0, 0, 0, 84, 0, 0, 0, 88, 0, 0, 0, 96, 0, 0, 0, 96, 2};
.const .align 4 .b8 pnanovdb_dither_lut[2048] = {70, 182, 19, 62, 172, 173, 36, 63, 175, 149, 84, 63, 42, 171, 169, 62, 33, 148, 215, 61, 175, 178, 26, 63, 21, 170, 43, 62, 176, 170, 42, 63, 193, 141, 100, 63, 44, 155, 201, 62, 36, 172, 167, 61, 170, 181, 20, 63, 180, 146, 90, 63, 51, 165, 181, 62, 181, 138, 106, 63, 54, 149, 213, 62, 171, 177, 28, 63, 0, 140, 231, 61, 175, 153, 76, 63, 41, 179, 153, 62, 157, 190, 2, 63, 41, 160, 63, 60, 182, 134, 114, 63, 55, 141, 229, 62, 43, 163, 185, 62, 176, 145, 92, 63, 62, 152, 79, 61, 170, 185, 12, 63, 48, 189, 133, 62, 178, 158, 66, 63, 23, 154, 75, 62, 177, 166, 50, 63, 44, 184, 15, 62, 37, 174, 35, 63, 36, 182, 19, 63, 39, 176, 159, 61, 31, 189, 5, 63, 41, 160, 191, 60, 59, 138, 235, 62, 56, 133, 117, 63, 59, 142, 99, 63, 65, 156, 199, 62, 29, 172, 167, 62, 58, 150, 83, 63, 19, 186, 139, 62, 52, 157, 69, 63, 51, 165, 53, 63, 33, 148, 87, 62, 69, 132, 247, 62, 61, 130, 123, 63, 28, 180, 151, 62, 57, 154, 75, 63, 51, 136, 239, 61, 33, 177, 29, 63, 57, 144, 95, 61, 31, 185, 13, 63, 39, 162, 59, 63, 51, 136, 111, 62, 35, 186, 11, 63, 41, 160, 63, 61, 54, 145, 93, 63, 56, 162, 187, 62, 53, 153, 77, 63, 53, 178, 155, 62, 169, 189, 4, 63, 52, 176, 159, 60, 47, 139, 233, 62, 194, 133, 116, 63, 177, 162, 58, 63, 26, 138, 107, 62, 157, 186, 10, 63, 47, 168, 47, 61, 40, 187, 137, 62, 174, 157, 68, 63, 173, 165, 52, 63, 74, 150, 83, 62, 56, 133, 245, 62, 199, 130, 122, 63, 49, 181, 149, 62, 179, 154, 74, 63, 77, 134, 115, 62, 173, 161, 60, 63, 45, 147, 217, 62, 194, 137, 108, 63, 19, 186, 11, 62, 176, 174, 34, 63, 50, 173, 165, 62, 179, 150, 82, 63, 195, 129, 124, 63, 48, 131, 249, 62, 172, 169, 44, 63, 72, 166, 51, 62, 180, 142, 98, 63, 52, 157, 197, 62, 158, 182, 18, 63, 41, 180, 151, 61, 35, 190, 3, 63, 19, 128, 127, 60, 49, 152, 79, 62, 38, 166, 51, 63, 28, 180, 23, 62, 50, 173, 37, 63, 54, 149, 85, 63, 55, 170, 171, 62, 27, 188, 135, 62, 56, 158, 67, 63, 60, 134, 115, 63, 67, 140, 231, 62, 55, 141, 101, 63, 57, 154, 203, 62, 47, 168, 175, 61, 32, 181, 21, 63, 58, 146, 91, 63, 30, 164, 183, 62, 59, 138, 107, 63, 66, 148, 215, 62, 52, 161, 61, 63, 35, 132, 119, 62, 51, 169, 45, 63, 30, 164, 55, 62, 84, 144, 223, 61, 37, 178, 27, 63, 47, 168, 47, 62, 38, 170, 43, 63, 61, 130, 251, 62, 56, 129, 125, 63, 58, 146, 219, 62, 55, 137, 109, 63, 34, 188, 135, 61, 162, 183, 16, 63, 163, 175, 32, 63, 35, 190, 3, 62, 56, 162, 59, 62, 168, 168, 46, 63, 169, 160, 62, 63, 61, 130, 123, 62, 183, 151, 80, 63, 25, 175, 161, 62, 60, 159, 193, 62, 184, 143, 96, 63, 190, 136, 110, 63, 71, 145, 221, 62, 73, 129, 253, 62, 191, 128, 126, 63, 31, 184, 15, 61, 161, 187, 8, 63, 186, 131, 120, 63, 64, 135, 241, 62, 58, 146, 91, 62, 169, 164, 54, 63, 165, 188, 6, 63, 30, 144, 223, 60, 183, 155, 72, 63, 23, 183, 145, 62, 42, 142, 99, 62, 181, 163, 56, 63, 190, 132, 118, 63, 72, 137, 237, 62, 31, 185, 141, 62, 170, 156, 70, 63, 69, 132, 119, 63, 51, 136, 239, 62, 49, 152, 207, 62, 51, 140, 103, 63, 31, 184, 143, 61, 40, 183, 17, 63, 63, 143, 97, 63, 40, 158, 195, 62, 84, 144, 95, 62, 47, 164, 55, 63, 46, 172, 39, 63, 12, 176, 31, 62, 45, 151, 81, 63, 37, 174, 163, 62, 60, 188, 7, 62, 41, 175, 33, 63, 73, 128, 127, 61, 44, 184, 15, 63, 48, 160, 63, 63, 86, 128, 127, 62, 63, 139, 105, 63, 41, 150, 211, 62, 65, 131, 121, 63, 77, 134, 243, 62, 49, 152, 79, 63, 45, 176, 159, 62, 52, 128, 255, 62, 69, 128, 127, 63, 63, 172, 39, 62, 42, 171, 41, 63, 67, 140, 103, 62, 43, 163, 57, 63, 164, 167, 48, 63, 40, 158, 67, 62, 83, 128, 127, 59, 161, 191, 0, 63, 166, 184, 14, 63, 51, 136, 111, 61, 102, 132, 247, 61, 167, 176, 30, 63, 63, 143, 225, 62, 186, 135, 112, 63, 182, 159, 64, 63, 22, 191, 129, 62, 33, 177, 157, 62, 187, 152, 78, 63, 188, 144, 94, 63, 35, 161, 189, 62, 164, 171, 40, 63, 37, 174, 35, 62, 59, 167, 177, 62, 184, 147, 88, 63, 166, 180, 22, 63, 44, 164, 183, 61, 53, 178, 27, 62, 168, 172, 38, 63, 62, 151, 209, 62, 185, 139, 104, 63, 162, 179, 24, 63, 52, 156, 199, 61, 34, 169, 173, 62, 188, 148, 86, 63, 189, 140, 102, 63, 36, 153, 205, 62, 47, 168, 175, 62, 49, 148, 87, 63, 48, 156, 71, 63, 44, 184, 143, 62, 42, 167, 49, 63, 65, 156, 71, 62, 35, 190, 131, 62, 44, 159, 65, 63, 45, 180, 23, 63, 54, 160, 191, 61, 73, 128, 255, 60, 27, 188, 7, 63, 42, 142, 227, 62, 64, 135, 113, 63, 39, 191, 1, 63, 62, 128, 255, 59, 47, 168, 47, 63, 81, 160, 63, 62, 19, 128, 255, 61, 45, 176, 31, 63, 36, 182, 147, 62, 44, 155, 73, 63, 38, 166, 179, 62, 62, 147, 89, 63, 50, 144, 223, 62, 68, 136, 111, 63, 50, 144, 95, 63, 48, 160, 191, 62, 40, 187, 9, 63, 52, 176, 31, 61, 41, 179, 25, 63, 116, 152, 207, 61, 227, 54, 18, 63, 43, 182, 147, 61, 247, 30, 66, 63, 152, 189, 132, 62, 234, 35, 56, 63, 63, 143, 97, 62, 230, 59, 8, 63, 34, 188, 7, 61, 155, 173, 164, 62, 248, 22, 82, 63, 41, 176, 31, 60, 226, 62, 2, 63, 202, 135, 240, 62, 255, 3, 120, 63, 162, 183, 144, 62, 235, 27, 72, 63, 226, 58, 10, 63, 49, 172, 39, 61, 247, 34, 58, 63, 47, 139, 105, 62, 230, 63, 0, 63, 83, 128, 255, 58, 231, 55, 16, 63, 35, 190, 131, 61, 154, 181, 148, 62, 248, 26, 74, 63, 194, 133, 244, 62, 251, 2, 122, 63, 161, 191, 128, 62, 235, 31, 64, 63, 163, 175, 160, 62, 236, 23, 80, 63, 116, 7, 113, 63, 180, 142, 226, 62, 115, 15, 97, 63, 178, 158, 194, 62, 186, 160, 190, 62, 119, 16, 95, 63, 184, 176, 158, 62, 118, 24, 79, 63, 19, 157, 69, 62, 112, 39, 49, 63, 14, 189, 5, 62, 111, 47, 33, 63, 98, 48, 31, 63, 61, 130, 251, 61, 97, 56, 15, 63, 75, 132, 119, 61, 111, 43, 41, 63, 16, 173, 37, 62, 112, 35, 57, 63, 88, 141, 101, 62, 187, 152, 206, 62, 120, 12, 103, 63, 119, 20, 87, 63, 185, 168, 174, 62, 179, 150, 210, 62, 116, 11, 105, 63, 182, 134, 242, 62, 134, 3, 121, 63, 98, 44, 39, 63, 33, 177, 29, 62, 42, 162, 187, 61, 97, 52, 23, 63, 44, 155, 73, 62, 229, 38, 50, 63, 191, 157, 196, 62, 250, 14, 98, 63, 53, 158, 195, 61, 232, 51, 24, 63, 58, 175, 33, 62, 233, 43, 40, 63, 251, 6, 114, 63, 193, 141, 228, 62, 228, 46, 34, 63, 40, 187, 9, 62, 253, 19, 88, 63, 164, 167, 176, 62, 254, 11, 104, 63, 166, 151, 208, 62, 42, 171, 41, 62, 229, 42, 42, 63, 74, 150, 211, 61, 228, 50, 26, 63, 56, 191, 1, 62, 232, 47, 32, 63, 60, 159, 65, 62, 233, 39, 48, 63, 250, 10, 106, 63, 192, 149, 212, 62, 249, 18, 90, 63, 190, 165, 180, 62, 254, 15, 96, 63, 165, 159, 192, 62, 255, 7, 112, 63, 168, 143, 224, 62, 176, 174, 162, 62, 114, 23, 81, 63, 173, 190, 130, 62, 113, 31, 65, 63, 122, 0, 127, 63, 191, 128, 254, 62, 120, 8, 111, 63, 188, 144, 222, 62, 93, 55, 17, 63, 32, 186, 139, 61, 91, 63, 1, 63, 41, 160, 191, 59, 40, 129, 125, 62, 117, 32, 63, 63, 35, 161, 61, 62, 116, 40, 47, 63, 28, 180, 23, 61, 92, 59, 9, 63, 50, 154, 203, 61, 110, 51, 25, 63, 118, 28, 71, 63, 149, 184, 142, 62, 190, 136, 238, 62, 121, 4, 119, 63, 113, 27, 73, 63, 174, 182, 146, 62, 115, 19, 89, 63, 177, 166, 178, 62, 78, 136, 239, 60, 96, 60, 7, 63, 116, 36, 55, 63, 37, 145, 93, 62, 175, 153, 204, 62, 2, 13, 102, 63, 3, 5, 118, 63, 177, 137, 236, 62, 38, 156, 71, 61, 222, 57, 12, 63, 42, 142, 227, 61, 223, 49, 28, 63, 237, 44, 38, 63, 7, 179, 25, 62, 79, 147, 89, 62, 238, 36, 54, 63, 244, 25, 76, 63, 179, 179, 152, 62, 245, 17, 92, 63, 181, 163, 184, 62, 10, 134, 243, 61, 236, 48, 30, 63, 54, 140, 103, 61, 235, 56, 14, 63, 180, 171, 168, 62, 244, 21, 84, 63, 222, 61, 4, 63, 58, 184, 143, 60, 241, 16, 94, 63, 173, 161, 188, 62, 240, 24, 78, 63, 171, 177, 156, 62, 223, 53, 20, 63, 37, 174, 163, 61, 178, 187, 136, 62, 243, 29, 68, 63, 157, 186, 138, 62, 105, 29, 69, 63, 104, 37, 53, 63, 54, 149, 85, 62, 107, 42, 43, 63, 67, 169, 45, 62, 106, 50, 27, 63, 247, 145, 219, 61, 100, 61, 5, 63, 47, 168, 175, 60, 198, 138, 234, 62, 125, 5, 117, 63, 171, 148, 214, 62, 129, 10, 107, 63, 169, 164, 182, 62, 127, 18, 91, 63, 126, 1, 125, 63, 199, 130, 250, 62, 197, 146, 218, 62, 125, 9, 109, 63, 172, 140, 230, 62, 129, 6, 115, 63, 104, 62, 3, 63, 30, 144, 95, 60, 56, 133, 117, 62, 104, 33, 61, 63, 103, 41, 45, 63, 51, 165, 53, 62, 108, 38, 51, 63, 70, 153, 77, 62, 165, 188, 134, 62, 109, 30, 67, 63, 239, 28, 70, 63, 170, 185, 140, 62, 172, 169, 172, 62, 240, 20, 86, 63, 241, 41, 44, 63, 26, 167, 49, 62, 243, 33, 60, 63, 30, 135, 113, 62, 35, 152, 207, 60, 218, 60, 6, 63, 236, 52, 22, 63, 45, 166, 179, 61, 184, 147, 216, 62, 246, 9, 108, 63, 186, 131, 248, 62, 247, 1, 124, 63, 239, 32, 62, 63, 81, 131, 121, 62, 237, 40, 46, 63, 77, 163, 57, 62, 247, 5, 116, 63, 185, 139, 232, 62, 23, 183, 17, 62, 241, 45, 36, 63, 178, 129, 252, 62, 4, 1, 126, 63, 176, 145, 220, 62, 3, 9, 110, 63, 28, 151, 81, 62, 242, 37, 52, 63, 246, 13, 100, 63, 183, 155, 200, 62, 124, 13, 101, 63, 195, 154, 202, 62, 48, 170, 171, 61, 101, 53, 21, 63, 44, 164, 55, 61, 105, 58, 11, 63, 72, 137, 109, 62, 108, 34, 59, 63, 49, 181, 21, 62, 102, 45, 37, 63, 123, 21, 85, 63, 159, 170, 170, 62, 109, 26, 75, 63, 166, 180, 150, 62, 130, 2, 123, 63, 173, 132, 246, 62, 161, 162, 186, 62, 123, 17, 93, 63, 122, 25, 77, 63, 158, 178, 154, 62, 110, 22, 83, 63, 168, 172, 166, 62, 65, 185, 13, 62, 106, 46, 35, 63, 102, 49, 29, 63, 93, 138, 235, 61, 60, 148, 87, 61, 101, 57, 13, 63, 40, 178, 155, 61, 105, 54, 19, 63, 128, 14, 99, 63, 170, 156, 198, 62};
.global .align 8 .f64 _ZN2wp11_svd_configIdE17QR_GIVENS_EPSILONE = 0d3D719799812DEA11;
.global .align 4 .u32 _ZN2wp11_svd_configIdE17JACOBI_ITERATIONSE = 8;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIiE9GRID_TYPEE = 4;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIxE9GRID_TYPEE = 5;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIjE9GRID_TYPEE = 10;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIfE9GRID_TYPEE = 1;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsIdE9GRID_TYPEE = 2;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EfEEE9GRID_TYPEE = 6;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj3EdEEE9GRID_TYPEE = 7;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EfEEE9GRID_TYPEE = 17;
.global .align 4 .u32 _ZN2wp6volume12pnano_traitsINS_5vec_tILj4EdEEE9GRID_TYPEE = 18;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp6volume7CLOSESTE;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp6volume6LINEARE = 1;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp15LAUNCH_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp14ARRAY_MAX_DIMSE = 4;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp18ARRAY_TYPE_REGULARE;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp18ARRAY_TYPE_INDEXEDE = 1;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp17ARRAY_TYPE_FABRICE = 2;
.global .align 4 .u32 _ZN80_INTERNAL_00000000_49_wp_warp_ipc_energy_kinematic_constraint_energy_cu_953a223e2wp25ARRAY_TYPE_FABRIC_INDEXEDE = 3;
.global .align 1 .b8 $str[54] = {91, 67, 79, 79, 77, 97, 116, 114, 105, 120, 32, 79, 70, 66, 32, 69, 114, 114, 111, 114, 93, 9, 98, 108, 111, 99, 107, 95, 105, 110, 100, 101, 120, 58, 32, 37, 100, 44, 32, 115, 105, 122, 101, 58, 32, 37, 100, 33, 33, 33, 33, 33, 10};

.visible .entry compute_affine_kinematic_grad_cuda_kernel_forward(
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_3[56],
	.param .f64 compute_affine_kinematic_grad_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_forward_param_8[56]
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<58>;
	.reg .b32 	%r<145>;
	.reg .f64 	%fd<64>;
	.reg .b64 	%rd<89>;


	ld.param.v2.u32 	{%r70, %r71}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r72, %r73}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r78, %r79}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r86, %r87}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r94, %r95}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [compute_affine_kinematic_grad_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r102, %r103}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r110, %r111}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r118, %r119}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r126, %r127}, [compute_affine_kinematic_grad_cuda_kernel_forward_param_8+32];
	ld.param.u64 	%rd47, [compute_affine_kinematic_grad_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd45, [compute_affine_kinematic_grad_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd43, [compute_affine_kinematic_grad_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd41, [compute_affine_kinematic_grad_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd39, [compute_affine_kinematic_grad_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd37, [compute_affine_kinematic_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd35, [compute_affine_kinematic_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd34, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r130, %ntid.x;
	cvt.u64.u32 	%rd1, %r130;
	mov.u32 	%r131, %ctaid.x;
	mul.wide.u32 	%rd49, %r130, %r131;
	mov.u32 	%r132, %tid.x;
	cvt.u64.u32 	%rd50, %r132;
	add.s64 	%rd85, %rd49, %rd50;
	setp.ge.u64 	%p1, %rd85, %rd34;
	@%p1 bra 	$L__BB0_18;

	cvta.to.global.u64 	%rd4, %rd47;
	cvta.to.global.u64 	%rd5, %rd45;
	cvta.to.global.u64 	%rd6, %rd43;
	cvta.to.global.u64 	%rd8, %rd39;
	cvta.to.global.u64 	%rd9, %rd35;
	cvt.s64.s32 	%rd10, %r73;
	cvt.s64.s32 	%rd11, %r72;
	cvt.s64.s32 	%rd12, %r71;
	cvt.s64.s32 	%rd13, %r118;
	cvt.s64.s32 	%rd14, %r126;
	mov.u32 	%r133, %nctaid.x;
	cvt.u64.u32 	%rd51, %r133;
	mul.lo.s64 	%rd15, %rd1, %rd51;
	cvt.s64.s32 	%rd16, %r110;
	cvt.s64.s32 	%rd17, %r86;
	cvt.s64.s32 	%rd18, %r78;
	cvt.s64.s32 	%rd19, %r94;
	cvt.s64.s32 	%rd20, %r102;
	cvta.to.global.u64 	%rd21, %rd37;

$L__BB0_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd86, %rd85;
	@%p2 bra 	$L__BB0_6;

	or.b64  	%rd52, %rd85, %rd10;
	and.b64  	%rd53, %rd52, -4294967296;
	setp.eq.s64 	%p3, %rd53, 0;
	@%p3 bra 	$L__BB0_5;

	div.u64 	%rd86, %rd85, %rd10;
	bra.uni 	$L__BB0_6;

$L__BB0_5:
	cvt.u32.u64 	%r134, %rd10;
	cvt.u32.u64 	%r135, %rd85;
	div.u32 	%r136, %r135, %r134;
	cvt.u64.u32 	%rd86, %r136;

$L__BB0_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB0_10;

	or.b64  	%rd54, %rd86, %rd11;
	and.b64  	%rd55, %rd54, -4294967296;
	setp.eq.s64 	%p5, %rd55, 0;
	@%p5 bra 	$L__BB0_9;

	div.u64 	%rd86, %rd86, %rd11;
	bra.uni 	$L__BB0_10;

$L__BB0_9:
	cvt.u32.u64 	%r137, %rd11;
	cvt.u32.u64 	%r138, %rd86;
	div.u32 	%r139, %r138, %r137;
	cvt.u64.u32 	%rd86, %r139;

$L__BB0_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB0_14;

	or.b64  	%rd56, %rd86, %rd12;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p7, %rd57, 0;
	@%p7 bra 	$L__BB0_13;

	div.u64 	%rd86, %rd86, %rd12;
	bra.uni 	$L__BB0_14;

$L__BB0_13:
	cvt.u32.u64 	%r140, %rd12;
	cvt.u32.u64 	%r141, %rd86;
	div.u32 	%r142, %r141, %r140;
	cvt.u64.u32 	%rd86, %r142;

$L__BB0_14:
	cvt.s64.s32 	%rd58, %rd86;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd32, %rd58, 0, %p8;
	mul.lo.s64 	%rd59, %rd32, %rd13;
	add.s64 	%rd60, %rd5, %rd59;
	ld.global.s32 	%rd61, [%rd60];
	mul.lo.s64 	%rd62, %rd61, %rd14;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.u32 	%r143, [%rd63];
	add.s32 	%r144, %r143, -1;
	setp.lt.u32 	%p9, %r144, 2;
	@%p9 bra 	$L__BB0_17;

	mul.lo.s64 	%rd64, %rd32, %rd17;
	add.s64 	%rd65, %rd21, %rd64;
	ld.global.u8 	%rs57, [%rd65];
	setp.eq.s16 	%p10, %rs57, 0;
	@%p10 bra 	$L__BB0_17;

	mul.lo.s64 	%rd78, %rd32, %rd18;
	add.s64 	%rd79, %rd9, %rd78;
	mul.lo.s64 	%rd80, %rd32, %rd19;
	add.s64 	%rd81, %rd8, %rd80;
	ld.global.f64 	%fd26, [%rd81];
	ld.global.f64 	%fd27, [%rd79];
	sub.f64 	%fd28, %fd27, %fd26;
	ld.global.f64 	%fd29, [%rd81+8];
	ld.global.f64 	%fd30, [%rd79+8];
	sub.f64 	%fd31, %fd30, %fd29;
	ld.global.f64 	%fd32, [%rd81+16];
	ld.global.f64 	%fd33, [%rd79+16];
	sub.f64 	%fd34, %fd33, %fd32;
	ld.global.f64 	%fd35, [%rd81+24];
	ld.global.f64 	%fd36, [%rd79+24];
	sub.f64 	%fd37, %fd36, %fd35;
	ld.global.f64 	%fd38, [%rd81+32];
	ld.global.f64 	%fd39, [%rd79+32];
	sub.f64 	%fd40, %fd39, %fd38;
	ld.global.f64 	%fd41, [%rd81+40];
	ld.global.f64 	%fd42, [%rd79+40];
	sub.f64 	%fd43, %fd42, %fd41;
	ld.global.f64 	%fd44, [%rd81+48];
	ld.global.f64 	%fd45, [%rd79+48];
	sub.f64 	%fd46, %fd45, %fd44;
	ld.global.f64 	%fd47, [%rd81+56];
	ld.global.f64 	%fd48, [%rd79+56];
	sub.f64 	%fd49, %fd48, %fd47;
	ld.global.f64 	%fd50, [%rd81+64];
	ld.global.f64 	%fd51, [%rd79+64];
	sub.f64 	%fd52, %fd51, %fd50;
	ld.global.f64 	%fd53, [%rd81+72];
	ld.global.f64 	%fd54, [%rd79+72];
	sub.f64 	%fd55, %fd54, %fd53;
	ld.global.f64 	%fd56, [%rd81+80];
	ld.global.f64 	%fd57, [%rd79+80];
	sub.f64 	%fd58, %fd57, %fd56;
	ld.global.f64 	%fd59, [%rd81+88];
	ld.global.f64 	%fd60, [%rd79+88];
	sub.f64 	%fd61, %fd60, %fd59;
	mul.lo.s64 	%rd82, %rd32, %rd16;
	add.s64 	%rd83, %rd6, %rd82;
	ld.global.f64 	%fd62, [%rd83];
	mul.f64 	%fd63, %fd62, %fd1;
	mul.f64 	%fd3, %fd63, %fd28;
	mul.f64 	%fd5, %fd63, %fd31;
	mul.f64 	%fd7, %fd63, %fd34;
	mul.f64 	%fd9, %fd63, %fd37;
	mul.f64 	%fd11, %fd63, %fd40;
	mul.f64 	%fd13, %fd63, %fd43;
	mul.f64 	%fd15, %fd63, %fd46;
	mul.f64 	%fd17, %fd63, %fd49;
	mul.f64 	%fd19, %fd63, %fd52;
	mul.f64 	%fd21, %fd63, %fd55;
	mul.f64 	%fd23, %fd63, %fd58;
	mul.f64 	%fd25, %fd63, %fd61;
	mul.lo.s64 	%rd84, %rd32, %rd20;
	add.s64 	%rd66, %rd41, %rd84;
	// begin inline asm
	{ atom.add.f64 %fd2,[%rd66],%fd3; }

	// end inline asm
	add.s64 	%rd67, %rd66, 8;
	// begin inline asm
	{ atom.add.f64 %fd4,[%rd67],%fd5; }

	// end inline asm
	add.s64 	%rd68, %rd66, 16;
	// begin inline asm
	{ atom.add.f64 %fd6,[%rd68],%fd7; }

	// end inline asm
	add.s64 	%rd69, %rd66, 24;
	// begin inline asm
	{ atom.add.f64 %fd8,[%rd69],%fd9; }

	// end inline asm
	add.s64 	%rd70, %rd66, 32;
	// begin inline asm
	{ atom.add.f64 %fd10,[%rd70],%fd11; }

	// end inline asm
	add.s64 	%rd71, %rd66, 40;
	// begin inline asm
	{ atom.add.f64 %fd12,[%rd71],%fd13; }

	// end inline asm
	add.s64 	%rd72, %rd66, 48;
	// begin inline asm
	{ atom.add.f64 %fd14,[%rd72],%fd15; }

	// end inline asm
	add.s64 	%rd73, %rd66, 56;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd73],%fd17; }

	// end inline asm
	add.s64 	%rd74, %rd66, 64;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd74],%fd19; }

	// end inline asm
	add.s64 	%rd75, %rd66, 72;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd75],%fd21; }

	// end inline asm
	add.s64 	%rd76, %rd66, 80;
	// begin inline asm
	{ atom.add.f64 %fd22,[%rd76],%fd23; }

	// end inline asm
	add.s64 	%rd77, %rd66, 88;
	// begin inline asm
	{ atom.add.f64 %fd24,[%rd77],%fd25; }

	// end inline asm

$L__BB0_17:
	add.s64 	%rd85, %rd85, %rd15;
	setp.lt.u64 	%p11, %rd85, %rd34;
	@%p11 bra 	$L__BB0_2;

$L__BB0_18:
	ret;

}
	// .globl	compute_affine_kinematic_grad_cuda_kernel_backward
.visible .entry compute_affine_kinematic_grad_cuda_kernel_backward(
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_3[56],
	.param .f64 compute_affine_kinematic_grad_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_11[56],
	.param .f64 compute_affine_kinematic_grad_cuda_kernel_backward_param_12,
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_14[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_15[56],
	.param .align 8 .b8 compute_affine_kinematic_grad_cuda_kernel_backward_param_16[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<90>;
	.reg .b32 	%r<213>;
	.reg .f64 	%fd<267>;
	.reg .b64 	%rd<157>;


	ld.param.v2.u32 	{%r106, %r107}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r108, %r109}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r114, %r115}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r122, %r123}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r130, %r131}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd78, [compute_affine_kinematic_grad_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r138, %r139}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r146, %r147}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r154, %r155}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r162, %r163}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r170, %r171}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r178, %r179}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r186, %r187}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_13+32];
	ld.param.v2.u32 	{%r194, %r195}, [compute_affine_kinematic_grad_cuda_kernel_backward_param_14+32];
	ld.param.u64 	%rd71, [compute_affine_kinematic_grad_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd69, [compute_affine_kinematic_grad_cuda_kernel_backward_param_13];
	ld.param.u64 	%rd67, [compute_affine_kinematic_grad_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd65, [compute_affine_kinematic_grad_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd63, [compute_affine_kinematic_grad_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd61, [compute_affine_kinematic_grad_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd60, [compute_affine_kinematic_grad_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd59, [compute_affine_kinematic_grad_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd58, [compute_affine_kinematic_grad_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd56, [compute_affine_kinematic_grad_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd55, [compute_affine_kinematic_grad_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd53, [compute_affine_kinematic_grad_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd52, [compute_affine_kinematic_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd51, [compute_affine_kinematic_grad_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd50, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r198, %ntid.x;
	cvt.u64.u32 	%rd1, %r198;
	mov.u32 	%r199, %ctaid.x;
	mul.wide.u32 	%rd73, %r198, %r199;
	mov.u32 	%r200, %tid.x;
	cvt.u64.u32 	%rd74, %r200;
	add.s64 	%rd153, %rd73, %rd74;
	setp.ge.u64 	%p1, %rd153, %rd50;
	@%p1 bra 	$L__BB1_34;

	cvta.to.global.u64 	%rd12, %rd69;
	cvta.to.global.u64 	%rd13, %rd63;
	cvta.to.global.u64 	%rd14, %rd61;
	cvta.to.global.u64 	%rd15, %rd59;
	cvta.to.global.u64 	%rd16, %rd58;
	cvta.to.global.u64 	%rd17, %rd55;
	cvta.to.global.u64 	%rd18, %rd51;
	cvt.s64.s32 	%rd19, %r109;
	cvt.s64.s32 	%rd20, %r108;
	cvt.s64.s32 	%rd21, %r107;
	cvt.s64.s32 	%rd22, %r154;
	cvt.s64.s32 	%rd23, %r162;
	mov.u32 	%r201, %nctaid.x;
	cvt.u64.u32 	%rd75, %r201;
	mul.lo.s64 	%rd24, %rd1, %rd75;
	cvt.s64.s32 	%rd25, %r146;
	cvt.s64.s32 	%rd26, %r122;
	cvt.s64.s32 	%rd27, %r114;
	cvt.s64.s32 	%rd28, %r130;
	cvt.s64.s32 	%rd29, %r194;
	cvt.s64.s32 	%rd30, %r186;
	cvt.s64.s32 	%rd31, %r138;
	cvt.s64.s32 	%rd32, %r178;
	cvt.s64.s32 	%rd33, %r170;
	cvta.to.global.u64 	%rd34, %rd53;

$L__BB1_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd154, %rd153;
	@%p2 bra 	$L__BB1_6;

	or.b64  	%rd76, %rd153, %rd19;
	and.b64  	%rd77, %rd76, -4294967296;
	setp.eq.s64 	%p3, %rd77, 0;
	@%p3 bra 	$L__BB1_5;

	div.u64 	%rd154, %rd153, %rd19;
	bra.uni 	$L__BB1_6;

$L__BB1_5:
	cvt.u32.u64 	%r202, %rd19;
	cvt.u32.u64 	%r203, %rd153;
	div.u32 	%r204, %r203, %r202;
	cvt.u64.u32 	%rd154, %r204;

$L__BB1_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB1_10;

	or.b64  	%rd78, %rd154, %rd20;
	and.b64  	%rd79, %rd78, -4294967296;
	setp.eq.s64 	%p5, %rd79, 0;
	@%p5 bra 	$L__BB1_9;

	div.u64 	%rd154, %rd154, %rd20;
	bra.uni 	$L__BB1_10;

$L__BB1_9:
	cvt.u32.u64 	%r205, %rd20;
	cvt.u32.u64 	%r206, %rd154;
	div.u32 	%r207, %r206, %r205;
	cvt.u64.u32 	%rd154, %r207;

$L__BB1_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB1_14;

	or.b64  	%rd80, %rd154, %rd21;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.eq.s64 	%p7, %rd81, 0;
	@%p7 bra 	$L__BB1_13;

	div.u64 	%rd154, %rd154, %rd21;
	bra.uni 	$L__BB1_14;

$L__BB1_13:
	cvt.u32.u64 	%r208, %rd21;
	cvt.u32.u64 	%r209, %rd154;
	div.u32 	%r210, %r209, %r208;
	cvt.u64.u32 	%rd154, %r210;

$L__BB1_14:
	cvt.s64.s32 	%rd82, %rd154;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd45, %rd82, 0, %p8;
	mul.lo.s64 	%rd83, %rd45, %rd22;
	add.s64 	%rd84, %rd14, %rd83;
	ld.global.s32 	%rd85, [%rd84];
	mul.lo.s64 	%rd86, %rd85, %rd23;
	add.s64 	%rd87, %rd13, %rd86;
	ld.global.u32 	%r211, [%rd87];
	add.s32 	%r212, %r211, -1;
	setp.lt.u32 	%p9, %r212, 2;
	@%p9 bra 	$L__BB1_33;

	mul.lo.s64 	%rd46, %rd45, %rd25;
	mul.lo.s64 	%rd88, %rd45, %rd26;
	add.s64 	%rd89, %rd34, %rd88;
	ld.global.u8 	%rs89, [%rd89];
	setp.eq.s16 	%p10, %rs89, 0;
	mov.f64 	%fd266, 0d0000000000000000;
	@%p10 bra 	$L__BB1_29;

	ld.param.u64 	%rd151, [compute_affine_kinematic_grad_cuda_kernel_backward_param_13];
	setp.eq.s64 	%p11, %rd151, 0;
	mul.lo.s64 	%rd47, %rd45, %rd27;
	add.s64 	%rd90, %rd18, %rd47;
	mul.lo.s64 	%rd48, %rd45, %rd28;
	add.s64 	%rd91, %rd17, %rd48;
	ld.global.f64 	%fd80, [%rd91];
	ld.global.f64 	%fd81, [%rd90];
	sub.f64 	%fd1, %fd81, %fd80;
	ld.global.f64 	%fd82, [%rd91+8];
	ld.global.f64 	%fd83, [%rd90+8];
	sub.f64 	%fd2, %fd83, %fd82;
	ld.global.f64 	%fd84, [%rd91+16];
	ld.global.f64 	%fd85, [%rd90+16];
	sub.f64 	%fd3, %fd85, %fd84;
	ld.global.f64 	%fd86, [%rd91+24];
	ld.global.f64 	%fd87, [%rd90+24];
	sub.f64 	%fd4, %fd87, %fd86;
	ld.global.f64 	%fd88, [%rd91+32];
	ld.global.f64 	%fd89, [%rd90+32];
	sub.f64 	%fd5, %fd89, %fd88;
	ld.global.f64 	%fd90, [%rd91+40];
	ld.global.f64 	%fd91, [%rd90+40];
	sub.f64 	%fd6, %fd91, %fd90;
	ld.global.f64 	%fd92, [%rd91+48];
	ld.global.f64 	%fd93, [%rd90+48];
	sub.f64 	%fd7, %fd93, %fd92;
	ld.global.f64 	%fd94, [%rd91+56];
	ld.global.f64 	%fd95, [%rd90+56];
	sub.f64 	%fd8, %fd95, %fd94;
	ld.global.f64 	%fd96, [%rd91+64];
	ld.global.f64 	%fd97, [%rd90+64];
	sub.f64 	%fd9, %fd97, %fd96;
	ld.global.f64 	%fd98, [%rd91+72];
	ld.global.f64 	%fd99, [%rd90+72];
	sub.f64 	%fd10, %fd99, %fd98;
	ld.global.f64 	%fd100, [%rd91+80];
	ld.global.f64 	%fd101, [%rd90+80];
	sub.f64 	%fd11, %fd101, %fd100;
	ld.global.f64 	%fd102, [%rd91+88];
	ld.global.f64 	%fd103, [%rd90+88];
	sub.f64 	%fd12, %fd103, %fd102;
	add.s64 	%rd92, %rd15, %rd46;
	ld.global.f64 	%fd13, [%rd92];
	@%p11 bra 	$L__BB1_18;

	mul.lo.s64 	%rd93, %rd45, %rd30;
	add.s64 	%rd94, %rd12, %rd93;
	ld.global.f64 	%fd104, [%rd94];
	add.f64 	%fd265, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd94+8];
	add.f64 	%fd264, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd94+16];
	add.f64 	%fd263, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd94+24];
	add.f64 	%fd262, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd94+32];
	add.f64 	%fd261, %fd108, 0d0000000000000000;
	ld.global.f64 	%fd109, [%rd94+40];
	add.f64 	%fd260, %fd109, 0d0000000000000000;
	ld.global.f64 	%fd110, [%rd94+48];
	add.f64 	%fd259, %fd110, 0d0000000000000000;
	ld.global.f64 	%fd111, [%rd94+56];
	add.f64 	%fd258, %fd111, 0d0000000000000000;
	ld.global.f64 	%fd112, [%rd94+64];
	add.f64 	%fd257, %fd112, 0d0000000000000000;
	ld.global.f64 	%fd113, [%rd94+72];
	add.f64 	%fd256, %fd113, 0d0000000000000000;
	ld.global.f64 	%fd114, [%rd94+80];
	add.f64 	%fd255, %fd114, 0d0000000000000000;
	ld.global.f64 	%fd115, [%rd94+88];
	add.f64 	%fd254, %fd115, 0d0000000000000000;
	bra.uni 	$L__BB1_20;

$L__BB1_18:
	ld.param.u64 	%rd152, [compute_affine_kinematic_grad_cuda_kernel_backward_param_5+8];
	setp.eq.s64 	%p12, %rd152, 0;
	mov.f64 	%fd254, 0d0000000000000000;
	mov.f64 	%fd255, %fd254;
	mov.f64 	%fd256, %fd254;
	mov.f64 	%fd257, %fd254;
	mov.f64 	%fd258, %fd254;
	mov.f64 	%fd259, %fd254;
	mov.f64 	%fd260, %fd254;
	mov.f64 	%fd261, %fd254;
	mov.f64 	%fd262, %fd254;
	mov.f64 	%fd263, %fd254;
	mov.f64 	%fd264, %fd254;
	mov.f64 	%fd265, %fd254;
	@%p12 bra 	$L__BB1_20;

	mul.lo.s64 	%rd95, %rd45, %rd31;
	add.s64 	%rd96, %rd16, %rd95;
	ld.global.f64 	%fd128, [%rd96];
	add.f64 	%fd265, %fd128, 0d0000000000000000;
	ld.global.f64 	%fd129, [%rd96+8];
	add.f64 	%fd264, %fd129, 0d0000000000000000;
	ld.global.f64 	%fd130, [%rd96+16];
	add.f64 	%fd263, %fd130, 0d0000000000000000;
	ld.global.f64 	%fd131, [%rd96+24];
	add.f64 	%fd262, %fd131, 0d0000000000000000;
	ld.global.f64 	%fd132, [%rd96+32];
	add.f64 	%fd261, %fd132, 0d0000000000000000;
	ld.global.f64 	%fd133, [%rd96+40];
	add.f64 	%fd260, %fd133, 0d0000000000000000;
	ld.global.f64 	%fd134, [%rd96+48];
	add.f64 	%fd259, %fd134, 0d0000000000000000;
	ld.global.f64 	%fd135, [%rd96+56];
	add.f64 	%fd258, %fd135, 0d0000000000000000;
	ld.global.f64 	%fd136, [%rd96+64];
	add.f64 	%fd257, %fd136, 0d0000000000000000;
	ld.global.f64 	%fd137, [%rd96+72];
	add.f64 	%fd256, %fd137, 0d0000000000000000;
	ld.global.f64 	%fd138, [%rd96+80];
	add.f64 	%fd255, %fd138, 0d0000000000000000;
	ld.global.f64 	%fd139, [%rd96+88];
	add.f64 	%fd254, %fd139, 0d0000000000000000;

$L__BB1_20:
	mul.f64 	%fd140, %fd13, %fd78;
	fma.rn.f64 	%fd50, %fd140, %fd265, 0d0000000000000000;
	mov.f64 	%fd141, 0d0000000000000000;
	fma.rn.f64 	%fd51, %fd140, %fd264, 0d0000000000000000;
	fma.rn.f64 	%fd52, %fd140, %fd263, 0d0000000000000000;
	fma.rn.f64 	%fd53, %fd140, %fd262, 0d0000000000000000;
	fma.rn.f64 	%fd54, %fd140, %fd261, 0d0000000000000000;
	fma.rn.f64 	%fd55, %fd140, %fd260, 0d0000000000000000;
	fma.rn.f64 	%fd56, %fd140, %fd259, 0d0000000000000000;
	fma.rn.f64 	%fd57, %fd140, %fd258, 0d0000000000000000;
	fma.rn.f64 	%fd58, %fd140, %fd257, 0d0000000000000000;
	fma.rn.f64 	%fd59, %fd140, %fd256, 0d0000000000000000;
	fma.rn.f64 	%fd60, %fd140, %fd255, 0d0000000000000000;
	fma.rn.f64 	%fd61, %fd140, %fd254, 0d0000000000000000;
	fma.rn.f64 	%fd142, %fd1, %fd265, 0d0000000000000000;
	fma.rn.f64 	%fd143, %fd2, %fd264, %fd142;
	fma.rn.f64 	%fd144, %fd3, %fd263, %fd143;
	fma.rn.f64 	%fd145, %fd4, %fd262, %fd144;
	fma.rn.f64 	%fd146, %fd5, %fd261, %fd145;
	fma.rn.f64 	%fd147, %fd6, %fd260, %fd146;
	fma.rn.f64 	%fd148, %fd7, %fd259, %fd147;
	fma.rn.f64 	%fd149, %fd8, %fd258, %fd148;
	fma.rn.f64 	%fd150, %fd9, %fd257, %fd149;
	fma.rn.f64 	%fd151, %fd10, %fd256, %fd150;
	fma.rn.f64 	%fd152, %fd11, %fd255, %fd151;
	fma.rn.f64 	%fd62, %fd12, %fd254, %fd152;
	sub.f64 	%fd63, %fd141, %fd50;
	sub.f64 	%fd64, %fd141, %fd51;
	sub.f64 	%fd65, %fd141, %fd52;
	sub.f64 	%fd66, %fd141, %fd53;
	sub.f64 	%fd67, %fd141, %fd54;
	sub.f64 	%fd68, %fd141, %fd55;
	sub.f64 	%fd69, %fd141, %fd56;
	sub.f64 	%fd70, %fd141, %fd57;
	sub.f64 	%fd71, %fd141, %fd58;
	sub.f64 	%fd72, %fd141, %fd59;
	sub.f64 	%fd73, %fd141, %fd60;
	sub.f64 	%fd74, %fd141, %fd61;
	setp.eq.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB1_22;

	mul.lo.s64 	%rd109, %rd45, %rd32;
	add.s64 	%rd97, %rd67, %rd109;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd97],%fd63; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd98],%fd64; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd99],%fd65; }

	// end inline asm
	add.s64 	%rd100, %rd97, 24;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd100],%fd66; }

	// end inline asm
	add.s64 	%rd101, %rd97, 32;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd101],%fd67; }

	// end inline asm
	add.s64 	%rd102, %rd97, 40;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd102],%fd68; }

	// end inline asm
	add.s64 	%rd103, %rd97, 48;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd103],%fd69; }

	// end inline asm
	add.s64 	%rd104, %rd97, 56;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd104],%fd70; }

	// end inline asm
	add.s64 	%rd105, %rd97, 64;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd105],%fd71; }

	// end inline asm
	add.s64 	%rd106, %rd97, 72;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd106],%fd72; }

	// end inline asm
	add.s64 	%rd107, %rd97, 80;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd107],%fd73; }

	// end inline asm
	add.s64 	%rd108, %rd97, 88;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd108],%fd74; }

	// end inline asm
	bra.uni 	$L__BB1_24;

$L__BB1_22:
	setp.eq.s64 	%p14, %rd56, 0;
	@%p14 bra 	$L__BB1_24;

	add.s64 	%rd110, %rd56, %rd48;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd110],%fd63; }

	// end inline asm
	add.s64 	%rd111, %rd110, 8;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd111],%fd64; }

	// end inline asm
	add.s64 	%rd112, %rd110, 16;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd112],%fd65; }

	// end inline asm
	add.s64 	%rd113, %rd110, 24;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd113],%fd66; }

	// end inline asm
	add.s64 	%rd114, %rd110, 32;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd114],%fd67; }

	// end inline asm
	add.s64 	%rd115, %rd110, 40;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd115],%fd68; }

	// end inline asm
	add.s64 	%rd116, %rd110, 48;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd116],%fd69; }

	// end inline asm
	add.s64 	%rd117, %rd110, 56;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd117],%fd70; }

	// end inline asm
	add.s64 	%rd118, %rd110, 64;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd118],%fd71; }

	// end inline asm
	add.s64 	%rd119, %rd110, 72;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd119],%fd72; }

	// end inline asm
	add.s64 	%rd120, %rd110, 80;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd120],%fd73; }

	// end inline asm
	add.s64 	%rd121, %rd110, 88;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd121],%fd74; }

	// end inline asm

$L__BB1_24:
	setp.eq.s64 	%p15, %rd65, 0;
	@%p15 bra 	$L__BB1_26;

	mul.lo.s64 	%rd134, %rd45, %rd33;
	add.s64 	%rd122, %rd65, %rd134;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd122],%fd50; }

	// end inline asm
	add.s64 	%rd123, %rd122, 8;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd123],%fd51; }

	// end inline asm
	add.s64 	%rd124, %rd122, 16;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd124],%fd52; }

	// end inline asm
	add.s64 	%rd125, %rd122, 24;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd125],%fd53; }

	// end inline asm
	add.s64 	%rd126, %rd122, 32;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd126],%fd54; }

	// end inline asm
	add.s64 	%rd127, %rd122, 40;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd127],%fd55; }

	// end inline asm
	add.s64 	%rd128, %rd122, 48;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd128],%fd56; }

	// end inline asm
	add.s64 	%rd129, %rd122, 56;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd129],%fd57; }

	// end inline asm
	add.s64 	%rd130, %rd122, 64;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd130],%fd58; }

	// end inline asm
	add.s64 	%rd131, %rd122, 72;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd131],%fd59; }

	// end inline asm
	add.s64 	%rd132, %rd122, 80;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd132],%fd60; }

	// end inline asm
	add.s64 	%rd133, %rd122, 88;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd133],%fd61; }

	// end inline asm
	bra.uni 	$L__BB1_28;

$L__BB1_26:
	setp.eq.s64 	%p16, %rd52, 0;
	@%p16 bra 	$L__BB1_28;

	add.s64 	%rd135, %rd52, %rd47;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd135],%fd50; }

	// end inline asm
	add.s64 	%rd136, %rd135, 8;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd136],%fd51; }

	// end inline asm
	add.s64 	%rd137, %rd135, 16;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd137],%fd52; }

	// end inline asm
	add.s64 	%rd138, %rd135, 24;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd138],%fd53; }

	// end inline asm
	add.s64 	%rd139, %rd135, 32;
	// begin inline asm
	{ atom.add.f64 %fd233,[%rd139],%fd54; }

	// end inline asm
	add.s64 	%rd140, %rd135, 40;
	// begin inline asm
	{ atom.add.f64 %fd235,[%rd140],%fd55; }

	// end inline asm
	add.s64 	%rd141, %rd135, 48;
	// begin inline asm
	{ atom.add.f64 %fd237,[%rd141],%fd56; }

	// end inline asm
	add.s64 	%rd142, %rd135, 56;
	// begin inline asm
	{ atom.add.f64 %fd239,[%rd142],%fd57; }

	// end inline asm
	add.s64 	%rd143, %rd135, 64;
	// begin inline asm
	{ atom.add.f64 %fd241,[%rd143],%fd58; }

	// end inline asm
	add.s64 	%rd144, %rd135, 72;
	// begin inline asm
	{ atom.add.f64 %fd243,[%rd144],%fd59; }

	// end inline asm
	add.s64 	%rd145, %rd135, 80;
	// begin inline asm
	{ atom.add.f64 %fd245,[%rd145],%fd60; }

	// end inline asm
	add.s64 	%rd146, %rd135, 88;
	// begin inline asm
	{ atom.add.f64 %fd247,[%rd146],%fd61; }

	// end inline asm

$L__BB1_28:
	add.f64 	%fd249, %fd62, 0d0000000000000000;
	fma.rn.f64 	%fd266, %fd249, %fd78, 0d0000000000000000;

$L__BB1_29:
	add.f64 	%fd77, %fd266, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd71, 0;
	@%p17 bra 	$L__BB1_31;

	mul.lo.s64 	%rd148, %rd45, %rd29;
	add.s64 	%rd147, %rd71, %rd148;
	// begin inline asm
	{ atom.add.f64 %fd250,[%rd147],%fd77; }

	// end inline asm
	bra.uni 	$L__BB1_33;

$L__BB1_31:
	setp.eq.s64 	%p18, %rd60, 0;
	@%p18 bra 	$L__BB1_33;

	add.s64 	%rd149, %rd60, %rd46;
	// begin inline asm
	{ atom.add.f64 %fd252,[%rd149],%fd77; }

	// end inline asm

$L__BB1_33:
	ld.param.u64 	%rd150, [compute_affine_kinematic_grad_cuda_kernel_backward_param_0+24];
	add.s64 	%rd153, %rd153, %rd24;
	setp.lt.u64 	%p19, %rd153, %rd150;
	@%p19 bra 	$L__BB1_2;

$L__BB1_34:
	ret;

}
	// .globl	compute_affine_kinematic_energy_cuda_kernel_forward
.visible .entry compute_affine_kinematic_energy_cuda_kernel_forward(
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_3[56],
	.param .f64 compute_affine_kinematic_energy_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_forward_param_6[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<109>;
	.reg .f64 	%fd<77>;
	.reg .b64 	%rd<76>;


	ld.param.v2.u32 	{%r52, %r53}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r54, %r55}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r60, %r61}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r68, %r69}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r76, %r77}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd2, [compute_affine_kinematic_energy_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r84, %r85}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r92, %r93}, [compute_affine_kinematic_energy_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd39, [compute_affine_kinematic_energy_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd37, [compute_affine_kinematic_energy_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd35, [compute_affine_kinematic_energy_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [compute_affine_kinematic_energy_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [compute_affine_kinematic_energy_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_energy_cuda_kernel_forward_param_0+16];
	mov.u32 	%r96, %ntid.x;
	cvt.u64.u32 	%rd1, %r96;
	mov.u32 	%r97, %ctaid.x;
	mul.wide.u32 	%rd41, %r96, %r97;
	mov.u32 	%r98, %tid.x;
	cvt.u64.u32 	%rd42, %r98;
	add.s64 	%rd72, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd72, %rd30;
	@%p1 bra 	$L__BB2_17;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd6, %rd35;
	cvta.to.global.u64 	%rd7, %rd31;
	cvt.s64.s32 	%rd8, %r55;
	cvt.s64.s32 	%rd9, %r54;
	cvt.s64.s32 	%rd10, %r53;
	cvt.s64.s32 	%rd11, %r92;
	cvt.s64.s32 	%rd12, %r68;
	mov.u32 	%r99, %nctaid.x;
	cvt.u64.u32 	%rd43, %r99;
	mul.lo.s64 	%rd13, %rd1, %rd43;
	mul.f64 	%fd1, %fd2, 0d3FE0000000000000;
	cvt.s64.s32 	%rd14, %r60;
	cvt.s64.s32 	%rd15, %r76;
	cvt.s64.s32 	%rd16, %r84;
	cvta.to.global.u64 	%rd17, %rd33;

$L__BB2_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd73, %rd72;
	@%p2 bra 	$L__BB2_6;

	or.b64  	%rd44, %rd72, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB2_5;

	div.u64 	%rd73, %rd72, %rd8;
	bra.uni 	$L__BB2_6;

$L__BB2_5:
	cvt.u32.u64 	%r100, %rd8;
	cvt.u32.u64 	%r101, %rd72;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd73, %r102;

$L__BB2_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB2_10;

	or.b64  	%rd46, %rd73, %rd9;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB2_9;

	div.u64 	%rd73, %rd73, %rd9;
	bra.uni 	$L__BB2_10;

$L__BB2_9:
	cvt.u32.u64 	%r103, %rd9;
	cvt.u32.u64 	%r104, %rd73;
	div.u32 	%r105, %r104, %r103;
	cvt.u64.u32 	%rd73, %r105;

$L__BB2_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB2_14;

	or.b64  	%rd48, %rd73, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p7, %rd49, 0;
	@%p7 bra 	$L__BB2_13;

	div.u64 	%rd73, %rd73, %rd10;
	bra.uni 	$L__BB2_14;

$L__BB2_13:
	cvt.u32.u64 	%r106, %rd10;
	cvt.u32.u64 	%r107, %rd73;
	div.u32 	%r108, %r107, %r106;
	cvt.u64.u32 	%rd73, %r108;

$L__BB2_14:
	cvt.s64.s32 	%rd50, %rd73;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd28, %rd50, 0, %p8;
	mul.lo.s64 	%rd51, %rd28, %rd12;
	add.s64 	%rd52, %rd17, %rd51;
	ld.global.u8 	%rs41, [%rd52];
	setp.eq.s16 	%p9, %rs41, 0;
	@%p9 bra 	$L__BB2_16;

	mul.lo.s64 	%rd65, %rd28, %rd11;
	add.s64 	%rd66, %rd4, %rd65;
	ld.global.f64 	%fd27, [%rd66];
	mul.f64 	%fd28, %fd1, %fd27;
	mul.lo.s64 	%rd67, %rd28, %rd14;
	add.s64 	%rd68, %rd7, %rd67;
	mul.lo.s64 	%rd69, %rd28, %rd15;
	add.s64 	%rd70, %rd6, %rd69;
	ld.global.f64 	%fd29, [%rd70];
	ld.global.f64 	%fd30, [%rd68];
	sub.f64 	%fd31, %fd30, %fd29;
	mul.f64 	%fd32, %fd31, %fd31;
	mul.f64 	%fd4, %fd28, %fd32;
	mul.lo.s64 	%rd71, %rd28, %rd16;
	add.s64 	%rd64, %rd37, %rd71;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd64],%fd4; }

	// end inline asm
	ld.global.f64 	%fd33, [%rd70+8];
	ld.global.f64 	%fd34, [%rd68+8];
	sub.f64 	%fd35, %fd34, %fd33;
	mul.f64 	%fd36, %fd35, %fd35;
	mul.f64 	%fd6, %fd28, %fd36;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd64],%fd6; }

	// end inline asm
	ld.global.f64 	%fd37, [%rd70+16];
	ld.global.f64 	%fd38, [%rd68+16];
	sub.f64 	%fd39, %fd38, %fd37;
	mul.f64 	%fd40, %fd39, %fd39;
	mul.f64 	%fd8, %fd28, %fd40;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd64],%fd8; }

	// end inline asm
	ld.global.f64 	%fd41, [%rd70+24];
	ld.global.f64 	%fd42, [%rd68+24];
	sub.f64 	%fd43, %fd42, %fd41;
	mul.f64 	%fd44, %fd43, %fd43;
	mul.f64 	%fd10, %fd28, %fd44;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd64],%fd10; }

	// end inline asm
	ld.global.f64 	%fd45, [%rd70+32];
	ld.global.f64 	%fd46, [%rd68+32];
	sub.f64 	%fd47, %fd46, %fd45;
	mul.f64 	%fd48, %fd47, %fd47;
	mul.f64 	%fd12, %fd28, %fd48;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd64],%fd12; }

	// end inline asm
	ld.global.f64 	%fd49, [%rd70+40];
	ld.global.f64 	%fd50, [%rd68+40];
	sub.f64 	%fd51, %fd50, %fd49;
	mul.f64 	%fd52, %fd51, %fd51;
	mul.f64 	%fd14, %fd28, %fd52;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd64],%fd14; }

	// end inline asm
	ld.global.f64 	%fd53, [%rd70+48];
	ld.global.f64 	%fd54, [%rd68+48];
	sub.f64 	%fd55, %fd54, %fd53;
	mul.f64 	%fd56, %fd55, %fd55;
	mul.f64 	%fd16, %fd28, %fd56;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd64],%fd16; }

	// end inline asm
	ld.global.f64 	%fd57, [%rd70+56];
	ld.global.f64 	%fd58, [%rd68+56];
	sub.f64 	%fd59, %fd58, %fd57;
	mul.f64 	%fd60, %fd59, %fd59;
	mul.f64 	%fd18, %fd28, %fd60;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd64],%fd18; }

	// end inline asm
	ld.global.f64 	%fd61, [%rd70+64];
	ld.global.f64 	%fd62, [%rd68+64];
	sub.f64 	%fd63, %fd62, %fd61;
	mul.f64 	%fd64, %fd63, %fd63;
	mul.f64 	%fd20, %fd28, %fd64;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd64],%fd20; }

	// end inline asm
	ld.global.f64 	%fd65, [%rd70+72];
	ld.global.f64 	%fd66, [%rd68+72];
	sub.f64 	%fd67, %fd66, %fd65;
	mul.f64 	%fd68, %fd67, %fd67;
	mul.f64 	%fd22, %fd28, %fd68;
	// begin inline asm
	{ atom.add.f64 %fd21,[%rd64],%fd22; }

	// end inline asm
	ld.global.f64 	%fd69, [%rd70+80];
	ld.global.f64 	%fd70, [%rd68+80];
	sub.f64 	%fd71, %fd70, %fd69;
	mul.f64 	%fd72, %fd71, %fd71;
	mul.f64 	%fd24, %fd28, %fd72;
	// begin inline asm
	{ atom.add.f64 %fd23,[%rd64],%fd24; }

	// end inline asm
	ld.global.f64 	%fd73, [%rd70+88];
	ld.global.f64 	%fd74, [%rd68+88];
	sub.f64 	%fd75, %fd74, %fd73;
	mul.f64 	%fd76, %fd75, %fd75;
	mul.f64 	%fd26, %fd28, %fd76;
	// begin inline asm
	{ atom.add.f64 %fd25,[%rd64],%fd26; }

	// end inline asm

$L__BB2_16:
	add.s64 	%rd72, %rd72, %rd13;
	setp.lt.u64 	%p10, %rd72, %rd30;
	@%p10 bra 	$L__BB2_2;

$L__BB2_17:
	ret;

}
	// .globl	compute_affine_kinematic_energy_cuda_kernel_backward
.visible .entry compute_affine_kinematic_energy_cuda_kernel_backward(
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_3[56],
	.param .f64 compute_affine_kinematic_energy_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_9[56],
	.param .f64 compute_affine_kinematic_energy_cuda_kernel_backward_param_10,
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 compute_affine_kinematic_energy_cuda_kernel_backward_param_12[56]
)
{
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<177>;
	.reg .f64 	%fd<1481>;
	.reg .b64 	%rd<694>;


	ld.param.v2.u32 	{%r88, %r89}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r90, %r91}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r96, %r97}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r104, %r105}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r112, %r113}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd102, [compute_affine_kinematic_energy_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r120, %r121}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r128, %r129}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r136, %r137}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r144, %r145}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r152, %r153}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_11+32];
	ld.param.v2.u32 	{%r160, %r161}, [compute_affine_kinematic_energy_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd65, [compute_affine_kinematic_energy_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd63, [compute_affine_kinematic_energy_cuda_kernel_backward_param_11];
	ld.param.u64 	%rd61, [compute_affine_kinematic_energy_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd59, [compute_affine_kinematic_energy_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd58, [compute_affine_kinematic_energy_cuda_kernel_backward_param_6+8];
	ld.param.u64 	%rd57, [compute_affine_kinematic_energy_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd56, [compute_affine_kinematic_energy_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd54, [compute_affine_kinematic_energy_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd53, [compute_affine_kinematic_energy_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd51, [compute_affine_kinematic_energy_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd50, [compute_affine_kinematic_energy_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd49, [compute_affine_kinematic_energy_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd48, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+16];
	mov.u32 	%r164, %ntid.x;
	cvt.u64.u32 	%rd1, %r164;
	mov.u32 	%r165, %ctaid.x;
	mul.wide.u32 	%rd67, %r164, %r165;
	mov.u32 	%r166, %tid.x;
	cvt.u64.u32 	%rd68, %r166;
	add.s64 	%rd690, %rd67, %rd68;
	setp.ge.u64 	%p1, %rd690, %rd48;
	@%p1 bra 	$L__BB3_165;

	cvta.to.global.u64 	%rd12, %rd63;
	cvta.to.global.u64 	%rd14, %rd57;
	cvta.to.global.u64 	%rd15, %rd53;
	cvta.to.global.u64 	%rd16, %rd49;
	cvt.s64.s32 	%rd17, %r91;
	cvt.s64.s32 	%rd18, %r90;
	cvt.s64.s32 	%rd19, %r89;
	cvt.s64.s32 	%rd20, %r128;
	cvt.s64.s32 	%rd21, %r104;
	mul.f64 	%fd1, %fd102, 0d3FE0000000000000;
	cvt.s64.s32 	%rd22, %r96;
	cvt.s64.s32 	%rd23, %r112;
	mov.u32 	%r167, %nctaid.x;
	cvt.u64.u32 	%rd69, %r167;
	mul.lo.s64 	%rd24, %rd1, %rd69;
	cvt.s64.s32 	%rd25, %r160;
	cvt.s64.s32 	%rd26, %r152;
	cvt.s64.s32 	%rd27, %r120;
	cvt.s64.s32 	%rd28, %r144;
	cvt.s64.s32 	%rd29, %r136;
	cvta.to.global.u64 	%rd30, %rd51;

$L__BB3_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd691, %rd690;
	@%p2 bra 	$L__BB3_6;

	or.b64  	%rd70, %rd690, %rd17;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p3, %rd71, 0;
	@%p3 bra 	$L__BB3_5;

	div.u64 	%rd691, %rd690, %rd17;
	bra.uni 	$L__BB3_6;

$L__BB3_5:
	cvt.u32.u64 	%r168, %rd17;
	cvt.u32.u64 	%r169, %rd690;
	div.u32 	%r170, %r169, %r168;
	cvt.u64.u32 	%rd691, %r170;

$L__BB3_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB3_10;

	or.b64  	%rd72, %rd691, %rd18;
	and.b64  	%rd73, %rd72, -4294967296;
	setp.eq.s64 	%p5, %rd73, 0;
	@%p5 bra 	$L__BB3_9;

	div.u64 	%rd691, %rd691, %rd18;
	bra.uni 	$L__BB3_10;

$L__BB3_9:
	cvt.u32.u64 	%r171, %rd18;
	cvt.u32.u64 	%r172, %rd691;
	div.u32 	%r173, %r172, %r171;
	cvt.u64.u32 	%rd691, %r173;

$L__BB3_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB3_14;

	or.b64  	%rd74, %rd691, %rd19;
	and.b64  	%rd75, %rd74, -4294967296;
	setp.eq.s64 	%p7, %rd75, 0;
	@%p7 bra 	$L__BB3_13;

	div.u64 	%rd691, %rd691, %rd19;
	bra.uni 	$L__BB3_14;

$L__BB3_13:
	cvt.u32.u64 	%r174, %rd19;
	cvt.u32.u64 	%r175, %rd691;
	div.u32 	%r176, %r175, %r174;
	cvt.u64.u32 	%rd691, %r176;

$L__BB3_14:
	cvt.s64.s32 	%rd76, %rd691;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd41, %rd76, 0, %p8;
	mul.lo.s64 	%rd42, %rd41, %rd20;
	mul.lo.s64 	%rd77, %rd41, %rd21;
	add.s64 	%rd78, %rd30, %rd77;
	ld.global.u8 	%rs73, [%rd78];
	setp.eq.s16 	%p9, %rs73, 0;
	mov.f64 	%fd1480, 0d0000000000000000;
	@%p9 bra 	$L__BB3_160;

	cvta.to.global.u64 	%rd689, %rd56;
	ld.param.u64 	%rd688, [compute_affine_kinematic_energy_cuda_kernel_backward_param_11];
	setp.eq.s64 	%p10, %rd688, 0;
	add.s64 	%rd79, %rd14, %rd42;
	ld.global.f64 	%fd104, [%rd79];
	mul.f64 	%fd2, %fd1, %fd104;
	mul.lo.s64 	%rd43, %rd41, %rd22;
	add.s64 	%rd80, %rd16, %rd43;
	mul.lo.s64 	%rd44, %rd41, %rd23;
	add.s64 	%rd81, %rd15, %rd44;
	ld.global.f64 	%fd105, [%rd81];
	ld.global.f64 	%fd106, [%rd80];
	sub.f64 	%fd3, %fd106, %fd105;
	ld.global.f64 	%fd107, [%rd81+8];
	ld.global.f64 	%fd108, [%rd80+8];
	sub.f64 	%fd4, %fd108, %fd107;
	ld.global.f64 	%fd109, [%rd81+16];
	ld.global.f64 	%fd110, [%rd80+16];
	sub.f64 	%fd5, %fd110, %fd109;
	ld.global.f64 	%fd111, [%rd81+24];
	ld.global.f64 	%fd112, [%rd80+24];
	sub.f64 	%fd6, %fd112, %fd111;
	ld.global.f64 	%fd113, [%rd81+32];
	ld.global.f64 	%fd114, [%rd80+32];
	sub.f64 	%fd7, %fd114, %fd113;
	ld.global.f64 	%fd115, [%rd81+40];
	ld.global.f64 	%fd116, [%rd80+40];
	sub.f64 	%fd8, %fd116, %fd115;
	ld.global.f64 	%fd117, [%rd81+48];
	ld.global.f64 	%fd118, [%rd80+48];
	sub.f64 	%fd9, %fd118, %fd117;
	ld.global.f64 	%fd119, [%rd81+56];
	ld.global.f64 	%fd120, [%rd80+56];
	sub.f64 	%fd10, %fd120, %fd119;
	ld.global.f64 	%fd121, [%rd81+64];
	ld.global.f64 	%fd122, [%rd80+64];
	sub.f64 	%fd11, %fd122, %fd121;
	ld.global.f64 	%fd123, [%rd81+72];
	ld.global.f64 	%fd124, [%rd80+72];
	sub.f64 	%fd12, %fd124, %fd123;
	ld.global.f64 	%fd125, [%rd81+80];
	ld.global.f64 	%fd126, [%rd80+80];
	sub.f64 	%fd13, %fd126, %fd125;
	ld.global.f64 	%fd14, [%rd81+88];
	ld.global.f64 	%fd15, [%rd80+88];
	mul.lo.s64 	%rd82, %rd41, %rd26;
	add.s64 	%rd45, %rd12, %rd82;
	mul.lo.s64 	%rd83, %rd41, %rd27;
	add.s64 	%rd46, %rd689, %rd83;
	@%p10 bra 	$L__BB3_17;

	ld.global.f64 	%fd127, [%rd45];
	add.f64 	%fd1468, %fd127, 0d0000000000000000;
	bra.uni 	$L__BB3_19;

$L__BB3_17:
	setp.eq.s64 	%p11, %rd56, 0;
	mov.f64 	%fd1468, 0d0000000000000000;
	@%p11 bra 	$L__BB3_19;

	ld.global.f64 	%fd129, [%rd46];
	add.f64 	%fd1468, %fd129, 0d0000000000000000;

$L__BB3_19:
	sub.f64 	%fd130, %fd15, %fd14;
	mul.f64 	%fd131, %fd130, %fd130;
	fma.rn.f64 	%fd19, %fd131, %fd1468, 0d0000000000000000;
	mov.f64 	%fd132, 0d0000000000000000;
	fma.rn.f64 	%fd133, %fd2, %fd1468, 0d0000000000000000;
	fma.rn.f64 	%fd134, %fd130, %fd133, 0d0000000000000000;
	fma.rn.f64 	%fd135, %fd130, %fd133, %fd134;
	setp.eq.s64 	%p12, %rd61, 0;
	@%p12 bra 	$L__BB3_21;

	mov.f64 	%fd1447, 0d0000000000000000;
	sub.f64 	%fd1399, %fd1447, %fd135;
	add.f64 	%fd1398, %fd1399, 0d0000000000000000;
	mul.lo.s64 	%rd96, %rd41, %rd28;
	add.s64 	%rd84, %rd61, %rd96;
	// begin inline asm
	{ atom.add.f64 %fd137,[%rd84],%fd1447; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd139,[%rd85],%fd1447; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd141,[%rd86],%fd1447; }

	// end inline asm
	add.s64 	%rd87, %rd84, 24;
	// begin inline asm
	{ atom.add.f64 %fd143,[%rd87],%fd1447; }

	// end inline asm
	add.s64 	%rd88, %rd84, 32;
	// begin inline asm
	{ atom.add.f64 %fd145,[%rd88],%fd1447; }

	// end inline asm
	add.s64 	%rd89, %rd84, 40;
	// begin inline asm
	{ atom.add.f64 %fd147,[%rd89],%fd1447; }

	// end inline asm
	add.s64 	%rd90, %rd84, 48;
	// begin inline asm
	{ atom.add.f64 %fd149,[%rd90],%fd1447; }

	// end inline asm
	add.s64 	%rd91, %rd84, 56;
	// begin inline asm
	{ atom.add.f64 %fd151,[%rd91],%fd1447; }

	// end inline asm
	add.s64 	%rd92, %rd84, 64;
	// begin inline asm
	{ atom.add.f64 %fd153,[%rd92],%fd1447; }

	// end inline asm
	add.s64 	%rd93, %rd84, 72;
	// begin inline asm
	{ atom.add.f64 %fd155,[%rd93],%fd1447; }

	// end inline asm
	add.s64 	%rd94, %rd84, 80;
	// begin inline asm
	{ atom.add.f64 %fd157,[%rd94],%fd1447; }

	// end inline asm
	add.s64 	%rd95, %rd84, 88;
	// begin inline asm
	{ atom.add.f64 %fd159,[%rd95],%fd1398; }

	// end inline asm
	bra.uni 	$L__BB3_23;

$L__BB3_21:
	setp.eq.s64 	%p13, %rd54, 0;
	@%p13 bra 	$L__BB3_23;

	sub.f64 	%fd1403, %fd132, %fd135;
	add.f64 	%fd1402, %fd1403, 0d0000000000000000;
	add.s64 	%rd97, %rd54, %rd44;
	mov.f64 	%fd182, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd161,[%rd97],%fd182; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd163,[%rd98],%fd182; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd165,[%rd99],%fd182; }

	// end inline asm
	add.s64 	%rd100, %rd97, 24;
	// begin inline asm
	{ atom.add.f64 %fd167,[%rd100],%fd182; }

	// end inline asm
	add.s64 	%rd101, %rd97, 32;
	// begin inline asm
	{ atom.add.f64 %fd169,[%rd101],%fd182; }

	// end inline asm
	add.s64 	%rd102, %rd97, 40;
	// begin inline asm
	{ atom.add.f64 %fd171,[%rd102],%fd182; }

	// end inline asm
	add.s64 	%rd103, %rd97, 48;
	// begin inline asm
	{ atom.add.f64 %fd173,[%rd103],%fd182; }

	// end inline asm
	add.s64 	%rd104, %rd97, 56;
	// begin inline asm
	{ atom.add.f64 %fd175,[%rd104],%fd182; }

	// end inline asm
	add.s64 	%rd105, %rd97, 64;
	// begin inline asm
	{ atom.add.f64 %fd177,[%rd105],%fd182; }

	// end inline asm
	add.s64 	%rd106, %rd97, 72;
	// begin inline asm
	{ atom.add.f64 %fd179,[%rd106],%fd182; }

	// end inline asm
	add.s64 	%rd107, %rd97, 80;
	// begin inline asm
	{ atom.add.f64 %fd181,[%rd107],%fd182; }

	// end inline asm
	add.s64 	%rd108, %rd97, 88;
	// begin inline asm
	{ atom.add.f64 %fd183,[%rd108],%fd1402; }

	// end inline asm

$L__BB3_23:
	setp.eq.s64 	%p14, %rd59, 0;
	@%p14 bra 	$L__BB3_25;

	add.f64 	%fd1400, %fd135, 0d0000000000000000;
	mul.lo.s64 	%rd121, %rd41, %rd29;
	add.s64 	%rd109, %rd59, %rd121;
	mov.f64 	%fd206, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd185,[%rd109],%fd206; }

	// end inline asm
	add.s64 	%rd110, %rd109, 8;
	// begin inline asm
	{ atom.add.f64 %fd187,[%rd110],%fd206; }

	// end inline asm
	add.s64 	%rd111, %rd109, 16;
	// begin inline asm
	{ atom.add.f64 %fd189,[%rd111],%fd206; }

	// end inline asm
	add.s64 	%rd112, %rd109, 24;
	// begin inline asm
	{ atom.add.f64 %fd191,[%rd112],%fd206; }

	// end inline asm
	add.s64 	%rd113, %rd109, 32;
	// begin inline asm
	{ atom.add.f64 %fd193,[%rd113],%fd206; }

	// end inline asm
	add.s64 	%rd114, %rd109, 40;
	// begin inline asm
	{ atom.add.f64 %fd195,[%rd114],%fd206; }

	// end inline asm
	add.s64 	%rd115, %rd109, 48;
	// begin inline asm
	{ atom.add.f64 %fd197,[%rd115],%fd206; }

	// end inline asm
	add.s64 	%rd116, %rd109, 56;
	// begin inline asm
	{ atom.add.f64 %fd199,[%rd116],%fd206; }

	// end inline asm
	add.s64 	%rd117, %rd109, 64;
	// begin inline asm
	{ atom.add.f64 %fd201,[%rd117],%fd206; }

	// end inline asm
	add.s64 	%rd118, %rd109, 72;
	// begin inline asm
	{ atom.add.f64 %fd203,[%rd118],%fd206; }

	// end inline asm
	add.s64 	%rd119, %rd109, 80;
	// begin inline asm
	{ atom.add.f64 %fd205,[%rd119],%fd206; }

	// end inline asm
	add.s64 	%rd120, %rd109, 88;
	// begin inline asm
	{ atom.add.f64 %fd207,[%rd120],%fd1400; }

	// end inline asm
	bra.uni 	$L__BB3_27;

$L__BB3_25:
	setp.eq.s64 	%p15, %rd50, 0;
	@%p15 bra 	$L__BB3_27;

	add.f64 	%fd1401, %fd135, 0d0000000000000000;
	add.s64 	%rd122, %rd50, %rd43;
	mov.f64 	%fd230, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd209,[%rd122],%fd230; }

	// end inline asm
	add.s64 	%rd123, %rd122, 8;
	// begin inline asm
	{ atom.add.f64 %fd211,[%rd123],%fd230; }

	// end inline asm
	add.s64 	%rd124, %rd122, 16;
	// begin inline asm
	{ atom.add.f64 %fd213,[%rd124],%fd230; }

	// end inline asm
	add.s64 	%rd125, %rd122, 24;
	// begin inline asm
	{ atom.add.f64 %fd215,[%rd125],%fd230; }

	// end inline asm
	add.s64 	%rd126, %rd122, 32;
	// begin inline asm
	{ atom.add.f64 %fd217,[%rd126],%fd230; }

	// end inline asm
	add.s64 	%rd127, %rd122, 40;
	// begin inline asm
	{ atom.add.f64 %fd219,[%rd127],%fd230; }

	// end inline asm
	add.s64 	%rd128, %rd122, 48;
	// begin inline asm
	{ atom.add.f64 %fd221,[%rd128],%fd230; }

	// end inline asm
	add.s64 	%rd129, %rd122, 56;
	// begin inline asm
	{ atom.add.f64 %fd223,[%rd129],%fd230; }

	// end inline asm
	add.s64 	%rd130, %rd122, 64;
	// begin inline asm
	{ atom.add.f64 %fd225,[%rd130],%fd230; }

	// end inline asm
	add.s64 	%rd131, %rd122, 72;
	// begin inline asm
	{ atom.add.f64 %fd227,[%rd131],%fd230; }

	// end inline asm
	add.s64 	%rd132, %rd122, 80;
	// begin inline asm
	{ atom.add.f64 %fd229,[%rd132],%fd230; }

	// end inline asm
	add.s64 	%rd133, %rd122, 88;
	// begin inline asm
	{ atom.add.f64 %fd231,[%rd133],%fd1401; }

	// end inline asm

$L__BB3_27:
	fma.rn.f64 	%fd22, %fd1, %fd19, 0d0000000000000000;
	@%p10 bra 	$L__BB3_29;

	ld.global.f64 	%fd233, [%rd45];
	add.f64 	%fd1469, %fd233, 0d0000000000000000;
	bra.uni 	$L__BB3_31;

$L__BB3_29:
	setp.eq.s64 	%p17, %rd56, 0;
	mov.f64 	%fd1469, 0d0000000000000000;
	@%p17 bra 	$L__BB3_31;

	ld.global.f64 	%fd235, [%rd46];
	add.f64 	%fd1469, %fd235, 0d0000000000000000;

$L__BB3_31:
	mul.f64 	%fd236, %fd13, %fd13;
	fma.rn.f64 	%fd26, %fd236, %fd1469, 0d0000000000000000;
	mov.f64 	%fd237, 0d0000000000000000;
	fma.rn.f64 	%fd238, %fd2, %fd1469, 0d0000000000000000;
	fma.rn.f64 	%fd239, %fd13, %fd238, 0d0000000000000000;
	fma.rn.f64 	%fd240, %fd13, %fd238, %fd239;
	@%p12 bra 	$L__BB3_33;

	mov.f64 	%fd1446, 0d0000000000000000;
	sub.f64 	%fd1393, %fd1446, %fd240;
	add.f64 	%fd1392, %fd1393, 0d0000000000000000;
	mul.lo.s64 	%rd146, %rd41, %rd28;
	add.s64 	%rd134, %rd61, %rd146;
	// begin inline asm
	{ atom.add.f64 %fd242,[%rd134],%fd1446; }

	// end inline asm
	add.s64 	%rd135, %rd134, 8;
	// begin inline asm
	{ atom.add.f64 %fd244,[%rd135],%fd1446; }

	// end inline asm
	add.s64 	%rd136, %rd134, 16;
	// begin inline asm
	{ atom.add.f64 %fd246,[%rd136],%fd1446; }

	// end inline asm
	add.s64 	%rd137, %rd134, 24;
	// begin inline asm
	{ atom.add.f64 %fd248,[%rd137],%fd1446; }

	// end inline asm
	add.s64 	%rd138, %rd134, 32;
	// begin inline asm
	{ atom.add.f64 %fd250,[%rd138],%fd1446; }

	// end inline asm
	add.s64 	%rd139, %rd134, 40;
	// begin inline asm
	{ atom.add.f64 %fd252,[%rd139],%fd1446; }

	// end inline asm
	add.s64 	%rd140, %rd134, 48;
	// begin inline asm
	{ atom.add.f64 %fd254,[%rd140],%fd1446; }

	// end inline asm
	add.s64 	%rd141, %rd134, 56;
	// begin inline asm
	{ atom.add.f64 %fd256,[%rd141],%fd1446; }

	// end inline asm
	add.s64 	%rd142, %rd134, 64;
	// begin inline asm
	{ atom.add.f64 %fd258,[%rd142],%fd1446; }

	// end inline asm
	add.s64 	%rd143, %rd134, 72;
	// begin inline asm
	{ atom.add.f64 %fd260,[%rd143],%fd1446; }

	// end inline asm
	add.s64 	%rd144, %rd134, 80;
	// begin inline asm
	{ atom.add.f64 %fd262,[%rd144],%fd1392; }

	// end inline asm
	add.s64 	%rd145, %rd134, 88;
	// begin inline asm
	{ atom.add.f64 %fd264,[%rd145],%fd1446; }

	// end inline asm
	bra.uni 	$L__BB3_35;

$L__BB3_33:
	setp.eq.s64 	%p19, %rd54, 0;
	@%p19 bra 	$L__BB3_35;

	sub.f64 	%fd1397, %fd237, %fd240;
	add.f64 	%fd1396, %fd1397, 0d0000000000000000;
	add.s64 	%rd147, %rd54, %rd44;
	mov.f64 	%fd289, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd266,[%rd147],%fd289; }

	// end inline asm
	add.s64 	%rd148, %rd147, 8;
	// begin inline asm
	{ atom.add.f64 %fd268,[%rd148],%fd289; }

	// end inline asm
	add.s64 	%rd149, %rd147, 16;
	// begin inline asm
	{ atom.add.f64 %fd270,[%rd149],%fd289; }

	// end inline asm
	add.s64 	%rd150, %rd147, 24;
	// begin inline asm
	{ atom.add.f64 %fd272,[%rd150],%fd289; }

	// end inline asm
	add.s64 	%rd151, %rd147, 32;
	// begin inline asm
	{ atom.add.f64 %fd274,[%rd151],%fd289; }

	// end inline asm
	add.s64 	%rd152, %rd147, 40;
	// begin inline asm
	{ atom.add.f64 %fd276,[%rd152],%fd289; }

	// end inline asm
	add.s64 	%rd153, %rd147, 48;
	// begin inline asm
	{ atom.add.f64 %fd278,[%rd153],%fd289; }

	// end inline asm
	add.s64 	%rd154, %rd147, 56;
	// begin inline asm
	{ atom.add.f64 %fd280,[%rd154],%fd289; }

	// end inline asm
	add.s64 	%rd155, %rd147, 64;
	// begin inline asm
	{ atom.add.f64 %fd282,[%rd155],%fd289; }

	// end inline asm
	add.s64 	%rd156, %rd147, 72;
	// begin inline asm
	{ atom.add.f64 %fd284,[%rd156],%fd289; }

	// end inline asm
	add.s64 	%rd157, %rd147, 80;
	// begin inline asm
	{ atom.add.f64 %fd286,[%rd157],%fd1396; }

	// end inline asm
	add.s64 	%rd158, %rd147, 88;
	// begin inline asm
	{ atom.add.f64 %fd288,[%rd158],%fd289; }

	// end inline asm

$L__BB3_35:
	@%p14 bra 	$L__BB3_37;

	add.f64 	%fd1394, %fd240, 0d0000000000000000;
	mul.lo.s64 	%rd171, %rd41, %rd29;
	add.s64 	%rd159, %rd59, %rd171;
	mov.f64 	%fd313, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd290,[%rd159],%fd313; }

	// end inline asm
	add.s64 	%rd160, %rd159, 8;
	// begin inline asm
	{ atom.add.f64 %fd292,[%rd160],%fd313; }

	// end inline asm
	add.s64 	%rd161, %rd159, 16;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd161],%fd313; }

	// end inline asm
	add.s64 	%rd162, %rd159, 24;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd162],%fd313; }

	// end inline asm
	add.s64 	%rd163, %rd159, 32;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd163],%fd313; }

	// end inline asm
	add.s64 	%rd164, %rd159, 40;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd164],%fd313; }

	// end inline asm
	add.s64 	%rd165, %rd159, 48;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd165],%fd313; }

	// end inline asm
	add.s64 	%rd166, %rd159, 56;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd166],%fd313; }

	// end inline asm
	add.s64 	%rd167, %rd159, 64;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd167],%fd313; }

	// end inline asm
	add.s64 	%rd168, %rd159, 72;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd168],%fd313; }

	// end inline asm
	add.s64 	%rd169, %rd159, 80;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd169],%fd1394; }

	// end inline asm
	add.s64 	%rd170, %rd159, 88;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd170],%fd313; }

	// end inline asm
	bra.uni 	$L__BB3_39;

$L__BB3_37:
	setp.eq.s64 	%p21, %rd50, 0;
	@%p21 bra 	$L__BB3_39;

	add.f64 	%fd1395, %fd240, 0d0000000000000000;
	add.s64 	%rd172, %rd50, %rd43;
	mov.f64 	%fd337, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd314,[%rd172],%fd337; }

	// end inline asm
	add.s64 	%rd173, %rd172, 8;
	// begin inline asm
	{ atom.add.f64 %fd316,[%rd173],%fd337; }

	// end inline asm
	add.s64 	%rd174, %rd172, 16;
	// begin inline asm
	{ atom.add.f64 %fd318,[%rd174],%fd337; }

	// end inline asm
	add.s64 	%rd175, %rd172, 24;
	// begin inline asm
	{ atom.add.f64 %fd320,[%rd175],%fd337; }

	// end inline asm
	add.s64 	%rd176, %rd172, 32;
	// begin inline asm
	{ atom.add.f64 %fd322,[%rd176],%fd337; }

	// end inline asm
	add.s64 	%rd177, %rd172, 40;
	// begin inline asm
	{ atom.add.f64 %fd324,[%rd177],%fd337; }

	// end inline asm
	add.s64 	%rd178, %rd172, 48;
	// begin inline asm
	{ atom.add.f64 %fd326,[%rd178],%fd337; }

	// end inline asm
	add.s64 	%rd179, %rd172, 56;
	// begin inline asm
	{ atom.add.f64 %fd328,[%rd179],%fd337; }

	// end inline asm
	add.s64 	%rd180, %rd172, 64;
	// begin inline asm
	{ atom.add.f64 %fd330,[%rd180],%fd337; }

	// end inline asm
	add.s64 	%rd181, %rd172, 72;
	// begin inline asm
	{ atom.add.f64 %fd332,[%rd181],%fd337; }

	// end inline asm
	add.s64 	%rd182, %rd172, 80;
	// begin inline asm
	{ atom.add.f64 %fd334,[%rd182],%fd1395; }

	// end inline asm
	add.s64 	%rd183, %rd172, 88;
	// begin inline asm
	{ atom.add.f64 %fd336,[%rd183],%fd337; }

	// end inline asm

$L__BB3_39:
	fma.rn.f64 	%fd29, %fd1, %fd26, %fd22;
	@%p10 bra 	$L__BB3_41;

	ld.global.f64 	%fd338, [%rd45];
	add.f64 	%fd1470, %fd338, 0d0000000000000000;
	bra.uni 	$L__BB3_43;

$L__BB3_41:
	setp.eq.s64 	%p23, %rd56, 0;
	mov.f64 	%fd1470, 0d0000000000000000;
	@%p23 bra 	$L__BB3_43;

	ld.global.f64 	%fd340, [%rd46];
	add.f64 	%fd1470, %fd340, 0d0000000000000000;

$L__BB3_43:
	mul.f64 	%fd341, %fd12, %fd12;
	fma.rn.f64 	%fd33, %fd341, %fd1470, 0d0000000000000000;
	mov.f64 	%fd342, 0d0000000000000000;
	fma.rn.f64 	%fd343, %fd2, %fd1470, 0d0000000000000000;
	fma.rn.f64 	%fd344, %fd12, %fd343, 0d0000000000000000;
	fma.rn.f64 	%fd345, %fd12, %fd343, %fd344;
	@%p12 bra 	$L__BB3_45;

	mov.f64 	%fd1448, 0d0000000000000000;
	sub.f64 	%fd1405, %fd1448, %fd345;
	add.f64 	%fd1404, %fd1405, 0d0000000000000000;
	mul.lo.s64 	%rd196, %rd41, %rd28;
	add.s64 	%rd184, %rd61, %rd196;
	// begin inline asm
	{ atom.add.f64 %fd347,[%rd184],%fd1448; }

	// end inline asm
	add.s64 	%rd185, %rd184, 8;
	// begin inline asm
	{ atom.add.f64 %fd349,[%rd185],%fd1448; }

	// end inline asm
	add.s64 	%rd186, %rd184, 16;
	// begin inline asm
	{ atom.add.f64 %fd351,[%rd186],%fd1448; }

	// end inline asm
	add.s64 	%rd187, %rd184, 24;
	// begin inline asm
	{ atom.add.f64 %fd353,[%rd187],%fd1448; }

	// end inline asm
	add.s64 	%rd188, %rd184, 32;
	// begin inline asm
	{ atom.add.f64 %fd355,[%rd188],%fd1448; }

	// end inline asm
	add.s64 	%rd189, %rd184, 40;
	// begin inline asm
	{ atom.add.f64 %fd357,[%rd189],%fd1448; }

	// end inline asm
	add.s64 	%rd190, %rd184, 48;
	// begin inline asm
	{ atom.add.f64 %fd359,[%rd190],%fd1448; }

	// end inline asm
	add.s64 	%rd191, %rd184, 56;
	// begin inline asm
	{ atom.add.f64 %fd361,[%rd191],%fd1448; }

	// end inline asm
	add.s64 	%rd192, %rd184, 64;
	// begin inline asm
	{ atom.add.f64 %fd363,[%rd192],%fd1448; }

	// end inline asm
	add.s64 	%rd193, %rd184, 72;
	// begin inline asm
	{ atom.add.f64 %fd365,[%rd193],%fd1404; }

	// end inline asm
	add.s64 	%rd194, %rd184, 80;
	// begin inline asm
	{ atom.add.f64 %fd367,[%rd194],%fd1448; }

	// end inline asm
	add.s64 	%rd195, %rd184, 88;
	// begin inline asm
	{ atom.add.f64 %fd369,[%rd195],%fd1448; }

	// end inline asm
	bra.uni 	$L__BB3_47;

$L__BB3_45:
	setp.eq.s64 	%p25, %rd54, 0;
	@%p25 bra 	$L__BB3_47;

	sub.f64 	%fd1409, %fd342, %fd345;
	add.f64 	%fd1408, %fd1409, 0d0000000000000000;
	add.s64 	%rd197, %rd54, %rd44;
	mov.f64 	%fd394, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd371,[%rd197],%fd394; }

	// end inline asm
	add.s64 	%rd198, %rd197, 8;
	// begin inline asm
	{ atom.add.f64 %fd373,[%rd198],%fd394; }

	// end inline asm
	add.s64 	%rd199, %rd197, 16;
	// begin inline asm
	{ atom.add.f64 %fd375,[%rd199],%fd394; }

	// end inline asm
	add.s64 	%rd200, %rd197, 24;
	// begin inline asm
	{ atom.add.f64 %fd377,[%rd200],%fd394; }

	// end inline asm
	add.s64 	%rd201, %rd197, 32;
	// begin inline asm
	{ atom.add.f64 %fd379,[%rd201],%fd394; }

	// end inline asm
	add.s64 	%rd202, %rd197, 40;
	// begin inline asm
	{ atom.add.f64 %fd381,[%rd202],%fd394; }

	// end inline asm
	add.s64 	%rd203, %rd197, 48;
	// begin inline asm
	{ atom.add.f64 %fd383,[%rd203],%fd394; }

	// end inline asm
	add.s64 	%rd204, %rd197, 56;
	// begin inline asm
	{ atom.add.f64 %fd385,[%rd204],%fd394; }

	// end inline asm
	add.s64 	%rd205, %rd197, 64;
	// begin inline asm
	{ atom.add.f64 %fd387,[%rd205],%fd394; }

	// end inline asm
	add.s64 	%rd206, %rd197, 72;
	// begin inline asm
	{ atom.add.f64 %fd389,[%rd206],%fd1408; }

	// end inline asm
	add.s64 	%rd207, %rd197, 80;
	// begin inline asm
	{ atom.add.f64 %fd391,[%rd207],%fd394; }

	// end inline asm
	add.s64 	%rd208, %rd197, 88;
	// begin inline asm
	{ atom.add.f64 %fd393,[%rd208],%fd394; }

	// end inline asm

$L__BB3_47:
	@%p14 bra 	$L__BB3_49;

	add.f64 	%fd1406, %fd345, 0d0000000000000000;
	mul.lo.s64 	%rd221, %rd41, %rd29;
	add.s64 	%rd209, %rd59, %rd221;
	mov.f64 	%fd418, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd395,[%rd209],%fd418; }

	// end inline asm
	add.s64 	%rd210, %rd209, 8;
	// begin inline asm
	{ atom.add.f64 %fd397,[%rd210],%fd418; }

	// end inline asm
	add.s64 	%rd211, %rd209, 16;
	// begin inline asm
	{ atom.add.f64 %fd399,[%rd211],%fd418; }

	// end inline asm
	add.s64 	%rd212, %rd209, 24;
	// begin inline asm
	{ atom.add.f64 %fd401,[%rd212],%fd418; }

	// end inline asm
	add.s64 	%rd213, %rd209, 32;
	// begin inline asm
	{ atom.add.f64 %fd403,[%rd213],%fd418; }

	// end inline asm
	add.s64 	%rd214, %rd209, 40;
	// begin inline asm
	{ atom.add.f64 %fd405,[%rd214],%fd418; }

	// end inline asm
	add.s64 	%rd215, %rd209, 48;
	// begin inline asm
	{ atom.add.f64 %fd407,[%rd215],%fd418; }

	// end inline asm
	add.s64 	%rd216, %rd209, 56;
	// begin inline asm
	{ atom.add.f64 %fd409,[%rd216],%fd418; }

	// end inline asm
	add.s64 	%rd217, %rd209, 64;
	// begin inline asm
	{ atom.add.f64 %fd411,[%rd217],%fd418; }

	// end inline asm
	add.s64 	%rd218, %rd209, 72;
	// begin inline asm
	{ atom.add.f64 %fd413,[%rd218],%fd1406; }

	// end inline asm
	add.s64 	%rd219, %rd209, 80;
	// begin inline asm
	{ atom.add.f64 %fd415,[%rd219],%fd418; }

	// end inline asm
	add.s64 	%rd220, %rd209, 88;
	// begin inline asm
	{ atom.add.f64 %fd417,[%rd220],%fd418; }

	// end inline asm
	bra.uni 	$L__BB3_51;

$L__BB3_49:
	setp.eq.s64 	%p27, %rd50, 0;
	@%p27 bra 	$L__BB3_51;

	add.f64 	%fd1407, %fd345, 0d0000000000000000;
	add.s64 	%rd222, %rd50, %rd43;
	mov.f64 	%fd442, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd419,[%rd222],%fd442; }

	// end inline asm
	add.s64 	%rd223, %rd222, 8;
	// begin inline asm
	{ atom.add.f64 %fd421,[%rd223],%fd442; }

	// end inline asm
	add.s64 	%rd224, %rd222, 16;
	// begin inline asm
	{ atom.add.f64 %fd423,[%rd224],%fd442; }

	// end inline asm
	add.s64 	%rd225, %rd222, 24;
	// begin inline asm
	{ atom.add.f64 %fd425,[%rd225],%fd442; }

	// end inline asm
	add.s64 	%rd226, %rd222, 32;
	// begin inline asm
	{ atom.add.f64 %fd427,[%rd226],%fd442; }

	// end inline asm
	add.s64 	%rd227, %rd222, 40;
	// begin inline asm
	{ atom.add.f64 %fd429,[%rd227],%fd442; }

	// end inline asm
	add.s64 	%rd228, %rd222, 48;
	// begin inline asm
	{ atom.add.f64 %fd431,[%rd228],%fd442; }

	// end inline asm
	add.s64 	%rd229, %rd222, 56;
	// begin inline asm
	{ atom.add.f64 %fd433,[%rd229],%fd442; }

	// end inline asm
	add.s64 	%rd230, %rd222, 64;
	// begin inline asm
	{ atom.add.f64 %fd435,[%rd230],%fd442; }

	// end inline asm
	add.s64 	%rd231, %rd222, 72;
	// begin inline asm
	{ atom.add.f64 %fd437,[%rd231],%fd1407; }

	// end inline asm
	add.s64 	%rd232, %rd222, 80;
	// begin inline asm
	{ atom.add.f64 %fd439,[%rd232],%fd442; }

	// end inline asm
	add.s64 	%rd233, %rd222, 88;
	// begin inline asm
	{ atom.add.f64 %fd441,[%rd233],%fd442; }

	// end inline asm

$L__BB3_51:
	fma.rn.f64 	%fd36, %fd1, %fd33, %fd29;
	@%p10 bra 	$L__BB3_53;

	ld.global.f64 	%fd443, [%rd45];
	add.f64 	%fd1471, %fd443, 0d0000000000000000;
	bra.uni 	$L__BB3_55;

$L__BB3_53:
	setp.eq.s64 	%p29, %rd56, 0;
	mov.f64 	%fd1471, 0d0000000000000000;
	@%p29 bra 	$L__BB3_55;

	ld.global.f64 	%fd445, [%rd46];
	add.f64 	%fd1471, %fd445, 0d0000000000000000;

$L__BB3_55:
	mul.f64 	%fd446, %fd11, %fd11;
	fma.rn.f64 	%fd40, %fd446, %fd1471, 0d0000000000000000;
	mov.f64 	%fd447, 0d0000000000000000;
	fma.rn.f64 	%fd448, %fd2, %fd1471, 0d0000000000000000;
	fma.rn.f64 	%fd449, %fd11, %fd448, 0d0000000000000000;
	fma.rn.f64 	%fd450, %fd11, %fd448, %fd449;
	@%p12 bra 	$L__BB3_57;

	mov.f64 	%fd1449, 0d0000000000000000;
	sub.f64 	%fd1411, %fd1449, %fd450;
	add.f64 	%fd1410, %fd1411, 0d0000000000000000;
	mul.lo.s64 	%rd246, %rd41, %rd28;
	add.s64 	%rd234, %rd61, %rd246;
	// begin inline asm
	{ atom.add.f64 %fd452,[%rd234],%fd1449; }

	// end inline asm
	add.s64 	%rd235, %rd234, 8;
	// begin inline asm
	{ atom.add.f64 %fd454,[%rd235],%fd1449; }

	// end inline asm
	add.s64 	%rd236, %rd234, 16;
	// begin inline asm
	{ atom.add.f64 %fd456,[%rd236],%fd1449; }

	// end inline asm
	add.s64 	%rd237, %rd234, 24;
	// begin inline asm
	{ atom.add.f64 %fd458,[%rd237],%fd1449; }

	// end inline asm
	add.s64 	%rd238, %rd234, 32;
	// begin inline asm
	{ atom.add.f64 %fd460,[%rd238],%fd1449; }

	// end inline asm
	add.s64 	%rd239, %rd234, 40;
	// begin inline asm
	{ atom.add.f64 %fd462,[%rd239],%fd1449; }

	// end inline asm
	add.s64 	%rd240, %rd234, 48;
	// begin inline asm
	{ atom.add.f64 %fd464,[%rd240],%fd1449; }

	// end inline asm
	add.s64 	%rd241, %rd234, 56;
	// begin inline asm
	{ atom.add.f64 %fd466,[%rd241],%fd1449; }

	// end inline asm
	add.s64 	%rd242, %rd234, 64;
	// begin inline asm
	{ atom.add.f64 %fd468,[%rd242],%fd1410; }

	// end inline asm
	add.s64 	%rd243, %rd234, 72;
	// begin inline asm
	{ atom.add.f64 %fd470,[%rd243],%fd1449; }

	// end inline asm
	add.s64 	%rd244, %rd234, 80;
	// begin inline asm
	{ atom.add.f64 %fd472,[%rd244],%fd1449; }

	// end inline asm
	add.s64 	%rd245, %rd234, 88;
	// begin inline asm
	{ atom.add.f64 %fd474,[%rd245],%fd1449; }

	// end inline asm
	bra.uni 	$L__BB3_59;

$L__BB3_57:
	setp.eq.s64 	%p31, %rd54, 0;
	@%p31 bra 	$L__BB3_59;

	sub.f64 	%fd1415, %fd447, %fd450;
	add.f64 	%fd1414, %fd1415, 0d0000000000000000;
	add.s64 	%rd247, %rd54, %rd44;
	mov.f64 	%fd499, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd476,[%rd247],%fd499; }

	// end inline asm
	add.s64 	%rd248, %rd247, 8;
	// begin inline asm
	{ atom.add.f64 %fd478,[%rd248],%fd499; }

	// end inline asm
	add.s64 	%rd249, %rd247, 16;
	// begin inline asm
	{ atom.add.f64 %fd480,[%rd249],%fd499; }

	// end inline asm
	add.s64 	%rd250, %rd247, 24;
	// begin inline asm
	{ atom.add.f64 %fd482,[%rd250],%fd499; }

	// end inline asm
	add.s64 	%rd251, %rd247, 32;
	// begin inline asm
	{ atom.add.f64 %fd484,[%rd251],%fd499; }

	// end inline asm
	add.s64 	%rd252, %rd247, 40;
	// begin inline asm
	{ atom.add.f64 %fd486,[%rd252],%fd499; }

	// end inline asm
	add.s64 	%rd253, %rd247, 48;
	// begin inline asm
	{ atom.add.f64 %fd488,[%rd253],%fd499; }

	// end inline asm
	add.s64 	%rd254, %rd247, 56;
	// begin inline asm
	{ atom.add.f64 %fd490,[%rd254],%fd499; }

	// end inline asm
	add.s64 	%rd255, %rd247, 64;
	// begin inline asm
	{ atom.add.f64 %fd492,[%rd255],%fd1414; }

	// end inline asm
	add.s64 	%rd256, %rd247, 72;
	// begin inline asm
	{ atom.add.f64 %fd494,[%rd256],%fd499; }

	// end inline asm
	add.s64 	%rd257, %rd247, 80;
	// begin inline asm
	{ atom.add.f64 %fd496,[%rd257],%fd499; }

	// end inline asm
	add.s64 	%rd258, %rd247, 88;
	// begin inline asm
	{ atom.add.f64 %fd498,[%rd258],%fd499; }

	// end inline asm

$L__BB3_59:
	@%p14 bra 	$L__BB3_61;

	add.f64 	%fd1412, %fd450, 0d0000000000000000;
	mul.lo.s64 	%rd271, %rd41, %rd29;
	add.s64 	%rd259, %rd59, %rd271;
	mov.f64 	%fd523, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd500,[%rd259],%fd523; }

	// end inline asm
	add.s64 	%rd260, %rd259, 8;
	// begin inline asm
	{ atom.add.f64 %fd502,[%rd260],%fd523; }

	// end inline asm
	add.s64 	%rd261, %rd259, 16;
	// begin inline asm
	{ atom.add.f64 %fd504,[%rd261],%fd523; }

	// end inline asm
	add.s64 	%rd262, %rd259, 24;
	// begin inline asm
	{ atom.add.f64 %fd506,[%rd262],%fd523; }

	// end inline asm
	add.s64 	%rd263, %rd259, 32;
	// begin inline asm
	{ atom.add.f64 %fd508,[%rd263],%fd523; }

	// end inline asm
	add.s64 	%rd264, %rd259, 40;
	// begin inline asm
	{ atom.add.f64 %fd510,[%rd264],%fd523; }

	// end inline asm
	add.s64 	%rd265, %rd259, 48;
	// begin inline asm
	{ atom.add.f64 %fd512,[%rd265],%fd523; }

	// end inline asm
	add.s64 	%rd266, %rd259, 56;
	// begin inline asm
	{ atom.add.f64 %fd514,[%rd266],%fd523; }

	// end inline asm
	add.s64 	%rd267, %rd259, 64;
	// begin inline asm
	{ atom.add.f64 %fd516,[%rd267],%fd1412; }

	// end inline asm
	add.s64 	%rd268, %rd259, 72;
	// begin inline asm
	{ atom.add.f64 %fd518,[%rd268],%fd523; }

	// end inline asm
	add.s64 	%rd269, %rd259, 80;
	// begin inline asm
	{ atom.add.f64 %fd520,[%rd269],%fd523; }

	// end inline asm
	add.s64 	%rd270, %rd259, 88;
	// begin inline asm
	{ atom.add.f64 %fd522,[%rd270],%fd523; }

	// end inline asm
	bra.uni 	$L__BB3_63;

$L__BB3_61:
	setp.eq.s64 	%p33, %rd50, 0;
	@%p33 bra 	$L__BB3_63;

	add.f64 	%fd1413, %fd450, 0d0000000000000000;
	add.s64 	%rd272, %rd50, %rd43;
	mov.f64 	%fd547, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd524,[%rd272],%fd547; }

	// end inline asm
	add.s64 	%rd273, %rd272, 8;
	// begin inline asm
	{ atom.add.f64 %fd526,[%rd273],%fd547; }

	// end inline asm
	add.s64 	%rd274, %rd272, 16;
	// begin inline asm
	{ atom.add.f64 %fd528,[%rd274],%fd547; }

	// end inline asm
	add.s64 	%rd275, %rd272, 24;
	// begin inline asm
	{ atom.add.f64 %fd530,[%rd275],%fd547; }

	// end inline asm
	add.s64 	%rd276, %rd272, 32;
	// begin inline asm
	{ atom.add.f64 %fd532,[%rd276],%fd547; }

	// end inline asm
	add.s64 	%rd277, %rd272, 40;
	// begin inline asm
	{ atom.add.f64 %fd534,[%rd277],%fd547; }

	// end inline asm
	add.s64 	%rd278, %rd272, 48;
	// begin inline asm
	{ atom.add.f64 %fd536,[%rd278],%fd547; }

	// end inline asm
	add.s64 	%rd279, %rd272, 56;
	// begin inline asm
	{ atom.add.f64 %fd538,[%rd279],%fd547; }

	// end inline asm
	add.s64 	%rd280, %rd272, 64;
	// begin inline asm
	{ atom.add.f64 %fd540,[%rd280],%fd1413; }

	// end inline asm
	add.s64 	%rd281, %rd272, 72;
	// begin inline asm
	{ atom.add.f64 %fd542,[%rd281],%fd547; }

	// end inline asm
	add.s64 	%rd282, %rd272, 80;
	// begin inline asm
	{ atom.add.f64 %fd544,[%rd282],%fd547; }

	// end inline asm
	add.s64 	%rd283, %rd272, 88;
	// begin inline asm
	{ atom.add.f64 %fd546,[%rd283],%fd547; }

	// end inline asm

$L__BB3_63:
	fma.rn.f64 	%fd43, %fd1, %fd40, %fd36;
	@%p10 bra 	$L__BB3_65;

	ld.global.f64 	%fd548, [%rd45];
	add.f64 	%fd1472, %fd548, 0d0000000000000000;
	bra.uni 	$L__BB3_67;

$L__BB3_65:
	setp.eq.s64 	%p35, %rd56, 0;
	mov.f64 	%fd1472, 0d0000000000000000;
	@%p35 bra 	$L__BB3_67;

	ld.global.f64 	%fd550, [%rd46];
	add.f64 	%fd1472, %fd550, 0d0000000000000000;

$L__BB3_67:
	mul.f64 	%fd551, %fd10, %fd10;
	fma.rn.f64 	%fd47, %fd551, %fd1472, 0d0000000000000000;
	mov.f64 	%fd552, 0d0000000000000000;
	fma.rn.f64 	%fd553, %fd2, %fd1472, 0d0000000000000000;
	fma.rn.f64 	%fd554, %fd10, %fd553, 0d0000000000000000;
	fma.rn.f64 	%fd555, %fd10, %fd553, %fd554;
	@%p12 bra 	$L__BB3_69;

	mov.f64 	%fd1450, 0d0000000000000000;
	sub.f64 	%fd1417, %fd1450, %fd555;
	add.f64 	%fd1416, %fd1417, 0d0000000000000000;
	mul.lo.s64 	%rd296, %rd41, %rd28;
	add.s64 	%rd284, %rd61, %rd296;
	// begin inline asm
	{ atom.add.f64 %fd557,[%rd284],%fd1450; }

	// end inline asm
	add.s64 	%rd285, %rd284, 8;
	// begin inline asm
	{ atom.add.f64 %fd559,[%rd285],%fd1450; }

	// end inline asm
	add.s64 	%rd286, %rd284, 16;
	// begin inline asm
	{ atom.add.f64 %fd561,[%rd286],%fd1450; }

	// end inline asm
	add.s64 	%rd287, %rd284, 24;
	// begin inline asm
	{ atom.add.f64 %fd563,[%rd287],%fd1450; }

	// end inline asm
	add.s64 	%rd288, %rd284, 32;
	// begin inline asm
	{ atom.add.f64 %fd565,[%rd288],%fd1450; }

	// end inline asm
	add.s64 	%rd289, %rd284, 40;
	// begin inline asm
	{ atom.add.f64 %fd567,[%rd289],%fd1450; }

	// end inline asm
	add.s64 	%rd290, %rd284, 48;
	// begin inline asm
	{ atom.add.f64 %fd569,[%rd290],%fd1450; }

	// end inline asm
	add.s64 	%rd291, %rd284, 56;
	// begin inline asm
	{ atom.add.f64 %fd571,[%rd291],%fd1416; }

	// end inline asm
	add.s64 	%rd292, %rd284, 64;
	// begin inline asm
	{ atom.add.f64 %fd573,[%rd292],%fd1450; }

	// end inline asm
	add.s64 	%rd293, %rd284, 72;
	// begin inline asm
	{ atom.add.f64 %fd575,[%rd293],%fd1450; }

	// end inline asm
	add.s64 	%rd294, %rd284, 80;
	// begin inline asm
	{ atom.add.f64 %fd577,[%rd294],%fd1450; }

	// end inline asm
	add.s64 	%rd295, %rd284, 88;
	// begin inline asm
	{ atom.add.f64 %fd579,[%rd295],%fd1450; }

	// end inline asm
	bra.uni 	$L__BB3_71;

$L__BB3_69:
	setp.eq.s64 	%p37, %rd54, 0;
	@%p37 bra 	$L__BB3_71;

	sub.f64 	%fd1421, %fd552, %fd555;
	add.f64 	%fd1420, %fd1421, 0d0000000000000000;
	add.s64 	%rd297, %rd54, %rd44;
	mov.f64 	%fd604, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd581,[%rd297],%fd604; }

	// end inline asm
	add.s64 	%rd298, %rd297, 8;
	// begin inline asm
	{ atom.add.f64 %fd583,[%rd298],%fd604; }

	// end inline asm
	add.s64 	%rd299, %rd297, 16;
	// begin inline asm
	{ atom.add.f64 %fd585,[%rd299],%fd604; }

	// end inline asm
	add.s64 	%rd300, %rd297, 24;
	// begin inline asm
	{ atom.add.f64 %fd587,[%rd300],%fd604; }

	// end inline asm
	add.s64 	%rd301, %rd297, 32;
	// begin inline asm
	{ atom.add.f64 %fd589,[%rd301],%fd604; }

	// end inline asm
	add.s64 	%rd302, %rd297, 40;
	// begin inline asm
	{ atom.add.f64 %fd591,[%rd302],%fd604; }

	// end inline asm
	add.s64 	%rd303, %rd297, 48;
	// begin inline asm
	{ atom.add.f64 %fd593,[%rd303],%fd604; }

	// end inline asm
	add.s64 	%rd304, %rd297, 56;
	// begin inline asm
	{ atom.add.f64 %fd595,[%rd304],%fd1420; }

	// end inline asm
	add.s64 	%rd305, %rd297, 64;
	// begin inline asm
	{ atom.add.f64 %fd597,[%rd305],%fd604; }

	// end inline asm
	add.s64 	%rd306, %rd297, 72;
	// begin inline asm
	{ atom.add.f64 %fd599,[%rd306],%fd604; }

	// end inline asm
	add.s64 	%rd307, %rd297, 80;
	// begin inline asm
	{ atom.add.f64 %fd601,[%rd307],%fd604; }

	// end inline asm
	add.s64 	%rd308, %rd297, 88;
	// begin inline asm
	{ atom.add.f64 %fd603,[%rd308],%fd604; }

	// end inline asm

$L__BB3_71:
	@%p14 bra 	$L__BB3_73;

	add.f64 	%fd1418, %fd555, 0d0000000000000000;
	mul.lo.s64 	%rd321, %rd41, %rd29;
	add.s64 	%rd309, %rd59, %rd321;
	mov.f64 	%fd628, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd605,[%rd309],%fd628; }

	// end inline asm
	add.s64 	%rd310, %rd309, 8;
	// begin inline asm
	{ atom.add.f64 %fd607,[%rd310],%fd628; }

	// end inline asm
	add.s64 	%rd311, %rd309, 16;
	// begin inline asm
	{ atom.add.f64 %fd609,[%rd311],%fd628; }

	// end inline asm
	add.s64 	%rd312, %rd309, 24;
	// begin inline asm
	{ atom.add.f64 %fd611,[%rd312],%fd628; }

	// end inline asm
	add.s64 	%rd313, %rd309, 32;
	// begin inline asm
	{ atom.add.f64 %fd613,[%rd313],%fd628; }

	// end inline asm
	add.s64 	%rd314, %rd309, 40;
	// begin inline asm
	{ atom.add.f64 %fd615,[%rd314],%fd628; }

	// end inline asm
	add.s64 	%rd315, %rd309, 48;
	// begin inline asm
	{ atom.add.f64 %fd617,[%rd315],%fd628; }

	// end inline asm
	add.s64 	%rd316, %rd309, 56;
	// begin inline asm
	{ atom.add.f64 %fd619,[%rd316],%fd1418; }

	// end inline asm
	add.s64 	%rd317, %rd309, 64;
	// begin inline asm
	{ atom.add.f64 %fd621,[%rd317],%fd628; }

	// end inline asm
	add.s64 	%rd318, %rd309, 72;
	// begin inline asm
	{ atom.add.f64 %fd623,[%rd318],%fd628; }

	// end inline asm
	add.s64 	%rd319, %rd309, 80;
	// begin inline asm
	{ atom.add.f64 %fd625,[%rd319],%fd628; }

	// end inline asm
	add.s64 	%rd320, %rd309, 88;
	// begin inline asm
	{ atom.add.f64 %fd627,[%rd320],%fd628; }

	// end inline asm
	bra.uni 	$L__BB3_75;

$L__BB3_73:
	setp.eq.s64 	%p39, %rd50, 0;
	@%p39 bra 	$L__BB3_75;

	add.f64 	%fd1419, %fd555, 0d0000000000000000;
	add.s64 	%rd322, %rd50, %rd43;
	mov.f64 	%fd652, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd629,[%rd322],%fd652; }

	// end inline asm
	add.s64 	%rd323, %rd322, 8;
	// begin inline asm
	{ atom.add.f64 %fd631,[%rd323],%fd652; }

	// end inline asm
	add.s64 	%rd324, %rd322, 16;
	// begin inline asm
	{ atom.add.f64 %fd633,[%rd324],%fd652; }

	// end inline asm
	add.s64 	%rd325, %rd322, 24;
	// begin inline asm
	{ atom.add.f64 %fd635,[%rd325],%fd652; }

	// end inline asm
	add.s64 	%rd326, %rd322, 32;
	// begin inline asm
	{ atom.add.f64 %fd637,[%rd326],%fd652; }

	// end inline asm
	add.s64 	%rd327, %rd322, 40;
	// begin inline asm
	{ atom.add.f64 %fd639,[%rd327],%fd652; }

	// end inline asm
	add.s64 	%rd328, %rd322, 48;
	// begin inline asm
	{ atom.add.f64 %fd641,[%rd328],%fd652; }

	// end inline asm
	add.s64 	%rd329, %rd322, 56;
	// begin inline asm
	{ atom.add.f64 %fd643,[%rd329],%fd1419; }

	// end inline asm
	add.s64 	%rd330, %rd322, 64;
	// begin inline asm
	{ atom.add.f64 %fd645,[%rd330],%fd652; }

	// end inline asm
	add.s64 	%rd331, %rd322, 72;
	// begin inline asm
	{ atom.add.f64 %fd647,[%rd331],%fd652; }

	// end inline asm
	add.s64 	%rd332, %rd322, 80;
	// begin inline asm
	{ atom.add.f64 %fd649,[%rd332],%fd652; }

	// end inline asm
	add.s64 	%rd333, %rd322, 88;
	// begin inline asm
	{ atom.add.f64 %fd651,[%rd333],%fd652; }

	// end inline asm

$L__BB3_75:
	fma.rn.f64 	%fd50, %fd1, %fd47, %fd43;
	@%p10 bra 	$L__BB3_77;

	ld.global.f64 	%fd653, [%rd45];
	add.f64 	%fd1473, %fd653, 0d0000000000000000;
	bra.uni 	$L__BB3_79;

$L__BB3_77:
	setp.eq.s64 	%p41, %rd56, 0;
	mov.f64 	%fd1473, 0d0000000000000000;
	@%p41 bra 	$L__BB3_79;

	ld.global.f64 	%fd655, [%rd46];
	add.f64 	%fd1473, %fd655, 0d0000000000000000;

$L__BB3_79:
	mul.f64 	%fd656, %fd9, %fd9;
	fma.rn.f64 	%fd54, %fd656, %fd1473, 0d0000000000000000;
	mov.f64 	%fd657, 0d0000000000000000;
	fma.rn.f64 	%fd658, %fd2, %fd1473, 0d0000000000000000;
	fma.rn.f64 	%fd659, %fd9, %fd658, 0d0000000000000000;
	fma.rn.f64 	%fd660, %fd9, %fd658, %fd659;
	@%p12 bra 	$L__BB3_81;

	mov.f64 	%fd1451, 0d0000000000000000;
	sub.f64 	%fd1423, %fd1451, %fd660;
	add.f64 	%fd1422, %fd1423, 0d0000000000000000;
	mul.lo.s64 	%rd346, %rd41, %rd28;
	add.s64 	%rd334, %rd61, %rd346;
	// begin inline asm
	{ atom.add.f64 %fd662,[%rd334],%fd1451; }

	// end inline asm
	add.s64 	%rd335, %rd334, 8;
	// begin inline asm
	{ atom.add.f64 %fd664,[%rd335],%fd1451; }

	// end inline asm
	add.s64 	%rd336, %rd334, 16;
	// begin inline asm
	{ atom.add.f64 %fd666,[%rd336],%fd1451; }

	// end inline asm
	add.s64 	%rd337, %rd334, 24;
	// begin inline asm
	{ atom.add.f64 %fd668,[%rd337],%fd1451; }

	// end inline asm
	add.s64 	%rd338, %rd334, 32;
	// begin inline asm
	{ atom.add.f64 %fd670,[%rd338],%fd1451; }

	// end inline asm
	add.s64 	%rd339, %rd334, 40;
	// begin inline asm
	{ atom.add.f64 %fd672,[%rd339],%fd1451; }

	// end inline asm
	add.s64 	%rd340, %rd334, 48;
	// begin inline asm
	{ atom.add.f64 %fd674,[%rd340],%fd1422; }

	// end inline asm
	add.s64 	%rd341, %rd334, 56;
	// begin inline asm
	{ atom.add.f64 %fd676,[%rd341],%fd1451; }

	// end inline asm
	add.s64 	%rd342, %rd334, 64;
	// begin inline asm
	{ atom.add.f64 %fd678,[%rd342],%fd1451; }

	// end inline asm
	add.s64 	%rd343, %rd334, 72;
	// begin inline asm
	{ atom.add.f64 %fd680,[%rd343],%fd1451; }

	// end inline asm
	add.s64 	%rd344, %rd334, 80;
	// begin inline asm
	{ atom.add.f64 %fd682,[%rd344],%fd1451; }

	// end inline asm
	add.s64 	%rd345, %rd334, 88;
	// begin inline asm
	{ atom.add.f64 %fd684,[%rd345],%fd1451; }

	// end inline asm
	bra.uni 	$L__BB3_83;

$L__BB3_81:
	setp.eq.s64 	%p43, %rd54, 0;
	@%p43 bra 	$L__BB3_83;

	sub.f64 	%fd1427, %fd657, %fd660;
	add.f64 	%fd1426, %fd1427, 0d0000000000000000;
	add.s64 	%rd347, %rd54, %rd44;
	mov.f64 	%fd709, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd686,[%rd347],%fd709; }

	// end inline asm
	add.s64 	%rd348, %rd347, 8;
	// begin inline asm
	{ atom.add.f64 %fd688,[%rd348],%fd709; }

	// end inline asm
	add.s64 	%rd349, %rd347, 16;
	// begin inline asm
	{ atom.add.f64 %fd690,[%rd349],%fd709; }

	// end inline asm
	add.s64 	%rd350, %rd347, 24;
	// begin inline asm
	{ atom.add.f64 %fd692,[%rd350],%fd709; }

	// end inline asm
	add.s64 	%rd351, %rd347, 32;
	// begin inline asm
	{ atom.add.f64 %fd694,[%rd351],%fd709; }

	// end inline asm
	add.s64 	%rd352, %rd347, 40;
	// begin inline asm
	{ atom.add.f64 %fd696,[%rd352],%fd709; }

	// end inline asm
	add.s64 	%rd353, %rd347, 48;
	// begin inline asm
	{ atom.add.f64 %fd698,[%rd353],%fd1426; }

	// end inline asm
	add.s64 	%rd354, %rd347, 56;
	// begin inline asm
	{ atom.add.f64 %fd700,[%rd354],%fd709; }

	// end inline asm
	add.s64 	%rd355, %rd347, 64;
	// begin inline asm
	{ atom.add.f64 %fd702,[%rd355],%fd709; }

	// end inline asm
	add.s64 	%rd356, %rd347, 72;
	// begin inline asm
	{ atom.add.f64 %fd704,[%rd356],%fd709; }

	// end inline asm
	add.s64 	%rd357, %rd347, 80;
	// begin inline asm
	{ atom.add.f64 %fd706,[%rd357],%fd709; }

	// end inline asm
	add.s64 	%rd358, %rd347, 88;
	// begin inline asm
	{ atom.add.f64 %fd708,[%rd358],%fd709; }

	// end inline asm

$L__BB3_83:
	@%p14 bra 	$L__BB3_85;

	add.f64 	%fd1424, %fd660, 0d0000000000000000;
	mul.lo.s64 	%rd371, %rd41, %rd29;
	add.s64 	%rd359, %rd59, %rd371;
	mov.f64 	%fd733, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd710,[%rd359],%fd733; }

	// end inline asm
	add.s64 	%rd360, %rd359, 8;
	// begin inline asm
	{ atom.add.f64 %fd712,[%rd360],%fd733; }

	// end inline asm
	add.s64 	%rd361, %rd359, 16;
	// begin inline asm
	{ atom.add.f64 %fd714,[%rd361],%fd733; }

	// end inline asm
	add.s64 	%rd362, %rd359, 24;
	// begin inline asm
	{ atom.add.f64 %fd716,[%rd362],%fd733; }

	// end inline asm
	add.s64 	%rd363, %rd359, 32;
	// begin inline asm
	{ atom.add.f64 %fd718,[%rd363],%fd733; }

	// end inline asm
	add.s64 	%rd364, %rd359, 40;
	// begin inline asm
	{ atom.add.f64 %fd720,[%rd364],%fd733; }

	// end inline asm
	add.s64 	%rd365, %rd359, 48;
	// begin inline asm
	{ atom.add.f64 %fd722,[%rd365],%fd1424; }

	// end inline asm
	add.s64 	%rd366, %rd359, 56;
	// begin inline asm
	{ atom.add.f64 %fd724,[%rd366],%fd733; }

	// end inline asm
	add.s64 	%rd367, %rd359, 64;
	// begin inline asm
	{ atom.add.f64 %fd726,[%rd367],%fd733; }

	// end inline asm
	add.s64 	%rd368, %rd359, 72;
	// begin inline asm
	{ atom.add.f64 %fd728,[%rd368],%fd733; }

	// end inline asm
	add.s64 	%rd369, %rd359, 80;
	// begin inline asm
	{ atom.add.f64 %fd730,[%rd369],%fd733; }

	// end inline asm
	add.s64 	%rd370, %rd359, 88;
	// begin inline asm
	{ atom.add.f64 %fd732,[%rd370],%fd733; }

	// end inline asm
	bra.uni 	$L__BB3_87;

$L__BB3_85:
	setp.eq.s64 	%p45, %rd50, 0;
	@%p45 bra 	$L__BB3_87;

	add.f64 	%fd1425, %fd660, 0d0000000000000000;
	add.s64 	%rd372, %rd50, %rd43;
	mov.f64 	%fd757, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd734,[%rd372],%fd757; }

	// end inline asm
	add.s64 	%rd373, %rd372, 8;
	// begin inline asm
	{ atom.add.f64 %fd736,[%rd373],%fd757; }

	// end inline asm
	add.s64 	%rd374, %rd372, 16;
	// begin inline asm
	{ atom.add.f64 %fd738,[%rd374],%fd757; }

	// end inline asm
	add.s64 	%rd375, %rd372, 24;
	// begin inline asm
	{ atom.add.f64 %fd740,[%rd375],%fd757; }

	// end inline asm
	add.s64 	%rd376, %rd372, 32;
	// begin inline asm
	{ atom.add.f64 %fd742,[%rd376],%fd757; }

	// end inline asm
	add.s64 	%rd377, %rd372, 40;
	// begin inline asm
	{ atom.add.f64 %fd744,[%rd377],%fd757; }

	// end inline asm
	add.s64 	%rd378, %rd372, 48;
	// begin inline asm
	{ atom.add.f64 %fd746,[%rd378],%fd1425; }

	// end inline asm
	add.s64 	%rd379, %rd372, 56;
	// begin inline asm
	{ atom.add.f64 %fd748,[%rd379],%fd757; }

	// end inline asm
	add.s64 	%rd380, %rd372, 64;
	// begin inline asm
	{ atom.add.f64 %fd750,[%rd380],%fd757; }

	// end inline asm
	add.s64 	%rd381, %rd372, 72;
	// begin inline asm
	{ atom.add.f64 %fd752,[%rd381],%fd757; }

	// end inline asm
	add.s64 	%rd382, %rd372, 80;
	// begin inline asm
	{ atom.add.f64 %fd754,[%rd382],%fd757; }

	// end inline asm
	add.s64 	%rd383, %rd372, 88;
	// begin inline asm
	{ atom.add.f64 %fd756,[%rd383],%fd757; }

	// end inline asm

$L__BB3_87:
	fma.rn.f64 	%fd57, %fd1, %fd54, %fd50;
	@%p10 bra 	$L__BB3_89;

	ld.global.f64 	%fd758, [%rd45];
	add.f64 	%fd1474, %fd758, 0d0000000000000000;
	bra.uni 	$L__BB3_91;

$L__BB3_89:
	setp.eq.s64 	%p47, %rd56, 0;
	mov.f64 	%fd1474, 0d0000000000000000;
	@%p47 bra 	$L__BB3_91;

	ld.global.f64 	%fd760, [%rd46];
	add.f64 	%fd1474, %fd760, 0d0000000000000000;

$L__BB3_91:
	mul.f64 	%fd761, %fd8, %fd8;
	fma.rn.f64 	%fd61, %fd761, %fd1474, 0d0000000000000000;
	mov.f64 	%fd762, 0d0000000000000000;
	fma.rn.f64 	%fd763, %fd2, %fd1474, 0d0000000000000000;
	fma.rn.f64 	%fd764, %fd8, %fd763, 0d0000000000000000;
	fma.rn.f64 	%fd765, %fd8, %fd763, %fd764;
	@%p12 bra 	$L__BB3_93;

	mov.f64 	%fd1452, 0d0000000000000000;
	sub.f64 	%fd1429, %fd1452, %fd765;
	add.f64 	%fd1428, %fd1429, 0d0000000000000000;
	mul.lo.s64 	%rd396, %rd41, %rd28;
	add.s64 	%rd384, %rd61, %rd396;
	// begin inline asm
	{ atom.add.f64 %fd767,[%rd384],%fd1452; }

	// end inline asm
	add.s64 	%rd385, %rd384, 8;
	// begin inline asm
	{ atom.add.f64 %fd769,[%rd385],%fd1452; }

	// end inline asm
	add.s64 	%rd386, %rd384, 16;
	// begin inline asm
	{ atom.add.f64 %fd771,[%rd386],%fd1452; }

	// end inline asm
	add.s64 	%rd387, %rd384, 24;
	// begin inline asm
	{ atom.add.f64 %fd773,[%rd387],%fd1452; }

	// end inline asm
	add.s64 	%rd388, %rd384, 32;
	// begin inline asm
	{ atom.add.f64 %fd775,[%rd388],%fd1452; }

	// end inline asm
	add.s64 	%rd389, %rd384, 40;
	// begin inline asm
	{ atom.add.f64 %fd777,[%rd389],%fd1428; }

	// end inline asm
	add.s64 	%rd390, %rd384, 48;
	// begin inline asm
	{ atom.add.f64 %fd779,[%rd390],%fd1452; }

	// end inline asm
	add.s64 	%rd391, %rd384, 56;
	// begin inline asm
	{ atom.add.f64 %fd781,[%rd391],%fd1452; }

	// end inline asm
	add.s64 	%rd392, %rd384, 64;
	// begin inline asm
	{ atom.add.f64 %fd783,[%rd392],%fd1452; }

	// end inline asm
	add.s64 	%rd393, %rd384, 72;
	// begin inline asm
	{ atom.add.f64 %fd785,[%rd393],%fd1452; }

	// end inline asm
	add.s64 	%rd394, %rd384, 80;
	// begin inline asm
	{ atom.add.f64 %fd787,[%rd394],%fd1452; }

	// end inline asm
	add.s64 	%rd395, %rd384, 88;
	// begin inline asm
	{ atom.add.f64 %fd789,[%rd395],%fd1452; }

	// end inline asm
	bra.uni 	$L__BB3_95;

$L__BB3_93:
	setp.eq.s64 	%p49, %rd54, 0;
	@%p49 bra 	$L__BB3_95;

	sub.f64 	%fd1433, %fd762, %fd765;
	add.f64 	%fd1432, %fd1433, 0d0000000000000000;
	add.s64 	%rd397, %rd54, %rd44;
	mov.f64 	%fd814, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd791,[%rd397],%fd814; }

	// end inline asm
	add.s64 	%rd398, %rd397, 8;
	// begin inline asm
	{ atom.add.f64 %fd793,[%rd398],%fd814; }

	// end inline asm
	add.s64 	%rd399, %rd397, 16;
	// begin inline asm
	{ atom.add.f64 %fd795,[%rd399],%fd814; }

	// end inline asm
	add.s64 	%rd400, %rd397, 24;
	// begin inline asm
	{ atom.add.f64 %fd797,[%rd400],%fd814; }

	// end inline asm
	add.s64 	%rd401, %rd397, 32;
	// begin inline asm
	{ atom.add.f64 %fd799,[%rd401],%fd814; }

	// end inline asm
	add.s64 	%rd402, %rd397, 40;
	// begin inline asm
	{ atom.add.f64 %fd801,[%rd402],%fd1432; }

	// end inline asm
	add.s64 	%rd403, %rd397, 48;
	// begin inline asm
	{ atom.add.f64 %fd803,[%rd403],%fd814; }

	// end inline asm
	add.s64 	%rd404, %rd397, 56;
	// begin inline asm
	{ atom.add.f64 %fd805,[%rd404],%fd814; }

	// end inline asm
	add.s64 	%rd405, %rd397, 64;
	// begin inline asm
	{ atom.add.f64 %fd807,[%rd405],%fd814; }

	// end inline asm
	add.s64 	%rd406, %rd397, 72;
	// begin inline asm
	{ atom.add.f64 %fd809,[%rd406],%fd814; }

	// end inline asm
	add.s64 	%rd407, %rd397, 80;
	// begin inline asm
	{ atom.add.f64 %fd811,[%rd407],%fd814; }

	// end inline asm
	add.s64 	%rd408, %rd397, 88;
	// begin inline asm
	{ atom.add.f64 %fd813,[%rd408],%fd814; }

	// end inline asm

$L__BB3_95:
	@%p14 bra 	$L__BB3_97;

	add.f64 	%fd1430, %fd765, 0d0000000000000000;
	mul.lo.s64 	%rd421, %rd41, %rd29;
	add.s64 	%rd409, %rd59, %rd421;
	mov.f64 	%fd838, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd815,[%rd409],%fd838; }

	// end inline asm
	add.s64 	%rd410, %rd409, 8;
	// begin inline asm
	{ atom.add.f64 %fd817,[%rd410],%fd838; }

	// end inline asm
	add.s64 	%rd411, %rd409, 16;
	// begin inline asm
	{ atom.add.f64 %fd819,[%rd411],%fd838; }

	// end inline asm
	add.s64 	%rd412, %rd409, 24;
	// begin inline asm
	{ atom.add.f64 %fd821,[%rd412],%fd838; }

	// end inline asm
	add.s64 	%rd413, %rd409, 32;
	// begin inline asm
	{ atom.add.f64 %fd823,[%rd413],%fd838; }

	// end inline asm
	add.s64 	%rd414, %rd409, 40;
	// begin inline asm
	{ atom.add.f64 %fd825,[%rd414],%fd1430; }

	// end inline asm
	add.s64 	%rd415, %rd409, 48;
	// begin inline asm
	{ atom.add.f64 %fd827,[%rd415],%fd838; }

	// end inline asm
	add.s64 	%rd416, %rd409, 56;
	// begin inline asm
	{ atom.add.f64 %fd829,[%rd416],%fd838; }

	// end inline asm
	add.s64 	%rd417, %rd409, 64;
	// begin inline asm
	{ atom.add.f64 %fd831,[%rd417],%fd838; }

	// end inline asm
	add.s64 	%rd418, %rd409, 72;
	// begin inline asm
	{ atom.add.f64 %fd833,[%rd418],%fd838; }

	// end inline asm
	add.s64 	%rd419, %rd409, 80;
	// begin inline asm
	{ atom.add.f64 %fd835,[%rd419],%fd838; }

	// end inline asm
	add.s64 	%rd420, %rd409, 88;
	// begin inline asm
	{ atom.add.f64 %fd837,[%rd420],%fd838; }

	// end inline asm
	bra.uni 	$L__BB3_99;

$L__BB3_97:
	setp.eq.s64 	%p51, %rd50, 0;
	@%p51 bra 	$L__BB3_99;

	add.f64 	%fd1431, %fd765, 0d0000000000000000;
	add.s64 	%rd422, %rd50, %rd43;
	mov.f64 	%fd862, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd839,[%rd422],%fd862; }

	// end inline asm
	add.s64 	%rd423, %rd422, 8;
	// begin inline asm
	{ atom.add.f64 %fd841,[%rd423],%fd862; }

	// end inline asm
	add.s64 	%rd424, %rd422, 16;
	// begin inline asm
	{ atom.add.f64 %fd843,[%rd424],%fd862; }

	// end inline asm
	add.s64 	%rd425, %rd422, 24;
	// begin inline asm
	{ atom.add.f64 %fd845,[%rd425],%fd862; }

	// end inline asm
	add.s64 	%rd426, %rd422, 32;
	// begin inline asm
	{ atom.add.f64 %fd847,[%rd426],%fd862; }

	// end inline asm
	add.s64 	%rd427, %rd422, 40;
	// begin inline asm
	{ atom.add.f64 %fd849,[%rd427],%fd1431; }

	// end inline asm
	add.s64 	%rd428, %rd422, 48;
	// begin inline asm
	{ atom.add.f64 %fd851,[%rd428],%fd862; }

	// end inline asm
	add.s64 	%rd429, %rd422, 56;
	// begin inline asm
	{ atom.add.f64 %fd853,[%rd429],%fd862; }

	// end inline asm
	add.s64 	%rd430, %rd422, 64;
	// begin inline asm
	{ atom.add.f64 %fd855,[%rd430],%fd862; }

	// end inline asm
	add.s64 	%rd431, %rd422, 72;
	// begin inline asm
	{ atom.add.f64 %fd857,[%rd431],%fd862; }

	// end inline asm
	add.s64 	%rd432, %rd422, 80;
	// begin inline asm
	{ atom.add.f64 %fd859,[%rd432],%fd862; }

	// end inline asm
	add.s64 	%rd433, %rd422, 88;
	// begin inline asm
	{ atom.add.f64 %fd861,[%rd433],%fd862; }

	// end inline asm

$L__BB3_99:
	fma.rn.f64 	%fd64, %fd1, %fd61, %fd57;
	@%p10 bra 	$L__BB3_101;

	ld.global.f64 	%fd863, [%rd45];
	add.f64 	%fd1475, %fd863, 0d0000000000000000;
	bra.uni 	$L__BB3_103;

$L__BB3_101:
	setp.eq.s64 	%p53, %rd56, 0;
	mov.f64 	%fd1475, 0d0000000000000000;
	@%p53 bra 	$L__BB3_103;

	ld.global.f64 	%fd865, [%rd46];
	add.f64 	%fd1475, %fd865, 0d0000000000000000;

$L__BB3_103:
	mul.f64 	%fd866, %fd7, %fd7;
	fma.rn.f64 	%fd68, %fd866, %fd1475, 0d0000000000000000;
	mov.f64 	%fd867, 0d0000000000000000;
	fma.rn.f64 	%fd868, %fd2, %fd1475, 0d0000000000000000;
	fma.rn.f64 	%fd869, %fd7, %fd868, 0d0000000000000000;
	fma.rn.f64 	%fd870, %fd7, %fd868, %fd869;
	@%p12 bra 	$L__BB3_105;

	mov.f64 	%fd1453, 0d0000000000000000;
	sub.f64 	%fd1435, %fd1453, %fd870;
	add.f64 	%fd1434, %fd1435, 0d0000000000000000;
	mul.lo.s64 	%rd446, %rd41, %rd28;
	add.s64 	%rd434, %rd61, %rd446;
	// begin inline asm
	{ atom.add.f64 %fd872,[%rd434],%fd1453; }

	// end inline asm
	add.s64 	%rd435, %rd434, 8;
	// begin inline asm
	{ atom.add.f64 %fd874,[%rd435],%fd1453; }

	// end inline asm
	add.s64 	%rd436, %rd434, 16;
	// begin inline asm
	{ atom.add.f64 %fd876,[%rd436],%fd1453; }

	// end inline asm
	add.s64 	%rd437, %rd434, 24;
	// begin inline asm
	{ atom.add.f64 %fd878,[%rd437],%fd1453; }

	// end inline asm
	add.s64 	%rd438, %rd434, 32;
	// begin inline asm
	{ atom.add.f64 %fd880,[%rd438],%fd1434; }

	// end inline asm
	add.s64 	%rd439, %rd434, 40;
	// begin inline asm
	{ atom.add.f64 %fd882,[%rd439],%fd1453; }

	// end inline asm
	add.s64 	%rd440, %rd434, 48;
	// begin inline asm
	{ atom.add.f64 %fd884,[%rd440],%fd1453; }

	// end inline asm
	add.s64 	%rd441, %rd434, 56;
	// begin inline asm
	{ atom.add.f64 %fd886,[%rd441],%fd1453; }

	// end inline asm
	add.s64 	%rd442, %rd434, 64;
	// begin inline asm
	{ atom.add.f64 %fd888,[%rd442],%fd1453; }

	// end inline asm
	add.s64 	%rd443, %rd434, 72;
	// begin inline asm
	{ atom.add.f64 %fd890,[%rd443],%fd1453; }

	// end inline asm
	add.s64 	%rd444, %rd434, 80;
	// begin inline asm
	{ atom.add.f64 %fd892,[%rd444],%fd1453; }

	// end inline asm
	add.s64 	%rd445, %rd434, 88;
	// begin inline asm
	{ atom.add.f64 %fd894,[%rd445],%fd1453; }

	// end inline asm
	bra.uni 	$L__BB3_107;

$L__BB3_105:
	setp.eq.s64 	%p55, %rd54, 0;
	@%p55 bra 	$L__BB3_107;

	sub.f64 	%fd1439, %fd867, %fd870;
	add.f64 	%fd1438, %fd1439, 0d0000000000000000;
	add.s64 	%rd447, %rd54, %rd44;
	mov.f64 	%fd919, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd896,[%rd447],%fd919; }

	// end inline asm
	add.s64 	%rd448, %rd447, 8;
	// begin inline asm
	{ atom.add.f64 %fd898,[%rd448],%fd919; }

	// end inline asm
	add.s64 	%rd449, %rd447, 16;
	// begin inline asm
	{ atom.add.f64 %fd900,[%rd449],%fd919; }

	// end inline asm
	add.s64 	%rd450, %rd447, 24;
	// begin inline asm
	{ atom.add.f64 %fd902,[%rd450],%fd919; }

	// end inline asm
	add.s64 	%rd451, %rd447, 32;
	// begin inline asm
	{ atom.add.f64 %fd904,[%rd451],%fd1438; }

	// end inline asm
	add.s64 	%rd452, %rd447, 40;
	// begin inline asm
	{ atom.add.f64 %fd906,[%rd452],%fd919; }

	// end inline asm
	add.s64 	%rd453, %rd447, 48;
	// begin inline asm
	{ atom.add.f64 %fd908,[%rd453],%fd919; }

	// end inline asm
	add.s64 	%rd454, %rd447, 56;
	// begin inline asm
	{ atom.add.f64 %fd910,[%rd454],%fd919; }

	// end inline asm
	add.s64 	%rd455, %rd447, 64;
	// begin inline asm
	{ atom.add.f64 %fd912,[%rd455],%fd919; }

	// end inline asm
	add.s64 	%rd456, %rd447, 72;
	// begin inline asm
	{ atom.add.f64 %fd914,[%rd456],%fd919; }

	// end inline asm
	add.s64 	%rd457, %rd447, 80;
	// begin inline asm
	{ atom.add.f64 %fd916,[%rd457],%fd919; }

	// end inline asm
	add.s64 	%rd458, %rd447, 88;
	// begin inline asm
	{ atom.add.f64 %fd918,[%rd458],%fd919; }

	// end inline asm

$L__BB3_107:
	@%p14 bra 	$L__BB3_109;

	add.f64 	%fd1436, %fd870, 0d0000000000000000;
	mul.lo.s64 	%rd471, %rd41, %rd29;
	add.s64 	%rd459, %rd59, %rd471;
	mov.f64 	%fd943, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd920,[%rd459],%fd943; }

	// end inline asm
	add.s64 	%rd460, %rd459, 8;
	// begin inline asm
	{ atom.add.f64 %fd922,[%rd460],%fd943; }

	// end inline asm
	add.s64 	%rd461, %rd459, 16;
	// begin inline asm
	{ atom.add.f64 %fd924,[%rd461],%fd943; }

	// end inline asm
	add.s64 	%rd462, %rd459, 24;
	// begin inline asm
	{ atom.add.f64 %fd926,[%rd462],%fd943; }

	// end inline asm
	add.s64 	%rd463, %rd459, 32;
	// begin inline asm
	{ atom.add.f64 %fd928,[%rd463],%fd1436; }

	// end inline asm
	add.s64 	%rd464, %rd459, 40;
	// begin inline asm
	{ atom.add.f64 %fd930,[%rd464],%fd943; }

	// end inline asm
	add.s64 	%rd465, %rd459, 48;
	// begin inline asm
	{ atom.add.f64 %fd932,[%rd465],%fd943; }

	// end inline asm
	add.s64 	%rd466, %rd459, 56;
	// begin inline asm
	{ atom.add.f64 %fd934,[%rd466],%fd943; }

	// end inline asm
	add.s64 	%rd467, %rd459, 64;
	// begin inline asm
	{ atom.add.f64 %fd936,[%rd467],%fd943; }

	// end inline asm
	add.s64 	%rd468, %rd459, 72;
	// begin inline asm
	{ atom.add.f64 %fd938,[%rd468],%fd943; }

	// end inline asm
	add.s64 	%rd469, %rd459, 80;
	// begin inline asm
	{ atom.add.f64 %fd940,[%rd469],%fd943; }

	// end inline asm
	add.s64 	%rd470, %rd459, 88;
	// begin inline asm
	{ atom.add.f64 %fd942,[%rd470],%fd943; }

	// end inline asm
	bra.uni 	$L__BB3_111;

$L__BB3_109:
	setp.eq.s64 	%p57, %rd50, 0;
	@%p57 bra 	$L__BB3_111;

	add.f64 	%fd1437, %fd870, 0d0000000000000000;
	add.s64 	%rd472, %rd50, %rd43;
	mov.f64 	%fd967, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd944,[%rd472],%fd967; }

	// end inline asm
	add.s64 	%rd473, %rd472, 8;
	// begin inline asm
	{ atom.add.f64 %fd946,[%rd473],%fd967; }

	// end inline asm
	add.s64 	%rd474, %rd472, 16;
	// begin inline asm
	{ atom.add.f64 %fd948,[%rd474],%fd967; }

	// end inline asm
	add.s64 	%rd475, %rd472, 24;
	// begin inline asm
	{ atom.add.f64 %fd950,[%rd475],%fd967; }

	// end inline asm
	add.s64 	%rd476, %rd472, 32;
	// begin inline asm
	{ atom.add.f64 %fd952,[%rd476],%fd1437; }

	// end inline asm
	add.s64 	%rd477, %rd472, 40;
	// begin inline asm
	{ atom.add.f64 %fd954,[%rd477],%fd967; }

	// end inline asm
	add.s64 	%rd478, %rd472, 48;
	// begin inline asm
	{ atom.add.f64 %fd956,[%rd478],%fd967; }

	// end inline asm
	add.s64 	%rd479, %rd472, 56;
	// begin inline asm
	{ atom.add.f64 %fd958,[%rd479],%fd967; }

	// end inline asm
	add.s64 	%rd480, %rd472, 64;
	// begin inline asm
	{ atom.add.f64 %fd960,[%rd480],%fd967; }

	// end inline asm
	add.s64 	%rd481, %rd472, 72;
	// begin inline asm
	{ atom.add.f64 %fd962,[%rd481],%fd967; }

	// end inline asm
	add.s64 	%rd482, %rd472, 80;
	// begin inline asm
	{ atom.add.f64 %fd964,[%rd482],%fd967; }

	// end inline asm
	add.s64 	%rd483, %rd472, 88;
	// begin inline asm
	{ atom.add.f64 %fd966,[%rd483],%fd967; }

	// end inline asm

$L__BB3_111:
	fma.rn.f64 	%fd71, %fd1, %fd68, %fd64;
	@%p10 bra 	$L__BB3_113;

	ld.global.f64 	%fd968, [%rd45];
	add.f64 	%fd1476, %fd968, 0d0000000000000000;
	bra.uni 	$L__BB3_115;

$L__BB3_113:
	setp.eq.s64 	%p59, %rd56, 0;
	mov.f64 	%fd1476, 0d0000000000000000;
	@%p59 bra 	$L__BB3_115;

	ld.global.f64 	%fd970, [%rd46];
	add.f64 	%fd1476, %fd970, 0d0000000000000000;

$L__BB3_115:
	mul.f64 	%fd971, %fd6, %fd6;
	fma.rn.f64 	%fd75, %fd971, %fd1476, 0d0000000000000000;
	mov.f64 	%fd972, 0d0000000000000000;
	fma.rn.f64 	%fd973, %fd2, %fd1476, 0d0000000000000000;
	fma.rn.f64 	%fd974, %fd6, %fd973, 0d0000000000000000;
	fma.rn.f64 	%fd975, %fd6, %fd973, %fd974;
	@%p12 bra 	$L__BB3_117;

	mov.f64 	%fd1454, 0d0000000000000000;
	sub.f64 	%fd1441, %fd1454, %fd975;
	add.f64 	%fd1440, %fd1441, 0d0000000000000000;
	mul.lo.s64 	%rd496, %rd41, %rd28;
	add.s64 	%rd484, %rd61, %rd496;
	// begin inline asm
	{ atom.add.f64 %fd977,[%rd484],%fd1454; }

	// end inline asm
	add.s64 	%rd485, %rd484, 8;
	// begin inline asm
	{ atom.add.f64 %fd979,[%rd485],%fd1454; }

	// end inline asm
	add.s64 	%rd486, %rd484, 16;
	// begin inline asm
	{ atom.add.f64 %fd981,[%rd486],%fd1454; }

	// end inline asm
	add.s64 	%rd487, %rd484, 24;
	// begin inline asm
	{ atom.add.f64 %fd983,[%rd487],%fd1440; }

	// end inline asm
	add.s64 	%rd488, %rd484, 32;
	// begin inline asm
	{ atom.add.f64 %fd985,[%rd488],%fd1454; }

	// end inline asm
	add.s64 	%rd489, %rd484, 40;
	// begin inline asm
	{ atom.add.f64 %fd987,[%rd489],%fd1454; }

	// end inline asm
	add.s64 	%rd490, %rd484, 48;
	// begin inline asm
	{ atom.add.f64 %fd989,[%rd490],%fd1454; }

	// end inline asm
	add.s64 	%rd491, %rd484, 56;
	// begin inline asm
	{ atom.add.f64 %fd991,[%rd491],%fd1454; }

	// end inline asm
	add.s64 	%rd492, %rd484, 64;
	// begin inline asm
	{ atom.add.f64 %fd993,[%rd492],%fd1454; }

	// end inline asm
	add.s64 	%rd493, %rd484, 72;
	// begin inline asm
	{ atom.add.f64 %fd995,[%rd493],%fd1454; }

	// end inline asm
	add.s64 	%rd494, %rd484, 80;
	// begin inline asm
	{ atom.add.f64 %fd997,[%rd494],%fd1454; }

	// end inline asm
	add.s64 	%rd495, %rd484, 88;
	// begin inline asm
	{ atom.add.f64 %fd999,[%rd495],%fd1454; }

	// end inline asm
	bra.uni 	$L__BB3_119;

$L__BB3_117:
	setp.eq.s64 	%p61, %rd54, 0;
	@%p61 bra 	$L__BB3_119;

	sub.f64 	%fd1445, %fd972, %fd975;
	add.f64 	%fd1444, %fd1445, 0d0000000000000000;
	add.s64 	%rd497, %rd54, %rd44;
	mov.f64 	%fd1024, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1001,[%rd497],%fd1024; }

	// end inline asm
	add.s64 	%rd498, %rd497, 8;
	// begin inline asm
	{ atom.add.f64 %fd1003,[%rd498],%fd1024; }

	// end inline asm
	add.s64 	%rd499, %rd497, 16;
	// begin inline asm
	{ atom.add.f64 %fd1005,[%rd499],%fd1024; }

	// end inline asm
	add.s64 	%rd500, %rd497, 24;
	// begin inline asm
	{ atom.add.f64 %fd1007,[%rd500],%fd1444; }

	// end inline asm
	add.s64 	%rd501, %rd497, 32;
	// begin inline asm
	{ atom.add.f64 %fd1009,[%rd501],%fd1024; }

	// end inline asm
	add.s64 	%rd502, %rd497, 40;
	// begin inline asm
	{ atom.add.f64 %fd1011,[%rd502],%fd1024; }

	// end inline asm
	add.s64 	%rd503, %rd497, 48;
	// begin inline asm
	{ atom.add.f64 %fd1013,[%rd503],%fd1024; }

	// end inline asm
	add.s64 	%rd504, %rd497, 56;
	// begin inline asm
	{ atom.add.f64 %fd1015,[%rd504],%fd1024; }

	// end inline asm
	add.s64 	%rd505, %rd497, 64;
	// begin inline asm
	{ atom.add.f64 %fd1017,[%rd505],%fd1024; }

	// end inline asm
	add.s64 	%rd506, %rd497, 72;
	// begin inline asm
	{ atom.add.f64 %fd1019,[%rd506],%fd1024; }

	// end inline asm
	add.s64 	%rd507, %rd497, 80;
	// begin inline asm
	{ atom.add.f64 %fd1021,[%rd507],%fd1024; }

	// end inline asm
	add.s64 	%rd508, %rd497, 88;
	// begin inline asm
	{ atom.add.f64 %fd1023,[%rd508],%fd1024; }

	// end inline asm

$L__BB3_119:
	@%p14 bra 	$L__BB3_121;

	add.f64 	%fd1442, %fd975, 0d0000000000000000;
	mul.lo.s64 	%rd521, %rd41, %rd29;
	add.s64 	%rd509, %rd59, %rd521;
	mov.f64 	%fd1048, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1025,[%rd509],%fd1048; }

	// end inline asm
	add.s64 	%rd510, %rd509, 8;
	// begin inline asm
	{ atom.add.f64 %fd1027,[%rd510],%fd1048; }

	// end inline asm
	add.s64 	%rd511, %rd509, 16;
	// begin inline asm
	{ atom.add.f64 %fd1029,[%rd511],%fd1048; }

	// end inline asm
	add.s64 	%rd512, %rd509, 24;
	// begin inline asm
	{ atom.add.f64 %fd1031,[%rd512],%fd1442; }

	// end inline asm
	add.s64 	%rd513, %rd509, 32;
	// begin inline asm
	{ atom.add.f64 %fd1033,[%rd513],%fd1048; }

	// end inline asm
	add.s64 	%rd514, %rd509, 40;
	// begin inline asm
	{ atom.add.f64 %fd1035,[%rd514],%fd1048; }

	// end inline asm
	add.s64 	%rd515, %rd509, 48;
	// begin inline asm
	{ atom.add.f64 %fd1037,[%rd515],%fd1048; }

	// end inline asm
	add.s64 	%rd516, %rd509, 56;
	// begin inline asm
	{ atom.add.f64 %fd1039,[%rd516],%fd1048; }

	// end inline asm
	add.s64 	%rd517, %rd509, 64;
	// begin inline asm
	{ atom.add.f64 %fd1041,[%rd517],%fd1048; }

	// end inline asm
	add.s64 	%rd518, %rd509, 72;
	// begin inline asm
	{ atom.add.f64 %fd1043,[%rd518],%fd1048; }

	// end inline asm
	add.s64 	%rd519, %rd509, 80;
	// begin inline asm
	{ atom.add.f64 %fd1045,[%rd519],%fd1048; }

	// end inline asm
	add.s64 	%rd520, %rd509, 88;
	// begin inline asm
	{ atom.add.f64 %fd1047,[%rd520],%fd1048; }

	// end inline asm
	bra.uni 	$L__BB3_123;

$L__BB3_121:
	setp.eq.s64 	%p63, %rd50, 0;
	@%p63 bra 	$L__BB3_123;

	add.f64 	%fd1443, %fd975, 0d0000000000000000;
	add.s64 	%rd522, %rd50, %rd43;
	mov.f64 	%fd1072, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1049,[%rd522],%fd1072; }

	// end inline asm
	add.s64 	%rd523, %rd522, 8;
	// begin inline asm
	{ atom.add.f64 %fd1051,[%rd523],%fd1072; }

	// end inline asm
	add.s64 	%rd524, %rd522, 16;
	// begin inline asm
	{ atom.add.f64 %fd1053,[%rd524],%fd1072; }

	// end inline asm
	add.s64 	%rd525, %rd522, 24;
	// begin inline asm
	{ atom.add.f64 %fd1055,[%rd525],%fd1443; }

	// end inline asm
	add.s64 	%rd526, %rd522, 32;
	// begin inline asm
	{ atom.add.f64 %fd1057,[%rd526],%fd1072; }

	// end inline asm
	add.s64 	%rd527, %rd522, 40;
	// begin inline asm
	{ atom.add.f64 %fd1059,[%rd527],%fd1072; }

	// end inline asm
	add.s64 	%rd528, %rd522, 48;
	// begin inline asm
	{ atom.add.f64 %fd1061,[%rd528],%fd1072; }

	// end inline asm
	add.s64 	%rd529, %rd522, 56;
	// begin inline asm
	{ atom.add.f64 %fd1063,[%rd529],%fd1072; }

	// end inline asm
	add.s64 	%rd530, %rd522, 64;
	// begin inline asm
	{ atom.add.f64 %fd1065,[%rd530],%fd1072; }

	// end inline asm
	add.s64 	%rd531, %rd522, 72;
	// begin inline asm
	{ atom.add.f64 %fd1067,[%rd531],%fd1072; }

	// end inline asm
	add.s64 	%rd532, %rd522, 80;
	// begin inline asm
	{ atom.add.f64 %fd1069,[%rd532],%fd1072; }

	// end inline asm
	add.s64 	%rd533, %rd522, 88;
	// begin inline asm
	{ atom.add.f64 %fd1071,[%rd533],%fd1072; }

	// end inline asm

$L__BB3_123:
	fma.rn.f64 	%fd78, %fd1, %fd75, %fd71;
	@%p10 bra 	$L__BB3_125;

	ld.global.f64 	%fd1073, [%rd45];
	add.f64 	%fd1477, %fd1073, 0d0000000000000000;
	bra.uni 	$L__BB3_127;

$L__BB3_125:
	setp.eq.s64 	%p65, %rd56, 0;
	mov.f64 	%fd1477, 0d0000000000000000;
	@%p65 bra 	$L__BB3_127;

	ld.global.f64 	%fd1075, [%rd46];
	add.f64 	%fd1477, %fd1075, 0d0000000000000000;

$L__BB3_127:
	mul.f64 	%fd1076, %fd5, %fd5;
	fma.rn.f64 	%fd82, %fd1076, %fd1477, 0d0000000000000000;
	mov.f64 	%fd1077, 0d0000000000000000;
	fma.rn.f64 	%fd1078, %fd2, %fd1477, 0d0000000000000000;
	fma.rn.f64 	%fd1079, %fd5, %fd1078, 0d0000000000000000;
	fma.rn.f64 	%fd1080, %fd5, %fd1078, %fd1079;
	@%p12 bra 	$L__BB3_129;

	mov.f64 	%fd1467, 0d0000000000000000;
	sub.f64 	%fd1456, %fd1467, %fd1080;
	add.f64 	%fd1455, %fd1456, 0d0000000000000000;
	mul.lo.s64 	%rd546, %rd41, %rd28;
	add.s64 	%rd534, %rd61, %rd546;
	// begin inline asm
	{ atom.add.f64 %fd1082,[%rd534],%fd1467; }

	// end inline asm
	add.s64 	%rd535, %rd534, 8;
	// begin inline asm
	{ atom.add.f64 %fd1084,[%rd535],%fd1467; }

	// end inline asm
	add.s64 	%rd536, %rd534, 16;
	// begin inline asm
	{ atom.add.f64 %fd1086,[%rd536],%fd1455; }

	// end inline asm
	add.s64 	%rd537, %rd534, 24;
	// begin inline asm
	{ atom.add.f64 %fd1088,[%rd537],%fd1467; }

	// end inline asm
	add.s64 	%rd538, %rd534, 32;
	// begin inline asm
	{ atom.add.f64 %fd1090,[%rd538],%fd1467; }

	// end inline asm
	add.s64 	%rd539, %rd534, 40;
	// begin inline asm
	{ atom.add.f64 %fd1092,[%rd539],%fd1467; }

	// end inline asm
	add.s64 	%rd540, %rd534, 48;
	// begin inline asm
	{ atom.add.f64 %fd1094,[%rd540],%fd1467; }

	// end inline asm
	add.s64 	%rd541, %rd534, 56;
	// begin inline asm
	{ atom.add.f64 %fd1096,[%rd541],%fd1467; }

	// end inline asm
	add.s64 	%rd542, %rd534, 64;
	// begin inline asm
	{ atom.add.f64 %fd1098,[%rd542],%fd1467; }

	// end inline asm
	add.s64 	%rd543, %rd534, 72;
	// begin inline asm
	{ atom.add.f64 %fd1100,[%rd543],%fd1467; }

	// end inline asm
	add.s64 	%rd544, %rd534, 80;
	// begin inline asm
	{ atom.add.f64 %fd1102,[%rd544],%fd1467; }

	// end inline asm
	add.s64 	%rd545, %rd534, 88;
	// begin inline asm
	{ atom.add.f64 %fd1104,[%rd545],%fd1467; }

	// end inline asm
	bra.uni 	$L__BB3_131;

$L__BB3_129:
	setp.eq.s64 	%p67, %rd54, 0;
	@%p67 bra 	$L__BB3_131;

	sub.f64 	%fd1460, %fd1077, %fd1080;
	add.f64 	%fd1459, %fd1460, 0d0000000000000000;
	add.s64 	%rd547, %rd54, %rd44;
	mov.f64 	%fd1129, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1106,[%rd547],%fd1129; }

	// end inline asm
	add.s64 	%rd548, %rd547, 8;
	// begin inline asm
	{ atom.add.f64 %fd1108,[%rd548],%fd1129; }

	// end inline asm
	add.s64 	%rd549, %rd547, 16;
	// begin inline asm
	{ atom.add.f64 %fd1110,[%rd549],%fd1459; }

	// end inline asm
	add.s64 	%rd550, %rd547, 24;
	// begin inline asm
	{ atom.add.f64 %fd1112,[%rd550],%fd1129; }

	// end inline asm
	add.s64 	%rd551, %rd547, 32;
	// begin inline asm
	{ atom.add.f64 %fd1114,[%rd551],%fd1129; }

	// end inline asm
	add.s64 	%rd552, %rd547, 40;
	// begin inline asm
	{ atom.add.f64 %fd1116,[%rd552],%fd1129; }

	// end inline asm
	add.s64 	%rd553, %rd547, 48;
	// begin inline asm
	{ atom.add.f64 %fd1118,[%rd553],%fd1129; }

	// end inline asm
	add.s64 	%rd554, %rd547, 56;
	// begin inline asm
	{ atom.add.f64 %fd1120,[%rd554],%fd1129; }

	// end inline asm
	add.s64 	%rd555, %rd547, 64;
	// begin inline asm
	{ atom.add.f64 %fd1122,[%rd555],%fd1129; }

	// end inline asm
	add.s64 	%rd556, %rd547, 72;
	// begin inline asm
	{ atom.add.f64 %fd1124,[%rd556],%fd1129; }

	// end inline asm
	add.s64 	%rd557, %rd547, 80;
	// begin inline asm
	{ atom.add.f64 %fd1126,[%rd557],%fd1129; }

	// end inline asm
	add.s64 	%rd558, %rd547, 88;
	// begin inline asm
	{ atom.add.f64 %fd1128,[%rd558],%fd1129; }

	// end inline asm

$L__BB3_131:
	@%p14 bra 	$L__BB3_133;

	add.f64 	%fd1457, %fd1080, 0d0000000000000000;
	mul.lo.s64 	%rd571, %rd41, %rd29;
	add.s64 	%rd559, %rd59, %rd571;
	mov.f64 	%fd1153, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1130,[%rd559],%fd1153; }

	// end inline asm
	add.s64 	%rd560, %rd559, 8;
	// begin inline asm
	{ atom.add.f64 %fd1132,[%rd560],%fd1153; }

	// end inline asm
	add.s64 	%rd561, %rd559, 16;
	// begin inline asm
	{ atom.add.f64 %fd1134,[%rd561],%fd1457; }

	// end inline asm
	add.s64 	%rd562, %rd559, 24;
	// begin inline asm
	{ atom.add.f64 %fd1136,[%rd562],%fd1153; }

	// end inline asm
	add.s64 	%rd563, %rd559, 32;
	// begin inline asm
	{ atom.add.f64 %fd1138,[%rd563],%fd1153; }

	// end inline asm
	add.s64 	%rd564, %rd559, 40;
	// begin inline asm
	{ atom.add.f64 %fd1140,[%rd564],%fd1153; }

	// end inline asm
	add.s64 	%rd565, %rd559, 48;
	// begin inline asm
	{ atom.add.f64 %fd1142,[%rd565],%fd1153; }

	// end inline asm
	add.s64 	%rd566, %rd559, 56;
	// begin inline asm
	{ atom.add.f64 %fd1144,[%rd566],%fd1153; }

	// end inline asm
	add.s64 	%rd567, %rd559, 64;
	// begin inline asm
	{ atom.add.f64 %fd1146,[%rd567],%fd1153; }

	// end inline asm
	add.s64 	%rd568, %rd559, 72;
	// begin inline asm
	{ atom.add.f64 %fd1148,[%rd568],%fd1153; }

	// end inline asm
	add.s64 	%rd569, %rd559, 80;
	// begin inline asm
	{ atom.add.f64 %fd1150,[%rd569],%fd1153; }

	// end inline asm
	add.s64 	%rd570, %rd559, 88;
	// begin inline asm
	{ atom.add.f64 %fd1152,[%rd570],%fd1153; }

	// end inline asm
	bra.uni 	$L__BB3_135;

$L__BB3_133:
	setp.eq.s64 	%p69, %rd50, 0;
	@%p69 bra 	$L__BB3_135;

	add.f64 	%fd1458, %fd1080, 0d0000000000000000;
	add.s64 	%rd572, %rd50, %rd43;
	mov.f64 	%fd1177, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1154,[%rd572],%fd1177; }

	// end inline asm
	add.s64 	%rd573, %rd572, 8;
	// begin inline asm
	{ atom.add.f64 %fd1156,[%rd573],%fd1177; }

	// end inline asm
	add.s64 	%rd574, %rd572, 16;
	// begin inline asm
	{ atom.add.f64 %fd1158,[%rd574],%fd1458; }

	// end inline asm
	add.s64 	%rd575, %rd572, 24;
	// begin inline asm
	{ atom.add.f64 %fd1160,[%rd575],%fd1177; }

	// end inline asm
	add.s64 	%rd576, %rd572, 32;
	// begin inline asm
	{ atom.add.f64 %fd1162,[%rd576],%fd1177; }

	// end inline asm
	add.s64 	%rd577, %rd572, 40;
	// begin inline asm
	{ atom.add.f64 %fd1164,[%rd577],%fd1177; }

	// end inline asm
	add.s64 	%rd578, %rd572, 48;
	// begin inline asm
	{ atom.add.f64 %fd1166,[%rd578],%fd1177; }

	// end inline asm
	add.s64 	%rd579, %rd572, 56;
	// begin inline asm
	{ atom.add.f64 %fd1168,[%rd579],%fd1177; }

	// end inline asm
	add.s64 	%rd580, %rd572, 64;
	// begin inline asm
	{ atom.add.f64 %fd1170,[%rd580],%fd1177; }

	// end inline asm
	add.s64 	%rd581, %rd572, 72;
	// begin inline asm
	{ atom.add.f64 %fd1172,[%rd581],%fd1177; }

	// end inline asm
	add.s64 	%rd582, %rd572, 80;
	// begin inline asm
	{ atom.add.f64 %fd1174,[%rd582],%fd1177; }

	// end inline asm
	add.s64 	%rd583, %rd572, 88;
	// begin inline asm
	{ atom.add.f64 %fd1176,[%rd583],%fd1177; }

	// end inline asm

$L__BB3_135:
	fma.rn.f64 	%fd85, %fd1, %fd82, %fd78;
	@%p10 bra 	$L__BB3_137;

	ld.global.f64 	%fd1178, [%rd45];
	add.f64 	%fd1478, %fd1178, 0d0000000000000000;
	bra.uni 	$L__BB3_139;

$L__BB3_137:
	setp.eq.s64 	%p71, %rd56, 0;
	mov.f64 	%fd1478, 0d0000000000000000;
	@%p71 bra 	$L__BB3_139;

	ld.global.f64 	%fd1180, [%rd46];
	add.f64 	%fd1478, %fd1180, 0d0000000000000000;

$L__BB3_139:
	mul.f64 	%fd1181, %fd4, %fd4;
	fma.rn.f64 	%fd89, %fd1181, %fd1478, 0d0000000000000000;
	mov.f64 	%fd1182, 0d0000000000000000;
	fma.rn.f64 	%fd1183, %fd2, %fd1478, 0d0000000000000000;
	fma.rn.f64 	%fd1184, %fd4, %fd1183, 0d0000000000000000;
	fma.rn.f64 	%fd1185, %fd4, %fd1183, %fd1184;
	@%p12 bra 	$L__BB3_141;

	sub.f64 	%fd1462, %fd1182, %fd1185;
	add.f64 	%fd1461, %fd1462, 0d0000000000000000;
	mul.lo.s64 	%rd596, %rd41, %rd28;
	add.s64 	%rd584, %rd61, %rd596;
	// begin inline asm
	{ atom.add.f64 %fd1187,[%rd584],%fd1182; }

	// end inline asm
	add.s64 	%rd585, %rd584, 8;
	// begin inline asm
	{ atom.add.f64 %fd1189,[%rd585],%fd1461; }

	// end inline asm
	add.s64 	%rd586, %rd584, 16;
	// begin inline asm
	{ atom.add.f64 %fd1191,[%rd586],%fd1182; }

	// end inline asm
	add.s64 	%rd587, %rd584, 24;
	// begin inline asm
	{ atom.add.f64 %fd1193,[%rd587],%fd1182; }

	// end inline asm
	add.s64 	%rd588, %rd584, 32;
	// begin inline asm
	{ atom.add.f64 %fd1195,[%rd588],%fd1182; }

	// end inline asm
	add.s64 	%rd589, %rd584, 40;
	// begin inline asm
	{ atom.add.f64 %fd1197,[%rd589],%fd1182; }

	// end inline asm
	add.s64 	%rd590, %rd584, 48;
	// begin inline asm
	{ atom.add.f64 %fd1199,[%rd590],%fd1182; }

	// end inline asm
	add.s64 	%rd591, %rd584, 56;
	// begin inline asm
	{ atom.add.f64 %fd1201,[%rd591],%fd1182; }

	// end inline asm
	add.s64 	%rd592, %rd584, 64;
	// begin inline asm
	{ atom.add.f64 %fd1203,[%rd592],%fd1182; }

	// end inline asm
	add.s64 	%rd593, %rd584, 72;
	// begin inline asm
	{ atom.add.f64 %fd1205,[%rd593],%fd1182; }

	// end inline asm
	add.s64 	%rd594, %rd584, 80;
	// begin inline asm
	{ atom.add.f64 %fd1207,[%rd594],%fd1182; }

	// end inline asm
	add.s64 	%rd595, %rd584, 88;
	// begin inline asm
	{ atom.add.f64 %fd1209,[%rd595],%fd1182; }

	// end inline asm
	bra.uni 	$L__BB3_143;

$L__BB3_141:
	setp.eq.s64 	%p73, %rd54, 0;
	@%p73 bra 	$L__BB3_143;

	sub.f64 	%fd1466, %fd1182, %fd1185;
	add.f64 	%fd1465, %fd1466, 0d0000000000000000;
	add.s64 	%rd597, %rd54, %rd44;
	mov.f64 	%fd1234, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1211,[%rd597],%fd1234; }

	// end inline asm
	add.s64 	%rd598, %rd597, 8;
	// begin inline asm
	{ atom.add.f64 %fd1213,[%rd598],%fd1465; }

	// end inline asm
	add.s64 	%rd599, %rd597, 16;
	// begin inline asm
	{ atom.add.f64 %fd1215,[%rd599],%fd1234; }

	// end inline asm
	add.s64 	%rd600, %rd597, 24;
	// begin inline asm
	{ atom.add.f64 %fd1217,[%rd600],%fd1234; }

	// end inline asm
	add.s64 	%rd601, %rd597, 32;
	// begin inline asm
	{ atom.add.f64 %fd1219,[%rd601],%fd1234; }

	// end inline asm
	add.s64 	%rd602, %rd597, 40;
	// begin inline asm
	{ atom.add.f64 %fd1221,[%rd602],%fd1234; }

	// end inline asm
	add.s64 	%rd603, %rd597, 48;
	// begin inline asm
	{ atom.add.f64 %fd1223,[%rd603],%fd1234; }

	// end inline asm
	add.s64 	%rd604, %rd597, 56;
	// begin inline asm
	{ atom.add.f64 %fd1225,[%rd604],%fd1234; }

	// end inline asm
	add.s64 	%rd605, %rd597, 64;
	// begin inline asm
	{ atom.add.f64 %fd1227,[%rd605],%fd1234; }

	// end inline asm
	add.s64 	%rd606, %rd597, 72;
	// begin inline asm
	{ atom.add.f64 %fd1229,[%rd606],%fd1234; }

	// end inline asm
	add.s64 	%rd607, %rd597, 80;
	// begin inline asm
	{ atom.add.f64 %fd1231,[%rd607],%fd1234; }

	// end inline asm
	add.s64 	%rd608, %rd597, 88;
	// begin inline asm
	{ atom.add.f64 %fd1233,[%rd608],%fd1234; }

	// end inline asm

$L__BB3_143:
	@%p14 bra 	$L__BB3_145;

	add.f64 	%fd1463, %fd1185, 0d0000000000000000;
	mul.lo.s64 	%rd621, %rd41, %rd29;
	add.s64 	%rd609, %rd59, %rd621;
	mov.f64 	%fd1258, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1235,[%rd609],%fd1258; }

	// end inline asm
	add.s64 	%rd610, %rd609, 8;
	// begin inline asm
	{ atom.add.f64 %fd1237,[%rd610],%fd1463; }

	// end inline asm
	add.s64 	%rd611, %rd609, 16;
	// begin inline asm
	{ atom.add.f64 %fd1239,[%rd611],%fd1258; }

	// end inline asm
	add.s64 	%rd612, %rd609, 24;
	// begin inline asm
	{ atom.add.f64 %fd1241,[%rd612],%fd1258; }

	// end inline asm
	add.s64 	%rd613, %rd609, 32;
	// begin inline asm
	{ atom.add.f64 %fd1243,[%rd613],%fd1258; }

	// end inline asm
	add.s64 	%rd614, %rd609, 40;
	// begin inline asm
	{ atom.add.f64 %fd1245,[%rd614],%fd1258; }

	// end inline asm
	add.s64 	%rd615, %rd609, 48;
	// begin inline asm
	{ atom.add.f64 %fd1247,[%rd615],%fd1258; }

	// end inline asm
	add.s64 	%rd616, %rd609, 56;
	// begin inline asm
	{ atom.add.f64 %fd1249,[%rd616],%fd1258; }

	// end inline asm
	add.s64 	%rd617, %rd609, 64;
	// begin inline asm
	{ atom.add.f64 %fd1251,[%rd617],%fd1258; }

	// end inline asm
	add.s64 	%rd618, %rd609, 72;
	// begin inline asm
	{ atom.add.f64 %fd1253,[%rd618],%fd1258; }

	// end inline asm
	add.s64 	%rd619, %rd609, 80;
	// begin inline asm
	{ atom.add.f64 %fd1255,[%rd619],%fd1258; }

	// end inline asm
	add.s64 	%rd620, %rd609, 88;
	// begin inline asm
	{ atom.add.f64 %fd1257,[%rd620],%fd1258; }

	// end inline asm
	bra.uni 	$L__BB3_147;

$L__BB3_145:
	setp.eq.s64 	%p75, %rd50, 0;
	@%p75 bra 	$L__BB3_147;

	add.f64 	%fd1464, %fd1185, 0d0000000000000000;
	add.s64 	%rd622, %rd50, %rd43;
	mov.f64 	%fd1282, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1259,[%rd622],%fd1282; }

	// end inline asm
	add.s64 	%rd623, %rd622, 8;
	// begin inline asm
	{ atom.add.f64 %fd1261,[%rd623],%fd1464; }

	// end inline asm
	add.s64 	%rd624, %rd622, 16;
	// begin inline asm
	{ atom.add.f64 %fd1263,[%rd624],%fd1282; }

	// end inline asm
	add.s64 	%rd625, %rd622, 24;
	// begin inline asm
	{ atom.add.f64 %fd1265,[%rd625],%fd1282; }

	// end inline asm
	add.s64 	%rd626, %rd622, 32;
	// begin inline asm
	{ atom.add.f64 %fd1267,[%rd626],%fd1282; }

	// end inline asm
	add.s64 	%rd627, %rd622, 40;
	// begin inline asm
	{ atom.add.f64 %fd1269,[%rd627],%fd1282; }

	// end inline asm
	add.s64 	%rd628, %rd622, 48;
	// begin inline asm
	{ atom.add.f64 %fd1271,[%rd628],%fd1282; }

	// end inline asm
	add.s64 	%rd629, %rd622, 56;
	// begin inline asm
	{ atom.add.f64 %fd1273,[%rd629],%fd1282; }

	// end inline asm
	add.s64 	%rd630, %rd622, 64;
	// begin inline asm
	{ atom.add.f64 %fd1275,[%rd630],%fd1282; }

	// end inline asm
	add.s64 	%rd631, %rd622, 72;
	// begin inline asm
	{ atom.add.f64 %fd1277,[%rd631],%fd1282; }

	// end inline asm
	add.s64 	%rd632, %rd622, 80;
	// begin inline asm
	{ atom.add.f64 %fd1279,[%rd632],%fd1282; }

	// end inline asm
	add.s64 	%rd633, %rd622, 88;
	// begin inline asm
	{ atom.add.f64 %fd1281,[%rd633],%fd1282; }

	// end inline asm

$L__BB3_147:
	fma.rn.f64 	%fd92, %fd1, %fd89, %fd85;
	@%p10 bra 	$L__BB3_149;

	ld.global.f64 	%fd1283, [%rd45];
	add.f64 	%fd1479, %fd1283, 0d0000000000000000;
	bra.uni 	$L__BB3_151;

$L__BB3_149:
	setp.eq.s64 	%p77, %rd56, 0;
	mov.f64 	%fd1479, 0d0000000000000000;
	@%p77 bra 	$L__BB3_151;

	ld.global.f64 	%fd1285, [%rd46];
	add.f64 	%fd1479, %fd1285, 0d0000000000000000;

$L__BB3_151:
	mul.f64 	%fd1286, %fd3, %fd3;
	fma.rn.f64 	%fd96, %fd1286, %fd1479, 0d0000000000000000;
	mov.f64 	%fd1287, 0d0000000000000000;
	fma.rn.f64 	%fd1288, %fd2, %fd1479, 0d0000000000000000;
	fma.rn.f64 	%fd1289, %fd3, %fd1288, 0d0000000000000000;
	fma.rn.f64 	%fd1290, %fd3, %fd1288, %fd1289;
	add.f64 	%fd97, %fd1290, 0d0000000000000000;
	sub.f64 	%fd1291, %fd1287, %fd1290;
	add.f64 	%fd98, %fd1291, 0d0000000000000000;
	@%p12 bra 	$L__BB3_153;

	mul.lo.s64 	%rd646, %rd41, %rd28;
	add.s64 	%rd634, %rd61, %rd646;
	// begin inline asm
	{ atom.add.f64 %fd1292,[%rd634],%fd98; }

	// end inline asm
	add.s64 	%rd635, %rd634, 8;
	// begin inline asm
	{ atom.add.f64 %fd1294,[%rd635],%fd1287; }

	// end inline asm
	add.s64 	%rd636, %rd634, 16;
	// begin inline asm
	{ atom.add.f64 %fd1296,[%rd636],%fd1287; }

	// end inline asm
	add.s64 	%rd637, %rd634, 24;
	// begin inline asm
	{ atom.add.f64 %fd1298,[%rd637],%fd1287; }

	// end inline asm
	add.s64 	%rd638, %rd634, 32;
	// begin inline asm
	{ atom.add.f64 %fd1300,[%rd638],%fd1287; }

	// end inline asm
	add.s64 	%rd639, %rd634, 40;
	// begin inline asm
	{ atom.add.f64 %fd1302,[%rd639],%fd1287; }

	// end inline asm
	add.s64 	%rd640, %rd634, 48;
	// begin inline asm
	{ atom.add.f64 %fd1304,[%rd640],%fd1287; }

	// end inline asm
	add.s64 	%rd641, %rd634, 56;
	// begin inline asm
	{ atom.add.f64 %fd1306,[%rd641],%fd1287; }

	// end inline asm
	add.s64 	%rd642, %rd634, 64;
	// begin inline asm
	{ atom.add.f64 %fd1308,[%rd642],%fd1287; }

	// end inline asm
	add.s64 	%rd643, %rd634, 72;
	// begin inline asm
	{ atom.add.f64 %fd1310,[%rd643],%fd1287; }

	// end inline asm
	add.s64 	%rd644, %rd634, 80;
	// begin inline asm
	{ atom.add.f64 %fd1312,[%rd644],%fd1287; }

	// end inline asm
	add.s64 	%rd645, %rd634, 88;
	// begin inline asm
	{ atom.add.f64 %fd1314,[%rd645],%fd1287; }

	// end inline asm
	bra.uni 	$L__BB3_155;

$L__BB3_153:
	setp.eq.s64 	%p79, %rd54, 0;
	@%p79 bra 	$L__BB3_155;

	add.s64 	%rd647, %rd54, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd1316,[%rd647],%fd98; }

	// end inline asm
	add.s64 	%rd648, %rd647, 8;
	mov.f64 	%fd1339, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1318,[%rd648],%fd1339; }

	// end inline asm
	add.s64 	%rd649, %rd647, 16;
	// begin inline asm
	{ atom.add.f64 %fd1320,[%rd649],%fd1339; }

	// end inline asm
	add.s64 	%rd650, %rd647, 24;
	// begin inline asm
	{ atom.add.f64 %fd1322,[%rd650],%fd1339; }

	// end inline asm
	add.s64 	%rd651, %rd647, 32;
	// begin inline asm
	{ atom.add.f64 %fd1324,[%rd651],%fd1339; }

	// end inline asm
	add.s64 	%rd652, %rd647, 40;
	// begin inline asm
	{ atom.add.f64 %fd1326,[%rd652],%fd1339; }

	// end inline asm
	add.s64 	%rd653, %rd647, 48;
	// begin inline asm
	{ atom.add.f64 %fd1328,[%rd653],%fd1339; }

	// end inline asm
	add.s64 	%rd654, %rd647, 56;
	// begin inline asm
	{ atom.add.f64 %fd1330,[%rd654],%fd1339; }

	// end inline asm
	add.s64 	%rd655, %rd647, 64;
	// begin inline asm
	{ atom.add.f64 %fd1332,[%rd655],%fd1339; }

	// end inline asm
	add.s64 	%rd656, %rd647, 72;
	// begin inline asm
	{ atom.add.f64 %fd1334,[%rd656],%fd1339; }

	// end inline asm
	add.s64 	%rd657, %rd647, 80;
	// begin inline asm
	{ atom.add.f64 %fd1336,[%rd657],%fd1339; }

	// end inline asm
	add.s64 	%rd658, %rd647, 88;
	// begin inline asm
	{ atom.add.f64 %fd1338,[%rd658],%fd1339; }

	// end inline asm

$L__BB3_155:
	@%p14 bra 	$L__BB3_157;

	mul.lo.s64 	%rd671, %rd41, %rd29;
	add.s64 	%rd659, %rd59, %rd671;
	// begin inline asm
	{ atom.add.f64 %fd1340,[%rd659],%fd97; }

	// end inline asm
	add.s64 	%rd660, %rd659, 8;
	mov.f64 	%fd1363, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1342,[%rd660],%fd1363; }

	// end inline asm
	add.s64 	%rd661, %rd659, 16;
	// begin inline asm
	{ atom.add.f64 %fd1344,[%rd661],%fd1363; }

	// end inline asm
	add.s64 	%rd662, %rd659, 24;
	// begin inline asm
	{ atom.add.f64 %fd1346,[%rd662],%fd1363; }

	// end inline asm
	add.s64 	%rd663, %rd659, 32;
	// begin inline asm
	{ atom.add.f64 %fd1348,[%rd663],%fd1363; }

	// end inline asm
	add.s64 	%rd664, %rd659, 40;
	// begin inline asm
	{ atom.add.f64 %fd1350,[%rd664],%fd1363; }

	// end inline asm
	add.s64 	%rd665, %rd659, 48;
	// begin inline asm
	{ atom.add.f64 %fd1352,[%rd665],%fd1363; }

	// end inline asm
	add.s64 	%rd666, %rd659, 56;
	// begin inline asm
	{ atom.add.f64 %fd1354,[%rd666],%fd1363; }

	// end inline asm
	add.s64 	%rd667, %rd659, 64;
	// begin inline asm
	{ atom.add.f64 %fd1356,[%rd667],%fd1363; }

	// end inline asm
	add.s64 	%rd668, %rd659, 72;
	// begin inline asm
	{ atom.add.f64 %fd1358,[%rd668],%fd1363; }

	// end inline asm
	add.s64 	%rd669, %rd659, 80;
	// begin inline asm
	{ atom.add.f64 %fd1360,[%rd669],%fd1363; }

	// end inline asm
	add.s64 	%rd670, %rd659, 88;
	// begin inline asm
	{ atom.add.f64 %fd1362,[%rd670],%fd1363; }

	// end inline asm
	bra.uni 	$L__BB3_159;

$L__BB3_157:
	setp.eq.s64 	%p81, %rd50, 0;
	@%p81 bra 	$L__BB3_159;

	add.s64 	%rd672, %rd50, %rd43;
	// begin inline asm
	{ atom.add.f64 %fd1364,[%rd672],%fd97; }

	// end inline asm
	add.s64 	%rd673, %rd672, 8;
	mov.f64 	%fd1387, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd1366,[%rd673],%fd1387; }

	// end inline asm
	add.s64 	%rd674, %rd672, 16;
	// begin inline asm
	{ atom.add.f64 %fd1368,[%rd674],%fd1387; }

	// end inline asm
	add.s64 	%rd675, %rd672, 24;
	// begin inline asm
	{ atom.add.f64 %fd1370,[%rd675],%fd1387; }

	// end inline asm
	add.s64 	%rd676, %rd672, 32;
	// begin inline asm
	{ atom.add.f64 %fd1372,[%rd676],%fd1387; }

	// end inline asm
	add.s64 	%rd677, %rd672, 40;
	// begin inline asm
	{ atom.add.f64 %fd1374,[%rd677],%fd1387; }

	// end inline asm
	add.s64 	%rd678, %rd672, 48;
	// begin inline asm
	{ atom.add.f64 %fd1376,[%rd678],%fd1387; }

	// end inline asm
	add.s64 	%rd679, %rd672, 56;
	// begin inline asm
	{ atom.add.f64 %fd1378,[%rd679],%fd1387; }

	// end inline asm
	add.s64 	%rd680, %rd672, 64;
	// begin inline asm
	{ atom.add.f64 %fd1380,[%rd680],%fd1387; }

	// end inline asm
	add.s64 	%rd681, %rd672, 72;
	// begin inline asm
	{ atom.add.f64 %fd1382,[%rd681],%fd1387; }

	// end inline asm
	add.s64 	%rd682, %rd672, 80;
	// begin inline asm
	{ atom.add.f64 %fd1384,[%rd682],%fd1387; }

	// end inline asm
	add.s64 	%rd683, %rd672, 88;
	// begin inline asm
	{ atom.add.f64 %fd1386,[%rd683],%fd1387; }

	// end inline asm

$L__BB3_159:
	fma.rn.f64 	%fd1480, %fd1, %fd96, %fd92;

$L__BB3_160:
	add.f64 	%fd101, %fd1480, 0d0000000000000000;
	setp.eq.s64 	%p82, %rd65, 0;
	@%p82 bra 	$L__BB3_162;

	mul.lo.s64 	%rd685, %rd41, %rd25;
	add.s64 	%rd684, %rd65, %rd685;
	// begin inline asm
	{ atom.add.f64 %fd1388,[%rd684],%fd101; }

	// end inline asm
	bra.uni 	$L__BB3_164;

$L__BB3_162:
	setp.eq.s64 	%p83, %rd58, 0;
	@%p83 bra 	$L__BB3_164;

	add.s64 	%rd686, %rd58, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd1390,[%rd686],%fd101; }

	// end inline asm

$L__BB3_164:
	ld.param.u64 	%rd687, [compute_affine_kinematic_energy_cuda_kernel_backward_param_0+24];
	add.s64 	%rd690, %rd690, %rd24;
	setp.lt.u64 	%p84, %rd690, %rd687;
	@%p84 bra 	$L__BB3_2;

$L__BB3_165:
	ret;

}
	// .globl	init_affine_kinematic_target_kernel_cuda_kernel_forward
.visible .entry init_affine_kinematic_target_kernel_cuda_kernel_forward(
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_3[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_forward_param_5[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<109>;
	.reg .f64 	%fd<91>;
	.reg .b64 	%rd<65>;


	ld.param.v2.u32 	{%r52, %r53}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r54, %r55}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r60, %r61}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r68, %r69}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r76, %r77}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_3+32];
	ld.param.v2.u32 	{%r84, %r85}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r92, %r93}, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_5+32];
	ld.param.u64 	%rd39, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd37, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd35, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [init_affine_kinematic_target_kernel_cuda_kernel_forward_param_0+16];
	mov.u32 	%r96, %ntid.x;
	cvt.u64.u32 	%rd1, %r96;
	mov.u32 	%r97, %ctaid.x;
	mul.wide.u32 	%rd41, %r96, %r97;
	mov.u32 	%r98, %tid.x;
	cvt.u64.u32 	%rd42, %r98;
	add.s64 	%rd61, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd61, %rd30;
	@%p1 bra 	$L__BB4_17;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd5, %rd37;
	cvta.to.global.u64 	%rd6, %rd35;
	cvta.to.global.u64 	%rd7, %rd33;
	cvt.s64.s32 	%rd8, %r55;
	cvt.s64.s32 	%rd9, %r54;
	cvt.s64.s32 	%rd10, %r53;
	cvt.s64.s32 	%rd11, %r60;
	mov.u32 	%r99, %nctaid.x;
	cvt.u64.u32 	%rd43, %r99;
	mul.lo.s64 	%rd12, %rd1, %rd43;
	cvt.s64.s32 	%rd13, %r68;
	cvt.s64.s32 	%rd14, %r84;
	cvt.s64.s32 	%rd15, %r92;
	cvt.s64.s32 	%rd16, %r76;
	cvta.to.global.u64 	%rd17, %rd31;

$L__BB4_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd62, %rd61;
	@%p2 bra 	$L__BB4_6;

	or.b64  	%rd44, %rd61, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB4_5;

	div.u64 	%rd62, %rd61, %rd8;
	bra.uni 	$L__BB4_6;

$L__BB4_5:
	cvt.u32.u64 	%r100, %rd8;
	cvt.u32.u64 	%r101, %rd61;
	div.u32 	%r102, %r101, %r100;
	cvt.u64.u32 	%rd62, %r102;

$L__BB4_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB4_10;

	or.b64  	%rd46, %rd62, %rd9;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB4_9;

	div.u64 	%rd62, %rd62, %rd9;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r103, %rd9;
	cvt.u32.u64 	%r104, %rd62;
	div.u32 	%r105, %r104, %r103;
	cvt.u64.u32 	%rd62, %r105;

$L__BB4_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB4_14;

	or.b64  	%rd48, %rd62, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p7, %rd49, 0;
	@%p7 bra 	$L__BB4_13;

	div.u64 	%rd62, %rd62, %rd10;
	bra.uni 	$L__BB4_14;

$L__BB4_13:
	cvt.u32.u64 	%r106, %rd10;
	cvt.u32.u64 	%r107, %rd62;
	div.u32 	%r108, %r107, %r106;
	cvt.u64.u32 	%rd62, %r108;

$L__BB4_14:
	cvt.s64.s32 	%rd50, %rd62;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd28, %rd50, 0, %p8;
	mul.lo.s64 	%rd51, %rd28, %rd11;
	add.s64 	%rd52, %rd17, %rd51;
	ld.global.u8 	%rs41, [%rd52];
	setp.eq.s16 	%p9, %rs41, 0;
	@%p9 bra 	$L__BB4_16;

	mul.lo.s64 	%rd53, %rd28, %rd13;
	add.s64 	%rd54, %rd7, %rd53;
	mul.lo.s64 	%rd55, %rd28, %rd14;
	add.s64 	%rd56, %rd5, %rd55;
	mul.lo.s64 	%rd57, %rd28, %rd15;
	add.s64 	%rd58, %rd4, %rd57;
	ld.global.f64 	%fd1, [%rd58];
	add.f64 	%fd2, %fd1, 0d3FF0000000000000;
	ld.global.f64 	%fd3, [%rd58+16];
	add.f64 	%fd4, %fd3, 0d0000000000000000;
	ld.global.f64 	%fd5, [%rd58+8];
	add.f64 	%fd6, %fd5, 0d3FF0000000000000;
	ld.global.f64 	%fd7, [%rd56];
	sub.f64 	%fd8, %fd1, %fd7;
	ld.global.f64 	%fd9, [%rd56+8];
	sub.f64 	%fd10, %fd5, %fd9;
	ld.global.f64 	%fd11, [%rd56+16];
	sub.f64 	%fd12, %fd3, %fd11;
	add.f64 	%fd13, %fd5, 0d0000000000000000;
	add.f64 	%fd14, %fd1, 0d0000000000000000;
	add.f64 	%fd15, %fd3, 0d3FF0000000000000;
	ld.global.f64 	%fd16, [%rd54];
	ld.global.f64 	%fd17, [%rd54+24];
	ld.global.f64 	%fd18, [%rd54+48];
	ld.global.f64 	%fd19, [%rd54+8];
	mul.f64 	%fd20, %fd19, %fd10;
	ld.global.f64 	%fd21, [%rd54+32];
	mul.f64 	%fd22, %fd21, %fd10;
	ld.global.f64 	%fd23, [%rd54+56];
	mul.f64 	%fd24, %fd23, %fd10;
	fma.rn.f64 	%fd25, %fd16, %fd8, %fd20;
	fma.rn.f64 	%fd26, %fd17, %fd8, %fd22;
	fma.rn.f64 	%fd27, %fd18, %fd8, %fd24;
	ld.global.f64 	%fd28, [%rd54+16];
	ld.global.f64 	%fd29, [%rd54+40];
	ld.global.f64 	%fd30, [%rd54+64];
	fma.rn.f64 	%fd31, %fd28, %fd12, %fd25;
	fma.rn.f64 	%fd32, %fd29, %fd12, %fd26;
	fma.rn.f64 	%fd33, %fd30, %fd12, %fd27;
	add.f64 	%fd34, %fd7, %fd31;
	add.f64 	%fd35, %fd9, %fd32;
	add.f64 	%fd36, %fd11, %fd33;
	sub.f64 	%fd37, %fd2, %fd7;
	sub.f64 	%fd38, %fd13, %fd9;
	sub.f64 	%fd39, %fd4, %fd11;
	ld.global.f64 	%fd40, [%rd54+72];
	add.f64 	%fd41, %fd40, %fd34;
	ld.global.f64 	%fd42, [%rd54+80];
	add.f64 	%fd43, %fd42, %fd35;
	ld.global.f64 	%fd44, [%rd54+88];
	add.f64 	%fd45, %fd44, %fd36;
	mul.f64 	%fd46, %fd19, %fd38;
	mul.f64 	%fd47, %fd21, %fd38;
	mul.f64 	%fd48, %fd23, %fd38;
	fma.rn.f64 	%fd49, %fd16, %fd37, %fd46;
	fma.rn.f64 	%fd50, %fd17, %fd37, %fd47;
	fma.rn.f64 	%fd51, %fd18, %fd37, %fd48;
	fma.rn.f64 	%fd52, %fd28, %fd39, %fd49;
	fma.rn.f64 	%fd53, %fd29, %fd39, %fd50;
	fma.rn.f64 	%fd54, %fd30, %fd39, %fd51;
	add.f64 	%fd55, %fd7, %fd52;
	add.f64 	%fd56, %fd9, %fd53;
	add.f64 	%fd57, %fd11, %fd54;
	sub.f64 	%fd58, %fd14, %fd7;
	sub.f64 	%fd59, %fd6, %fd9;
	add.f64 	%fd60, %fd40, %fd55;
	add.f64 	%fd61, %fd42, %fd56;
	add.f64 	%fd62, %fd44, %fd57;
	mul.f64 	%fd63, %fd19, %fd59;
	mul.f64 	%fd64, %fd21, %fd59;
	mul.f64 	%fd65, %fd23, %fd59;
	fma.rn.f64 	%fd66, %fd16, %fd58, %fd63;
	fma.rn.f64 	%fd67, %fd17, %fd58, %fd64;
	fma.rn.f64 	%fd68, %fd18, %fd58, %fd65;
	fma.rn.f64 	%fd69, %fd28, %fd39, %fd66;
	fma.rn.f64 	%fd70, %fd29, %fd39, %fd67;
	fma.rn.f64 	%fd71, %fd30, %fd39, %fd68;
	add.f64 	%fd72, %fd7, %fd69;
	add.f64 	%fd73, %fd9, %fd70;
	add.f64 	%fd74, %fd11, %fd71;
	sub.f64 	%fd75, %fd15, %fd11;
	add.f64 	%fd76, %fd40, %fd72;
	add.f64 	%fd77, %fd42, %fd73;
	add.f64 	%fd78, %fd44, %fd74;
	fma.rn.f64 	%fd79, %fd16, %fd58, %fd46;
	fma.rn.f64 	%fd80, %fd17, %fd58, %fd47;
	fma.rn.f64 	%fd81, %fd18, %fd58, %fd48;
	fma.rn.f64 	%fd82, %fd28, %fd75, %fd79;
	fma.rn.f64 	%fd83, %fd29, %fd75, %fd80;
	fma.rn.f64 	%fd84, %fd30, %fd75, %fd81;
	add.f64 	%fd85, %fd7, %fd82;
	add.f64 	%fd86, %fd9, %fd83;
	add.f64 	%fd87, %fd11, %fd84;
	add.f64 	%fd88, %fd40, %fd85;
	add.f64 	%fd89, %fd42, %fd86;
	add.f64 	%fd90, %fd44, %fd87;
	mul.lo.s64 	%rd59, %rd28, %rd16;
	add.s64 	%rd60, %rd6, %rd59;
	st.global.f64 	[%rd60], %fd41;
	st.global.f64 	[%rd60+8], %fd43;
	st.global.f64 	[%rd60+16], %fd45;
	st.global.f64 	[%rd60+24], %fd60;
	st.global.f64 	[%rd60+32], %fd61;
	st.global.f64 	[%rd60+40], %fd62;
	st.global.f64 	[%rd60+48], %fd76;
	st.global.f64 	[%rd60+56], %fd77;
	st.global.f64 	[%rd60+64], %fd78;
	st.global.f64 	[%rd60+72], %fd88;
	st.global.f64 	[%rd60+80], %fd89;
	st.global.f64 	[%rd60+88], %fd90;

$L__BB4_16:
	add.s64 	%rd61, %rd61, %rd12;
	setp.lt.u64 	%p10, %rd61, %rd30;
	@%p10 bra 	$L__BB4_2;

$L__BB4_17:
	ret;

}
	// .globl	init_affine_kinematic_target_kernel_cuda_kernel_backward
.visible .entry init_affine_kinematic_target_kernel_cuda_kernel_backward(
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 init_affine_kinematic_target_kernel_cuda_kernel_backward_param_10[56]
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<177>;
	.reg .f64 	%fd<340>;
	.reg .b64 	%rd<130>;


	ld.param.v2.u32 	{%r88, %r89}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r90, %r91}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r96, %r97}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r104, %r105}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r112, %r113}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3+32];
	ld.param.v2.u32 	{%r120, %r121}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r128, %r129}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r136, %r137}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r144, %r145}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r152, %r153}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r160, %r161}, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd63, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd61, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd59, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd57, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd56, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd55, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd54, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd53, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd52, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd50, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2+8];
	ld.param.u64 	%rd49, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd47, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd46, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+16];
	mov.u32 	%r164, %ntid.x;
	cvt.u64.u32 	%rd1, %r164;
	mov.u32 	%r165, %ctaid.x;
	mul.wide.u32 	%rd65, %r164, %r165;
	mov.u32 	%r166, %tid.x;
	cvt.u64.u32 	%rd66, %r166;
	add.s64 	%rd126, %rd65, %rd66;
	setp.ge.u64 	%p1, %rd126, %rd46;
	@%p1 bra 	$L__BB5_32;

	cvta.to.global.u64 	%rd12, %rd59;
	cvta.to.global.u64 	%rd13, %rd55;
	cvta.to.global.u64 	%rd14, %rd53;
	cvta.to.global.u64 	%rd15, %rd52;
	cvta.to.global.u64 	%rd16, %rd49;
	cvt.s64.s32 	%rd17, %r91;
	cvt.s64.s32 	%rd18, %r90;
	cvt.s64.s32 	%rd19, %r89;
	cvt.s64.s32 	%rd20, %r96;
	mov.u32 	%r167, %nctaid.x;
	cvt.u64.u32 	%rd67, %r167;
	mul.lo.s64 	%rd21, %rd1, %rd67;
	cvt.s64.s32 	%rd22, %r104;
	cvt.s64.s32 	%rd23, %r120;
	cvt.s64.s32 	%rd24, %r128;
	cvt.s64.s32 	%rd25, %r144;
	cvt.s64.s32 	%rd26, %r112;
	cvt.s64.s32 	%rd27, %r160;
	cvt.s64.s32 	%rd28, %r152;
	cvt.s64.s32 	%rd29, %r136;
	cvta.to.global.u64 	%rd30, %rd47;

$L__BB5_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd127, %rd126;
	@%p2 bra 	$L__BB5_6;

	or.b64  	%rd68, %rd126, %rd17;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.eq.s64 	%p3, %rd69, 0;
	@%p3 bra 	$L__BB5_5;

	div.u64 	%rd127, %rd126, %rd17;
	bra.uni 	$L__BB5_6;

$L__BB5_5:
	cvt.u32.u64 	%r168, %rd17;
	cvt.u32.u64 	%r169, %rd126;
	div.u32 	%r170, %r169, %r168;
	cvt.u64.u32 	%rd127, %r170;

$L__BB5_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB5_10;

	or.b64  	%rd70, %rd127, %rd18;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.eq.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB5_9;

	div.u64 	%rd127, %rd127, %rd18;
	bra.uni 	$L__BB5_10;

$L__BB5_9:
	cvt.u32.u64 	%r171, %rd18;
	cvt.u32.u64 	%r172, %rd127;
	div.u32 	%r173, %r172, %r171;
	cvt.u64.u32 	%rd127, %r173;

$L__BB5_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB5_14;

	or.b64  	%rd72, %rd127, %rd19;
	and.b64  	%rd73, %rd72, -4294967296;
	setp.eq.s64 	%p7, %rd73, 0;
	@%p7 bra 	$L__BB5_13;

	div.u64 	%rd127, %rd127, %rd19;
	bra.uni 	$L__BB5_14;

$L__BB5_13:
	cvt.u32.u64 	%r174, %rd19;
	cvt.u32.u64 	%r175, %rd127;
	div.u32 	%r176, %r175, %r174;
	cvt.u64.u32 	%rd127, %r176;

$L__BB5_14:
	cvt.s64.s32 	%rd74, %rd127;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b64 	%rd41, %rd74, 0, %p8;
	mul.lo.s64 	%rd75, %rd41, %rd20;
	add.s64 	%rd76, %rd30, %rd75;
	ld.global.u8 	%rs73, [%rd76];
	setp.eq.s16 	%p9, %rs73, 0;
	@%p9 bra 	$L__BB5_31;

	ld.param.u64 	%rd124, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_8];
	setp.eq.s64 	%p10, %rd124, 0;
	mul.lo.s64 	%rd42, %rd41, %rd22;
	add.s64 	%rd77, %rd16, %rd42;
	ld.global.f64 	%fd1, [%rd77];
	ld.global.f64 	%fd2, [%rd77+8];
	ld.global.f64 	%fd3, [%rd77+16];
	ld.global.f64 	%fd4, [%rd77+24];
	ld.global.f64 	%fd5, [%rd77+32];
	ld.global.f64 	%fd6, [%rd77+40];
	ld.global.f64 	%fd7, [%rd77+48];
	ld.global.f64 	%fd8, [%rd77+56];
	ld.global.f64 	%fd9, [%rd77+64];
	mul.lo.s64 	%rd43, %rd41, %rd23;
	add.s64 	%rd78, %rd14, %rd43;
	mul.lo.s64 	%rd44, %rd41, %rd24;
	add.s64 	%rd79, %rd13, %rd44;
	ld.global.f64 	%fd10, [%rd79];
	ld.global.f64 	%fd11, [%rd79+16];
	ld.global.f64 	%fd12, [%rd79+8];
	ld.global.f64 	%fd13, [%rd78];
	ld.global.f64 	%fd14, [%rd78+8];
	ld.global.f64 	%fd15, [%rd78+16];
	@%p10 bra 	$L__BB5_17;

	mul.lo.s64 	%rd80, %rd41, %rd25;
	add.s64 	%rd81, %rd12, %rd80;
	ld.global.f64 	%fd73, [%rd81];
	add.f64 	%fd339, %fd73, 0d0000000000000000;
	ld.global.f64 	%fd74, [%rd81+8];
	add.f64 	%fd338, %fd74, 0d0000000000000000;
	ld.global.f64 	%fd75, [%rd81+16];
	add.f64 	%fd337, %fd75, 0d0000000000000000;
	ld.global.f64 	%fd76, [%rd81+24];
	add.f64 	%fd336, %fd76, 0d0000000000000000;
	ld.global.f64 	%fd77, [%rd81+32];
	add.f64 	%fd335, %fd77, 0d0000000000000000;
	ld.global.f64 	%fd78, [%rd81+40];
	add.f64 	%fd334, %fd78, 0d0000000000000000;
	ld.global.f64 	%fd79, [%rd81+48];
	add.f64 	%fd333, %fd79, 0d0000000000000000;
	ld.global.f64 	%fd80, [%rd81+56];
	add.f64 	%fd332, %fd80, 0d0000000000000000;
	ld.global.f64 	%fd81, [%rd81+64];
	add.f64 	%fd331, %fd81, 0d0000000000000000;
	ld.global.f64 	%fd82, [%rd81+72];
	add.f64 	%fd330, %fd82, 0d0000000000000000;
	ld.global.f64 	%fd83, [%rd81+80];
	add.f64 	%fd329, %fd83, 0d0000000000000000;
	ld.global.f64 	%fd84, [%rd81+88];
	add.f64 	%fd328, %fd84, 0d0000000000000000;
	bra.uni 	$L__BB5_19;

$L__BB5_17:
	ld.param.u64 	%rd125, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_3+8];
	setp.eq.s64 	%p11, %rd125, 0;
	mov.f64 	%fd328, 0d0000000000000000;
	mov.f64 	%fd329, %fd328;
	mov.f64 	%fd330, %fd328;
	mov.f64 	%fd331, %fd328;
	mov.f64 	%fd332, %fd328;
	mov.f64 	%fd333, %fd328;
	mov.f64 	%fd334, %fd328;
	mov.f64 	%fd335, %fd328;
	mov.f64 	%fd336, %fd328;
	mov.f64 	%fd337, %fd328;
	mov.f64 	%fd338, %fd328;
	mov.f64 	%fd339, %fd328;
	@%p11 bra 	$L__BB5_19;

	mul.lo.s64 	%rd82, %rd41, %rd26;
	add.s64 	%rd83, %rd15, %rd82;
	ld.global.f64 	%fd97, [%rd83];
	add.f64 	%fd339, %fd97, 0d0000000000000000;
	ld.global.f64 	%fd98, [%rd83+8];
	add.f64 	%fd338, %fd98, 0d0000000000000000;
	ld.global.f64 	%fd99, [%rd83+16];
	add.f64 	%fd337, %fd99, 0d0000000000000000;
	ld.global.f64 	%fd100, [%rd83+24];
	add.f64 	%fd336, %fd100, 0d0000000000000000;
	ld.global.f64 	%fd101, [%rd83+32];
	add.f64 	%fd335, %fd101, 0d0000000000000000;
	ld.global.f64 	%fd102, [%rd83+40];
	add.f64 	%fd334, %fd102, 0d0000000000000000;
	ld.global.f64 	%fd103, [%rd83+48];
	add.f64 	%fd333, %fd103, 0d0000000000000000;
	ld.global.f64 	%fd104, [%rd83+56];
	add.f64 	%fd332, %fd104, 0d0000000000000000;
	ld.global.f64 	%fd105, [%rd83+64];
	add.f64 	%fd331, %fd105, 0d0000000000000000;
	ld.global.f64 	%fd106, [%rd83+72];
	add.f64 	%fd330, %fd106, 0d0000000000000000;
	ld.global.f64 	%fd107, [%rd83+80];
	add.f64 	%fd329, %fd107, 0d0000000000000000;
	ld.global.f64 	%fd108, [%rd83+88];
	add.f64 	%fd328, %fd108, 0d0000000000000000;

$L__BB5_19:
	sub.f64 	%fd109, %fd10, %fd13;
	sub.f64 	%fd110, %fd12, %fd14;
	sub.f64 	%fd111, %fd11, %fd15;
	add.f64 	%fd112, %fd10, 0d3FF0000000000000;
	sub.f64 	%fd113, %fd112, %fd13;
	add.f64 	%fd114, %fd12, 0d0000000000000000;
	sub.f64 	%fd115, %fd114, %fd14;
	add.f64 	%fd116, %fd11, 0d0000000000000000;
	sub.f64 	%fd117, %fd116, %fd15;
	add.f64 	%fd118, %fd10, 0d0000000000000000;
	sub.f64 	%fd119, %fd118, %fd13;
	add.f64 	%fd120, %fd12, 0d3FF0000000000000;
	sub.f64 	%fd121, %fd120, %fd14;
	add.f64 	%fd122, %fd11, 0d3FF0000000000000;
	sub.f64 	%fd123, %fd122, %fd15;
	add.f64 	%fd124, %fd330, 0d0000000000000000;
	add.f64 	%fd125, %fd329, 0d0000000000000000;
	add.f64 	%fd126, %fd328, 0d0000000000000000;
	fma.rn.f64 	%fd127, %fd124, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd128, %fd124, %fd115, 0d0000000000000000;
	fma.rn.f64 	%fd129, %fd124, %fd123, 0d0000000000000000;
	fma.rn.f64 	%fd130, %fd125, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd131, %fd125, %fd115, 0d0000000000000000;
	fma.rn.f64 	%fd132, %fd125, %fd123, 0d0000000000000000;
	fma.rn.f64 	%fd133, %fd126, %fd119, 0d0000000000000000;
	fma.rn.f64 	%fd134, %fd126, %fd115, 0d0000000000000000;
	fma.rn.f64 	%fd135, %fd126, %fd123, 0d0000000000000000;
	mul.f64 	%fd136, %fd4, %fd125;
	mul.f64 	%fd137, %fd5, %fd125;
	mul.f64 	%fd138, %fd6, %fd125;
	fma.rn.f64 	%fd139, %fd1, %fd124, %fd136;
	fma.rn.f64 	%fd140, %fd2, %fd124, %fd137;
	fma.rn.f64 	%fd141, %fd3, %fd124, %fd138;
	fma.rn.f64 	%fd142, %fd7, %fd126, %fd139;
	fma.rn.f64 	%fd143, %fd8, %fd126, %fd140;
	fma.rn.f64 	%fd144, %fd9, %fd126, %fd141;
	add.f64 	%fd145, %fd142, 0d0000000000000000;
	add.f64 	%fd146, %fd143, 0d0000000000000000;
	add.f64 	%fd147, %fd144, 0d0000000000000000;
	sub.f64 	%fd148, %fd124, %fd145;
	sub.f64 	%fd149, %fd125, %fd146;
	sub.f64 	%fd150, %fd126, %fd147;
	add.f64 	%fd151, %fd333, 0d0000000000000000;
	add.f64 	%fd152, %fd332, 0d0000000000000000;
	add.f64 	%fd153, %fd331, 0d0000000000000000;
	fma.rn.f64 	%fd154, %fd151, %fd119, %fd127;
	fma.rn.f64 	%fd155, %fd151, %fd121, %fd128;
	fma.rn.f64 	%fd156, %fd151, %fd117, %fd129;
	fma.rn.f64 	%fd157, %fd152, %fd119, %fd130;
	fma.rn.f64 	%fd158, %fd152, %fd121, %fd131;
	fma.rn.f64 	%fd159, %fd152, %fd117, %fd132;
	fma.rn.f64 	%fd160, %fd153, %fd119, %fd133;
	fma.rn.f64 	%fd161, %fd153, %fd121, %fd134;
	fma.rn.f64 	%fd162, %fd153, %fd117, %fd135;
	add.f64 	%fd163, %fd124, %fd151;
	add.f64 	%fd164, %fd125, %fd152;
	add.f64 	%fd165, %fd126, %fd153;
	add.f64 	%fd166, %fd151, %fd148;
	add.f64 	%fd167, %fd152, %fd149;
	add.f64 	%fd168, %fd153, %fd150;
	mul.f64 	%fd169, %fd4, %fd152;
	mul.f64 	%fd170, %fd5, %fd152;
	mul.f64 	%fd171, %fd6, %fd152;
	fma.rn.f64 	%fd172, %fd1, %fd151, %fd169;
	fma.rn.f64 	%fd173, %fd2, %fd151, %fd170;
	fma.rn.f64 	%fd174, %fd3, %fd151, %fd171;
	fma.rn.f64 	%fd175, %fd7, %fd153, %fd172;
	fma.rn.f64 	%fd176, %fd8, %fd153, %fd173;
	fma.rn.f64 	%fd177, %fd9, %fd153, %fd174;
	add.f64 	%fd178, %fd175, 0d0000000000000000;
	add.f64 	%fd179, %fd176, 0d0000000000000000;
	add.f64 	%fd180, %fd177, 0d0000000000000000;
	sub.f64 	%fd181, %fd166, %fd178;
	sub.f64 	%fd182, %fd167, %fd179;
	sub.f64 	%fd183, %fd168, %fd180;
	add.f64 	%fd184, %fd336, 0d0000000000000000;
	add.f64 	%fd185, %fd335, 0d0000000000000000;
	add.f64 	%fd186, %fd334, 0d0000000000000000;
	fma.rn.f64 	%fd187, %fd184, %fd113, %fd154;
	fma.rn.f64 	%fd188, %fd184, %fd115, %fd155;
	fma.rn.f64 	%fd189, %fd184, %fd117, %fd156;
	fma.rn.f64 	%fd190, %fd185, %fd113, %fd157;
	fma.rn.f64 	%fd191, %fd185, %fd115, %fd158;
	fma.rn.f64 	%fd192, %fd185, %fd117, %fd159;
	fma.rn.f64 	%fd193, %fd186, %fd113, %fd160;
	fma.rn.f64 	%fd194, %fd186, %fd115, %fd161;
	fma.rn.f64 	%fd195, %fd186, %fd117, %fd162;
	add.f64 	%fd196, %fd163, %fd184;
	add.f64 	%fd197, %fd164, %fd185;
	add.f64 	%fd198, %fd165, %fd186;
	add.f64 	%fd199, %fd184, %fd181;
	add.f64 	%fd200, %fd185, %fd182;
	add.f64 	%fd201, %fd186, %fd183;
	mul.f64 	%fd202, %fd4, %fd185;
	mul.f64 	%fd203, %fd5, %fd185;
	mul.f64 	%fd204, %fd6, %fd185;
	fma.rn.f64 	%fd205, %fd1, %fd184, %fd202;
	fma.rn.f64 	%fd206, %fd2, %fd184, %fd203;
	fma.rn.f64 	%fd207, %fd3, %fd184, %fd204;
	fma.rn.f64 	%fd208, %fd7, %fd186, %fd205;
	fma.rn.f64 	%fd209, %fd8, %fd186, %fd206;
	fma.rn.f64 	%fd210, %fd9, %fd186, %fd207;
	add.f64 	%fd211, %fd208, 0d0000000000000000;
	add.f64 	%fd212, %fd209, 0d0000000000000000;
	add.f64 	%fd213, %fd210, 0d0000000000000000;
	sub.f64 	%fd214, %fd199, %fd211;
	sub.f64 	%fd215, %fd200, %fd212;
	sub.f64 	%fd216, %fd201, %fd213;
	add.f64 	%fd217, %fd339, 0d0000000000000000;
	add.f64 	%fd218, %fd338, 0d0000000000000000;
	add.f64 	%fd219, %fd337, 0d0000000000000000;
	fma.rn.f64 	%fd220, %fd217, %fd109, %fd187;
	fma.rn.f64 	%fd221, %fd217, %fd110, %fd188;
	fma.rn.f64 	%fd222, %fd217, %fd111, %fd189;
	fma.rn.f64 	%fd223, %fd218, %fd109, %fd190;
	fma.rn.f64 	%fd224, %fd218, %fd110, %fd191;
	fma.rn.f64 	%fd225, %fd218, %fd111, %fd192;
	fma.rn.f64 	%fd226, %fd219, %fd109, %fd193;
	fma.rn.f64 	%fd227, %fd219, %fd110, %fd194;
	fma.rn.f64 	%fd228, %fd219, %fd111, %fd195;
	add.f64 	%fd229, %fd196, %fd217;
	add.f64 	%fd230, %fd197, %fd218;
	add.f64 	%fd231, %fd198, %fd219;
	add.f64 	%fd232, %fd217, %fd214;
	add.f64 	%fd233, %fd218, %fd215;
	add.f64 	%fd234, %fd219, %fd216;
	mul.f64 	%fd235, %fd4, %fd218;
	mul.f64 	%fd236, %fd5, %fd218;
	mul.f64 	%fd237, %fd6, %fd218;
	fma.rn.f64 	%fd238, %fd1, %fd217, %fd235;
	fma.rn.f64 	%fd239, %fd2, %fd217, %fd236;
	fma.rn.f64 	%fd240, %fd3, %fd217, %fd237;
	fma.rn.f64 	%fd241, %fd7, %fd219, %fd238;
	fma.rn.f64 	%fd242, %fd8, %fd219, %fd239;
	fma.rn.f64 	%fd243, %fd9, %fd219, %fd240;
	add.f64 	%fd244, %fd241, 0d0000000000000000;
	add.f64 	%fd245, %fd242, 0d0000000000000000;
	add.f64 	%fd246, %fd243, 0d0000000000000000;
	sub.f64 	%fd52, %fd232, %fd244;
	sub.f64 	%fd53, %fd233, %fd245;
	sub.f64 	%fd54, %fd234, %fd246;
	add.f64 	%fd55, %fd229, 0d0000000000000000;
	add.f64 	%fd56, %fd230, 0d0000000000000000;
	add.f64 	%fd57, %fd231, 0d0000000000000000;
	add.f64 	%fd58, %fd220, 0d0000000000000000;
	add.f64 	%fd59, %fd221, 0d0000000000000000;
	add.f64 	%fd60, %fd222, 0d0000000000000000;
	add.f64 	%fd61, %fd223, 0d0000000000000000;
	add.f64 	%fd62, %fd224, 0d0000000000000000;
	add.f64 	%fd63, %fd225, 0d0000000000000000;
	add.f64 	%fd64, %fd226, 0d0000000000000000;
	add.f64 	%fd65, %fd227, 0d0000000000000000;
	add.f64 	%fd66, %fd228, 0d0000000000000000;
	add.f64 	%fd247, %fd145, %fd244;
	add.f64 	%fd248, %fd146, %fd245;
	add.f64 	%fd249, %fd147, %fd246;
	add.f64 	%fd250, %fd178, %fd247;
	add.f64 	%fd251, %fd179, %fd248;
	add.f64 	%fd252, %fd180, %fd249;
	add.f64 	%fd253, %fd211, %fd250;
	add.f64 	%fd254, %fd212, %fd251;
	add.f64 	%fd255, %fd213, %fd252;
	add.f64 	%fd67, %fd253, 0d0000000000000000;
	add.f64 	%fd68, %fd254, 0d0000000000000000;
	add.f64 	%fd69, %fd255, 0d0000000000000000;
	setp.eq.s64 	%p12, %rd63, 0;
	@%p12 bra 	$L__BB5_21;

	mul.lo.s64 	%rd87, %rd41, %rd27;
	add.s64 	%rd84, %rd63, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd256,[%rd84],%fd67; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd258,[%rd85],%fd68; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd260,[%rd86],%fd69; }

	// end inline asm
	bra.uni 	$L__BB5_23;

$L__BB5_21:
	setp.eq.s64 	%p13, %rd56, 0;
	@%p13 bra 	$L__BB5_23;

	add.s64 	%rd88, %rd56, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd262,[%rd88],%fd67; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	// begin inline asm
	{ atom.add.f64 %fd264,[%rd89],%fd68; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd266,[%rd90],%fd69; }

	// end inline asm

$L__BB5_23:
	setp.eq.s64 	%p14, %rd61, 0;
	add.f64 	%fd70, %fd52, 0d0000000000000000;
	add.f64 	%fd71, %fd53, 0d0000000000000000;
	add.f64 	%fd72, %fd54, 0d0000000000000000;
	@%p14 bra 	$L__BB5_25;

	mul.lo.s64 	%rd94, %rd41, %rd28;
	add.s64 	%rd91, %rd61, %rd94;
	// begin inline asm
	{ atom.add.f64 %fd268,[%rd91],%fd70; }

	// end inline asm
	add.s64 	%rd92, %rd91, 8;
	// begin inline asm
	{ atom.add.f64 %fd270,[%rd92],%fd71; }

	// end inline asm
	add.s64 	%rd93, %rd91, 16;
	// begin inline asm
	{ atom.add.f64 %fd272,[%rd93],%fd72; }

	// end inline asm
	bra.uni 	$L__BB5_27;

$L__BB5_25:
	setp.eq.s64 	%p15, %rd54, 0;
	@%p15 bra 	$L__BB5_27;

	add.s64 	%rd95, %rd54, %rd43;
	// begin inline asm
	{ atom.add.f64 %fd274,[%rd95],%fd70; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd276,[%rd96],%fd71; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd278,[%rd97],%fd72; }

	// end inline asm

$L__BB5_27:
	setp.eq.s64 	%p16, %rd57, 0;
	@%p16 bra 	$L__BB5_29;

	mul.lo.s64 	%rd110, %rd41, %rd29;
	add.s64 	%rd98, %rd57, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd280,[%rd98],%fd58; }

	// end inline asm
	add.s64 	%rd99, %rd98, 8;
	// begin inline asm
	{ atom.add.f64 %fd282,[%rd99],%fd59; }

	// end inline asm
	add.s64 	%rd100, %rd98, 16;
	// begin inline asm
	{ atom.add.f64 %fd284,[%rd100],%fd60; }

	// end inline asm
	add.s64 	%rd101, %rd98, 24;
	// begin inline asm
	{ atom.add.f64 %fd286,[%rd101],%fd61; }

	// end inline asm
	add.s64 	%rd102, %rd98, 32;
	// begin inline asm
	{ atom.add.f64 %fd288,[%rd102],%fd62; }

	// end inline asm
	add.s64 	%rd103, %rd98, 40;
	// begin inline asm
	{ atom.add.f64 %fd290,[%rd103],%fd63; }

	// end inline asm
	add.s64 	%rd104, %rd98, 48;
	// begin inline asm
	{ atom.add.f64 %fd292,[%rd104],%fd64; }

	// end inline asm
	add.s64 	%rd105, %rd98, 56;
	// begin inline asm
	{ atom.add.f64 %fd294,[%rd105],%fd65; }

	// end inline asm
	add.s64 	%rd106, %rd98, 64;
	// begin inline asm
	{ atom.add.f64 %fd296,[%rd106],%fd66; }

	// end inline asm
	add.s64 	%rd107, %rd98, 72;
	// begin inline asm
	{ atom.add.f64 %fd298,[%rd107],%fd55; }

	// end inline asm
	add.s64 	%rd108, %rd98, 80;
	// begin inline asm
	{ atom.add.f64 %fd300,[%rd108],%fd56; }

	// end inline asm
	add.s64 	%rd109, %rd98, 88;
	// begin inline asm
	{ atom.add.f64 %fd302,[%rd109],%fd57; }

	// end inline asm
	bra.uni 	$L__BB5_31;

$L__BB5_29:
	setp.eq.s64 	%p17, %rd50, 0;
	@%p17 bra 	$L__BB5_31;

	add.s64 	%rd111, %rd50, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd304,[%rd111],%fd58; }

	// end inline asm
	add.s64 	%rd112, %rd111, 8;
	// begin inline asm
	{ atom.add.f64 %fd306,[%rd112],%fd59; }

	// end inline asm
	add.s64 	%rd113, %rd111, 16;
	// begin inline asm
	{ atom.add.f64 %fd308,[%rd113],%fd60; }

	// end inline asm
	add.s64 	%rd114, %rd111, 24;
	// begin inline asm
	{ atom.add.f64 %fd310,[%rd114],%fd61; }

	// end inline asm
	add.s64 	%rd115, %rd111, 32;
	// begin inline asm
	{ atom.add.f64 %fd312,[%rd115],%fd62; }

	// end inline asm
	add.s64 	%rd116, %rd111, 40;
	// begin inline asm
	{ atom.add.f64 %fd314,[%rd116],%fd63; }

	// end inline asm
	add.s64 	%rd117, %rd111, 48;
	// begin inline asm
	{ atom.add.f64 %fd316,[%rd117],%fd64; }

	// end inline asm
	add.s64 	%rd118, %rd111, 56;
	// begin inline asm
	{ atom.add.f64 %fd318,[%rd118],%fd65; }

	// end inline asm
	add.s64 	%rd119, %rd111, 64;
	// begin inline asm
	{ atom.add.f64 %fd320,[%rd119],%fd66; }

	// end inline asm
	add.s64 	%rd120, %rd111, 72;
	// begin inline asm
	{ atom.add.f64 %fd322,[%rd120],%fd55; }

	// end inline asm
	add.s64 	%rd121, %rd111, 80;
	// begin inline asm
	{ atom.add.f64 %fd324,[%rd121],%fd56; }

	// end inline asm
	add.s64 	%rd122, %rd111, 88;
	// begin inline asm
	{ atom.add.f64 %fd326,[%rd122],%fd57; }

	// end inline asm

$L__BB5_31:
	ld.param.u64 	%rd123, [init_affine_kinematic_target_kernel_cuda_kernel_backward_param_0+24];
	add.s64 	%rd126, %rd126, %rd21;
	setp.lt.u64 	%p18, %rd126, %rd123;
	@%p18 bra 	$L__BB5_2;

$L__BB5_32:
	ret;

}
	// .globl	compute_soft_kinematic_energy_cuda_kernel_forward
.visible .entry compute_soft_kinematic_energy_cuda_kernel_forward(
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_3[56],
	.param .f64 compute_soft_kinematic_energy_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_5[56],
	.param .u32 compute_soft_kinematic_energy_cuda_kernel_forward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_forward_param_7[56]
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<114>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<65>;


	ld.param.v2.u32 	{%r53, %r54}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r55, %r56}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r61, %r62}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r69, %r70}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r77, %r78}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd2, [compute_soft_kinematic_energy_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r85, %r86}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_5+32];
	ld.param.u32 	%r43, [compute_soft_kinematic_energy_cuda_kernel_forward_param_6];
	ld.param.v2.u32 	{%r93, %r94}, [compute_soft_kinematic_energy_cuda_kernel_forward_param_7+32];
	ld.param.u64 	%rd39, [compute_soft_kinematic_energy_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd37, [compute_soft_kinematic_energy_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd35, [compute_soft_kinematic_energy_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd33, [compute_soft_kinematic_energy_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd31, [compute_soft_kinematic_energy_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd30, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r6, [compute_soft_kinematic_energy_cuda_kernel_forward_param_0+16];
	mov.u32 	%r97, %ntid.x;
	cvt.u64.u32 	%rd1, %r97;
	mov.u32 	%r98, %ctaid.x;
	mul.wide.u32 	%rd41, %r97, %r98;
	mov.u32 	%r99, %tid.x;
	cvt.u64.u32 	%rd42, %r99;
	add.s64 	%rd61, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd61, %rd30;
	@%p1 bra 	$L__BB6_17;

	cvta.to.global.u64 	%rd4, %rd39;
	cvta.to.global.u64 	%rd6, %rd35;
	cvta.to.global.u64 	%rd7, %rd31;
	cvt.s64.s32 	%rd8, %r56;
	cvt.s64.s32 	%rd9, %r55;
	cvt.s64.s32 	%rd10, %r54;
	cvt.s64.s32 	%rd11, %r69;
	cvt.s64.s32 	%rd12, %r93;
	mov.u32 	%r100, %nctaid.x;
	cvt.u64.u32 	%rd43, %r100;
	mul.lo.s64 	%rd13, %rd1, %rd43;
	mul.f64 	%fd1, %fd2, 0d3FE0000000000000;
	cvt.s64.s32 	%rd14, %r61;
	cvt.s64.s32 	%rd15, %r77;
	cvt.s64.s32 	%rd16, %r85;
	cvta.to.global.u64 	%rd17, %rd33;

$L__BB6_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd62, %rd61;
	@%p2 bra 	$L__BB6_6;

	or.b64  	%rd44, %rd61, %rd8;
	and.b64  	%rd45, %rd44, -4294967296;
	setp.eq.s64 	%p3, %rd45, 0;
	@%p3 bra 	$L__BB6_5;

	div.u64 	%rd62, %rd61, %rd8;
	bra.uni 	$L__BB6_6;

$L__BB6_5:
	cvt.u32.u64 	%r101, %rd8;
	cvt.u32.u64 	%r102, %rd61;
	div.u32 	%r103, %r102, %r101;
	cvt.u64.u32 	%rd62, %r103;

$L__BB6_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB6_10;

	or.b64  	%rd46, %rd62, %rd9;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p5, %rd47, 0;
	@%p5 bra 	$L__BB6_9;

	div.u64 	%rd62, %rd62, %rd9;
	bra.uni 	$L__BB6_10;

$L__BB6_9:
	cvt.u32.u64 	%r104, %rd9;
	cvt.u32.u64 	%r105, %rd62;
	div.u32 	%r106, %r105, %r104;
	cvt.u64.u32 	%rd62, %r106;

$L__BB6_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB6_14;

	or.b64  	%rd48, %rd62, %rd10;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p7, %rd49, 0;
	@%p7 bra 	$L__BB6_13;

	div.u64 	%rd62, %rd62, %rd10;
	bra.uni 	$L__BB6_14;

$L__BB6_13:
	cvt.u32.u64 	%r107, %rd10;
	cvt.u32.u64 	%r108, %rd62;
	div.u32 	%r109, %r108, %r107;
	cvt.u64.u32 	%rd62, %r109;

$L__BB6_14:
	cvt.u32.u64 	%r110, %rd62;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r111, %r110, 0, %p8;
	cvt.s64.s32 	%rd28, %r111;
	mul.lo.s64 	%rd50, %rd28, %rd11;
	add.s64 	%rd51, %rd17, %rd50;
	ld.global.u8 	%rs41, [%rd51];
	setp.eq.s16 	%p9, %rs41, 0;
	@%p9 bra 	$L__BB6_16;

	cvt.u32.u64 	%r112, %rd28;
	mul.lo.s64 	%rd53, %rd28, %rd12;
	add.s64 	%rd54, %rd4, %rd53;
	ld.global.f64 	%fd5, [%rd54];
	mul.f64 	%fd6, %fd1, %fd5;
	add.s32 	%r113, %r112, %r43;
	cvt.s64.s32 	%rd55, %r113;
	mul.lo.s64 	%rd56, %rd55, %rd14;
	add.s64 	%rd57, %rd7, %rd56;
	mul.lo.s64 	%rd58, %rd28, %rd15;
	add.s64 	%rd59, %rd6, %rd58;
	ld.global.f64 	%fd7, [%rd59];
	ld.global.f64 	%fd8, [%rd57];
	sub.f64 	%fd9, %fd8, %fd7;
	ld.global.f64 	%fd10, [%rd59+8];
	ld.global.f64 	%fd11, [%rd57+8];
	sub.f64 	%fd12, %fd11, %fd10;
	ld.global.f64 	%fd13, [%rd59+16];
	ld.global.f64 	%fd14, [%rd57+16];
	sub.f64 	%fd15, %fd14, %fd13;
	mul.f64 	%fd16, %fd12, %fd12;
	fma.rn.f64 	%fd17, %fd9, %fd9, %fd16;
	fma.rn.f64 	%fd18, %fd15, %fd15, %fd17;
	mul.f64 	%fd4, %fd6, %fd18;
	mul.lo.s64 	%rd60, %rd28, %rd16;
	add.s64 	%rd52, %rd37, %rd60;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd52],%fd4; }

	// end inline asm

$L__BB6_16:
	add.s64 	%rd61, %rd61, %rd13;
	setp.lt.u64 	%p10, %rd61, %rd30;
	@%p10 bra 	$L__BB6_2;

$L__BB6_17:
	ret;

}
	// .globl	compute_soft_kinematic_energy_cuda_kernel_backward
.visible .entry compute_soft_kinematic_energy_cuda_kernel_backward(
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_3[56],
	.param .f64 compute_soft_kinematic_energy_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_5[56],
	.param .u32 compute_soft_kinematic_energy_cuda_kernel_backward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_10[56],
	.param .f64 compute_soft_kinematic_energy_cuda_kernel_backward_param_11,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_12[56],
	.param .u32 compute_soft_kinematic_energy_cuda_kernel_backward_param_13,
	.param .align 8 .b8 compute_soft_kinematic_energy_cuda_kernel_backward_param_14[56]
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<74>;
	.reg .b32 	%r<183>;
	.reg .f64 	%fd<69>;
	.reg .b64 	%rd<108>;


	ld.param.v2.u32 	{%r89, %r90}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r91, %r92}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r97, %r98}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r105, %r106}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r113, %r114}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd20, [compute_soft_kinematic_energy_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r121, %r122}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r129, %r130}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r137, %r138}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r145, %r146}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r153, %r154}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r161, %r162}, [compute_soft_kinematic_energy_cuda_kernel_backward_param_14+32];
	ld.param.u64 	%rd64, [compute_soft_kinematic_energy_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd62, [compute_soft_kinematic_energy_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd60, [compute_soft_kinematic_energy_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd58, [compute_soft_kinematic_energy_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd57, [compute_soft_kinematic_energy_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd56, [compute_soft_kinematic_energy_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd55, [compute_soft_kinematic_energy_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd53, [compute_soft_kinematic_energy_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd52, [compute_soft_kinematic_energy_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd50, [compute_soft_kinematic_energy_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd49, [compute_soft_kinematic_energy_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd48, [compute_soft_kinematic_energy_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd47, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r6, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+16];
	mov.u32 	%r165, %ntid.x;
	cvt.u64.u32 	%rd1, %r165;
	mov.u32 	%r166, %ctaid.x;
	mul.wide.u32 	%rd66, %r165, %r166;
	mov.u32 	%r167, %tid.x;
	cvt.u64.u32 	%rd67, %r167;
	add.s64 	%rd104, %rd66, %rd67;
	setp.ge.u64 	%p1, %rd104, %rd47;
	@%p1 bra 	$L__BB7_33;

	cvta.to.global.u64 	%rd12, %rd62;
	cvta.to.global.u64 	%rd13, %rd56;
	cvta.to.global.u64 	%rd14, %rd55;
	cvta.to.global.u64 	%rd15, %rd52;
	cvta.to.global.u64 	%rd16, %rd48;
	cvt.s64.s32 	%rd17, %r92;
	cvt.s64.s32 	%rd18, %r91;
	cvt.s64.s32 	%rd19, %r90;
	cvt.s64.s32 	%rd20, %r105;
	cvt.s64.s32 	%rd21, %r129;
	mul.f64 	%fd1, %fd20, 0d3FE0000000000000;
	cvt.s64.s32 	%rd22, %r97;
	cvt.s64.s32 	%rd23, %r113;
	mov.u32 	%r168, %nctaid.x;
	cvt.u64.u32 	%rd68, %r168;
	mul.lo.s64 	%rd24, %rd1, %rd68;
	cvt.s64.s32 	%rd25, %r161;
	cvt.s64.s32 	%rd26, %r153;
	cvt.s64.s32 	%rd27, %r121;
	cvt.s64.s32 	%rd28, %r145;
	cvt.s64.s32 	%rd29, %r137;
	cvta.to.global.u64 	%rd30, %rd50;

$L__BB7_2:
	setp.lt.s32 	%p2, %r6, 4;
	mov.u64 	%rd105, %rd104;
	@%p2 bra 	$L__BB7_6;

	or.b64  	%rd69, %rd104, %rd17;
	and.b64  	%rd70, %rd69, -4294967296;
	setp.eq.s64 	%p3, %rd70, 0;
	@%p3 bra 	$L__BB7_5;

	div.u64 	%rd105, %rd104, %rd17;
	bra.uni 	$L__BB7_6;

$L__BB7_5:
	cvt.u32.u64 	%r169, %rd17;
	cvt.u32.u64 	%r170, %rd104;
	div.u32 	%r171, %r170, %r169;
	cvt.u64.u32 	%rd105, %r171;

$L__BB7_6:
	setp.lt.s32 	%p4, %r6, 3;
	@%p4 bra 	$L__BB7_10;

	or.b64  	%rd71, %rd105, %rd18;
	and.b64  	%rd72, %rd71, -4294967296;
	setp.eq.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB7_9;

	div.u64 	%rd105, %rd105, %rd18;
	bra.uni 	$L__BB7_10;

$L__BB7_9:
	cvt.u32.u64 	%r172, %rd18;
	cvt.u32.u64 	%r173, %rd105;
	div.u32 	%r174, %r173, %r172;
	cvt.u64.u32 	%rd105, %r174;

$L__BB7_10:
	setp.lt.s32 	%p6, %r6, 2;
	@%p6 bra 	$L__BB7_14;

	or.b64  	%rd73, %rd105, %rd19;
	and.b64  	%rd74, %rd73, -4294967296;
	setp.eq.s64 	%p7, %rd74, 0;
	@%p7 bra 	$L__BB7_13;

	div.u64 	%rd105, %rd105, %rd19;
	bra.uni 	$L__BB7_14;

$L__BB7_13:
	cvt.u32.u64 	%r175, %rd19;
	cvt.u32.u64 	%r176, %rd105;
	div.u32 	%r177, %r176, %r175;
	cvt.u64.u32 	%rd105, %r177;

$L__BB7_14:
	cvt.u32.u64 	%r178, %rd105;
	setp.gt.s32 	%p8, %r6, 0;
	selp.b32 	%r179, %r178, 0, %p8;
	cvt.s64.s32 	%rd41, %r179;
	mul.lo.s64 	%rd75, %rd41, %rd20;
	add.s64 	%rd76, %rd30, %rd75;
	mul.lo.s64 	%rd42, %rd41, %rd21;
	ld.global.u8 	%rs73, [%rd76];
	setp.eq.s16 	%p9, %rs73, 0;
	mov.f64 	%fd68, 0d0000000000000000;
	@%p9 bra 	$L__BB7_28;

	ld.param.u32 	%r182, [compute_soft_kinematic_energy_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd102, [compute_soft_kinematic_energy_cuda_kernel_backward_param_12];
	cvt.u32.u64 	%r180, %rd41;
	add.s64 	%rd77, %rd13, %rd42;
	ld.global.f64 	%fd2, [%rd77];
	add.s32 	%r181, %r180, %r182;
	cvt.s64.s32 	%rd43, %r181;
	mul.lo.s64 	%rd44, %rd43, %rd22;
	add.s64 	%rd78, %rd16, %rd44;
	mul.lo.s64 	%rd45, %rd41, %rd23;
	add.s64 	%rd79, %rd15, %rd45;
	ld.global.f64 	%fd22, [%rd79];
	ld.global.f64 	%fd23, [%rd78];
	sub.f64 	%fd3, %fd23, %fd22;
	ld.global.f64 	%fd24, [%rd79+8];
	ld.global.f64 	%fd25, [%rd78+8];
	sub.f64 	%fd4, %fd25, %fd24;
	ld.global.f64 	%fd26, [%rd79+16];
	ld.global.f64 	%fd27, [%rd78+16];
	sub.f64 	%fd5, %fd27, %fd26;
	mul.f64 	%fd28, %fd4, %fd4;
	fma.rn.f64 	%fd29, %fd3, %fd3, %fd28;
	fma.rn.f64 	%fd6, %fd5, %fd5, %fd29;
	setp.eq.s64 	%p10, %rd102, 0;
	@%p10 bra 	$L__BB7_17;

	mul.lo.s64 	%rd80, %rd41, %rd26;
	add.s64 	%rd81, %rd12, %rd80;
	ld.global.f64 	%fd30, [%rd81];
	add.f64 	%fd67, %fd30, 0d0000000000000000;
	bra.uni 	$L__BB7_19;

$L__BB7_17:
	ld.param.u64 	%rd103, [compute_soft_kinematic_energy_cuda_kernel_backward_param_5+8];
	setp.eq.s64 	%p11, %rd103, 0;
	mov.f64 	%fd67, 0d0000000000000000;
	@%p11 bra 	$L__BB7_19;

	mul.lo.s64 	%rd82, %rd41, %rd27;
	add.s64 	%rd83, %rd14, %rd82;
	ld.global.f64 	%fd32, [%rd83];
	add.f64 	%fd67, %fd32, 0d0000000000000000;

$L__BB7_19:
	fma.rn.f64 	%fd10, %fd6, %fd67, 0d0000000000000000;
	mov.f64 	%fd33, 0d0000000000000000;
	mul.f64 	%fd34, %fd1, %fd2;
	fma.rn.f64 	%fd35, %fd34, %fd67, 0d0000000000000000;
	add.f64 	%fd36, %fd3, %fd3;
	add.f64 	%fd37, %fd4, %fd4;
	add.f64 	%fd38, %fd5, %fd5;
	fma.rn.f64 	%fd11, %fd36, %fd35, 0d0000000000000000;
	fma.rn.f64 	%fd12, %fd37, %fd35, 0d0000000000000000;
	fma.rn.f64 	%fd13, %fd38, %fd35, 0d0000000000000000;
	sub.f64 	%fd14, %fd33, %fd11;
	sub.f64 	%fd15, %fd33, %fd12;
	sub.f64 	%fd16, %fd33, %fd13;
	setp.eq.s64 	%p12, %rd60, 0;
	@%p12 bra 	$L__BB7_21;

	mul.lo.s64 	%rd87, %rd41, %rd28;
	add.s64 	%rd84, %rd60, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd84],%fd14; }

	// end inline asm
	add.s64 	%rd85, %rd84, 8;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd85],%fd15; }

	// end inline asm
	add.s64 	%rd86, %rd84, 16;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd86],%fd16; }

	// end inline asm
	bra.uni 	$L__BB7_23;

$L__BB7_21:
	setp.eq.s64 	%p13, %rd53, 0;
	@%p13 bra 	$L__BB7_23;

	add.s64 	%rd88, %rd53, %rd45;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd88],%fd14; }

	// end inline asm
	add.s64 	%rd89, %rd88, 8;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd89],%fd15; }

	// end inline asm
	add.s64 	%rd90, %rd88, 16;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd90],%fd16; }

	// end inline asm

$L__BB7_23:
	setp.eq.s64 	%p14, %rd58, 0;
	@%p14 bra 	$L__BB7_25;

	mul.lo.s64 	%rd94, %rd43, %rd29;
	add.s64 	%rd91, %rd58, %rd94;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd91],%fd11; }

	// end inline asm
	add.s64 	%rd92, %rd91, 8;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd92],%fd12; }

	// end inline asm
	add.s64 	%rd93, %rd91, 16;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd93],%fd13; }

	// end inline asm
	bra.uni 	$L__BB7_27;

$L__BB7_25:
	setp.eq.s64 	%p15, %rd49, 0;
	@%p15 bra 	$L__BB7_27;

	add.s64 	%rd95, %rd49, %rd44;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd95],%fd11; }

	// end inline asm
	add.s64 	%rd96, %rd95, 8;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd96],%fd12; }

	// end inline asm
	add.s64 	%rd97, %rd95, 16;
	// begin inline asm
	{ atom.add.f64 %fd61,[%rd97],%fd13; }

	// end inline asm

$L__BB7_27:
	fma.rn.f64 	%fd68, %fd1, %fd10, 0d0000000000000000;

$L__BB7_28:
	add.f64 	%fd19, %fd68, 0d0000000000000000;
	setp.eq.s64 	%p16, %rd64, 0;
	@%p16 bra 	$L__BB7_30;

	mul.lo.s64 	%rd99, %rd41, %rd25;
	add.s64 	%rd98, %rd64, %rd99;
	// begin inline asm
	{ atom.add.f64 %fd63,[%rd98],%fd19; }

	// end inline asm
	bra.uni 	$L__BB7_32;

$L__BB7_30:
	setp.eq.s64 	%p17, %rd57, 0;
	@%p17 bra 	$L__BB7_32;

	add.s64 	%rd100, %rd57, %rd42;
	// begin inline asm
	{ atom.add.f64 %fd65,[%rd100],%fd19; }

	// end inline asm

$L__BB7_32:
	ld.param.u64 	%rd101, [compute_soft_kinematic_energy_cuda_kernel_backward_param_0+24];
	add.s64 	%rd104, %rd104, %rd24;
	setp.lt.u64 	%p18, %rd104, %rd101;
	@%p18 bra 	$L__BB7_2;

$L__BB7_33:
	ret;

}
	// .globl	compute_affine_kinematic_hess_cuda_kernel_forward
.visible .entry compute_affine_kinematic_hess_cuda_kernel_forward(
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_1[56],
	.param .f64 compute_affine_kinematic_hess_cuda_kernel_forward_param_2,
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_3[184],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_4[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_forward_param_6[56]
)
{
	.local .align 8 .b8 	__local_depot8[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<47>;
	.reg .b32 	%r<120>;
	.reg .f64 	%fd<76>;
	.reg .b64 	%rd<121>;


	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r51, %r52}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r53, %r54}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r59, %r60}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_1+32];
	ld.param.f64 	%fd2, [compute_affine_kinematic_hess_cuda_kernel_forward_param_2];
	mov.b64 	%rd35, compute_affine_kinematic_hess_cuda_kernel_forward_param_3;
	ld.param.v2.u32 	{%r67, %r68}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_4+32];
	ld.param.v2.u32 	{%r75, %r76}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r83, %r84}, [compute_affine_kinematic_hess_cuda_kernel_forward_param_6+32];
	ld.param.u64 	%rd40, [compute_affine_kinematic_hess_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd38, [compute_affine_kinematic_hess_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd36, [compute_affine_kinematic_hess_cuda_kernel_forward_param_4];
	ld.param.u64 	%rd33, [compute_affine_kinematic_hess_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd32, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r14, [compute_affine_kinematic_hess_cuda_kernel_forward_param_0+16];
	mov.u32 	%r87, %ntid.x;
	cvt.u64.u32 	%rd1, %r87;
	mov.u32 	%r88, %ctaid.x;
	mul.wide.u32 	%rd42, %r87, %r88;
	mov.u32 	%r89, %tid.x;
	cvt.u64.u32 	%rd43, %r89;
	add.s64 	%rd117, %rd42, %rd43;
	setp.ge.u64 	%p1, %rd117, %rd32;
	@%p1 bra 	$L__BB8_29;

	add.u64 	%rd44, %SP, 0;
	add.u64 	%rd4, %SPL, 0;
	cvta.to.global.u64 	%rd5, %rd40;
	cvta.to.global.u64 	%rd6, %rd38;
	cvta.to.global.u64 	%rd7, %rd36;
	cvt.s64.s32 	%rd8, %r54;
	cvt.s64.s32 	%rd9, %r53;
	cvt.s64.s32 	%rd10, %r52;
	cvt.s64.s32 	%rd11, %r75;
	cvt.s64.s32 	%rd12, %r83;
	mov.u32 	%r90, %nctaid.x;
	cvt.u64.u32 	%rd45, %r90;
	mul.lo.s64 	%rd13, %rd1, %rd45;
	cvt.s64.s32 	%rd14, %r67;
	cvt.s64.s32 	%rd15, %r59;
	cvta.to.global.u64 	%rd16, %rd33;

$L__BB8_2:
	setp.lt.s32 	%p2, %r14, 4;
	mov.u64 	%rd118, %rd117;
	@%p2 bra 	$L__BB8_6;

	or.b64  	%rd46, %rd117, %rd8;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.eq.s64 	%p3, %rd47, 0;
	@%p3 bra 	$L__BB8_5;

	div.u64 	%rd118, %rd117, %rd8;
	bra.uni 	$L__BB8_6;

$L__BB8_5:
	cvt.u32.u64 	%r91, %rd8;
	cvt.u32.u64 	%r92, %rd117;
	div.u32 	%r93, %r92, %r91;
	cvt.u64.u32 	%rd118, %r93;

$L__BB8_6:
	setp.lt.s32 	%p4, %r14, 3;
	@%p4 bra 	$L__BB8_10;

	or.b64  	%rd48, %rd118, %rd9;
	and.b64  	%rd49, %rd48, -4294967296;
	setp.eq.s64 	%p5, %rd49, 0;
	@%p5 bra 	$L__BB8_9;

	div.u64 	%rd118, %rd118, %rd9;
	bra.uni 	$L__BB8_10;

$L__BB8_9:
	cvt.u32.u64 	%r94, %rd9;
	cvt.u32.u64 	%r95, %rd118;
	div.u32 	%r96, %r95, %r94;
	cvt.u64.u32 	%rd118, %r96;

$L__BB8_10:
	setp.lt.s32 	%p6, %r14, 2;
	@%p6 bra 	$L__BB8_14;

	or.b64  	%rd50, %rd118, %rd10;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.eq.s64 	%p7, %rd51, 0;
	@%p7 bra 	$L__BB8_13;

	div.u64 	%rd118, %rd118, %rd10;
	bra.uni 	$L__BB8_14;

$L__BB8_13:
	cvt.u32.u64 	%r97, %rd10;
	cvt.u32.u64 	%r98, %rd118;
	div.u32 	%r99, %r98, %r97;
	cvt.u64.u32 	%rd118, %r99;

$L__BB8_14:
	cvt.u32.u64 	%r100, %rd118;
	setp.gt.s32 	%p8, %r14, 0;
	selp.b32 	%r101, %r100, 0, %p8;
	cvt.s64.s32 	%rd28, %r101;
	mul.lo.s64 	%rd52, %rd28, %rd11;
	add.s64 	%rd53, %rd6, %rd52;
	ld.global.s32 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd12;
	add.s64 	%rd56, %rd5, %rd55;
	ld.global.u32 	%r102, [%rd56];
	add.s32 	%r103, %r102, -1;
	setp.lt.u32 	%p9, %r103, 2;
	@%p9 bra 	$L__BB8_28;

	mul.lo.s64 	%rd57, %rd28, %rd14;
	add.s64 	%rd58, %rd7, %rd57;
	mul.lo.s64 	%rd59, %rd28, %rd15;
	add.s64 	%rd60, %rd16, %rd59;
	ld.global.f64 	%fd3, [%rd58];
	mul.f64 	%fd1, %fd3, %fd2;
	ld.global.u8 	%rs33, [%rd60];
	setp.eq.s16 	%p10, %rs33, 0;
	@%p10 bra 	$L__BB8_28;

	cvt.u32.u64 	%r104, %rd28;
	shl.b32 	%r2, %r104, 4;
	ld.param.u32 	%r3, [%rd35+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs34, 1, 0, %p11;
	shr.u32 	%r105, %r104, 27;
	cvt.u16.u32 	%rs35, %r105;
	and.b16  	%rs36, %rs35, 1;
	or.b16  	%rs37, %rs36, %rs34;
	setp.eq.s16 	%p12, %rs37, 0;
	@%p12 bra 	$L__BB8_18;

	st.local.v2.u32 	[%rd4], {%r2, %r3};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 79, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r106, [retval0+0];
	} // callseq 79
	bra.uni 	$L__BB8_19;

$L__BB8_18:
	ld.param.u32 	%r107, [%rd35+144];
	mul.wide.s32 	%rd73, %r107, %r2;
	ld.param.u64 	%rd74, [%rd35+112];
	add.s64 	%rd64, %rd74, %rd73;
	// begin inline asm
	{ atom.add.f64 %fd4,[%rd64],%fd1; }

	// end inline asm
	add.s64 	%rd65, %rd64, 8;
	mov.f64 	%fd19, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd6,[%rd65],%fd19; }

	// end inline asm
	add.s64 	%rd66, %rd64, 16;
	// begin inline asm
	{ atom.add.f64 %fd8,[%rd66],%fd19; }

	// end inline asm
	add.s64 	%rd67, %rd64, 24;
	// begin inline asm
	{ atom.add.f64 %fd10,[%rd67],%fd19; }

	// end inline asm
	add.s64 	%rd68, %rd64, 32;
	// begin inline asm
	{ atom.add.f64 %fd12,[%rd68],%fd1; }

	// end inline asm
	add.s64 	%rd69, %rd64, 40;
	// begin inline asm
	{ atom.add.f64 %fd14,[%rd69],%fd19; }

	// end inline asm
	add.s64 	%rd70, %rd64, 48;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd70],%fd19; }

	// end inline asm
	add.s64 	%rd71, %rd64, 56;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd71],%fd19; }

	// end inline asm
	add.s64 	%rd72, %rd64, 64;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd72],%fd1; }

	// end inline asm

$L__BB8_19:
	ld.param.u32 	%r4, [%rd35+172];
	add.s32 	%r5, %r2, 5;
	setp.le.s32 	%p13, %r4, %r5;
	selp.u16 	%rs38, 1, 0, %p13;
	shr.u32 	%r108, %r5, 31;
	cvt.u16.u32 	%rs39, %r108;
	or.b16  	%rs40, %rs38, %rs39;
	setp.eq.s16 	%p14, %rs40, 0;
	@%p14 bra 	$L__BB8_21;

	add.s32 	%r117, %r2, 5;
	st.local.v2.u32 	[%rd4], {%r117, %r4};
	mov.u64 	%rd75, $str;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 80, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd76;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r109, [retval0+0];
	} // callseq 80
	bra.uni 	$L__BB8_22;

$L__BB8_21:
	ld.param.u32 	%r110, [%rd35+144];
	mul.wide.s32 	%rd87, %r110, %r5;
	ld.param.u64 	%rd88, [%rd35+112];
	add.s64 	%rd78, %rd88, %rd87;
	// begin inline asm
	{ atom.add.f64 %fd22,[%rd78],%fd1; }

	// end inline asm
	add.s64 	%rd79, %rd78, 8;
	mov.f64 	%fd37, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd24,[%rd79],%fd37; }

	// end inline asm
	add.s64 	%rd80, %rd78, 16;
	// begin inline asm
	{ atom.add.f64 %fd26,[%rd80],%fd37; }

	// end inline asm
	add.s64 	%rd81, %rd78, 24;
	// begin inline asm
	{ atom.add.f64 %fd28,[%rd81],%fd37; }

	// end inline asm
	add.s64 	%rd82, %rd78, 32;
	// begin inline asm
	{ atom.add.f64 %fd30,[%rd82],%fd1; }

	// end inline asm
	add.s64 	%rd83, %rd78, 40;
	// begin inline asm
	{ atom.add.f64 %fd32,[%rd83],%fd37; }

	// end inline asm
	add.s64 	%rd84, %rd78, 48;
	// begin inline asm
	{ atom.add.f64 %fd34,[%rd84],%fd37; }

	// end inline asm
	add.s64 	%rd85, %rd78, 56;
	// begin inline asm
	{ atom.add.f64 %fd36,[%rd85],%fd37; }

	// end inline asm
	add.s64 	%rd86, %rd78, 64;
	// begin inline asm
	{ atom.add.f64 %fd38,[%rd86],%fd1; }

	// end inline asm

$L__BB8_22:
	ld.param.u32 	%r6, [%rd35+172];
	add.s32 	%r7, %r2, 10;
	setp.le.s32 	%p15, %r6, %r7;
	selp.u16 	%rs41, 1, 0, %p15;
	shr.u32 	%r111, %r7, 31;
	cvt.u16.u32 	%rs42, %r111;
	or.b16  	%rs43, %rs41, %rs42;
	setp.eq.s16 	%p16, %rs43, 0;
	@%p16 bra 	$L__BB8_24;

	add.s32 	%r118, %r2, 10;
	st.local.v2.u32 	[%rd4], {%r118, %r6};
	mov.u64 	%rd89, $str;
	cvta.global.u64 	%rd90, %rd89;
	{ // callseq 81, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd90;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r112, [retval0+0];
	} // callseq 81
	bra.uni 	$L__BB8_25;

$L__BB8_24:
	ld.param.u32 	%r113, [%rd35+144];
	mul.wide.s32 	%rd101, %r113, %r7;
	ld.param.u64 	%rd102, [%rd35+112];
	add.s64 	%rd92, %rd102, %rd101;
	// begin inline asm
	{ atom.add.f64 %fd40,[%rd92],%fd1; }

	// end inline asm
	add.s64 	%rd93, %rd92, 8;
	mov.f64 	%fd55, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd42,[%rd93],%fd55; }

	// end inline asm
	add.s64 	%rd94, %rd92, 16;
	// begin inline asm
	{ atom.add.f64 %fd44,[%rd94],%fd55; }

	// end inline asm
	add.s64 	%rd95, %rd92, 24;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd95],%fd55; }

	// end inline asm
	add.s64 	%rd96, %rd92, 32;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd96],%fd1; }

	// end inline asm
	add.s64 	%rd97, %rd92, 40;
	// begin inline asm
	{ atom.add.f64 %fd50,[%rd97],%fd55; }

	// end inline asm
	add.s64 	%rd98, %rd92, 48;
	// begin inline asm
	{ atom.add.f64 %fd52,[%rd98],%fd55; }

	// end inline asm
	add.s64 	%rd99, %rd92, 56;
	// begin inline asm
	{ atom.add.f64 %fd54,[%rd99],%fd55; }

	// end inline asm
	add.s64 	%rd100, %rd92, 64;
	// begin inline asm
	{ atom.add.f64 %fd56,[%rd100],%fd1; }

	// end inline asm

$L__BB8_25:
	ld.param.u32 	%r8, [%rd35+172];
	add.s32 	%r9, %r2, 15;
	setp.le.s32 	%p17, %r8, %r9;
	selp.u16 	%rs44, 1, 0, %p17;
	shr.u32 	%r114, %r9, 31;
	cvt.u16.u32 	%rs45, %r114;
	or.b16  	%rs46, %rs44, %rs45;
	setp.eq.s16 	%p18, %rs46, 0;
	@%p18 bra 	$L__BB8_27;

	add.s32 	%r119, %r2, 15;
	st.local.v2.u32 	[%rd4], {%r119, %r8};
	mov.u64 	%rd103, $str;
	cvta.global.u64 	%rd104, %rd103;
	{ // callseq 82, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd104;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd44;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r115, [retval0+0];
	} // callseq 82
	bra.uni 	$L__BB8_28;

$L__BB8_27:
	ld.param.u32 	%r116, [%rd35+144];
	mul.wide.s32 	%rd115, %r116, %r9;
	ld.param.u64 	%rd116, [%rd35+112];
	add.s64 	%rd106, %rd116, %rd115;
	// begin inline asm
	{ atom.add.f64 %fd58,[%rd106],%fd1; }

	// end inline asm
	add.s64 	%rd107, %rd106, 8;
	mov.f64 	%fd73, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd60,[%rd107],%fd73; }

	// end inline asm
	add.s64 	%rd108, %rd106, 16;
	// begin inline asm
	{ atom.add.f64 %fd62,[%rd108],%fd73; }

	// end inline asm
	add.s64 	%rd109, %rd106, 24;
	// begin inline asm
	{ atom.add.f64 %fd64,[%rd109],%fd73; }

	// end inline asm
	add.s64 	%rd110, %rd106, 32;
	// begin inline asm
	{ atom.add.f64 %fd66,[%rd110],%fd1; }

	// end inline asm
	add.s64 	%rd111, %rd106, 40;
	// begin inline asm
	{ atom.add.f64 %fd68,[%rd111],%fd73; }

	// end inline asm
	add.s64 	%rd112, %rd106, 48;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd112],%fd73; }

	// end inline asm
	add.s64 	%rd113, %rd106, 56;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd113],%fd73; }

	// end inline asm
	add.s64 	%rd114, %rd106, 64;
	// begin inline asm
	{ atom.add.f64 %fd74,[%rd114],%fd1; }

	// end inline asm

$L__BB8_28:
	add.s64 	%rd117, %rd117, %rd13;
	setp.lt.u64 	%p19, %rd117, %rd32;
	@%p19 bra 	$L__BB8_2;

$L__BB8_29:
	ret;

}
	// .globl	compute_affine_kinematic_hess_cuda_kernel_backward
.visible .entry compute_affine_kinematic_hess_cuda_kernel_backward(
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_1[56],
	.param .f64 compute_affine_kinematic_hess_cuda_kernel_backward_param_2,
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_3[184],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_4[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_7[56],
	.param .f64 compute_affine_kinematic_hess_cuda_kernel_backward_param_8,
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_9[184],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 compute_affine_kinematic_hess_cuda_kernel_backward_param_12[56]
)
{
	.local .align 8 .b8 	__local_depot9[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<63>;
	.reg .b32 	%r<152>;
	.reg .f64 	%fd<150>;
	.reg .b64 	%rd<158>;


	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r64, %r65}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r66, %r67}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r72, %r73}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_1+32];
	ld.param.f64 	%fd30, [compute_affine_kinematic_hess_cuda_kernel_backward_param_2];
	mov.b64 	%rd43, compute_affine_kinematic_hess_cuda_kernel_backward_param_3;
	ld.param.v2.u32 	{%r80, %r81}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_4+32];
	ld.param.v2.u32 	{%r88, %r89}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r96, %r97}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r104, %r105}, [compute_affine_kinematic_hess_cuda_kernel_backward_param_10+32];
	ld.param.u64 	%rd50, [compute_affine_kinematic_hess_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd48, [compute_affine_kinematic_hess_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd46, [compute_affine_kinematic_hess_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd45, [compute_affine_kinematic_hess_cuda_kernel_backward_param_4+8];
	ld.param.u64 	%rd44, [compute_affine_kinematic_hess_cuda_kernel_backward_param_4];
	ld.param.u64 	%rd41, [compute_affine_kinematic_hess_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd40, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r18, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+16];
	mov.u32 	%r108, %ntid.x;
	cvt.u64.u32 	%rd1, %r108;
	mov.u32 	%r109, %ctaid.x;
	mul.wide.u32 	%rd52, %r108, %r109;
	mov.u32 	%r110, %tid.x;
	cvt.u64.u32 	%rd53, %r110;
	add.s64 	%rd154, %rd52, %rd53;
	setp.ge.u64 	%p1, %rd154, %rd40;
	@%p1 bra 	$L__BB9_49;

	add.u64 	%rd54, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	cvta.to.global.u64 	%rd7, %rd48;
	cvta.to.global.u64 	%rd8, %rd46;
	cvta.to.global.u64 	%rd9, %rd44;
	cvt.s64.s32 	%rd10, %r67;
	cvt.s64.s32 	%rd11, %r66;
	cvt.s64.s32 	%rd12, %r65;
	cvt.s64.s32 	%rd13, %r88;
	cvt.s64.s32 	%rd14, %r96;
	mov.u32 	%r111, %nctaid.x;
	cvt.u64.u32 	%rd55, %r111;
	mul.lo.s64 	%rd15, %rd1, %rd55;
	cvt.s64.s32 	%rd16, %r80;
	cvt.s64.s32 	%rd17, %r72;
	cvt.s64.s32 	%rd18, %r104;
	cvta.to.global.u64 	%rd19, %rd41;

$L__BB9_2:
	setp.lt.s32 	%p2, %r18, 4;
	mov.u64 	%rd155, %rd154;
	@%p2 bra 	$L__BB9_6;

	or.b64  	%rd56, %rd154, %rd10;
	and.b64  	%rd57, %rd56, -4294967296;
	setp.eq.s64 	%p3, %rd57, 0;
	@%p3 bra 	$L__BB9_5;

	div.u64 	%rd155, %rd154, %rd10;
	bra.uni 	$L__BB9_6;

$L__BB9_5:
	cvt.u32.u64 	%r112, %rd10;
	cvt.u32.u64 	%r113, %rd154;
	div.u32 	%r114, %r113, %r112;
	cvt.u64.u32 	%rd155, %r114;

$L__BB9_6:
	setp.lt.s32 	%p4, %r18, 3;
	@%p4 bra 	$L__BB9_10;

	or.b64  	%rd58, %rd155, %rd11;
	and.b64  	%rd59, %rd58, -4294967296;
	setp.eq.s64 	%p5, %rd59, 0;
	@%p5 bra 	$L__BB9_9;

	div.u64 	%rd155, %rd155, %rd11;
	bra.uni 	$L__BB9_10;

$L__BB9_9:
	cvt.u32.u64 	%r115, %rd11;
	cvt.u32.u64 	%r116, %rd155;
	div.u32 	%r117, %r116, %r115;
	cvt.u64.u32 	%rd155, %r117;

$L__BB9_10:
	setp.lt.s32 	%p6, %r18, 2;
	@%p6 bra 	$L__BB9_14;

	or.b64  	%rd60, %rd155, %rd12;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.eq.s64 	%p7, %rd61, 0;
	@%p7 bra 	$L__BB9_13;

	div.u64 	%rd155, %rd155, %rd12;
	bra.uni 	$L__BB9_14;

$L__BB9_13:
	cvt.u32.u64 	%r118, %rd12;
	cvt.u32.u64 	%r119, %rd155;
	div.u32 	%r120, %r119, %r118;
	cvt.u64.u32 	%rd155, %r120;

$L__BB9_14:
	cvt.u32.u64 	%r121, %rd155;
	setp.gt.s32 	%p8, %r18, 0;
	selp.b32 	%r122, %r121, 0, %p8;
	cvt.s64.s32 	%rd31, %r122;
	mul.lo.s64 	%rd62, %rd31, %rd13;
	add.s64 	%rd63, %rd8, %rd62;
	ld.global.s32 	%rd64, [%rd63];
	mul.lo.s64 	%rd65, %rd64, %rd14;
	add.s64 	%rd66, %rd7, %rd65;
	ld.global.u32 	%r123, [%rd66];
	add.s32 	%r124, %r123, -1;
	setp.lt.u32 	%p9, %r124, 2;
	@%p9 bra 	$L__BB9_48;

	mul.lo.s64 	%rd32, %rd31, %rd16;
	add.s64 	%rd67, %rd9, %rd32;
	mul.lo.s64 	%rd68, %rd31, %rd17;
	add.s64 	%rd69, %rd19, %rd68;
	ld.global.f64 	%fd34, [%rd67];
	mul.f64 	%fd1, %fd34, %fd30;
	ld.global.u8 	%rs45, [%rd69];
	setp.eq.s16 	%p10, %rs45, 0;
	mov.f64 	%fd138, 0d0000000000000000;
	mov.f64 	%fd139, %fd138;
	mov.f64 	%fd140, %fd138;
	@%p10 bra 	$L__BB9_44;

	cvt.u32.u64 	%r125, %rd31;
	shl.b32 	%r2, %r125, 4;
	ld.param.u32 	%r3, [%rd43+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs46, 1, 0, %p11;
	shr.u32 	%r126, %r125, 27;
	cvt.u16.u32 	%rs47, %r126;
	and.b16  	%rs1, %rs47, 1;
	or.b16  	%rs48, %rs1, %rs46;
	setp.eq.s16 	%p12, %rs48, 0;
	@%p12 bra 	$L__BB9_18;

	st.local.v2.u32 	[%rd6], {%r2, %r3};
	mov.u64 	%rd70, $str;
	cvta.global.u64 	%rd71, %rd70;
	{ // callseq 83, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd71;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r127, [retval0+0];
	} // callseq 83
	bra.uni 	$L__BB9_19;

$L__BB9_18:
	ld.param.u32 	%r128, [%rd43+144];
	mul.wide.s32 	%rd82, %r128, %r2;
	ld.param.u64 	%rd83, [%rd43+112];
	add.s64 	%rd73, %rd83, %rd82;
	// begin inline asm
	{ atom.add.f64 %fd35,[%rd73],%fd1; }

	// end inline asm
	add.s64 	%rd74, %rd73, 8;
	mov.f64 	%fd50, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd37,[%rd74],%fd50; }

	// end inline asm
	add.s64 	%rd75, %rd73, 16;
	// begin inline asm
	{ atom.add.f64 %fd39,[%rd75],%fd50; }

	// end inline asm
	add.s64 	%rd76, %rd73, 24;
	// begin inline asm
	{ atom.add.f64 %fd41,[%rd76],%fd50; }

	// end inline asm
	add.s64 	%rd77, %rd73, 32;
	// begin inline asm
	{ atom.add.f64 %fd43,[%rd77],%fd1; }

	// end inline asm
	add.s64 	%rd78, %rd73, 40;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd78],%fd50; }

	// end inline asm
	add.s64 	%rd79, %rd73, 48;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd79],%fd50; }

	// end inline asm
	add.s64 	%rd80, %rd73, 56;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd80],%fd50; }

	// end inline asm
	add.s64 	%rd81, %rd73, 64;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd81],%fd1; }

	// end inline asm

$L__BB9_19:
	ld.param.u32 	%r4, [%rd43+172];
	add.s32 	%r5, %r2, 5;
	setp.le.s32 	%p13, %r4, %r5;
	selp.u16 	%rs49, 1, 0, %p13;
	shr.u32 	%r129, %r5, 31;
	cvt.u16.u32 	%rs2, %r129;
	or.b16  	%rs50, %rs49, %rs2;
	setp.eq.s16 	%p14, %rs50, 0;
	@%p14 bra 	$L__BB9_21;

	add.s32 	%r146, %r2, 5;
	st.local.v2.u32 	[%rd6], {%r146, %r4};
	mov.u64 	%rd84, $str;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 84, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r130, [retval0+0];
	} // callseq 84
	bra.uni 	$L__BB9_22;

$L__BB9_21:
	ld.param.u32 	%r131, [%rd43+144];
	mul.wide.s32 	%rd96, %r131, %r5;
	ld.param.u64 	%rd97, [%rd43+112];
	add.s64 	%rd87, %rd97, %rd96;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd87],%fd1; }

	// end inline asm
	add.s64 	%rd88, %rd87, 8;
	mov.f64 	%fd68, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd88],%fd68; }

	// end inline asm
	add.s64 	%rd89, %rd87, 16;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd89],%fd68; }

	// end inline asm
	add.s64 	%rd90, %rd87, 24;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd90],%fd68; }

	// end inline asm
	add.s64 	%rd91, %rd87, 32;
	// begin inline asm
	{ atom.add.f64 %fd61,[%rd91],%fd1; }

	// end inline asm
	add.s64 	%rd92, %rd87, 40;
	// begin inline asm
	{ atom.add.f64 %fd63,[%rd92],%fd68; }

	// end inline asm
	add.s64 	%rd93, %rd87, 48;
	// begin inline asm
	{ atom.add.f64 %fd65,[%rd93],%fd68; }

	// end inline asm
	add.s64 	%rd94, %rd87, 56;
	// begin inline asm
	{ atom.add.f64 %fd67,[%rd94],%fd68; }

	// end inline asm
	add.s64 	%rd95, %rd87, 64;
	// begin inline asm
	{ atom.add.f64 %fd69,[%rd95],%fd1; }

	// end inline asm

$L__BB9_22:
	ld.param.u32 	%r6, [%rd43+172];
	add.s32 	%r7, %r2, 10;
	setp.le.s32 	%p15, %r6, %r7;
	selp.u16 	%rs51, 1, 0, %p15;
	shr.u32 	%r132, %r7, 31;
	cvt.u16.u32 	%rs3, %r132;
	or.b16  	%rs52, %rs51, %rs3;
	setp.eq.s16 	%p16, %rs52, 0;
	@%p16 bra 	$L__BB9_24;

	add.s32 	%r147, %r2, 10;
	st.local.v2.u32 	[%rd6], {%r147, %r6};
	mov.u64 	%rd98, $str;
	cvta.global.u64 	%rd99, %rd98;
	{ // callseq 85, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd99;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r133, [retval0+0];
	} // callseq 85
	bra.uni 	$L__BB9_25;

$L__BB9_24:
	ld.param.u32 	%r134, [%rd43+144];
	mul.wide.s32 	%rd110, %r134, %r7;
	ld.param.u64 	%rd111, [%rd43+112];
	add.s64 	%rd101, %rd111, %rd110;
	// begin inline asm
	{ atom.add.f64 %fd71,[%rd101],%fd1; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	mov.f64 	%fd86, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd73,[%rd102],%fd86; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd75,[%rd103],%fd86; }

	// end inline asm
	add.s64 	%rd104, %rd101, 24;
	// begin inline asm
	{ atom.add.f64 %fd77,[%rd104],%fd86; }

	// end inline asm
	add.s64 	%rd105, %rd101, 32;
	// begin inline asm
	{ atom.add.f64 %fd79,[%rd105],%fd1; }

	// end inline asm
	add.s64 	%rd106, %rd101, 40;
	// begin inline asm
	{ atom.add.f64 %fd81,[%rd106],%fd86; }

	// end inline asm
	add.s64 	%rd107, %rd101, 48;
	// begin inline asm
	{ atom.add.f64 %fd83,[%rd107],%fd86; }

	// end inline asm
	add.s64 	%rd108, %rd101, 56;
	// begin inline asm
	{ atom.add.f64 %fd85,[%rd108],%fd86; }

	// end inline asm
	add.s64 	%rd109, %rd101, 64;
	// begin inline asm
	{ atom.add.f64 %fd87,[%rd109],%fd1; }

	// end inline asm

$L__BB9_25:
	ld.param.u32 	%r8, [%rd43+172];
	add.s32 	%r9, %r2, 15;
	setp.le.s32 	%p17, %r8, %r9;
	selp.u16 	%rs53, 1, 0, %p17;
	shr.u32 	%r135, %r9, 31;
	cvt.u16.u32 	%rs4, %r135;
	or.b16  	%rs54, %rs53, %rs4;
	setp.eq.s16 	%p18, %rs54, 0;
	@%p18 bra 	$L__BB9_27;

	add.s32 	%r148, %r2, 15;
	st.local.v2.u32 	[%rd6], {%r148, %r8};
	mov.u64 	%rd112, $str;
	cvta.global.u64 	%rd113, %rd112;
	{ // callseq 86, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r136, [retval0+0];
	} // callseq 86
	bra.uni 	$L__BB9_28;

$L__BB9_27:
	ld.param.u32 	%r137, [%rd43+144];
	mul.wide.s32 	%rd124, %r137, %r9;
	ld.param.u64 	%rd125, [%rd43+112];
	add.s64 	%rd115, %rd125, %rd124;
	// begin inline asm
	{ atom.add.f64 %fd89,[%rd115],%fd1; }

	// end inline asm
	add.s64 	%rd116, %rd115, 8;
	mov.f64 	%fd104, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd91,[%rd116],%fd104; }

	// end inline asm
	add.s64 	%rd117, %rd115, 16;
	// begin inline asm
	{ atom.add.f64 %fd93,[%rd117],%fd104; }

	// end inline asm
	add.s64 	%rd118, %rd115, 24;
	// begin inline asm
	{ atom.add.f64 %fd95,[%rd118],%fd104; }

	// end inline asm
	add.s64 	%rd119, %rd115, 32;
	// begin inline asm
	{ atom.add.f64 %fd97,[%rd119],%fd1; }

	// end inline asm
	add.s64 	%rd120, %rd115, 40;
	// begin inline asm
	{ atom.add.f64 %fd99,[%rd120],%fd104; }

	// end inline asm
	add.s64 	%rd121, %rd115, 48;
	// begin inline asm
	{ atom.add.f64 %fd101,[%rd121],%fd104; }

	// end inline asm
	add.s64 	%rd122, %rd115, 56;
	// begin inline asm
	{ atom.add.f64 %fd103,[%rd122],%fd104; }

	// end inline asm
	add.s64 	%rd123, %rd115, 64;
	// begin inline asm
	{ atom.add.f64 %fd105,[%rd123],%fd1; }

	// end inline asm

$L__BB9_28:
	ld.param.u32 	%r10, [%rd43+172];
	setp.le.s32 	%p19, %r10, %r9;
	selp.u16 	%rs55, 1, 0, %p19;
	or.b16  	%rs56, %rs55, %rs4;
	setp.eq.s16 	%p20, %rs56, 0;
	mov.f64 	%fd138, 0d0000000000000000;
	mov.f64 	%fd139, 0d0000000000000000;
	mov.f64 	%fd140, 0d0000000000000000;
	@%p20 bra 	$L__BB9_30;

	add.s32 	%r149, %r2, 15;
	st.local.v2.u32 	[%rd6], {%r149, %r10};
	mov.u64 	%rd126, $str;
	cvta.global.u64 	%rd127, %rd126;
	{ // callseq 87, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r138, [retval0+0];
	} // callseq 87
	bra.uni 	$L__BB9_32;

$L__BB9_30:
	ld.param.u64 	%rd35, [%rd43+120];
	setp.eq.s64 	%p21, %rd35, 0;
	@%p21 bra 	$L__BB9_32;

	cvta.to.global.u64 	%rd129, %rd35;
	ld.param.u32 	%r139, [%rd43+144];
	mul.wide.s32 	%rd130, %r139, %r9;
	add.s64 	%rd131, %rd129, %rd130;
	ld.global.f64 	%fd113, [%rd131];
	add.f64 	%fd140, %fd113, 0d0000000000000000;
	ld.global.f64 	%fd114, [%rd131+32];
	add.f64 	%fd139, %fd114, 0d0000000000000000;
	ld.global.f64 	%fd115, [%rd131+64];
	add.f64 	%fd138, %fd115, 0d0000000000000000;

$L__BB9_32:
	ld.param.u32 	%r11, [%rd43+172];
	setp.le.s32 	%p22, %r11, %r7;
	selp.u16 	%rs57, 1, 0, %p22;
	or.b16  	%rs58, %rs57, %rs3;
	setp.eq.s16 	%p23, %rs58, 0;
	@%p23 bra 	$L__BB9_34;

	add.s32 	%r150, %r2, 10;
	st.local.v2.u32 	[%rd6], {%r150, %r11};
	mov.u64 	%rd132, $str;
	cvta.global.u64 	%rd133, %rd132;
	{ // callseq 88, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd133;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r140, [retval0+0];
	} // callseq 88
	bra.uni 	$L__BB9_36;

$L__BB9_34:
	ld.param.u64 	%rd36, [%rd43+120];
	setp.eq.s64 	%p24, %rd36, 0;
	@%p24 bra 	$L__BB9_36;

	cvta.to.global.u64 	%rd135, %rd36;
	ld.param.u32 	%r141, [%rd43+144];
	mul.wide.s32 	%rd136, %r141, %r7;
	add.s64 	%rd137, %rd135, %rd136;
	ld.global.f64 	%fd116, [%rd137];
	add.f64 	%fd140, %fd140, %fd116;
	ld.global.f64 	%fd117, [%rd137+32];
	add.f64 	%fd139, %fd139, %fd117;
	ld.global.f64 	%fd118, [%rd137+64];
	add.f64 	%fd138, %fd138, %fd118;

$L__BB9_36:
	ld.param.u32 	%r12, [%rd43+172];
	setp.le.s32 	%p25, %r12, %r5;
	selp.u16 	%rs59, 1, 0, %p25;
	or.b16  	%rs60, %rs59, %rs2;
	setp.eq.s16 	%p26, %rs60, 0;
	@%p26 bra 	$L__BB9_38;

	add.s32 	%r151, %r2, 5;
	st.local.v2.u32 	[%rd6], {%r151, %r12};
	mov.u64 	%rd138, $str;
	cvta.global.u64 	%rd139, %rd138;
	{ // callseq 89, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd139;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r142, [retval0+0];
	} // callseq 89
	bra.uni 	$L__BB9_40;

$L__BB9_38:
	ld.param.u64 	%rd37, [%rd43+120];
	setp.eq.s64 	%p27, %rd37, 0;
	@%p27 bra 	$L__BB9_40;

	cvta.to.global.u64 	%rd141, %rd37;
	ld.param.u32 	%r143, [%rd43+144];
	mul.wide.s32 	%rd142, %r143, %r5;
	add.s64 	%rd143, %rd141, %rd142;
	ld.global.f64 	%fd119, [%rd143];
	add.f64 	%fd140, %fd140, %fd119;
	ld.global.f64 	%fd120, [%rd143+32];
	add.f64 	%fd139, %fd139, %fd120;
	ld.global.f64 	%fd121, [%rd143+64];
	add.f64 	%fd138, %fd138, %fd121;

$L__BB9_40:
	ld.param.u32 	%r13, [%rd43+172];
	setp.le.s32 	%p28, %r13, %r2;
	selp.u16 	%rs61, 1, 0, %p28;
	or.b16  	%rs62, %rs1, %rs61;
	setp.eq.s16 	%p29, %rs62, 0;
	@%p29 bra 	$L__BB9_42;

	st.local.v2.u32 	[%rd6], {%r2, %r13};
	mov.u64 	%rd144, $str;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 90, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r144, [retval0+0];
	} // callseq 90
	bra.uni 	$L__BB9_44;

$L__BB9_42:
	ld.param.u64 	%rd38, [%rd43+120];
	setp.eq.s64 	%p30, %rd38, 0;
	@%p30 bra 	$L__BB9_44;

	cvta.to.global.u64 	%rd147, %rd38;
	ld.param.u32 	%r145, [%rd43+144];
	mul.wide.s32 	%rd148, %r145, %r2;
	add.s64 	%rd149, %rd147, %rd148;
	ld.global.f64 	%fd122, [%rd149];
	add.f64 	%fd140, %fd140, %fd122;
	ld.global.f64 	%fd123, [%rd149+32];
	add.f64 	%fd139, %fd139, %fd123;
	ld.global.f64 	%fd124, [%rd149+64];
	add.f64 	%fd138, %fd138, %fd124;

$L__BB9_44:
	add.f64 	%fd125, %fd140, 0d0000000000000000;
	add.f64 	%fd126, %fd125, %fd139;
	add.f64 	%fd127, %fd126, %fd138;
	fma.rn.f64 	%fd29, %fd127, %fd30, 0d0000000000000000;
	setp.eq.s64 	%p31, %rd50, 0;
	@%p31 bra 	$L__BB9_46;

	mul.lo.s64 	%rd151, %rd31, %rd18;
	add.s64 	%rd150, %rd50, %rd151;
	// begin inline asm
	{ atom.add.f64 %fd128,[%rd150],%fd29; }

	// end inline asm
	bra.uni 	$L__BB9_48;

$L__BB9_46:
	setp.eq.s64 	%p32, %rd45, 0;
	@%p32 bra 	$L__BB9_48;

	add.s64 	%rd152, %rd45, %rd32;
	// begin inline asm
	{ atom.add.f64 %fd130,[%rd152],%fd29; }

	// end inline asm

$L__BB9_48:
	ld.param.u64 	%rd153, [compute_affine_kinematic_hess_cuda_kernel_backward_param_0+24];
	add.s64 	%rd154, %rd154, %rd15;
	setp.lt.u64 	%p33, %rd154, %rd153;
	@%p33 bra 	$L__BB9_2;

$L__BB9_49:
	ret;

}
	// .globl	compute_soft_kinematic_grad_cuda_kernel_forward
.visible .entry compute_soft_kinematic_grad_cuda_kernel_forward(
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_3[56],
	.param .f64 compute_soft_kinematic_grad_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_5[56],
	.param .u32 compute_soft_kinematic_grad_cuda_kernel_forward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_8[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_forward_param_9[56]
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<58>;
	.reg .b32 	%r<149>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<80>;


	ld.param.v2.u32 	{%r72, %r73}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r74, %r75}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r80, %r81}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_1+32];
	ld.param.v2.u32 	{%r88, %r89}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_2+32];
	ld.param.v2.u32 	{%r96, %r97}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_3+32];
	ld.param.f64 	%fd1, [compute_soft_kinematic_grad_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r104, %r105}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_5+32];
	ld.param.u32 	%r44, [compute_soft_kinematic_grad_cuda_kernel_forward_param_6];
	ld.param.v2.u32 	{%r112, %r113}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_7+32];
	ld.param.v2.u32 	{%r120, %r121}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_8+32];
	ld.param.v2.u32 	{%r128, %r129}, [compute_soft_kinematic_grad_cuda_kernel_forward_param_9+32];
	ld.param.u64 	%rd48, [compute_soft_kinematic_grad_cuda_kernel_forward_param_9];
	ld.param.u64 	%rd46, [compute_soft_kinematic_grad_cuda_kernel_forward_param_8];
	ld.param.u64 	%rd44, [compute_soft_kinematic_grad_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd42, [compute_soft_kinematic_grad_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd40, [compute_soft_kinematic_grad_cuda_kernel_forward_param_3];
	ld.param.u64 	%rd38, [compute_soft_kinematic_grad_cuda_kernel_forward_param_2];
	ld.param.u64 	%rd36, [compute_soft_kinematic_grad_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd35, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r7, [compute_soft_kinematic_grad_cuda_kernel_forward_param_0+16];
	mov.u32 	%r132, %ntid.x;
	cvt.u64.u32 	%rd1, %r132;
	mov.u32 	%r133, %ctaid.x;
	mul.wide.u32 	%rd50, %r132, %r133;
	mov.u32 	%r134, %tid.x;
	cvt.u64.u32 	%rd51, %r134;
	add.s64 	%rd76, %rd50, %rd51;
	setp.ge.u64 	%p1, %rd76, %rd35;
	@%p1 bra 	$L__BB10_18;

	cvta.to.global.u64 	%rd4, %rd48;
	cvta.to.global.u64 	%rd5, %rd46;
	cvta.to.global.u64 	%rd6, %rd44;
	cvta.to.global.u64 	%rd8, %rd40;
	cvta.to.global.u64 	%rd9, %rd36;
	cvt.s64.s32 	%rd10, %r75;
	cvt.s64.s32 	%rd11, %r74;
	cvt.s64.s32 	%rd12, %r73;
	cvt.s64.s32 	%rd13, %r120;
	cvt.s64.s32 	%rd14, %r128;
	mov.u32 	%r135, %nctaid.x;
	cvt.u64.u32 	%rd52, %r135;
	mul.lo.s64 	%rd15, %rd1, %rd52;
	cvt.s64.s32 	%rd16, %r112;
	cvt.s64.s32 	%rd17, %r88;
	cvt.s64.s32 	%rd18, %r80;
	cvt.s64.s32 	%rd19, %r96;
	cvt.s64.s32 	%rd20, %r104;
	cvta.to.global.u64 	%rd21, %rd38;

$L__BB10_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd77, %rd76;
	@%p2 bra 	$L__BB10_6;

	or.b64  	%rd53, %rd76, %rd10;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p3, %rd54, 0;
	@%p3 bra 	$L__BB10_5;

	div.u64 	%rd77, %rd76, %rd10;
	bra.uni 	$L__BB10_6;

$L__BB10_5:
	cvt.u32.u64 	%r136, %rd10;
	cvt.u32.u64 	%r137, %rd76;
	div.u32 	%r138, %r137, %r136;
	cvt.u64.u32 	%rd77, %r138;

$L__BB10_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB10_10;

	or.b64  	%rd55, %rd77, %rd11;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p5, %rd56, 0;
	@%p5 bra 	$L__BB10_9;

	div.u64 	%rd77, %rd77, %rd11;
	bra.uni 	$L__BB10_10;

$L__BB10_9:
	cvt.u32.u64 	%r139, %rd11;
	cvt.u32.u64 	%r140, %rd77;
	div.u32 	%r141, %r140, %r139;
	cvt.u64.u32 	%rd77, %r141;

$L__BB10_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB10_14;

	or.b64  	%rd57, %rd77, %rd12;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p7, %rd58, 0;
	@%p7 bra 	$L__BB10_13;

	div.u64 	%rd77, %rd77, %rd12;
	bra.uni 	$L__BB10_14;

$L__BB10_13:
	cvt.u32.u64 	%r142, %rd12;
	cvt.u32.u64 	%r143, %rd77;
	div.u32 	%r144, %r143, %r142;
	cvt.u64.u32 	%rd77, %r144;

$L__BB10_14:
	cvt.u32.u64 	%r145, %rd77;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r145, 0, %p8;
	add.s32 	%r146, %r2, %r44;
	cvt.s64.s32 	%rd32, %r146;
	mul.lo.s64 	%rd59, %rd32, %rd13;
	add.s64 	%rd60, %rd5, %rd59;
	ld.global.s32 	%rd61, [%rd60];
	mul.lo.s64 	%rd62, %rd61, %rd14;
	add.s64 	%rd63, %rd4, %rd62;
	ld.global.u32 	%r147, [%rd63];
	add.s32 	%r148, %r147, -1;
	setp.lt.u32 	%p9, %r148, 2;
	@%p9 bra 	$L__BB10_17;

	cvt.s64.s32 	%rd33, %r2;
	mul.lo.s64 	%rd64, %rd33, %rd17;
	add.s64 	%rd65, %rd21, %rd64;
	ld.global.u8 	%rs57, [%rd65];
	setp.eq.s16 	%p10, %rs57, 0;
	@%p10 bra 	$L__BB10_17;

	mul.lo.s64 	%rd69, %rd33, %rd16;
	add.s64 	%rd70, %rd6, %rd69;
	ld.global.f64 	%fd8, [%rd70];
	mul.f64 	%fd9, %fd8, %fd1;
	mul.lo.s64 	%rd71, %rd32, %rd18;
	add.s64 	%rd72, %rd9, %rd71;
	mul.lo.s64 	%rd73, %rd33, %rd19;
	add.s64 	%rd74, %rd8, %rd73;
	ld.global.f64 	%fd10, [%rd74];
	ld.global.f64 	%fd11, [%rd72];
	sub.f64 	%fd12, %fd11, %fd10;
	ld.global.f64 	%fd13, [%rd74+8];
	ld.global.f64 	%fd14, [%rd72+8];
	sub.f64 	%fd15, %fd14, %fd13;
	ld.global.f64 	%fd16, [%rd74+16];
	ld.global.f64 	%fd17, [%rd72+16];
	sub.f64 	%fd18, %fd17, %fd16;
	mul.f64 	%fd3, %fd9, %fd12;
	mul.f64 	%fd5, %fd9, %fd15;
	mul.f64 	%fd7, %fd9, %fd18;
	mul.lo.s64 	%rd75, %rd33, %rd20;
	add.s64 	%rd66, %rd42, %rd75;
	// begin inline asm
	{ atom.add.f64 %fd2,[%rd66],%fd3; }

	// end inline asm
	add.s64 	%rd67, %rd66, 8;
	// begin inline asm
	{ atom.add.f64 %fd4,[%rd67],%fd5; }

	// end inline asm
	add.s64 	%rd68, %rd66, 16;
	// begin inline asm
	{ atom.add.f64 %fd6,[%rd68],%fd7; }

	// end inline asm

$L__BB10_17:
	add.s64 	%rd76, %rd76, %rd15;
	setp.lt.u64 	%p11, %rd76, %rd35;
	@%p11 bra 	$L__BB10_2;

$L__BB10_18:
	ret;

}
	// .globl	compute_soft_kinematic_grad_cuda_kernel_backward
.visible .entry compute_soft_kinematic_grad_cuda_kernel_backward(
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_1[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_2[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_3[56],
	.param .f64 compute_soft_kinematic_grad_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_5[56],
	.param .u32 compute_soft_kinematic_grad_cuda_kernel_backward_param_6,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_8[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_9[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_10[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_11[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_12[56],
	.param .f64 compute_soft_kinematic_grad_cuda_kernel_backward_param_13,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_14[56],
	.param .u32 compute_soft_kinematic_grad_cuda_kernel_backward_param_15,
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_16[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_17[56],
	.param .align 8 .b8 compute_soft_kinematic_grad_cuda_kernel_backward_param_18[56]
)
{
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<90>;
	.reg .b32 	%r<218>;
	.reg .f64 	%fd<78>;
	.reg .b64 	%rd<121>;


	ld.param.v2.u32 	{%r108, %r109}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r110, %r111}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r116, %r117}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_1+32];
	ld.param.v2.u32 	{%r124, %r125}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_2+32];
	ld.param.v2.u32 	{%r132, %r133}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_3+32];
	ld.param.f64 	%fd24, [compute_soft_kinematic_grad_cuda_kernel_backward_param_4];
	ld.param.v2.u32 	{%r140, %r141}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r148, %r149}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r156, %r157}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_8+32];
	ld.param.v2.u32 	{%r164, %r165}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_9+32];
	ld.param.v2.u32 	{%r172, %r173}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_10+32];
	ld.param.v2.u32 	{%r180, %r181}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_12+32];
	ld.param.v2.u32 	{%r188, %r189}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_14+32];
	ld.param.v2.u32 	{%r196, %r197}, [compute_soft_kinematic_grad_cuda_kernel_backward_param_16+32];
	ld.param.u64 	%rd72, [compute_soft_kinematic_grad_cuda_kernel_backward_param_16];
	ld.param.u64 	%rd70, [compute_soft_kinematic_grad_cuda_kernel_backward_param_14];
	ld.param.u64 	%rd68, [compute_soft_kinematic_grad_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd66, [compute_soft_kinematic_grad_cuda_kernel_backward_param_10];
	ld.param.u64 	%rd64, [compute_soft_kinematic_grad_cuda_kernel_backward_param_9];
	ld.param.u64 	%rd62, [compute_soft_kinematic_grad_cuda_kernel_backward_param_8];
	ld.param.u64 	%rd61, [compute_soft_kinematic_grad_cuda_kernel_backward_param_7+8];
	ld.param.u64 	%rd60, [compute_soft_kinematic_grad_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd59, [compute_soft_kinematic_grad_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd57, [compute_soft_kinematic_grad_cuda_kernel_backward_param_3+8];
	ld.param.u64 	%rd56, [compute_soft_kinematic_grad_cuda_kernel_backward_param_3];
	ld.param.u64 	%rd54, [compute_soft_kinematic_grad_cuda_kernel_backward_param_2];
	ld.param.u64 	%rd53, [compute_soft_kinematic_grad_cuda_kernel_backward_param_1+8];
	ld.param.u64 	%rd52, [compute_soft_kinematic_grad_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd51, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r7, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+16];
	mov.u32 	%r200, %ntid.x;
	cvt.u64.u32 	%rd1, %r200;
	mov.u32 	%r201, %ctaid.x;
	mul.wide.u32 	%rd74, %r200, %r201;
	mov.u32 	%r202, %tid.x;
	cvt.u64.u32 	%rd75, %r202;
	add.s64 	%rd117, %rd74, %rd75;
	setp.ge.u64 	%p1, %rd117, %rd51;
	@%p1 bra 	$L__BB11_34;

	cvta.to.global.u64 	%rd12, %rd70;
	cvta.to.global.u64 	%rd13, %rd64;
	cvta.to.global.u64 	%rd14, %rd62;
	cvta.to.global.u64 	%rd15, %rd60;
	cvta.to.global.u64 	%rd16, %rd59;
	cvta.to.global.u64 	%rd17, %rd56;
	cvta.to.global.u64 	%rd18, %rd52;
	cvt.s64.s32 	%rd19, %r111;
	cvt.s64.s32 	%rd20, %r110;
	cvt.s64.s32 	%rd21, %r109;
	cvt.s64.s32 	%rd22, %r156;
	cvt.s64.s32 	%rd23, %r164;
	mov.u32 	%r203, %nctaid.x;
	cvt.u64.u32 	%rd76, %r203;
	mul.lo.s64 	%rd24, %rd1, %rd76;
	cvt.s64.s32 	%rd25, %r148;
	cvt.s64.s32 	%rd26, %r124;
	cvt.s64.s32 	%rd27, %r116;
	cvt.s64.s32 	%rd28, %r132;
	cvt.s64.s32 	%rd29, %r196;
	cvt.s64.s32 	%rd30, %r188;
	cvt.s64.s32 	%rd31, %r140;
	cvt.s64.s32 	%rd32, %r180;
	cvt.s64.s32 	%rd33, %r172;
	cvta.to.global.u64 	%rd34, %rd54;

$L__BB11_2:
	setp.lt.s32 	%p2, %r7, 4;
	mov.u64 	%rd118, %rd117;
	@%p2 bra 	$L__BB11_6;

	or.b64  	%rd77, %rd117, %rd19;
	and.b64  	%rd78, %rd77, -4294967296;
	setp.eq.s64 	%p3, %rd78, 0;
	@%p3 bra 	$L__BB11_5;

	div.u64 	%rd118, %rd117, %rd19;
	bra.uni 	$L__BB11_6;

$L__BB11_5:
	cvt.u32.u64 	%r204, %rd19;
	cvt.u32.u64 	%r205, %rd117;
	div.u32 	%r206, %r205, %r204;
	cvt.u64.u32 	%rd118, %r206;

$L__BB11_6:
	setp.lt.s32 	%p4, %r7, 3;
	@%p4 bra 	$L__BB11_10;

	or.b64  	%rd79, %rd118, %rd20;
	and.b64  	%rd80, %rd79, -4294967296;
	setp.eq.s64 	%p5, %rd80, 0;
	@%p5 bra 	$L__BB11_9;

	div.u64 	%rd118, %rd118, %rd20;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r207, %rd20;
	cvt.u32.u64 	%r208, %rd118;
	div.u32 	%r209, %r208, %r207;
	cvt.u64.u32 	%rd118, %r209;

$L__BB11_10:
	setp.lt.s32 	%p6, %r7, 2;
	@%p6 bra 	$L__BB11_14;

	or.b64  	%rd81, %rd118, %rd21;
	and.b64  	%rd82, %rd81, -4294967296;
	setp.eq.s64 	%p7, %rd82, 0;
	@%p7 bra 	$L__BB11_13;

	div.u64 	%rd118, %rd118, %rd21;
	bra.uni 	$L__BB11_14;

$L__BB11_13:
	cvt.u32.u64 	%r210, %rd21;
	cvt.u32.u64 	%r211, %rd118;
	div.u32 	%r212, %r211, %r210;
	cvt.u64.u32 	%rd118, %r212;

$L__BB11_14:
	ld.param.u32 	%r217, [compute_soft_kinematic_grad_cuda_kernel_backward_param_6];
	cvt.u32.u64 	%r213, %rd118;
	setp.gt.s32 	%p8, %r7, 0;
	selp.b32 	%r2, %r213, 0, %p8;
	add.s32 	%r214, %r2, %r217;
	cvt.s64.s32 	%rd45, %r214;
	mul.lo.s64 	%rd83, %rd45, %rd22;
	add.s64 	%rd84, %rd14, %rd83;
	ld.global.s32 	%rd85, [%rd84];
	mul.lo.s64 	%rd86, %rd85, %rd23;
	add.s64 	%rd87, %rd13, %rd86;
	ld.global.u32 	%r215, [%rd87];
	add.s32 	%r216, %r215, -1;
	setp.lt.u32 	%p9, %r216, 2;
	@%p9 bra 	$L__BB11_33;

	cvt.s64.s32 	%rd46, %r2;
	mul.lo.s64 	%rd47, %rd46, %rd25;
	mul.lo.s64 	%rd88, %rd46, %rd26;
	add.s64 	%rd89, %rd34, %rd88;
	ld.global.u8 	%rs89, [%rd89];
	setp.eq.s16 	%p10, %rs89, 0;
	mov.f64 	%fd77, 0d0000000000000000;
	@%p10 bra 	$L__BB11_29;

	ld.param.u64 	%rd115, [compute_soft_kinematic_grad_cuda_kernel_backward_param_14];
	setp.eq.s64 	%p11, %rd115, 0;
	add.s64 	%rd90, %rd15, %rd47;
	ld.global.f64 	%fd26, [%rd90];
	mul.f64 	%fd1, %fd26, %fd24;
	mul.lo.s64 	%rd48, %rd45, %rd27;
	add.s64 	%rd91, %rd18, %rd48;
	mul.lo.s64 	%rd49, %rd46, %rd28;
	add.s64 	%rd92, %rd17, %rd49;
	ld.global.f64 	%fd27, [%rd92];
	ld.global.f64 	%fd28, [%rd91];
	sub.f64 	%fd2, %fd28, %fd27;
	ld.global.f64 	%fd29, [%rd92+8];
	ld.global.f64 	%fd30, [%rd91+8];
	sub.f64 	%fd3, %fd30, %fd29;
	ld.global.f64 	%fd31, [%rd92+16];
	ld.global.f64 	%fd32, [%rd91+16];
	sub.f64 	%fd4, %fd32, %fd31;
	@%p11 bra 	$L__BB11_18;

	mul.lo.s64 	%rd93, %rd46, %rd30;
	add.s64 	%rd94, %rd12, %rd93;
	ld.global.f64 	%fd33, [%rd94];
	add.f64 	%fd76, %fd33, 0d0000000000000000;
	ld.global.f64 	%fd34, [%rd94+8];
	add.f64 	%fd75, %fd34, 0d0000000000000000;
	ld.global.f64 	%fd35, [%rd94+16];
	add.f64 	%fd74, %fd35, 0d0000000000000000;
	bra.uni 	$L__BB11_20;

$L__BB11_18:
	ld.param.u64 	%rd116, [compute_soft_kinematic_grad_cuda_kernel_backward_param_5+8];
	setp.eq.s64 	%p12, %rd116, 0;
	mov.f64 	%fd74, 0d0000000000000000;
	mov.f64 	%fd75, %fd74;
	mov.f64 	%fd76, %fd74;
	@%p12 bra 	$L__BB11_20;

	mul.lo.s64 	%rd95, %rd46, %rd31;
	add.s64 	%rd96, %rd16, %rd95;
	ld.global.f64 	%fd39, [%rd96];
	add.f64 	%fd76, %fd39, 0d0000000000000000;
	ld.global.f64 	%fd40, [%rd96+8];
	add.f64 	%fd75, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd96+16];
	add.f64 	%fd74, %fd41, 0d0000000000000000;

$L__BB11_20:
	fma.rn.f64 	%fd14, %fd1, %fd76, 0d0000000000000000;
	mov.f64 	%fd42, 0d0000000000000000;
	fma.rn.f64 	%fd15, %fd1, %fd75, 0d0000000000000000;
	fma.rn.f64 	%fd16, %fd1, %fd74, 0d0000000000000000;
	mul.f64 	%fd43, %fd3, %fd75;
	fma.rn.f64 	%fd44, %fd2, %fd76, %fd43;
	fma.rn.f64 	%fd17, %fd4, %fd74, %fd44;
	sub.f64 	%fd18, %fd42, %fd14;
	sub.f64 	%fd19, %fd42, %fd15;
	sub.f64 	%fd20, %fd42, %fd16;
	setp.eq.s64 	%p13, %rd68, 0;
	@%p13 bra 	$L__BB11_22;

	mul.lo.s64 	%rd100, %rd46, %rd32;
	add.s64 	%rd97, %rd68, %rd100;
	// begin inline asm
	{ atom.add.f64 %fd45,[%rd97],%fd18; }

	// end inline asm
	add.s64 	%rd98, %rd97, 8;
	// begin inline asm
	{ atom.add.f64 %fd47,[%rd98],%fd19; }

	// end inline asm
	add.s64 	%rd99, %rd97, 16;
	// begin inline asm
	{ atom.add.f64 %fd49,[%rd99],%fd20; }

	// end inline asm
	bra.uni 	$L__BB11_24;

$L__BB11_22:
	setp.eq.s64 	%p14, %rd57, 0;
	@%p14 bra 	$L__BB11_24;

	add.s64 	%rd101, %rd57, %rd49;
	// begin inline asm
	{ atom.add.f64 %fd51,[%rd101],%fd18; }

	// end inline asm
	add.s64 	%rd102, %rd101, 8;
	// begin inline asm
	{ atom.add.f64 %fd53,[%rd102],%fd19; }

	// end inline asm
	add.s64 	%rd103, %rd101, 16;
	// begin inline asm
	{ atom.add.f64 %fd55,[%rd103],%fd20; }

	// end inline asm

$L__BB11_24:
	setp.eq.s64 	%p15, %rd66, 0;
	@%p15 bra 	$L__BB11_26;

	mul.lo.s64 	%rd107, %rd45, %rd33;
	add.s64 	%rd104, %rd66, %rd107;
	// begin inline asm
	{ atom.add.f64 %fd57,[%rd104],%fd14; }

	// end inline asm
	add.s64 	%rd105, %rd104, 8;
	// begin inline asm
	{ atom.add.f64 %fd59,[%rd105],%fd15; }

	// end inline asm
	add.s64 	%rd106, %rd104, 16;
	// begin inline asm
	{ atom.add.f64 %fd61,[%rd106],%fd16; }

	// end inline asm
	bra.uni 	$L__BB11_28;

$L__BB11_26:
	setp.eq.s64 	%p16, %rd53, 0;
	@%p16 bra 	$L__BB11_28;

	add.s64 	%rd108, %rd53, %rd48;
	// begin inline asm
	{ atom.add.f64 %fd63,[%rd108],%fd14; }

	// end inline asm
	add.s64 	%rd109, %rd108, 8;
	// begin inline asm
	{ atom.add.f64 %fd65,[%rd109],%fd15; }

	// end inline asm
	add.s64 	%rd110, %rd108, 16;
	// begin inline asm
	{ atom.add.f64 %fd67,[%rd110],%fd16; }

	// end inline asm

$L__BB11_28:
	add.f64 	%fd69, %fd17, 0d0000000000000000;
	fma.rn.f64 	%fd77, %fd69, %fd24, 0d0000000000000000;

$L__BB11_29:
	add.f64 	%fd23, %fd77, 0d0000000000000000;
	setp.eq.s64 	%p17, %rd72, 0;
	@%p17 bra 	$L__BB11_31;

	mul.lo.s64 	%rd112, %rd46, %rd29;
	add.s64 	%rd111, %rd72, %rd112;
	// begin inline asm
	{ atom.add.f64 %fd70,[%rd111],%fd23; }

	// end inline asm
	bra.uni 	$L__BB11_33;

$L__BB11_31:
	setp.eq.s64 	%p18, %rd61, 0;
	@%p18 bra 	$L__BB11_33;

	add.s64 	%rd113, %rd61, %rd47;
	// begin inline asm
	{ atom.add.f64 %fd72,[%rd113],%fd23; }

	// end inline asm

$L__BB11_33:
	ld.param.u64 	%rd114, [compute_soft_kinematic_grad_cuda_kernel_backward_param_0+24];
	add.s64 	%rd117, %rd117, %rd24;
	setp.lt.u64 	%p19, %rd117, %rd114;
	@%p19 bra 	$L__BB11_2;

$L__BB11_34:
	ret;

}
	// .globl	compute_soft_kinematic_hess_cuda_kernel_forward
.visible .entry compute_soft_kinematic_hess_cuda_kernel_forward(
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_1[56],
	.param .f64 compute_soft_kinematic_hess_cuda_kernel_forward_param_2,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_3[184],
	.param .u32 compute_soft_kinematic_hess_cuda_kernel_forward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_5[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_6[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_forward_param_7[56]
)
{
	.local .align 8 .b8 	__local_depot12[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<37>;
	.reg .b32 	%r<101>;
	.reg .f64 	%fd<21>;
	.reg .b64 	%rd<80>;


	mov.u64 	%SPL, __local_depot12;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r46, %r47}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0];
	ld.param.v2.u32 	{%r48, %r49}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0+8];
	ld.param.v2.u32 	{%r54, %r55}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_1+32];
	ld.param.f64 	%fd2, [compute_soft_kinematic_hess_cuda_kernel_forward_param_2];
	mov.b64 	%rd34, compute_soft_kinematic_hess_cuda_kernel_forward_param_3;
	ld.param.u32 	%r18, [compute_soft_kinematic_hess_cuda_kernel_forward_param_4];
	ld.param.v2.u32 	{%r62, %r63}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_5+32];
	ld.param.v2.u32 	{%r70, %r71}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_6+32];
	ld.param.v2.u32 	{%r78, %r79}, [compute_soft_kinematic_hess_cuda_kernel_forward_param_7+32];
	ld.param.u64 	%rd39, [compute_soft_kinematic_hess_cuda_kernel_forward_param_7];
	ld.param.u64 	%rd37, [compute_soft_kinematic_hess_cuda_kernel_forward_param_6];
	ld.param.u64 	%rd35, [compute_soft_kinematic_hess_cuda_kernel_forward_param_5];
	ld.param.u64 	%rd32, [compute_soft_kinematic_hess_cuda_kernel_forward_param_1];
	ld.param.u64 	%rd31, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0+24];
	ld.param.u32 	%r8, [compute_soft_kinematic_hess_cuda_kernel_forward_param_0+16];
	mov.u32 	%r82, %ntid.x;
	cvt.u64.u32 	%rd1, %r82;
	mov.u32 	%r83, %ctaid.x;
	mul.wide.u32 	%rd41, %r82, %r83;
	mov.u32 	%r84, %tid.x;
	cvt.u64.u32 	%rd42, %r84;
	add.s64 	%rd76, %rd41, %rd42;
	setp.ge.u64 	%p1, %rd76, %rd31;
	@%p1 bra 	$L__BB12_20;

	add.u64 	%rd43, %SP, 0;
	add.u64 	%rd4, %SPL, 0;
	cvta.to.global.u64 	%rd5, %rd39;
	cvta.to.global.u64 	%rd6, %rd37;
	cvta.to.global.u64 	%rd7, %rd35;
	cvt.s64.s32 	%rd8, %r49;
	cvt.s64.s32 	%rd9, %r48;
	cvt.s64.s32 	%rd10, %r47;
	cvt.s64.s32 	%rd11, %r70;
	cvt.s64.s32 	%rd12, %r78;
	mov.u32 	%r85, %nctaid.x;
	cvt.u64.u32 	%rd44, %r85;
	mul.lo.s64 	%rd13, %rd1, %rd44;
	cvt.s64.s32 	%rd14, %r62;
	cvt.s64.s32 	%rd15, %r54;
	cvta.to.global.u64 	%rd16, %rd32;

$L__BB12_2:
	setp.lt.s32 	%p2, %r8, 4;
	mov.u64 	%rd77, %rd76;
	@%p2 bra 	$L__BB12_6;

	or.b64  	%rd45, %rd76, %rd8;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.eq.s64 	%p3, %rd46, 0;
	@%p3 bra 	$L__BB12_5;

	div.u64 	%rd77, %rd76, %rd8;
	bra.uni 	$L__BB12_6;

$L__BB12_5:
	cvt.u32.u64 	%r86, %rd8;
	cvt.u32.u64 	%r87, %rd76;
	div.u32 	%r88, %r87, %r86;
	cvt.u64.u32 	%rd77, %r88;

$L__BB12_6:
	setp.lt.s32 	%p4, %r8, 3;
	@%p4 bra 	$L__BB12_10;

	or.b64  	%rd47, %rd77, %rd9;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p5, %rd48, 0;
	@%p5 bra 	$L__BB12_9;

	div.u64 	%rd77, %rd77, %rd9;
	bra.uni 	$L__BB12_10;

$L__BB12_9:
	cvt.u32.u64 	%r89, %rd9;
	cvt.u32.u64 	%r90, %rd77;
	div.u32 	%r91, %r90, %r89;
	cvt.u64.u32 	%rd77, %r91;

$L__BB12_10:
	setp.lt.s32 	%p6, %r8, 2;
	@%p6 bra 	$L__BB12_14;

	or.b64  	%rd49, %rd77, %rd10;
	and.b64  	%rd50, %rd49, -4294967296;
	setp.eq.s64 	%p7, %rd50, 0;
	@%p7 bra 	$L__BB12_13;

	div.u64 	%rd77, %rd77, %rd10;
	bra.uni 	$L__BB12_14;

$L__BB12_13:
	cvt.u32.u64 	%r92, %rd10;
	cvt.u32.u64 	%r93, %rd77;
	div.u32 	%r94, %r93, %r92;
	cvt.u64.u32 	%rd77, %r94;

$L__BB12_14:
	cvt.u32.u64 	%r95, %rd77;
	setp.gt.s32 	%p8, %r8, 0;
	selp.b32 	%r2, %r95, 0, %p8;
	add.s32 	%r96, %r2, %r18;
	cvt.s64.s32 	%rd51, %r96;
	mul.lo.s64 	%rd52, %rd51, %rd11;
	add.s64 	%rd53, %rd6, %rd52;
	ld.global.s32 	%rd54, [%rd53];
	mul.lo.s64 	%rd55, %rd54, %rd12;
	add.s64 	%rd56, %rd5, %rd55;
	ld.global.u32 	%r97, [%rd56];
	add.s32 	%r98, %r97, -1;
	setp.lt.u32 	%p9, %r98, 2;
	@%p9 bra 	$L__BB12_19;

	cvt.s64.s32 	%rd28, %r2;
	mul.lo.s64 	%rd57, %rd28, %rd15;
	add.s64 	%rd58, %rd16, %rd57;
	ld.global.u8 	%rs33, [%rd58];
	setp.eq.s16 	%p10, %rs33, 0;
	@%p10 bra 	$L__BB12_19;

	mul.lo.s64 	%rd59, %rd28, %rd14;
	add.s64 	%rd60, %rd7, %rd59;
	ld.global.f64 	%fd1, [%rd60];
	ld.param.u32 	%r3, [%rd34+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs34, 1, 0, %p11;
	shr.u32 	%r99, %r2, 31;
	cvt.u16.u32 	%rs35, %r99;
	or.b16  	%rs36, %rs34, %rs35;
	setp.eq.s16 	%p12, %rs36, 0;
	@%p12 bra 	$L__BB12_18;

	st.local.v2.u32 	[%rd4], {%r2, %r3};
	mov.u64 	%rd61, $str;
	cvta.global.u64 	%rd62, %rd61;
	{ // callseq 91, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd62;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd43;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r100, [retval0+0];
	} // callseq 91
	bra.uni 	$L__BB12_19;

$L__BB12_18:
	mul.f64 	%fd20, %fd1, %fd2;
	ld.param.s32 	%rd73, [%rd34+144];
	mul.lo.s64 	%rd74, %rd73, %rd28;
	ld.param.u64 	%rd75, [%rd34+112];
	add.s64 	%rd64, %rd75, %rd74;
	// begin inline asm
	{ atom.add.f64 %fd3,[%rd64],%fd20; }

	// end inline asm
	add.s64 	%rd65, %rd64, 8;
	mov.f64 	%fd18, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd5,[%rd65],%fd18; }

	// end inline asm
	add.s64 	%rd66, %rd64, 16;
	// begin inline asm
	{ atom.add.f64 %fd7,[%rd66],%fd18; }

	// end inline asm
	add.s64 	%rd67, %rd64, 24;
	// begin inline asm
	{ atom.add.f64 %fd9,[%rd67],%fd18; }

	// end inline asm
	add.s64 	%rd68, %rd64, 32;
	// begin inline asm
	{ atom.add.f64 %fd11,[%rd68],%fd20; }

	// end inline asm
	add.s64 	%rd69, %rd64, 40;
	// begin inline asm
	{ atom.add.f64 %fd13,[%rd69],%fd18; }

	// end inline asm
	add.s64 	%rd70, %rd64, 48;
	// begin inline asm
	{ atom.add.f64 %fd15,[%rd70],%fd18; }

	// end inline asm
	add.s64 	%rd71, %rd64, 56;
	// begin inline asm
	{ atom.add.f64 %fd17,[%rd71],%fd18; }

	// end inline asm
	add.s64 	%rd72, %rd64, 64;
	// begin inline asm
	{ atom.add.f64 %fd19,[%rd72],%fd20; }

	// end inline asm

$L__BB12_19:
	add.s64 	%rd76, %rd76, %rd13;
	setp.lt.u64 	%p13, %rd76, %rd31;
	@%p13 bra 	$L__BB12_2;

$L__BB12_20:
	ret;

}
	// .globl	compute_soft_kinematic_hess_cuda_kernel_backward
.visible .entry compute_soft_kinematic_hess_cuda_kernel_backward(
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_0[32],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_1[56],
	.param .f64 compute_soft_kinematic_hess_cuda_kernel_backward_param_2,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_3[184],
	.param .u32 compute_soft_kinematic_hess_cuda_kernel_backward_param_4,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_5[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_6[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_7[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_8[56],
	.param .f64 compute_soft_kinematic_hess_cuda_kernel_backward_param_9,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_10[184],
	.param .u32 compute_soft_kinematic_hess_cuda_kernel_backward_param_11,
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_12[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_13[56],
	.param .align 8 .b8 compute_soft_kinematic_hess_cuda_kernel_backward_param_14[56]
)
{
	.local .align 8 .b8 	__local_depot13[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<47>;
	.reg .b32 	%r<121>;
	.reg .f64 	%fd<59>;
	.reg .b64 	%rd<98>;


	mov.u64 	%SPL, __local_depot13;
	cvta.local.u64 	%SP, %SPL;
	ld.param.v2.u32 	{%r56, %r57}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0];
	ld.param.v2.u32 	{%r58, %r59}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+8];
	ld.param.v2.u32 	{%r64, %r65}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_1+32];
	ld.param.f64 	%fd12, [compute_soft_kinematic_hess_cuda_kernel_backward_param_2];
	mov.b64 	%rd40, compute_soft_kinematic_hess_cuda_kernel_backward_param_3;
	ld.param.v2.u32 	{%r72, %r73}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_5+32];
	ld.param.v2.u32 	{%r80, %r81}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_6+32];
	ld.param.v2.u32 	{%r88, %r89}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_7+32];
	ld.param.v2.u32 	{%r96, %r97}, [compute_soft_kinematic_hess_cuda_kernel_backward_param_12+32];
	ld.param.u64 	%rd47, [compute_soft_kinematic_hess_cuda_kernel_backward_param_12];
	ld.param.u64 	%rd45, [compute_soft_kinematic_hess_cuda_kernel_backward_param_7];
	ld.param.u64 	%rd43, [compute_soft_kinematic_hess_cuda_kernel_backward_param_6];
	ld.param.u64 	%rd42, [compute_soft_kinematic_hess_cuda_kernel_backward_param_5+8];
	ld.param.u64 	%rd41, [compute_soft_kinematic_hess_cuda_kernel_backward_param_5];
	ld.param.u64 	%rd38, [compute_soft_kinematic_hess_cuda_kernel_backward_param_1];
	ld.param.u64 	%rd37, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+24];
	ld.param.u32 	%r9, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+16];
	mov.u32 	%r100, %ntid.x;
	cvt.u64.u32 	%rd1, %r100;
	mov.u32 	%r101, %ctaid.x;
	mul.wide.u32 	%rd49, %r100, %r101;
	mov.u32 	%r102, %tid.x;
	cvt.u64.u32 	%rd50, %r102;
	add.s64 	%rd94, %rd49, %rd50;
	setp.ge.u64 	%p1, %rd94, %rd37;
	@%p1 bra 	$L__BB13_28;

	add.u64 	%rd51, %SP, 0;
	add.u64 	%rd6, %SPL, 0;
	cvta.to.global.u64 	%rd7, %rd45;
	cvta.to.global.u64 	%rd8, %rd43;
	cvta.to.global.u64 	%rd9, %rd41;
	cvt.s64.s32 	%rd10, %r59;
	cvt.s64.s32 	%rd11, %r58;
	cvt.s64.s32 	%rd12, %r57;
	cvt.s64.s32 	%rd13, %r80;
	cvt.s64.s32 	%rd14, %r88;
	mov.u32 	%r103, %nctaid.x;
	cvt.u64.u32 	%rd52, %r103;
	mul.lo.s64 	%rd15, %rd1, %rd52;
	cvt.s64.s32 	%rd16, %r72;
	cvt.s64.s32 	%rd17, %r64;
	cvt.s64.s32 	%rd18, %r96;
	cvta.to.global.u64 	%rd19, %rd38;

$L__BB13_2:
	setp.lt.s32 	%p2, %r9, 4;
	mov.u64 	%rd95, %rd94;
	@%p2 bra 	$L__BB13_6;

	or.b64  	%rd53, %rd94, %rd10;
	and.b64  	%rd54, %rd53, -4294967296;
	setp.eq.s64 	%p3, %rd54, 0;
	@%p3 bra 	$L__BB13_5;

	div.u64 	%rd95, %rd94, %rd10;
	bra.uni 	$L__BB13_6;

$L__BB13_5:
	cvt.u32.u64 	%r104, %rd10;
	cvt.u32.u64 	%r105, %rd94;
	div.u32 	%r106, %r105, %r104;
	cvt.u64.u32 	%rd95, %r106;

$L__BB13_6:
	setp.lt.s32 	%p4, %r9, 3;
	@%p4 bra 	$L__BB13_10;

	or.b64  	%rd55, %rd95, %rd11;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.eq.s64 	%p5, %rd56, 0;
	@%p5 bra 	$L__BB13_9;

	div.u64 	%rd95, %rd95, %rd11;
	bra.uni 	$L__BB13_10;

$L__BB13_9:
	cvt.u32.u64 	%r107, %rd11;
	cvt.u32.u64 	%r108, %rd95;
	div.u32 	%r109, %r108, %r107;
	cvt.u64.u32 	%rd95, %r109;

$L__BB13_10:
	setp.lt.s32 	%p6, %r9, 2;
	@%p6 bra 	$L__BB13_14;

	or.b64  	%rd57, %rd95, %rd12;
	and.b64  	%rd58, %rd57, -4294967296;
	setp.eq.s64 	%p7, %rd58, 0;
	@%p7 bra 	$L__BB13_13;

	div.u64 	%rd95, %rd95, %rd12;
	bra.uni 	$L__BB13_14;

$L__BB13_13:
	cvt.u32.u64 	%r110, %rd12;
	cvt.u32.u64 	%r111, %rd95;
	div.u32 	%r112, %r111, %r110;
	cvt.u64.u32 	%rd95, %r112;

$L__BB13_14:
	ld.param.u32 	%r120, [compute_soft_kinematic_hess_cuda_kernel_backward_param_4];
	cvt.u32.u64 	%r113, %rd95;
	setp.gt.s32 	%p8, %r9, 0;
	selp.b32 	%r2, %r113, 0, %p8;
	add.s32 	%r114, %r2, %r120;
	cvt.s64.s32 	%rd59, %r114;
	mul.lo.s64 	%rd60, %rd59, %rd13;
	add.s64 	%rd61, %rd8, %rd60;
	ld.global.s32 	%rd62, [%rd61];
	mul.lo.s64 	%rd63, %rd62, %rd14;
	add.s64 	%rd64, %rd7, %rd63;
	ld.global.u32 	%r115, [%rd64];
	add.s32 	%r116, %r115, -1;
	setp.lt.u32 	%p9, %r116, 2;
	@%p9 bra 	$L__BB13_27;

	cvt.s64.s32 	%rd31, %r2;
	mul.lo.s64 	%rd32, %rd31, %rd16;
	mul.lo.s64 	%rd65, %rd31, %rd17;
	add.s64 	%rd66, %rd19, %rd65;
	ld.global.u8 	%rs42, [%rd66];
	setp.eq.s16 	%p10, %rs42, 0;
	mov.f64 	%fd53, 0d0000000000000000;
	mov.f64 	%fd54, %fd53;
	mov.f64 	%fd55, %fd53;
	@%p10 bra 	$L__BB13_23;

	add.s64 	%rd67, %rd9, %rd32;
	ld.global.f64 	%fd1, [%rd67];
	ld.param.u32 	%r3, [%rd40+172];
	setp.le.s32 	%p11, %r3, %r2;
	selp.u16 	%rs43, 1, 0, %p11;
	shr.u32 	%r117, %r2, 31;
	cvt.u16.u32 	%rs1, %r117;
	or.b16  	%rs44, %rs43, %rs1;
	setp.eq.s16 	%p12, %rs44, 0;
	@%p12 bra 	$L__BB13_18;

	st.local.v2.u32 	[%rd6], {%r2, %r3};
	mov.u64 	%rd68, $str;
	cvta.global.u64 	%rd69, %rd68;
	{ // callseq 92, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd69;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd51;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r118, [retval0+0];
	} // callseq 92
	bra.uni 	$L__BB13_19;

$L__BB13_18:
	mul.f64 	%fd33, %fd1, %fd12;
	ld.param.s32 	%rd80, [%rd40+144];
	mul.lo.s64 	%rd81, %rd80, %rd31;
	ld.param.u64 	%rd82, [%rd40+112];
	add.s64 	%rd71, %rd82, %rd81;
	// begin inline asm
	{ atom.add.f64 %fd16,[%rd71],%fd33; }

	// end inline asm
	add.s64 	%rd72, %rd71, 8;
	mov.f64 	%fd31, 0d0000000000000000;
	// begin inline asm
	{ atom.add.f64 %fd18,[%rd72],%fd31; }

	// end inline asm
	add.s64 	%rd73, %rd71, 16;
	// begin inline asm
	{ atom.add.f64 %fd20,[%rd73],%fd31; }

	// end inline asm
	add.s64 	%rd74, %rd71, 24;
	// begin inline asm
	{ atom.add.f64 %fd22,[%rd74],%fd31; }

	// end inline asm
	add.s64 	%rd75, %rd71, 32;
	// begin inline asm
	{ atom.add.f64 %fd24,[%rd75],%fd33; }

	// end inline asm
	add.s64 	%rd76, %rd71, 40;
	// begin inline asm
	{ atom.add.f64 %fd26,[%rd76],%fd31; }

	// end inline asm
	add.s64 	%rd77, %rd71, 48;
	// begin inline asm
	{ atom.add.f64 %fd28,[%rd77],%fd31; }

	// end inline asm
	add.s64 	%rd78, %rd71, 56;
	// begin inline asm
	{ atom.add.f64 %fd30,[%rd78],%fd31; }

	// end inline asm
	add.s64 	%rd79, %rd71, 64;
	// begin inline asm
	{ atom.add.f64 %fd32,[%rd79],%fd33; }

	// end inline asm

$L__BB13_19:
	ld.param.u32 	%r4, [%rd40+172];
	setp.le.s32 	%p13, %r4, %r2;
	selp.u16 	%rs45, 1, 0, %p13;
	or.b16  	%rs46, %rs45, %rs1;
	setp.eq.s16 	%p14, %rs46, 0;
	mov.f64 	%fd53, 0d0000000000000000;
	mov.f64 	%fd54, 0d0000000000000000;
	mov.f64 	%fd55, 0d0000000000000000;
	@%p14 bra 	$L__BB13_21;

	st.local.v2.u32 	[%rd6], {%r2, %r4};
	mov.u64 	%rd83, $str;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 93, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd84;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd51;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r119, [retval0+0];
	} // callseq 93
	bra.uni 	$L__BB13_23;

$L__BB13_21:
	ld.param.u64 	%rd35, [%rd40+120];
	setp.eq.s64 	%p15, %rd35, 0;
	@%p15 bra 	$L__BB13_23;

	cvta.to.global.u64 	%rd86, %rd35;
	ld.param.s32 	%rd87, [%rd40+144];
	mul.lo.s64 	%rd88, %rd87, %rd31;
	add.s64 	%rd89, %rd86, %rd88;
	ld.global.f64 	%fd40, [%rd89];
	add.f64 	%fd55, %fd40, 0d0000000000000000;
	ld.global.f64 	%fd41, [%rd89+32];
	add.f64 	%fd54, %fd41, 0d0000000000000000;
	ld.global.f64 	%fd42, [%rd89+64];
	add.f64 	%fd53, %fd42, 0d0000000000000000;

$L__BB13_23:
	add.f64 	%fd43, %fd55, 0d0000000000000000;
	add.f64 	%fd44, %fd43, %fd54;
	add.f64 	%fd45, %fd44, %fd53;
	fma.rn.f64 	%fd11, %fd45, %fd12, 0d0000000000000000;
	setp.eq.s64 	%p16, %rd47, 0;
	@%p16 bra 	$L__BB13_25;

	mul.lo.s64 	%rd91, %rd31, %rd18;
	add.s64 	%rd90, %rd47, %rd91;
	// begin inline asm
	{ atom.add.f64 %fd46,[%rd90],%fd11; }

	// end inline asm
	bra.uni 	$L__BB13_27;

$L__BB13_25:
	setp.eq.s64 	%p17, %rd42, 0;
	@%p17 bra 	$L__BB13_27;

	add.s64 	%rd92, %rd42, %rd32;
	// begin inline asm
	{ atom.add.f64 %fd48,[%rd92],%fd11; }

	// end inline asm

$L__BB13_27:
	ld.param.u64 	%rd93, [compute_soft_kinematic_hess_cuda_kernel_backward_param_0+24];
	add.s64 	%rd94, %rd94, %rd15;
	setp.lt.u64 	%p18, %rd94, %rd93;
	@%p18 bra 	$L__BB13_2;

$L__BB13_28:
	ret;

}

 
